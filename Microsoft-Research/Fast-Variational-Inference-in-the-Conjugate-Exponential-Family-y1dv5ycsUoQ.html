<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Variational Inference in the Conjugate Exponential Family | Coder Coacher - Coaching Coders</title><meta content="Fast Variational Inference in the Conjugate Exponential Family - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Variational Inference in the Conjugate Exponential Family</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/y1dv5ycsUoQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
if you can start by thanking my
co-authors near loans you might know
Neil and Magnus wat PHA now move to the
University of Manchester I've been
working with these guys for a couple of
years now and reinforces in all kinds of
models Bayesian models that we tend to
solve variational me and after a little
while we were looking this models and
thinking how can we make them bigger too
with a big bioinformatics data sets and
so I'm picking with the variational
machinery and we totally failed at
making them bigger at all but we did
manage to make quite a few interesting
connections to some of the recent
literature and in particular we've made
the fairly standard vb update procedure
go quite a lot faster that's kind of
where I'm going to talk about there are
quite a lot of connecting themes and
here which connect to some ideas which
people may may not be familiar with and
at the end this what happens to be a
really tiny tweak that gets us quite a
lot of formed speed up ok so here's how
my talk is kind of laid out and start
talking about variational bays quite a
small audience so I guessing you're
fairly specialized so maybe we can skip
some of these parts and some people
recently been invested in this idea of
collapsed variational bass vocals are
about that then I'm going to talk about
how the optimization on these collapsed
kind of problems optimization of the
approximate Mysteria and finally I'm
going to tell you about the tweaks that
we can make that make this thing go a
bit faster ok I have no idea whether
this slide is totally trivial to all of
you or totally gobbledygook but maybe
you can give you some feedback in a sec
so
this is this is kind of the starting
point for any bayesian problem and I've
cut of it according to whether things
are tractable or not so we suffer some
likelihood PFD given H some prior p of h
and you can manipulate there was much
groupon we can evaluate them wherever we
want what we can't do is integrate them
so we can't integrate them to get either
the posterior don't cut a pink Twinkie
red because it's kind of not practical
or this normalizing here hoping that's
fairly straightforward for everybody
right good so here's a really
straightforward derivation how we get to
the variational base just swap the two
pink bits over and then introduced this
think you of h which is going to be our
approximation Asteria 4h meters the
hidden other babbles very sensitive and
and then take a log of both sides and
spit it out like this it's a fairly
straightforward sort of procedure and
then say okay now I can just take the
expectation on both sides under q of 8
so the left hand side isn't random in
eight so it's a change and we end up
with this thing here which is pretty
much the expression we need to do
variational base so we have this model
evidence on the left or which is equal
to this thing L which we hope we can
evaluate and this thing kale between
cure of H and posterior which we can't
evaluate but we know that it's positive
so I'll lower bound the model evidence
if I okay good variational beige kind of
suffers from a bit of a identity crisis
because there are lots of other
approximate infants methods fee might
also call variational Bates but when I
say very full base at least for today
this is kind of what I'm talking about
and you may notice that this thing I've
got a blue at the bottom here it's l
which is it going to serve as our
objective isn't necessarily tractable
but it might be tractable if there is
some kind of factor as
you rate this is the general procedure
for what we're doing we say 82 consists
of two sets of variables I'm calling X
and Z and what people generally do is
they have some kind of factorize
posterior of X in cube said there may be
further factorizations inside them but
that's fairly irrelevant and they get
this bound l and then we go away and we
have this procedure for optimizing q of
h and this bound all the little evidence
which is serving as our objective in the
optimization I am going to call lmf that
day L mean field for today because the
pendulum is mean field approximation
nothing new so far so once i was looking
at various models for doing things like
clustering of gene expression data and
so on came across lots of other lipid
care where people are doing what they
were calling collapsed variational
inference well okay this isn't like a
good idea the general procedure is you
take your joint probability d + 8 and
you might and lies out some of the
variables analytically so if you've got
a mixing model problem you might analyze
out the cluster means and variances and
the mixing proportions and you just left
with these things I the latent variable
Z or you could click the X or Z around
either way and then you can run some
kind of MCMC sampling on this collapse
problem and hopefully that's better but
what these guys including Mac swelling
and clover doing we're saying okay let's
make it modern I some things out and
then make a variational approximation to
what we've got left to this kind of
nasty collapsed distribution and the
problem is they said okay I can't
actually do so by doing it integral in
the first place I lost a lot of the
tractability of the problem and now I
can't perform my variational integral
properly at all so we have to make some
kind of approximation and amid a Taylor
expansion for ox emissions they took the
log of this
a collapse distribution and approximated
by its Taylor expansion around the mean
of cube of Zed anyway so what they were
doing was coming up with an
approximation to their objective which
they were then optimizing I said okay
that's not very nice because now
optimizing this thing and it's no longer
a lower bound and just kind of hoping
that it approximately something sensible
so I went back and after for a while I
came up with this alternative way of
looking at the similar kind of ideas
which is assuming model looks something
like this we've got to chose from 0 to X
and Zed and instead of Martin izing some
of them out take a variational
approximation to the conditional
probability the data given some of the
variables so this is just a standard
variational integral if you want so we
get rid of Z like this but we haven't
mods and lighted without properly we've
just made an approximation to the
posterior instead using cubes head
somehow but this is still conditioned on
X the cool thing about this kind of
variational integration is you haven't
broken what the call conjugacy that
makes the mean-field algorithm possible
so now you can get rid of X by dis
marginalizing so if l1 here I lower
bounds pay a fee given X then I can show
that take the exponent of both sides so
but e to the l 100 rounds P of B given X
going to substitute that straight in
here I've got another bound this bound
also bounced I the marginal likelihood
but it only had one variational
distribution so what I'm doing my
optimization of my of my variational
parameters I've got kind of smaller
space to optimize over I'm hoping that
going to be kind of easier that's my
that's my thinking behind this there are
quite a few problems that arise in a
second but
that's kind of where we're going it's up
okay yeah this is not really the classy
she's not ready it's not the same notion
of collapsing that was on the previous
sorry I mean the collapsing you're doing
here is not the same kind of collapsing
that was on the previous notes not
exactly that's a really excellent point
so this kind of collapsing here is
potentially much better so you end up
with this joint distribution of FD + Z
and that's quite rich right because it's
got a lot of things marginalized out and
then you say let's make a variational
approximation to that that's great the
problem comes when you have to make this
approximation then you have to take a
Taylor expansion idea and what everybody
is doing is taking a zero or one or two
order to a new expansion around some
meat so it's really only capturing local
information there's another another
thing going on here welcome to in a
second but the first order of embassy
here is people's foil me they spoil it
by introducing these other approximation
Sally but but my point is just that the
notion of collapsing is different no the
equations turned out to be exactly the
same once you make these other grades
yes so this collapsing is obviously
which is what does collapsing yeah so
this thing this type of collapsing is
kind of super rich but you ruined it
with the approximation yeah totally but
this type of collapsing physical appt
singh is all is not super rich you still
made this crappy approximation that q of
where i got a cube Zed it's totally
independent of X so you've still got the
means the other something in there the
only thing you've gained over the moon
field is the yoga with Winnie parameter
slot bonus so I'm not claiming my
approximations any better than me filled
in fact i'm going to show is identical
in a second but i am claiming i also
that neither is this one neither is true
collapse infants because the
first order term the Taylor expansion is
zero the zero order Sam gives you
exactly my approximation it's very
straightforward to proof and I'll talk
about that in a second and the second
order term well people have only done
that for specific models and I do all
kinds of horrible things like assume
that this of Gaussian t somehow through
some law of large numbers even though
it's discrete variable and actually was
a paper last year that said the best
thing to do is just to drop the second
order terms it works much better see ya
did you see that so I think the sake is
a bit wav the second order terms because
I think about these things as just
sufficient statistics so second order
terms we pretty critical if your
distribution was Gaussian because there
would be in the sufficient attest Ock's
but for the street variables second
order terms seem a bit weird i don't
know maybe you have sports on now okay
so they learn a fool say but after this
approximation they come out being
identical so here's one of the cool
connections that we've made in there in
the mean-field variational approximation
this is a procedure but we update once
I've available so then the opposite of
variables or we update them in a in a
round-robin sort of thing and they said
step this distribution to be like this
and you get a kind of a message passing
like approach and in little to the right
seat is certainly in the textbooks it
says Q star of X is proportional to
these things and I can tell you that the
constant proportionality was missing is
the bound that I've derived or the
exponent of band that I've derived so
this thing here looks a bit ly Bayes
rule his PX is the prior here's
something which lower bounds the
likelihood like I showed a couple of
slides ago here's so e to the minus kr
may be back live on the bottom there's
like a normal a so it is a little babble
model evidence and here's some
approximation of steering on this side
it's exactly like facial so this is what
happens so if you take this formula on
the bottom here and really dig a little
bit this is where you end up with and
you can see that you could have started
with X or said so this message passing
that goes on in mean field variational
base is a bit like sort of fixing the
sufficient statistics for some of your
variables and then doing raising infants
the rest of them because the graph is
now a tree or something you can do in
from sin and then fixing that part and
do bayesian sent back in the other side
okay so it is dis a little bit of
background on the little chap is lots of
people have been interested in this
people are doing it years ago but
they've come up with all kinds of weird
wonderful derivations and I think this
is kind of the cleanest swamps going on
and this is why you were talking about a
second ago this is what we're doing by
taking this Express part of the
expression there are the terms in here
but this is kind of a crux of the
problem we take an expectation of a log
and the cube said exponentiate it and
they get rid of X collapse vbr doing it
the other way around Britt is as you
pointed out much for a glitcher but
don't you make this approximation that
you have to bring this Q inside using
your taylor expansion the two end up
being exactly the same and let me talk
about second order terms as well second
order terms may I'm going to help very
much that's kind of open question I'd
lesson discussion on that here's my
summary of what's happening with the my
approximation so you can see it's glad
Phoebe problem of showing that kind of
equivalents you can see it as this idea
of doing exactly influence and half the
graph at a time and then just fixing
with sufficient statistics on each side
or this is
really cool result which is that exactly
the same as doing me feel difference you
show that the mean-field bound can be as
good as my belt by the inequality has an
equal sign underneath it and the way you
could do that would be to just fiddle
around with Q of Z but whatever you did
always keep Q of X updated so whenever
you move Q of said before you did
anything else you immediately update the
other the rest of the variables bath
some pretty important ramifications I'll
talk about a second right these are any
more questions okay so the next thing is
people off something really important
here which is we've lost this cool
procedure for optimizing our moon field
approach which said do this procedure on
these nodes and this procedure on these
nodes and this procedure on these nodes
and you've always increase the mound
because now we've only got one set of
Arabs we've only one set of a
variational distribution to deal with so
how are we going to optimize them so
other people are doing is saying but
we'll just sort of parameter is it in
some freeform way and then throw it into
some kind of optimizer empirically I
shown this works really really badly for
reasons that I'll elaborate on a second
the other thing you could do is say well
there's some remaining factorization
inside these notes that i am explicitly
for ammeter izing so there's similar
meaning facto relation inside Zed so
there may be there the latent variables
in a mixture model problem and I could
update them in a round-robin and kind of
procedure but that's the dumbest thing I
can think of because you're going to do
expensive for analytics as expensive a
computation for a single latent variable
as you wear for the mean-field approach
all of the phones so you just made your
thing way way harder
so then the scent of mine auntie honk
Oliver this paper where he said the UVB
and using women gradients I'll talk
about those a bit more in a second and
make a fairly integral part of the
puzzle here and these things that we're
going to compute these gradients are
really straightforward if we've already
got the mean-field problem so we really
got some code that does the Mueller
problem i can give you all the code to
do it my way in just a couple of lines
because of this property so the vbm
algorithm this procedure i update one of
the nodes of the time it's actually a
steep descent method it's a coordinate
wise steepest descent method and this
was shown for some small models in 2001
and it's kind of the cracks for work by
Dave blind mat Hoffman I mean Paisley at
the moment on stochastic variation
lessons people have come across that
these updates you can see there was
message passing things but you can also
just see them as steps in our gradient
direction the derivation is kind of
horrible did paper in 2001 by this guy
Misaki Sato is like six pages long just
full of lines and lines of equations
it's totally impenetrable but if you try
it for any model that you then you care
about you'll see that it's true and the
cool thing is the gradients of the bound
that I'm proposing our wages of the
gradients of the mean-field bound like
this so I'll not go through the mouth
too much but we were basically
interested in the derivative of the
bound of respect to the parameters of
theatres Edd and that's given by this
expression here for the mule bound it
depends on what Q of X is the company
setting and the derivatives of my bound
are kind of identical but after you've
updated QX in the mutual setting that
make sense
I think do something similar for the
curvature in the care because it makes
four terms pop up and the whole thing
can be summarized pretty much by this
where this graph which says here's the
bound that I'm using he's the mean-field
bound where I'm assuming that theta X so
the parameters of the other distribution
that I have collapsed out I fixed and at
some point they meet like that at some
point they're equivalent and their
equivalent after you've updated theta is
dead and at that point not only are they
equivalent but the pic gradients of
equivalent as well so if I want to get
the gradients of my thing all I need to
do is look at the radiance of the
mean-field thing and somebody else
already shown that the mean field update
procedure is a gradient method so if I
brought me for not this easy they
perceive yeah I got the gradients which
means of us with the greatest of my
thing working backwards bit of a logical
step there but it works I can also show
some properties about the curvature
bound as well this basically is a
requirement for my bount always be
higher than the mean field bound ok
is that okay so do you guys know
anything about information geometry know
a little bit I know okay so like if like
my 32nd undergraduate level walk through
it just makes it totally clear for me
not too much math involved but but
optimizing these distributions right and
then think about your distribution being
a Gaussian with a mean and variance and
you've got optimize the mean variance of
this calcium so I did have a really cool
demo on my laptop but it died it's as I
started to set it up here so I'll just
draw it on the board but think about
this pleased to meet my Gaussian and
here's the variance like two
distributions once here and once here
and then say well so this distribution
has a mean over here I mean over here
and it's kind of narrow and this
distribution has a mean over here and it
I love wider but the point is what is
this line mean the Milan means nothing
at all the Lions totally minions all in
other words if I'm here and I want to
make it radiant move in that direction
then that's kind of done because I could
have just done this let's have a mean
here and log worn over Sigma square here
and I've got some other planet ization
now clicking unit steps or taking a
gradient step in this parameter space
it's not the same as taking a rating
step in this parameter space that kind
of claim so
this idea is Euclidean distances between
two points or moving along a line which
is what we do when we optimize in a
parameter space is really done so what
do we do well we're saying what kind of
have some intuition about what we should
do with distributions people spend a lot
of time looking it divergences between
distributions if you take your metric of
interest as how we measure the distance
between two distributions the KL
divergence we end up with a Riemann
manifold of distributions whose
character is defined by the fish of
information that all sounds really
technical and I put down my pointer but
what it means in practice is we've got
some reasonable way of describing how
far apart we distributions are totally
independent 54 and flies them what we'd
like to do is optimize somehow sort of
using that information so those the sky
and he still works in Japan name is
Sudhir Maui and he's written this other
totally impending from the tone around
information geometry and the key result
is if you've got a gradient in one of
these parameter spaces and you want to
move along it the direction should move
in is not given by the gradient it's
given by some linear transform of the
gradient and the linear transformation
verse fish for information so here's
some things about the information
geometry the opportunity interested this
is the key so you've got the derivative
of your objective with respect to some
parameters and this thing that equals
the natural gradient or the direction
you should move in is given by inverse
of this Fisher matrix which is totally
dependent on Q of of Z if things that
you're this distribution you're
optimizing already was my plan together
that the direction moving and this is
really nice result for the exponential
family which is all to do with the way
is parameterised we're going to give you
a little bit of background here
hopefully you're thinking oh crap now
every time I need to move in my
optimizer and they can revert our matrix
well that's not very nice at all the
neat result is you can get rid of that
inverse through the properties of the
exponential family so if you have an
exponential family distribution which is
I'm sure you aware anything like a
Gaussian or price or more most of the
distributions are familiar with there
are two parameterizations the canonical
parameterization on the application
thermalization which have this property
resist that g of theta is equal to the
derivative of 1 parameterization with
respect to the other which means we can
just plug in the chain rule and get the
natural gradient an ETA is given by the
gradient and theta or alternatively the
natural radiant in theta is given by the
gradient eater ok well that's kinda nice
now I can avoid my Fisher inverse but
there's another cool connection here the
mean-field algorithm is doing this
already mean-field algorithm is giving
us a unit step in this natural gradient
direction that's what these guys were
proving a little while ago but the cool
thing is the means of algorithm is
already sort of circumventing this
inverse G of Peter ok that's kind at the
end of my rambling about connections in
variational base
so what do we do to make this faster
what do we do what did I implement to
guess and cool results so I see it's a
couple of models and I worked out what
this bound would be and I worked out how
to work out this this stephanie you can
do it using the mean field equations and
converting them or you can just do it
directly but if I just took unit steps
in the natural gradient direction I just
shown that just turned backwards I guess
that I would end up with the mean-field
algorithm again so that would be pretty
stupid so the tiny tweak that I made was
just I let's just do conte good
optimization so here I mean conjugates
in the parameter space geometrically
Gandhi ass against distribution
conjugate it confusing you use both of
those words to me too as far as I know
unrelated things but amongst optimizes
every time there's a little line
searcher would take some steps it
changed direction slightly according to
the previous search direction sure most
people are familiar with that idea and
you have to compute this conjugate
variable which says how much of the
previous search direction should i
include in my current search direction
so there's some nasty things to do they
have to compute some inner product Sonia
rumor manifold and Nancy Hong color the
guy was doing this originally showed me
how to do this fairly straightforward
and its really really cheap just turned
out to be an inner product know your
matrix inverses required so his
here's the first result that my
algorithms pass out this is a mixture of
gaussians and we've got fire kind of
blobs here the black outline is showing
with the outline of identity this is
Phoebe e.m this is the mean-field update
method and each box shows us one one
step of the album are basing all of the
notes once the missus our method from
the exactly the same initial condition
this is a totally a typical run I've not
pulled this out as we're from the same
this condition the first step is
identical because we've got no conjugacy
I first step is exactly the same the
second step not too different but just
after a couple of steps you can see that
what we're doing is taking bigger steps
on our bound because this thing is
collapsed we can take bigger steps if
you're going to worry about updating the
other variables before we climb back up
again maybe Dustin to clear but I think
it's pretty close you know one step in
my algorithm quickly starts getting you
more pleasant solutions than steps of
the VBR so this thing actually converges
a few is a recent O'Donnell so there's
some numerical evidence here so I
thought a little bit about how can I
show people that this thing is better I
mean sometimes the two albums converts
of different solutions because they're
kind of local minima in the problem so
what I try to evaluate was if I was to
use my algorithm how many iterations
would I expect to use to find what I
think is the global open so I counted I
ran the fee optimizes the vbe m the
mean-field updated things and my things
the 500 random restarts using the same
sets of research each and then I counted
the total number of iterations taken to
convergence each thing and divided by
the number of times that I found
the best solution that I knew around or
came very close to the best solution
that I know about and there's also a
little bit surprising so what's
happening here is I've got the problem I
feel you on the previous page with
somebody else devised so I copied we've
got this mixture of gaussians problem
with various separation the of the data
so for I was one there pretty much
overlapping four hours five really wide
apart and this is be expecting them of
iterations of these 500 restarts and for
vbm sometimes it just doesn't find as
good a solution if any of the restart as
we can find using our conjugate method
it is really surprising I'll talk about
in a second but in general at least
three first lines of various slight
tweaks on our algorithm different
context see calculations in general why
kind of kind of significantly faster
generally do this problem so we looked
into why vbm wasn't fine with such good
solutions because we're using the same
convergence criteria we can compute
using our knowledge of the the riemann
manifold we can compute the true
gradient length so the length of the
gradient vector on this manifold is very
straightforward we can use that as a
converted try teria we can use the
change in the bounder supplies from
bacteria we can use change some of the
parameters the convergence criteria
matter how you look at it our our
algorithm is finding better solutions
and we think what's happening is there
are plateaus in the objective surface
because we know that vbe m the
mean-field procedure is a steepest
descent algorithm if it finds somewhere
that's very flat is just going to stop
it was because we've got this conduct
see part on our algorithm we says fine
summer that's flat we'll just keep
adding in parts of the previous update
until become out of the plateau
hopefully find that we can raise up a
little bit later
next thing ok so I took all of the
papers from the 2011 nipped conference
and a wrap Layton too risky allocation
exactly the same kind of thing and this
time we've got about ten fold kind of
speed up so the support of my logarithms
it takes about thirty eight minutes to
do it BBM takes about three in from
seventeen minutes this might seem like
big times but this is probably because
my Python implementation is signing up
quite a lot of my computational power
but the two things are approximately
equivalent apart from this tiny nation
of the the Condor goodness this is for
the typical graph looks like this time
on the x-axis and the objective function
on the wax you can see it optimizes much
faster and by the time the algorithms
have converged we're still a fair few
nuts off Amelia method ok any questions
Froy wrap up yeah so don't you get a
venir mode restart your Fletcher reads
occasionally yeah yes i did sorry i said
this these things kind of come with a i
think that says you should we start as
often as the number of i'm not very good
optimization right but yeah i use some
kind of heuristic all tricks I use I've
got the heuristic fix that fairly
standard like we start the computation
of bt every so often sometimes basicos
negative know anything about it goes
negative sign I just say okay make it
zero some reason the Pollock will be air
one which seems to be the most popular
literature that doesn't work sometimes
but I think it's because of the
weirdness of beauty on this manifold
okay yeah yeah most of these methods
some of them require exact line search
to have been done right okay whereas
others don't so so so you may see a
difference if you haven't done an exact
line search another exact line search I
don't I show you me like are you doing
an exact line search when you as you
find your search direction oh yeah okay
okay no okay sweet right that a little
bit light side is a bit weird on this
manifold right there are 22 particularly
weird things first is that the
mean-field algorithm takes a step of
length 1 I don't know why but that's
what it does takes is one of these
conjugate these these steps of length
exactly one so that's kind of blinded as
well I fix their path to one I seem to
work okay let's do a line search I
unless you slide standard wolf
conditions now it's a bit confusing you
how to compute the bullpen ditions on
this manner hold because length is a
real weird so to compute the length on a
manifold you have to take into account
the official information metric so just
to compute the worst conditions are fair
bit of thinking about about how i would
do and i have to really looking if you
just consider the worst conditions in
the in the parameter space for getting
the manifold for a second it's in the
face of parameters then they seem to be
satisfied reasonably well I step length
of one all the time and yeah I know it
so normally you do it this content
things don't really work you found the
minimum along that direction so that you
can see right I know it's weird isn't it
so there's a strong I have a strong
feeling that what's happening is
slightly the conjugate isn't really
helping
but the faculty taking bigger steps is
so if I go back to this picture this is
just a somatic right but if I'm here and
I want to move on the mean-field bound
wonder the mean-field update i know
we'll find the optimum conditioned on
the other variables being fixed but
because my thing is kind of less curved
it was arrayed more slowly maybe I can
actually take a bigger step on my thing
when I could have done on the moon feel
about so one thing that I haven't tried
yet is just taking the mean-field update
applying it to my thing and it's
multiplying by 1 point 1 I seeing if
that works just as well because maybe
the conjugacy is just it's just making
for me no I guess is it'll work okay for
the first few iterations and then you
better go back to one okay yeah others
may have better guesses so as I say I'm
not I would never claim to be any kind
of expert in optimization I would love
to be but it all eludes me a little bit
I yeah I understand I need to align
search but this thing sort of heuristic
Lee seems to work disturbingly well
which seems to be the case in
optimization right there even these
computations for the conjugacy a kind of
approximation to support happen if we
had a quadratic and yeah so kind of
 in a bit seems to be the key in
optimization okay I've up up
supposed to be doing vb like this well I
think my derivation was pretty
straightforward the way I showed you you
take the variation approximation and
then Martin eyes but then do any Taylor
expansions or anything I've got a proper
lower bound on the margin like we didn't
have with the other collapse EB case so
hopefully optimizing does something
sensible or at least it will converge
have expressions a simple in vb am so
you can obtain this bound by taking the
vbm bound on the likelihood of
substituting one of the update equations
back in you find the whole of the status
cancels out so actually to compute the
bound just less computationally
expensive there's less terms in the
equation yeah you just did steepest
descent and its method then you would
get to vb am back it's really easy to
work out if they've got existing code
that does the mean-field algorithm how
do i how do i use it to work out the
gradients in my oven that's pretty cool
i looked at differing large-scale pliers
as well so non parametric models no it's
pretty well and of course the real
reason we're doing this is performance
it just seems to work about ten times
faster for lots of moderately difficult
problems the additional overhead of
computing the conjugate to conjugate
number and the optimization is tiny
compared with the cost of the update so
it really adds on the overhead at all
okay why not why shouldn't I do it like
this well you've only want to do a
properly collapse problem this isn't a
lapse at all this isn't giving you the
nice which posterior that akal a problem
to give you but then neither is anybody
else's first order approximation
and we never achieved our original goal
so what we're really looking for was how
we scale this thing up to be bigger and
you can see fairly straightforwardly by
the similarities of the mean-field
approach the scales in exactly the same
way it's the mean-field approach okay
I'm done thank you very much yeah i know
this is donovan youth although you've
got a little matrix G of theta inverse
cheap but Fisher information matrix is
where the yes both spotted Batman if you
were doing a Newton method that's
exactly where the Hessian would go isn't
that weird there's another thing to
notice here which is okay I'll come to
that point in a second though it's so
we're ideas out for a second it's not
the Hessian though it's not the
character the objective and we don't
have to compute it or invert it is the
other thing because we kind of go round
are using these tricks that's kind of
neat I it's only going to work those
tricks getting rounder we're only going
to work in the exponential family
because the because of the derivation I
showed you the next part of the problem
is that that T of theta that we're using
is the true Fisher information for the
mean-field bound but actually is not
quite the right official information for
our bound then this takes a bit of
thought and I haven't explored it very
far but if you
our parameterization of the problem has
just been parameters by one set of the
variables you say okay I'm always going
to update the other set no matter what
I'm doing so if time I move this one I
move this one kind of empower then
there's kind of more information
attached to that original set of
variables then the mean-field approach
would let you believe so the second
distribution isn't totally really
totally independent as you move the
tables around it moves along with you so
this kind of these extra terms you
should really add into that fish
information arrive from that kind of
effect maybe that's not very clear
unless you're totally absorbed in the
problem like i am yeah do you have any
idea any other connections between kind
of the Newton method and what's going on
here I mean the obvious next thing is it
causing even is it the j transpose J of
something and there's a hint that there
might be because you talked about two
parameterizations sort of canonical and
natural or something yeah so if you
write down what are they for a Gaussian
so the short answer your question is for
Gaussian they are brothers they are
Sigma inverse time from you and minus
half sigma inverse these are the season
Tita Sol and the other one is mu and nu
u transpose plus Sigma music eater
that's the short answer so you want to
move this in the direction of the
gradient Ernest and this the direction
of the gradient in this and the slightly
longer answer is there in any
exponential distribution there however
you're right like this so P of X 2
so they appear in this form in the
exponential form theta appears like that
and eater is equal to the expected value
of T of X way so is this this ladies
you're probably familiar that these are
the expected statistics of the Gaussian
this for a Malaysian is pretty weird but
that's that's kind of what pop pops out
in that form okay
any more questions okay there no more
questions then let's time James Akane's
and James will be here til 430 or four
o'clock when he has to catch his friend</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>