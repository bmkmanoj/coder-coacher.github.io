<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 7 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 7 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 7</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zg5UWel8Jmo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay up next we have our spotlight
presentations and as always if you have
questions please visit them tonight at
their poster hi I'm I in Chakrabarty and
the title of my paper is color constancy
by learning to predict chromaticity from
luminance so what's color constancy the
color of an object that we observe which
is essentially the color of the light
that reaches us is loosely speaking a
product of the color of the object
itself and the color of the light
illuminating the same what this means is
that when we see the same object under
different lights the color we observe is
going to change with the light and yet
the color we perceive is not what this
means that our perceptual machinery is
able to extract the true color of the
object from the observed color and
disambiguate the effect of of the
unknown illumination now this is this is
what's called color constancy and it's a
fairly remarkable property of the human
visual system and what we'd like to do
is to do this computationally because
it's useful for applications ranging
from white balance and imagery lighting
to using colour as a reliable cue for
illumination so folk people have been
working on this for a while and it's
essentially an ill-posed inverse problem
and what complicates matters is the
world we live in is pretty colorful the
distribution of true colors in nashville
scene's is in no way compact and as a
result of this most modern methods for
color constancy have sort of evolved to
use complex features like you know
spatial derivatives of color semantic
context and so on in this paper we sort
of bucked the trend and go back to
looking at individual pixels one at a
time without looking at their spatial or
semantic context so to give you a brief
overview of what the method does
essentially we'll be looking at one
pixel at a time and you'll and we'll use
the pixel and each pixel is going to
essentially cast votes for the color of
the scene illumination and the way it's
going to and you know we're going to
assume a single scene illuminant in this
paper and so we're just going to
aggregate these votes add up these votes
across all pics
so the core of the method is essentially
a luminance to chromaticity classifier
what this means is that we're going to
look at the luminance or average
brightness value of a single pixel you
can think of this as looking at the
pixel in the gray scale version of the
observed image and we're trying to go in
to try and reason about the hue and
saturation of that pixel and there are
two ways we're going to learn this
classifier the first is just very simply
by using empirical histograms and what's
surprising is that even with this very
simple method we're actually able to
match state-of-the-art performance of
methods which use far more complex
features what is more these histograms
are actually you know interpretable we
can look at the relative distributions
and sort of you know interpret them in
the context of physics and color science
the second version is gone in the second
version we're going to learn this end to
end and you know we go to keep the same
parametric form but you're going to use
stochastic gradient descent and back
propagation but remember you're going to
use this to learn a histogram not a
complex neural network and it's also
going to involve some form of dropout
like regularization strategy so if
you're interested in knowing more and
you're you know interested in sort of
looking at the implications of this for
national image statistics and other
applications please stop by postal
number 16 thank
hello so I'm presenting backpropagation
for energy-efficient neuromorphic
computing and this is work by myself
Steve esser and my colleagues at IBM
Research kumaraswamy Palmer ola john
arthur and armand de moda so the main
motivation for our work is an interest
in using neural networks to solve
real-world problems in embedded systems
such as cars or phones such systems of
course have performance requirements
like accuracy and we have many examples
where machine learning can deliver on
that requirement but they also have
platform requirements like low power
consumption a small physical footprint
and real-time capabilities and we
believe that neuromorphic computing is
well suited to deliver on that
requirement we're particularly
interested in the true north
neuromorphic chip this chip has 1
million neurons 256 million synapses and
can run using very little power only 63
milliwatts to get that low power
capability however it requires spiking
neurons low precision discrete synapses
and places certain constraints on
connectivity all three things of which
don't map that well to traditional
machine learning which likes to work in
a continuous differentiable space so how
then can we bridge machine learning with
neuromorphic computing well one approach
a simple approach would be to simply use
conventional methods to train a network
and then figure out some way to map that
network to the hardware to the
constraints that's not what we do but it
can contrast nicely with what we
actually do which is to take the
hardware constraints and build them
directly into learning algorithm itself
so a constrain then train approached the
training algorithm can then be used to
produce a network that can be directly
implemented on the hardware the
specifics of what we do involves using
back propagation operating in a
probabilistic space that has a direct
interpretation in the hardware and using
this approach to train a network and
then running that network on the true
north hardware we achieve near
state-of-the-art accuracy on the amnesty
de sete we do this using three orders of
magnitude less energy per classification
than the next best low power approach
that we could find in the literature and
finally we have some breaking results
that didn't actually make it into the
paper but extending our approach
to include convolution networks we're
able to achieve near state-of-the-art
accuracy on a number of additional data
sets which you can see listed here and
if you'd like to learn more please join
us at poster 13 tonight for details
discussions and a live demo running our
networks thank you
so my name is jamie bell and i'll talk
about awake sleep attention model so
there's a joint work with my colleagues
roslyn solar cutting off Roger gross and
Brandon fry so deep covenants are
amazingly powerful however in practice
if your input is very high dimensional
or volumetric data they can be very
expensive to evaluate and recently there
have been some new development in the
recurrent visual attention models so so
in the example shown at the bottom of
the slide and those are very exciting
new research directions because not only
to overcome the skill builder the
limitations of count that in test time
but also they provide some great insight
on what model has learned and so what's
the problem here for next generation
system so the learning signal for the
RAM models are stochastic way that's
estimated using the Monte Carlo samples
and often time they can be very noisy
and just from like the policy surgery
and reinforcement learning the high
variance can handle the learning or
often make the learning unstable so in
this project we set out to improve the
learning algorithm for Rams so unlike
the previous works that example from the
prior or the policy Network we
introduced an additional influence
network that's showing the blue so
intuitively that's saying if I know I'm
looking for some kind of bird that's
penguin and this knowledge to drive my
attention to look for the underground
instead of the sky so the additional is
influenced network acts as a teacher to
provide better guided search and also
with directly optimized for the
log-likelihood instead of just a
variational lower bound and turned out
with the limited samples it's a tighter
lower bound than the previous methods it
also we investigated some of the the
control Varitek niques or the baseline
techniques to further reduce the
variance of the gradient approximation
so with all the bells and whistles you
know project what's the final punch line
so the Wix liebrand models provide a
great reduction in the variance
estimation as showing the first plot and
so this directly translate to a much
better conversion speed in the training
time so we also experiment with the
model that attends to different levels
of the conf net and the seamless stories
has been preserved so we turn to so for
the for this whole paper we hope to
provide some new inspirations for
training the stochastic attention models
and also for reinforcements learnings so
for more details please come well poster
thanks
hi I'm Simone oh and this is joint work
with hana no and my advisor professor
Wang Han so this paper tackles the one
of the most center contribution problem
semantic segmentation which aims to
assign intense cement label to each
pixel in the image especially we are
interested in semi-supervised learning
of this problem where we have a few
training images with segmentation
annotations but much larger number of
images with image level class labels the
main challenge in this problem is to
train the model with these heterogeneous
and unbalanced training data so the key
idea of this paper is to decompose the
semantic segmentation to separate
problems of classification and
segmentation and learn individual
network for each task separately so the
figure on the top described the over
architecture of the proposed algorithm
our network is composed of three parts
the classification network and
segmentation network and the bridging
layers connecting the two networks in
this model semantic segmentation is
performed by separate or successive
operations of classification and
segmentation so given an input image the
classification Network identifies class
labels associated with the image and the
segmentation Network produces the
foreground segmentation of each
identified label given this architecture
training the model with hybrid o
notation is easy and straightforward by
training the classification network
using the examples with segmentation of
notations author with imogen level class
labels and the training the segmentation
network using the examples with arms
segmentation annotations
the result shows that decomposing the
classification and segmentation is very
effective in the semi-supervised
learning scenario our method outperforms
the state-of-the-art same is provide
semantic segmentation algorithms with
substantial margin even using much
smaller number of snow strong notations
also comparison to the approach to use
only strong rotations for training show
that the contesta coupled architecture
is able to take advantage of both weak
and strong annotations to conclude we
proposed a noble decode architecture for
semi-supervised semantic segmentation
due to the simple nature of the proposed
architecture we expect that we can
extend the similar principle to
semi-supervised learning of the other
structure prediction problems so for
more information please come to our
poster thank you hello I'm Keenan
whirling this is joint work with Arun
chaganti Percy Liang and chris manning
and the stanford NLP group so I'm up
here advertising this new framework
we're proposing on-the-job learning and
the high-level idea is that if we allow
ml models to query humans at test time
in a principled way we can solve a lot
of the problems with deploying ml in the
real world so let's imagine that last
night there was a natural disaster in
New Orleans and your lab decides to help
by monitoring Twitter for useful
information and passing the data you
collect on to relief workers now you
consider two methods to do this first
you could use crowd workers to parse
every tweet as it arrives or second you
could use ml techniques like batch
online or active learning now the
benefit of the crowd is that their
accuracy is constant in the number of
examples that have been seen by the
system and so they can be deployed right
away however every new tweet requires
another human to look at it and so your
costs can grow unbounded Lee large and
probably won't scale to Twitter ml
classifiers on the other hand don't
require any human labor on each example
so they will scale to the Twitter fire
hose the catch is that ml classifiers
need a certain
minimum accuracy to be usefully
deployable which usually requires that
humans label a large number of examples
which can be expensive and take a long
time so on-the-job learning is this new
framework that gets the best of both
worlds if we let our ml model query
humans at test time and learn from their
responses before returning an answer we
can hit the ground running like a human
and scale like a machine so here's how
this looks in practice let's imagine
we're interested in finding the
locations of resources and people out of
tweets so we're doing some sort of
sequence labeling so it sweet arrives
the model decides that the token George
is ambiguous and so it's going to launch
two queries in parallel in order to hide
the latency of these human labelers a
few seconds later we get two responses
back the model decides it's still not
certain enough and so it launches an
additional query on George and Aquarion
street because that might be informative
a few seconds after that the system is
able to arrive at the correct labeling
and turn it in all right so managing
this asynchronous real-time querying is
hard and you have to balance latency
cost and accuracy when making these
decisions we cast this is a game and we
use Beijing decision theory to formulate
the optimal choice but since exhaustive
calculation is intractable we use Monte
Carlo tree search with progressive
widening to handle continuous time as an
approximation our results are excellent
with human labor augmented by on-the-job
learning going around 11 times as far as
it went unassisted so you should stop by
our poster which is number 15 and that's
as a mnemonic this year thank you
hi my name is armand Jeanine and i'm
here to present or joint work with the
mesh microphone inferring algorithmic
buttons which documented recurrent
networks hakan networks are powerful
model for sequence leveling a prediction
which has obtained state-of-the-art
performance on many task however it's
still have difficulty to learn complex
patterns for example sequence generated
by algorithms which contains
memorization out of the reach of
recurrent neural network in this work we
propose an extension of Alan to be able
to learn this algorithmic patterns our
contribution is to add a structured
memory to the laker neural network it
can note what right and went to read it
its capacity is unbounded which means
that it can grow as the complexity of
the input sequence gets more complex and
we must be focused on in this work on
memory which are based on stacks but it
is easy to extend it to other memory
topology like lists or tapes as we shown
the figure the memo is controlled by an
action Bible which is controlled by the
hidden layer the action that can be
performing these tasks are push a value
on the top puppets or do nothing the
value stored on the stacks are nonlinear
a confirmation of the hidden layer so
everything is continuous and can be
trained with any optimization schemes
such as stochastic gradient descent here
is an example of what is alone on the
task of a binary addition so to input
our concatenation of binary addition of
two numbers and we provide no
supervision for model basically it just
has to predict the next token in the in
the sequence as you can see it learns
how to make its tax collaborate and some
of the tax learns to store the input
numbers while others learn to do how to
do the carry the code for the for this
model is available online and if you
want to know more about how a model
works please come to our posture thank
you hello my name is junior call and
this is a joint work with Xiao shall go
nutley Richard Lewis and sat in their
scene in this work we consider a new
video prediction problem where the
future frames are dependent on control
signals as well as the previous frames
we think this problem is important
because this is learning a model of the
dynamics of the Asian environment
interaction in reinforcement learning
context to solve this problem we
proposed a new deep neural net
architecture that performs action
conditional future prediction based on
convolutional neural networks and
recurrent neural networks we also
present one application of our
predictive model by improving
exploration in the actual control
problem here is the summary of our
reserves our model can make realistic
long-term predictions up to 32 500
future steps depending on game domains
and the top figure shows some examples
of predictions over 50 steps and also
our model is used to improve exploration
in the actual gameplay the second figure
shows heat maps of the player trajectory
in game pac-man and you can see that our
method in the right side explores the
entire map more extensively than random
exploration and this method improves
deep canet work which is the
state-of-the-art al player on some game
domains finally our model can discover
correlations between actions and
distinguish control
and uncontrollable objects in image by
simply learning to predict future frames
depending on actions for more
information please come to our poster
and sees some demo videos or predictions
thank you hello I'm uvula rel and this
is joint work with Ranma here and
Manfred offer our motivation is studying
neural encoding and decoding in
biological systems so the setting is the
following we have a dynamic environment
state which is observed by an organism
and encoded by sensory cells as series
of spikes where the instantaneous firing
rate of each cell is given by tuning
function mapping the current state to a
firing rate these banks are later
decoded to produce behavior and we model
decoding as online filtering producing
the posterior distribution of the state
conditioned on the full spark history so
in this model we can ask two questions
first how is decoding performed and
secondly with the model of decoding we
can study optimal encoding as minimizing
the coding error most existing work in
the setting falls into three categories
it either uses numerical Monte Carlo
methods which tend to lead to limited
analytic insight whether you assume a
fixed date and study asymptotic results
for infinite time decoding or else they
assume uniform coating not uniform
coating means is that the tuning
functions are placed on a uniform grid
so that the Sun is approximately
constant this assumption leads to a
coffin posterior and a closed form
filter so in the more general case of
non uniform coating we have a
non-gaussian posterior leading to an
intractable infinite dimensional filter
we propose a method to overcome this
difficulty as follows we parameterize
the cell population by distribution of
the tuning curve centers this allows us
to apply assume density filtering to the
resulting filter
and gives us closed form equations for
the posterior moments compared to the
uniform code in case these equations
have an additional term corresponding to
information from lack of spikes this
term causes the posterior to change
between spikes and not just at spark
times and as in the uniform coating case
to illustrate this here is an example of
recording a simple one-dimensional
process which is shown here in black the
yellow dots are spikes and they're
vertical location is the center of the
tuning curve of the firing sell the
shaded area in blue shows the output of
our filter that is the posterior mean
and posterior standard deviation we can
see that the posterior changes between
spikes and moves away from the center of
the population reflecting the fact that
absence of Speights indicates a low
firing rate which is obtained at the
tails of the distribution our model also
successfully explains physiologically
observed encoding strategies as
illustrated in our poster so please come
see our poster for additional details
thank you this concludes the first
afternoon session join me in thanking
all the speakers again for their
presentations
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>