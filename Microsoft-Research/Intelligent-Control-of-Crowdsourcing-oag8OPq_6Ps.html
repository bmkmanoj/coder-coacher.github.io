<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intelligent Control of Crowdsourcing | Coder Coacher - Coaching Coders</title><meta content="Intelligent Control of Crowdsourcing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intelligent Control of Crowdsourcing</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oag8OPq_6Ps" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone our guest speaker today is
Professor than well from the University
of Washington across the bridge over his
career then has made contributions to
many areas of computer science his
brother speaking his interests lie in
making computers easier and more
effective to use which includes various
aspects of HCI crowdsourcing it used to
also include sequential decision making
under uncertainty I don't know if that's
true anymore but anyway that's how I got
my PhD under his supervision then is
also an active interpreter is he is a
member of the madrona venture group he
has started several startups and with
that I'm going to head him and over the
stage to him thanks so much so it's
great to have a small audience make it
really interactive trying to cover a
reasonably broad span of material some
of which may be be able to see which
case we can go right through that and
some of which I know you haven't seen
because I didn't see it until yesterday
and so we can spend more time talking
about that as Andre said one of my
biggest interests is making computers
easier to use and especially be
personalized whether it's different kind
of interfaces on phones or tablets or
whether it's the the agents in Cortana
or Amazon echo or what-have-you speech
based agents and I think if the
crowdsourcing work is fitting into that
that frame on the other half of my life
is machine reading and information
extraction and unfortunately i won't
really be able to talk about that at all
although you'll see i snuck it in part
way through
so thinking about AI and crowdsourcing
if you just spend lots and lots of work
of AI models applied to crowdsourcing
but almost all of it in this notion of
collective assessment making sense of
the results that come back from the
workers and this is a very very passive
view very much focused on the state
estimation problem in AI and how do we
sort of track what's going on track the
skills of workers track what the right
answers are they gave them as hidden
variables and so on and the overall
thesis of today's talk is that there's
lots more to AI and there's lots other
ways of applying that to crowdsourcing
so we can use AI methods to to take and
optimize simple crowdsourcing workflows
trying to reduce redundancy where we
don't need it and put more where we do
we can look at complicated tasks and and
use AI methods to optimize large-scale
workflows and this subsumes a be testing
which is really common in crowdsourcing
today as well as even more complex
workflows we can think about ways of
routing the right task to the right
people and we can also think about ways
of making the crowd source workers
themselves more skilled and which
includes both testing them and and also
teaching them so all these things are as
Andre pointed out sequential decision
making problems and so we can use our
favorite AI planning algorithms to
control most of these most of these
tasks so that's going to be the thesis
that intelligent control and particular
sequential decision making is essential
to get the most out of crowdsourcing and
so you're going to see a variant on this
slide over and over again the basic idea
some sort of task comes in an AI system
television controller decides exactly
what to do next generates a task sends
it off to the crowd interprets the
result with some probabilistic reasoning
and then either returns the result
or else maybe chooses another task yep
such a job or like is it the task comes
and it needs to get done and then or
does that task a program um for I mean I
don't think it has to be one to one for
most of the things I'll talk about today
it is one to one okay so let's start out
with the simple base case and I'm
assuming every knows about crowdsourcing
so I'll try to spare you all those kinds
of slides but do we have any bird
watchers in here so is this actually an
indigo bunting yes or no oh it's not
this is actually a bluebird but the
point is obviously you can send jobs out
to mechanical turk and actually the
workers are pretty good and people who
you know know lots about birds actually
will probably do better than you guys
but the but the results that come back
are going to be noisy hopefully on
average they're going to be they're
going to be right typically the way that
people deal with this is by asking a
number of different workers assuming the
majority is right using some form of
expectation maximization to come up with
better estimates for the hidden
variables and estimate those hidden
variables are both what's the correct
answer to the job and also how accurate
are the different workers and this is
great there's been lots and lots and
lots of papers about this but it's very
passive as i said earlier so it doesn't
address things like well how many
workers should we ask typically systems
ask the same number of workers hard
problems may need more easy problems may
be less really hard problems or maybe so
hard that there's not really any point
in asking too many questions better to
skip so our objective is to maximize the
quality of the data minus the cost that
we've paid to the crowd source workers
so in the simplest case here we've got
some sort of yes/no question we decide
should we asked another worker if so we
send it off to the crowd we get a result
now we have to do some probabilistic
reasoning usually with a.m. to update
our posteriors now we come back again do
we need to ask you know still more
workers if not we return return the job
you know
our current estimate that the maximum
probability estimate and you can view
this as decision making in ER certainly
in a partially observable markov
decision process in fact this one's a
belief MVP sort of simple kind of palm
DP but that's the basic idea for those
of you let me just go through the palm
BP slide so palm BP is simply a set of
states a set of actions in this case
generating a ballad job or submitting
the best answer a set of transitions
with probabilities to take us to the
different states an observation model so
that each time we take a transition we
get some sort of noisy observation the
cost is the money spent per work we sent
out to Mechanical Turk or a labor market
and the reward is some sort of
user-defined penalty on the accuracy and
again we're trying to maximize the
expected reward minus the cost and you
solve these using Bellman's equations
and dynamic programming and I'll skip
that stuff so in in action if we see
this we send a ballot job out and that
allows us to update our posterior we
send another ballot job out we update
our posterior at this point we've got
enough confidence and so we just return
return our result so this model is
actually very similar to some of the
work that that a che is done and and
like her work on Galaxy Zoo we've found
that this kind of intelligent control
approach really does much much better so
we get much more accurate answers when
we're controlling for cost then if you
used a static workflow even with a.m. ok
so that's simple tasks and now what I
want to do is talk about slightly more
complicated tasks how do we put those
building blocks together um so yes I
just guys might get a sense of how much
difference in my or 11 more steps is by
not by me answers you ask for but like
the value of quarkons
I don't run into some of that means um
so so what's the value so the Crusaders
there's sort of a trade-off between to
make sense of this obviously you have to
pay say one dollar for each job you send
a mechanical turk or 1 cent or what have
you the user gets to specify utility
function saying how much you know what's
the cost of false positives basically
and so this x-axis is basically saying
as you vary that penalty then you get in
fact as you increase your interest in
accuracy then you can do you can do much
much better at the same for the same
price you're paying the same I out to
the two different workflows but and
again I'm sort of going yeah feeling of
how much difference the compu he's made
is crazy it makes a big big difference
I'm again nha's found similar kinds of
things in the Galaxy Zoo and the
galaxy's work I don't probably about as
tiny the experiments so we do do they
tunes I'm sleepers I'm sorry say I'm no
I knew the oven looking for but I don't
know how to do SEO a show car shows up
my petrol seems out of my control key to
support sequential policy um so so the
way we do most of these jobs is by
putting up one job on Mechanical Turk
enough or you know maybe many instances
of that but is when the worker actually
comes and clicks on it we generate a
dynamic page and decide exactly what
we're going to ask that worker at that
time once they actually click we know a
little bit about them in terms of their
you know our history with them it's all
dynamic you used the Amazon interface
you it's much harder to do these things
I'm sorry second there's the turkic
toolkit for sequentially posting tasks
Oh media worked on it yeah yeah Michael
to mems thing and and Chris and and
Jonathan are also trying to build up
a toolkit which is taking parts they've
used that too is taking parts of that
but then trying to put more of the sense
though one of our longer term objective
is actually to put out a much better set
of tools that have this kind of palm DP
stuff built in ok so let's go to more
complicated workflows and and here I'm
going to show what's called an iterative
improvement workflow developed by a Greg
Little along with Lydia and Rob Miller a
number of years ago and the basic idea
is you take some sort of initial
artifact I'll make this more concrete in
a second um you ask a worker if they can
improve it now you've got the original
and the improved now you ask a different
worker which one of these two is better
you take the better one maybe there's
many votes you take the better one and
you come back and you try to improve it
improve it again so here's an example
and the task is given a picture create a
description English language description
of the picture so here's the first
version and after going around this
iterative improvement workflow with with
three votes at each iteration going
around at eight times that's the
description that comes out which is a
pretty fantastic description is probably
better than any one person could have
written if you'd paid them the whole sum
in fact Greg showed that that it was so
it sort of demonstrates the wisdom of
the crowds and it's it's really really
cool and he showed that it works on many
other things including deciphering
handwriting and which is
incomprehensible gibberish if i look at
it but the crowd got it um but there's a
whole bunch of questions that aren't
answered in this and one is like well
how many times how did you know to go
around eight times another one is why I
asked three people in fact what he did
was he asked to people and if they
agreed he figured that he was done and
if they disagree to ask the third person
to disambiguate why is that the right
strategy for making the decision about
which of these two things is better so
one thing that
we did pong die former PhD student of
masa and myself is is basically again
frame it as a pom DP and here is you
know do we actually need to improve the
description if so we send out a job
saying please improve it otherwise we
basically say well are we sure which one
is better if not then maybe we should
ask some people to judge it and we go
around that loop a number of different
times that's basically the same simple
decision making we saw a few slides ago
and then we come back again and see can
we do we need to improve it more or
should we should we just send it back
and again for coming up with images
descriptions of images where we're
controlling for costs we see that the
palm BP generates much better
descriptions than the hand coded policy
which was the one out of Greg's Greg's
thesis um and if you instead croak
control for quality it costs thirty
percent less labor to generate
descriptions of the same quality what's
even more interesting is why so if you
look at gregs policy you see that as it
goes through the iterations it asks
about two and a half questions per
iteration right because it has two
people if they agree then it just
decides that that one's better they
disagree it gets a third person to
disambiguate what the pom TP does is
that and when i first saw that I you
know it was like there's a but go back
fix the palm BP solver and it was only
after we fall about it a little bit we
realized actually it's doing exactly the
right thing because in the early
iterations like even a chimpanzee can
make the description better and then
after it's gone around a couple times
and sometimes the workers actually make
it worse not better and so you want to
spend more of your resources deciding
whether you've actually made progress or
whether you've move backwards and
furthermore you can now go around the
the loop of one more time with the same
budget so palm DP is actually doing
something that's pretty smart
I have to decide whether you need my
improvement how does the model know that
um so there's lots and lots of details
here it's modeling the quality of the
descriptions with a number between zero
and one which sort of maps to the
probability it thinks that the next
worker is likely to actually make it
better versus making it worse and again
what is driving the whole thing is a
utility function the user has to specify
which basically says how much do I value
different quality levels so that's an
input and that's what the system is
using to trade off whether it's better
to go to keep trying to improve it or
whether it's better to go on to the next
image yep then I'll finding a collection
to feel today I'm in this version there
was a movie actually experimented with a
number and Chris later so this is
usually largely supervised learning to
learn the model in some follow-up work
that Chris did both on this workflow and
then for most of the work flows that are
coming we do reinforcement learning but
this earlier earlier work use supervised
learning to learn that to learn the
models more questions oh okay so what I
want to do now is talk about comp less
tax and I talked about it Erick
improvement and now I want to talk about
taxonomy generation and I'm going to
describe some of the work that Lydia at
Shelton did and so it'll be a brief a
couple slides away from Palm DPS and
then we'll get back to Palm dps in a
minute so what Lydia was interested in
is how do you take a large collection of
data and actually make it easier for
people to make sense of and Dubrow so
for example got a whole bunch of
pictures you'll be really great if we
could organize those into a taxonomy
like the one here so we've got our tiger
pictures which
our animal pictures and we've got our
pictures of workers which are people how
do we both generate this taxonomy and
also populate it with the different data
items similarly you might have textual
responses from some cue a website and
we'd like to again texana mais and these
are about air travel you know tips for
saving time at the air and some of them
have to do with packing ahead of time
some of them do with having to get
through security how we come up that
taxonomy to make it easier for people to
browse the data so generating a taxonomy
is really really hard for the crowd
because a good taxonomy means you need
to look at all the data but then how do
you actually parallel eyes it so Lydia
tried a couple things the first thing
she tried was iterative improvement
where they're sort of a taxonomy on one
side and some items on the other side
and it didn't really work at all because
the taxonomy became overwhelming to the
workers and the worst got confused about
what they were supposed to do and she
couldn't make it work the same taxonomy
growing over time yes everyone every the
eminent of the implementation she tried
the workers could do it they could at it
they could edit the taxonomy and they
could place things in it and then the
next worker would see it and try to try
to improve it then people would judge
you know is this tax on me better than
that taxonomy and it was just very hard
for the workers to make those tests so
the lesson was these tasks are too big
and complicated need to decompose it so
next thing she tried was asking workers
about different possible nodes in the
taxonomy is this one more general than
that one this was sort of decomposed
smaller problems but the workers really
found it hard to make those judgments on
because without context they didn't
really know whether one was a subclass
of the other or whether went the other
way around so the takeaway was you don't
actually want to ask workers about these
abstractions you need to ask them about
concrete things
as you tried a couple other things but
ended up with an algorithm called
cascade where she uses the crowd to
generate the category names to select
the best categories to place the data
into the categories and then uses the
machine to then turn that into a
taxonomy so I'll illustrate this on
colors since that's the the easiest way
first thing to do is to subsample the
data into a small set and then generate
the category so for each color you would
ask the workers well what's a category
that might be good for this and so you
get a bunch of categories out and these
are sort of the candidates the second
thing is to go through and for each
color go through and say well for this
color which of these categories looks
like the best one to describe it and
that allows you to get rid of vague
categories and to get rid of noise in
the data and then the third thing is to
go for each color and category basically
say okay which which categories is this
one fit into so this color is both green
and greenish and then you sum up those
things and then the final machine step
basically looks at this and is able to
eliminate duplicates and look at the
sort of subset super set a relationship
and outputs outputs the category so
outputs the whole the whole taxonomy so
that's yeah done initially and then
they're fixed going forward so like do
you ever go back and revise it kind of
you know no well yes yes and no no I'll
say no first and i'll come back and say
why yes no you don't this is the whole
process now in fact what I think Lydia
should have done is now put an iterative
improvements step on top of this
afterwards to sort of prune up the
categories and make sure that sibling
nodes are you know are appropriately you
know it makes sense sibling
relationships make sense and so on but
this and I think there's a lot to follow
on work some
other people have done but this is the
way she left it so that's the the no but
she should have answer and then the
other answer is the yes answer is
actually the final step is to recurse
because we started by subsampling so now
we need to go back and take all the rest
of the data and try to put that into the
taxonomy and sometimes when you do that
you realize that some of these new items
don't have a place to fit and you have
to actually add new elements of the
taxonomy and see where they go so the
recursion is a little bit more
complicated than what I just said just
so I'm wondering you wasted categoria
textured bad thing always that oh yeah
hi Rocky um so I'm so here we see light
blue as a subcategory of blue so it's
building a deep hierarchy that said the
individual workers are only making
judgments about does this fit in this
particular category so that sub
structure of the hierarchy comes because
some categories overlap very strongly
with another category it's added name
where you have too many categories in it
um yeah and this gets into the whole
problem of evaluation and you know
what's the purpose of the taxonomy some
taxonomy is maybe you don't want them to
be too deep if you're using it for
another reason maybe you do want it to
be very deep it's actually really hard
to evaluate how good a taxonomy is and
it's a very squishy thing and was
actually the big thorn and Lydia Saad
and one of the reasons why she decided
not to keep working on this problem
afterwards instead as I mentioned
earlier she's working on joke-telling
which seems squishy too but actually
it's pretty easy to measure whether a
joke is funny or not and so that's what
she's doing next yeah I hers and in
pictures of elephants and pictures looks
very destroyed cat classes that there's
no subset of super sent relationship
and everyone very clearly labeled it as
a tiger or an elephant and they almost
kind of agree on that so then with the
hierarchy flat tire could be totally
flat or hopefully somebody would have
proposed hopefully somebody would have
proposed so I mean there'd be a single
route I guess hopefully somebody would
have proposed animal as being a good
thing and all those things would have
fit into animal and so we'd get a sub
structure where animals at the top tiger
elephant are are down below again I
should just point out this this is a
single workflow I think it's a very cool
workflow it worked much better than the
other things she tried is definitely not
perfect and I think there's a number of
ways that one could improve it so like
one thing that it doesn't do well is it
doesn't make sure that siblings are
parallel in any way but there's other
things that doesn't doesn't necessarily
guarantee either still it works pretty
well um how well does it work okay so
it's very difficult to evaluate this but
what she did was get a number of
different in this case textual
descriptions and then get a bunch of
people from the information school and
ask them to build taxonomy xand then
look at the overlap between one human
and other humans and the computer versus
the other humans and then found that
actually the inter annotator agreement
was pretty much as good or almost as
good between the computer a cascade
algorithm and and the other humans one
human versus the other humans was
slightly better but a quality overall is
really is really pretty pretty good on
the other thing she did was look at how
much it cost and unfortunately here as
the jobs got bigger it ended up costing
quite a bit more to use cascade than it
did to just hire a single person to do
it and you know the flip side though is
actually it's really hard to hire these
people to do it especially if you need
experts sometimes it's basically
impossible to do it and then
can not only and so it takes a long time
that's why we don't have a lot of these
hierarchies and the crowd source
workflow can be parallelized and
actually produce these output very
quickly so so that's kind of cool just
as a segue at the madrona venture group
we saw a company we did actually didn't
invest in it but it got funded it's
pretty cool and what it does it tackles
the problem of evaluating surgeon
performance do they videotape a surgeon
doing an operation possibly on a
simulator and then they have three
expert surgeons watch and see how
competent is that resident and the
problem is the expert surgeons are
really busy and they don't want to do it
and they just don't do it and there's
this huge backlog you can't get the
feedback back to to the to the residence
what they did is they put this out on
Mechanical Turk and they were able by
asking enough truckers to get expert
level sort of indistinguishable from the
surgeon estimates about how well the the
residents were doing and they got these
very very quickly and so now they're
selling that to hospitals totally were
the people who are giving feedback just
completely it could be you yeah maybe
what are they actually they're just
ordinary mechanical Turkish good good
have been long as i don't know why they
could have been doctors yeah no i don't
know but i'm willing to bet that they're
not doctors i'm guessing it there you
know housewives and house husbands and I
mean they have to go through a little
training phase but actually you know if
you look at the video like one person
like drop something and then you know is
like their hands is kind of like my hand
and whereas the expert is like very
smooth and fluid and the motions are you
it's actually it make make sense yeah
good so then I was just going to say
like just to make sure I understand the
procedure correctly there seems to be a
pretty interesting connection between
and what's down in cognitive science
terms of Norman that's where the pass
key is if you give me a word I'm first
supposed to think of attributes that
this word has it first fold axes memo
etc I'm asking you do that then there is
the second stage which is exactly
paralleling this given that I'm asked
like does the dog has those this thing
has fur now I rang I checked a lot of
objects with uncommon or not the end
result is a matrix and then that's given
to a clustering algorithm to what
everyone since life that's someone close
to um I think I think yeah I think
there's a lot of overlap um okay i'm
looking at time I want to keep going so
um Lydia was very disappointed by that
cost result but I was overjoyed because
remember we haven't talked about Palm
dp's in a while so the natural thing to
do is to put decision theoretic control
on top of this and the first step is
like why is this workflow expensive and
the reason is because of this select
best step where you have to ask lots and
lots and lots and lots of select best
questions because this one question for
each color or for each element and each
possible category the category I step
doesn't take very many in here she was
asking five workers each one of those
questions and there's sort of a you know
squared number of questions um do you
really need all those questions no you
don't really need all those questions
and and furthermore if you optimize the
order of the questions you can do much
better so again you can frame this as a
pom DP and here you're basically saying
you know what questions exactly should
we ask and then we're going to do an
update on both what's are the
probability of the labels for this
particular element what is the label
co-occurrence model and then also what's
the accuracy of the worker that we're
talking to based on
agreement with other workers we're going
to update models all three of those we
looked at a number of different
graphical models to represent the
co-occurrence model which is the big one
and the net effect is as we look at
categorizing doing this multi class
categorization the more questions you
ask the higher the performance you get
is and so this is a model which is doing
joint inference with a sort of simple
probabilistic model and is greedily
asking the best possible question and
what happens is we get the same
percentage same accuracy that Lydia was
able to get but we only have to ask
thirteen percent as many questions and
all of a sudden now it's much cheaper
than asking an expert to do the same
thing so that's pretty cool okay let's
go on to the next question which is is
task routing and that is really the
following model let's assume that
workers have different level of skill
and the questions we have are of
different difficulties how should we
match those match those up to actually
hit Andre give this part of the talk
since he was a co-author on the paper
but basically at each time step we're
going to want to assign jobs to workers
and intuitively what we're probably
going to want to do is assign the really
hard questions to the really skilled
workers and the easy questions to the
not so skilled workers although maybe if
a question so hard that nobody's going
to get a right you know we shouldn't ask
to anybody or give it to an easy worker
and save you know a medium difficulty
question for the hard worker there's
many different variations on this
problem depending on whether you know
the worker skill whether you know the
question difficulty whether you don't
know either one and in fact it's a
difficult problem even if you know the
skill in the difficulty and of course
it's even harder if you have to learn
those along the way but you can frame
this again as a palm DP you've seen that
diagram before
and here are some results that we got
yes you get to choose what we're giving
the access to or model is that the
worker constant the model is the worker
comes but but once the worker comes we
have our past relationship of that
worker so we know how we have some
information about how good they are and
then we can give them any question that
we want and we've got a you know we have
expectations about which workers might
be coming as well no we don't feel like
I want more of this okay there's there's
different models where you could do that
and there are some platforms where you
could do that but the surly Mechanical
Turk you can't do that to ya you get the
workers to come I okay so this was
looking at problems doing named entity
linking and natural language for a noun
phrase figure out which Wikipedia entry
it's talking about and round robin is a
pretty natural baseline that people use
and the decision theoretic controller
did quite a bit better that curve may
not look that much higher but to get to
ninety-five percent accuracy of sort of
the asymptotic accuracy it takes only
half as much a half as much labor if you
actually do that assignment correctly
okay this was supposed to say interlude
at the beginning of the talk I said half
of my life is natural language
processing and in particular what we're
interested in doing is information
extraction going from news text or web
text to a big knowledge base which is
schematically drawn there and what we've
done in the in our NLP papers is look at
different kinds of distant supervision
that allow us to to train these
information extractors without any human
labeled data and one of them is sort of
aligning a background knowledge base to
a corpus heuristic Lee and we got a
whole bunch of papers on how to do
that and another kind is looking at
newswire text and using some cool
co-occurrence information to
automatically identify events and then
we automatically learn extractors for
those events some really cool papers on
that but these were like two separate
camps and I had one group working on
crowdsourcing and another group working
on information extraction and so the
obvious thing is how come we're not
using information in the crowdsourcing
to get some supervised labels which
could make our information extraction
doing better and then we could train
using a mixture of distant supervision
and supervised data and so so that's
what we've been working on lately and so
let me give a couple observations the
first is that we're not the only person
to think of this so Chris Callison birch
at UPenn spends two hundred and fifty
thousand dollars a year this is an
academic researcher but 250 thousand
dollars a year on Mechanical Turk the
linguistics data consortium has 44
full-time annotators all they do is
create supervised training data for the
rest of academia so generating this
training and I know you guys here and
Microsoft do a lot of training data
generation as well so a number of people
have tried doing this distant
supervision semi supervision and their
results for relation extraction and
they've reported that it doesn't work at
all so in contrast to some previous
things actually they're getting really
crappy data out and Chris raise group
said getting the label from Mechanical
Turk it's worse than distant supervision
just doesn't really help at all of
course he's a database researcher and
he'd been scaling up distance
supervision to like Tara corpus corpora
so maybe he had an incentive to say that
his database techniques would work
better because he actually not really a
crowdsourcing researcher even though he
did the paper and then a reasonably
recent paper by Chris Mannix group at
Stanford says you know with some fancy
act
learning we can actually do a little bit
better but fundamentally there's a bunch
of negative results which just struck me
as a like it's wrong they should be able
to do much much better so what's going
wrong one thing is that they're not
using the latest crowdsourcing another
thing is that they're actually not
training their workers very well and I
think the dirty holistic of
crowdsourcing is the instructions really
really really matter and like nobody can
write a paper saying we improve the
instructions of our crowdsource job and
we got a fifty percent improvement in
the quality of the results but in fact
that's the truth and so I'm still trying
to figure out how we can get that
instruction optimization into a research
paper and how we can automate that
process sometimes just iterative design
but there's got to be something in there
another thing is they're not qualifying
and testing the workers very well and a
final thing is generally speaking
they're thinking about data quality not
the quality of the classifier in fact
for all of the different things I've
shown before we've been trying to get
high quality data out not trying to
generate a hydrology quality classifier
and those two things are not necessarily
the same so anyway we've been pushing on
this and so the rest of what I'm going
to talk about today is hot off the press
actually it's not even on the press it's
off out of the experiments and now the
stuff is published and some of its
subject to change but I hope I'll get
feedback about instructions I mean how
can we trust results in crowdsourcing
literature if instructions matter that
much I mean I understand for simple
tasks for repetitive tasks okay they're
kind of sub 7 sometimes I want me to do
for something more complicated I mean
when somebody says that we can improve
the accuracy by this much and now the
papers has been providing us with this
part of it well and I think you have to
you have to look very carefully at how
with their testing methodology is and/or
you know are they say what are they
using to test
you know and if the instructions that
they're giving to the workers don't
match the instructions they're giving to
the graters then that presumably is just
going to cause your performance to be
bad or if they've put instructions out
to the workers but they're not actually
making sure that the workers are reading
those instructions that's going to lead
to bad performance I don't think you
have to worry about the performance
being better than expected I think the
question is if you're getting bad
performance why are you getting bad
performance one says you know the before
this level but one says the phone at
this level it could be that the
differences explained away by absolutely
absolutely and a lot of those papers
they never actually said how good was
the data they were getting out of the
crowd source workers they got data from
the crowd source workers and they put
into the learning our rhythm and I said
gee it's not working very well did you
see whether or not the annotations were
good or where the annotations not good
we got some of the data the annotations
are not good so they're not doing
quality control right yeah we'll use
instructions to covert a subjective task
so not joking for example maybe people
would have different opinions about a
task but you don't know how to handle
subjectivity very well in quality
management so I've seen people writing
very detailed instructions saying this
is how to do George and at that
point you actually turn a subjective
task into an objective task is this an
issue in these kind of tasks as well for
example if you had experts they would
completely agree there's an objective
way of extracting information from these
news articles or there is a subjectivity
piece in here as well all of the tests
that we've looked at our objective tasks
pretty much if there is such a thing and
I think you're absolutely right is much
much harder if you're dealing with
subjective tasks as well but I mean like
tricky for example in this information
extraction right Joe Blow was born in
Barcelona
is that give positive evidence that he
lived in Barcelona Spain you know it's a
corner case in fact the annotation
guidelines say no that does not mean
that he lived there I like my thinking
about topology is well there's like a
half open interval where he lived in any
way but the bottom line is you know if
that's the annotation guideline that
you're going to be grading against you
better make sure the workers know that
because it's possibly counterintuitive
um okay so I don't wanna talk about this
is NLP stuff we haven't published it yet
but we're actually able to get much much
better and now the question is how can
we automate that so let's talk about how
we automate that first thing I want to
talk about is making sure you've got
skilled workers so here the work that
Jonathan Bragg is working on right now
is basically suppose at every time step
you've got a choice should I give the
worker some training should I give the
worker a test to see how good they are
should I actually have the worker do
some work for me keeping in mind you
know that just because I train them they
won't necessarily get much better and
furthermore they might leave at any time
so if I train them too much and then
they quit I've wasted a lot of money so
how should I optimize that and there's
been some related work for example by
Emma brunskill and Zoren Popovic at
u-dub on using palm DP models to teach
better which i think is really really
cool but this is a different problem
because we actually don't care so much
whether they're learning we care about
getting the most work done and so it's a
different objective so it's a palm DP
and yeah and that's what it looks like
and let's go on so here's some of this
is a just looking at the Palm DP model
and the behavior we get out this is not
got teaching and it's just got testing
you can see what the system's doing is
doing a lot of testing upfront and then
it's firing people it's a log-log scale
which is why it looks a little funny and
then it's getting some work done and
then it's a little bit worried that
maybe they've forgotten so it's testing
a little bit more and it's backing off
the testing slowly as it goes as it goes
on that is to fire people um sorry was
the question what was where's the
teaching part aureus this is not a
teaching this is riyadh justice with
just sorry he's done some work on the
teaching too but the results i have to
show today are actually just where
there's three actions you can either ask
somebody do some work you can test to
see how good they are or you can fire
them and take the next worker and then
the workers also drop off on their on
their own and this is assuming we've got
two classes of workers ninety percent
accurate and sixty percent accurate and
and this is a simulation experiment he's
starting to run on the real workflow but
I don't have those results yet but on
the simulation studies what you see is
that the pom DP the red curve does
really very very well here's a baseline
which corresponds to current practice
which is basically to insert a random of
you know like between ten and twenty
percent you know seventeen percent gold
questions and if the worker ever gets
less than eighty percent of the gold
questions they get fired and that does
pretty well another baseline which is
stupid is just ask people to work all
the time and that way you can't get rid
of the bad workers and then the purple
and the yellow curves are two
reinforcement learning models one which
is learning just the class ratio another
one which is learning both the class
ratio and a model of the worker behavior
and if we're learning just a class ratio
where we've got the true parameters for
the other things the system does super
super well it starts out with an
exploration policy which is the green
baseline and it very quickly goes up and
does as well as the as the optimal
policy and what we see is that the when
we're trying to do reinforcement
learning
more parameters is not working quite as
well yet and that may just mean that we
need to figure out a better exploration
policy which is what we're working on
Mimi we need to go for farther what you
can see is this learning some of the
parameters this is the error estimate is
learning to of the parameters really
well and the other two parameters it's
actually got a big air actually can't
distinguish between the two classes so
so we're still trying to get that to
work better and so right now with
Jonathan's doing is actually trying this
on real-world domains as well as
introducing the testing actions and the
teaching actions together something he
did earlier but it was working well
without the reinforcement learning but
he was having trouble with the
reinforcement learning yeah testing is
on gold gold digga you have the right
answer so is there some sort of like
testing budget or something where you
you have to decide when you're gonna
hell it like you want this system is
doing is just trying to figure out if
all i care about is how much good work
gets done what's the right testing
methodology and this pool of tests i can
give or it just okay although we
actually it doesn't end up asking all
that many tests so we've got more than
that amount of data right now there is a
challenge that people have found you
know that the test questions maybe get
identified and shared amongst the
workers that's that's an issue so in
real life one needs to be careful about
how many test questions you have and
reusing test questions noreen that is
true of them cheating but the UM you've
never really had an issue then where the
the number of tests wants to ask is more
than the size of your tests that your
gold test we haven't had that problem
okay um but as I said in people at
people a crowd flower have had that
problem they've had to come up ways of
generating automatically generating new
test questions and you can also generate
new crest questions by asking one
question to a whole bunch of workers who
you know are good and then looking at a
consensus answer and then taking that as
a test question and you know in general
a test question
where there's very little disagreement
about it so there's not a whole lot of
nuance there's going to be a better test
question than one you know actually so
the educational literature knows lots
about creating good test questions and
there's a whole methodology for doing
that which I think one could plug in so
the test questions or it's / users you
only need enough test questions to
satisfy all the tests you're gonna ask
one user you don't need to create enough
that is correct assuming that they're
not sharing a lecture sharing but like
that's why you don't have to make them
that's right yeah okay so the final part
of the talk is is is actually addressing
the other the other issue and that we
sort of come to when we're trying to
generate training data and as I
mentioned pretty much everybody is
focused on how do we get the highest
quality come up with a crowdsource
workflow that gets the highest quality
data that we can come up with but maybe
a better question is if we've got a
fixed budget and all we care about is
generating a really good machine
learning classifier may be focusing on
the data quality isn't the right thing
to do in particular there's a trade-off
if we've got a budget of nine and a
whole bunch of images we want to label
we could get one label for each one of
these images saying you know is it a
bird yes or no or we could and the
workers are going to make lots of
mistakes so assume the workers are 75
percent accurate or what we could do is
ask only show three images and get three
labels for each of those images and then
do majority vote or expectation
maximization and in that case we get
results that are eighty-four percent
accurate or you know we could ask online
workers to label a single image and get
something that's ninety eight percent
accurate what's the right thing to do in
practice what everybody does is they do
something like 23 relabeling which is
get two people to label an image and if
they agree then we're done otherwise get
a third person to do some big
that's what LDC does that's what lots of
people do for most these crowds were
studies that's pretty much what
everybody does because just seems we
can't trust these workers but in fact
you know labeling with really crummy
labels might be a much better strategy
so in a really nice paper by chris linn
last year he identified the sort of
aspects of the learning problem that
affect this decision so in particular if
the inductive bias is really weak if
you've got a very expressive
representation language that you're
trying to learn for example many many
many many features then relabeling is
more important otherwise you know
labeling is more likely to be good
strategy if your workers are really
really good obviously you don't want to
re label if your workers are really
really bad you also don't want to re
label the workers are sort of at
seventy-five percent that's when
relabeling gives the maximum benefit if
your budget is really large you might
think you should just definitely real
able everything because why not your
budgets really large in fact if your
budgets really large you're much better
off you know labeling and allowing the
learning system to deal with the noise
what Chris has been working on more
recently and just submitted a paper to
triple-a I on is the problem of reactive
learning and that's basically like
active learning you're trying to pick
what's the next data point to label
reactive learning is not just what's the
next data point to label then maybe
should i go back and re label something
as well I'm going to trade off those two
different kinds of actions so a natural
thing to do is to take a good active
learning algorithm like uncertainty
sampling and it turns out it doesn't
work it oftentimes loops infinitely and
starves all the other data points and
with expected error reduction developed
here we actually see the same kind of
the same kind of behavior looping
behavior again yeah what Lou means here
Luke means the system says I'm really
uncertain about data point X so I want
to get a
able for that now has the label or maybe
it's a second label and it said what am
I most uncertain about actually I'm
still really uncertain about X and then
it just from then on it only asks about
X and because a certain point that label
doesn't change the classifier and so
it's uncertainty remains on that and the
basic idea is you need to take into
account not just the classifier
uncertainty but also how likely is a new
label going to change the classifier so
that is the idea behind impact sampling
and the idea is think about a new label
how likely is that going to be to change
the classifier and we want to pick the
label the next point that has the
highest expected impact to the to the
classifier and there's actually many
different variations on this idea the
first one is should we actually look for
the expected impact or should we be
optimistic and choose the data point
which has the largest possible impact
another one is sometimes if you've
labeled a point twice and you've gotten
two positives a third label is not going
to change your belief in that data point
but maybe you still want the system
maybe to go back so if you do look ahead
search then you could realize well if we
got two or three labels and they are all
negative that would change our
classifier but any one label isn't going
to change the classifier so in general
look ahead search is much better than
greedy search but you know it's also
much more expensive so what Chris looked
at was something called pseudo look
ahead where he calculates how many data
points would you need to execute to
actually come up with a change and then
divide that impact by the number of data
points kind of a quick heuristic that
gives you the benefits of look ahead
without having to actually do anything
other than myopic search and then of
course there's many many data points and
so to make any of these things practical
you need to sample the data points that
you're going to consider for for the
expected impact and and so here's a set
of studies the first are on Gaussian
data sets
with 90 features and we look at a number
of different strategies including
passive sampling expected error
reduction different kinds of uncertainty
sampling they've been fixed to avoid the
infinite loops and impact sampling which
does much much better on the simulated
data sets another thing we've done is
take a number of UCI irvine data sets so
real-world datasets and then use a noisy
annotator model to great simulated
labels since all we have are the gold
labels and here we see that on the
Internet ads we actually do quite a bit
better and on rythme a-- we also do
quite a bit better there's some other
data sets I shouldn't B'Elanna Slee say
that we don't do much better so that's
too bad and then finally a che was nice
enough to give us some Galaxy Zoo data
and we've also tested on the relation
extraction data and we see that the
benefits are slimmer unfortunately here
with the real annotators we get a
definite improvement in relation
extraction for some of the relations in
the galaxy's new data it's
indistinguishable from from some of the
other from some of the other methods and
so so that's where we are on that yes I
mean how do you even run this like if
you're Agron needs to say get another
label for this access point yeah so the
way we do this is we go out and we get
like a really large number of labels and
from the real annotators and then we
have the decision theoretic controller
which doesn't actually go out to the
crowd in a said goes to the database and
says give me what a person would have
said if I'd gone to the crowd that way
we can repeat the process yeah yeah and
that way we can also do a number of
randomized randomized the workers and
get these confidence intervals
worker in a model um so we're not
modeling the time-varying behavior of
the workers here and we're we are
assuming that we're getting one label
and then we're doing some thinking about
what the best label is to get again and
then we're going out to get another
label so if we actually ran this on
Mechanical Turk in a live experiment
there is a practicality challenge that
it would be very slow because at any one
time there would only be one Turk or
working on the task there's ways of
fixing that by you know asking like ten
questions at once kinda you know
batching things up that would make it
more practical but actually that's a
important point that I should have
should have made that we're taking a
pretty optimistic case for our system
because it's it's assuming that the the
labels are very very expensive and
trying to do the best thing I'm also
like expected error reduction this is a
pretty computationally intensive method
so if your labels are really really
cheap and your compute power is
expensive maybe you're better off with a
with a simpler strategy the annotation
ever made for these data sets and try to
see if the trends are changing with
respect to the um we we haven't and
that's another key thing in the earlier
experiments with the simulated data
we're telling the Palm DP what the error
rate is in these experiments we're not
telling it and it doesn't know about the
error rate so there's a couple reasons
why we're not doing as well here three
possible reasons one is there's
correlations between the human annotator
error which we're not modeling in the
simulation studies another one
just forgotten and the third one is that
it's something we don't know so rats
that's embarrassing anyway when we were
running easy time of analysis on the
data access with data right we actually
saw that in even in terms of solving the
phone VP the look-ahead has to be much
larger than the other data sets you've
seen before to actually reason about the
value of getting another label right
because the internet was so high that
you have to reason about it look like
long Luca had to be able to see about
your gift annotation so I'm wondering if
the same kind of pattern is emerging for
just study as well that the innovative
workers are so hot that even look at how
to impact policy may not be um yeah no I
remember what you're saying we haven't
we haven't seen that yet and I think
it's because of the pseudo look ahead
but but again these experiments are ones
that have just been running like the
last day or two so yeah it'd be fun to
talk more about it um he is learning
what they're the label error is as its
gathering labels it is learning yeah how
often the workers disagree um ok so you
guys probably believes in pompey peace
when you came in but hopefully I've
argued that they're great for
controlling crowdsourcing and that you
can apply them to all sorts of different
kinds of decision problems where you're
trying to decide what to ask who to ask
when to teach how often to test and so
on and that we've looked at two
different things one is how do we get
high quality data especially good if
you're looking for a test set and then
more more recently you know forget about
the data what we really want is a good
classifier maybe more bad data is better
than than really good data and you know
these are you know our first steps
towards a bigger vision about how
do we build mixed initiative systems
where we combine human intelligence and
machine intelligence and wouldn't that
be cool yes definitely be cool so let me
just end by thanking my many many
collaborators including ma some and
James who've co supervised many of these
people and then also really really
wonderful funding agents and thanks for
much I see we're over time and you've
already asked lots of questions so if
people need to leave you should
certainly leave if not I'm love
questions okay we can take the mic off
and then you can ask questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>