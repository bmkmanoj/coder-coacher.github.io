<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - From Text to Entities and from Entities to Insight: a Perspective on Unstructured Big Data | Coder Coacher - Coaching Coders</title><meta content="BDA - From Text to Entities and from Entities to Insight: a Perspective on Unstructured Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - From Text to Entities and from Entities to Insight: a Perspective on Unstructured Big Data</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_muAtKgk7EY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
for the invitation thanks for running
this workshop I'm looking forward to
having two interesting days it's an
honor to be the first speaker but it's
also an obligation in the risk you do
realize and I realize certainly that
this is a minority topic so tapping into
unstructured data text essentially is
not the mainstream of big data my goal
for this hour is to convince you that it
deserves way more attention than it has
so why do we work on big data at all so
this reminds me of that question why do
you want to climb the Mount Everest this
was asked to George Mallory a
mountaineer who was one of the first who
wanted to climb it and his plain answer
was because it's there now this is a
good answer for a mountaineer but I
don't think it's the answer that
Microsoft or any university or academic
player should have for that question on
big data this is not the way to attract
industries big money for example and
it's not the way to get onto the front
page of the london times especially in
the nonfiction section as you see here
so the promise of big data is more and
we have to keep this in mind so we're
not just let's manage huge amount of
data the largest number wins so if what
is after zettabytes right so these kinds
of of games is not what big data is
about we start with interesting data on
energy health traffic etc and then we do
have a big machinery a big data
analytics machinery and many of us know
how to do these things at large scale or
this is also a good piece of the
research in this area but then the main
point is at some point some human has a
deep insight into something and said and
this insight should be actionable so
actually we should sorry we should
change the world we should make the
world a better place solve the world's
traffic problems as you see here or
solve the world's health problems now if
this is the goal so it's not about just
managing huge amounts of data it's about
gaining insight and improving something
in the real world why don't we ignore
mankind's best
intellectual assets so the best
intellectual content that mankind has
produced is in written form is in text
form it's in essays and news articles
it's in books it's in scholarly
publications maybe even in social media
maybe in facebook who knows but it's not
just some number in some database right
this is this one number in the database
is not a big intellectual contribution
so we should not ignore this this is my
point so I'm going to continue a little
bit longer on interesting data making
that connection with text and then I
have main technical part on moving from
surface names that we spot in text but
perhaps also in in wake form in
ambiguous form in a structured data base
and then turning this into a crisp
entity and then the analogous story for
relations and then short part towards
wrapping up so interesting data so
there's the white belief that structured
data is good and unstructured data is
bad but when you look at this comparison
here so you see a table with locations
but then north and territory what kind
of location is it this is a part of
Australia that's larger than Central
Europe so we measure the temperature
with four decimal those four digits
after the decimal point so what does it
mean if it's a huge area the the time is
just a whole months it's not even saying
which month which year and is this
average temperature averaged over time
in one point average over time and space
what exactly does this mean so this
looks cool but it's the value for
gaining inside is actually not so big in
contrast you look at this text it it's
text yes but for a human it's way more
informative than the piece of data that
you see on the right hand side so the
goal should be to do with machines what
humans can do already right so we need
to get that insight from the text as
well second example is about health so
these are side effects of some medical
truck and it's from its official
information it looks it's a list but we
can easily cast this into a table it's
structured data it's a long long long
list and why is this because he may come
from the from a pharmaceutical come
so these guys have to be conservative
they would list each and every possible
potentially conceivable side effect here
now which ones to actually occur this is
what gives you insight not this super
conservative super list and to that to
get that point that information you have
to read you have to go to the biomedical
literature where people the clinical
studies and they would these studies
would say this rarely occurs this has
never been reported this has often been
reported or you even go to social media
there's plenty of online discussion
forums about health where people report
their subjective experience of course
it's subjective it's noisy it may even
be spam but then you can aggregate over
many of these right so my point is that
the world is not black or white good and
bad but it's much more complicated and
both sides have their value and the
combination actually has the best value
of course to get that value from text
one crucial piece is to identify the
entities of interest so we have to
identify Darwin and infer that it's a
place in this area we have to identify
the truck that is being mentioned the
names for the side effects and so on so
the truth is in the middle here so
connecting structure data with
unstructured data i think is a direction
to pursue and one of the playgrounds for
academics at least one of the most
interesting playgrounds is the web of
linked open data as a huge collection of
structured knowledge and data sources
which you can connect to text and total
more than 50 billion subject predicate
object triples so there's some knowledge
bases here in the middle but there's all
kinds of specialized data assets as well
on life sciences on music and movies
this in the upper left corner here this
a knowledge cute little knowledge base
about beaches and surf opportunities as
one about coffee but only in Russian and
so on so what do we have in these so
this could be information or knowledge
about Amy Winehouse who lived and died
not too far from here maybe 50
kilometers from here in London
and so you see these subject predicate
object triples but how did they get into
these knowledge and data sets well they
were actually extracted from
semi-structured data sources like licka
pedia so a lot of this comes from
infoboxes from category names from
headings from lists and tables inside
Wikipedia but some of that also comes
from doing information extraction from
text and that gives you provenance where
do these facts come from but also
additional information so additional
descriptions of the special
circumstances on why and how did she
actually perform this song Cupid and who
was the original composer and who else
has covered it and so forth so it's not
just wikipedia it may be news articles
web pages and so forth so this is
connecting structure data was text but
because we actually pulled a lot of that
structure data from text and this is the
direction where indeed we view text or
web pages as primary web contents but we
can identify our entities here so in
addition to Amy we have Janice Joplin
and Jimi Hendrix and now we can map
these to some to the web of link data to
the knowledge bases to a structure
databases and this get combines whatever
the text says with some additional facts
like birth and death dates and then
maybe we can automatically infer that
weird or infamous notion of the forever
27 club and that they are all members of
that infamous club there's plenty of
structured microdata now in embedded in
HTML so it's a little rdf like
structured pieces of information
embedded in web pages and there's of
course tables and web pages but these
are not database tables they are just
HTML tables so there's no schema there
is no real typing for the columns and so
forth and this is about covering songs
secondhand songs about my house but aim
is that so the list is not so big and it
won't be crawling further so I have a
second example and your Molly corner
who's an 85 year old italian composer
famous for film music scores and this is
a very very long
and this is not the only one or table is
not the only site that has such things
so here's another one now you can start
comparing things and this is interesting
because it not only says what he covered
sorry he was covered by the following
artists but it also says he sampled
pieces of compositions from other people
and they blended them into his own
compositions and so it just tells you
that he was influenced by a bunch of
classical composers now this is kind of
can still think of this as structured
data but now it's definitely non
schematic so it gets much closer into
text form into a structured form and
most people would think of it is
unstructured data moreover there's a ton
of context here so even if you think of
a table in a web page being structure
but to understand what it says you often
have to look at the context so it was
often a description of the tables and
discussion threads like as shown here
right for this specific site so let me
cast this into what I call the big data
and text challenge so take all that data
use public data and public text there
might be proprietary databases in the
music industry but then there's probably
no single proprietary database that has
everything it's a very dynamic industry
and then find out who covered which
other musician and and which versions
were the most successful ones or more
generally who influenced who so you
might think is peanuts but you have to
realize you might have to analyze all of
YouTube and YouTube is not exactly small
data you might think it's my personal
obsession maybe you're right but then
there's other examples so in health I
spell one out about side effects of
drugs and I'm sure I can come up with
examples in these other areas as well as
a general design pattern go after the
assets data and text identify entities
and maybe relationships between entities
then position them in time and space
because this is often where the inside
comes from how does this how does
America differ from Europe how were
certain things evolving over time and
then there's the usual group and
aggregate machinery and finally want to
find
insightful patterns and predict trends
etc so now in the technical part I will
focus on this part here on identifying
entities of interest and actually
addressing the ambiguity because what we
see in these non schematic data and in
the text is actually just a name so
these names can have many different
meanings so let's know the the technical
part me Ron you will guide me with time
right so because there's some part
towards the end which I could also skip
so just to drive my point home here's
another example this is what the what
link data knows about ennio morricone er
the people are dairy and Island have a
have some nice tools so they you can
search with a name and they give you all
the RDF triples that that could have
that have facts or knowledge about this
entity and so there's plenty of these
subject predicate object triples the
subject is always more like owner this
is what I search for and now let's zoom
into some of them and this is kind of
odd so you realize that Malik owner has
an absolute magnitude he has an
epilepsies it was born in 1928 but he
was discovered in 2005 strange and he
has an ax Parkinson so what happened
here is that there's actually two more
oconus they were not kept apart yes and
there's any amala corner the composer
and then there's an asteroid which
passes by earth once in a century and
that s terror it was named after Molly
corner so interesting even such a highly
specific name Molly corner this is not a
it's not the joe smith of italy right
this is a rare name so but in that is
not unique so you face ambiguity
everywhere so now i'm going to walk walk
you through like the state of the art in
this entity name disambiguation
essentially and it's part of our work
but also without making everything
explicit there's other people's work
worked into this as well so let's take
this example text sergio talk to you
about Eli's role in the ecstasy scene
and so on
the question is who is this guy Eli
right is this Eli from the Bible or is
this this guy eli wallach first thing is
how do we know that we have these
options well and and and also it's not
the only name that occurs here this
surface names that we think could denote
entities but we don't know which
entities we call them entity mentions
and mapping them to real entities saying
eli is for example eli from the Bible
this is the actual disambiguation so
there's a first a recognition step but
the recognition actually it's called
entity recognition in the literature but
is actually recognizing mentions names
and so CRFs conditional random fields is
a state-of-the-art techniques for doing
this and this only gives you the names
that are marked here on the left side
the mapping to the right side is the
action of this immigration the
combination of both is sometimes called
nerd named entity recognition and dis
ambulation so knowledge bases or that
web of link data and you can also think
of product catalogs if you want a
business application so they give you a
huge numbers of pairs of names and
entities that might be denoted but this
is not yet the disambiguation it just
gives you a candidate space so you see
that eli could also be an acronym
meaning extreme light infrastructure
ecstasy could mean methyl dioxide
methamphetamine practice this five hours
so this is the drug ecstasy obviously
and so on so this way we get a space of
or a set of possible candidate entities
and then we can also generate
systematically this set off or the space
of possible mappings and one of these
mappings is the correct one and this
immigration is computing this correct
mapping so there's three main
ingredients that all the methods use
here in various forms and with
additional bells and whistles etc so I'm
simplifying here little so the first
ingredient is the popularity of a named
entity combination if you just hear Eli
we probably thank you I Bible if we just
hear ecstasy we think drugs so how can
we
estimate this conditional probability of
an entity given a name and only that
name nothing else no context given so
the entities that we consider here in
our entity repository product catalog or
our knowledge base they often have
textual descriptions in the knowledge
bases in the web of linked data these
are typical Wikipedia articles because
there's a one-to-one correspondence
between the entities and the Wikipedia
articles this also for example the case
was the Google knowledge graph and I
think Microsoft must have a similar
project I don't know in the name so and
Wikipedia or whatever ask that you have
here in the background often comes with
with some cross references and Wikipedia
these are hyperlinks and hyperlinks are
named entity pairs they have an anchor
text which is usually a short name
sometimes an acronym and then they point
to an article which is an entity so this
gives you a handle for estimating these
conditional probabilities of course
popularity alone this is kind of a prior
right you cannot work with this alone
because it would always prefer prominent
entities and never go for the not so
prominent ones so it's only in
combination with the other ingredients
that this helps and the main ingredient
that people usually consider its context
similarity so we have here a dimension
Eli and we can construct a textual
window around it and cast this into a
bag of words model or maybe a statistic
language model over words phrases
whatever and we do the same with the
textual descriptions that we have for
the entities in on the right hand side
so with eli bible and with eli wallach
and now we have apples here and apples
there we can compare by different kinds
of similarity measures like cosine
similarity dice coefficient kullback
leibler divergence you name them
depending on how exactly we created the
models so and obviously the higher
similarity is more likely to be the
right entity the third ingredient has to
do with a situation that we have a set
of mentions at our hands and it's
beneficial to
do joint inference for all of them
together you might think you you you
want to process them one at a time by
cleverly ordering them so for example
you mapped annual first and nu is also a
rare Italian first name so once you have
any amala corner a lot of the other
things fall into place but finding that
right order is non-obvious and maybe you
start with ecstasy first if you start
with ecstasy you're bound to to move to
the track and the rest will go wrong as
well so yeah it's much better to do this
jointly and one big inference step and
from the semantics perspective the acid
that you can leverage here is the
semantic relatedness of entities or
coherence so these entities don't occur
co-occur randomly uniformly why would a
web page mention Eli from the Bible the
track ecstasy and the Lord of the Rings
trilogy in one together right so there's
no connection and that isn't a
consideration that's independent of how
you phrase it in in the actual wording
right so it doesn't matter what words
you choose and what else you say about
them they're just very unlikely to
co-occur and in in these situations when
you have a multivariate data space the
sweet spot is typically go for the pairs
so we can compute a coherence measure
four pairs of entities and if this is a
quantitative measure we can turn these
into weighted edges that connect the
entity candidate entities on the right
hand side and the weights can be derived
from different ingredients so one is a
type system if or could be a category
system for prod for a product catalog
but here we use semantic classes for
entities in a knowledge base and we look
at overlaps and how close are the
different types in the taxonomy that the
type taxonomy here is a is a huge DAC
with several hundred thousand of notes
or if we have Wikipedia in the
background why not use it it has a rich
link structure so we can look at
overlaps in incoming links or outgoing
links or maybe both or and that's
actually the most interesting
because it's the most generalizable we
do another text mining task first so we
mined key phrases that are salient for
entities so this by mutual information
argument salient and then we can look at
overlaps between the key phrases of
different entities of a pair of entities
and even if there's no exact match
between the phrases here and the phrases
there there's often partial matches so
if you have the right matching model
here you can still infer that I don't
know on I should point here and your
mobile corner composition and soundtrack
by Ennio Morricone there's a big
connection obviously without
understanding that ennio morricone is
the name of a person etc so putting
everything together it gives you a craft
construction like this which is almost
bipartite except that you have these
additional edges here so these are
mentioned and these are candidate
entities and now we need to work with
this kind of graph and there's different
approaches one is to view this as a from
a machine learning perspective as a
probabilistic graph where the the model
is kind of a probabilistic generative
model is this one here so given a set of
entities what's the probability of
seeing these these mentions these names
and it's a multivariate space again you
have 24 tractability for sparseness
essentially it's not just computation
costs but also data sparseness that
forces you to factorize so you have to
make conditional independence
assumptions and that typically leads to
looking only at these pairs of couplings
here and then these are the factors in a
probabilistic factor graph essentially
all the edges in the craft become
factors in a probabilistic factor graph
and then you have the whole machinery
Monte Carlo Markov chain sampling
casting this into integer linear program
programming relaxing this into LP with
randomized surrounding you name it right
so the literature is full of this all of
this is expensive and these graphs are
not necessarily as small as they are in
PowerPoint so if you if you have many of
these names have hundreds of candidates
so you quickly if you have a decent
sized text
you quickly have a craft which has
10,000 nodes or several 10,000 notes on
the right-hand side so an alternative
modeling is to think graph algorithmic
so within that big craft we want to
compute a smaller craft maybe a dense
sub graph where density could be the
total edge weight that we pick up from
that sub graph and we have to satisfy
constraints namely that each mention
left is mapped to exactly one entity
right otherwise we haven't disambiguated
or if we allow partial mappings so some
mentions are just there to exotic so we
say well we don't map them at all we met
them to null then to the constraint
would be map it's no left to at most one
note right so of course NP heart it
turns out these all these give you
decent solutions but you can do better
the reason is that especially the last
model with a dense sub graph often tends
to to produce sub graphs that have
multiple clicks click here or click here
click there and then very loose low
weight very weak connections in between
and then from the semantic perspective
when the human later interprets the
result it's broken so it's the output is
poor and what we want to do is we want
to make the weakest link in these
coherent structures as strong as
possible and this is what we could
formalize by looking at way to decrease
so the way the decree of a node is the
total edge weight of its incident edges
so when you look at the hundred and
forty comes from this ten here plus 50
plus 50 plus 30 and making the weakest
link as strong as possible means
maximizing the minimum weight a decree
in this sub graph I'm simplifying here a
little because in the sub graph the the
way to decrease are smaller and that
kept the original once the final ones
are the ones that matter no free lunch
everything is NP hot here in the space
if you want to consider the crosstalk
among the entities right you can come up
with some clever feature engineering
that avoids that computational
complexity but then it becomes really
a dark black art feature engineering and
it's also not equivalent it's not the
same right so you simplify and you lose
some of the benefits this is the method
that we use in our tool the tool is
available as open source and we also
have a service can try it out including
a little demo there's one technical
ingredient which I want to bring out
here and i gave you the simpler
simplified picture so far and there's
several bells and whistles in various
spots but this is one of them so I set
for the mention entity similarity do
bag-of-words similarity measures that's
what you think of after a hot shower
that's but that's not doesn't go so far
so there's a much better way because we
have these key phrases associated with
entities so if we think of the entity
ecstasy of gold the composition that's a
musical composition and it has a
Wikipedia article so there's but also
articles in music online forums so it
can mine characteristic key phrases and
quantify their salience by point wise
mutual information and Metallica tribute
to ennio morricone might be one of the
key phrases now we look at an input text
like this one the ecstasy piece was
covered here this is dimension and the
candidate entity were thinking of his
ecstasy of gold but it's not spelled out
here so now we try to match the key
phrase in this text but these are
phrases now they're more expressive than
single words but then they also rarer so
it's bound to have low probability of
being matched exactly so we need a
partial matching model and this is what
we have done here so we try to match as
many of the tokens of the phrase in a
small window which we call the cover so
that cover kind of should maximize like
the ratio of a number of matched words
or tokens over the size of the window
and there's a scoring model behind this
the details don't don't matter here we
can do this for all the key phrases for
one candidate entity and aggregate and
over these scores
and then we have to do it for all
candidate entities for one mention right
to compute this this is actually a
computation that constructs the craft as
part of the graph construction it's not
yet of part of finding a dense subcraft
so I hope I convince you these are these
are computationally difficult and
challenging problems so here's a
screenshot demo I have this running
example with some little variations
don't get irritated by these double
brackets here so we use a Stanford any
art agora for finding mentions and that
is good in precision but not so good and
recall so we help it a little it has
this is kind of a de facto standard
notation in computational linguistics so
and we get everything right here in the
dollar strategy and also yo your mother
cello player and the candidate space is
big so trilogies the knowledge base is
no many hundreds of different trilogies
here's a very tough example and also a
british example not italian page blade
cash me on a gibson and why would an
airy page meet with mel gibson for
playing chess near the himalaya unlikely
so this is jimmy page from the legendary
led zeppelin group here from the UK and
he played that song on a gibson guitar
you can you can also take as input
tables or a set of RDF triples the model
is very general of course then you may
want to add additional considerations
right I'm not saying this is the best
model for that but it would work so
here's this made-up table Steve and Mac
Dennis and see Richard and canoe and it
does a decent job of mapping these two
named entities so there's systematic
studies including a recent dub dub dub
paper by pollo fettuccine on the nerd
topic so which includes the recognition
the mentioned recognition which changes
the picture a little bit so I'll skip
this now in the interest of time give
you some pointers here to other people's
work with focus on online demos that you
can try out so what's going on here what
is future
work items of course there's always room
for improving the efficiency especially
when you're thinking you want to have
this and do this in real time so
consider some someone browsing a long
text maybe a book or maybe scholarly
publications journal articles and as you
browse you you should immediately see
which entities show up where and at
least the most prominent ones so here
efficiency matters high-throughput dirt
if you want to process a big batch say
one day's postings in all the social
media of the world or maybe just the
YouTube postings you could even think of
this as a streamer input there's issues
with long tail and newly emerging
entities which are come to in a minute
short text and very long texts like
tweets and books are difficult for very
different reasons but then there's also
inherently difficult texts like fictions
you can apply it to tables and lists so
structured non schematic structure data
on the web and of course it's a special
case of a more general word sense
disambiguation problem which in full
generality is may be interesting for
computer linguists but not for real
practice but there's some other classes
especially work phrases often denote
relations and disambiguating these into
relations that you can actually then
look up in a database is of interest and
I have a little apart on this in a few
minutes so time was I'm doing fine right
so long tail entities here I have a
different example about nick cave and
australian singer and so he has a song
hallelujah and there's the famous
Leonard Cohen song which has been
covered at a million times but this case
song is a different one just happens to
be the same name and why not this is not
a patented name and there's even buck
choruses with that name and so on right
so there's actually so many different
hallelujah songs or conversations so
that hallelujah is not in Wikipedia it
is in last.fm which is a like a
community platform on
on music pretty big one and so if we
relied for coherence only on Wikipedia
links we would get a very sparse graph
and we would be misled and do weird
mappings so instead what we do here is
we use a key phrase model also for
coherence I introduce it for the
contextual similarity between a mention
left and an entity right but it's also
good for looking at entity pairs and
it's more general than what we did with
Wikipedia because key phrases can be
mined from whatever we have so last.fm
once we know here's a page about this
entity a ton of user comments about it
we can do key phrase mining and so we
have done this and then we use the key
phrase set overlap measures essentially
in order to compute a coherence graph
like this one and that's this is a much
more expressive coherent craft and gives
us much better this immigration mappings
there's some specifics here about how
these overlaps our measures are computed
it's a two-stage process because we
compared to key phrases from the two
entities and then and then and then they
might have a partial overlap so here we
use a weighted jurka a coefficient and
then we need to look at the sets of all
key phrases on both sides and there's
efficiency issues in the implementation
so hash sketches play a role here to
speed things up now some entities are
not in whatever entity repository we
have at all so how do we know and when
should we say well we don't map it at
all because any mapping we do is bound
to be wrong and might actually affect
then in the joint inference the the
mappings of the other mentions so we can
do actual damage to the other dimensions
instead we should say let's map them to
know so one approach along these lines
is to say well we introduced Udo
entities which say for these mentions
water's edge and mermaids any other
entity with that name and that's like a
mention specific form of null in our
model and for
these we can mine key phrases by looking
at the occurrences of that name all over
the web or all over a huge coppers could
be a digital library whatever and then
from that that's like the universal key
phrases for that name not for an entity
but then we have the other candidate
entities and they have key phrases so we
can now subtract models either if
they're set subtract sets if there were
statistic language models we have to
work a little bit with this but we can
still do this and then these pseudo
entities the null mappings become part
of the regular model so we don't need to
invent anything special and I think it's
a principle model for dealing with this
issue now if they are not mapped so then
they stay mentions so we pick them up we
would say here's a piece of text we
found the following 20 entities that
occur here here here here here and then
in addition there's 50 names which we
can't figure out but you can actually
handle them you can cluster them you can
croup them and in computational
linguistics this is known as cross
document coreference resolution cool
refreshing Ocean is mostly is within one
text we're mostly you would say here is
a pronoun and this actually denotes or
refers to the following noun phrase
further up in the text but you can
generalize this across document and so
in this example here we could map these
two mentions to a known entity these
three but then maybe these here there's
no fitting entity in the repository
repository on the right hand side so we
don't map it at all but we can cluster
them together group them into an
equivalence class and do this across
documents likewise for this guy and for
another one so there's plenty of work on
this cross document coreference
resolution it's a texture counterpart to
entity resolution in you you merge 10
customer databases and you form
equivalent classes it's a little more
general because here you have multiple
you have kind of duplicates per data
assets right so each document can
mention the same entities many times in
a customer database in each database
customer should be only one but you
never know when world is messy sometimes
but the
the nation would be looking at Newark
and the ccr problem together because
there's potential synergies it has not
been done yet so why is this all about
big data there's plenty of big data
ingredients here web scale key phrase
and statistical mining if you prefer
these machine learning probabilistic
graph models so then there's maximum a
posterior inference which is a tough
problem or you are more into graph
algorithms dense subgraphs is
notoriously hard and we even have
variations of this the underlying
infrastructure data web of link data our
knowledge bases they are big by
themselves and just building
constructing the craft is not a pc of
cake right you have to have do lots of
lookups to these underlying knowledge
bases unless you speed them up and you
cash this you catch that you're ending
up with minutes of response time rather
than a few seconds and there's
applications conceivable to to
large-scale input batches like find all
the entities in weeks a week full of
social media postings maybe with a
special focus or look at one months of
scholarly publications on some diseases
so that a life scientist can kind of get
a power briefing on what's new in my
area and that would potentially interest
me and here thinking in terms of
entities and not just here's yes 10,000
abstracts and new abstraction pubmed
nobody is going to look at these right
or you are a journalist or a political
or business media analyst and you want
to track a set of people over in a news
archive over several decades so i have
15 more minutes is this correct or yeah
okay so so much for mapping names into
entities and now we're going to look at
the the counterpart for the relationship
so my model of the world is entity an
entity-relationship data model so why is
this important well if we want to sorry
if we want to tackle that task who
covered whom in music so we need to
learn to read these sentences
and the wording for covering another
song or another artist is very different
as it's widely diverse so there's cover
songs by interpretation off singing of
voice in her version of performed Zhang
and so on right so for the purpose of
this conceived big data analytics we
have to find equivalent classes we have
to understand that so many of these
phrases are equivalent for that purpose
and that they actually denote
paraphrases of that postulated relation
singer a cover song and likewise for who
created which or composed which song
because what we're actually after is a
joint between singer performing a song
and song created by another musician
right so we in the end we want to know
who borrowed material intellectual
property from whom okay so one recent
piece of work that we did again we're
not the only ones i don't in the slides
there's i didn't want to put references
on every slide i put references to our
own papers of course but not to other
people's work but I end co-worker of
mine Fabian so her neck we have a we
have a tutorial in the upcoming sigmod
conference which has a five page page
thing in the proceedings and has 150
references it's not the same right
directly the same topic as this talk but
it overlaps so you can get references
there so the wit the contribution we've
come up with here is a a concept of
syntactic lexical ontological patterns
so syntactic lexical a standard it's
essentially about using like like work
patterns maybe with some syntactic
generalizations like in particular using
so-called part of speech tax which award
category so you say well there's this
pattern but there's one word in the
middle that always varies but it's
always an adjective so then we actually
say well there's a pattern a syntactic
Lexi or lexical pattern this would any
adjective and this word right so this is
standard but ontological is a new thing
here
you will use semantic types like singer
musician etc a bass player or
left-handed electric guitar player like
Jimi Hendrix and these become now part
of the patterns and there's a notion of
it because in the end we want to also
have quantity a quantitative measure of
strengths of a pattern so there's some
just examine we have a notion of a
support set here where we say well we
have core occurrences of a pattern with
a pair of entities and this pair of
entities fits with a pair of types that
we say form a good pattern there's
examples singers adjectives voice any
wild card in song so the type signature
is singer by song you see instantiations
of the pattern and you see a support set
here we did this at large scale using
different ingredients like the full text
of wikipedia a big web crawl et cetera
and here are some examples and you see
that the type signature goes a long way
it helps a lot in interpreting the
patterns because there is one pattern
covered that is actually at the surface
level identical for two different
relations and we can keep them apart by
having type signature so a singer
covering a song is has nothing to do
with a book covering an event we've
mined synonym sets including these the
generalizations was part of speech what
categories and even sub sumption
relations so it's available online let's
keep this there's some examples why is
this again big data I'm not saying this
is the end of the story we're not
claiming victory because when you look
carefully you find lots of noise still
in the output right so there's a we
haven't conquered the or succeeded
completely and dealing with a precision
recall trade-off in this space so
there's more work to be done well at the
heart of this is frequent sequence
mining and this is how we implemented it
actually where we you go you utilize
this generalization hierarchy so any
work can be generalized along the word
category
shallow a generalization hierarchy and
the types actually have a pretty deep
generalization hierarchy and so as we
see that certain combinations don't have
enough support we can generalize it was
just like in in association rule mining
if we don't have enough support for
let's say peanuts were brought together
with coke then we generalize into snacks
with soft drinks right so this is what's
happening here a lot of our machinery is
MapReduce paralyzed not everything but
the core pieces and I think it's a
decent piece of work but especially if
we want more data at larger scale these
some of these will break again right so
then we need new ideas of algorithmic
nature ongoing research here is indeed
countering the sparseness especially the
subsumption hierarchy I'm not so happy
with that so it's very sparse and that
has to do with the sparseness in the
input data so we would need larger web
crawls of higher quality the one we use
is this group web crawl from 2009 that
seem you provides 500 million web pages
but still there's a lot of crap in it so
it's a much smaller it doesn't give you
the mileage that you would love to to
have then and therefore having to cope
with even larger scale input is a
challenge crowdsourcing might play a
role here and of course brute force it's
trivial but brute force is prohibitively
expensive so you need some smartness and
there's applications of utilizing these
type signatures for patterns in
particular and using these typed
patterns when they co-occur with
mentions that might be newly emerging
entities so we don't have them in any of
our entity repositories yet they help us
in at least inferring a semantic type
and that helps in turn and the lifecycle
of maintaining all these entities
because once in a while you would tell
some some experts so we found the
following the name the following name 50
times and we think it's always a new
musician maybe should be entered into
the entity repository right and the
pattern sunsets also play a role in
trance
leading national language questions into
structure queries so we would ideally
like to have interaction especially if
the world moves towards speech based
interfaces with our smartphones and so
on then then it's much more natural to
use natural language so nobody wants to
talk to a smartphone select star from
so-and-so a right and even keyword
search is artificial in spoken form so
we need to map them to the the available
relations in an attributes in the
underlying data and this is one of the
assets here I'll skip this part and so I
have a short wrap-up on the bigger
picture and then the true wrap up the
summary so I started with this challenge
right so who covered whom I think it is
a challenge I think working on this as
an example would be very fertile from a
research perspective even without
solving that actual problem I'm not so
keen on solving that specific problem
and as I mentioned earlier you find
similar or kind of isomorphic analytic
tasks in health politics business and I
want to remember you about this general
design pattern and the talk was only
about one the first stage here the other
stages are non trivial as well right so
there's plenty of challenges in the
other stages in grouping and aggregating
although we have done it in SQL for 30
years but now you apply it at a larger
scale with different kinds of input data
so a lot once text is involved there's
also a lot of grouping and aggregation
of a variable length and cramps for
example and this is not so easy so I
have two more application examples one
is from the past and the next one is
from the future so this from the past
this is computing baked disease networks
which created some splash in the life
science literature six or seven years
ago so they looked at all the pubmed
articles or maybe the full of articles
medline which is like the largest
digital library on life sciences and
then they created these nice-looking
crafts of how human dizzy
is relate to each other at the phenotype
level what you observe in patients and
at the genetic level the genotype level
and but these graphs they look so nice
and oppressive but they they they cover
up one shortcoming I believe this was
done just by looking up name name
co-occurrences so the notion of
relatedness is a very soft one they are
related but how exactly there's no
refined notion of which relationship and
also the disease names were just simple
names so they did not do any entity this
immigration here and this is what you
would actually need when you realize for
example diabetes is a family of diseases
in a certain variants and there's maybe
fifty or hundred different names and so
many of them are synonyms but so many of
them actually do you know different
variants that you shouldn't confuse with
with each other I don't think they have
solved this so this is still a challenge
and this is a future application it's a
killer application sadly in the literal
sense so consider doing a really big
picture opinion mining on the political
landscape of on gun control some
controversial issue and here are pick
gun control because it's a triple us
topic but maybe it was an incident in
London again some killing so who knows
and this is a large this is really a
challenge because you need to identify
the entities that play a role and
they're not just not just Obama and some
other guy right so really a larger scale
going into the long tail of interesting
entities all kinds of organizations that
are behind them then refine by general
geo region cultural background so there
are lots of things to consider dude
analytics over time in particular
because you want to see how opinions
change before after certain incidents
and the incidents are not directly given
to you so you may just start with all
the newest articles in the world from
the last ten years and all the social
media postings that's a big challenge
and there's terms of technical challenge
you can derive from this so summary
so I believe entities and relations are
the key to connect structure data with
unstructured data this bottle name here
and once you know this is the following
entity now you can connect it with
structured data that you have about this
entity it's even in the structure data
world because structure data does not
mean there's a schema and every entity
is precisely nailed down right so but
lots of names are still ambiguous even
in structured data so the disambiguation
is really at a key problem I believe the
state of the art on entity name
disambiguation is not that it's not just
our work is a bunch of people i know the
company is like you guys microsoft has
has good work on this google has good
work so google you I know Google uses
some of that in there in their search
interfaces and recommendations Microsoft
probably too i just don't happen any
direct insight into this and so the this
is entities are crucial for big data
analytics on data and text and as i
previously mentioned i believe we're
going to see more speech in the future
and at some point we want to tap into
video and the part of videos that is
where we have some hope where there's
light on the horizon is the speech track
so then once we have a handle on this
then we can combine with let's say some
object detection in some of the stills
from a video take home message i connect
structured and unstructured data
entities and relationships are key for
this and if we can solve my personal
challenge who covered whom i think you
can solve lots of other interesting
things thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>