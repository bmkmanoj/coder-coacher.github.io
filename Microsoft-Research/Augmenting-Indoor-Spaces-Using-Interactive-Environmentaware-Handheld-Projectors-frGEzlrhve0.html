<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Augmenting Indoor Spaces Using Interactive Environment-aware Handheld Projectors | Coder Coacher - Coaching Coders</title><meta content="Augmenting Indoor Spaces Using Interactive Environment-aware Handheld Projectors - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Augmenting Indoor Spaces Using Interactive Environment-aware Handheld Projectors</b></h2><h5 class="post__date">2011-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/frGEzlrhve0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this paper we present three systems
for augmenting indoor spaces with
digital content using interactive
environment to wear handheld projectors
our prototypes which explore several
different aspects of the design space
are to varying degrees aware of their
position and orientation in space and of
the environment around them the first
system the camera projector is a
tethered handheld unit which contains a
pico projector a coaxial IR camera with
diffusely lumination and an IMU the IMU
provides orientation information whilst
the IR camera can detect objects in
front of the device such as the user's
hands and fingers this enables shadow
based interactions where a real shadow
costs on the image can be used to
control the displayed content here this
ability is demonstrated using a physics
based simulation
it is also possible to use shadows to
instantiate a menu of different icons
when an item is selected with a finger
gesture this is indicated by displaying
the relevant icon in the palm of the
hand
another shadow based interaction
technique uses pinch gestures to pan a
digital document a second gesture may be
used to activate a fingertip annotation
tool
we call our second prototype the rim
projector because it combines the
handheld unit shown previously with a
lightweight infrastructure based on four
ceiling mounted Kinect depth cameras
when a user interacts in this space the
absolute position of the projector is
tracked using the Kinect cameras these
devices also generate real-time depth
data which is fused to generate a mesh
representation of the scene
the raw depth data is noisy this becomes
very apparent when the data from each
camera is merged this is overcome by
segmenting out a smooth background mesh
tracking only foreground objects in
real-time since the position and
orientation of the handheld projector
attract the area which the projector
shines on to is accurately modeled it is
possible to place virtual images within
the modeled environment and reveal them
in the real room inset in the video by
shining the mobile projector on them in
the manner of a virtual flashlight as
with the camera projector we can detect
shadow gestures using the built-in IR
camera here we show a virtual painting
technique where the fingertip is used to
project graphics onto the surface behind
unlike before this dynamic content may
be fixed to any surface in the scene
it is also possible to track the users
hands using the data generated from the
depth cameras in this example when the
IR camera detects a pinch gesture the
user can paint directly into 3d space
visual feedback is generated in
real-time using the projector shown in
set
a particle based physics simulation
enables realistic interactions between
the user and virtual objects feedback is
provided to the user by using the
projector as a flashlight into the
virtual world shadow gestures can be
used to extend these physics based
interactions in this example virtual
rods are extended through the shadow to
enable physics based manipulation of
virtual 3d content even when it is out
of reach of the user
another way of displaying virtual
content in the real world is to cast a
shadow of a virtual object as if the
projector were a real flashlight here
a donkey shaped pinata is repeatedly
struck using a real shadow until virtual
candies fallout has can be seen inset
we can also use the users hand as a
projection surface to reveal virtual
content anywhere within the 3d space
our final prototype the slam projector
combines a pico projector with a Kinect
depth camera and an IMU in a single
handheld unit applying the slam
technique to the real-time depth data
the system quickly builds a high
fidelity 3d map of the environment
if virtual content is associated with
the model it may be rendered in place in
the real world using the projector
touch-based interactions are possible
because the system knows the geometry of
the scene and the relative position of
the users fingertips
virtual 3d buttons may be activated by
reaching into them and they're close by
or by casting a shadow onto them from
further away
digital copies of real objects in the
scene can be made
here a model of a human torso is
selected and a virtual copy which is
rendered by the projector and follows
the user's hand may be placed elsewhere
in the scene
as with the rim projector the geometry
awareness of the system enables various
particle based physics simulations here
touching any object in the scene causes
a stream of virtual particles to be
injected at that point the inset image
shows how these virtual objects interact
with the digitized representation of the
scene whilst in the main image the
virtual content is made real via
projection
the clothes registration between the
real world and the projected virtual
content is even more apparent when
viewed from the users point of view</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>