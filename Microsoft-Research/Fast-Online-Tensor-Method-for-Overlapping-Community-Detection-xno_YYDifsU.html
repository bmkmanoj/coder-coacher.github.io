<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Online Tensor Method for Overlapping Community Detection | Coder Coacher - Coaching Coders</title><meta content="Fast Online Tensor Method for Overlapping Community Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Online Tensor Method for Overlapping Community Detection</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xno_YYDifsU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thank you and I'm a late addition to the
workshop but it's nice to be here so
this is um you know some joint work with
my students for Omni engine and Mohammed
but overall the scheme of using tensor
decomposition methods for unsupervised
machine learning something I've walked
our world like the past 100 years so you
know I was recently at the workshop at
ISE so I gave a much longer series of
lectures there on you know how we can
give strong theoretical guarantees for
these methods so the idea is if you want
to do unsupervised machine learning you
know the the most natural technique
would be perhaps expectation
maximization or Gibbs sampling or you
know more recently they've been all
these variational Bayesian methods where
the idea is you do a relaxation of the
likelihood to make it much more
tractable to do the I know like an
approximation to the maximum likelihood
solution so I would call these as like
you know various heuristics that do not
have any theoretical guarantees that you
will be able to learn the model
correctly when I was on the other hand
this class of tens of methods we've been
able to prove very efficient theoretical
guarantees in terms of you know very
like you know low sample complexity
which is a small polynomial in the data
and dimensions as well as a low
computational complexity so if you think
of the latent variable models such as
topic models which are very popular for
you know categorizing text such as the
late and originally allocation for
instance and they also you know the
classical mixture models such as
Gaussian mixtures hidden Markov models
and more complicated structures such as
I know tree mixtures or the parsing
grammars and so on so many of these you
know we have a survey paper that shows
we can use tensile decomposition methods
for learning them so the idea is you
know these tensors are actually
functions of law order observed moments
from day
and then you you know manipulate them to
find the decomposition and these relate
to the parameters of my latent variable
model so that's the relationship between
you know like latent variable models and
having tensors for learning them and so
in this talk I'll talk about a specific
application of this and this is one of
detecting hidden communities in networks
I mean this is a very popular problem in
so many different domains right so in
social networks in both even in
theoretical computer science like you
know looking at what are the lower
bounds for when you can detect small
communities and machine learning of
course and so on so here I'll talk about
you know a much more challenging notion
of community detection where you know
the idea is now people can belong to
multiple communities at the same time
the usual notion is that every node is
part of only a single community right so
there's like think of like hard
clustering problem there like multiple
clusters and you want to find them from
data but now if we have the clusters
that overlap this becomes much more
challenging and so the first part of
this work was a theoretical analysis of
this models who became had a model where
how you know people can now belong to
multiple communities and the probability
that there is an edge between two nodes
is like a you know average over all the
community memberships so it's kind of
like a joint effect of being present in
multiple communities at the same time
and so today I'll talk much more on the
implementation side we deployed it you
know so we had a GPU implementation as
well as an OC pu implementation i'll
talk about the different trade-offs as
well as you know i'd like to get the
feedback from an especially like the
systems side of why now like the things
how I'm sure the various ways we can
even improve it further so that's the
overall idea so you know so as I said
like the idea of communities in networks
is very popular right the whole kind of
the notion
of communities came from social network
setting aware you know bigger like
people interact based on what they grew
groups or you know beliefs that they
have so you know for instance I have
research interests in multiple areas
machine learning in a signal processing
and so on so you know my interaction
would be based on like you know with
people with similar interests so that's
a very popular notion for having
communities in social networks and you
can also think of modeling so many like
biological domain problems in terms of
communities right so you can think of
communities of genes and how the genes
are being commonly regulated is based on
like the groups they belong to and
similarly like neurons in the brain like
you know how the synapse with one
another could possibly be related on the
origin of the neurons or their
structural and functional properties and
so on so you know so think of like
communities as like kind of common
underlying attributes that leads to
possibly having more edges from the same
group although that's not a constraint
will impose in our method and you can
also think of like recommendation
systems through these probabilistic
community models although you know this
is not the only approach that's there
for recommendations so you can think of
like reviews on a lab so users go the
review various businesses on Yelp and
now you can ask like how people you know
how do they pick out which businesses to
review right will be dependent or the
common location right the users have to
be in the same location as their
businesses and also as interests may be
like you know there's a certain kind of
cuisine they like so they tend to go and
review those businesses more so so
they're communities in a lot of these
applications but the goal is how we can
detect this efficiently right so in may
all many of these instances you only
observe the network you don't have
labels so you don't have usually like
you know information about what the
underlying communities are and you would
like to detect us and that's what I'd
like to stress that these are
unsupervised learning methods so it's
much more challenging because we may not
labels of what communities these people
belong to
okay so I'll very briefly talk about
some of the probabilistic models we've
you know for which we can provide
guarantees through the tensile methods
and also these probabilistic models give
intuitions on how you know these
communities tend to be formed from you
know based on the memberships right so
the classical model is the notion of
this stochastic block model so the idea
is that you know people now randomly are
placed in say communities so every
person is in a single community and the
edges are formed in a conditionally
independent manner so the idea is once I
tell you what the memberships of the
nodes are right all the edges are just
drawn independently so think of it as
like there is a higher chance of
connection if two nodes are in the same
community right so there's a different
probability of connection rather than if
they were in different communities then
it would be a probably a lower chance of
connection but all these edges are
conditionally independent so this is the
statistical model but you know this has
been studied a lot there been you know
methods such as spectral clustering
where the guarantees that you can detect
such communities efficiently but you
know in reality especially in large
online social networks they're more much
more like overlapping communities right
so like a person does not belong to just
a single community but a group of
different communities I mean I've just
listed some of the networks that I've
been involved in you know I currently at
UC Irvine I graduated from Cornell I was
at MIT I know I was a visiting
researcher at Microsoft Research and so
on so you know so I have multiple
different communities that I belong to
how do i model such you know notion of
memberships into multiple communities
and now if I'm just given the graph how
do I classify people into now multiple
communities right so now this is a soft
clustering unsupervised learning problem
instead of assigning like you know data
to like hard clusters I want to now make
a soft assignment possibly having
memberships
many different clusters at the same time
so how do I do this in an efficient
manner and so this is the idea of the
mixed membership model this has been in
the Bayesian literature for a few years
now so I'll introduce the model but then
the method we used we are using tensor
decomposition is a novel way to do
detection under this model so what is
this model so the idea is you know if
you recall the stochastic block model
was that I just assigned nodes to having
these community memberships and I'll
draw it just based on the community
memberships right yeah no no I don't
have any labels so this completely
unsupervised yeah so completely
unsupervised problem so of course with
labels I could get even better
performance but this is like kind of
saying the compass amidst ik aspect of
where if I have absolutely no labels so
it's more like semi-supervised first so
this so many no solve mostly here deal
with the completely unsupervised setting
but with some partial label information
you know in practice of course we can
get much better performance but
theoretically they'll be interesting to
like you know how by how much I can show
improvement in performance so here the
idea is now if I have mixed memberships
so I can now you know say have some
assignment of nodes so now nodes can be
in multiple communities and they are
fractional so think of it as I'll just
normalize the community membership for
every node so it's like saying you know
this person spends 50% of time and
sports 50% of time on computers and so
on so I'll have this fractional
assignment for every node to every
community like what is them you know
think of it as the amount of time or the
amount of effort in that particular
community right so this is now more of a
weighted membership rather than like the
unweighted one in the previous case and
so
now think of it as like I want to still
use the ideas before that you know that
the a chance of connection between two
nodes is based on what communities they
belong to the only difference here is
that there are multiple communities that
are node belongs to so I'll think of
like the overall probability of an edge
is due to like the average effect of all
the communities they belong to and
because there's a weight associated with
each community it's like the average
probability of memory no over all
different memberships that each of the
nodes are that should take like a pair
of nodes so say this is like you know
50% you know like child probability of
being interested in sports and then this
other person is also interested in
sports you weighted by you know if there
would have been certain underlying
probability of connection if there were
pure communities right but because there
was only fractional amount of time spent
in those communities by these nodes you
weight them according to that right like
the log yeah exactly and this one is
tractable so that would be how many no
so so and also if the probabilities are
small enough this linear approximation
is a approximation to the log linear
model and and it's not clear I mean
again which one would be better in
practice because that one you know in
general you cannot have a guaranteed
solution to learn the model whereas this
one we can learn efficiently okay so
that's the idea whereas statistically
we'll still retain the conditional
independence relationships of different
edges so I'm still drawing the edges
conditionally independently once I fix
all the node memberships so the only
difference here is that I can now have
mixed memberships for nodes instead of
pure memberships right so that's the
idea so this is the mixed membership
model I mean this was you know proposed
by aldy at all I think in 2008 so the
inspiration of this model came from
topic modeling
so if you're aware there was this late
and originally a location which is a I
would say the most popular model for
modeling documents so if there are like
you know text in the documents and we
want to classify the documents you can
think of the you know latent variables
as the topics right so the document can
be about sports politics medicine and so
on and the idea is instead of again you
could think of the very simple mixture
model is that each document is about a
single topic but we know in reality this
is not a good model so the later and
additionally allocation was to relax it
and say now I can have multiple topics
in each document and this is a similar
notion here for communities that I can
have multiple communities for every
person
so how people connect with one another
is kind of a combined effect due to
their memberships in all these different
communities and so the only thing that's
left unspecified so far is how we assign
communities to nodes right in the in the
classical stochastic block model it was
just a random draw I mean there's
certain probability of like being in
different communities which is
essentially the size of those
communities right so I'll just randomly
assign nodes to communities with those
probabilities but here it's a bit more
involved because I want to now have like
you know vector for each node right if
it's like a communities it's a length k
vector on like and have like kind of
fractional assignments so I can have
like you know different kind of
proportion of community memberships so
and so that's the idea that this would
be a distribution on the simplex right
because like the overall community
membership of a node is normalized and
the values are non-negative between 0
and 1 and so you know if we draw a
community membership vector it would be
on this simplex with K vertices right
and so we'll take and this I know the
model that was proposed by a Raleigh at
all used a parametric model for how
community memberships are drawn and it's
a durational a distribution so you
assume that all the node membership
vectors are drawn from this durational a
distribution and
has this particular form so now I'll
give some motivations of why this is a
good choice for modeling community
memberships so I mean you know just like
as a form it's just like product of like
the different dimensions right there are
like K dimensions where K is the number
of communities and you're just weighing
it by like this parameter alpha J I mean
this is known as like the concentration
parameter so let me just you know
illustrate like the this distribution in
various regimes and then it becomes much
very clear on why you know this is very
flexible for handling a range of
different possible community membership
models so in one extreme if you like
have these alpha J's go to zero so this
concentration parameter goes to zero the
probability mass is only on the vertices
of this simplex so this is nothing but
the pure membership model right so you
can only have realizations as one of the
vertices so if it's a vertex it's only
one on one of the entries and zero on
the others so that means like you can
have membership only on a single
community equal to one and it's zero
everywhere else so we are back to the
original pure membership model as a
degenerate case here so this is a richer
class of models that also has the pure
membership model as a special case and
this is nice I mean if like you know
there are communities that are close to
being pure we can also detect those and
the other hand as if keep increasing now
this concentration parameter you have
like probability mass now spread all
over the simplex which means now you can
have nodes which are in multiple
communities right but still if you have
this concentration parameter be small
enough in fact if it's less than one you
can show that there's like more
probability mass towards the vertices of
the simplex so this you can think of it
as a sparsity encouraging prior right so
most of like the you know if you look at
like realizations from this distribution
so most of the probability mass is only
on a few indices meaning like you know
this is also realistic in practice right
I like a person may belong to just a few
communities compared to every possible
community I mean if the communities are
like research interests of a person and
they're only few of them compared to all
possible research interests and so you
know in cactus it's like very restricted
to have a single community for every
person but you know it's also not
realistic to have like all possible
communities for a person right so on
this model we can uncreate sparsity in
having community memberships for a
person sure sure I mean this is doing it
and kind of you know the thing is this
is tractable right so there are so many
different ways to assign community
memberships and in general those like
you know become harder learning problems
and this one is tractable so that's kind
of the trade-off involved here and
that's what I'd like to argue that this
is handling multiple different aspects
that are desirable like having like you
know being able to have a flexible way
to have sparse community memberships as
well as you know have this stochastic
block model as a special case and and
also have like a probabilistic
interpretation of communities so this
not only you know in the end gives just
like what are the communities it gives
you a model so you know you can do link
prediction you can like predict the
structure of the network and so on so in
that sense it's trying to address
multiple different I know
goals at the same time okay and so
that's the idea that you know with this
concentration parameter we can tune for
the amount extent of community
memberships and the idea is I mean this
this alpha J is one is just a special
case where it's just uniform over the
simplex right and if you keep increasing
this parameter there's more probability
mass towards the center of the simplex
and in the other extreme it's just a
single point it's just a deterministic
value which is just like equal
membership in all the communities right
so this is mostly like not of practical
interest I mean you know if every node
has just like equal membership in all
the communities and that's just like
kind of the you know there's no
community structure
right but that's like the other end of
this bar that would be interesting yeah
but we are many I mean precisely where I
guess work right there's like an
anomalous community versus like the
usual one right right right yeah right
absolutely yeah so we mostly kind of you
know like the validation we've done is
where there's already ground truth
available and we are detecting the
communities but absolutely they're my
know applications beyond okay
I mean except for the normalization
that's what it says right if it's
completely product form this would have
been completely independent but I'm
normalizing them so it's kind of a weak
dependence because of this normalization
so it is there there is dependence but
because because of this form so it's not
a completely arbitrary and that's what
this is saying and yeah so that's what
I'd like to stress here that you know
can control this parsecity and even like
theoretically you can argue that like
kind of the sparsity level meaning the
number of nonzero entries in this
membership vector is proportional to
this quantity alpha naught is like the
sum of these averages so I mean though
those are the details but the idea is I
can have kind of a flexible way to
control the sparsity level in my
community memberships through this model
so this is the model now the question is
how do I learn this just given the graph
data right so all I'm going to observe
is this graph and I'd like to classify
nodes into the different communities
right so in this simulated graph here
you see that it's pure communities this
is the if you recall as when R equals 0
is like the stochastic LOC maurella the
pure membership model so here nodes see
no these are all nodes in the blue
community here green and red so right so
here you can see that I can very easily
visually say with what the communities
are right of course I've arranged them
in a nice way to do that but once you
have like these overlapping communities
it becomes a lot more challenging right
so you can already see visually that the
community clusters are not as well
defined here so these colors here now
represent the mixed membership of a node
so earlier a node could either be just
blue green or red red but now it's a mix
of those and that's what these colors
here represent and as I keep increasing
the level of sparsity it becomes even
harder to say what the communities are I
mean just like the goal of this
pictorial is to kind of just you know of
course it's obvious that with
overlapping communities the problem
becomes challenging and you can see that
even visual
here yeah
so more on the implementation part are
yes oh yes sure sure sure
so I would like to know put them into
two bins like you know one with like
theoretical guarantees and one which are
like practical right so and I would say
like this is the one that has the boat
so no so the many theoretical methods
for overlapping communities but mostly
combinatorial and they are really not
practical like you know you can't even
it's not a realistic method the other
ones which are much you know the
practical side you know like for
instance your Alaska which I think you
know one of the methods requires like a
non-negative matrix factorization I mean
general these are all np-hard problems
and and there are no guarantees that
these methods will succeed so that's one
of the aspect of the other methods the
other is there many of them are not
probabilistic models so you don't
recover an actual model but only say the
community labels right so this is doing
both that it's learning the model as
well as giving the communities but no
but they're not they don't go to the
global optimum they just go to the local
optimum in the end you just do if you do
a variational approximation you're doing
a relaxation of the likelihood and then
there is no guarantee of how that
relates T but you know I'm very sure
that there is no other paper that has
theoretical guarantees for overlapping
communities so I mean we can talk
offline but I'm very sure about that
fact so this is the one that and also
this is else I will show you the results
these are extremely fast because of
being able to paralyze in a natural way
so you know in most methods if you have
to paralyze if to worry about losing
performance but because these are matrix
and tensor operations they're just
naturally parallelizable so that I would
also claim to be another strength of
this method but we'll see the details
okay so so here the point of this slide
was to motivate the fact that now with
overlapping communities we have a lot
more challenging problem on hand
being in a you know in the ability to
actually tell apart what the communities
are right and to learn them efficiently
so I won't describe the theoretical part
of this work this appeared in cold all
the conference on learning theory this
year and so here we establish that you
know we have a procedure that can
correctly learn the communities of all
the notes and in fact you know for even
the stochastic block model which was the
special case of pure communities we got
the best possible performance so these
match the lower bounds in terms of both
like you know what are the size of the
clusters and how densely connected the
clusters are right so the idea is if
they're very sparse connections for you
know like I just have like it's a very
sparse graph it's much harder to say
what the communities are so we can even
handle like up to the limit of how
sparse the graphs can be and how large
each clusters can be human if the
clusters are small again it's hard to
detect them so we give bounds on what
you know how how these can scale and we
can still learn them efficiently and
also with respect to this parameter
alpha naught right this was the sparsity
level of my community memberships we
also say how the performance degrades as
this increases right I mean as you saw
even visually that if I have more
overlapping communities it becomes
harder to detect so now we quantify
exactly what it is in fact we'll see
that like the number of nodes in the
network needs to scale only
quadratically in this parameter so the
idea is as I have denser and denser
memberships I need more nodes in to
observe right this is like a larger
number of samples and so but the
dependence is only quadratic which is
nice it's not exponential at some of the
other theoretical methods have like
exponential dependence on the extent of
overlap between the communities and this
is not the case and so those were the
theoretical results I won't address them
here but they give a brief idea of what
the method does and the previous one had
like you know I didn't focus on the
implementation part it just like kind of
said
okay this is a you know method with
having you know if you can do tensile
decomposition sufficiently then you can
do the learning but here we'll you know
see how when it comes to a practical
diploma and there are many more issues
that we want address but the main idea
that I'd like to emphasize here is you
know kind of for all these tensile
decomposition methods is to think of
kind of using the observed moments to
form the tensors so you know if you
think of like just like the first order
moment right it's for if it's a random
vector then it's like a vector right the
first order mean is a vector now if you
look at like the second order moment or
the pairwise relationships that's a
matrix so now if we look at third order
relationships that's a tensor so that's
where the tensors come in they formed as
higher order moments of data so the idea
is I know just if you look at pairwise
information that's not sufficient
information to get the community so you
need this higher order moment
information you know typically like you
know when people were thinking about
these problems in classical statistics
right there was very little data so
going to higher order moments was not
advisable but now with lots of data you
know going to like third or fourth order
it's very much like you know it's fine
like you have lots of data so you can
hope to get like good concentration in
terms of being able to compute these
quantities efficiently but the thing is
how do we exploit the additional
information in this higher order moments
efficiently and that's where these
tensor decomposition approaches come in
okay so so the main kind of the overall
goal is I have a I know these tensor
formed from the higher order moment of
observed data if I do the decomposition
I can relate them to the parameters of
my model so in this case you know I can
relate them to the community memberships
of the nodes so that's kind of the very
big picture view of this method but the
many kind of details in how we go about
doing this tensor decomposition so so
them and also would like to address how
we can parallel eyes
efficiently scale it up to very large
scale like you know the memory bar
I know depends like dependence on like
how much memory and so on and so that's
what we'd like to address so think of it
as like the main contributions of the
you know part of the talk that I'm
addressing now you know will be to do
the tensor decomposition efficiently and
those mainly consists of two steps so
the idea is you know there's this is a
large tensor like if they end notes in
the network this is I n by n by an
tensor we'll see in a moment what
exactly that that tensor is but this is
like a n cube object right if N is a
million or even a billion in cube is
ridiculously high so if we have to
actually form or store this tensor this
would be a very bad idea but this tensor
is actually computed from data so we'll
be showing how we can implicitly
manipulate this and in fact is like a
stochastic gradient descent so in a very
fast manner like you know just passing
through data we can update this like the
the decomposition of the tensor and we
can do this efficiently so that's one of
like the main aspects that although this
involves decomposition of a very large
tensor right a potentially and cube
object we do this carefully by never
explicitly forming the tensor and doing
it as like a stochastic optimization so
you run through data rather than you
know doing it as a batch one which would
be much slower and and and also the
other aspect is not to do the tensor
decomposition in the ambient dimensional
space but to first do a dimensionality
reduction so you are now decomposing a
much smaller dimensional tensor and this
also makes a huge difference I mean this
is kind of the same ideas as like you
know sketching or like various different
dimensionality reduction ideas right
even if you want to do just like a
low-rank approximation of a matrix like
you want to do similar value
decomposition for very large matrices
you can first do a dimensionality
reduction and then do the exact SVD and
this is much faster than trying to
decompose the original
tensor itself all sorry the original
matrix itself into its SVD form and so
we'll use all these ideas to do the
tensor decomposition efficiently and
there are two pieces of code one is a
GPU implementation so here you know we
like you know wanted to optimize to
reduce communication between CPU and GPU
I mean you know from the earlier talks
you know the this was addressed to that
the communication becomes a big
bottleneck when it comes to GPU
implementation and we I'll show like the
timing results where without you know
worrying about communication too versus
having like kind of this device
interface where we minimize
communication there's like at least an
order of magnitude running time
difference so we can get been a much
faster performance by doing these
careful optimizations and most of the
graphs that we see on real data like if
you think of online social network
graphs are sparse so having all these
manipulations like you know all these
vector vector products or Wecht or
matrix products as sparse
implementations or even like sparse
slideshows for a sweetie implementation
you know may also made a huge difference
and in fact like if it's a million by
million matrix if you have to just rode
it on to ram that's not feasible but
most of these graphs is false so even on
a million gnawed graph we could load it
on to just the memory so it's makes a
huge difference in having this farce
implementation and probably I won't talk
about this in a lot of detail we also
give like kind of statistical basis for
testing what is a good community model
right so the thing is if I also have
like ground truth community so think on
ALP I have like for every business what
type of cuisine it is if it's a
restaurant or what location and so on
right so these are all you can think of
as communities of the business nodes and
all I know from my method I'm estimating
some community memberships but because
this is unsupervised I don't know how to
relate the two right so I'm like you
know extracting a set of labels there
are some ground truth labels and I want
how you know how well does my method
perform how why is this a good way to
detect communities and we used like up
kind of a p-value test or multiple
hypothesis testing to decide the
relationship between estimated and
ground truth communities and use that as
a basis to calculate the error so again
this usually you know p-value tests are
very popular in computational biology
but I haven't seen it being used a lot
more on the social network side and this
also I view it as a contribution here
and I'll talk about it if this time so
how much time do I have 10 to 15 ok so
then I'll go very quickly I won't talk
about the kind of the the theoretical
aspects but the main idea is what is
this tensor that I'm referring to all
till now it's kind of you know thinking
about like kind of in the way it's a
neighbor of neighbor kind of
relationship right so for every triplet
of node I asked how many three stars
it's part of or how many common
neighbors does every triplet of nodes
have right so here there's a node a B
and C and I'm asking how many common
neighbors it has in this set X I'm just
like you know taking a part of this
tensor like I'm just making all these
four nodes in two different sets
disjoint sets to remove some in
dependency issues so you know this makes
it much more convenient just to take
like each node in this sub graph to be
in a different set so this is the
statistical use so the idea is I'll take
a triplet of nodes in these sets here
right and I'll ask how many common Abers
it has in this set X right that's
exactly this quantity here and on and
you know if you form it as a tensor it
would be like the outer product of the
neighborhood vectors so this
neighborhood vector I take for X to set
a set B and set C and I'm finding the I
know this is like the tensor product or
the outer product right and I averaged
over all these nodes here and I get the
third order moment so this is the idea
that this is a third order tensor
because for every triplet I have I count
how many common neighbors have right so
that's why you do it for the size of
this is like uh order and cube if each
set is like order in here so I'm just
gonna you know randomly partition the
nodes into four almost equal size sets
and then I'm like finding this quantity
okay so and in social networks
literature using this friends or friends
is like the very popular heuristic but
usually P you know it's like a local
test usually people tend to see like oh
if I have a large number of friends to
friends then you know maybe we are in
the same community or not but we are
using the measurement in a much more
sophisticated manner to get overlapping
communities and the main idea is you
know statistically if I look at the
expectation of this quantity right so if
I had the actual expectation which I
don't but you know practice I only have
samples and have to argue that even from
samples they may the moment I estimate
is close to the truth but if I had the
expected one it would be just like a
round K tensor so the idea of rank K
tensor is very similar to a rank K
matrix right if the matrix has rank K it
has a singular value decomposition of K
terms so each term is a rank one term
right and here too it's the same idea or
knee that it's a rank one tensor here so
if it's a rank one tensor it's like the
I you know tensor product of three
vectors and I have K such vectors where
K is the number of communities are in my
model and so this is the idea that if I
you know presume this probabilistic
model and look at this three star count
object right this M three object in
expectation it looks like a low rank
tensor and that's why now I can hope to
you know I learn the model from this
object right so this is like much more
constrained if I just looked at a
general object of size n by n by n it
would have a very large rack but because
I'm I I know assuming the presence of
like these communities it would have a
much lower rank and so now the question
is how I can do this
efficiently so so as I said there are
two main steps to doing this tensor
decomposition right the task is to find
like this tensor decomposition right I
need to find each of these terms and I
need to find yeah yeah so I phase just
some fixed matrices I didn't you know
describe what these are so these relate
to the communities so essentially once I
find these matrices f afdf see they're
related to communities who through some
just linear transformations the details
are in the paper but the main idea is
that it's a you know overall it's a rank
K tensor and if I can find this
decomposition efficiently then I can
find the communities that's kind of the
very big picture I can give at the
moment it's fixed it's like K yeah it's
this quantity K here so if the K
community is this tensor would be a rank
K tensor
yeah so I mean in real world we would
tune it right we can try it for a
different sized number of communities
and see which one gets a good fit and in
fact we can even like you know because
the first step is this singular value
decomposition the decay of this singular
values as well we can adaptively
estimate what the number of communities
are yeah so the thing is there would
always be like a limit like many small
communities no method can detect so it
depends on how small it is right and we
could even do it as like a peeling one
and that's what like my students are now
testing that you first recover the large
communities you subtract them out and
then the hope is we can recover smaller
communities through refinement so
they're different ways to kind of even
push beyond in terms of having detection
of very small communities yeah so in the
remaining few minutes like I will just
very quickly describe what the procedure
in tiles right so this tells a
decomposition as I said it's much more
robust and also computationally
efficient to do it in two parts
so the thing is this is like a third
order tensor here this cube that I've
shown it's a n by n by an object but now
I'm going to reduce it to a where much
smaller tensor so this would be the
first step and I can do it through
singular value decomposition so I'll
find like a linear transformation that
if I allow apply along each of the modes
which I call it as a multi linear
transformation I get a much smaller
tensor and the size of this tensor is
only K by K by K where K is the number
of communities so the idea is because
this is a R and K object the only kind
of relevant number of parameters should
be found in this K dimensional object
rather than an N dimensional object it's
the same idea even for like Laura and
matrix approximation and if it's a low
rank matrix I need you know even with
dimensionality reduction I don't lose
information because it has these
constraints because of low rank and it's
the same idea here but for the tensor
case and so the idea is I think you know
but doing the these even these
manipulations
Stephen products carefully is an
important aspect to doing this part so
so this is the part of like the
dimensionality reduction so for
dimensionality reduction we need to like
kind of compute some pseudo inverses and
matrix multiplications and then do like
a low rank matrix approximation and the
comment here was to show that you know
these are sparse matrices but if I took
the universe of this and did it in a
naive way I would end up getting like
you know and by an object where this is
a dense one right and I don't want to do
such dense manipulations and in fact if
I had to load them on memory that would
not be feasible because a dense object
would be like kind of tens of TB whereas
if it's like you know if I I know that
these are rank K objects so these are
low rank objects I have to manipulate
them directly through their low rank
approximations and not do it naively so
even simple things like that and just I
know is showing like how if I manipulate
the order of my multiplications right so
here I'm only maintaining like n by K
objects by doing this order of matrix
multiplication compared to the previous
one where I would form large and by n
objects so or even these simple ideas
matter a lot if you think of n as
million or even more and so so you know
so we kind of in a took care of having
like sparse implementations for these
multiplications or I am maintaining like
low-rank approximation of the matrices
to do these manipulations and we also
use like kind of some of the standard
randomized low rank matrix approximation
methods they kind of column subsampling
or kind of Nystrom kind of methods to do
dimensionality reduction and then to do
like singular value decomposition of
only tall thin matrices so instead of
doing I n by n matrix right it's like n
by K matrices that we manipulate and
this made a huge difference and the next
part which I don't have time to go in
detail so they once I reduce the tensor
to like a K dimensional tensor I still
have to decompose it and what we show is
this
and reduce to an eigenvalue problem on
tensors I mean this is kind of a
nonlinear manipulation of tensile to
think of eigenvalues right so in the
matrix if I hit the matrix with a vector
and if I get back the same vector that's
a stationary point or an eigenvector of
a matrix here it's the same idea but
it's like a nonlinear operation because
the tensor has three modes think of it
as a cube and if I hit along two of the
modes that is like taking in a product
along two of the modes and if I get back
the same vector then it's called an
eigenvector and let it also this
involves kind of quite a bit of
theoretical analysis to argue that the
only stable points of like this power
method kind of idea is only like the
components of this tensor decomposition
and they also many differences from the
matrix power method but I won't go into
detail seconds discuss offline with you
but the main idea is if I you know did
this as naively as like a batch method
and I form this tensor this would be
very expensive so and and also if it's
serially done like I find an eigenvector
I deflate it I go back again find
another eigenvector again this would be
slow so what we do is in a stochastic
gradient descent of the tensor and in an
implicit manner so we are never forming
the tensor explicitly so here the idea
again is to you know so these are like
vectors which are just white and
neighborhood points so I take a
neighborhood point I do a linear
transformation of it and I get these
vectors here and so what we show is we
can find all the different eigenvectors
as this optimization problem right so
this is a non convex optimization
problem in general this would not have
guarantees of approaching the global
optimum right but in what we can show is
if we make this this theta is like a
cost for orthogonality so you may can
make it large enough so you can you know
I encourage the eigenvectors to be more
orthogonal to one another and the other
part of this objective function is kind
of a correlation remote I want my
i convicted to be close to the observed
tensor that i formed from data so it's
kind of balancing the board that I'm
simultaneously updating my estimates of
all the eigenvectors there are K
eigenvectors because it's around K
tensor and I'm you know trying to
maintain them to be orthogonal to one
another and at the same time I want to
make them well correlated with my data
and so that's the idea that even though
there is a tensor this is the only
update that matters right this is the
gradient and I'm updating the gradient
and in each step that is just like
taking inner products so you know
although this like kind of theoretically
involves finding a decomposition of a
large tensor we can do it through just
shy implicit manipulations are never
forming this external tensor
so because of like kind of we are
relaxing this orthogonality cost and
yeah so I mean it depends on like how
much how with you in this parameter and
in practice what we see is the
orthogonality is the most not the most
important goal here right because we
want to ultimately use it to detect
communities so it's not necessary for to
make it very strictly orthogonal to one
another and that's why we can like save
from not doing gram-schmidt than those
kind of costly orthogonalization
procedures because the end goal is not
to find an orthogonal decomposition and
so that's the idea and like if we look
at like kind of the running the
computational complexity of this method
under like a parallel computation model
it's only linear in the number of nodes
in the network and you know this K is
the number of communities because K is
usually much smaller than n like even in
a million no network maybe there are a
few thousand communities right like not
much more than that so this is kind of
not as important as like the dependence
on n and and we can join also even like
space complexity we're only using n
times K objects rather than n square
like n square would have been very
expensive and so we're being careful
even about that and this variational
method was I would say like kind of the
closest competing method in terms for
this model because this I mean they
assume the exact same model but they are
doing a relaxation of the likelihood and
updating it and it's not as naturally
parallelizable as the method here and
also it takes much like even
theoretically it requires much more
computational complexity than yeah
yeah but we are estimating the
parameters of the model so from that you
can then compute so so you know so we
get the community memberships for every
node I mean that's what the tensor
decomposition I'm I didn't give the
details so once we find the tensor
decomposition we get back the estimates
of what the communities are for every
node so and so very quickly maybe then
just give the experimental results I
won't talk about the code optimizations
this was the part saying that you know
in the gpo we minimize the communication
for computing these eigenvectors so we
stole the eigenvectors in the GPU itself
as a device interface and only moved the
data points to update the stochastic
gradient descent for tensor
decomposition and this gave like an
order of magnitude difference in terms
of the running time so this is likely
the best one is the GPU into you know
implementation with device interface
where we minimize communication the
other one is this standard interface for
cooler this was just like kind of the
CPU one with sparsity so I think you
know if it's small enough number of
communities for sparse data this could
be competitive but as I increase it this
is like the GPU one because of the
parallelism so this was synthetic
experiments so this was for like that
just the part about the stochastic
gradient descent for tensity
compositions and this was just like the
coldest matlab toolbox for tensors
that's kind of the most if you're just
starting tensors that's the easiest one
to use but of course MATLAB you know it
will even crash after like some I think
few hundred communities so it's not very
scalable and this one here it there's
only dependence on the number of
communities because we assume it there's
already dimensionality reduction so you
know the number of nodes does not matter
because in this step we've already
reduced the dimension of the tensor and
made it a k-dimensional tensor so the
only dependence would be on k4 this part
of the method so this only part of the
method
we have given the timing in this plot
and the the main difference I wanted to
bring out was that by reducing the
communication to the GPU we can get a
gain in terms of running time so this K
here is from like few hundred to a few
thousand here K is the number of
communities and this is only like not
the entire algorithm it's only the part
of the algorithm so this is just this
was actually the dense setting like you
know we use just the dams one so in that
sense like I guess only the number of
communities is small like so I think
this part actually you're right is not
due to sparsity but like I think because
if it's very small you know the
communication still has no where we're
headed with the GPU so if we do it right
on the CPU maybe it's good enough but as
I have more and more communities the
parallelism of the GPU helps me to do
better and I think that's why there's
crossing here this is a cpu
implementation with the Eiger and
library ok so this was for the
stochastic iterations I won't go into
these validation let me quickly go
through the oh sorry ok I'm so sorry so
just one minute just and because this is
just the main result I wanted to show
and so you know so the main result I'd
like to emphasize is the largest one
we've run is about a 1 million by you
know node Network and this one took just
about one and half hours to run this is
including the time that was needed to
load the data on to the memory so the
actual execution of all these algorithm
was around 10 minutes for the method and
the other the competing one the
variational one is extremely slow
compared to this so so that's kind of
the main result so but I guess we'll
take more discussions offline sorry
about running over time
ah no I mean the thing is after
dimensionality reduction it tends to be
dense if you have already multiplied but
we can keep them as sparse once you know
so we can kind of do it on the fly as
well as sparse manipulations so
everything we are kind of doing it on
the fly yeah sorry about that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>