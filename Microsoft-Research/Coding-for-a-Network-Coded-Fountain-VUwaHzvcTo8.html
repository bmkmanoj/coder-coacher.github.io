<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Coding for a Network Coded Fountain | Coder Coacher - Coaching Coders</title><meta content="Coding for a Network Coded Fountain - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Coding for a Network Coded Fountain</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VUwaHzvcTo8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay thank you very much for the Marc
Gasol I suppose ray I think you very
much for the kind introduction ok so the
talk i'm going to give today is has a
title bat-scope coding for a netbook
coded fountain okay so uh we're going to
first talk about the problem now in this
picture we we show a network with a pack
is generated at the source node s okay
being multicast to two signals t1 and t2
what is this seller has nothing to do
specifically with the butterfly never
die we're using it anyway so here we see
packets in green and red where the green
packers are those packets that are
successfully arrived at the destination
and the red ones are those that got a
drop a long way okay so we are talking
about sending out files which are
relatively big for example like 20
megabytes consisting of about 20,000
packets okay so we'd like to find a
practical solutions to this to this
problem which such that it has a low
computational and storage costs a source
close refers to the amount of storage
that you are you need at the
intermediate nodes and we want one with
a high transmission rate and also with a
small protocol overhead okay so one
possibility to do this is through tcp
okay by doing so you need to do a
acknowledgment hop-by-hop which is not
scalable for for Maalik ass again it
mainly because the cost of feed bags too
high okay we can also consider using
some well-known for error correction
scheme such as fun encodes okay it's
very scalable for multicast because you
don't need to feed backs every so often
and so so therefore in a in a case of
multicast
be implemented quite efficiently ok this
is the regarding the complexity of
funding coast with routing which is in
fact extremely efficient so we consider
a file consisting of K packets yeah ok
so remember this notation kk is an
important parameter in this problem ok
we've got a file consists of K packets
and for each packet it consists of atty
symbols way so T is pretty much a
constant a so if 44 encode the coding is
extremely efficient because it uses a
kind of sparse coding is 0 of tea per
packet which means that the the encoding
complexity just does not depend on the
file size at all whenever whenever you
don't see a k is something good ok now
for decoding it uses a belief
propagation decoding again is r is a
constant ok which does not depend on
file size and the routing in between is
it's very simple operation and you need
only a very small buffer for that now
this picture shows how how things work
ok so on the top are the the input
packets or the sauce packets ok and here
we go ok so at the top are they are the
sauce packets and we there's an encoder
s here which encodes them into different
encoded packets and that this sent to
the source no sense to intermediate note
you and that and what it does is nothing
but a store and forward and along the
way there's some red packets they're
being lost are now at the decoder at the
receiving end t it implies that belief
propagation decode them so that it
doesn't have to receive all the packets
as long as it received enough packets it
can start decoding properly and a 10 you
would be able to recover all these
source sauce packets ok however there's
a drawback with a
using found in course is now let's
consider this case when we try to send
things from s to T through you and these
links are repaired drop rate equal to
point to you I was told that in LTE you
know this is not something too uncommon
to happen because the rates so high now
the for this network the capacity is
equal to 0 point 8 the reason is that
suppose you can you send a file from s
to you okay because the pecker is equal
to point to you can set it by using a
foreign error correction like a final
cause TCP ok you can you can send things
from s to you at red equal 2.8 then you
can decode the whole file and then you
re encode and you send it on another
link with a game with packet rate pega
drop rate equal to point to ok so by
doing so you are you're repeating what
you're doing at you is just to repeat
what you do it at s ok so by doing so
you are you actually can send things
from s to you at red equal 2.8 but
though it is not what the way I've shown
you it's not necessarily the best thing
to do now if you apply for forwarding ok
and retransmission n2n for a fun encode
the maximum rate that you get is only
point six for the reason is that this
guy that's nothing but forwarding the
package that it receives so from here to
here you lose twenty percent so you get
eighty percent remaining and from here
there here you lose another twenty
percent so you get the maximum rate
equal 2.64 this is for both
retransmission and also for our for fun
and coats ok so what fun Kobasew is not
the rate but is the efficiency okay all
right now the in this is this
theoretical upper limit for the rate
that can be achieved by a final code but
in reality if you work with a final coat
with a small pea-sized and are there's
extra lat gap between this of a bar and
a real rate that you can you can
actually achieve
now I'm I also i mentioned that you know
in principle you can get a rate equal
2.8 by setting the file here the code
and then re-encode right now there are
two two problems with this
implementation first of all you are
there's a delay incurred because you
have to decode before you really encode
it which means that you have to wait
until the whole file come comes and if
you have a a multi-hop network and every
help you you incur a delay which is not
something very desirable and also that
this note has to store all the packets
before you can encode which means that
the the pop of buffer size required at
the intermediate nodes grows with the
file size which is also something not
not good to have okay now we know that
if you apply a random linear network
coding at note you in principle you can
achieve rate equal 2.8 now by randomly
no coding I mean a following you have a
buffer at node you and this
implementation actually was it's the
same thing as the app launch system that
was invented by the Microsoft people at
the Cambridge so what you do is that you
have a buffer here that stores all the
packets that that arrives at note you
and whenever you are you send out a
packet you just take a random linear
combination of whatever you have on hand
okay even if there's no new arrival
package next time you send out and other
pecker you take a different random
linear combination okay so there's no
delay incurred at all it's just
pipelining okay however the if you apply
okay before I I tell you the drawbacks
of this straightforward implementation
of nano coating I I'll try to convince
you that with a random lenient
nano-coating you actually can achieve
rate equal 2.8 now this picture actually
shows the operation of random linear
neural coding so are the first row is
the role representing a note s okay the
middle road representing
you and the last load of representing
the tea so you go horizontally this
represents time so this note here is s
note s at T equals 0 equals 1 t equals
to so and so forth same thing for note
you t equals 0 note you t equals 1 so
and so forth now here we have some red
arrows okay these arrows were represents
the are the transmission from no s to
note you every time unit okay and it has
capacity equal to 1 and their cross is
here once in a while representing the
packet drop rate on it on the average
you lose about twenty percent now the
same thing from the new layer to the T
layer okay again we have a pack of drops
here and there now there is also another
kind of arrows in black that goes
horizontally ok now this represents the
memory from the past maybe because we
weave buffer everything and we assume
that you remember things in the past
okay and for the sake of convenience we
assume that this link has capacity
infinity but in reality it doesn't have
to be larger than the file size a now
are now because uh ah we have you think
of these as water pipes okay and then
you try to pump water in at no 10 sat
time go 0 and you want to know you know
how much water you can get at the bottom
layer as time goes by now because our 20
of these twenty percent of these pipes
are broken you can send you can press
wallet down from what I think is
inspired to assume that these pipes are
block instead of the pressure be broken
because they're for a broken pipe you
lose the water okay so for this
discussion you know we think of them I
think of these pipes being blocked in so
because twenty percent of these pipes
are blocked you can the press water down
from the top layer to the middle layer
at three equal 2.8 okay now you can also
press boil it down from the middle layer
to the bottom layer and r equal 2.8 okay
because uh
about twenty percent of these pipes are
block the question is whether you can
press water down from the top layer to
the bottom layer and rate equal 2.8 now
the reason why it can be done is because
of the asics of the existence of these
thick pipes that goes horizontally the
things that you press one down from the
middle layer to the bottom layer at
breaking point aids and the water can
travel horizontally and find their way
down to the to the bottom layer okay now
if we examine one of these notes in the
middle layer carefully okay so it has to
input links one is the link from the
past and do another input link is the
link that you receive the new packets
okay so what you do is that you by
applying randomly Ninevah coding what
you do is that you take a random linear
combination of the past and the newly
arrival packets and then you send out a
new packet so this picture this time
parametrized graph or you want to call
the trellis diagram depicts precisely
what's the the Avalanche system is doing
ok so in the in randomly nian encoding
all we care about is is the maximum flow
so now we've seen that the maximum flow
rate actually the maximum flow from s to
the bottom layer actually grows with
time at rate equal to point H that's why
you know we can with a linear network
coding at note you you you actually can
send in for me send information from s
to T at rate equal 2.8 ok this is the
intuitive explanation why random linear
neural coding would do the job ok
however you sacrifice a form efficiency
because the reason why I finally codes
are so efficient is that there the
encoding is very very sparse ok whereas
for randomly ninel coding which is
depicted here ok you just take this
package is formed by taking random
linear combination of whatever you have
and this is a this is a dance and coding
again when you do
randomly nano-coating at the end and the
intermediate node is it is again dance
encoding so that you can still decode
but the decoding is not efficient in
particular the encoding complexity is o
of TK per packet okay as I told you the
K is the size of the file and this thing
that as long as K turns out it's not
something good okay we don't want that
for decoding the I've you use a
straightforward gaussian elimination the
complexities of K Square + TK per packet
so essentially so T is a constant this
is essentially skate square again it's
not something very desirable and so for
the intermediate know if you apply no
coding again there's some complexity
associated with it and for this
straightforward implementation it
requires a YouTube buffer order K
packets which which is essentially the
whole fire and this so if you if you are
one should has been a bigger file then
you require the intermediate nodes to
have a bigger buffer which is something
not very desirable either okay so after
seeing these slides we come up with a
quick summary on the one hand we can
have a routing plus fun and code which
is low complexity but the rate is also
not not satisfactory if you want to have
a high rate you can go for an hour
coding but but a complexity is high so
let us first review some existing
schemes to try to tackle the problem
okay um now the very reason why why
applying rent rent millionaire coding
we're in the middle node with screw
things up is that now according changes
the degree distribution of the received
packets now in designing a final code
choosing the right degree distribution
is is the main thing okay if you do
random randomization between you screw
up the the degree distribution and solo
the decoding complexity cannot be
guaranteed
so there have been some efforts in the
22 trying to get around the problem okay
the the main idea is to try to treat the
random Lena Neville coding at the
intermediate nodes so that n to n it
still looks like a a fountain coke but
this is something rather ad hoc and it's
very hard to extend beyond a very simple
never wait but even if you do that the
intimate notes the computational cost is
too still high and you also need to
store all the K packets ok and then
there has been some work coming from
this group actually it's actually square
wire goes almost ten years ago you guys
really no no no what problems are
important ok so the idea is to use
so-called chunks to reduce the the
coding complexity ok so we know that's
the the colon complexities is is gross
at a rate higher than linea but as long
as you keep things small things are
still manageable ok with if you use if
you chop things up in your chunks so
that you do our random leaning down
coding within this chunk here and also
within this chunk here ok see here you
see that the first node depends on lean
on the first know the same know depends
on the first oh and if second load is to
see a kind of causality know our
constraint on the coding ok so so the
idea is to keep things more ok if you do
that the encoding complexity is o of tkl
where is the chunk size case the case
still there are and decoding complexity
is o of KL square plus tkl this is a a
little better because I previously we
have K square but here is only a solve
linear with with k where we can regard
at here now essentially as constant ok
but how about no buffer requirement in
the
intermediate knows it really depends on
depends on the implementation but one
one big problem with with using the
Chung approach is that is how to
transmit the channels okay so they have
been different approaches the obvious
approach is to do see sequential
scheduling of chunks which means that I
i transmit things for this chunk and I
wait until everybody are done with this
and then I move on to the to the same
charm but one drawback of this this
scheduling is that the it's not scalable
for multicast because for wallah casse
there could be some you know many many
receivers and cut some can be faster
some can be slower way so it's not not
really scalable form for multicast
another approach is to use a random
scheduling of chunks meaning that I have
all these chunks and I rarely pick one
or two to transmit and hopefully this is
a new one to to you now with this
implementation however the intermediate
node again have to cash all the K
packets okay unlike a sequential
scheduling for sequential scheduling you
don't have to our our cash all the K
packets now however this such a scheme
would become a less efficient when the
major fraction of all the chunks have
been decoded the thing is that if that
there are 100 trunks chunks and you
already have received 90 of them if i
randomly pick one then ninety percent of
the time it would be a redundant chunk
for you okay okay and they have oh
there's also been a FS along the line of
overlap chunks okay the lab the chunks
are not totally independent of each
other but this can improve the
throughput of random scheduling but
still it cannot reduce the buffer size
so so what we learned from all this
discussion is that you know funnel codes
are not really compatible with no coding
but rate lessness is a good property
that we
we want in the multicast application
Chung's can be used for no no coding but
our difficulties to schedule so we are
trying to address all these issues using
a new approach which we call a bats code
best code refers to batched spas codes
okay and the operation of Ben's code is
as shown in this picture here so on the
top again we have all these sauce
packets and we form we organized the
encoder packages into what we call
chumps okay in this case the I'm sorry
is a battery a patch is different from a
chunk in the sense that the forge for
chunks weeks kind of thing of them as
operating independently but as we are
going to see a badge actually inter
operate with each other okay so here the
a badge has size equal to 3 and so let's
see how we reform the first batch okay
let's see how we found the first packet
in the first batch now to form the first
packet in first batch we draw a degree
distribution are okay we draw a degree
from a degree distribution and let's say
we are the degree is equal to 4 and then
we randomly pick four of these sauce
packets let's say we pick this one this
one this one this one okay so what we do
is we take a random linear combination
of these four sauce packets to form the
first packet for the first batch okay
and the form this second packet of the
first match we stick with the same
subset of sauce packets but we take a
different random linear combination okay
so on and so forth so we're done with
the the first batch now to form the
second batch we draw another degree from
the degree distribution and this time
let's say other degree is equal one two
three four five okay and then we
randomly pick five of the input
of the five of these sauce packets and
from a random linear combination to form
the first packet here now to form the
second packet here we we use the same
five sauce packets but use a different
random linear combination okay so it
goes on like this and and then we are
anti intermediate nodes there can be
more than one but here I show only one
you do random linear now coding but only
within the same batch okay all right so
this picture gives a little more detail
of the operation so first you are
obtaining a degree D by sampling a
certain degree distribution sigh and
then you pick a d distinct input
pakistan de lis okay as i said and then
you generate a batch of em coded packets
using the d packets so m is the size a
capital m is the size of a of a batch
okay so uh so these are here we form the
benches x1 x2 x3 x4 so on and so forth
hear hear the details so X I is the ice
patch and so the degree and then we are
we used for the batch excise you could
equal to di okay and the pack is bi 1 bi
2 up to be idir the pack is involved in
forming the ifetch okay and then you
multiply this by to a generator matrix
GI and then you form this I fetch equal
to bi which is this row back to x GI
okay
a question Sam I always a savior this
kept our am oh great alternative and
magnetic thank you for the ratio
constant it's always like our time steal
our to offer times says when you shoes
the degree do I say that match sighs
constant multiplied actually it is a
good idea to keep the better Sizemore's
we are going to see okay so we we form
these patches and okay everybody I can
go back to this picture and try to
explain a little bit more so this coding
scheme actually can be understood as an
outer coat okay here which which is kind
of like a matrix generalization of a
fountain coat and then there's an inner
code here and in a code are the randomly
ninel coating applied within the network
within each patch you don't cross do
cross patch randomly in a neural coding
okay so you form these patches and and
their sends through the network which
can do arbitrary linear neural coding it
doesn't matter as long as things a
linear and then you get patches out as
y1 y2 y3 so and so forth where y is
equal to x I'd input patch into the
network x h I where is that the transfer
matrix that it goes through within the
network okay now because we we don't do
cross match leaning our coding why I
only depends on X I but not other
batches okay that's why you that's how
you are you keep the structure of outer
coat intent even though if you do
randomly nano-coating within the within
the network ok so the end to end effect
is is the following on the on the top we
have these input packets and on the
bottom we have some channels are
characterized by the matrices GIH hi ok
so in the if you're familiar with belief
propagation final code what they do is
that you find a check node with degree 1
and then you are you start propagating
away now in this case we
instead of doing the same thing we are
we look for a check node I with degree i
equal to the rang of gix eh I for
example for this check no here supposed
to what degree is equal to suppose the
rang is also equal to 2 then you can de
you can decode b1 b3 and you can start
propagating ok so the I want
specifically the the linear equation
associated with the check know is why I
is equal to bi which is the VI is the
director of all the packets that involve
involved in in batch I and you x GI x
extra idea is the generator matrix and h
is the transfer matrix in the network ok
so you can also apply the technique of
precoding as in the raptor codes a so
the idea is that you are you pick up I
fixed rate erasure correction code so
you're so these are the sauce packets
you expand it into a a larger number of
number of packets pine erase your code
and when you do the belief propagation
here upon ruiz able to decode a a
fraction 1 minus e2 of all all these
packets you would be able to recover the
original source package by means of the
racial code so this actually can give
you a higher rate and also also lower
complexity ok so we need a degree
distribution size such that the PD
propagation can decode successfully with
high probability the encoding decoding
complexity is low end and and the
cooling rate is high so i'm not going to
get into the details of this asymptotic
optimization program oh and Leo I want
to say is that our this optimization
actually has to do with the expected has
to do with it rang
distribution of the transfer of
transformation matrix actually it does
not depend on the details of this
transfer matrix X ray does depend on the
rank distribution okay and then the one
can do some optimization accordingly and
this is the the complexity of our
sequential scheduling of all these
patches now the so you may ask so are
just a moment ago we said that
sequential scheduling is not efficient
because it's not scalable scalable for
for model casting then why do we use a
sequential scheduling here now the
reason why sequential scheduling was not
efficient for chunk based rendering in
E&amp;amp;M coding is because you you need to
that there is a receiver you need to
receive every chunk okay yeah I if you
cannot receive it you have to wait until
you receive it before you can move on to
the next channel but for for Best Co
because it has a kind of like a matrix
fund request an outer coat you actually
don't have to receive all the branches
you only have to receive a sufficient
number of matches and then you can start
the belief propagation decoding it so so
the result is that the the source no
encoding complexity is o of TM okay m is
the batch size ok and then so these two
are constants it doesn't depend on k
wait destination of decoding Oh of M
squared plus TM again it doesn't depend
on k okay so no matter how large your
your fire is the encoding decoding
complexity complexity remains constant
okay now for the intermediate note the
fall for this particular configuration
okay which I will elaborate a little bit
okay this particular configuration is
such that the from the source to all
these signals it forms it has a tree
structure so that our packets cannot
overtake each other I'm going to
elaborate further why this is important
okay
the buffer size only needs to be all of
TM okay again is independent of odd file
size and for the name according
operation it the complexities of TM per
packet it so yes last time when we were
getting complexity numbers the reason
you had k was because it was the
complexity for the
becoming the entire file oh actually i i
i i clarify with my post last night in
fact the the sly you saw last time it
had it had a que in it it is for the
total complexity okay but now I I talk
about a complexity per packet yeah so
this total file okay yes of course yeah
yeah there are off of to decode a lot of
our you need to work harder but but for
practice you it is a nice football per
packet you don't have to work harder
okay so so T you pretty much can forget
about is just the just a link to a
packet that doesn't change so k is the
number of packets that you that that
depends on the file size and m is there
is a parameter to choose we is a branch
sighs okay so the one thing i would like
to mention is that the here here the
optimal value of theta Risa is is almost
the same thing as the rate of the code
which is very close to expectation of
the rank of age okay so this is when you
have a1 have only one hot then the the
rank of a is just as corresponds to the
eurasia probability okay when you're
multiple hops and this is a what you
have to look at okay it can be proved
that when expectation ok ok this is the
optimum value R theta it can be proved
to be exactly equal to expectation of
rang of K when expectation of rain of k
is equal to M which is the best size
times probably what rang of k equal rank
rank of h is equal to m okay okay so
let's go back to this example with
packet loss is equal to point to so here
we apply you know a bad score at node
asked which encodes PK packets and no
new actually only needs to catch one
batch the reason is that the from s to T
there's only one path and so the Packers
cannot overtake each other so for at
note you you only need to install one
batch as long as you see you know pekus
from a new batch coming engine you know
that it all batches over and then you
can just throw away on everything wait
and the and no t only needs to send one
feedback after a successful decoding ok
so here are some parameters that we
obtained by simulation and here i want
to look at this we have not applied the
the precoding techniques yet way if you
apply the precoding technics then the
number with low key
better so here here k is the file size
okay sixteen hundred thousand 32,000
64,000 and Q is the size of the finite
view in for Q is equal to 2 this is
binary this is a good it cast for to
school for is a very small point of view
and here we choose patch size equal to
32 okay and as we see on the lower right
corner okay the array can already exceed
point 64 which is the theoretical upper
bound on the rate of a final coat which
actually cannot be achieved with a with
a small fan of view for 444 final in any
case so things looks are pretty
encouraging okay and in fact from the
theoretical point of view what we have
obtained is actually a framework which
are on the one extreme and compares this
a raptor coast or the final chord that
family okay when m is equal to one when
the batch size is equal to one okay then
Vasco degenerates 22 following codes and
which has low complexity but it doesn't
enjoy the benefit of network coding on
the other extreme when you take m equals
k which is the whole file and degree is
also equal okay that is you you take
randomly a linear combination of all the
packets in the in the source file then
when basco becomes a full-fledged
randomly nail enamel coating and which
has high complexity but at the same time
you know it enjoys the full benefit of
no coding and somewhere between we are
trying to are we trying to choose some
parameters such that it is it performs
well but and at the same time you know
the efficiency our complexity is low
that's what we are trying to do okay so
let's talk about some recent
developments so the one thing i would
like to mention is that this one yes yes
yeah ok slip back
so this discussion is about I mean
prospective code is showing the
performance is about one hour too long
yeah right so if I have multiple oh how
the code oh yeah the more the more hops
you are you have that the better it is
because I if if you have packet loss
then you lose pack it along the way I
mean that you do if you don't do
anything in between it then you keep
losing packing things of you see the
performance batteries come compare with
the router compared with a router are
you I'm also being compared with Rendon
linear code you cannot beat and in terms
of already cannot beat a random in-n-out
putting up there I mean of course yeah
how much is a performance gap to the ren
de lÃ­nea code changes where you have
multiple ha ah ok so I mean foil for
your best okay that's something will you
be are we are in the process of
investigating okay so we we need to do
much more simulation to see how it
actually works in a real and real
environment
like point six eight it was was it we
should be like point eight let's go it
should be close the point yeah because
uh what at what point eight is actually
let's say okay develop a network then
normally only the capacity is equal to
one but because of ten percent drop here
you got point nine this maybe you get
point eight and this is point eight five
and blah blah blah so in principle if
you applied randomly in enamel coating
you would be able to achieve their min
cut off this graph so the so the
advantage of such a coding scheme is
that you prevent the packet loss to
accumulate and also at the same time you
try to are you prevent delay from
accumulating yeah exactly prevention I
mean simply because if it's too hot and
there's already a drop of performance
from point eight 2.68 something like
that therefore 64 after all yours I feel
this performance in league together in
fact we we just got a funding from the
government to a proper to build a
prototype are using bats go on apply to
p2p networks because it's the same thing
and also that's the best co can also
handle the situation when you have some
intermediate nodes which are just help a
note they are just there to help they
they don't want to decode a whole file
it okay so um well one thing I would
like to mention is that for font encode
the the asymptotic optimal degree
distribution actually does not depend on
the on the erasure probability which is
something good okay so you don't need to
know the channel condition before you
decide on the degree distribution having
said that the the actual final code
being used is the Asha deviates from the
theoretical asymptotically optimal
degree distribution I think the
for the final code that gets into the
standard like raptor q they actually
obtain the is a recommended degree
distribution that is obtained by very
extensive simulation in different
situations okay so so even even though
the the theoretical the optimal degree
distribution doesn't depend on the on
the eurasia probability of the channel
they don't use that one in practice okay
so this may or may not be a very big
issue we are trying to see so we
although the the theoretical optimal is
a degree distribution does depend on the
rank distribution of the transfer matrix
we see that the dependence may not be
dead severe so we are trying to come
with some robust degree distribution you
know for different rank distribution and
see you see how you know things work out
and we also you know conducting some
finite length analysis is being done by
one of my undergraduate project students
and we are building testing systems on
multihop wireless transmission and also
as I mentioned in for PDP p2p file
transfer systems okay as a summary besco
provide a digital fountain solution for
networks employing linear neural coding
and also as I mentioned the more hops
between the source door in the sink node
the larger the benefit of Best Co
compared with applying end-to-end
fountain coats and further improvement
would include a further development
would include a proof of the nearly
capacity achievement of best code and in
a more general setup and also design of
intermediate operations to maximize the
expectation of the rank of H which is a
transfer matrix and and to minimize the
debuff Assize as I mentioned if you are
if you have a tree structured such that
packers cannot overtake each other
then you only need to to buffer one
packet but in a general topology when
there are multiple perhaps connecting
the nose is not clear how how big you
need a buffer it really i think that
takes a lot of a lot more experiment
here before we can tell you know what to
say now good size of the ophtho buffer
that one it needs to maintain so that's
the end of my talk thank you very much
so if what if you had
the degree distribution that was bigger
than aa bigger than its ok so how do you
do the decoding purely enough belief
propagation legacy ah you get a D
actually I i forgot a details ah the the
degree distribution they use actually
have a upper limit I don't remember
exactly how it is being chosen ok I have
to I have to co-parent look at it eat it
it seems like you presented the decoding
yeah I know what you mean i think i
think probably they were set ah the
support is from from one to something
which is smaller than Emma yes that that
would be that's where you push coral a
strong correlation between it and the
degree yeah that's that's a very good
question yeah I have to locate and look
packages into the details beforehand I
can answer the question
just kind of follow up on this question
so you only find a local including me in
the back never tried to put all the
equations through high five together and
try to rob tossing them bitches in fire
exactly sir okay so glad you must be
something here right um let's see you
yeah you lose something meaning in the
in the extreme case when you when you
have M equals quickie and great hills
can you basically falls back to to to
full flash randomly in an hour
it seems like think in the mia case you
essentially have like a cool a rich man
so we need the batch we try to match the
codes and propagate those things back
yeah as phil was asking if you cannot
include a bad just because I'm rent
please some other ranks
so if you restrict your decoding there
they are ah what if you stop propagating
this are there also you know we are
looking into the literature of our final
code they have their many techniques
that that can be employed here powerful
movement for moving forward so we're
looking into that period seems like it
could be very fast decoding for most of
the stuff right and then if you still
have any ambiguity left over and you
could do eek out the last remaining here
we are dezeen pumpkin codes they have
the problem if they have cycles
possession okay so you don't have to be
one knows the economical but then you
are you having official
here
when I have two questions why Thomas
around Syria fun design implementation
so I mean so this for this network which
is a two-hot network with some packets
and the example you show is that with a
fixed loss friend mm-hmm now let's
assume the loss rate fluctuates across
time by understanding your foods
basically seem familiar CRE basically
base and internal performance it should
be the basically revert to the original
basically network simply because if you
use network coding you can have infinite
amount of memory right how are the
memory doesn't have to be large enough
our size basically you need a larger
memory to air it out losses I actually
did not not not quite so far for phone
calls you don't have to it does not
really depends on the statistical model
of the channel ok let you think about
you think about phonica ok if I have a
black out and then you don't receive a
package during that period of time if
you resume then you just just
apprehended nothing has happened and you
don't even have to know that something's
happened
my point is seems to me that you will I
mean basic is the rate fluctuates quite
a lot I mean the end if your memory is
not large enough you may not be able to
take advantage of the higher rate period
ah let me see me so I mean very lazy I
think I think the issue that you brought
up with becomes significant if the
Packers can overtake each other oh I
mean obviously current fluctuation oh
like you a successful is you know it
shuts off for a lot the packet loss goes
to one hundred percent for some period
that's a pretty big fluctuation also
like this I mean further unfolding
within the beginning the first pipe has
full capacity no loss for half of the
time and there then for the later half
of time to process easier we mean the
second pipe has zero capacity for first
half in 100 capacity for the later half
the mall to achieve the full capacity
you need the basically memory equal to
the whole files to be able to dump all
the waters in
if you do badges then you will not be
able to take basically advantage of the
capacitive fluctuation work actually the
it is not sensitive to the fluctuation
in the sense that all it matters is the
number of patches that can arrive
because you think you think the thing up
of this this this yeah I mean yeah yeah
and you think you think about it think
about this in terms of the encoding
graph if you a peck cannot arrive then
you just delete that from the froth and
then because this random it actually
looks like like it looks the same
everywhere my second question is related
to the basically implementations what if
I implement basically I do the county
separately so it's this I mean the badge
code innocence have two stages right the
first stage is basically says
intermediate node some of the
informations will arrive with respect
right and then you have a second stage
which is this a recipe is coated with
English right now let's say we are
operating on a packet basis and so the
overhead which packet is relatively
so I mean can I simply basically I mean
during the second stage why try to
decode what the first stage it's okay so
you mean the intermediate nodes find a
decode a lot of the internals try to
base a leak okay let's go back to this
picture never can be reference
okay so uh at what stage are you trying
solid aunty is this I'm sorry this is
not the right picture
that's indeed a two-stage ground so it's
like this so the first stage for example
p immediate note basically the call the
information came in here yeah here say
so no please three uh yeah is that okay
for this case of this arrives at the
intermediate this arrives this is the
president there's another arrived yes so
i basically basically put that patterns
into the message put that hat into the
message icic okay so let's say I you the
final back to assess okay okay oh ok I
see so yeah okay so that this is fixed
at the size of this is fixed and that
year you don't need to tell okay you
tell the downstream that the third guy
actually had not arrived and I'm only
taking random linear combination on the
first two yes okay so uh yeah and you're
trying to see whether this can help well
I mean where this skin is sinker whether
this scheme is simpler I but how you're
going to make use of that information um
let's say this can be transmitted over
okay no but mommy I remember what about
but in what way is this information
useful to those those dancers the
downstream only need to know how many
varied packet is in that branch okay and
ur simply design a basically code which
is equal to the size of
basically equal to they'll be able to
decode that branch so for example the
badge here is sighs okay okay now other
water I don't know that's a good time
for example okay then I mean after the
first stage in the ball packet arrives
is anywhere between 12 and right right
okay I only need to basically get the
information of what packet is arrived in
the same the end effector send to the
basically receivers the receiver
basically have a rate lyst code to cover
exactly the number of packet received
during the intermediate node and the
weathered received basically says okay
so here of course the receiver is
innocence act to the intermediate Theory
basically on this caper okay I think I
think about it more carefully and see
whether we can take advantage of this I
think thanks thanks for you
so let go back to that question I use
castanet why she's constantly because a
scientist in this case of our example
you you need to store in packets yeah I
think that's only reason you Celine
intermediate nodes bother right if you
said n equal to D then kind of way well
and what the it really depends on the
application for example in the in the
current generation of PDP system where
there's no helper know so everybody
wants to have to say I hope our anyway
so puffing is not an issue so yeah for
for some for some repetition it doesn't
matter but this is meaningful for
helping us or something like a router
you don't want it don't run the the
toughest eyes of the router to grow with
the wicked firing I kissed it if you
you've already got d an upper limit on
TV hmm and you already pick empty ah but
this upper limit then you can M tends to
be paper
you know so you could get to be bigger
than the upper limit on d but maybe if d
you picked a d that's smaller than that
upper limit then they need to just use a
smaller batch what the knot the size of
em cannot be too small otherwise you
cannot get a benefit of randomly enough
yeah but this well actually what what we
find a little surprising is that M can
be chosen to be quite small and yet the
result list is already exceeds pharma
company
Thanks and I thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>