<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Approximating Integer Programming Problems by Partial Resampling | Coder Coacher - Coaching Coders</title><meta content="Approximating Integer Programming Problems by Partial Resampling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Approximating Integer Programming Problems by Partial Resampling</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s7XPkEcaRDc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged public showing internet or
public display is forbidden and may
violate copyright law
welcome everyone it's a pleasure to have
David Harris from Marilyn today is going
to tell us about a new randomized
rounding algorithm based on Louis local
Emma hi this is a work with a meaner
arvind stuyvesant about a patch over
sampling so we're going to consider two
types of integer programs of there is
talk so the simplest one to describe and
I'll spend most of the time onyx it's
just a little bit easier to describe it
is the cover integer covering problem so
you have n integer variables and you
have some linear constraints on them so
some covering constraints so all the
coefficients are positive and you have a
positive right hand side and you can
just scale all these constraints of the
coefficients are all on the range 0 to 1
and you want to solve these constraints
and you want to minimize some linear
objective function CX so that you can
kind of think of this as a weighted I'd
generalization of set cover and so in a
set cover instance you're given a
collection of sets and you want to find
a subset of them that that covers the
entire space so you can kind of think of
this as each element and Nero grounds
that gives you a cover and constraint
but the coefficients are all just 0 or 1
and in the set cover instance you want
to find the small set cover so your
projector function is just the sum with
coefficient 1 so the energy recovering
problem you can have other objective
functions of other weights in your
abductor functions and other weights in
your constraints yeah oh ok I just
didn't work so another integer
programming problem I want to consider
is the assignment packing problem so you
have variables x1 through xn but these
are just kind of categorical variables
so they can take on some values in some
set gay one through JN or just just a
set of integers for example and and you
have the constraints that are they're
all there are linear constraints but the
they have the coefficient a ki jai but
they also have a term which is an
indicator variable for whether variable
variable i takes on value J so this is
just the Iverson notation it's one if
variable I takes on value J and 0
otherwise so these are all the concerns
that are all packing constraints you
also have the additional constraint that
ever variable has to take at least it
has to take one value out of some range
and you want to find just some values
for the variables would satisfy all the
constraints there's not necessarily an
objective function here you just want to
find a feasible solution so it's a
packing your packing constraints but you
also have assignment constraints because
you need to assign every variable one
value from a set so you can't solve
these problems exactly you can you can
approximate them and there are a lot of
different ways you can talk about
approximating these types of integer
programs and we'll talk about mostly
about the integer covering problem
because it's a lot simpler to describe
so one scheme is you you will need to
satisfy all the constraints exactly but
you want to minimize the objective
function you want to get the objective
function as close to the optimal one is
possible you know you there are other
types of approximations where you might
only you know approx satisfy the
covering constraints and so on but we'll
just talk about this variant where you
satisfy the cover and concerns exactly
and you satisfy the objective function
approximately so one type of
approximation algorithm is based on the
LP relaxation followed by randomized
rounding paradigm so the first step is
you replace every constraint that the
variables have to be integers with the
constraint that they have to be real
numbers and if you do that the
you get a linear program which you can
solve exactly and you can get a
fractional solution and the fractional
solution has value less than the optimal
one so the next step is you want to find
an integer value for the variables
that's cool that makes it close to the
fractional value for the objective
function and we're actually do something
a little bit more general we're going to
create a random process with the
property that for any individual
variable the expected value of x I the
integer value X I is it most some
parameter beta times the fractional
value X hat I and this will be true for
each variable individually in this case
it's automatically true that the
expected objective function expected
value of the objective function is the
most beta times the optimal one so you
you automatically get a beta
approximation algorithm but in fact it's
kind of an oblivious approximation
algorithm because when you're running
this algorithm you don't actually need
to know what the objective function is
yes yes so the the X i satisfy the
feasible bility constraints with
probability 1 and they they have this
expected value property which kind of
automatically gives you an oblivious
approximation algorithm at least an
expectation you by just repeating this
algorithm multiple times you can get
very close in actuality to the expected
value so I won't talk about that if you
just getting a good approximation ratio
an expectation will be good enough for
us so the simplest brand is browning
scheme as you just draw the variables to
be Bernoulli independently with a
probability which is slightly bigger
than X hat I and I'm going to assume
here that all the values of x had I are
very small so that you can you can
multiply them by constant fact about you
know small constant factors and you
don't have
worried about them becoming bigger than
one and and that being probabilities
anymore it turns out that that's the
hardest case to deal with and in
reducing the general case to that is
kind of cumbersome so I won't really get
into that now I'll just assume that X
hat I is small so if you do this then if
you look at any individual covering
constraint well the expected value of
the variables X I is at least equal to
alpha times AK because the fractional
solution is equal to AK so you have a
sum of independent 01 random variables
with mean alpha AK and you want to know
what's the probability that it is
actually at least equal to AK in in a
quality not just an expectation greater
than equal to a que ya and you can use
the standard Chernoff bound and if you
do this and you set the parameter alpha
to be about 1 plus log M over the
minimum value of a of the right hand
side plus the square root term you have
to remember that either a min or m could
be big so it's possible that a min is
going to infinity so in that case in the
case that a min is very big and m is
small then the square root term becomes
the dominant one and you get close to a
approximation factor 1 so if you set
alpha did this value then all the
constraints are satisfied with high
probability and you can show the
expected value of the excise given that
the constraints are satisfied is still
close to alpha so you get this same kind
of approximation ratio 1 plus this term
which is like log of M over a min plus
it's square root of that same value so
that's the standard kernof bound
standard randomized running so one
problem with this type of approximation
algorithm is that the dependence of the
approximation ratio depends on the
overall number of constraints which is
kind of
I can go to an fid knee as the problem
size becomes big so we you often like as
a scale-free approximation ratio one
which does not depend on the overall
size of the system but kind of only on
its structural property and one very
common way of getting this in this
context is in terms of the system is
column sparse that is every variable
appears in relatively few constraints so
there are two ways you could measure how
sparse how column sparse the system is
two common ways are in terms of the l0
or l1 norms of the columns so the l0
norm is just the number of nonzero
entries in every column and the l1 norm
is just the sum of the coefficients in a
column and remember we've scaled all the
entries to the coefficients are in the
range 0 to 1 so the l1 norm is always
smaller than the l0 norm and it's
possible that you could have systems in
which the l1 norm is much smaller and
these are both much smaller than em so
can you get an approximation ratio which
is a function of these columns varsity
measures not the overall system size so
it was previous work by sreenivasan
which gave an approximation algorithm
and it was based on a random process was
analyzed using the fkg inequality I
won't get into a lot of detail with it
here but it gives you an approximation
ratio that has this form sorry there's a
error on the slide there should be an
extra term here there should be an extra
term but when it for the square root
from there should be an extra term log
of a min over a minute I left that off
the slide you could in this
approximation ratio and the the work of
sort of Awesome was not based on the
lovas local em up but the lovas local
emma is another a very standard
technique for getting these kind of
scale-free approximation ratios and you
could use that tool to get a similar
approximation ratio although that was
not the approach taken based on vossen
so let's review the basic form of the
low-pass local Emma and how it would
apply to this problem first so in the
lobos local Emma you have bad events in
some probability space and in our
contacts a bad event would be that one
of our covering constraints is violated
and these these bad events involve a
subset of the variables so in this case
the variables are the integer variables
you're drawing which are Bernoulli pie
and you have this a separate bad event
for every covering it straight namely
the sum of the variables is less than a
sub K which it's supposed to be and the
the key property and understanding the
local M is is to decide whether bad
events affect each other and in the case
of the local lemma this would be bad
events affect each other if they overlap
on a variable if if there's a common
variable that affects both of them so
one thing you have to be careful of in
the local llama context is that there's
a very binary classification of whether
a variable affects a bad event or not
that is if the bad event is a function
of that variable then that variable
affects that bad event even if the
effect the if it's hardly ever affects
it even if the amount of the effect is
very small the local Emma just says does
this variable effect that bad event and
you could imagine a system where all the
coefficients are nonzero but are all
tiny and in that case every variable is
affecting every constraint and so
everything overlaps with everything else
so this is why you if you use the local
lemma you'll always get an approximation
ratio that's faith as it's phrased in
terms of the l0 norm of the of a column
the number of nonzero entries because an
entry which is very small but nonzero is
the point of view of the local lemma
affects a constraint just as much as if
the coefficient were big even though if
the coefficient is very small you might
think that heuristic we shouldn't really
matter for that constraint so you'll get
that's why you get these Delta 0 terms
in the approximation ratio if you use
the local Emma all right so the local
Emma by itself is not a constructive it
only shows a very small probability that
you satisfy all the constraints so
that's not an algorithm outright you can
turn into an algorithm using the
framework of Mozart our dish which turns
almost all the applications of the local
Emma into constructive algorithms and
you could use it for this this problem
just like you could use it for
everything else it would basically work
like this you you begin by drawing all
your variables from the original
distribution bernoulli p and if you find
some cover and constraint is violated
that is the the sum the sum of the
variables is less than the right hand
side dates of k then for every nonzero
coefficient a sub ki you draw X I from
its original distribution again if a sub
K I is 0 then you don't kink its value
you just leave it alone and if you set
alpha to be that same value for the
approximation ratio of this algorithm
converges so I just want to talk up just
heuristic alee why this algorithm even
though it is kind of the generic way of
transforming the local Emma it doesn't
really make sense for this problem so
suppose you come to a violated
constraint some some covering constraint
k is violated well if x 0 x is equal to
1 then the algorithm says you should
still you still might need to resemble
that variable if the coefficient is
nonzero but why I mean if X sub I is 1
then then that variable is kind of
helping that constraint be satisfied so
you're kind of make going in the
opposite direction of the progress if
you're resampling it and maybe setting
it to 0 your kind of messing that
variable up that variable is helping you
you shouldn't change it and if x 0 x is
0 then probably XII is not really at
fault for violating that constraint it
probably didn't have a very big effect
on that constraint I mean most of the
variables would
may be expected to be equal to 0 anyway
so that variable is probably not causing
that constraint to be violated you could
think that the guilty variables the ones
that are causing the constraint are the
difference between the actual number of
0 variables and the expected number
that's why the constraint is violated is
you had fewer variables being one than
you expected so only about square root
of them are kind of the difference
between what you what the mean is and
what you expect to happen in a in a
deviation so you should really be really
resampling about maybe square root ASAP
a of the variables not all a sub K of
them so the most erotic algorithm is
really resembling wait too many
variables for constraint so instead of
resampling all variables will use
partial resembling and this is actually
a very general framework which extends
the local Emma in in a very general way
you can apply to many problems involving
Latin transversals packet routing etc I
just want to describe how this applies
to the integer covering problems integer
programming problems where I don't
really need to get into the full
generality of the framework so for this
particular application here is how you
would apply partial or sampling so again
you draw x1 through xn through from the
original distribution and now if you
come to some constraint that K that's
that's violated you do this if X I is
equal to 1 then you just leave it alone
it's helping you so don't mess with it
if X I is equal to 0 then you resample
it but you don't draw it with the
original probability P I you draw out
with a smaller probability this
probability is a depends linearly on the
coefficient a sub K I and it also is
multiplied by another scaling parameter
Sigma so you can see that if the
coefficient is 0 you never resample it
just like in the local Emma but this is
kind of smoothly interpolating between a
0 coefficient and a coefficient of 1 and
also you can see that the values of x I
are always increasing over time
so you you never change a 120 you only
change 0 to 1 so this algorithm
obviously terminates so the only
question is what's the expected value of
x I at the end of this process because
it because it certainly will will
satisfy all the coupling constraints and
we will show that this satisfies this
type of approximation expect a proximate
in ratio with this expected value where
the expected the probability that
variable is equal to 1 is a small
multiple of the fractional value which
will automatically give us our good
approximation ratio so we're going to
analyze this algorithm in a kind of
strange way so if we come to a
constraint k which is violated the
algorithm says you resample exile with
probability Sigma times a sub ki times P
I so instead of thinking of it is
drawing X I as a Bernoulli random
variable with this probability you think
of it as a two-step process you have a
set of variables Y and each variable I
goes into the set Y with probability
Sigma times a sub ki and then you look
at all the variables and why and you
draw them as new Bernoulli variables
with probability P sub I so this is
obviously equivalent this two-step
process but this kind of two-step wave
thing the things ought to be very
important to analyzing the algorithm
even though it's kind of weird that
you're breaking apart for no no really
good reason so our goal is to bound get
an upper bound on the probability the x
I is equal to 1 and in order to do
though so we're going to construct a
kind of a witness which explains why you
set x 2 x equal to 1 this witness is
going to be a just a structure which is
kind of a the explanation for that
variable then you're going to take a
Union bound over all possible witnesses
and then the expected value of x I is it
most the sum over all these witnesses of
the probability of seeing that
particular witness so this is the same
proof strategy for the original Moser
and tardive algorithm but our witnesses
will be much simpler than theirs
before I talk about the witnesses for
this algorithm I'm going to try to
motivate this approach by talking about
how you just do witnesses for a standard
Chernoff bound not any kind of algorithm
a resampling algorithm just a turn-off
bounds for the lower tail so suppose you
have n independent Bernoulli P variables
and mu as they're mean and you want to
bound the lower tail the probability the
sum of these is less than T or T is
something that's smaller than the mean
so consider the following process if the
sum of the variables Z is less than T
you mark a subset of the variables you
mark them how if zi is equal to 0 then
it gets marked independently with
probability Sigma otherwise you do not
mark it well since you only have any
mark variables at all if some of the
variables is less than equal to t you
have this obvious equality the
probability that's less than equal to t
is the sum over all possible subsets of
2n that v is the marked set of variables
and v you can think of it as a witness
the sum was too small now suppose you
consider some fixed v subset of
unendurable 'he's the following are
necessary conditions for v to be the
mark set well first any variable inside
v had to have z is i equal to 0 and that
has probability 1 minus p to the size of
v any I and V had to be marked well that
has probability Sigma to the v the third
condition is that any integer which is
not marked but still had zi is equal to
0 must have any variable which is not
envy but it was equal to 0 must have
been unmarked otherwise you would have
put into v the last term well you take
the product of 1 minus Sigma over all
variables which we r equal to 0 but not
envy but the key point here is there
have to be at least n minus t minus the
size of v of them in order to mark
anything otherwise you would not have
had the sum of Zi less than equal to T
so this last term is it most equal to 1
minus Sigma
to the power of n minus t minus the size
of V so the overall probability that V
was the dumb mark set is it most the
product of those three terms and if you
sum over all V you get the probability
that the sum of Z is less than T is at
most that expression there so that's
true that that bound is a valid bound
for any Sigma in the range 0 to 1 so we
can optimize it and when we do and do
some further calculus we get the
following expression which is the
classical turn off bound so you've kind
of given a witness based proof for the
standard turn off lowered hellbound so
if you really keep this example in mind
as we talk about other witnesses which
are more complex so we need to give
about a kind of a witness bound not just
for the event that the initial values
for these variables failed the to
satisfy some constraint but the values
of the some variables after multiple
rounds of resembling failed to satisfy
some covering those constraints there's
going to be a more complicated witness
the same intuition will apply so for any
constraint you can list all the result
sets for those constraints for that
constraint remember we have this
two-step process first we choose a set Y
and then for every variable and Y we
draw the variables from with probability
P sub I so y sub K 1 Y sub K 2 are the
rest and sets the the first set you draw
in this two-step process so there are
two ways you could have had some
variable X i equal to 1 well first of
all in the very first step of the
algorithm you could have drawn x 0 x is
equal to one in that case your witness
structure will just be the empty empty
list will just be the null structure the
second way you could have it is during
the L for a sampling of some constraint
k you set X I is equal to 1 for the
first time and in that case the witness
will be the list of sets y sub K 1
through Y sub K L and you necessarily
have to have variable I inside why
KL because otherwise the variable X I
won't change during letra sampling so
the witness will be the list those list
of sets um yes sorry kids the constraint
what is the 120 these are the canoe
might need to do multiple rounds of
resampling in order to fix a constraint
yes then so for simplicity in order to
explain this this argument let's just
say L is equal to 1 so we're just
dealing with the simplest type of
structure which is a single deck for
some constraint k so you have a
potential witness which is just the set
out you so you're talking about so so
fix them set Z sub K 1 what is the
probability that the actual witness that
you generate for this variable is equal
to this fixed value Z sub K 1 so y sub K
1 is a random variable Z sub K 1 is just
some fixed value which is just a set a
subset of the variables so the following
events are necessary in order to have Y
sub k 1 equal this fixed set Z after the
first / sampling of that constraint k
you must set x of i equal to 1 that's
what just what we said that on that that
was the definition of the the length of
the list of the witness is the time when
variable X I've got equal to 1 for any
variable in that set you must have set X
sub J equal to 0 initially during the
initial ring of the variables y if x sub
k is equal to 1 initially that it will
never be resampled in particular it
cannot be resampled during the first
four sampling of constraint k third that
set Z was chosen as the first Reese
ample set for that constraint k those
are all necessary conditions in order to
have this particular witness structure
so just writing those conditions again
the first event has probability P sub I
because
at every state every time you fix a
resembled set the variables are drawn
independently from their distribution
pi/4 any variable inside Z the second
event has probability 1 minus p j and
those are all independent because
they're all based on the initial
sampling and for the third event well if
you consider the current value of the
variables at the time you were sampled
that constraint so not their initial
value but whatever value they had just
guessed at the time you were about to
resemble them again if a variable had
value 0 then it can't then it goes into
the set Z with probability Sigma a sub
KJ if VJ is equal to 1 then it cannot go
and is that into Z again letting v1
through V and note the current value of
the variables just at the time you were
sampled k so the probability that you
set y 1 equal to Z is this product where
it's in terms of the current value of
the variables that for any variable
which is equal to 0 um it goes into Z
with probability sigma x akj and if it's
equal to 1 you don't it definitely does
not go into Z and again you make the key
observation that the constraint k is not
currently being satisfied in order to
have it being more sampled so the sum of
AK JV j is less than a k at the time it
was resembled and if you plug that into
this expression you see that this term
there can be upper bounded by 1 minus
Sigma to the minus AK so if you put this
all together you see that the total
probability even have been countering
this witness structure is is it most
this probability which you see and if
you some now overall value so this is
just for one particular type of witness
structure the simplest type that has
just a single set in it so now if you
sum over all possible witness structures
including allowing k and l
the very you can get this expression
here and you see that you have the sum
of a sub ki so you have the bound you
can put in is the l1 norm of that column
and the last line you just have to
choose Sigma and alpha carefully in
order to you know you basically optimize
them in order to minimize that
expression which is kind of involved but
it's nothing to too interesting so you
get this dependence on Sigma 1 instead
of Sigma 0 because you actually have the
sum of the Aki and in fact the
coefficient there is one so it's not
constant it's not one plus constant
times that term it's actually one plus
that term that constant for that that
first term is actually equal to one so
you get this this bound on the expected
value of any variable and that
automatically gives you that same
approximation ratio so we've talked
about before so let's see if we can get
any lower bounds on this approximation
ratio how close is this approximation
ratio to optimal well well for one thing
when when the minimum value a min goes
to infinity the nura approximation ratio
basically is something of the order of 1
plus 0 square root of the natural log of
Delta 1 plus 1 over a min so how close
is that it is that top them well that's
one half of the asymptotics how the
other half is what happens when Delta is
very large in that case you get an
approximation ratio which is basically 1
minus some little o of one term times
the natural log of Delta 1 plus 1 over
amen so that actually is is is optimal
if you reduce to set cover so the
hardness of approximating set cover
shows you that that at least the first
order term the first order term is
optimal including the optimal constant
but that kind of hardness ratio is
really vacuous when a man goes to
infinity so the hardness of set cover
gives you nothing when a min is large
because that's you're saying the
hardness is some value less than 1 which
is stupid i mean the approximation ratio
can't be better than one so previously
there were no nontrivial bounds that
were actually known in that regime when
a man is much larger than delta yes yes
so here's actually constructing an
integral T gap for the case when a man
is as large so you can kind of you can
consider the following in it you're
covering system so for all you think of
I and K that I is the set of variables
and K is a set of constraints as vectors
over GF 2 to the N and you can consider
a system which is defined by the sum
over all I us which are perpendicular to
K that could be that such that K is
equal to 0 I left that off my slide of
those X I is at least equal to a so you
have to to the end constraints in 2 to
the n variables and the objective
function is just the symmetric one where
you just summing over all X I so this is
a very simple fractional solution where
you just set X i equal to a over 2 to
the N minus 1 and there's a previous
analysis by vazirani which which was
really only targeted for the case when a
is equal to 1 just the simplest case
which showed that any integral solution
has to satisfy that sum of X sub I has
to be at least equal to n and this kind
of gives you an integral T gap on these
types of energy recovering systems but
this analysis I don't know sorry
but again this this analysis is not
helpful when a is large because the
integrality gap that claims is not even
bigger than one so one result we have is
that any integral solution actually has
to satisfy a stronger condition which is
it has to be at least equal to 2a plus
Omega of n and with this basically
amounts to showing is that any sparse
boolean function has to have a large
Fourier coefficient for our coefficient
meaning / GF 2 to the N and this shows
you that this covering system hatchery
has an integral T gap of 1 plus
something on the order of log of Delta 1
plus 1 over a min so our approximation
algorithm kind of is almost optimal it
has a square root instead of a linear
dependence on that term so it's it's
kind of close but there's kind of an
interesting polynomial gap there this is
but this is the first non-trivial bound
at all for what happens in the regime
when a min becomes large another kind of
interesting if you talk about what this
algorithm is what happens if you have
multiple objective functions so you
might have some method of a balancing
them so let's say you want to yo u of l
different objective functions and you
want to minimize the max of them so can
you tryn let's say you can solve the
fractional relaxation of that however
you decide to balance them if you decide
to take the max of max of the men's then
that's just can still be solved using a
linear program can you get a solution in
which all L objective functions are
simultaneously close to their fractional
values so this is one way in which other
types of algorithms for set cover and
related algorithms don't really extend
if you have a group the other main
algorithm per set cover is a greedy
algorithm where you always choose
the set which kind of increases your
coverage the most in a single time step
but it's not even really clear how you
define a greedy criterion if you have
multiple objectives I mean you kind of
need to boil all the objectives down
into a single number in order to make a
decision about what variable to accept
and there's not any obvious way to do
that so the greedy algorithm has a real
hard time even getting started for these
type of multiple objective problems the
LP based solutions can handle this much
more cleanly so we can show is that not
only is the expected value of sia of CX
close to the fractional value but it's
actually concentrated in a very similar
way to a Chernoff bound would be
concentrated to the Chernoff bound
concentration around this value beta
times clx head so with high probability
you have that for every individual
constraint see all that X is close to
beta times see all that X hat and so
with high probability you get all of all
the objective functions are
simultaneously close to their means this
doesn't follow this from this expected
value property for the variables and the
way you do this is you show basically
abound on the correlation the bound on
the product of monomials so we
previously showed that the probability
that any individual variable is equal to
one is at most this term moroso by where
r 0 so x is equal to the probability
plus some small approximation terms so
in fact you can show that for any subset
of variables you have this bound on the
probability that they're all
simultaneously equal to 1 which is the
product of the row eyes so this is
basically the same as would be as if
they were independently drawn as
Bernoulli with probably row I and
particular this type of of a monomial
product property is enough to give you
turn off upper tail bounds
and the way you feel that is given the
set are you can build a witness for the
event that all these variables are all
equal to one not just about a witness
that any individual variable is equal to
one and the way you do that is for each
variable that was equal to one you find
out which resampling of which constraint
made it equal to one and you list all
the ramp old sets for those constraints
before that last time when that variable
got equal to one so some of these lists
might appear twice because you might
have two different variables which would
be or equal to 1 on different race amp
Ling's of the same constraint but that's
ok you just list both of them and you
can do a very similar thing where you
use some over witness structures to get
this joint probability property another
extension you can talk about is if
you're given multiplicity constraints so
in the statement of the problem I just
said that X I has to be an integer of
unbounded size but you can also consider
a version in which there are up
constraints on the up size of the
variables not just an integer of
unbounded size but some upper bound on
its on its size d sub I these are called
multiplicity constraints so these can be
easily incorporated into the linear
relaxation they're not very easy to
incorporate into a greedy algorithm but
the LP based approach can easily put
them in but can you still get a good
approximation ratio while trying to
preserve these multiplicity constraints
so if you just analyze the algorithm
straightforwardly you will see the
solution size can be much bigger than
the fractional value so I was only
talking about the case lunar to the
fractional values were all small but it
turns out the reduction from the general
case to the case when they're all small
loses a lot in the solution size
possibly so coolio posen young gave an
algorithm that you can't respect them
exactly but you can violate these
constraints compared to the optimal
solution by a 1 plus epsilon factor and
if you do so you have a term in the
approximation ratio which is basically
on the order of 1 over epsilon squared
and if you want to satisfy them exactly
you can do it but now you get an
approximation ratio which doesn't depend
on a and depends on the log of Delta
zero the so we show that if you just
modify a few parameters of our algorithm
don't make any other changes just
basically change alpha and Sigma 2 new
values then you can still get this
expected value property where now the
approximation ratio is only inflated by
a factor of 1 over epsilon and you can
see that this improves on the result of
quilt lapis in a lot of different ways
so these deltas can be Delta 1 so we get
delta 1 instead of delta 0 you've got
one over epsilon instead of 1 over
epsilon squared and so on and in fact
you can feel a hardness of on the order
of natural log of Delta 1 plus 1 over a
min times epsilon so this is essentially
optimal approximation ratio for this
case when you have these types of
multiplicity constraints all right so
now I've talked a lot about the integer
covering problem so I'll talk about how
to extend these type of analysis to the
assignment packing problem so recall the
assignment packing problem you have
variables which take on values and some
range jji and you have all these
constraints which are all sums of
indicator variables for that for these
variables and you want to find some
values about values X which nearly
satisfy the constraints so the usual the
first the first step is you can find a
fractional solution which satisfies all
the constraints if integer solution
exists and if you use a simple
application of the local lemma plus some
randomized rounding you get an
approximation ratio looks like this
where you have Delta Z rose again the
same issue that if a coefficient is
nonzero but tiny then for the local
lemma it means that that variable
affects that constraint so again you
could use the most erotic algorithm to
solve two to solve this of the local to
get an algorithmic form of
what the local lemon gives you and again
you have the same issue suppose you come
to some violated constraint so the
straightforward application the most
retarded algorithm would say for any
variable with a nonzero coefficient you
have to resemble that value that
variable but but that doesn't make any
sense you know fistic alee if the
coefficient is tiny and that durable has
almost no effect so you probably
shouldn't be resampled it and if that
bear and if that variable takes on a
value different than the bad one then
that variable is helping you so why are
you changing it so you can use a similar
type of resembling partial or sampling
if you have a very violated constraint
we should do is you should choose about
the square root of our variables for
sample are is the right hand side of
this assignment packing constraint and
you shouldn't choose the variables you
shouldn't use the set uniformly at
random among all sets of that
cardinality we should is that the the
proper the probability of choosing any
set should be proportional to the
product of the coefficients the
corresponding coefficients and once you
choose this were sampled set you draw
the variables from the original
distribution and this this avoids all
the problems you least turistica Lee of
the loser tarnish algorithm and again
why about square root of our well it's
the difference between what you expect a
bad event to look like and what the
expectation is so the expected number of
variables that are going to be true and
this constraint is about our it's about
our because that's what the expected
value expected value of the sum was and
usually if a sum of random variables is
bigger than its mean is about a standard
deviation from its mean so about only
about square root R of the variables are
kind of doing
being worse than you expect so those are
kind of only the guilty variables that
are that are really causing the problem
that's kind of the intuition between y
you C square root of R yeah but if you
just think of a typical bad constraint
it's probably typically bad because is
about squared bar I mean this is just
kind of the intuition no no if it's you
don't need to pick more depending on the
violation then the number you pick you
kind of the number you you pick is kind
of optimized for the smallest violation
so if there if it's a bigger violation
you can just kind of ignore the extra
variables so you can show that this
algorithm terminates with probability
one with a small number for samplings
and as long as you and you get a value
of x I which they don't satisfy the
original constraints exactly but they
have a small discrepancy term and and it
looks like this we're again instead of
the Delta set of the l0 norm you get the
l1 norm if you look closely you can also
see that instead of log in the second
term you get the square root of R times
log Delta one so you also are saving a
square root of our times log R term
which is relevant when r is much bigger
than Delta so it's saving that term as
well so an example application of this
is a multidimensional scheduling problem
so you have some machines and jobs and
every job has D dimensions of cost and
you assign this to some machine and you
want to minimize the total makespan not
just the makespan in one dimension like
time but the makespan overall D
dimensions so you want to minimize the
maximum the sum of all the costs to that
job but the maximum overall dimensions
so a simple algorithm is you can first
guess what the optimal makespan is and
then you can form an LP solution which
an LP which is feasible
and if some job costs more than this
makes band then then you can't do it on
that machine so you just set that
fractional variable equal to 0 otherwise
you just allowed to be some fractional
value and you can just plug this in
almost directly that fee that fractional
solution into this partial with sampling
and you get a solution which makes pan T
times this extra factor which is logged
the over log log D so you're getting a
log D over log log the approximation to
the optimal may expand this is not the
the only way to get this type of
approximation ratio maybe it's not even
the best but it is certainly really
simple once you have this assignment
packing framework you can basically just
plug this LP directly into this
assignment packing framework almost for
free another thing this type of analysis
gives you is that you can handle cases
in which you have different right hand
side values that you can get an
approximation ratio which kind of trades
off between us a multiplicative factor
and additives plus standard deviation
factor this is very difficult to do for
other approaches to these types of SM
packing problems so again you can
analyze this algorithm by building
witness trees it's very similar to the
mosser tiredish algorithm analysis but
the key idea is you only keep track of
the receivables and the values they take
on if a variable was not resembled you
just ignore it when you're building the
witness tree so if you were sampled
three different constraints and you
selected three different resampled sets
you'd build a witness tree in terms of
these just the sampled sets and you
don't say anything about s2 if it's not
in the witness tree the key ah lemma for
this is you can show a witness tree
lemma which is that if you have any
particular witness tree whose nodes are
labeled by resampled sets and the
probability of encountering
tree can be bounded in terms of this
resembling probability those times the
probability distribution on the
variables it's a similar proof to the
current problem and the key idea is that
the total number of expected of
resemblance is the most equal to the
this sum over all witness trees the
property of the tree and by only keeping
crack up for sample variables you are
greatly reducing the total number of
witness trees because most variables are
not being counted in the witness trees
so you're losing information but you're
also really reducing the total number of
witness trees are considering so this
sum is over a much smaller set by
ignoring those variables and so the sum
becomes smaller so wondering whether you
can continue to apply this type of
partial resembling methodology to other
types of integer programming problems
packing problems mix problems involving
covering and unpacking so we have bounds
in terms of the l 1 and l 0 norms but
can you also get any bounds in terms of
higher higher norms and also this kind
of interesting question is what is the
approximation ratio for the covering
integer problem when a min is becoming
large is it do you have that square root
turn there or not IIIi think the linear
term is the correct one so guess that's
all thank you
approach where you were using a turn of
concentration yes and will you have but
you know when you have the variables
that are very small then there are much
better concentration qualities to use
like first event give you better
estimates there just uh am I my
impression was that if you have you know
an infinite number of very small values
then then the purif bound is is kind of
the crackers I mean it approaches a
Poisson which is basically what the
Chernoff bound is ok I guess it doesn't
lose a local my topic hope you just
entered using the wasn't out a shock yes
yes is there a way to apply the local
Emmaus and get the same grounds
physically multiple times there either
there is a nick harvey has this approach
to assignment to a similar type of
problem and we basically kind of
quantized the coefficients you know the
coefficient between half and one you do
resampling on those and then you get
some discrepancy then you look at the
coefficients in the range you know a
quarter to a half you get another
discrepancy but the discrepancy is
smaller and so if you sum them all up
you still get some you know small value
so you can do that it's kind of
cumbersome to use this multistage
approach and i don't know how general it
is can't you know</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>