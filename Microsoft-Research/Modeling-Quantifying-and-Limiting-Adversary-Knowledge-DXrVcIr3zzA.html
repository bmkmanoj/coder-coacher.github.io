<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modeling, Quantifying, and Limiting Adversary Knowledge | Coder Coacher - Coaching Coders</title><meta content="Modeling, Quantifying, and Limiting Adversary Knowledge - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modeling, Quantifying, and Limiting Adversary Knowledge</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DXrVcIr3zzA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright thanks for coming we have an
interview talk this morning with Pierre
Taub module who's here from you Maryland
student of my kicks sort of a friend of
the lab and our group and Peter does
work on security and I and policy
programming and is kind of well aligned
with with the interest of the group and
he's here to talk about modeling
measuring and limiting adversary
knowledge theater oh okay hear me okay
alright that's me as you have heard so
this is mostly my thesis work so it's
you might notice it's maybe the sort of
the full scope of the work is a little
convoluted to fit into one one joint
piece of work as you'd expect from a
series of papers glued into one semi
coherent thesis so keep that in mind
okay some of you might have seen a web
page like this when you sign up to
Facebook and you might have thought
about what happens to the information
you provide to facebook or to other
online services and there's some of you
and myself might feel a little uneasy
about providing this information for two
reasons one of them is hidden in these
little terms and conditions when you
sign up which essentially say that
Facebook owns your data as in they can
do anything they want with the
information you give to them and it's
why would you feel uneasy having your
information in facebook and date and
having them try to exploit your
information to as to its full extent as
possible this is exemplified by this
nice quote I found if you're not paying
your the product right these online
services have incentives to use your
information to the fullest extent
possible to monetize it and there's many
examples of this happening and in the
real world the the most recent I don't
know if this is the most recent now but
uber the the car sharing or taxi sharing
service got in trouble because they were
keeping and form a
that users did not want them to keep or
did not even know that they were keeping
I'm not sure the details but users got
up in arms over this and and the second
reason you might feel uneasy is even if
information is used as intended by these
services there is still bad guys on the
internet that your information gets
exposed to if it's stored on servers
somewhere so then there's a lot of
examples of this too in popular in the
real world and I think the most recent
one most recent high-profile one is this
Sony data breach where a lot of
information about I think not just data
not Sony employees but just users of
Sony services and even whole movies got
leaked through some data breach so
either either way there is data misused
in these cases there is either
intentional intentional misuse by these
services or unintentional where some bad
guy comes in and steals their data for
potentially nefarious purposes so what
is no alternative right could you wall
yourself off from the internet and hide
under a rock somewhere well that's not a
viable alternative right because there's
first of all there's friends on the
internet there is useful social
interactions you're getting by
participating in these services and
these services by themselves offer
useful things maybe some of the I don't
know if any of you guys actually use
some of these websites there's Amazon
there's dig you know you can imagine you
actually wanting to use some of these
things you're actually getting something
out of this and by hiding yourself from
from these services you're sort of
removing the economic incentives that
make these things possible to begin with
so it's unrealistic to wall yourself off
from the bad guys because you're also
wallowing yourself off from these good
things you want so perhaps a more
plausible alternative is to separate
yourself but provide some interface for
others to query your information for
useful purposes so for example you might
want these services to ask you questions
for example this is a real so this is a
pair of
raising of a real query done by a
Facebook advertiser that wants to offer
you a cake for your birthday or a coupon
for a cake for your birthday so they ask
you is your birthday within next week
okay and at the same time if you provide
such an interface you might want to look
at questions of that reveal more
information about you like what is your
entire like demographic information you
want to say that you know that's too
much information I think this
combination I forgot zip code here but
the combination of birthday gender &amp;amp; zip
code which is the postcode United States
is enough to uniquely identify you so
that might be too much information and
other questions that you might want to
be able to usefully answer our ones that
collaboratively are computed among
multiple individuals that try to protect
themselves for example which one of us
is oldest so this might be a mite it's
hard to it's hard to justify sort of
these kinds of queries but I'm just
trying to present ones that are sort of
approachable but there actually are
real-world queries in which you want to
combine private and secret protected
information to derive useful conclusions
or now maybe I'll mention a little bit
of those later so you want to be able to
allow these kinds of queries allow these
kinds of queries but you want to have
some way of detecting a query when it's
too dangerous and you don't want to you
want to make sure you you sort of reject
a query of that sort who's asking so it
will probably be based also on who's
asking and I'll say a little bit more
about that in terms of not know naming
individuals but rather who has asked you
what before so you have to keep track of
what people have asked you before that's
it picture imply some kind of central
server you know me my pc which answers
all these questions yeah you can imagine
that is essentially something under your
control your pc well that's what you
have some more vision implementation but
that's the basic idea yes that is the
that's the basic idea though I'm not
sure whether I'm not trying to push this
idea as I su end-all solution to misuse
of in front
the internet is just one interesting
approach that you might see well how
close it is to implement this and see
how far you can get into into it what
Sinan cakes the people isn't gonna would
mean you'd have to pull everybody in the
on Facebook every week to see whether
their brows this week okay so so so
there's some some design problems that
you'd have to overcome to to get some
practice working but if if these
advertisers remember for example your
birthday they would not wear you this
over and over again and there's there's
things that could be it could be done to
make this more plausible so this this
problem is so the question is now how do
you distinguish these these queries that
you might want to answer from the ones
that you want to reject and so so this
problem of safe sharing is a very
pervasive topic any time any two people
want to transfer one bit of information
one of them asked the question should I
actually transfer that bit of
information and that the the motivating
scenario that actually funded my PhD
studies comes from this coat these
coalition settings where armies in some
some theatres of war want to communicate
and share information from their sensor
networks or their resources allocation
and they want to share it in such a way
that does not does not allow someone
who's creating this to sort of attack
those resources so for example if the
United States has a sensor network and
they want to allow some interface to
readings from that network they want to
make sure that those readings or a
combination of readings do not
compromise the network itself or the
sensors that are producing those
readings so one thing that sort of
colors my approach to this this question
of safe sharing is the distinction
between online and offline and it's
worthwhile to briefly talk a little bit
about what the distinction is between
offline sharing and online sharing so
offline sharing is when you sanitize you
have some data you want to you want to
sanitize and release ahead of time to
people who might want to use it for
useful purposes whereas online sharing
is
you release information ads as its
requested from you and so this has some
strengths and some some benefits but
some cons as well first of all when
you're doing online sharing you're
tailing the information that's released
to the person who is requesting it
whereas if you're releasing it ahead of
time you sort of have to figure out how
to sanitize the data for everyone at
once and another can't a big con for
online sharing is that you have to
actually do this online so you cannot
just do this step ahead of time and be
done with it you have to wait for these
requests and respond them as they come
but on the other hand you do not need to
know what information is actually of
interest in the data you really seem
because you can just wait for people to
tell you what they're interested in okay
so given these distinction between
offline and online and sort of the
requirements i presented for this
sharing problem there's some three up
three initial observations that i want
to mention first of all what does it
mean for a query to be benign from
benign how do you distinguish a benign
query from an ngon benign query so that
comes to the idea of risk how risky is
it to answer the query for example of
this sort there should be a zip code in
there as well so what happens if you
answer a question of this sort maybe
imagine that an adversary that gets this
information goes off to a bank and fills
out a loan application and gets alone in
your name and then your your credit
rating I don't know if you have such a
thing in the UK you probably do so okay
you so it screws up your credit rating
so there's a risk of something bad
happening when you share information and
so there's a risk of a bad outcome and
in this talk I'm going to approximate
the sort of the the idea of risk to just
measure the probability of a bad outcome
and I'm not going to I'm not going to
try to enumerate the bad outcomes i'm
just going to approximate it with this
idea of vulnerability which just says
how likely is an adversary guess of
guessing your secret in one try so it's
just a rough estimate of what you might
imagine risk is so the higher the
vulnerability
the higher the risk public things and
secret things and you don't meet your
adversary to guess the secret things yes
how many practice it's a bit more
nuanced than that isn't it everything is
more nuanced ternary alloy yes we don't
really have information that is that is
the combination here that's secrets
rather than any particular that is true
but I'm afraid for this I'm just going
to assume that the stuff you're
protecting I'm going to measure the risk
in or the vulnerability of the stuff
you're just you're holding yourself and
we deciding to release I'm just going to
assume that the people you're
interacting with the queries have no
other information about you to begin
with as a as a rough start so then the
second observation is that risk depends
on what you have already revealed so for
example if your ass if you're deciding
whether to ask answer the question is
your birthday within the next week the
risk in the adversary guessing your
birthday after learning this very much
depends on whether you've answered a
question of this sort earlier so you
have to integrate you have to take into
account what you have answered before
and the final observation is that you
cannot you cannot this cannot take a
long time when you're doing these online
sharing questions I'd you cannot wait
five months to the two end and spend
five years worth of men hours to hire
statisticians to answer this question
for you ready this has to be automated
so altogether summarizing all these here
is the the approach that my thesis
attempted to take to solve this question
of deciding whether whether to share or
not in these online sharing scenarios
and the approach is has three steps
first is modeling the adversary
knowledge which now is going to
integrate the prior answers you've
released it and then the second step is
going to involve the measuring a risk of
a bad outcome and I'm going to use this
idea of vulnerability which measures the
likelihood of the adversary guessing
your secret in one try and the final
step is going to be a way of rejecting
questions that result in high risk and
all of that is going to be automated I
guess there's some disclaimers here that
says that the innovators
some human modeling involved in in this
process and so I'm going to separate the
rest of the talk in these roughly three
parts these three pieces of work that
I've done over my PhD studies first I'm
going to start with a very simple
example of of all three of these
modeling measuring and limiting steps in
a simple example and that i'm going to
go to these collaborative queries that i
mentioned earlier at the beginning and
if I have time I'll say a little bit
more about what happens when you're
trying to measure risk as the secret
you're protecting changes over time okay
so then the first work this is joint
work with Steven McGill my advisor my
kicks and Whitaker from IBM TJ watson
okay so how here's a very simple example
so this is this is meant to be extremely
simple so it's understandable to a
vaguely general audience so how do these
three steps work for a very simple
example let's say we have an adversary
that has some prior knowledge so this is
step one you model adversary knowledge
I'm going to get into details how I'm
going to model this knowledge but this
is sort of the vague picture that
describes the situation first we're
going to model the adversary knowledge
then we're going to consider which are
going to speculate as to what is the
knowledge of the adversary in the case
where you decide to answer a question
and what is their knowledge if I decide
not to answer the question so I'm going
to speculate about these two and then
I'm going to measure is this is this
amount of knowledge tolerable or not if
it's tolerable I'm going to answer the
question and I'm going to update my
model of the adversary knowledge to the
revised knowledge and then after having
done this if there's another question
that comes in i have to speculate about
what will happen after this second
question is answered and likewise
measure risk if that's tall boom would
answer but if it's not tolerable I'm not
going to answer and I'm going to recover
the sort of the the level of knowledge
that was before I've answered that
second question and continue going on in
this online manner so that was the high
level picture so now let's see a very
simple example in terms of this cakes
calm wants to offer you a coupon for a
birthday cake so they
screw this question is your birthday
within the next week and you have to
decide whether you want to answer this
or not so naturally I'm in the
programming languages group so that
question is a program and this is so
this is a very simple program that just
goes from your birthday which I'm going
to model as an integer between as let's
say zero and 365 and it just returns
true if your birthday is within a
certain range of 270 which is today in
some encoding so this is a very simple
program that just returns true for those
seven days around 270 so step one is
modeling adversary knowledge so the way
I'm going to do this I'm going to use a
probability distribution that assigns a
probability to each possible secret
value so it's going to assign for all
possible birthdays between one and three
hundred and sixty-five the uniform value
that the adversary not knowing anything
else about you to be to begin with
assigns a uniform probability
distribution to your birthday so this is
actually not I'm not true in reality the
your birthdays are not distributed
uniformly but I'm just going to assume
that it is distributed uniformly in this
in a simple example so this is it is not
it is knowledged its knowledge that
birthdays are distributed uniformly no
knowledge that's harmful to me right so
far it's nothing about you have been has
been revealed okay so what happens after
you answer true to this question so you
do some elementary probability and you
compute what is the distribution eldest
rebutia of your birthday given that the
output of this program i'm just going to
shorten that's true here but it's really
this event that i'm conditioning over
and not just updates distribution to be
uniform in the range of values between
two 7276 where this is today so if your
birthday is within seven of today which
is 270 this is what the distribution the
posterior distribution of the adversary
would be after learning that question
learning the output of that and that
query so the second step is to measure
the risk in in this knowledge so what is
the risk before you answer that question
so this idea of wannabe
I mention is very nice because in order
to calculate a vulnerability all you
have to do is figure out what is the
probability of the most probable point
in the distribution so it's uniform so
they're all one over 365 in the prior so
the probability of the adversary
guessing your secret is 1 over 365 to
begin with before they learn anything
about you but after they learn the
output of the query it's 1 over 7 so it
is increased from that to that yes in
one try be a risk to have been guessing
your birthday to within a week share
probabilities gone to one so there's so
there's different measures of risk I'm
gonna use vulnerability for this talk
which is a very specific definition and
that and this is what it will give you
so your definition is he guesses the
exam exact number one try yeah guessing
a close number might also be yeah so you
could use different metrics of risk but
for this example I'm going to use this
one and so when I use vulnerability this
is actually it i should say min vern
ability this is a technical term that
refers to exactly this definition of
risk so sorry that might have been
confusing that means I'm take the risk
you're guessing it's within a week I
mean what about that excuse you of
events we even see what you're just
saying I don't care about that no the
present purposes for the roses might but
you yeah but we just met sorry just
making an assumption here that we yes
yes a simplifying assumption yes
questions not as requested yes I'm going
to get into that next questions so I'm
going to get into the what happens after
you answer this one but then you have to
answer another one so I'm also going to
sorry does the order the matter to ask
these questions it probably does not but
I haven't thought about it probably not
anything else so I'm going to get to
those two questions next okay so then
the third step was limiting risk VR so
I'm just going to assume I'm going to
have a threshold of vulnerability tea
and there is a lot to say about how you
pick this tea and I'm not going to say
anything about it other than this idea
of an ability which is a technical term
is very nice because it's measuring
probability of a bad event so
for example you can use this if you have
some metric that says if a bad event
happens the one I'm I'm measuring if if
the loss incurred when that happens it's
a el dollars I should have changed just
pounds if you're if you expected to lose
el pounds once the bad event happens and
when the vulnerability is V all you have
to do is multiply them to figure out
what your expected losses given that
this measures a probability of that
event so there's other metrics as Simon
pointed out there's this whole slew of
other metrics you could use to measure a
risk but i'm just going to use this one
and I'm just going to note that it's
this this one is actually pretty nice
because it's very easy to connect it to
economic quantities which some other
metrics might not be able to do that's
that's one possibility how far you can
use this yes but then usually charge a
little more so you're expected income is
a little less greater than zero right
okay so in this example after you answer
q1 you speculated that the risk is going
to increase to 17 which is less than
one-half so that's taller both so I'm
just going to say I'm going to answer
that with true and then okay so I'm
going to answer to it I'm going to
update my representation of the
adversary knowledge now what happens
when i get another question so now this
is q2 and it's the same question except
beat the to delay value was updated by
one so the advertiser is asking the same
question the next day okay so the
question so notice that for this for
this question if your birthday was
actually 270 you should have returned
true for the second query okay so what
is the posterior knowledge of the
adversary after learning q1 and q2 it's
exactly uniform in just one value right
they learn exactly what your birthday is
so the risk in this is one because they
have probability of one of guessing your
secret in one try okay so this is not
tolerable so do you reject this question
so unfortunately if you reject this
question you get into
trouble because it will leak information
so for example consider what would have
happened in the alternate world where
your birthday was not to 70 but it was
271 you would have answered true for the
second question in fact your events are
true no matter whether your birthday was
about between 271 and 276 so what this
means is that an output true implies
your birthday is within a certain range
and a rejection implies your birthday is
270 exactly so the problem as someone
noted before in that when you're when
you decide not to answer your the the
adversary knowledge actually increases
so this this correspondence between
prior and if i don't answer it that
they're supposed to be the same has been
broken because there was a flow of
information i didn't account for so the
problem stems from a very simple design
flaw that the risk measurement that I
described to you and the way you decide
to reject things depends on the value of
your secret that is if when you're
computing this when you're speculating
as to what is the the risk of entering
q1 you're checking what is the risk of
answer with q1 with the output of q1 on
your real birthday right you're only
checking with it what is the risk if I
answer this with true and true is the
value of this birthday evaluated on your
actual secret so this whole procedure
depends on this this input which is your
secret so this might not be clear but
maybe it'll become clear when they
describe this the simplest possible
solution to this problem and that is
using this this thing called worst case
for an ability and here instead of just
speculating at what the risk would be if
I answer with the output what of the
query on my true secret I'm going to
speculate what will happen on all
possible outputs of the query that are
consistent with the knowledge of the
adversary at that point so for example I
would just I would not look I would not
just speculate as to the risk of of
answering q1 with true I would also look
at what would happen if I answer q1 with
false and if the risk and both of those
is tolerable I'm going to answer and if
either of them is not trouble I'm going
reject if the embassy says gives your
name Simon annual birthday 18th of
January and your zipcode TV one day and
you'll have to enjoy yes or no well
usually for most people that we know and
they would reveal very little all right
for me will be yes and I do a lot
everything that was the possibility then
you would have to reject so so for the
worst case then you're saying everybody
should reject yes even if the people who
are not you would we have to reject in
order to be free from the case of them
actually asking them a specific question
as well in the future uses to July and
then reduce your risk so lying is an
interesting question and I'm not
considering any line that is not modeled
by the queries themselves so you can
certainly have queries that have non
determinism in it for example you can
have a query that says is your birthday
within next week and then with
probability epsilon flip the answer so
if you consider that lying that's
perfectly model in this in this whole
skiing here because the adversary is
going to model that kind of sort of
probabilistic query when they're when
they're doing this probabilistic
revision but lying without without the
adversary knowing what the nature of
your of the query is then I'm not
considering that case because it's hard
to hard to model that situation that I
answer a question you want to reduce
your risk then you just deliberately lie
yeah so yes I'm not considering that but
I should say that you could in
expectation your risk only increases but
in some cases when you have non
determinism and unlikely output happens
your risk might decrease so that's a
technical thing you have to it would
take too much to get into here but it
does happen and that that actually makes
it complicates the idea of how do you
solve the problem of collusion when
you're doing this with multiple people
and you're tracking their knowledge
individually what if they collude
and you know things get a little
complicated we can talk more about that
later I guess I mentioned earlier that
this this whole modeling knowledge and
doing this probabilistic revision
everything here is trivial in terms of
you know just elementary probability as
long as you have this wonderful tool
called probabilistic programming which
given a distribution of the inputs and
given given the outputs lets you to let
you figure out what is the conditional
the distribution of this input given
some output so doing this is undecidable
in general and when even when it's
decidable it's a little intractable so
what is the solution so i guess the
common solution problems in a
programming research is approximation
specifically sound approximation so
notice the way I described these
probably distributions already suggested
a way of implementing some
representation of these distributions
for example you would not represent
probability distributions as sort of
long list of value of probability values
for each possible input you'd represent
it in some more concise or maybe
abstract way which I already suggested
when I described these distributions
right you might represent it as some
regions some set of regions that are
perhaps behaving nicely with some
probabilities associated with them and
so this is an abstract representation of
a distribution that would otherwise be
complicated to represent and if you have
when you have to do these conditional
conditional distributions for example if
you want to represent a distribution of
the knowledge of the adversary given the
output of the query was false then you'd
you'd represent it as two disjoint
regions so this is the region in which
the query would have been true so if it
was false you have to represent that as
two disjoint regions with probability
bounds but this is a much nicer night
much more compact representation of a
distribution with over a potentially
large estate space so
to dress it is like a computer
probability distributions and have
observations so this is probabilistic
programming so I yes it's the same yeah
this is a new she say this but it's got
a big log it's got a lot of research
work on this particular yes yeah right
although for me there's a there's a
different restriction and that I want to
be sound because I want to claim a
building security research so I want to
be sound in some sense so my approach a
paralytic programming the summary of the
representation of distributions is these
disjoint regions with probability bounds
so the over approximation of a
probability of each point lets you
figure out a sound over approximation of
this vulnerability metric because all it
is is that probability of the most
probable point but you also need to do a
story and under and up under up over
under approximation of each probability
because if you remember your Bayes rule
there's a quotient in there and to do
conditional distributions you need to
apply a Bayes rule and you need to
divide by a probability so in order to
get an over approximation of a
probability after conditioning you need
to have an under approximation of some
the probability of some events that's
why I store both of these to get a sound
of over approximation of the of the
vulnerability so this is an
approximation of distribution so it
having just these bounds lets you
represent many distributions so this is
good thing and a bad thing a good thing
in that you don't have to know the exact
prior distribution of the adversary you
can just sort of envelope it in some
abstract way but it's a bad thing
because this is lose a lot of precision
there's a lot of different distributions
that sort of fit into those bounds all
right so the good thing and some and
some bad things about this so there are
some other pieces of work I did in this
part it's already talked about this
simulator belen fortunate so this is the
idea that deciding whether to answer or
not should not reveal information so
there's the the usual solution to this
is use sim
late ability and that just means that
the adversary can decide whether you
will accept their question or not
without actually interacting with you
and if that's the case you can tell that
your decision to answer or not cannot
leak information so I sorry so this is
similar ability is this idea so so I
noted earlier how deciding whether to
accept a query or not released
information and the simulate ability
means that if the adversary can figure
out on their own without interacting
with you that you will accept them or
not if that happens then that policy
decision cannot leak information does
that make sense that means that they
must know my algorithm for release yeah
reading secrets which might itself be a
secret okay so I'm assuming everything
here is public and I the algorithm i'm
using here essentially models the
adversary knowledge so if i'm working
under the assumption that the adversary
sort of has a similar reasoning power to
myself and easily i'm going to do right
yes but on the other hand if they're a
rational adversary they would do exactly
this reasoning that they were they will
update their knowledge about you using
the Bayes rule and things like that
there will be a Beijing agent if if
they're trying to if they're trying to
attack you at an optimal manner the
decision that where the two of you might
be conditioned on your secret which say
you hope they don't know so if they know
if you're not doing the worst-case thing
yeah if you're doing the conditional
your secret thing then they're really
not going to be able to predict what you
say oh they yes and that's why you need
to be able that's why you need to decide
whether to accept or not not based on
your secret and therefore making it
simulator ball and therefore not
revealing anything you should not base
you right I realized ah sorry okay so so
that was the the important part of that
slide where you're deciding true or
false you consider both those cases your
secret is one of one among them but the
adversary knows this already so you're
not revealing anything new that the
adversary didn't know when you're
answering the question
we can talk about a little bit about
that later but let me continue to the
next piece of work which is very
interesting so this is this is a joint
work with my advisor as well and Mitaka
again and jonathan katz was a
cryptographer at maryland although this
doesn't really involve that much crypto
so I mentioned earlier at the beginning
of the talk of wanting to support these
things collaborative queries when you
enter answer a question that depends on
secured values for multiple people at
once so since there is crypto somewhere
i'm going to use alice and bob as the
parties and i'm going to do everything
in turn from the perspective of alice
and bob is going to be the other person
i want to interact with to do
computation so the question like this
which 110 f is oldest I'm just going to
assume everyone's born in the same year
so all you need to do to answer that
question is compare our individual bdays
betta a and B they be so this too is a
program and let's say my birthday was
270 and Bob's was say 300 so you could
try to do this similar sort of modeling
measuring and limiting adversary
behavior but the first problem is what
if Bob themselves are protecting
themselves and they don't want to reveal
their secrets so the first question you
might have is how is this even possible
how can I answer a question of this sort
if I don't know what birth birthday is
so neither can I answer this nor can
they because they don't know what my
value is so fortunately there's this
idea called secure multi-party
computation which lets parties compute a
function of private inputs without
revealing anything about those private
inputs just the final output of that
function so essentially the the crypto
protocol that implements secular
computational emulates a trusted third
party so it emulates the situation where
the two parties send their secret to a
third party that third party produced a
computation and sends the final output
and the crypto soda emulates that third
party which is not really necessary they
all do this in their heads and that
disclaimer there is the security
guarantees of smc is that not
beyond what is implied by the output is
revealed but the stuff that's implied by
the output might be a lot so so here's
the the picture of secure multi-party
computation you send your information to
the trusted third-party they do the
computation and return the final output
and in reality there is no trusted third
party is just fancy math so how do we do
this modeling measuring and limiting
adversary knowledge in this in this case
here okay so let's do the first step
let's just assume that Bob's now Bob's
prior knowledge about my birthday is the
same as before it's just uniform in a
certain range ok that's fine so we did
step one so bob is adversary where Alice
it's not this ability won't you sure so
yeah we're but but we're doing this from
perspective of Alice so we're going to
call Bob the adversary although Bob will
probably be doing the same reasoning as
we are in this because this is sort of a
symmetric sort of setting here willing
to appeal his birthday to Alice maybe
but I'm gonna assume that he yeah but
let's say he's not he's protecting
himself too ok so how do we how do we
figure out what is bob's knowledge after
return true so the problem is that if we
try to do this conditional distribution
as we did before we cannot do it because
we don't know what Bob's birthday is and
the the so the problem is that Bob's
procuring knowledge very much depends on
what his secret is which we don't know
so you can see this the extremes of what
his knowledge could be based on two
examples for example if if Bob's
birthday was the largest possible
birthday right he's the oldest possible
and if the question is he all this
return true that reveals absolutely
nothing because it reveals absolutely
nothing to Bob because he would he knew
he was the oldest on the other hand if
he was the youngest and that returned
true there's only one possible value for
Alice's secret and that's one and Alice
cannot distinguish between those two
cases because she does not know what
Bob's birthday is ok so one potential
easy trivial solution is to consider all
these possibility
and what I would call the worst worst
case vulnerability so as before we
considered what happens to Bob's
knowledge when I return when I answer
true or false now I have to consider
what happens to Bob's knowledge when his
secret is one and I return true or false
and then what happens if a secret is to
and I return true and false and so on
for all his possibilities for all the
ones that are consistent with my own
knowledge so this you might imagine this
solution is extremely conservative so I
think you already mentioned that the
previous worst kids vulnerability was
conservative to begin with now this is
even more conservative because if there
is a one possibility that Bob could
learn a lot about me I'm not going to
answer this question even though 364 out
of 365 times Bob's birthday would not be
that one that would reveal so much so
this is extremely conservative so
another solution is we trust the trusted
third party to do the oldest modeling
measuring and limiting risk on our
behalf instead and the nice thing about
the trusted third party is its trusted
to know the secrets of the individuals
so what you do here is you send the
secrets to that trusted third party you
send sort of your parlor policy
thresholds to the trusted third party
and they do these policy enforcement and
decide whether to answer or not on your
behalf so there's another problem here
that it's sort of hard to see in that
the policy decision that determines
whether Bob will learn too much now
depends on Bob secret and if Alice
learned whether bob was able to learn
the output of the function that secret
might leak through that through that
channel of information so if Alice
learned whether this is reject or accept
she might be able to learn something
about Bob's value so the so the solution
to this might be not symmetric in that
the this process my Tech's might reject
one party but might accept one party and
give them the output and it must be that
the parties cannot see the other party's
output so this is a very strange
situation because here we have allies
who has a policy but she's not allowed
to know whether that policy accepted
query or not because that would leak
information you're just doing the fancy
bath its defensive enough capable of
doing this too so theoretically it is
but I think the implementation wise it's
a little out of scope of present
implementations but if theoretically the
fancy math can do any computation that
you can describe I guess using circuits
and maybe there's some other models that
hope you moving in coops yeah it's
really good at what you can do it's
related to this yes all this is slow
down 10 to the booty yes very big slow
down okay extremely big slowdown
progress yes ok so that was left with
you you really had really no choice a
third party if yeah this would work that
yeah but I guess all of the stuff I've
been doing is trying to avoid having a
real trusted third party right we want
to do everything on our own so I try to
avoid that yes even if it's gonna cost
me a lot okay so in this work I
described these two approaches then I
did the simulator belen forcement that I
just described and even though this this
first approach was very conservative I
actually showed some example queries in
which that conservative approach will
still let you answer some queries even
though it's so conservative and one
common feature of those courses just add
randomness to the to the mix google
results so i have looked at some queries
i didn't try to implement this
inefficiency I just sort of simulated if
a trusted third party did this would
they answer these questions so I only
simulated sort of the policy decisions
so I have did I have done that so I
measured sort of the risk involved if
you answer these sets of queries to see
that even the conservative approach lets
you enter some queries so you can look
at this paper there to see that so in
the last few minutes I'm going to talk
about this this case what happens when
the thing you're trying to protect
actually changes over time so
so far we've had sort of this model
where we're measuring the risk of
guessing your birthday over time as
you're starting from your prior
knowledge to the knowledge after you
revealed outputs to various questions
okay so secrets change occasionally so
maybe not birthdays but let's say you're
trying to protect a location maybe
you're using some sort of location-based
service and you don't want to reveal too
much about your location one as you're
doing this so the first thing to notice
is that once the secrets changed now
this sort of corresponds to a moving
target for the adversary now they're not
necessarily going to be guessing the
sort of the same thing over time they're
going to be trying to guess the thing
that changes over time right this is a
moving target that they're trying to
guess and at the same time when you're
answering questions you're not answering
questions about your your your location
that's some time at some time in the
past your you're answering questions
about where you are now so you're the
adversary is learning sort of this
moving moving a secret the queries of
removing secret and they're trying to
guess at the same time this this moving
secret so if here is a very simple
picture of what this would look like so
I think there was a question of can you
recover risk and so this is one
situation in which the risk is lower so
here we have a situation where you sort
of you have some prior risk and you're
answering questions over time so so the
risk increases increases increases and
at some point this there the secret
changes so maybe you move to a different
location and a secret sort of drops down
so the risk drops down because now the
adversary is no longer that sure of
where you are after you've moved and you
can this may be this repeats so you had
if you have a knowledge threshold here
in which you said you know this is
acceptable on this side it's acceptable
and here it's not this might let you
sort of keep yourself under that in that
acceptable range but there's there's
problems here because real secrets that
change over time they're correlated
they're not completely independent for
example your location
a your location now is not completely
independent of your location in the past
right there's even physical constraints
laws of the universe that prevent those
things from being independent and a very
good example is this of this is let's
say if i told you at time zero that i
have a train ticket for london for
saturday morning then if you think about
what you know about my location for the
next 24 hours you might not be sure
where I am 12 hours from now but you
probably pretty sure of what when I'm
going to be saturday morning that means
even if I don't tell you anything else
about my location from now on so the
risk of you guessing where I am is going
to increase with time up to saturday
morning at which point you know exactly
where I am so this idea that the risk
could increase without further
interaction with me is sort of a very
big problem here so because this this
idea that risk could increase over time
without for during at further
interaction you really have to have some
way of predicting the risk over time not
just you cannot just respond to this
sort of on in an online matter you have
to predict what's going to happen in the
future so the model in this work we
integrated this this stuff from before
we had model of the adversary knowledge
in a prior but we also had a model of
the at what the adversary believes is
the correlation between secrets i'm i'm
i'm hiding a lot of details into this
work because there's there's not enough
time to get into it but you can
certainly talk to me afterwards so
integrating both these things and since
we're doing prediction of your risk in
the future we have to also consider what
would the adversary we do in the
meantime all right if if i want to
predict the risk of my location and
saturday at saturday morning i have to
consider which questions will you ask me
in the meantime or whether you're
actually going to try to guess my
location in the meantime all right so
these these asterix that there's
actually sort of adversary has some
choices here what they want to attack
right they they might be able to attack
my location at the most opportune time
and at the same time there observe
certain things about me as as time goes
on so if you integrate both those things
into this model this picture changes
instead of sort of going up and down up
and down it's actually monotonically
increasing the risk is monotonically
increasing when you when you consider
optimal adversary behavior why is it so
this most this comes mostly from the
fact that in the model here we are
assuming the adversary can decide when
they want to when they want to guess
your secret based on what they learn
about you then over time they know my
location on Saturday pretty well but
they really no don't leave I much about
my location right so so this graph does
not doesn't want to be good so so this
graph is actually showing what is the
risk what is the eventual risk up to
this point which might involve the
adversary guessing your secret earlier
so the point here is that let's say I'm
I'm trying to figure out what is the
risk at time eight that might involve
the adversary learning something for the
first for time steps and based on that
information deciding to guess a time for
so this measures sort of the Prophet
what is the expected probability of that
bad hat bad thing happening up to this
point what the white line is not based
on the grave of all it's using the same
example it's the same data I don't
remember it what it is but the only
thing that changes now the adversary is
adaptive in terms of when they decide to
attack so the difference is the
differences between deciding when to
attack based on what you learn or not
doing that so it's like an adversary
deciding to guess where you are on a
Wednesday for example that would be sort
of non-adaptive versus an adversary
deciding to to guess where you are based
on what they're learning about you over
time so this adaptivity result since
monotonically you and three and four and
seven eight so actually in this example
they ask you at each time step is just
it only changes at every for time steps
but still given that they can decide
when to guess based on the observation
this increases monotonically
that's real data that yes but this is a
synthetic example a very tight example
trivially true then if you say what's
what's the resident in time aight the
adverse we knew in any previous time
when my location was obviously that's
going to increase knowledge okay you
would have to so that the measurement is
they'll have to known that at that point
so it's not like you might learn
something about at time ten something
about your previous time but that we're
not measuring that the adversary has to
know something about time three and a
guess at time three in order to sort of
take that into account it's not like I'm
measuring so the overall the overall
information that's released over time I
I'm measuring their their ability to
respond to the information as it comes
in okay so so there's an interesting
thing here where you're you're trading
off sort of recovering a risk of the
secret but at the same time you're
revealing more about how the secret
changes and there's an interesting
example in the paper that that the more
you change your secret the quick of the
adversary can guess your secret which is
really strange okay so that was that so
in terms of what I'm working now I guess
the most trivial thing is to make these
things faster the technology that
involved that is involved in doing these
this modeling and this inference making
this go faster I'm also involved in a
little bit about trying to get SMC
faster this is the secure multi-party
computation which is much slower than
normal computation I've done some work
trying to get that faster and this is an
important thing that which I think will
be really useful is to characterize this
thing that I only briefly mentioned at
the very end where you have situations
which you change your secret more often
and this leads you to revealing your
secret quicker which is really a strange
situation so I'm trying to characterize
that in order to be able to figure out
how to avoid that and another thing I'm
trying to do is in all this work I
assume sort of the defender we are sort
of passive our secret might change over
time but how a change is sort of does
not depend on what the adversary learns
so it'd be nice to look at what if I
actually respond to what the adversary
is learning in order to decide how to
how to be
and then this gets you into interesting
stuff in and game theory in a cooler
bein all a kind of a good stuff okay
that's that's all i have for you do you
have any more questions so everything
I've done is here is discrete the first
part of your dog when you mentioned that
you were using a problem stick abstract
interpretation yes to calculate the risk
and so presumably that's an over
approximate analysis right so for
example if I use an interval domain then
I guaranteed that whatever risk i
calculate lies within that interval but
you also said that it was sound with
respect to vulnerabilities so surely I
mean you do have an idea what the
probability ranges but so make mistakes
right so what do you mean by soundness
with respect to sadness in terms of the
the way I decide what is the
vulnerability metric I never under
underestimate that I always over us to
over-estimate that and if you're using
this in terms of having a policy that
has a knowledge threshold that policy
that sense would be sound in that if
you're ever it will never say it's okay
to answer when it's not okay that's what
i mean by sound these notations yes on a
probability threshold yes yes so who
basis something is that you get to
answer all the questions but in real
life what seems to happen is we reveal
our information to facebook or somebody
and they their privacy controls say
something out oh we will reveal this
piece of information to those people
under these circumstances and that so
you're sort of your kind of delegating
your thing and that makes it much harder
to this kind of centralized whisper
diction yeah yeah it's also the key to
unlocking the value of you know because
you actually want facebook to reveal
some
information to other people so that
you'll get on no koku dead birds well
ideally you wouldn't even send it to
facebook at all but let Facebook be sort
of the mediator between you and their
advertisers and let Facebook only just
convey the questions they want to ask
you and never send your all your
information to facebook to begin with I
don't know how realistic this is but
that's the sort of the argument i'm
trying to make is that you don't ever
you don't ever let your data set out of
your control you always you're always in
there deciding these things for yourself
to these boats authentication models
that say i'll reveal this information
and delegate control of ism yeah so no
delegation doing everything is on on my
own you already give me to face my
descendants county yeah and i think that
the biggest the biggest problems all of
this is are not technical but social
right who's going to go through all the
trouble of not of getting a getting rid
of facebook and getting rid of all these
useful social interactions that are very
convenient with facebook where's this is
probably not so convenient online yeah
but yet because i mean this is a
standard thing me he said aggregate data
like in differential privacy means you
had some noise to the the truth so that
so I never Zone still build anything in
the hand dividual so I definitely allow
adding noise to questions as long as
that's modeled as part of the query so
for example if someone wants you ask you
a question but you don't want to reveal
it as is they can certainly ask you a
question that add noise to it like if
you don't want to answer a birthday
query I'm going to answer birthday query
plus a small probability of flipping the
answer if you if they provide you with
that kind of question you're certainly
can model that situation and decide to
answer that as long as you model sort of
the noise and Harrington and the
question the query is noisy land on yes
I certainly allow that yes and that's
actually one of the ways I did it any
good in what we do
better in your new study done then not
allowing the user to lie so so I I so
the problem is here what do we what do
we mean by lying so if someone asks you
a question that does have randomness in
it I wouldn't consider that lying it's
just I would consider that noising yeah
and it's a useful tool to be able to
answer more than we would otherwise have
and I certainly take that into kind and
in a second work I described by have
these conservative method and less
conservative method that's one of the
ways in which we shown that you can
answer questions even using the very
conservative method is allowing adding
noise at the end of these queries and
and taking that into account slide into
the eye can see almost immediately just
qualitatively adding noise is going to
lay down some more questions you know
one is you're gonna get me utility of
role but you don't own it so it's more
it's not about more but more of a
resolution and in terms of it you might
have situations in which
non-deterministic query could be
revealed so you have to you have to
noise it in order to be able to reveal
anything at all so it's not like so that
the amount of information you're
releasing whether you're deterministic
or not does not matter it does not
increase the amount of information you
can release it just allows you to sort
of get to a threshold closer than you
would otherwise be able to do let that
make some sense anything else</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>