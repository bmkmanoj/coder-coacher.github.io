<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modeling Systems from Logs of their Behavior | Coder Coacher - Coaching Coders</title><meta content="Modeling Systems from Logs of their Behavior - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modeling Systems from Logs of their Behavior</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RaUF_5meWNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright welcome everybody I'm happy to
introduce Ivan bitch ass Nick we were
something graduated from u-dub and we'll
be starting as a system professor at UBC
up in Vancouver and he'll talk about his
work in software engineering and
particularly how to understand complex
logs from distributed systems right
thanks Peter everybody so today I'll be
telling you about my basically
dissertation work and the the
perspective that you should have in this
work is that when I'm attempting to do
is the following take the logs like the
one you're seeing on the left and
attempting to convert it into a model
more abstract model like this thing on
the right and the idea is that you can
use this model for various other tasks
once you've done this conversion so the
kind of the high-level view though for
this is that I've started in systems
I've been building large systems some of
them distributed and I've been always
running into this problem right which a
lot of people face which is that you
build the system and then somehow it
behaves in an unexpected way it does
something that you would not expect it
to do and so the question is how do you
answer this question and typically the
setting here is that you have written
some code and perhaps lots of code and
you've looked at it and you've tempted
to test it but you still have this
question and of course there's some
developer some confused developer and
this developer has a mental model of
what the code is supposed to do right so
she's thinking about the code she wrote
down the code and the reason this
question comes up is that there's some
some disconnect right there's some
mismatch between the mental model the
developer has of their artifact and then
the actual artifact the implementation
so one typical way to kind of try to
bridge these two is to instrument the
system run it get a bunch of runtime
observations of behavior of the system
so you would output this log and then
you would inspect this log and try to
find out okay there's some some lines in
this log that somehow when mapped to my
mental model
give me some kind of contradiction right
they create you know maybe there's a
loop on that invalid state and that loop
you know I have never thought about a
loop but it actually exists in the
implementation and so the idea here is
that you would generate this log
inspected and then check it for validity
against your mental model and this
sounds really plausible except that in
practice your systems are going to have
huge logs so your logs might be
gigabytes long that might be very
elaborate and as a developer faced with
this very large log you're not really
sure where to look within the log or
what to look for all right so the
problem is that the log gives you a very
concrete view of the system and it's
very easy to get a very large log right
I just you know record all the methods I
call I just record all the activity it's
very easy to get a lot of information
the problem is to to actually inspect it
so the you know and and then in a
distributed setting things get worse
right the problem is that now you have
multiple hosts processes and you have to
reconstruct a distributed execution
where you've captured logs for the
different entities in your system and
somehow you have to string them all
together so going back to this to this
kind of setting what I'm going to tell
you about today is an attempt to
actually replace the log with something
a little bit more abstract so you want
to not deal with the log but deal with
something that matches your mental model
more closely right and the idea is that
if you provide the developer with a
model that models the log a different
representation if you will that's closer
to their mental model then it might be
it would be very much easier for the
developer to actually inspect this model
and find the disconnect find where the
where their mental model differs from
the implementation so that's kind of the
context for the work and this process of
going from a log to a model is typically
referred to as model inference at least
that's what I'm going to refer to a task
and lots of great use cases for this so
I've told you about mental model
validation but you could also use this
for test case generation so for example
you generate the model from the slog and
if your model generalize
then you can predict behaviors that are
plausible for your system and then you
can use those behaviors to actually
induced induced executions in the system
you could also use it to evaluate test
Suites so you might say I tested my
system you know in this one environment
and then I deploy it in production so
its behavior in in the test environment
it going to differ from production
behavior and so how do I compare that
behavior how do I know what's present in
production that's not present in during
testing and so one way to do that is to
generate the two different models and
then compare them right we know how to
compare models in different ways and you
could find paths that are you know in
production that are not in testing that
should be exercised you could also use
it for anomaly detection this is kind of
the classic case where you take the
model from last week and then you take
the model from today and then you
compare the two so a bunch of numerous
applications that you could use this for
in my case is going to be mostly about
mental model validation and so I'm not
the first person to work on this topic
probably people have worked on this in
the software engineering domain and they
refer to this as specification mining or
process discovery and usually the name
here depends on the task that you're
going to use that you're going to apply
to the model and prior work spans at
least a decade of research and a lot of
the challenges that remain in this prior
work of things like efficiency accuracy
and distribution and those are the ones
that I'm going to serve talk about in my
work today so the first one is
efficiency how do you get this process
this model inference process to work in
very large logs right so it's okay you
know a lot of the prior work works fine
if you have 100 100 you know this log is
100 lines long or turning red lines long
I did get it to work on a gigabyte log
how do you make this model more accurate
what notion of accuracy can you use and
then finally how do you get it to work
for a distributed system distributed
setting and so the three tools that I
built our synoptic done optic in
environment and to briefly over you them
the contributions of synoptic is it
gains efficiency through the process of
refinement and I'll go into more details
about what that means I gains accuracy
by mining certain properties for
this log and then making sure that those
properties are going to be true in the
final model and then the knob take a
sort of a follow-on work on synoptic
which in first a different kind of model
so the model i'm going to attempt to
infer here is one where i have a finite
state machine / process so synoptic and
first i find a state machine like model
and then the knob the case i actually
want to model my distributed system as a
set of finite state machines and indan
optic it's going to apply some of the
same same aspects as an optic like
refinement and mining properties but
also it's going to handle distribution
and then the final work is environment
which is sort of very different from the
top two and I'm not going to talk about
it much in this talk but it's it fits in
in to this puzzle in that it takes the
idea of mining properties and composing
them to to kind of to the extreme
essentially where the there is no
refinement and environment where it just
Minds a set of properties here and then
composes them in an interesting way so
this is really the motivation for my
talk and next I'll really tell you about
synoptic and in optics so there's not
going to be in any environment in the
talk so jumping into synoptic so the
goal is you have this log and you want
to produce this model and the way
synoptic is going to do this you know
initial ating tell you about any
constraints on the log so the first step
is going to be just to parse this log so
I'm going to assume that these will give
me a log and a regular set of regular
expressions that will match the lines
that they care about in the log so if
you care about disc in your system then
you give me regular expressions to
extract disc events and then I'll build
you a model that's relevant to just
those sets of events the second step is
to then build a compact model you want
to build a model that will that will
include as many behaviors as possible so
include the behaviors that you have in
the log but then many more things and
then what we're going to do is mine
these properties or what I call log
invariants and they're going to be very
temporal like things like lock is always
followed by unlock open always precedes
three
and then use these invariants to
constrain the model so I'm going to take
this initial model use the invariance
that I mind and then build you a more
accurate model expressions specific just
this is really nice coating also has to
specify these are produced a sandy
civilization advanced right I'll give
you yeah I'll give you an example of
what I mean by regular expressions but
overall I think of this log is a set of
events so I do not reason about state at
all and so the model i'm going to give
you actually is going to be an event
based model so I'm modeling sequences of
events and so your regular expressions
basically have to tell me for every log
line that your care about associate an
abstract event with that log line so if
you're sending a message like it might
be an acknowledgement message like a
luncheon back in TCP it has you know
sequence number it has all sorts of
stuff in it but your abstract event type
is acknowledgement right that would make
sense if you're attempting to come up
with a model that reasons just about the
event types and depending on the setting
you might say that's unreasonable right
so you might say okay maybe I care about
every fifth acknowledgement packet so if
you acknowledgment five acknowledgement
10 right so so this right so you would
include that into your regular
expressions but in general this does not
reason about data so it's not powerful
enough to reason about data and state
you have these acknowledgements wanted
right if I want to model a handshake
then i have to give all the things like
wrapping up set up and all these events
uh yeah you would I mean you you know
the answer here it depends on what the
log looks like right so if your log is
in a certain format then you can just
say you have a regular expression that
matches different kinds of events and
extract some you know subset let me work
through through these different steps
and actually I'll go through them in
order and the example that I'll use is
kind of very simplified version of
two-phase commit so in two-phase commit
you have a manager and you have some
number of replicas and the manager is
going to propose a transaction
and then your replicas will reply with
either commit on the board and then the
manager will say okay collect all this
information and then either apply with
transaction commits if I've seen only
commit or reply with transaction on
board if there's if there's one abort or
more so the the way we're going to cheat
here and use synoptic is that we're
going to maintain a totally ordered log
add the manager so we're not going to
care about logs or anywhere else because
I can have a total ordered view global
view at the manager and that's the log
I'll plug into synoptic so in this case
the my input might be maybe sets of
events and then my regular expressions
are going to extract just the packet
types or event types the day that I care
about and for two-phase commit the
things you care about are you know what
kind of messages are you sending
proposed aboard transaction aboard
commit so forth so I'm going to
essentially extract these execution
chains from this log one thing that I
should mention is that you also need to
have something that tells you when
basically tells you execution execution
boundaries so in this case the
transaction ID is going to be your
execution boundary so you know every new
transaction will induce a new execution
and the kind of the format i'm using
here is that the square note is going to
be this initial node so all of the
proposed nodes the first ones are going
to be the first ones and then the very
bottom most node that terminal node is
going to be this rhombus so I have this
initial set of traces it's going to be a
huge number of them and what I want to
do next is build this compact model so
the compact model is going to be built
very simply where I want one node one
abstract node for every kind of event
that i have so i take all of the for
example I might take all of the proposed
nodes and create one single proposed
node to represent all of them and I'll
take all of the commit notes and create
one commits note to represent all of
them and I'll do this for all the event
types that I have and then i'll create
edges between these based on the
concrete observations
so if there's a if there's a concrete
edge between commit and transaction
commits then there's going to be an
abstract edge between the commit in the
abstract model and the transaction
commits in the abstract model so
basically I just built you a model
that's very compact compact in the sense
that there's only one node for event
type and it admits all the behaviors
that I observed in the log by
construction right but this model also
admits lots of other things so you know
now the question becomes how do i how do
I get this model to be a little bit more
accurate and this is where this is where
invariance will come in or there's log
properties so build this compact model
now we're going to mine the assailant
login variants and it turns out that you
can get away from a pretty complex
systems with just very simple properties
right so for for synoptic we're actually
going to use just these three properties
they're going to be temporal properties
you can express them in ltl but not
going to show that to you but in general
we know from prior work by Dwyer at all
for example that in general when you
specify systems there are very few
patterns that you tend to reuse right
and so these three actually cover the
top top six patterns out of 10 or so
that the why are documented and so the
patterns here are going to be X always
fall by Y and on the example log x 0 x 5
x y would look like a board is always
followed by transaction aboard so when
you see an abort event then you know
that before the tray sends you you will
see transaction aboard and commit always
precedes transaction commits is kind of
the reverse you know looking back so you
look at transaction commits and the
third trace and looking back you must
have gone you must have gone through a
commit commit event so it's a this one
really parallels causality and then the
final one is a board never followed by
transaction commits which basically is
what you think it is if I reach an abort
event then I'm never I'll never see
transaction commits in the same
execution daddy
to build a model based on the world you
said you want to do more it's the most
accurately captures exactly what happens
this is actually what I'm trying to do
now is work coming this compact model
now it's actually that initial model
that i told you about rate that
construction it's the model is
represents the log but it also captures
other things so their paths in that
model that you have never observed that
are illegal and so if i could happen
because you're essentially stitching
together executions so here in this
model this edge might have come aboard
always followed by transaction aboard
might have come from from here right but
then the edge preceding it might have
come from a different execution and so
you might get a path in there that
actually composes two different
executions right and this is the this is
really the power of this model in that
it generalizes by stitching together
different executions based on based on
common events well now my question is I
have this model and it generalizes right
how can I make it a little bit more
accurate and accuracy will come from
these from these invariants so these
things that i told you about what I'll
do next what I'll do next is actually
mine these invariants from the log and
then I'll tell you about a process for
changing the model to satisfy these
properties so that for example this
first one aboard always filed by
transaction aboard if you actually mine
this property from the log right you
would like the the property to be true
of your model that's also a kind of a
generalizing statement it says that
there's a bunch of behaviors that we
haven't seen of your system right but if
we've seen this property for all the
executions then we would like this
property to be true of the model right
and this property is also a correctness
requirement for two-phase commit right
so you would like this to be true of the
model as well so these are going to be
mined yeah exactly so I'll show you that
in a sec I have a note here about kind
of related work I mean there's been a
lot of work on actually inferring these
temporal properties from from sequences
and different domains and a lot in the
software engineer
domain and I guess the contribution of
this work is to actually use these
properties for model inference right so
they're not I mean you can see them in
the tool and I'll show you where you can
see them but that's not the purpose of
the of the tool so for two-phase commit
here all the invariants you would mind
right so this is the set of all of them
and some of them are going to be false
positives because your log might not
include all possible executions of your
system and some would be actually true
of your system right and depending on
how interested you are in inspecting
these you can deselect some of them
right to have the model be not
constrained by the false positive no
these are mind automatically based on
those templates those three kinds of
invariants so now the question is how do
you combine these two so you have this
initial model you have these properties
and you want to refine the model to
satisfy the properties and to give you
an example so for this initial model
right the ones that I grayed out below
all of these properties already true of
this model and what I mean by true or
not true is that for example the top one
aboard always followed by transaction
aboard this property is not true of this
model because there's a path in this
model that violates this property right
so there's a path propose a board commit
transaction commits where you go through
the board node but then you don't reach
the transaction aboard so that property
is not true of the model so those three
are not satisfied now the question is
how can you satisfy it and the answer is
going to be essentially for each
violation right for each of these
counter examples to the property we're
going to use counter example guided
abstraction refinement to eliminate it
and I'll kind of a technical term oh
I'll show you what how it works so this
initial model is what you start with and
then you have this invariant and this
invariant is mined from the log so it's
true of the log but it's false in this
current model right so the first step is
to find the counter example so this
counter example is exactly the same path
that I showed you in the previous slide
and you want to change the model to
eliminate this counter example so the
the thing to realize about this model is
that really it's a partition graph so
it's an abstraction of the underlying
concrete events so that commit no this
commit abstract node contains a bunch of
concrete instances of commits from the
log right so you could you have this
underlying graph structure induced on it
and so to refine a partition or to one
way to change this graph is to refine
this partition right so somehow we
grouped all these commit notes assuming
that they're the same great but the but
the realization is that actually they're
not the same and the way we're going to
differentiate them is based on these
properties and so you can change this
larger commit partition into these two
partitions that are smaller right and by
doing so you actually eliminate that
path so now when you kind of look at the
more abstract version of this graph when
you reach the board node you have to go
to the transaction aboard right there's
no way you can get to transaction
commits and so by doing this refinement
you've eliminated this counter example
right and you're going to do this over
and over again right so you're going to
eventually satisfy all of the properties
that you mind from the log and get a
more accurate model and that's kind of
the the core of the of the Sonata
procedure
all traces still parsed right that's
right the model will still always accept
the de locked executions and like a huge
there there's you probably have a large
set of embarrassing mine and now
depending on the order in which you
addressed in variance and all the common
examples to each and arian you have a
huge search space of graphic final draft
that you might and that's perfect you're
leaving ready for my next slide so so
yeah so that the work expands on youtube
it finally okay I'm going to explain how
well their assignment this is we use
there's a set of heuristics that we use
for refinement because to eliminate the
path what we do is actually find the
first node that you're stitching
multiple executions along the counter
example so the first partition where
that happens and then the way you break
up the partition I mean it's a little
bit detailed because there's kind of
different sets of concrete events right
there events that are that are there
concrete events that are from these two
two different paths that you're
stitching together that you should
eliminate right so you should have them
in two different partitions right and
then everything else that's in that
partition and so the different
strategies are make these partitions as
balanced as possible and assign the
remaining remaining commit notes they're
randomly right so there's actually
different kinds of you certainly won't
look at the walkers
and then according to this problem but
actually the lock was not set of
unrelated item expressions it was a set
of sequences like a much stronger count
as execution was not a regular
expression it was actually break yet
every eight nice of them so this was the
program you kind of forgot about their
fascination now you've been regretting
yes we can be back versus maybe that's
what you should have done initially it
was all execution to the model and then
just say okay these are the same model
this well I guess I you know the initial
regular expression parsing is intended
to abstract the log right so you're you
know I don't care what I lost some
things but the idea with the regular
expressions that you can post them right
so you get to choose what you lose and
what you keep right so do you care about
the disk or do you care about network
you know or do you care about both which
events interest you right that that is
that is the concern that the user will
have you have mystical you treat each
event individually you just look at
heirs of events well you look at pairs
of events when you create the initial
model preg you look at sequences of 30
but there's already more or maybe you
know so you but those are already
satisfied right so in the model the
model is what if you maybe build a model
but I'm generalizing as much as possible
nobody's but then you have to fight it
and then you have to that's right yeah
yeah that's a good point i should tell
you so let me move to this to this lag
let me let me describe this so here i
worked out an example for you on a small
little smaller log that has a bunch of
these models right and so the model on
the left is serve the trace graph this
is what you would actually parse from
the log right so these are the
individual chains and then the model on
the right is kind of the initial model
that you start with where you have one
kind of partition for every event right
and so this is this is your spam right
the model over there is the most
abstract one and it's the most condensed
one right so it admits a lot of behavior
the one here is very large because it's
very concrete and it admits the only
things only the things that you've seen
right so now the question is
like which model do you want so this
model is the log so if you want to look
at the gigabyte log you can use this
model that model over there is very
small but it's going to admit a lot of
stuff that you don't want Iran huge log
only by doing analysis we can compaction
to the model they're always pretty not
exactly that will be a huge model well
so there there have been there been
other approaches that start with this
and actually go this way right I guess
the approach that I'm describing here is
one where you start with that model and
you go this way the problem with
starting from this direction right is
that you're going to have to do a lot of
compaction right what we found in
general is that starting there is going
to be way quicker so performance
actually results for our technique are
way better than the versions of
techniques where you can do compaction
right because here we actually have to
do this one algorithm which is like k
tales where you do compaction based on
length of strings right and you find
ones that have identical run and then
you can pack them and you merge some
right so that algorithm is going to be
very inefficient on a very large graph
right whereas this thing is going to be
much more efficient because you start
with something much more come back so
that that's really there's a lot of
trade-offs you know that so our
algorithm is to basically say there's
going to be this dividing line and this
dividing line separates models that you
know set up so satisfy all the
invariants and obviously this won't
satisfy all of them by definition and
then models that violate at least one a
variant those are the ones in red and so
so some some invariant violated all
invariants satisfied right and then the
exploration stage is going to be well
and here's the model you actually want
to find right you want to find a model
that is as abstract and small as
possible right but is in this green
space right because it satisfies these
invariants and you can change the
definition of the space right you can
add another kind of invariant and this
line will move to the left right or you
can remove an invariant and the line
will move to the right right and now the
operation that you have is refinement
right refinement will start with some
model that's furthest to the right and
will produce a number of models to the
left right depending on your heuristic
depending on which invariant you
to refine depending on which counter
example you want to eliminate you know
and so forth and then one and then other
techniques use course inning right so
refinement is the is the thing I
described questioning here is going to
be the technique from prior work which
is like k tales where you can emerge
note so this one splits partitions this
one merges partitions and you have these
two things now the question is how do
you get to this this intended goal model
and that's really the problem that's an
optic a dumpster self and you know in
practice what this will look like is you
started this initial model you have some
number of choices you're gonna have to
make a choice and in practice we found
that you know there's a lot of
strategies but they're all suboptimal
unfortunately you're never going to get
the global optimum here right and so
you're going to we select something very
simple and cheap as cheap as possible
because each of these partitions might
have you know thousands of events so to
some simple strategy for actually doing
the splitting of nodes and then keep
going so you end up at this model you
have some more refinement choices and
then once you jump over this line and
end up in this in the model that
satisfies all the invariants then we're
going to apply course and the idea with
course inning is that it's served like a
local optimization right it's a it's a
local minima where you want to merge
partitions locally right but it's not
guaranteed to find a not guaranteed to
fight a global global minimum so that's
that's the full that's the full
algorithm and one note about prior work
they this idea of starting from from the
far left and then going to the right
which is just using coarse inning has
been explored in Prior work you know as
far back as the 70s but it's very
inefficient right and so really the
contribution here is to use is to come
up with refinement and also to use these
invariants as a guide for what kind of
refinement to actually perform so that's
that's kind of the synoptic synoptic
technique now the evaluation you know
we've done a couple of different
evaluations one was to apply to a system
reverse trace route second one was to
actually have students in a class use
these so the handwritten diagram that I
showed you at the very beginning was by
student in the class there were they
first had to kind of write down the
model of their system
and then run synoptic on the system and
then compare the two models and then
we've also done some formal evaluation
to show that you know the algorithm
always terminates that it has these nice
properties like always satisfying you
know always accepting the log satisfying
the properties and so forth so I'll only
tell you about kind of this small user
study with the developer of reverse race
wrap and so first to tell you reverse
trace red is a is a system for actually
finding the reverse routing path on the
internet so typically you run trace
route to find the forward path but the
reverse path is usually obscure and you
don't see it so the verse trace route is
a system which uses multiple vantage
points and a controller to find out you
know what path your packets are taking
on the reverse path and deployed
internally at Google that's been a
deployment for a while and what we've
done is apply synoptic to the server
logs and so the developer had to change
the login code which is not good and
ideally you would like for this to apply
to just existing logs but they had to
change the login code to to have better
formatting you know so it's to freeform
is too difficult to read the regular
expressions and we've basically
processed about a million events in 12
minutes and the model that we got is the
following thing so unless you're a
developer reverse trace route this will
not make a whole lot of sense to you but
basically a single path in this model is
a is a path that reverse trace round
takes as an algorithm right so sometimes
you perform measurements sometimes you
do you assume symmetry because you don't
have any measurements and so forth so
those were the key key things that were
logged by the developer and each of
these corresponds to a method in the in
the code at the controller and so the
I'm highlighting couple of things and
one thing i should say is that their
numbers on these edges and the numbers
are actually probabilities so and they
don't know we sum up to 1 in this
diagram because for this developer this
graph was more complicated so we have to
hide the low probability edges in this
diagram for this to be more readable so
it's kind of you could think of this
model as you know you could look at just
common behavior which is what we're
showing by hiding
low probability edges or you could look
at rare behavior right by if you want to
find rare occurring bugs for example and
then you would only show the low
probability I just right so you could
have different use cases so this one was
showing the common behavior and the
things i'm highlighting our are two
issues that the developer found one was
these rhombus nodes that are shaded our
terminal nodes but they shouldn't have
been terminal so the system was
essentially one execution of the system
would terminate at an event that was not
supposed to be a terminal event and that
was one thing that the developer found
by looking at the model and then the
second thing are these dashed edges so
these dashed red edges were not in the
we're not present in the final model and
so but they should have been and so that
was the other feature that developer
found say that the sugar should not be
terminal they developer just new dash
right or was that conferred by know the
developer you that right so both of
these features were found by the
developer right so we so I basically
overlaid on top kind of the developers
interpretation of what is a bug and what
is not right this time there is an
illusion with violence
do you think is correct and empty for
apologies having this line along
yeah so I'll show you the well yeah so
here's let me let me actually show you
the tool I haven't running online and
here it is it's called synoptic and so
the so here's an example log right so
this log is going to be an Apache log
where you have a bunch of get requests
to Apache and so you have access access
log and this log is going to be for a
web application that's like a shopping
cart right so people come online they
they check out they get credit card you
know and so forth so the input to the
system is going to be this log and then
these two regular expressions so the
first regular expression you'll note
that it it matches every log line and
you know so it matches this thing
perfectly there's some IP address and
then there's this get HTTP and then
there's this magic keyword type and type
is going to be our abstract event type
right so the abstract event type is
going to be just the name of the PHP
file that you're that you're getting and
then the other thing that you need is to
actually somehow split these executions
so what is an execution boundary and in
this case the execution boundary is
simply the IP address right so as a new
client to the server is a new execution
of the system and so basically you have
executions that are match one to one
with the clients yeah and that's that's
here you guys actually notice that
they're all and just you know intermix
so it's going to actually pull them out
yeah so this is the this is the only
input to the system and then the the
model that you'll get out will be
something like this it's i'm not i'm not
a graphics person so i couldn't make
this like fantastically looking but but
here it is
so basically you start off in this
initial node and then at race starts in
the initial ends in terminal and then
goes through these to these partitions
and you can click on any one of these
partition and then find out the log
lines that are matched up that are
basically merged into that one partition
right so there's some invalid nodes and
then there's to check out nodes so there
are two check out notes that were
differentiated based on the invariance
right because somehow they're considered
different see if I could make this look
a little bit better yeah so the so the
question for you guys then is can you
find a bug in this model first yeah
that's right so that's that's kind of
the idea right do you guys see that all
right so there's a path where the
invalid coupon you would assume would
take you back to check out for example
but it doesn't goes to reduce price so
you know what does that mean well it
means that there's this transition is
actually in the log and you can you know
select two of these partitions and say
you know what are what are the paths
that go through these and you'll find
that there's some 15 traces that
actually goes through both of these
nodes right and then ideally you would
jump back to the log and actually find
those traces I didn't build that thing
yet but that's that's the intuition for
how you would use this model right it's
an abstraction of what's in the log the
log has a lot more detail right it has
timestamps it has a PS as all this other
stuff but really when you think about
the logic of your program maybe you
don't care about any of that right what
you care about is just the event
sequence and whether it's reasonable
how do you find a way filippi's like
point 51 ah yes so the probabilities are
based on the probabilities are based on
their number of events right so
initially so like this check out node
contains n events right and then n over
to go to this guy and then and over to
go to this guy so the probabilities are
based on the concrete based on the
concrete observed events so easy is it
like you should check out you and
happens I mean I'm only another coupon
can I mean check out can post mini
coopers I like it so how do you say that
this check our case for this in order to
open it yeah so in the log right there
were some traces that went this way and
then some traces went this way right and
so so exactly half of them took this
edge and then the other half took this
other edge and that's where the numbers
came from I didn't talk about them in
the talk because you could you could
show much more information on top of
this model and probabilities are just
one thing that we thought about I mean
the one other thing you could show is
actually like time or a distribution of
time right because if you have time
between events then you can say here's
the shape of like the time stamps for
example but it's it's based on the
concrete underlying log and it's added
to the model after the fact so it's not
used in the actual modern France
national this this product is that you
are getting here you're kind of assuming
that the process where the load comes
from are completely
right yeah you assume that is the
sequence of events then and that it's
reasonable to model a secret servants at
in in this formalism right yeah two
branches growing up an initial check out
one that dr. at credit card and mother
could poop on the pepperoni one oh you
mean this one rate yeah that's sorry
that's actually a bug okay I keep you
know it's it's sad i keep presenting it
over and over people keep finding it i
have to actually go fix it oh yeah so
i'm talking about so there's this
interplay between choose filter and a
variance and lexie the model that you
generate right and at least as you
presented it was okay we discover the
variant that we select which ones we're
going to what we select all of them
right now so as a developer right now
you don't you know so in that screen in
that input screen you don't get a choice
I mean one thing I didn't tell you
absent threshold for the strength of the
invasion the alien variants are all very
simple right now so they have to be true
for all of the executions right so yes
if you ideally you would want like a
probabilistic invariant that would be
more robust so you'd say ninety nine
percent of the time this is true maybe
the one percent of traces are actually
just malformed yes and we have use
office
yeah so and you could look at them here
right so here all the invariants that we
mind for this thing right and here's a
low of visualization that you can serve
see you know always followed by I always
precedes an arrow file buyer or read and
so you could you could take these and
then actually you know remove them and
then the model would change right and so
the so yeah so this gives you some some
kind of knob to go and tweak so if you
disable all the invariants then your
model will be the initial model right
you would perform no refinement yeah I
was worrying and worried what's key to
be if you rely on the visual
representation of a state machine I mean
usually doesn't scale more than 20 1500
smacks or something like that so one
that's the case what do you recommend
you of what you recommend like
simplifying removing some of these
things or do that or use fancy
visualization for very large things yeah
the vision you know I when I started
this project I didn't think that
visualization would be a kind of an
important component of it and it turns
out that it is we found that in the in
the regular x4 so for example in the
traceroute model that i showed you we
have to remove nodes and edges of low
probabilities in order to make that
model kind of fit on the screen and be
readable so that was one simplification
that we've done on top of it I would
actually argue that what you really want
to do is choose a different just a
different smaller component or choose a
different abstraction free model so
basically to me it means that this place
of abstraction that you selected with
these regular expressions on top of the
log is simply too close to the log to
concrete right so you should raise the
level abstraction in order to make that
model the bmore interleave to interpret
but that's not always the case you know
sometimes you do have a lot of it really
is a complex state machine so this tool
I you know definitely would not work for
a very complex system that has a
complexity yeah
he's asking what are the bugs you found
in the reverse trace credit help
critical were they board a sort of
global impact because i think they
actually cost crashes or performance
problems they would have noticed them
earlier yeah or you know if you have any
sense yeah so the developer knew about
one of them like they knew that this
thing happened they didn't fully
understand where it came from and
actually you know the model doesn't tell
you where it comes from either right
it's like a diagnosis tool you then have
to go and find the root cause after all
so it'd the underlying cause with was a
threading problem was a concurrency
problem the bugs were not they didn't
crash the system so so they were not
severe they're not that tutor okay so
i'll i'll switch over so showed you this
demo thing so to summarize you know
synoptic takes this log from a single
totally ordered log and produces this
event based model and it does this with
the refinement and then leverages these
mining variants that it satisfies in the
final model and primarily the use case
here was comprehension so the and the
idea is that you want to help developers
with large and complex log that was that
was the original intent and it's open
source since you know deployed on this
web interface sort of sort of scales you
know did ya please Lord just log that
you mean like a million log lines
um in Reverse yeah different versions of
reverse research we've done larger ones
for it the yeah the complexity of the
algorithm really depends on the number
of event types and the the diversity of
the log that you have so the more
connected your graph becomes the more
difficult it is to to check those
properties to model check those
properties and then refinement of course
flows down as you have more events so
the next part sorry I will talk it yeah
please did you do any investigation how
um robustness is to light the loss of
lines or if you like happen to log
inside one condition right so it
wouldn't pick things up that you haven't
logged so it's very much constrained by
what you have in a lot how bad could
that screw up write a question a if you
have another execution that is just like
this one then wouldn't change the model
because you would essentially you have
this robot redundancy right you have
underlying concrete executions would
still have the same edges and you would
mind the same invariants and refinement
would perform you know we perform the
same but if you remove a logline that
would remove an invariant for example
then it might change the model radically
Gabby Casey in one execution you have a
event and the other it does not then
that will she was an evening what do you
mean so same one execution has some logs
on meeting another execution there are
some logs so you will show it in the
execution red lobster on detail as an
either as an error we wouldn't while you
the model would present both executions
right and would merge those sections of
it that it would be able to merge but it
wouldn't you know wouldn't show you an
error actually would attempt to satisfy
both it would temp to model both
and it's certain ingredients what about
what ingredients risk so certain yeah so
so the invariants have to be true for
every execution so if there is at least
one execution you know even if you have
a million of them that doesn't satisfy
the immigrant then you throw it out
right then you're not going to mine it
and then you're not going to use it for
one quiz that demonstrates what he looks
like an invariant because the
precondition for them very only happened
in that one train and if you have grown
a million more runs maybe you would have
found the case we hit that precondition
but go the other way so it's not really
in varying right it would be a false
positives O'Leary so yeah you would mind
the false positives along with with the
truant I mean the answer here is really
you want as much information as possible
right for the model to be accurate you
need to observe more things okay they
should so solutely we really need one
was just blonde in your skin on the wall
right
especially now when you do they find it
was invited as meaning you're looking
around the boxes back into the world and
for the de-risking to store memory or
Hollywood yeah we
yeah those memories memory pressures
definitely a bottleneck for because you
need to store the whole you need to have
the full log in memory right now there's
some optimizations that you can make
where you really need to store the some
concrete aspects of the traces right you
don't need to store all of them so for
example if you have two identical traces
I just need one of them I don't need
both right so that kind of this great
what's on the list you're right i would
just just compose them and there are
other cases where you might not need to
store some things because they you know
during the merge the invariants are
satisfied right so the if certain
partitions are not going to be ever
refined so then you can lose them well
you you can know that sometimes because
the underlying refinement assumes that
you can you can only refine a partition
if it stitches multiple executions
together right that have the front that
have different futures essentially right
and that's when you're going to want to
refine it so the optimization would be
you know I take that partition and then
I check if I can check that all the
invariants are satisfied below then it
can throw away the state for that
partition that would be in one
optimization but yeah we haven't
implemented any of those so that would
be kind of future work yeah I think yeah
definitely this is single threaded Java
process right now so this is not
optimized at all it's mostly
experimental so let me let me jump into
this Dena flick thing because it has a
bunch of details they you guys might or
might not enjoy so um so this the
sequential case was you have this you
know the intuitive model there is kind
of a sequence of events you know the
question is what is intuitive in the
distributed case and in a distributed
case when we told students to write down
a model of their system they came back
with pictures that look like this right
so basically they would model each
component as a finite state machine
right you would model the client find a
state machine you have the server finite
state machine you know the only catch is
that the server emits events that the
client consumes right so they're
actually linked
you know and so so this was kind of this
is very intuitive to students and the
idea was that well people are familiar
with fine state machines so let's use a
formalism that's close to this so in the
synoptic case you would infer an
event-based model in the dena tech case
we would actually infer a model that is
communicating fsm communicating fine
state machines and let me let me tell
you more about what these things are so
CFS m's basically have some number of
processes and they're connected with 50
cues that are reliable so one example
fsm see fsm that i'll use here is a very
simple one where you have process one on
the Left process two on the right and
you actually have you know States and
they both start off in their initial
States and then you have these funny
funny looking transitions where some of
them may be communication event so
exclamation em means that you want to
send em on q1 so sort of think um into
that queue and then ? M means receives
that M on that key so and once you
receive it you can proceed right but
unless but you cannot execute this event
unless there's actually an M at the head
of the queue right so this is really
modeling message passing and if it also
includes local events so you can execute
a local event for free right and then
you might communicate back over a
different key because the keys are
unidirectional and then you would
consume it on this end and that would be
a complete execution just a way to get
data is that there could be anything yes
I'm not assuming anything this could be
shared memory this could be a socket
right yeah so this is one execution of
this see fsm there's only one execution
of the CFS em but in general they may be
asynchronous right because there might
be another process that I'm
communicating with independently right
and they're not they don't have to match
up so so the idea here now is what if we
have a log and we want to produce this
kind of model right how would we infer
this kind of model from a set of events
from the log so you know the gnostic
very similar sounding to synoptic so our
pipe
is going to resemble synoptic I'm going
to have very similar steps where we
parse the log build this compact model
mind some properties these properties
are going to be more interesting because
now they're going to be events at
certain processes so I might have
something like send that process one
he's always followed by receive at
process two and then i'm going to use
something like refinement to get the
final model so very similar and so i'll
describe all of these steps kind of now
in the dena patek setting so so the
first question is okay so you have a log
how do you get back and execution out of
how do you parse it well it's a
distributed log so what you're going to
need is something like vector timestamps
right so you want to actually capture
the total order the partial order of
your of your system execution so these
vector timestamps are logical clocks
that you can use to reconstruct the dag
the partial order of the execution as it
occurred and what we have built is like
a low library that you can compile it's
for Java you compile your Java recompile
it with this jar and then existing log
messages would then have these vector
clocks put in automatically right so you
get this logging for free so you don't
have to have it part of your system
we'll add it in automatically it's going
to have an overhead but you don't have
to worry about generating these tanks
down yes so assume that your initial log
was you know so like if you didn't have
these things initially right you just
had the thing on the right so then you
compile it with a jar and it
automatically tracks sends and receives
and tracks causality and then just adds
these to every logged line so when you
log normally you get you know Q
exclamation m that but then it would
also pre prefix this vector clock so the
idea is that you just want to capture
the partial water right and I don't want
to change your log because you log some
things for a reason right you just keep
that but I want to keep the causality
and went to reconstruct it so very basic
vector clock mechanism okay so you have
this give this log now so how do we
build the initial model it's going to
have kind of two steps so first of all
we're going to deal with state
unfortunately because these CF SMS have
q's you actually have to reason about
such message fifo state you know what
messages are outstanding so first we're
going to reconstruct the state and then
we're going to do partitioning not based
on events but based on kind of cue state
so let me show you how this looks so
initially you have this time space dag
that you parse from the log and then you
want to come up with a state-based AG
and the process here is going to be very
simple it's going to be simulation where
I first see the first event well so I
start off with a state where both of the
cues have no no messages they're empty
then if you see the first event its ass
end of em and so you know my simulation
will basically say okay I just want to
add em to that Q then I receive it then
I go to a new state that does that has
both keys empty I execute a local event
doesn't change the cues and then I
execute the ACK kind of sequence so the
idea is that you would parse a bunch of
these from from your dag and now you
have a model that has States and events
more complicated than synoptic but you
can apply some of the same ideas and so
the idea now is you want to build this
initial global model that is compaction
and remember in synoptic what we've done
is we've seen commit and commit and
we've merged them together we assume
they're the same in this case we're
going to build a state-based version of
that so we're going to take look at
these cues and then we're going to merge
cues that are the same and actually in
practice there's going to be
approximation here where we're going to
merge cues that have the same top K
messages right so we're just going to
assume that after k the cues are
identical right so this is where we're
going to lose our this is where it's not
going to be exact so q1 and q2 is going
to be this abstract state that
represents all of these orange States
and I'll do the same thing for this guy
and the same thing for this guy so now I
have all of my abstract States and I you
know create arrows between them in the
same way so i have a transition between
a state where having them and the state
where but old keys are empty on receive
them if this actually happened in
practice right if I saw the concrete
event that made this transformation so
now have this initial global model it
has
the same great features a synoptic right
so it accepts the log where every one of
these executions in the log is going to
be a valid trace in this model but this
model is actually you can think of it
like a cross product of the process
models right it's not actually see fsm
you have to reason about global state
you know global events of all of your
nodes so the actual decomposition is
going to come up in refinement but let
me tell you about invariance so
invariance I'm going to be very
straightforward right you have a bunch
of these DAGs and what you're going to
do is mine the same kinds of events the
same kinds of templates that we had in
synoptic except that now your events
have kind of a process ID associated
with them right so you might have a an
event that only execute at a process one
you know is always followed by an event
that executes the process do so you mind
the same set of things and now the
question is how do you use both the
initial model and invariance to get this
see fsm formalism that I told you about
before all right there like how we f
message and we send on q1 event receive
it's always been forced by construction
by your love no let me see something
here ah that one yeah I think you're
right it is enforced by construction in
the logging yes so in but you know it's
not enforced by model so you do have to
you have to you still have to I haven't
thought of that that's actually a really
great point so you don't have to mine it
in a sense yeah so you can just add
those yeah the reason it's actually
there is because the the library that
adds the vector time stamps came in
after the fact you know so it's so if
you implement your own version of vector
clock so if your message loss you know
then this would be one way of handling
it so now you want to compose these to
use both of them together so this is the
really really fun part the really
complex part so you have this global
model so the first step is going to be
to decompose it into a CFS m and the
decomposition is to is pretty
straightforward where you just pay
attention to the events of the
individual processes right so i take
this model and only look at events for
process one
and I treat events for other processes
as epsilon transitions right so I going
to receive an M and then this is going
to be i'm going to send in them and then
this is going to be an epsilon
transition for process one because it
basically says and then something
happens somewhere else and i don't care
what it is right but for me locally you
know I transition to do this other state
so so using that approach you can
decompose this thing into these two CF
SMS and these UCF SMS are very compact
as you can see they're just one state
each and the reason for that is because
it is the most compact global model for
that example and then the the next step
is to use prior work on formal methods
luckily i didn't have to invent this so
there's been prior work and actually
model checking CF SMS right and so we're
going to throw it add add this add this
model that we have right now to get back
account example for an invariant if
there is one so for example in this guy
there is an invariant em send of em is
always followed by receive of em and we
use a model checker called MCS cm which
model checks CF SMS exactly and so it
might not terminate so we're thinking of
using spin but that's kind of a site
point but the point is you want to check
this invariant and you find that there's
a counterexample right there's an
execution where I can send them and then
I can receive and then I can execute a
local event and then I terminate and so
this execution is a counterexample to
that invariant and that that execution
is then going to induce a refinement of
this graph so here I can I can you know
I can send an M and I can execute a
local event right and so the idea is
that you want to split this guy out to
require that every time you send an M
you're going to receive a nap right and
so this is this is refinement as as
before in this an optic case and once
you have this new global model you do
this step again right you're going to
come back with a new CFS m-model check
it and then eventually you're going to
be done and so for this example that I
worked out you know the the knob that
will give you exactly these two traces
you know that you observed you know will
give you exactly the CFS
for the for that example so that's
that's the dynamic process it's more
involved it has much more formal methods
parts in it and it's it's a little
trickier you know I I feel like we're
still struggling to understand all
aspects of it you know but we've done
some preliminary evaluation on this and
so we've simulated some protocols give
some simulated traces we've evaluated
with voldemort DHT which has a
replication protocol inside of it and so
we selected just the messages that have
to do with replication which turns out
to be really trivial so it's perfect for
a tool and then we've done a case study
with TCP where we only looked at opening
closing handshake the problem with CF
SMS is that they again don't model data
so you cannot you cannot you know reason
about sequence numbers for example so
you cannot do the datastage of TCP but
you can do the opening closing handshake
pretty pretty easily and then there's
been a bunch of looking at formal
evaluation and usability of these models
so I'll just show you the the DHT result
so Voldemort is those distributed hash
tables it's basically has this very
narrow interface right you can associate
some value with a key or you can
retrieve the value for certain key and
you know this is actually deployed but
it's open source and you can download it
and exercise it so we're andan opticon
logs generated with unit tests by
Voldemort and we just targeted the the
protocol messages for replication and so
this is the CFS em that you get out it's
not as pretty I had to prettify it
manually but basically you have these
unit tests used to replicas on one
client and this replication protocol is
really straightforward you basically
have kind of a right side to it on the
left side on the redhead right hand side
you're executing put your associating a
value with a key and so this guy is
going to essentially execute put on this
replica wait for reply and then execute
I put on the lower replica and then the
same thing for get you're going to have
to do the same exact path so there they
kind of mirror mirror each other and so
through inspection we found out that
this is indeed the true model for
replication in Baltimore so it's really
really simple and voldemort
you know so that's why this model is
pretty is can is feasibly you can
inspect it and then it succinctly
capture as a three note you know the
stupid execution from Voldemort so the
contribution here are very much like in
the synoptic case except that we add one
more which is to handle distribution
right so how do you handle a log that's
generated by multiple processes our
answer is you want one final state
machine / process right and that would
be one way of doing that and in our case
we've found that it elucidates
distributor protocols because this fsm
you know so logs that have no partial
water in distributor hard logs that have
partial order are exact but are even
more difficult because now you have to
draw these things so a more general
model can help you understand these
particles better and so it's open source
but it's not actually deployed to but
you can try it out so before I conclude
I want to thank a bunch of people I work
with a trio of advisors on this project
and my collaborator and a ton of
students at UW and generally funded by
DARPA google and SF so the contributions
of this talk is that you know I think
logs have a lot of potential they have a
lot of content than them and what I
attempted to do is to apply basically
formal method techniques to log in
analysis and model inference in these
two tools synoptic and first sequential
models then optic infers distributive
models and the idea is that you can use
these two then help developers
understand what goes on in their systems
so you can find out more at this URL and
thanks for your attention thanks for
coming yeah
in next step I actually I'm excited
about applying model and friends to
other domains like other other kinds of
problems so I think then optic is a
interesting theoretical kind of tool I
think it's really difficult to make it
scale so I was thinking of taking the
synoptic approach and then applying it
to logs that have more information so I
was telling Peter about logs that have
like timing information for example so
you could have probability on edges but
you could also have time on edges and
you could use this to actually think
about performance within your system so
now I'm actually modeling an execution
but I'm also reasoning about time that
it takes between events and I could
think about you know give me the path in
the model that is lowest you know based
on the observations rate and I could
think about that for doing things like
performance testing or performance
analysis so that's like an immediate
low-hanging fruit that I was thinking
about I guess in general i'm more of a
systems person and i'd like to apply
these techniques to systems and so I'm
thinking about doing test case
generation for distributed systems
that's one of my I think that would be a
great thing to go do next because
distributed system is very difficult to
test and I haven't ni and I feel like a
lot of people know how to test them well
you know so like when when people write
distributed systems code they don't test
as much but they test very specific
things and I was wondering if there's
some way to leverage that intuition that
developers have about their systems to
generate test cases better so they're
both kind of in the soft rangering
domain as applications as as techniques
but the applications would be kind of in
the system something
yeah you know there's some ways of
cheating with synoptic I also think that
you know I don't think modeling friends
is like Apple to all problems you know
so I wouldn't necessarily floral prints
fourth St cherish it would be nice to
but not sure it seems like you're happy
you touched on it but you said that one
of those sort of major ass that he
seemed to craft when people started
using visualization was actually more
useful than then the raw data
right on Teddy put more thought into
what you could do or what can't be done
to improve that side of business in to
see mr. McCain developer story for as
far as like you know about finding the
pre-med that seems most valuable right
yeah I think a tighter integration
between the model and the log with your
really useful so right now you have some
kind of window that you can peer through
you know into the abstraction so you
have this node and you can see the log
lines that are related to it but I think
you can ask more queries of the model
like if you can actually ask a question
like why are these two model why are
these two nodes split out you know why
can't I merge them you know and then the
answer would be well there's this
invariant and it would violate this
invariant you know or if I merge them
then there's path that would violate
this invariant so asking being able to
post questions like that I think would
be useful because oftentimes when you
have this log you have a very specific
question you know and so the question is
then now the I think the research
question is can you interpret that
question oppose it of this abstraction
and get an answer so I think that would
be very helpful it's actually you know
so right now it's very much open-ended
you know you could use it for
exploration but I think building it
towards a specific set of tasks would be
would be the right thing to do it's a
luxury to be able to just collect logs
and let's go find the bug this beauteous
like oh my god it's totally broken
what's happening assistant that's
usually why we will go into this way
sorry I'm being able to Jay have more
direct that was my initial goal it's
like just comprehension overall you know
because you do use logs for so many
different things yeah so if I just give
you something slightly easier to use
than a large stacks log you know it will
make your life a little bit better but I
certainly don't want to build in any
assumptions about what you might be
interested in you know which is why you
know the abstraction that is done by the
regular expressions is left completely
up to the user</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>