<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Main-Memory Join Algorithms: Sort or Hash? | Coder Coacher - Coaching Coders</title><meta content="Main-Memory Join Algorithms: Sort or Hash? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Main-Memory Join Algorithms: Sort or Hash?</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AfKz-ly9ypE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon today it's my pleasure to
introduce again stieber he is leading
the tight the database in your
information systems group at tu dormand
university in germany his research
interests include well are really data
processing on on modern hardware
platforms this includes FPGAs multi-core
platforms and hardware accelerated
networks so with that I'll keep the
instruction shirt yes yeah thanks a lot
and thanks for all coming and and of
course feel free to to interrupt I've
given this talk a little bit of a catchy
title sword vs hash that's one of the of
the long-standing debates in there in
the database communities or it's like a
pendulum swinging back and forth what is
to be put preferred as an approach and
I'm going to look at this discussion in
the main memory context so I didn't
exactly know how deep the database
knowledge is in that audience so
basically for all of the of the core
database operations be it aggregation
grouping duplicate elimination or joins
they basically all boil down to a very
similar implementation which can be
based on on sorting or hashing in this
talk I'm going to focus on joins so I'll
for the most part of the talk I'll talk
about joins so we want to implement
joints in main memory as efficiently as
possible and again there's basically two
approaches one of them is to use hashing
so has shown it I just as a recap try to
visualize it what you do is you have to
input tables and what you do is you scan
one of the two input tables while
scanning the input table you populate a
hash table with all the tuples from from
our and then in the second phase of the
algorithm you scan the other relation
and for each type of probe
into that hash table how strong is
popular because it has nice complexity
properties I wrote here a little bit
fuzzy ly o of n so it's it's it's linear
in the in the some of the of the sizes
of the two input tables and it's very
nice to paralyze we'll see that in a bit
alternatively and that's what I think
databases in that did run off of disks I
prefer today is a the so-called sword
merge-join so what you do is you have
your two input tables there are in some
order so as a first step of the
algorithm you sort both input tables and
once they are sorted joining all of a
sudden becomes trivial by just merging
the two so that's the so-called sort
merge join it's used a lot I think when
you will you deal with external memory
because the databases have techniques to
deal with to sort arbitrary amounts of
data quite efficiently strictly speaking
its complexity properties are not quite
as nice but we'll see actually in the
end they both boil down to be to be very
similar now I said I'm going to look at
both sides in a main memory context so
the question is which of the two
strategies do you want to prefer in a
main memory context classically hash
joint has always been the the preferred
choice because well if you if you take
random access memory in its in his
literal meaning then this this random
access pattern of that of the hash
doesn't hurt you when you when you run
stuff from main memory and then you
prefer the simplicity and the nice
complexity properties but the question
is how does this look like in on modern
hardware and that has like in the past
years led to two quite a big of a bit of
discussion in the community which of the
two strategies should be preferred I
think one of the quite well-known papers
here is one the data team with people
from Intel and from Oracle did was they
did a study where they where they wanted
to compare the two and they try to make
predictions of what's going to happen in
the years to come and what those guys
found out was that has shown in their
setup was the method of choice it was
about two times faster than then sorting
but they predicted that as Hardware
emerges or evolves it's quite likely
that very soon sort merge is going to
overtake hashing in performance and that
is because of the increasing simdi width
for the vector instruction capabilities
of modern CPUs those tend to be a very
good fit for sorting but not so much for
hashing that favors sort merge-join and
their projections from about four years
ago were that if you have a an
architecture with 256-bit similar
registers then both should be about the
same speed and after that as cindy
vector lengths continue to increase sort
merge is going to be faster now more
recently a group from Wisconsin they
looked at hash coins only and they sort
of did conclusions only within the hash
join world we're going to see what
exactly
this statement here means what I
basically said is that a relative thing
that Eve implementation of hash join is
superior over over implementations that
are highly specialized to modern
hardware highly optimized to modern
hardware I'm I listed that also because
even more recently there was another
paper that did compare sort merge and
hashing that's a group from from tu
munich they took these results here did
a transitive conclusion so they found
that their sword merge-join algorithm is
faster than this hash joint
implementation and by transitivity this
is also faster than this one which means
sword merge-join is the method of choice
that's kind of what they say we have
similar sized relationships or you can
make us go and do capitally better by if
you have a tiny build side and we are we
going to see that indeed a little bit
right and so so in fact that kind of
brings us to to what I what I want to do
in this talk I want to of course the
comparisons on that previous slides they
were really apple to oranges comparisons
right in particularly these these
transitive conclusions right you choose
different where you view you prove
something on one workload and then the
transitive conclusion on another
workload so from that you can conclude
anything so I'm first well eventually
at the end is large enough common in any
constant factor which might get from the
linear one so depends on how big the
relations are also overall what's going
to go as sorry as you want it sort merge
is n log n right that will eventually
dominate any algorithm and in terms of
its total cost will be larger yes but it
turns out that efficient has shown
implementations typically also you do
will will see that you do multi pass a
hash joints and then you end up with
with a log and factor again so it's not
really clear whether that complexity
argument really makes sense ok so yeah
what I'm going to do is is I'll try to
do apples-to-apples comparisons and I'll
sketch a little bit like implementation
details that we found out to matter and
and we try to try to end in the end make
a little bit conclusions about where are
we in terms of the sword vs hash
discussion so the agenda for today is
I'll first look at hash coins in more
detail than a sort merge joints and then
I'll compare the two ok so again house
showing is is nice because it's so nice
to paralyze so what we're going to look
at is of course we look at modern
hardware so that includes multi-core
parallelism of course this is just a
slide to show that hash join is
basically trivial to paralyze you just
chunk up both input relations in two
partitions that you assign to individual
course and then you can do the hash
table build and the probe in parallel of
course the one after the other but then
you can use all the course to do
joint built and the joint probe this in
principle works very well that's because
you do need a locking mechanism here
because you know that that in the in the
build phase the hash table is accessed
concurrently but um you you benefit here
from the I mean the cardinalities are
easily in the millions right and then
you're that the chances of getting a
collision on that hash table is
basically negligible so so this
paralyzes very well do you you won't see
any any lock contention in practice just
because there's so many many hash tables
hash table buckets in practice so this
is like the the basic hash to an
implementation that we're going to look
at first and this is what this group
from Wisconsin claims to be the
preferred one because it's fast and
because it's simple now if we first look
at so what we did was we took their
implementation analyze the carefully and
try to see whether the comparisons that
they make are actually valid whether
they make sense or whether they're doing
apples and oranges comparisons and for
that for that what we first did was we
try to see whether there is any kind of
situations where we can improve their
code and I have one example here just to
show you that you have to be very
careful to implement such an algorithm
otherwise you're going to suffer quite
significantly from effects in modern
hardware and one of these examples is so
what I'm showing here is the hash table
implementation that the people from
Wisconsin used so in order to implement
that hash table you need some locking
mechanisms so you have to implement that
that as a separate will hatch array and
then
the hash table consists of a hash
directory that points into into
individual buckets that are that are
allocated from a heap now when you do
any operation on that hash table and
what it means in the end is you need to
access three different locations in
memory you need to first get a latch
then you need to follow the pointer and
then read out your data which amounts to
free memory accesses per couple and of
course you can also do that with just a
single memory access per couple by
simply collapsing that into a sim single
into a single data structure and why
that is important you see on this slide
here so when you start analyzing the
performance of that implementation on
modern hardware what you see is that so
the amount of work that you have to do /
input couple is actually extremely small
so looking at the at the hash table
build phase here what you need to spend
is / topple about 34 assembly
instructions and now it makes a real
difference whether for the during those
30 for assembly instructions you spent
you you see one or three cache misses in
practice we saw something around 1.5
cache misses that that was because well
it depends on how you cache line align
your data you can you can also bring
that down to one if you want to and we
saw TLB misses quite a lot exactly yeah
these are for for for this implement so
here you see three misses I'm here you
see 1.5 the 1.5 comes from so if if an
aligned address starts here all right
then this is the first row and the next
one will come right after here and it
will span over to cache lines
and then you get to miss to mrs. for
just a single read and then you can do a
trade off we played around a little bit
with that you can well we wanted to keep
the the number of tuples per bucket
consistent otherwise you just enter you
introduce another parameter which you
know can vary around arbitrarily you can
you can blow up a little bit so you
spend more memory which causes you more
cash miss court more cash occupation on
one hand side but with the lined access
you can reduce the amount of cash misses
per couple but performance wise it
didn't make a huge difference okay so
either way what you end up with is you
you have in the order of 30 cpu cycles
work to do and that well and at the same
time you need to do well 1.5 memory
accesses plus a bunch of TLB misses that
you'll suffer so here you're easily
talking about 200 cycles or so that you
need right so in the end you're going to
be severely latency bound right you only
all the costume is you're measuring is
those cache misses and that's why in
practice or we think that such an
implementation on modern hardware won't
make sense people have realized that
earlier one way to avoid these many
cache misses is to choose a partition
hash join in Stan so the idea there is
that before you build up your hash table
you break up your problem into into
subproblems so you use hash partitioning
to break up your input relation in cash
sized chunks you do that for both input
tables and then you basically need to
only join corresponding partitions and
if those are small enough then you can
keep all the processing
for for one such pair of partitions
inside the cash and you won't suffer any
you won't see any cache misses any more
here so once you do that you can each
each of those joints basically runs out
of caches and you will see basically no
cache misses any more again you have to
be careful when you implement that so
what the what the Wisconsin guys did is
I took this very literally they
partitioned build all the hash tables
and then probed into all the hash tables
what this means is that you build up
this hash table you build up all those
other hash tables and eventually you
done with all your hash tables and then
you go back up here and by the time you
want to access that hash table its of
course long gone long sense from your
cash right if instead you you interleave
that build and probe phases you can
reach a situation where you build up
that hash table it sits in the cache
then you probe from that side huh you
use the data that's readily in the cache
and then you move on to the next
partition and again you can you can save
a lot of cache misses from that person
so click off we have both semesters
memory
yes yes yes we I mean throughout the
taya soom everything's in my memory yeah
yeah yeah I didn't say that explicitly
enough yeah so what I'm assuming is is I
have everything in my memory we also
assume like a pretty much cotton
oriented storage model so we assume very
narrow our tuples so you saw that here
so we assume in that case in that in
that picture we seem 16-byte Topiltzin
88 but if the one of the sides is really
another time then do it dude get
standing there look at what you can one
of the sides you can have multiple
joints going right right now you have
our journeys to Mt right the adjoining
fitting into the top drawing you have to
keep the result of that also is what you
say
well by the joint is blocking any way I
mean used to you anyway have to enough
to have rns memory 19 house adjourn s
also yeah yeah but you need to I mean
either way you need to keep the either
way uuuuu when you build up the hash
table / / r join us you need to keep
that in memory anyway external yes you
would only keep one of the sides okay
okay yeah okay okay okay okay yeah I
mean this is really assuming you know
memory is cheap just have everything in
memory yeah yeah which i think you know
is increasingly reasonable I mean you
guys are walking on in-memory processing
as well right so and yeah okay yeah once
we do that we indeed basically avoid it
almost all cache misses yeah you still
have some I mean at some point you still
have to bring stuff in cash so so you do
see a bunch of cache misses but not many
and at the same time the nice thing of
this about this partition hash joint is
that after partitioning you end up with
a lot of independent work so no need for
a shared hash table anymore you can just
do the the re 1 join s1r to join s to
you just do those in on separate course
and there's no need to synchronize
through locks anymore yeah he's sort of
as a general additions created well you
can only find matches between here and
here but not between here and here okay
because because you because you hash
partitioned here holy up alright so so
you use it you used your key value as a
partitioning function yeah all coupled
with the same key will go into the same
partition
you use the same partitioning key on
both sides okay so so you will you will
only find matches between you know I'm
gonna say that neighbor and partitions
here and then yeah you can you can run
this joint on one core and this joint on
another core and they won't interfere at
all with each other no need to protect
with locks or anything and that makes
your code even simpler so we end up with
15 to 21 instructions / tupple for
building probe faces almost no cache
misses any more so so this join I mean
we've reduced quite a bit of the cost
here I'm not really I don't understand
exactly what's a different thing the
pictures here in the picture the
previous
is it that each of your like roses now
big enough so that I don't need to
follow an X pointers to vector values
into a bucket is that the main
difference
well you because you you you partition
this stuff here these hash tables are
now hash tables over just a fraction of
the input in practice you don't do just
four partitions you you create thousands
of partitions okay I believe in the same
games well this is just a part of your
table it's just you know the hunt I 4 +
S 4 yes it's actually enough it's enough
to have the hash table in cash so so our
for has to fit in the cache assuming
that the hash table is approximately the
same size as as a table right that the
hash table has to fit into the cache
that's enough so so you break up this
input relation into chunks that are cash
sized suppose you have I don't know 400
megabytes here you have a 4 megabyte
cash you create a hundred partitions and
then for each of the partitions the hash
table will fit in the cache and and then
you can run the the hash join from the
cash keoki mrs. anywhere so the next
slide you said there's equals you well
it's well it's it's basically uh it's
it's it's of course there are you always
do get some right there's always some
compulsory misses and but these are you
know
the during probe everything's in the
cache me after your hash basically have
pointers with her which I left with at
random addresses right after hoschton I
say that these pointers are sequential
yeah yeah but they're all you know this
is small this fits into the cache and
once stuff is in the cache then that
that's what the cash is for right once
you once you keep your all your I mean
on both sides once you keep that
contained in the cache then you won't
suffer any cache misses any more and
even if the TLB also is enough to cover
the entire hash table if the hash table
is small enough yeah but the
partitioning isn't should spray exactly
yes yeah yeah you know that's what's
coming next yeah yeah that's what's
coming next yeah so so what I did not
yet tell you is that partitioning itself
introduces a new cost and that's where
where the TLB indeed comes in so if you
do partitioning then it depends on the
number of partitions that you want to
create concurrently partitions are
roughly cash sized so it's safe to
assume that one partition is larger than
a memory page memory page is just four
kilobytes so so in the end each
partition will reside on a on a separate
memory page so you need a TLB entry if
you want to do partitioning you need one
ptob entry for each of the partitions in
order to write into those partitions and
and we have some experiments for that so
when you start so so when you do
partitioning so these are assuming just
a single partitioning thread now now you
vary the number of partitions that you
want to create so this is the logarithm
of the number of partitions so you
create two to the power
for up to 2 to the power of 16
partitions and what you can see here is
that for a small number of partitions
partitioning can be done very very
efficiently about a hundred million
tuples per second that's because
partitioning is it's conceptually it's
very very little work you just compute a
hash value over over the key that tells
you where to put the couple and then
write it there and you're done it's it's
it's not much work but once you cash mr.
yeah the cache misses are not really
problematic because you're talking about
here let's say two to the power of 14
addresses where you concurrently writing
to so you need about 2 to the power of
14 cache lines right and that's not much
data that's it's it's it's it's yeah
it's it's a few few kilobytes so because
you right into each partition
sequentially right it's it's like it's
like having whatever if you would do
that in a file system huh this is
special
I tell you right into the petition
sequentially but I actual process a
partitioning is not a function just say
yeah yeah yeah but but for each
partition you only need to keep like the
next position in in memory and those
next partitions those aren't many those
are that that's not much data in total
so you can get that in a cash right the
like that for each partition does the
slot for the next couple to insert
that's just 2 to the power of 14 say
slots that's a few kilobytes of space
and you know in a you you can like in
the classical database you can you can
create you can create partitions over
terabytes of data with just a very small
buffer pool because all you need to keep
in the buffer pool is just the next page
to write out for that partition but you
don't need to keep the full partition or
something in the cash does that make
sense
if skeptical still a total number of
mrs. is is the total size of however
many apparently Romans you have divided
by the number of entries that you use
fitness and
cashman and effective
but yeah
but you know the how can you say that
alright saw em you're writing into each
partition sequentially so very likely
you like have for each of those
partitions the area around your current
writing position you have that in your
cash so you don't need to you don't and
then writing there it gets quite
efficient because you just need to write
into the cash and then if if that cache
line is full it's being written out to
memory but you don't even have to wait
for that you can still continue it's
it's it's it's it's right only
this right yeah okay right right and but
that's different like when you when you
when you during a hashtable builder
probe where you randomly put individual
tuples in there then typically you have
to bring in the cache line do the update
and bring up right out the cache line
again because you don't write those you
don't have this sequential pattern
though as you have it in partition I
don't know if you're making sense okay
now the problem is of course we want to
create many partitions because we want
to have those partitions small so they
fit into the cash now the smaller the
better so we can bring them in l2 or
something cash in practice in our
experiments what we would like to be for
that for the data sets we used you would
like to be around here so 2 to the power
of 14 partitions for the workload
configurations that we had this this was
a good choice but it does you know in
this picture it doesn't matter much
whether you know now what can we do
about this and the the classic way of
doing that is multipass partitioning or
also called radix partitioning it's
basically an idea that's been done also
in in this space systems you're limited
by the number of partitions that you can
write concurrently so if you can write
at most say 100 partitions concurrently
you can just do multiple passes and
refine your partitions again and again
and this way you never exceed your
maximum number of partitions /
partitioning paths and still end up with
as many partitions as you want in the
end and that's where the where the log n
factor gets into into hash join right
because now have you have to start
partitioning in multiple passes and the
number of passes is logarithmic in the
in the amount of data that you have
class that included and yes yes it's
true it's true uh yeah it's it's
basically the same operation a hash
table building it's it's true absolutely
figure your pocket is small enough it's
true yes yes yes mr. yeah and if you do
that now it turns out the a deity cost
of an additional pass its additional CPU
work that you do but it saves you all
those TLB misses and as you can see here
for the for the area where we're
interested in it gives in significant
advantage in terms of the partitioning
cost right you can do better even huh
the trick here is that so so this is how
you implement partitioning naively you
take a key hash it and then just write
it where it belongs to what you can do
alternatively and then this is always a
right right and for that you need a TLB
entry for writing that element out to
memory what you can do instead is like
create like a little buffer in between
I've written this as an algorithm here
so you would has your key not write it
out directly to memory but first fill up
a in cache buffer and only when that in
cache buffer is filled up then you flush
out the entire offer slot out to your
petition so you for each partition you
have a little a little buffer now you
fill up the buffer and only when the
buffer is full you flush it out to the
partition what this means is that if you
if you can have something like eight
tuples in each buffer slot yeah you are
you get a memory operation or an
operation that goes all the wallet all
the way to memory that needs a TLB entry
you need that only every eight couple so
you
can can push down the number of TLB
misses and once you have sufficiently
few TLB misses then they won't cause any
harm anymore because then the all the
out of order mechanisms in your CPU will
be able to hide the latency that you get
from those TLB misses you can you can
just keep processing while the hardware
just just does the deal dimas for you in
practice you choose your buffers to
match the cache line size because then
you can do those transfers particularly
efficient mechanism is only as good as
your hashing function right so super
hashing function sprays the laws or all
over then he hasn't quite too long and
we saw some belly yes oh was there any
correlation any case between your actual
data and the hashing function oh well um
on you know we don't have as an academic
we don't have real-world data so we were
dependent on synthetic data anyway right
once you operate on synthetic data and
then you know you can use anything for a
hash function because you know you hear
the ramp yeah you don't have that nasty
type of correlation in your data so we
kind of I cannot really well you're
dealing with this distinct values enough
so what we gonna do take values and you
have all sorts of distribution yeah what
we did do but I claim that's kind of
independent of the hash function right
if you have duplicate values in your in
your data yes we I don't I didn't bring
experiments with me but we did
measurements with with skewed data but
still then you know you don't have funny
correlations between the data that would
affect the hash table so how can you say
that these are new hash is like crc32 or
something each row will go to the
different place so this buffer will be
usable in some sense so oh because no
radix that you use for them for the hash
depends on how many the also you trying
to understand was that any knowledge of
the incoming data to make the hashing
function friendly enough to leverage
buffer or not because we the moment you
generate synthetic data I claim they're
always you every hash function is
friendly enough unless you you you build
very funny I mean very very artificial
distributions where you know if you so
we used zips distribute the data but we
/ muted our alphabet beforehand
otherwise you do get very strange
effects but they don't reflect reality
either Stinky's and they're distributed
in some way so you get fifty percent of
the keys are all the same or 157 kids
wear all this and you have very bad yes
yes yes yes we did do measurements I
didn't bring them with me but I I can
say a few words once we so actually we
will soon see a bunch of experiments
right so once you do this it turns out
that with a single pass you can actually
over over quite an interesting range of
petitioning fanouts you can do with a
single pass even better than with with
radix petitioning which is I think
up to up until now has been always
considered sort of the strategy of
choice so putting this together what I'm
what I'm doing what I did here is I I
show some measurements that we did
comparing this what the Wisconsin people
called no partitioning joint that is and
hardware oblivious join not considering
any caching effects or anything versus
this radix join which does the the
partitioning and all that stuff what you
see is that the partitioning indeed gets
responsible for a large share of the
cost whereas once you partitioned doing
the actual join gets negligible costs so
this is the number of CPU cycles per
output couple of your of your joint and
contrast to that because of them of the
many cache misses of course that you
lose a lot of time in the build and
probe phases of the no partitioning
algorithm yeah this is a benchmark
configuration that we took from from the
Wisconsin paper yeah and to give you a
feeling so on a on our new hello machine
that we that we use it's like a thing it
has four four cores you get in the order
of 100 million tuples per second in
joint throughput gasps I mean drew level
cache size for each was learning the
correlation between cache size and
uh so um I I don't remember no I leave
agency yeah can't you be remembering
numbers being a little tiny bit so I'm
just wondering there's correlation
between cache size this algorithm and
these look well this difference here
comes from the Sandy Bridge having much
more force so so so this is a parallel
implementation and then you basically
this is a measure of wall clock time
kind of yeah so so you have more course
you need less cycles per couple she was
the number of course used for 60 years
ago else we know we are yeah so this was
so so basically we used in all those
machines we used one socket but use that
I mean fully yeah and there was this
question earlier about the impact of
first of all so so to show you that you
know the we we work carefully so we
really try hard to make both
implementations as fast as we could yeah
we think we can show that by showing
that our implementation is roughly a
factor of three faster for both cases
approximately as compared to that
existing paper now what you see here is
that well the the Wisconsin guys based
on the on a picture similar to that
actually they had just those two sides
they argued that the hardware oblivious
implementation is still preferable over
a hardware optimized implementation
because depending on the on the platform
chances are higher to get to pay a
penalty here as compared to the little
benefit you get here
now once you go to a different workload
so this is a workload that has earlier
been used in the literature yeah all of
a sudden the game changes quite a bit so
these are two equal-sized partition
relations and they're the harbor
conscious implementation clearly has an
edge over over the harbor oblivious
implementation the harbor conscious
implementation also scales very very
nicely so the this is a sandy bridge
machine with with eight cores on on one
on one socket and if you start scaling
that up on a four socket machine you can
reach cash throughput join throughputs
of about 600 million tuples per second
there was a question previously about
skewed input data it turns out that the
the harbor optimized version this dis
red X partitioning turns out to be much
more robust toward skewed distribution
so you get about equal performance over
quite a range a wide range of skew
values for so the the the harbor
oblivious implementation is quite skew
sensitive there you typically get an
advantage from skew because you your
caches get more effective because you
hit the same cache line more often yeah
ok so a conclusion from that is that how
we're oblivious algorithms and you know
if you choose the right benchmark they
do look good but in order so we have an
icd paper on that there's there some
more experiments in there and those all
indicate that with
Hubbard conscious algorithm you can
typically do much better and
interestingly we found that these
Hubbard conscious implementations are
extremely robust also to the to the
parameters that you choose so there's a
few parameters in there like cache sizes
and number of partitions and all that
stuff yeah and of course getting those
parameters right might affect the
performance that you see but it turns
out that it's actually the algorithms
are quite robust toward that so a slight
miss setting of those parameters won't
kill your performance at all it doesn't
it doesn't make much of a difference ok
let's then have a look at sort merge
joints so sort merge joins essentially
they're all the cost boils down to
sorting once the data is sorted doing
the join becomes almost trivial sort
merge and then the sorting part you
typically implement as merge sort sorry
for the name confusion yeah that is you
first create some pre-sorted input runs
and then you merge them like recursively
until you end up with it with a solid
with sorted data in practice on modern
hardware yeah I try to visualize how
this looks like in practice in practice
what you do is you if you have a large
system that's going to have a NUMA
memory architecture so here I on the on
the graph I assume you have four Numa
regions what you then are or one way of
doing the sorting then is to perform
first so you assume the input data is
somehow distributed over your overall
Numa regions then you typically do first
local
sorting within each numeration and to
avoid memory traffic across the the Numa
regions then if you want to obtain a
globally sorted result which I'm
assuming here then what you what you
need to do is you need to merge across
those Numa regions as shown here so you
do the the two-way merge here and
another tomb way merge well to two-way
modules here and then another two way
march here until you end up with a
sordid result right huh the ability of
local versus remote you Mike it's being
paid in the second step anyways yes yes
yes yes but you know you want to avoid
you know moving data back and forth or
something alright so here at least
clearly you know if you if that is the
result that you want to have there's no
way no way around getting this data over
here so you need to cross that mean
neumann boundary somehow if that is what
you want as a result yeah we'll get to
that we'll get to that yeah that's true
yeah okay now i was already mentioning
here the the bandwidth that you need to
do the merging right and in fact the
merging is is a pretty critical part in
that in that sort operations if you so
for both parts for run generation and
for merging what you can do is you can
implement them quite efficiently using
cindy acceleration based on sorting
networks which is which is where the
this Intel work said in the future this
this this run generation and merging is
going to become even more efficient if
you have wider cindy registers
just a comment here you know like in in
in half showing partitioning was similar
to to a hash table build up here you
have a similar thing that merging for
merge sort is a very similar operation
to the to the join of the word so there
is a difference in the sense that you
it's tricky for that part to use Cindy
because you your tuples look differently
and that makes it tricky to use Cindy
but other than that it's a very similar
task so what we're now interested in is
is the merging part mostly yeah here I
have some numbers for for both parts for
run generation and for for merging in
the merge sort sense if you if you
implement this well using sandy
operations assuming you you create runs
with with four items each initial runs
you can do this with 26 assembly
instructions and for each of these
assembly instructions you for each of
these and for each so with with 28
assembly 26 assembly instructions you
can sort for four sets of data and then
assuming for byte keys and four byte
values you end up with reading in 64
bytes and writing out 64 bytes so in
total you there is a ratio of 26
assembly instructions to 128 bytes moved
between memory similarly for the merge
pass so you can do merging also with
cindy
err you for every for every four tuples
you are you you spend 16 Cindy
operations and uh and for load store
instructions what that means in the end
is that you do a lot of memory i/o and
compared to the to the very few assembly
instructions and that you're doing for
the photo photo sorting in between so in
practice sword merge-join get severely
bandwidth bound so you're going to be
bound by all this memory traffic and and
i'll get to that there was this this
question about global and local sorting
we'll see that in a bit where this
problem gets even worse when you when
you do it wrong yeah narrator using hash
party
like we're in a hash table like erasers
and creating the partitioning and then
you have the short thickness on the type
values well that's that's a radix sort
effectively right we didn't try radix
sort there is a I haven't like really
verified that yet you can do quite well
with with radix sort as well I don't
have a like apples to apples comparison
between the two group at columbia's is
currently like proposing that famous
creating the hash table as we live in
the Indianapolis has run and then
instead of look up and probe we can just
aha you mean okay ah yeah no no we
didn't try that we didn't try that yeah
so that would be a combination of hash
join and sort of action you know we
didn't try that that's interesting yeah
it's good idea yeah okay so we end up
with both phases actually being severely
latency bound now what can we do about
that now things are still easy for the
for the run generation park so I said
you need to write in and out these 64
byte chunks in in practice that's not
really a problem because what you can do
is you can like you have this merge tree
and you like take entire sub trees from
the bottom do them in one go and then
then then you have no no real memory
traffic so you can do chunks of up to up
to the size of your cash you can sort
them really in cash and then memory
bandwidth is not an issue as long as
you're in cash but for continued is
emerging hmm when you when you want to
have your entire thing sorted you don't
get around you don't get around doing
something about the high bandwidth need
of merging now one thing you can do is
you rather than just doing two-way
merging yeah this is what makes your
bandwidth demand so high because you
need to repeatedly do the merge so you
repeatedly need to basically reread the
same data but at the same time to
emerging is highly CPU efficient because
you can optimize it so well with Cindy
and so on so it turns out the the
bandwidth cost is so high that that it
may
sense to give up a little bit of the of
the CPU efficiency by doing what we call
a multi-way merging thing so what we did
was we do a emerging over I mean an N
way merging operation which internally
is broken down into a number of two-way
merges because the two-way emerges
that's exactly that thing that you can
do efficiently with Cindy right and and
now a single thread basically walks over
all these pairs all these merging pairs
yeah merge is a little bit here merges a
little bit there as long as there's data
there right it's like psychos in between
little buffers and then and then you try
to two discussed is this thread walks
over this merging tree and tries to
produce output right and the idea is to
keep all these intermediate merging
results it is like streams and keep them
within your last level cache and have
that thread again walk over all these
roaches yeah all the authority that we
have to be in a bigger run yes yes yes
do that
well yes so I I think a two slides on or
so so that I think that that kind of
clarifies the question ya know this
algorithm works because if i see that
who's going to go for you below zero
give me a buffer even or give me a
buffett which is bigger over this 1213
same thing right so this transition from
one new man ought to the order by the
same term you're paying lots of gospel
remote memory local which is I mean
effectively what would be as bad as the
latency um well okay first of all um
again as I said previously you cannot
fully avoid crossing the neumann
boundaries yeah so so this thread sits
on one Numa region right but but at some
point at some point you have to merge in
data from the remote Numa regions and by
the way this doesn't necessarily have to
come from separate Numa regions that
just wrote it here at the point here is
that you merge in from multiple sources
yeah now yeah the the thing here is that
yeah you save overall bandwidth because
originally you would have like Britain
out every here the merged result you
would have written out that to memory so
you save overall bandwidth what you pay
is that the you need a much more
sophisticated algorithm that walks over
these these binary merges you have
buffers in between you need to keep
track of the fill level of all of these
buffers that that that causes quite a
significant CPU cost but it saves
bandwidth and since we're bandwidth
bound there's a net saving in the end
seems to be whatever blocking me so
we're waiting on buffers for all my
notes other newman o's are locked
basically nobody second reporters right
i mean if you think of this again i try
to scale it to multiple threats how can
you ensure that both threads are busy
doing something useful without blocking
each other well you're not well blocking
is probably not the right word I mean I
agree while you're you're doing stuff
here there there's no no yeah right but
you have that in the when you just do
binary merging as well like you you you
first do the merging here and then you
do the merging here and on top of that I
think the problem is not as bad as it
might sound so because you started
reading here from that Newman region and
reading from that and then the hover is
going to see your sequential read and
we'll start prefetching right so when
you move over here well that the harbor
is still going to prefetch here and and
so yeah oh no no no thread always stays
on the same that death will go
yeah but that doesn't make the
difference Thank You agency I mean well
maybe yeah but that agency is at least
partially hidden by prefetching at you
you you you merge in I don't know a
thousand tuples from here and from here
huh and then you go over here but the
habari has seen I is just read a
thousand sequential tuples so let's just
prefetch and then you do the merging
here and by the time you come back at
least part of that data have already
been pre fetched so you don't suffer
that full whiten see you see what I'm
saying my point is the local press
remote right accessing local memory
verse X me more than me there's usually
a treat for X ratio that
me and said I don't think a new Moto G
or if it access memory from you and all
three it's got the three or four times
slower than accident from its local ooh
yeah I don't see I don't see your point
so you're assuming that this prefab days
of these machines support prefetch of
things that you can prefetch something
and then and then go off and do
something else while you wait for it to
get loaded at least to some extent
that's what happened of course it's not
you know it doesn't matter magically do
everything right but you know sigh well
what difference was between between
increasing the fan out of a merge versus
simply increasing the depth of the
buffers
um the thing is when you okay basically
that how can I say that if you if you
just uh if you have a low fan out then
you have to repeatedly basically swap
out this stuff to memory again and
re-read it from memory so that's why you
need much more memory bandwidth your
number of remote access is that unit
right it is younger it's going to be
it's going to be that that's going to do
the same no matter what it's the number
of local memory accesses that you think
that I find I'm changes
here you hear you're doing basically
you're you're breeding all your input
data once and writing it out once if you
had done just a binary merge you would
read right we dis and right and then
again read and write so you would go
twice over all your data and that cost
family where are the others well they
end up being in the in the cache of that
threat so that's why you why you choose
your buffers to have a size then that
matches your last level cache buffer
something I don't know of any muscle yes
yes yes yes in no way partition so at
the end we will need the one full sorted
yeah yes okay um
okay one second okay just I have just
one slide that shows the difference that
it that it makes so with in particular
once you scale up to two more threads
then what you see here is that at some
point you really get no benefit any more
from from more threads because because
you just need so much bandwidth you you
completely saturate the that the memory
subsystem whereas with this multi-way
merging we keep scaling the the reason
why scaling stops here is not the memory
bottleneck it's because here that the
machine only has 32 physical cores so so
over here we start seeing hyper threads
and so it's a different effect here okay
so now this question about total sorting
versus non total sorting so there's
indeed different strategies like do you
want a global sorting at all what do we
do about Numa and there's opposing views
on that so one of the views is the one
of tu munich that's which is what i'm
trying to illustrate here so what the
folks from munich do is they take their
input relation and then do range
partitioning so they basically it's hash
partitioning but with you known identity
hash function and you look at a certain
number of bits and and then do range
partitioning and then you know you know
so all the blue all the low low key
values end up here and so on and then
you sort locally in each Numa region
right and then you end up with a with a
fully sorted result now they the claim
of the of the and hearing the in the in
the range partitioning phase you do have
to cross the Numa boundaries right
whether do whether you do this with
merging as I just sketch before although
whether you do this with range
partitioning that doesn't really make it
you do have to cross the Neumann
boundary in achieving these our
complexity
so what I was talking about earlier that
instead of this local sort and
partitioning editor we could just do the
entire hashing and have several buckets
ready to finish
so and the strategy of this MPs em
massively parallel sword merge-join is
to not sort the other relation not
globally sort the other relation but
just do a local sort then you end up
with blue red green and yellow data on
each of the course and then you do the
join as follows on each of the course of
course you do the same thing you you you
do a sequence of joints merge joints
where you merge this guy with that guy
huh and then you merge that upper red
guy with with that red guy and so on
okay so that's one possible way of doing
that we'll we'll see performance numbers
in the minute the one downside of that
is you you don't end up with a globally
sorted result because you you merging in
values from from the same range multiple
times and then those will won't be
globally sorted okay alternatively what
you can do is you can do local sorting
first and then do the achieve of global
sorting with this merging operation as I
sketched before right and then you end
up with a globally sorted result this is
kind of you can either choose this or
you can do the range partitioning and
then the local sorting first doesn't
make so much of a difference what we are
proposing is to do the same thing for
the other relation as well so create
globally sorted data for both tables and
then you can you can compute the join
completely low local so you don't have
to do the merging the merge sort merging
across the Numa boundaries
probably reading stars some can be hard
to get the petition
actually that's that's a good point we
haven't really really measured ask you
problem yet so the with this range
partitioning what you're doing is you're
building anyway for petitioning you need
to first build up a histogram of your
data so you know how large your
petitions are going to be because you
need to allocate them in memory properly
and then based on those statistics you
also can choose your petition boundaries
properly if you're doing something like
that you have to do it slightly
different but at least here you have
sorted data so you know something about
the data distribution and then can
choose your partition boundaries
properly but that's something indeed
where I with skew that's something
that's that's very high on our priority
list it's true there's a pool table
speed and memory right yes
so putting things together so um
previously I was talking about has shown
alone yeah and and there we saw that
this radix join has an edge over over i
call it here in the eve it's a hardware
oblivious implementation here for for a
larger data set that we looked at before
this is the data set that was used by
the two um guys but you get you know we
saw similar numbers previously also for
smaller data sets sort merge in
comparison to that what i'm showing here
is to sort merge implementations
basically it's the two strategies that i
had on the slide before it's the EMPs em
strategy and the the one that uses total
global sorting so this is mps m and this
is a global sorting so our experiments
show that global sorting is actually
advantages in the end oh sorry oh ya not
to label my axes that's throughput no
that's million output couples per second
sorry oh yeah it's good point yeah no
sorry yeah this type of doing how many
rows qualify or auto Spotify or yeah you
have it's like there's a one-to-one
match earlier okay so what you can see
here is that these two implementations
don't yet use cindy acceleration
introducing simdi brings quite a bit of
an advantage and what we found makes a
real difference is this multi-way
merging so saving bandwidth really in
the end helps you quite a bit and well
implemented what we are seeing in our
experiments is that sword merge gets
close to hash but it's not quite as fast
as as hashim we previously have the
situation that you have to be a little
bit careful about the data sets where
you are where you are reason over so
this is the experiment that i just
showed before huh if you take a
different data set the picture does
change so this is the data set that we
saw before gigabyte joined with the jig
abide um here the well this is this is
to the advantage of the House John one
thing that I would like to note when
when you I've seen several papers and
talks about all this topic I was often
surprised how confusing you can present
results so what I'm showing here is how
nicely you can you can fool and readers
so in for a joint it's really unclear
what what is your what is really your
metric do you choose do you do you so do
you do you measure input tuples or
output tuples per second and most of the
papers chose to use output tuples per
second the t um paper chooses no
actually i did something wrong here no
no this is output and this is input on
so this must be this person must be
output and this must be input sorry so
depending on what your metric is yeah
you get really different scaling
properties so what what's being varied
here is that you keep one relation
constant and you increase the size of
the other relation so you change the
relative size and if you if you count
output topples per second which was the
metric on all the slides before and in
most of the papers then you seeing these
characteristics and if you count input
tuples per second you seeing this one of
course input doubles these are more than
output tuples so that's why you get
higher values here and when you sit in a
talk and see something saying someone's
saying is this
nine hundred thousand top 100 million
couples per second be careful what he's
talking about they should use abs oh I
guess yes this is actually a year to
have one huh d X 1 X uber yeah i bx 2 is
not not yet available right yeah yeah so
we're actually looking very much forward
to to try out on on hassle yeah yeah
yeah we don't have we don't have
hardware yet but we're very much looking
forward we actually play it around a
little bit there was this question
before about if sort merge starts to
become attractive for joints what about
the other operations such as aggregation
so here is terms of what I'm comparing
here is the state of the art Platt
algorithm for which is hash based for
aggregation and you see the throughput
depending on the number of distinct
groups for a group I operation for for
few groups hashing is gets very cash
efficient but at some point it just gets
expensive uh here we see that sort merge
is still you know depending on the
cardinality it can be significantly
slower but it is is it is very very
robust I mean this is something that's
that's I guess known also from from
classical databases and these are two
implementations one does just a plain
sort and then aggregate and the other
one does partial aggregation in between
and when you see in intermediate runs
you have sequences of the same values
you can aggregate them right away
a plant does that to some extent yeah
yeah okay with that let me conclude so
basically I argued about two things one
was about our how very conscious
optimizations worth the effort and this
is what we what I focused on a little
bit on the hashing side and our current
results indicate that yes it is really
worth the effort there they're much more
robust than you might think and they'd
simply faster we found that hash join is
still the faster alternative but stored
marginal is getting close what is like
how to amend to evaluate numerically is
that sort merge has this nice property
that it produces sorted output which
might be useful for upstream query
processing right so so that might be
another argument for sort merge joins
what we're really looking forward to is
have a look at Future hardware as well
has been announced also with I think at
least gather support so in the simile
registers you could read from from
multiple memory locations with one
instruction which of course savers
hashing so we have and we're going to
see wider cindy which favors sort merge
but we might also see functionality in
that direction and what does that mean
to the relative performance with that
and I'm oops good okay with that my app
crashed sorry yeah I wanted to say thank
you to two in particular to Chari the
PhD student who did that questions I
used up almost all the time sorry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>