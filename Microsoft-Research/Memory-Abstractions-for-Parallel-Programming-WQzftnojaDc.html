<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Memory Abstractions for Parallel Programming | Coder Coacher - Coaching Coders</title><meta content="Memory Abstractions for Parallel Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Memory Abstractions for Parallel Programming</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WQzftnojaDc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone thanks a lot for coming on
the sunny Seattle morning today so I'm
modern Maserati from the research and
software engineering group I'm it's my
pleasure to introduce Angelina Lee today
she comes here from MIT she just
finished her you know PhD and she's beat
she'll be joining as a postdoc in the
same group starting august right yeah
and so her interests are broadly in
parallel all aspects of parallel
programming you know programming models
programming languages runtime systems
and she likes working across the stack
both on the theoretical foundations and
on and on engineering problems so
Angelina thank you wow that's a really
nice introduction Thanks okay so let me
just ask who is in the fear of parallel
programming Wow okay so almost everybody
well I have some background material
just to give you the context of my
research but you know given that pretty
much everyone is in the view of parallel
computing I'm just gonna fly through the
first few slides okay today I'm going to
talk about memory abstraction for pillow
programming but before I get into that
let's say we are in the era of
multi-core right I think there's no
doubt about that so these are just like
a list of top selected top-rated
personal laptop that I picked from the
newegg com and what you notice that was
a few hundred dollars you can get a
personal laptop with multi-core and I
think that's pretty striking because
right now basically multi-core is a
commodity hardware and so how do we get
to this point well let's look at the
revolution of processors Moore's law
which says that the number of
transistors on chip doubles every two
year is still going strong ever since 75
however at some point the clock
frequency sort of plateaued and really
the power is the Rue fundamental cause
of all these because we have reached to
a point where we have reached the power
density that the device can handle we
just can't call these devices and
therefore we can no longer keep on
increasing the clock frequency so as a
result you know the chip vendor instead
of increasing the clock frequency and
double the frequency every two years is
now you get double the core every two
years um so as a result what that means
is that now we have to repeal the
program in order to unlock the
computation power provided by the modern
hardware but unfortunately writing
parallel program is not as easy as
writing serial code and I'm sure you can
all sympathize with that and I believe
to help writing parallel program easier
a concurrency platform can help and what
a comparison platform is it's a software
abstraction layer that runs between the
user application and the underlying
operating system that provides a
linguistic interface for the user to
specify the parallel parallelism of
their application and the software
abstraction layer basically handles load
balancing and task scheduling for the
user code okay um and many researchers
are interested in the problem of making
parallel programming easier and as a
result they are a lot of concurrency
platform being developed also include
out the TPL from Microsoft and ppl from
Microsoft and you know this is just a
list that i put together since 93 but
before that there's also a lot of other
parallel programming language that idea
is include these are just I guess the
modern concurrency platform okay so why
does concurrency platform help because
it provides the parallelism abstraction
the job of task scheduling and low
balancing are abstracted away and handle
automatically for the user application
and which brings us to the topic today
memory abstraction so I believe um
properly designed memory abstraction can
help ease the task of parallel
programming so what do I mean by memory
abstraction well memory abstraction is
an abstraction layer that runs between
the user program and underlying memory
such that it provides different view of
the memory depending on the execution
context in which the memory is accessed
so just to make it a little concrete for
example transactional memory is
consider as a memory abstraction because
memory accesses dynamically enclosed by
an autonomic blah appear to occur
automatically okay so in this talk my
thesis is probably design memory of
tractions help ease the task of parallel
programming and I'm going to demonstrate
that point by showing you two case
studies the first one is cactus sac
memory abstraction which is a cactus
stack that interoperate with linear
stack and the second one is memory
MapReduce or hyper object it's a
linguistic mechanism for avoiding
determine C race in the multi-threaded
computation ok any questions so far so
this is the outline of my talk today I
will mainly focus on the cactus stack
memory abstraction and then I'll briefly
touch on the memory MapReduce or hyper
object enjoy concluding remark ok so for
a concurrency platform I believe they
are three desirable comes criteria that
a concurrency platform should provide
the first one is what I call serial
parallel reciprocity and what that means
is that the parallel co should be able
to seamlessly interoperate with serial
code including legacy binaries and the
second one is it should provide good
performance and what that means is that
if the application has impaired ilysm
you should see near perfect linear
speed-up when you run the concurrency
platform and finally the last one is
bounded stack space which is to say that
when you run the computation in parallel
it's just consumed reasonable amount of
space compared to the serial execution
well why are these three the desirable
criteria I like to demonstrate the point
using a cartoon we have an engineer and
then we have a customer well the
customer won't like to paralyze the
software and then junior says sure use
my concurrency platform well if today
the engineer can only satisfy two out of
the three criteria than what he needs to
say as well you need to say it just for
he decide to forego the SP reciprocity
which is say interoperability between
parallel coin sirico they need to ask
the customer to recompile all his co
base well but that may not work so well
because the customer
use third-party binary you can recompile
source because he doesn't have the
source so if that's not an option maybe
the next one to forego is the space
usage and so what the engineer might say
we'll update your rim because I don't
know how much space you're going to
consume I don't think that will go too
well so the last one to forego if that's
also not an option is basically
performance um but well if you can get
the promoters writing parallel program
then you might as well just write the
serial code so I believe these are the
three desired criteria and we really
want to operate in the space where we
get all three criteria okay but it turns
out that there seemed to be a
fundamental trade-off between the three
criteria our group and the practitioner
in the field actually thought about
different strategies to build
concurrency platforms that can satisfy
all three criteria but it turns out that
many on strategy that we can think of
satisfy only two out of the three
criteria except for the last strategy
which is what I'm going to focus on
today and so the cactus stack problem is
basically the problem of how to satisfy
all three criteria simultaneously okay
so now we know what the cactus had
problem is let's see why we have this
problem well so if you think about it in
Ciriaco people will typically use a
linear stack and the execution of a
serial language can be thought of a
serial walk of the invocation tree so
here on the left I'm showing you an
invocation tree where a Cosby and call
see see calls d and e um these are the
view of the stack when the function is
active so when bees active is sees a and
B an exact when these active it sees a
CD and the linear stag works really well
for the serial program when you call the
function you allocate a stack frame when
they return it pops it off and so
essentially if you think about it this
is very space efficient because b and c
occupied the same place in the stack and
the parent and child are allocated in
the contiguous memory address space so
that's the linear sac and furthermore it
follows the rule pointer where the
parent can pass a pointer to its
children but not the other way around
okay so that's the linear sec so what's
the cactus ack ack ack the sack is
essentially just like a linear stack it
follows the same rule for pointers
except for now i need to support all the
views for sac for all the functions that
can be active in parallel so in this
case I'm showing you the same invocation
tree except for i'm using red arrow to
indicate that these functions are
spawned i haven't defined spawn yet but
basically what it says that B and C and
its sub computation can potentially
execute concurrently in parallel and
what that means is that the linear stag
no longer work because B and C now can
be active at the same time but they
cannot occupy the same space so linear
stack is no longer sufficient okay right
so that is cactus stack now obviously
people has been building Carol languages
for many decades so what do they do well
in many parallel language when they use
the cactus tag they basically built a
heap base cactus stack what that means
is that when a function become active
you allocate the frame in the heap so
the parent in the trial are not
allocated in the contiguous space but
rather you have a pointer found child
pointing back to the parents so you know
where to return to and this including a
silk 5in so + + + was this strategy good
time balance space-bound can be obtained
but it failed to interoperate with
legacy ciriaco because now you have to
follow those heap linkage where the call
return is performed via the freight
allocating in the heap and when you have
a piece of legacy serial code it does
not understand the key base cactus stack
is still allocate things on the stack
and so therefore you fail to
interoperate and so as I said many
strategies I would try to think of
failed except for the last strategy um
and I'm not going to get into all the
different strategy but one thing I'm
going to say is the main constraint of
all these of all these strategy is that
once the frame is allocated
the phrase the location in virtual
memory cannot change and that's a
problem okay so that is the cactus that
problem and next I'm going to overview
silk in which is a system that we build
to basically soft a cactus stack problem
okay so first of all throughout this the
rest of the talk I'm going to use the
program model in silk here i have a very
naive way of calculating creeping ochoco
where you just do filme minus one fit
ministry and then some the result
together and obviously fifth of minus
one and five minutes we can compute in
parallel so to write this naive fib
computation in silk the way you do it is
basically you spawn fib n minus 1 and
what's tall state is that the name child
function may execute in parallel with
the continuation of its parent so in
this case the continuation is the call
to fit them on a suit so in this case
fibbin minus one can execute ma execute
in parallel was the call to fit that
minus to sync on the other hand is the
counterpart of swamp it serves as a
local barrier which states that the
control cannot pass beyond point until
all previously spawn children in this
lexical scope have returned and note
that these skilled keyword grant
permission propeller execution they do
not command parallel execution what that
means is that the you programmer
specified the logical parallelism of his
application uses these keywords and
underline runtime system will schedule
them accordingly and whether the fit
moment of silence amendments to the
troop ilysm gets realized or not during
actual execution depends on the
underlying resource you have and the run
high scheduler was schedule accordingly
ok any questions so far so what's so
game well so Kim is basically a work
ceiling runtime system based on silk
that solves the cactus stack problem
using threat local memory mapping so
what's very local my memory mapping
t 0 mm CLM M is a virtual address range
in which each thread can map the
physical memory independently so what
that means is that in this range all the
threat sees the same virtual addresses
but they can potentially map different
physical page in that region and so the
high-level idea is that at the system
startup this is the stack where you have
heap sac and data and the run I system
will allocate the heap and the data and
the code in the share region but
allocate the stack in the TL mm region
where each threat can map independently
and a very high level and this is the
this is what the solution looked like
and right now bear with me was
unreasonable simplification let's assume
that we can map with arbitrary
granularity I'll come back to this point
later so we have three worker we have
again the same a vacation tree and say
p1 is now execute actively executing on
be in pjs act effectively executing md
and p3 excessively executing on e what
you can do in the terminal region is
that now you can allow the thread to
share a frame by mapping the same
physical page for that frame in the same
virtual address and in this case say for
example if you want you to share the
sack prefix of a they can map the
physical memory for a at the same
virtual address and map the actual same
physical out the same physical page on
the other hand they don't share b c and
d so they would map different physical
memory after freeing a and was that
because a is allocated at the same
virtual address for both read if you ape
has a reference down to its descendent
for both worker they will see the same
virtual address and when they do
reference that virtual address it will
actually point to the same physical
memory okay um any questions so far
okay I think I'm just gonna yeah that's
a good question Oh get that like
although much later so um if ya if i
don't answer your question buddy in the
top just a questions only i know i
understand it so suppose in p1 i take a
stick address of a local variable in B
and I secretly pass it along to p2 it
will really make the different like it
will go over all right that's correct
that's right tonight okay I
understanding it nice okay although I
have to say in multi-threaded
programming you shouldn't do that
because that's that partying say is this
that I really understand it another
critique because I coached in Crete x
tekken vs having the first place yes so
yeah your understanding is correct okay
right so this is the high level idea and
was Tina and based tech to stack we
guarantee time by myspace now but before
I get into the time mountains basement
of a TL mm base cactus stack I have to
first go over the time and space man of
heat base cactus stack so in silk using
a heat based cactus stack say we call TP
as the execution time on p processor
then t1 basically equates the execution
time on single processor which means
that's the total work of your
computation on the other hand T infinity
you can think that as the execution time
if you have infinitely many processor
and so what that translates you is the
span or some people call critical path
of the computation which is the longest
dependency chain in your computation so
we define parallelism to be t1 / t
infinity on the other hand with a stack
space we call SP as the sack space
consumed by P processor and s1 is the
sex based concern we execute serially on
one workers and was that using a heap
base hack to stack so guaranteed the
following time Milan space bound first
the time was TP equal to t1 over P
Plus big o T infinity and what that
balance tells you is that when the
number of processor you execute is much
smaller than the parallelism you have or
ie your application has ample
parallelism then the first turn dominate
and therefore when you execute a few
processor you should see near perfect
linear speed-up on the other hand
guarantees Spade spell that states that
each worker does not use more space than
s1 but with CPAs cats a sec it does not
support SP reciprocity now with silk am
using t LM base cactus stack to define
the time and space on spot space Pam I
have to use an additional parameter
called silk depths silk depths is the
maximum number of silk function
necessary on the stack during a serial
execution so here when I say silk
function I'm in a function that can
contain spawn in sync meaning that the
function is self-contained parallelism
so note that the silk depth is not the
same as spam dibs for example here i'm
showing you an invocation tree where I'm
marking the parallel function the silk
function in blue and I'm marking the
regular ordinary serial function in
green and the red arrow indicates pong
and the black arrow indicate call in
this case the silk depth is three
because you have at most three sub
function nested on the stack ie acd
under your transponder Ephesus only to
where see Spahn speed these phones f so
sup dips is define us the maximum number
of sub function that's there on the
stack during serial execution ok so this
is the silky I'm guarantee now in terms
of time bound instead of big o T
infinity we have this term s 1 plus T in
front of the T infinity and so what that
means is that with this bound your
application need to have a little bit
more parallelism in order to get near
perfect linear speed-up which is this on
the other hand with a space bound we
have a additional Plus D term and right
so one thing I forgot to mention here we
are always measuring the stack space in
terms of number pay
so in this case we have a additional
plus D term and but by giving up just a
little on the timeline of space bound
why you get back is serial parallel
reciprocity which means that the
compiler no longer need to distinguish
function types at the COS I and whether
you have parallelism or not is dictated
only by how a function is invoked so you
can actually spawn both a sub function
or c function or you can call a silk
function or a c function and this was
not possible in sofa or in the in silk
with heat-based cactus stack ok so we
implemented this we basically implement
a silken prototype based on the open
source took five runtime which supply
was the one was he based cactus stack
and TL mm well traditional operating
system does not provide such
functionality st relevant so we also
modify open source Linux kernel to
provide support for T element it's not a
big modification about 600 lines of code
and we ported the runtime system to work
with Intel's silk + compiler what that
means is that we can actually compile a
piece of C or C++ code that contains bye
NSYNC and it will run our a red line
system okay so let's take a look at the
performance and as I said earlier the
silk silk silk m has the time bound of
this where the sea is s 1 plus D and we
compare that with silk plus so silk plus
is also a silk based from my system
developed by Intel and um it the runtime
system has time-bound that's that's like
the heat based cactus stack and so when
you look at the performance here what
I'm showing you is I have a list of
application and I run each application
10 * and take the average with standard
deviation less than few percent and I
take the silk and running time and
normalize that by the sub + running time
so what that means is that if it's below
1 that means so can runs faster and if
it's a bowl 1 that means so plus runs
faster
and you can see that the performance of
the two are actually comparable in terms
of space consumption and now this is the
time bound s 1 plus D so again we use
the similar range of the same set of
benchmark and we have the first column D
which is the soap depths I just defined
and s1 this is the sack space use your
during serial execution measure in
number of pages and this is what the
bound predict but the battle is actually
kind of pessimistic and in practice when
you run in parallel these are the actual
SP over s P over P the number of pages
on average use per worker when you run
on p workers and this you can see the
amount of page views per worker is
willing to x factor compared to the
serial execution of cross benchmark even
though the space down predict that you
will use much more and later as I
describe how each of the component work
you will see why the BAM is kind of
pessimistic but yeah okay any questions
okay do you have comparison of the space
consumption between sodium in the vase
so plus um no unfortunately I do not
have that comparison but that's a good
question it's um at the time when we did
this work there was actually the run
high source for silk plus was not
available um so we couldn't like it was
an open source but now it's actually
open source but at the time was it so we
didn't we weren't able to take it try to
instrument that
okay um oh but to answer your question
we do actually have a space-bound
comparison number was silk five which
uses a heat based cactus stack which has
a better time bound and the space usage
that we have is comparable I don't
remember the number off the top of my
head but I can show you later after the
talk okay so um right so that's the sale
game overview and now I'm going to go
into a little bit more detail on each of
the component on how things work so the
first one is the silk and work stealing
scheduler okay so in a work stealing
scheduler during execution each worker
maintains the work deck of frames um it
basically keep track of what function
the worker has responsible to execute
and for the most part for the most of
the time the work of manipulate the
bottom of the deck just like a linear
stack so for example a very a worker
execute a call it pushes the conference
on to the bottom of deck on the other
hand if you execute a small it pushes
this palm print onto the bottom of its
deck and here I'm marking the frame
using how the frame is invoked and
obviously all the worker can do that in
parallel on the other hand if a worker
does a return a pop the frame of the
bottomless stack so as you can see for
the most part of workers behavior
operating on the deck just like linear
stack however the workers behavior
diverges when a work when a worker runs
out at work to do well in this case the
green worker has run another work to do
so it randomly choose chose up a victim
and steal from the top of the victim
stack so in this case say the green
workers happen to choose the blue
workers to steal and steal from the top
of the deck okay so can anyone tell me
why the green workers steal two
fragments there the one in this case
checking if you're actually paying
attention
in this case yes you're right that when
successful still occurred that's when
true parallelism B is realized
yes their continuation of the spawn is a
candidate for parallel execution
continuation call miss basic units
control that's correct yay someone's
paying attention okay yeah so well I'm
nothing the rest of your noms okay okay
yes you're right in this case because
the second frame is caught and what that
means is that the parent cannot be
resumed until this function return and
therefore when the green workers steal
it has to see a bowstring together okay
um right and so now the green worker
successful steel and it can continue to
execution as you pointed out this is the
point when the true parallelism is
realized okay and the theorem from bloom
bloom finalization states that was
sufficient parallelism workers steal
infrequently and you get near perfect
linear speed-up okay so that's the gist
of orc ceiling scheduler the next let's
look at t om base cactus tax see how we
maintain a cactus stack on top of this
work ceiling run high system so again
bear with me so we are using the
unreasonable simplification and I have
the same invocation tree ace tambien see
C small D and E and here I'm showing you
three workers and I'm showing you the TL
mm region of each of the worker so
during execution the worker basically
used the stack like a linear stack and
so p1 execute a and subsequently spawn
be Peter comes in and decide to CEO and
it happen to steal a because pretty much
that's the only thing available to steal
right now and when P to steal a in map
are you not copy the still on prefix in
this case the solar prefix is just frame
a to the same saying virtual addresses
so it mapped the physical memory
correspond to frame a aligning the same
virtual address as it what it is in p1
space in its own TRMM region and then it
continue execution and at this point it
just operate on it like a linear stack
p3 comes again and still see probably
speaking it should be stealing a failed
to make progress do again but let's just
for
sake of simplification let's say it's
TLC on this so is CL C from p2 in this
case the stolen prefix correspond to a
and C so in map the physical memory
correspond to frame a and C in its
interior region aligning at the same
virtual address as in p2 space and then
resuming continue execution ok so again
just to emphasize because they are
mapping the same physical memory and
aligning mapping at the same virtual
address when you have a pointer of local
variable it will realize to the same
virtual address all workers and when you
dereference it it will point to the
bright physical memory so now obviously
one cannot map it arbitrary granularity
so let's see how we address this note
oops yeah so here again I'm showing you
three workers and I'm showing you the
TMM region and let's say I'm using
vertical horizontal line to denote a
page size boundary so again p 1 executed
be and in this case when Peter comes in
and steal and map the page corresponding
to frame a into its own TMM region and
since a worker can only map at the page
granularity inevitably it's going to map
part of the frame be and so it needs to
be careful not to clobber frame be
that's executing on p1 so to do that
before p to resume basically it events
its stack pointer to the next page
boundary which creates some fermentation
and then they continue execution
similarly for p3 if it's still see again
and map the pages where a and series i
down into its own material in region and
again inevitable is going to map part of
b and d so again before it resume
execution and need to events the stack
pointer to the next page boundary which
further create additional fragmentation
so in silk m we use space reclaiming
heuristic where we reset the sack
pointer back to where it was once a
function sync because once a function
successfully sing you know there is no
longer any parallel sub computation so
you know you can reclaim the space okay
so that's how the resonance is to
maintain to your man based cactus stack
and let's look at the analysis how we
maintain the ban ok so again let's
review the heat based cactus stack where
it basically state that let us well me
the sex phase required by our execution
and spp the space required by parallel
execution using a heat based happiness
stack each worker does not use more than
s1 amount of space and the gist of the
proof is basically saying that the worst
thing run high system maintains what we
call busy lead property which is say
that every active frame and when I say
active is basically a frame that has
been invoked and has been allocated
every actively frame has a worker
executing on it and so what that means
that you can and most have P active
paths from the root of the computation
to leave and there is one worker on
every leaf and from the root to the leaf
you use at most s1 amount of space so
the entire computation each worker use
and most s fundamental is faced with the
silk game space bound it's very similar
except where you have this plus P plus D
term and this is because we do also
maintain the busily property but because
every time when we have a successful
steal you have two events the stack
pointer to the next page boundary so in
the worst-case scenario if every silk
function that realized the silk depth D
term gets stolen successfully then you
have that many number of fragmentation
in your stack and so therefore you have
this additional Plus D turn um but back
to the number as i said in practice it's
actually using much much less space
because the bama is kind of pessimistic
for one when you have successful stio
you are mapping the same physical make
physical page so the battle is actually
we'll counting those pages and
furthermore the worst case scenario
described by about that derived amount
rarely happen in practice because when
you have em for parallelism most of the
time the steel corresponds to the near
the root of the computation and then
each worker just execute okay in terms
of performance bound on the other hand
using a heat based at cactus stack this
is term and it's just a step at every
time step the worker eager do work or
steal and after counseling round of
Steel you will successfully steal and
make progress on your critical path and
so in this found the constant hidden
behind the big o roughly correspond to
the overhead that you have for
performing a successful seal and using a
heat based cactus stack that overheads
just switching a few pointers um but in
the soak in a bit in silk and using tlm
based Hector stack because every time
when you events when you have a
successful steal you have two events the
stack pointer you have to do the page
mapping and in the worst case you have
to map s 1 plus B number of pages and
that's why now the sea turn is no longer
a constant but rather a function growing
was s 1 plus D but as I said the worst
case scenario rarely happens in practice
and what the color says is that now you
need to have a bit more parallelism
meaning you have this additional turn
here in order to get near perfect linear
speed-up okay any questions so far
broccolis she's still it an unknown
place you can beat me
so Paige ah so recall that in the works
healing algorithm we always steal from
the top of the deck from the victim and
what that means is that you're always
stealing the oldest frame executing and
when your application has ample
parallelism what that means is that if
you imagine your computation tree your
ceiling near the root of computation
tree and then the each worker just do
work for the rest of the time because
whenever you see you you see a big chunk
avoids the beginning
um yes and it maintains a yeah well we
can talk more after yes um right okay so
that's the analysis of so cam and now
I'm going to tell you more about how we
maintain support for TL mm so when we
start this work they are two possibility
we can either make a worker a process
what that means is that every worker has
its own page table and by default
nothing is shared because in operating
system traditional operating system and
we have a process each process has its
own address space and what that means is
that you don't have to make change to
the operating system but since these
concurrency platform typically execute
applications I'll student to run on
shared memory now you have to use a MF
the running system has to manually do MF
to allow you to share the rest of like
the heap and the data and the code and
that's not so ideal because when a user
call when user application does
something to call em map the runtime
system actually have to intercept that
and map call and do the same
synchronized across all the page table
of all the worker and do the same
mapping in every page table and this
actually happens more common than you
may sink think because in say library
implementation or julep see when you
allocate memory that's larger than like
128 bytes it may do it may do it under M
map underneath that the user code is not
even aware of so we took the second
approach which is say we just make the
worker as a threat which is the original
abstraction and what that means is that
workers here is a single page table and
they share the entire virtual address
space by default everything is shared
and what that what we need to do is
basically we need to reserve a region to
be mapped independently but using the
strategy user called a map can operate
correctly okay so how do we do that yep
I explain a little bit to change the
appearance um well because traditionally
when you have multiple thread within one
process they share one single page table
and the entire address space is shared
so you cannot have this abstraction
where each thread sees the same virtual
address range by math different page
yes but the mapping of what physical
page map at what virtual address is
handled by the operating system French
which has changed
right like the way dr. Azis it's for
virtual address space that's handled by
the operating system
okay so how do we support that well
ideally this is what we like to do
ideally what we like to do is take a
region that's map independently in that
space we can call it the TMM region and
the rest is just shared so we will have
to suffer page table for each threats
where one page they will manage the
independent region and the utter manage
the share region so when you access in a
dress if it falls under t om and the
thread will access its own like private
page table if it's full under the share
region it will access the share page
table but that doesn't quite work
because in some hardware the hardware
walks the page table and each threat
must have a single root page directory
so you can have this true supper page
table and so what we did instead is we
allocate a unique root page directory
for each thread and we reserve one entry
to be mapped independently and however
if some texts some address in the share
space that costs the share region to be
initialized and mapped to the sum of
some other page then we synchronize the
root page directory across all threat to
point to the same thing and so what that
means is that you must synchronize the
root page directory but that does not
occur often because one entry in the
root page directory correspond to a huge
space in the only time you only need to
perform synchronization at most one
synchronization / / root page directory
entry when its first initialized so the
synchronization does not occur often and
once it's initialized it it just access
it any question yes you just in the
previous life you said oh you know
processes don't work because we become
to MF and threats don't work because we
we've just one base table so what do you
do here because now are you saying oh I
can't have multiple bait stages pretty
frigid right so you're shooting
processes here or are you too ah okay so
um each threat can only have one rule
page directory so what we did is that we
have um so in the base it's really an OS
thread right each if each there is an OS
thread there's only one good faith
there's only one page table which means
there's only one group page director and
when I say rapace director is this so
this is like one page table right that's
the root page directory right so to
enable to your attraction we basically
each thread they'll have single root
page directory but they are different
route page directory across the threads
and as same as having different page
tables for a single process um so in the
sense if there's a context which in one
friend to another thread the same
Francis you would have to change the
page
that's correct you I have to flush your
Tod it so how how do you do it like
honey like I'm wondering what is the X a
point this is running only limits or can
I run this a windows or just running
your qualifying go is um so we did
modify the OS this is only implement on
Linux I think you can do the same thing
in Windows now get it so you did
multiply the ways Oh can do multiple
threads in one process yes okay now
again I thought maybe you actually had
some trick I was hoping after this this
slide you would say like oh we did the
trick so you can expand do it anyway
before I see okay yeah sorry no no how
are you took the Sepoy no evidence it to
me so I see so it is a big throw bag
right to modify the operating system yes
mmm some may argue that but yes customer
might but um I do tobik it's okay
because I get like amazing speed ups
right um well so for one it allows you
to satisfy all three criteria
simultaneously and furthermore I think
it's actually a very interesting
abstraction that the operating system
can provide because traditionally you
know you either have threat share
everything or you have processes that
nothing is shared well why can't we have
something between where you have
partially shared and partially private
and I will demonstrate in my second
study we actually also use this
abstraction from the operating system to
implement a different linguistic
abstraction and other people have
studied different linguistic abstraction
that this seems to be a really useful
abstraction for the operating system to
provide I think that this dude with
other applications of in are having
different page
right domains within a single process
yes and are you aware of prior work of
making up popular systems community
people trying to do this for
appointments so I am aware of different
application where this would be useful
but they all did it by basically
creating workers processes and then do
the manual mm and map but right like if
you have to do that to implement some
abstraction then this will be really
useful then you don't have to do this
whole manual sharing in okay so um right
so this is human big space cactus stack
but there's one limitation we're
basically we do not work for coats we're
require one threat to see another threat
stack which is done I forgot his sorry
what's your name again then right then
surly a point like well if you've has a
local very reference to local variable
on the stack from one threat to the
other threat it doesn't work right um
okay so in summary so cam is a sea-based
concurrency platform its satisfy all
three criteria simultaneously and we
basically to allow that we basically use
the TL mm base cactus stack memory
abstraction on and we also to on top of
that we use legacy compatible linkage to
allow the interoperability between
parallel command serial code okay so
that's the cactus sac memory abstraction
on next I'm going to just briefly touch
on the memory map reducer hyper object
because I think I'm running out of time
okay so reduce our hyper object what's
the reducer hype object it's a useful
linguistic mechanism for avoiding
determinacy race in the dynamic
multi-threaded region so what I mean by
that well let's see a piece of code say
you have a piece of code you do some
computation and store the result
computation into an array of strings and
then later you want to concatenate the
string together in the same order from
index 0 to n and if you want to paralyze
this code naively if you just paralyzed
I say let me just run all the iterations
in parallel there's a determinacy race
because
now logically parallel sub computation
can access the same variable and at
least one of which is the right and in
this case every single axis is the right
so it's a determinist erase and the
result as a result the output stream is
non-deterministic you no longer have
this guarantee of the combination is
from index 0 to n okay and so a reducer
is basically linguistic mechanism to
avoid exactly that to use reducer you
basically declare re s the final output
string as a reducer instead of just a
regular string and now you no longer
have determinacy race and furthermore
the a poetry reducer is deterministic
it's the same alpha as the sequential
execution provided that the operator
that you use is associative okay so
conceptually how does it work well
during parallel execution here i have
three worker and say the loop iteration
is divided across three workers and
essentially conceptually what the
reducer does is that it create a local
view for each worker to accumulate its
update on and at the appropriate point
these local view gets reduced or
combined together in a way that produce
the same result of serial execution so
the result is determinant provided that
the operator is associated and in this
case it is string concatenation is
associated and note that I just need to
be associative it doesn't have to be
commutative okay and so ensure the
reducer allow different logical print
branches of the parallel execution to
maintain coordinated local views on some
non local variable okay so with memory
map reducer the high-level idea is that
given that the premises reducer is to
have local views for some non local
variable and you coordinate the axis
between them and combine in at the
appropriate moment wouldn't be cool if
you can leverage the virtual address
translation provided by the underlying
hardware to provide this mapping the
reducer to different local view for
different worker and so that's what we
did
and comparing to the existing approach
using a memory map approach we get fast
faster reduce or excess time every time
you access the reducer it needs to be
mapped to some local view and this axis
is now much faster it has parallel
overhead what that means is that the
application can potentially scale better
and finally it's simple around time
implementation actually which is always
good news for me ok so that's what we
did and here I'm showing you so we
implement this in soak em and this is
showing you the overhead normalized to
l1 cache so using memory map producer
each access essentially translates you
to memory access and predict and a
predictable branch so when you measure
that against when you normalize that
against one memory access to l1 cache
it's about three times overhead on the
other hand when you use the existing
approach it's actually about 12 times
overhead and just to show you a
supportive comparison if you use the
spin lock to protect the access variable
it's over 12 times overhead ok and to
implement this they're actually four
questions that we need to address one
will operating system support that's
required to do the memory map reducer
how do you support reducer with
different type sizes and lifespan and
how do you support a data structure that
we need to use for reducer that allow
you to do time come sometime look up an
efficient sequencing and finally how can
you allow efficient access from one
workers flow review by another worker
and I'm not going to go into detail of
these four questions and the only thing
I'm going to focus on this what
operating system support and the trans
out that TRMM is actually very good for
that and obviously the idea is that
allocate local view for a given reducer
for each worker in the tailbone region
at the same virtual address and so now
in soakin in the tail mm besides the
stag we also allocate region for reducer
for that purpose right ok so that's
the gist of memory my producer um I'll
be happy to talk about it if you have
questions on that okay so concluding
remark so recently researchers has
started exploring way to enable memory
abstractions using page mapping or page
protection mechanism right so like you
asked earlier these are all examples of
that so C sharp with atomic section i
believe it's we're coming out of
microsoft UK and example like grace t
thread they basically use the page
protection mechanism to enable
deterministic execution there's also
some work on using page mapping
protection to detect race and avoid that
lock and obviously adding to the list is
you know the team makes hack the stack
and silk em and memory map reducers so
this is the end of my talk and if you're
going to take away one thing from this
talk is this I hope I convince you that
if you have properly designed memory
abstraction you can easily have a
parallel programming either directly by
mitigating the complexity of
synchronization such as in the example
of reducer hyper object or indirectly
black by a lion concurrency platform to
utilize resources better such as in the
case of PLM based cactus stack and
memory map reducer and finally the
second takeaway it seems like the TMM
mechanism may provide you in Norway to
for building memory abstractions for
multi-threaded programs okay thank you
so in the beginning you motivated but
this party is relative the factory have
to recompile existing code in order to
use it this you know okay but can you
explain a bit right I mean it seems like
maybe you could just you know used it is
encoded it grows its own stack normally
on top of me today right sure yes so I
guess what you can do is I mean the main
problem so without recompiling Co you
need to enable some way to switch
between he base character second linear
stack but that's not really the main
problem per se you can use a rapper to
enable that but the problem is that then
you might run into a space-bound issue
where each worker actually used more
than s1 amount of space and I can go
into it example if you like but I'm
wondering if I should take more
questions or I should spend okay and we
can chat afterwards yeah
so you said in the reducer it's not like
by the way and don't need commutativity
and a little bit surprised because it
means right right yeah yes make sure
that all indexes that do go over alike
yes so well in the runtime system
basically um so notice that in in the
works feeling scheduler the execution is
always so we do steal the continuation
apparent answers the stealing child and
so what that means is that it well sorry
let me back up step so for silk if you
remove all the pair of silk program
actually has a serial Semitic in the
sense that when you remove out all the
so keyword that's a valid seat function
and when you execute a silk program on a
single worker the execution order is the
same as 0 execution and the way around
i'm stealing work schedule work is that
whenever you steal between each steal
the the code that's executing a worker
is actually the same as the serial
execution order that you would have and
what that means is that you just need to
keep track of the different relationship
between your parent child whenever you
perform a successful steel which we
already maintain the run high system and
so when you combine you know exactly
what are the relationship between the
two local views you have and you just
come by accordingly and so does answer
question sorry Eddie Henderson
yeah no I'll take it offline okay yeah
sorry movie idea grandma see you later
okay I guess to follow up on his let's
say there was the solute of a hundred
things there was covered by this object
and then now you want to steal do you
partition it right so the way that the
pair loop implemented in silk is that
essentially you do divide conquer on the
loop space so the first worker will
execute the first half and when you
steal you actually steal the second half
so it's like it's actually a binary tree
divine conquer of the loop iteration
space so actually Angelina has some open
slots from a schedule today so if you'd
like to meet with her send me an email
mother m at microsoft com so let's thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>