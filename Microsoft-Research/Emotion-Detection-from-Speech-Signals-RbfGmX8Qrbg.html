<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Emotion Detection from Speech Signals | Coder Coacher - Coaching Coders</title><meta content="Emotion Detection from Speech Signals - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Emotion Detection from Speech Signals</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RbfGmX8Qrbg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning everyone
it's got we are glad to have you here
today we have Cohan PhD student in Ohio
State University and he's going to
present his interim project emotion
detection from speech signals with a
stress on gaming scenarios without
further ado cool you have the floor okay
thank you very much
my name is Cohen I'm a PhD student in
the Ohio State University it's my honor
to be here for the summer internships
today I'm gonna present my internship
projects is the emotion detection for
speech signal at first I would like to
thank you all the people who give me
help first ions and the conversational
system Research Center especially for my
mentor event and he hosts the my
internship for the whole summer
thank you very much and also I was an
Xbox team Willie found my project and my
internship thank you also I need to send
dr. dong-yi and he gives me a lot of
help on my projects and we have some
very helpful discussion especially on
deep neural network ok let's move on to
the talk this is the other line so at
first I will give a brief introduction
for the project and being our discuss
some state of the art some previous
study and then our discuss some details
for the approach and and then our show
some experimental results the last part
is a conclusion and some future works ok
so what is emotion recognition so here
we focus on the emotion recognition from
speech so it means we want to extract
the emotional states from Speedos
emotional state state of a speaker from
a speech so basically there are two
types of
emotional recognition the first one is
the utterance level recognition
it means we just detect the emotional
emotional state affirm each sentence
typically the sentence is not very long
maybe less than 27 so we assume that the
emotion states in one sentence is
constant it doesn't change so each
sentence just has one emotional emotion
level and another is a dialog level so
it's kind of more than when people are
talking they're talking on some topic so
the emotional states can be changed
across the time so in this scenario
people can use some temporal dynamics to
capture the emotional states in our work
we only focus on the first one the
utterance level so there are some
applications on emotion recognition
since although we already make progress
on artificial intelligence but we still
very far from a natural way to
communicate with machine so with because
a machine doesn't know the emotional
state of the speaker so we can use
emotion recognition to improve the
experience of the users especially in
the human-computer interface in the
gaming scenario and we can use emotion
recognition results to improve the
gaming experience and also voice search
and eLearning and another important
application is the monitoring and the
control in this scenario because some
people work work under the stress for
example is a aircraft cockpit we it is
very important that we need to know the
emotional states of the pilots so ok so
emotional recognition actually is still
kind of new area there are a lot of
problems we need to soft I list some of
them maybe it's there are also some
other problem
but this three may be the most important
so the first one is the feature set so
it's not like the speech recognition
people know em FCC or PLP some some
features are very effective but in the
emotional recognition we don't know
which feature are effective for our task
so right now what people doing is just a
blinder trying different features
through the to the classifier and a get
the results but we don't know if this
feature is effective or not we just put
all the together and get a
classification result and another
representation so you can imagine that
the emotion states it's kind of like the
color so there are always some overlap
between different emotions so you can
not find as a clear boundary between
different emotion you can we can now say
okay this is the happiness this is the
excitement and there's boundary we
cannot do that so all will have always
exist so the representation will be a
problem here and the third one is
emotion actually is very highly
dependent on the speaker and the culture
so different people have different style
to express their emotion so so the
emotion recognition system probably will
consider this this problem okay okay now
we can discuss some existing studies
okay the first one is the emotion
labeling so the simplest way simulates
the way too late to label the emotion is
use that category it would just pick
some basic emotions and the tail and the
label one of them like the happiness
sadness anger neutral something but as I
said we cannot find the clear boundary
between different emotions so some other
study used
no representation it's kind of treat
each emotion as a coordinator on
emotional plane like here so and there
are two dimensions one is the valence it
means the emotion is a positive or
negative and another is a Roseau means
the emotion camp is active or passive
for example the end angry is a positive
is a negative emotion but it's kind of
very active but the part border is also
negative but this is the passive so if
you want you don't want to say something
so you can label each emotion on this
plane and it's so that there's just a
coordinate and some some other study
uses the third dimension like the
dominance the tension something but this
to this erode and the variance are very
commonly used our work actually just a
based on essentially the best concert
category leveling but we also gives us
score vector for each of the basic
emotions for the dimensional also it
fits kind of a principled way to
represent emotion but it's not easy to
explain when you give a label if it's
just coordinated explainations it's not
so straightforward and also some
previous studied the feature set in the
previous study includes the local
feature and the global feature so the
local feature is actually just extract a
feature from each frame like the pitch
magnitude and also some spectrum based a
feature like the MF c LP see something
also has some voiced quality features
like harmonic to noise ratio jitter
shimmer okay so this is a frame level
feature but we need to make decision for
each utterance
so we also need to extract the global
feature based
this local feature so the global feature
is the combination of the local feature
so we just collects all the local
feature from one address and the vein a
typical way to do it is just computers
statistics like the mean standard
deviation maximum minimum to get as a
local global feature to represent you
the feature for the all utterance
so then different feature which is just
different the classifier so basically
for the local feature the traditional
way is using Gaussian Gaussian mixture
model to for each emotion and the
computer likelihood and some other
people treated this as the speaker I
very similar to the speaker ID just uses
a GMM press the UVM and the construct of
the super vector and then use SVM to the
classification also you can use the HTML
to capture the temporal dynamics and
written a work use the LDA
we treated the each sentence corresponds
to the document and the each frame
correspond to the word and use the LDA
to do the classification and most the
reason the work actually likes you use
the global feature they just take the
statistic across the whole utterance
name to the classification the SVM is
the most commonly used classifier and
also some people use the key newest
labor and the decision tree something
and also mostly reason to study you that
you've been in network but they still
use this statistical feature to to to
light like the to the future extraction
and design use the SVM on top of that to
extract a to estimated emotion status
and some database these are commonly
used in motion recognition a different
database they have different results
some some database uses the acted acted
address and some use the actual
recording that is they don't ask act
register recorded some emotional speech
for example this one it's the recording
of the talk show and this one is ask
some kids to talk to the son in robot so
that is it's not as I acted emotion and
we use this one ie mocap that this one
is the activity are some extra chill to
pretend some emotion and this this
database has the audio and visual
signals we only use audio here and the
labeling is the categorical and the
dimensional and the the this database is
very large and very rich so that's why
we choose this database other other
database is relatively small okay that's
good approach so this is overview of the
system so given utterance okay and the
first things we just cut the utterance
to different segments and then we
extract the feature from each segment so
we get the segment level features and
then we through this segmented choose a
deep neural network which can deepen
your network to estimate the emotion
states of each segment okay with this
output we get the segment level output
that is the can be the probability of
each emotion States but this is the
segment level results in a way for one
utterance we collect all the cellular
level results
and we build we computed the utterance
level feature and use another classifier
to do the classification to make the
utterance level decision so this is a
over yeah our talk about it okay so when
we extract the segmented feature the
first thing we need to do the framing
and the way converts the signal from
time domain to the frequency domain the
window is 25 milliseconds with 10 minute
second step site the feature here we are
using the pitch based feature including
the pitch value and the harmonic to
noise ratio and also AM FCC features
also we used other feature across time
and then we can build these segments but
because the context informations are
very important so we include the frame
before the current frame and a frame
after the current frame so totally is a
25 frame long that is the the segment
and we concatenated a feature from each
frame and get these signal level
features the future center frame is
pretty much the standard speech
recognition content for speech basic
features is Hana
okay so now we have the seven level
features XT and when we change the
neural network we need to give the
training label since the label is we
only have a label for every utterance
and here essentially we give all the
segments from one apartment to the same
label the label is formed address and
also we don't use all the segment's in
their training data because arguments
also contains some thing like the
silence we don't want to use that and
also some speech the energy is very weak
they may not contain much emotional
information so we also throw that away
we just pick the top 10% segments with
the highest energy for the chain and the
calcification okay and the output of the
deep neural net will be the probability
vector for each emotion in this segment
okay this then then deepen your network
configuration here is we use the three
hidden layer we also try one two three
four and three it give give me the best
performance and and relative I wouldn't
need to go to the four or five and they
use ratified in in Europe and object
function is cross entropy and they use
mini-batch studies grating descends to -
training since it's very standard way
and so the divin Arenado output will be
will be here the if we have K different
emotions either will output the
probability of each emotion okay
okay this is the example so the this is
the blue the the blue line here
represents the probability of there are
five emotion the five emotions
excitement frustration happiness neutral
and certain is okay so basically for
most of the segment
the excitement is get the heart is the
probability and the some segments they
are not but overall it is a dominant
this utterance so yeah fortunately this
that this sentence is excitement but not
all the sentence has this good for free
miss something is it when you plot
that's it's very noisy so we need to use
another classifier to get the utterance
level decision okay so when we have the
second level output we want to get the
utterance level decision so so then for
the other level expectations the inputs
will be the utterance level feature so
first we get the segment level output
for each segment and then we we get this
set of segments who picks the maximum
many more and the mean for each emotions
then this is a one type of feature
another is we choose the number of the
segment with the high probability that
means we want to know how many segments
have supports this emotion so there will
be another feature okay we combine them
together as the admins level feature and
then we the outputs of the admins level
will be the emotion score vector for the
whole utterance
when we do this level this
classification there are two we try
different classifier the of course we
use the SVM that was very popular
classifier we also try another
classifier called extreme learning
machine and the world compare them okay
a short discussion on the extreme
learning machine shall four year LM y om
essentially is the single hidden layer
neural network but with special training
strategy so there are just one single
layer so from the weight from info layer
to the hidden layer is just randomly
assigned the ways it is random and from
the hidden layer to the output layer we
use minimize away McCleary as a memo
little square arrow to change this
weight so I think the magic is here we
people people typically use this one if
it means we use the the number of hidden
units is much much larger than the
number for input units so that is a
random projection from input to the
hidden layer when we have a lot of
hidden units we can get the good
representation for the training data but
also since the weights are random so
this unit this hidden representation is
not highly dependent on their training
data so it probably gave us a good a
generalization performance extinguishing
the input layer exactly just 2020
in the unit shows me yes we have five
emotions so this R 3 so 3 times 15 and
there are 5 emotion I tried different
configuration along like 100 US
government thanks yeah
I also tried more he no units the
performance is similar okay so so chain
what do we work will win you're needed
to Train it's just that the hidden layer
to the output the layer so we use the
minimal listed square arrow and just
minimizes so it turns out we just need
to solve this equation this is a pseudo
inverse so it's the very fast very
efficient way to to to do their training
and also essentially to give us a good
performance so this is some advantage of
this erm so there is no great ascent so
it's very fast and give a good
generalization and we also compared with
support vector machine exactly it gets a
better performance than SVM and then
it's much more efficient around 10 times
faster also this yah has a kernel
version we also use Conover and we will
do the comparison later
and for the performance measurement so
we use two types measurements the first
one is the weight called the way data
accuracy so the winning accuracy is and
it's just standard classification
accuracy okay use the correct label our
turn divided by the number of all other
ones and the on which the accuracy
essentially for each for each class we
computer the accuracy in this class and
then we take the average of of this
accuracy so essentially it's the for
this this measurement it's kind of
requires you get some balance accuracy
for each of the criteria each of the
emotion classes okay now experimental
results so this is the data set we are
using it's in mocap database it's the
active multi-model multi-speaker
database including the video speech
motion text transcription and something
and each audience is actually annotated
by three human and not haters so they
also use category and dimension labels
and the category deals like eight to
nine different category so since there
are three annotators when we build our
corpus we will see which just select a
sentence if more than two at annotator
gives the same level to listeners then
we will put them in our covers
apparently if three annotator give
different labels we don't use that
because we don't know you're going to
choose okay so in our covers will use
the happiness excitement sadness
frustration and a neutral this five
emotions are very common
in the gaming scenario and the training
eight speakers for male for female and
the test is two speakers they're not
seeing in their training set so it's
bigger in the speaker independent and we
don't use a visual signal or the only
user tax a transcription just speech
signal I also want to mention that we
don't use beacon normalization because
some study a lot of studies you speak
normalization you normalize the feature
for every speakers so it means you it
assumes that you know the speak ID
because when you normalize them in the
test face you need to know the
normalization factor for each speakers
but we don't use that some studies
showing that this speaker normalization
can get a very getting much more very
large improvement but we just use this
more challenges in our okay this is some
gaming scenario previously right now we
have the clean speech then we will
create the speech in the gaming scenario
so so in the gaming scenario we have a
three Sun Sun sauce the speech is a
speaker they were say something and we
wanted to label the emotion for this guy
and also we have five loud speaker they
are playing movie playing game something
music and it has some background noise
like the air conditioner some room noise
so all of these sound stores are
captured by the Kinect the Kinect has
four microphones so that is the mixture
also there are rumors evaporation
because in this room reverberation in
the room so the human to the microphone
there are some reversion and also the
speaker
so they work involved with room impulse
response get the mixture and then we use
the connect audio pipeline to attenuate
this noise so then we will get the
processed signal so this signal will be
used in our task okay this is the
configuration to create as a gaming
scenario covers the loudspeaker track
included ten different asana sauce five
game five movie and in this room so we
try we use twelve different positions so
different position will give different
from impulse response and from one meter
to four meters in the center left right
we just a random pick position and mixer
with the loudest speaker and also was a
big one noise create as a mixture also
the song level I random choose from this
so this is the example and I can play it
so this is a clean speech I'm so excited
I'm like a kid I came up a guy with a
house my fly zipped up this is uh
connects me this is the process speech
there are some distortion but most noise
I removed so where are you is the clean
speech and the process speech can anyone
tell me what is the motion of that
utterance I'll give you five options
excitement the frustration I have made
you have to understand this I can't play
again if you want never again
god I'm so excited I'm like a kid I keep
on the guard of the house my fly zipped
up there are five four one three and so
okay okay and anyway no happiness okay
okay I think most of our guy a good
evaluator because the answer is we have
three annotator to give excitements one
give happiness yes yeah this is a video
so there are some descript difference
between if you only listen to the speech
and when you leave also what the video
maybe you give different a label it is
possible yeah gives you a hint I think
it's not just yeah yeah that level of
their the way they're saying the words
it's the Arab world yeah and also
essentially video and a speech for
different emotions they have different
effects for some emotion you probably
you are getting the information forms of
the video but for some emotion maybe you
are kind of captures from
what it's allocated because they
understand the memories I have it yes
you're the English do they know that
they know that yeah you know at least
subconsciously they use the meaning to
the credits yes maybe they use all the
sauce we can get wouldn't it in the same
in the same sound sad if there's a
controversy between the meaning and the
emotional if you laugh at the end of the
sentence I understand okay so we will
compare our approach with two different
existing algorithm the first ones the
local feature with the edge mm-hmm they
just pick a frame without feature and
for each emotion is China
hmm hidden McMurdo and then use the
Maxima like how the topics emotion so we
train this one for each motion and they
use the four fully connected States for
each emotion and GMM to represent is the
observation for ready and determining
motion by the maximum likelihood and
also we compare with the global feature
process when there is a toolkit called
the open ear this is very popular to a
kid in motion recognition and he creates
a very very large feature sets the M FCC
pitch PC 0 crossing and probably around
like 8
2:19 some different acoustic features
and for each features they apply the
statistic function the mean variance
skewness kurtosis maximum minimum and
whatever a lot of statistics so the the
features that would be 988 imagine arity
and then applied the SVM to the
classification so so we compare this
five hmm
open here is SVM and our approach the
deep neural network with SVM utterance
level classification and the tip neural
network with the extremal any machine
this one this guy is a deal with yeah
I'm using the kernel version this is a
result is the weighted accuracy who
assumes a clean speech result and the
game a result in the gaming scenario
this side is the clean speech so
basically the hmm gets the lowest
performance and the open ear is a little
bit better
and the DNA based system is
significantly better than these two and
also you can compare this SVM with the
erm yeah it's just a classification
accuracy we since we have five emotion
we just count as a number of emotion
correct labeled divided by the number of
a whole other ones just standard
accuracy yeah
Wow yeah one yeah yeah yeah this is not
a very very high number
the the highest number on these covers
is like the 60 to 70 but they use the
speech plus visual plus speaker
normalization so all the information
together gets it like this number and
also they use the fourth way
classification here we use a five weight
classification okay is that one example
you showed us where two people attacked
it as I think it was excitement and
wanted tag to this happiness does that
mean that this it's a failure if you
don't get both excitement and happiness
nor is that well that one the label in
the training is excitement yeah yeah
terms are the two people so if we if you
classified in your algorithm happiness
that would count as a failure yes and
what it liked you you made you made your
point around video plus the word spoken
across the audio really need to work in
concert to get a really good knockers ii
hectic what is the actual is there like
a ground truth of if you just took audio
what a human could possibly do because
that would be the best your algorithm
could probably ever hope to achieve
yeah I always believe if you just take
the audio I mean even you ask annotator
to to the label can be very different it
can be different yeah yeah but this
corpus just provides the the label is a
bass downs of audio and audio and the
actually includes the text transcription
whatever that kind of the the qu we
believe that is that shoe emotion for
this address but of course this this is
not really true karana choice because
yeah the leveling for
nearest your a problem you you always
ask people to label the utterance thing
but people always have different a
feeling for the emotions yes yeah but
the cup which we can only based training
or Tesla based on the label provided by
the covers okay and yeah essentially if
you compare the convenience PG&amp;amp;E became
me there are some decrease on average
around five percent but it's not very
bad as it's five around five percent
twelve and this is the on which accuracy
it's essentially it's you you need to
get the kind of balance resulted for
each classes the vision is the same
trend you can find here and this is
clean when the HMA is even worse and the
SPM is better and the DM based system is
much better and also the the ERM is
better than SVM
but overall the year mm and the GMA with
kernel is get a pretty similar it's
comparable okay
this is the confusion matches okay this
is the game is not a screen scenario and
we have so this column is the to label
is a label from the training set this is
the label from our approach so you can
see that for the game is in error on
average each the accuracy is it kind of
is not different so much it's different
but not as large as the clean speech and
there's a very interesting thing is that
if you compare this gaming and the clean
the the excitement
vision neutral they get a very very
similar performance this is the point
five point six point three six the very
similar but the happiness and the
sadness are very different the clean
speech gets very good performance for
the sadness but very poor for the
happiness that the gaming is a pretty
much difference the happiness is good
and the sannyasis should think about
speech enhancement algorithm that
enforces but when you play here maybe
also it's wonder happiness
actually it's better from the gaming
Solaris
okay so let's go to the conclusion and
discuss our future work so basically we
design the emotion recognition algorithm
and we use simplified feature set and
use deep neural network for the stamina
level classification and use the yeah
and for the utterance level
classification and our new approach
achieve output from relatively 13% to
the state-of-the-art previous studies
and in the gaming environments we
already see there are some negative
effect on the emotion detection around 5
o'clock but still the new algorithm
outperforms the state-of-the-art around
searching heavily ok
and I also want to mention some
direction of the future work the first
one is the multimodal and also just we
are somehow technical one ok so
basically right now the most covers to
provide the speech and the audio and the
video signal and the some provided the
text transcription so we previous study
or an issue that when we included other
information like the visual or speech we
can improve the emotion recognition
resulted significantly so and this is
right now they are available in the
corpus it's so we can choose this
multimodal information and the vision
information of course includes either
gesture dynamics and also the face
expression is very important to
recognize the emotion and also we speak
recognizer we we if we know what is
their thing
maybe we can also this will be another
cues to do the emotion recognition
okay then for some technical one future
work so because previous study we in our
approach we use 10% segments with higher
instance as a training and test sample
so because we believe that this strong
segment contains more emotional
information there are informative for
our task but this we choose this 10% but
the question is can we determine this
informative segment directly from
learning the ideas we can we can choose
the best training sample from the last
trend model when we get this best the
training sample we threw that to the
next model to the training then then the
next model gets the better training
example and so we just throw away those
no non informative segment so
principally this new model should give
us a shopper probability because the we
have a better training sample then the
the trainer model should should give her
better performance and this is the idea
of this we first get the whole training
set and then trend en and with ztn we
get the training the second training set
this change that are chosen from this
one and then we keep training do this
for a few times and with this trend DN
in the test phase we just throw the
speedy to all of them and let just a
combination maybe we can get a better
performance I have some pretty results
so we so with this hierarchical training
with worse the ordinary training the for
the unweighted accuracy the killer like
at 2%
better performance though weighted are
pretty similar this this increment is
not very large but I still believe there
are some work we can do like how to
choose the best example and how to
combine this different model together
and get the good for our turns level
performance and also we we we should we
should improve incorporate some temporal
dynamics because previously the hmm is
chained under the unsupervised manner we
don't have the labor with just for each
emotion with gender hmm but here when we
have the DNA it can give the label for
each segments now with this initial
label we can use the civilized manner to
chain the atrium and principally it give
me the better performance danger hmm
maybe there's in our part it's possible
also this one is actually at the
beginning I do some work on this one
it's kind of very interesting because
right now the problem of emotion
recognition that we picked the handcraft
a feature like MFC the PRP whatever and
together to as a feature to chain the
system but essentially from the machine
learning points it is possible to use
the spectrum feature because
spectrometer doesn't lose any
information everything is in the
spectrogram and also motivated by the
written progress in speech recognition
they success for children speaker
recognizer using first bank so maybe in
the emotion we can steal directly trend
on the spectrogram and the length of
learning machine to learn every all the
feature and then do so Jenny but in my
experiments it's not very good results
it's firm level accuracy is the lower
than the
a surprise speech around the 4% lower
but it's worth trying maybe we can try
different parameters like the window
glanced or something and also maybe more
data work benefits for that Union
training okay let's see yeah so this is
the some important reference and yeah
pretty much down my presentation and I
want to share this one I have been in
the US for a few years and I would say
this summer is the most wonderful summer
I have had in US and those pictures was
took me few weeks ago I spent two days
to climb the mountain Adams it's 12,000
feet and it's very hard very cold but it
is very very nice experience and there's
no Sun and no voice here but if you only
look at the picture if you're looking at
look at my face my emotion is very very
excited
no questions no emotions acted very well
it's at fertility
so the algorithm you in the presentation
intestine are compacted I didn't marry
virus but I test on the data from the
semi Cobras but it's the actual data
there are some drop mmm you don't
remember the name but not very worth
whether there are some drop yeah I mean
if you want you if you really want to
treat with the actual data you may need
to train on the after data like it is
provided by your team where can you find
to test you or somebody did the same
work with additional state which is
unknown in neutral is not unknown
unknown is for example all those samples
out of all the three labels and gave
different yes so you
because many many times somebody says
something like of your jealousy yeah you
are right so yeah if if yeah you yeah
you can believe that if sweet and
Nutella give different labels saying
maybe this emotion is very difficult to
to describe but it's the or some
particular emotion yes that's right yeah
but you know in in our experiment is but
yeah it's it's the awesome emotion but
we don't know what is it yeah change I
mean I'm just thinking about people
sometimes speaking loudly like they
sweet they speak up louder when they
think that like something can't hear
them so if I have something running in
the background I'm trying to talk to the
connect my I might sound more excited or
something because I'm trying to project
did that kind of thing come out or was
it was it very did not really change
like you can still detect the emotion
even if somebody was trying to be more
emphatic I mean with the bed wander the
bet ones maybe Martha the the whether
you are thinking the background because
because of the ways they might be
changing the way they're talking to the
machine because they're trying to talk
over it try to talk over this out that
background so this is called Bloomberg
effects and though we didn't think of
that
so technically as people speak
differently when it stays a loud sound
they try to get more energy to what the
consciousness is just pure sin that they
can give the key and recording you go
over to tables responses we have the
revelation we have the noise but it's
the same voice regardless of the loud
speaker else and that may have been
captured in the precision of the
co-citation
you mentioned at the beginning that
different cultures express themselves in
different ways your data corpus though
had German
the united states average together did
you see like worried actually
substantially better with German sources
this is a very interesting question
because I I haven't used this Berman but
some privacy because I haven't already
they pick different languages together
like the German English Spanish
something together and the performance
is still very good but if you test on
different culture like the Chinese
Japanese then performance a very
difference so it means because you can
the the English or Spanish Germans are
kind of very similar to each other and
in the emotion part the art maybe has a
similar way to express their emotion but
if the language are very different they
think the system may not work for the
others so you didn't get deep into that
that where you would make a
recommendation oh for every language for
every country language pairing that you
would have a different training set
of course for different languages if you
train on particular languages you will
get a good performance better for me I
think you can put the similar language
to gether and the channel emotion the
channel system and the other different
multiple languages together another
model that will be good I think this is
an interesting place to go to verb in
your next steps like to really dig into
the difference is
there are cows because the emotions not
very well not highly depending on the
language even you don't understand the
language you can still say that he's a
happy or sad or something yeah there are
dependent futures it's more cultural
difference that's the Italian speaking
Norwegian it will be you better use the
Italian language setups because the guys
fortunately is more excited with more
emotions in the speech well the opposite
case Norwegian speaking Italian you know
you see the same kind of even phone in
different difficulties particularity
emotion in those early all examples from
Europe induced if you start to gross you
go cross races and continents is getting
even even more different so it's more
cultural than natural step and I said
okay and eventually it is possible to
find some that's it some blocks general
training data and to do something at the
patient thoughts
this culture is more emotional less
emotion to find some coordinate system
but for now what the state the state is
is you have to have a label retainer for
each language any means Spanish in Spain
may be different than Spanish in Mexico
no questions so let's give come</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>