<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 6 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 6</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9csKLvISXZw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
okay up next we have our spotlight
presenters and please make sure you come
visit them with all your questions
tonight hi my name is mike c arabic and
this is I work on spatial transformer
networks together with Korean and during
coy at google deepmind and so basically
the motivation for this work was that
we're missing a key ingredient for
neural networks and that's being able to
allow them to spatially warp their data
normally with complex we rely on
multiple layers of Max pooling to
achieve spatial invariance or some sort
of inherent pose normalization but here
we define a new module the spatial
transformer which actually has a
differentiable spatial warping mechanism
in it and because it's differentiable
you can just drop it into networks
anywhere you want and train it from the
global task loss without any extra
supervision and so the result of these
spatial transformer networks which can
actually actively spatially warp their
data so conditional on the input can
learn to attend to impose normalized
objects of interest and here for example
is the result of a spatial transformer
train to classify distorted endless
digits and on the left you can see the
raw input in the middle is the
transformation visualized on that raw
inputs and on the right is the output of
the spatial transformer which is then
fed to the remaining layers of the
classifier so there's three main
components of the spate of transformer
first you have this localization network
which looks at your input feature map
and then regreses the parameters of the
transformation to apply to this input
and so for example for an affine
transformation this would be the six
elements of the affine transformation
matrix then you have a grid generator
which takes these parameters and
produces a sampling grid the sampling
grid defines where and for each output
pixel the location in the inputs that
you take the value from and you put the
there's a differentiable sampler which
takes a sampling grid and actually does
the transformation for you and so this
module you can encapsulate it and drop
it into networks anywhere you want and
the result of these networks which
automatically attend to impose normalize
objects of interest and so for example
on the left
is at the top there's the mean image of
a distorted endless data set and if you
train a transformer network just some
classification labels the mean image of
the output of the transformer is this
pose normalized mean image below which
is a lot easier to classify we show
steady the art results and street view
house numbers by using multiple
transformers deep into the network
intertwined with convolutional layers
and we show state-of-the-art results and
fine-grained bird classification by by
having up to four transformers in
parallel which automatically learn to
focus on different body parts of the
Bears and the third party open source
code floating around for this and you
want to know more come to my poster and
come to my talk tomorrow at the deep
learning symposium where I'll give some
more details and show some cool videos
and stuff like that thanks
I'm will Whitney and I'm going to talk
about the deep convolutional and verse
graphics network mrs. work with pages
kulkarni bushmeat Kohli and Josh
Tannenbaum so our goal in this work was
to learn a disentangled and semantically
interpretable representation for 3d
images and in doing so we were inspired
by the representations that 3d rendering
engines use with the rendering engine
you can specify a 3d model then tell it
where to put that model in the image and
at what angle and with what lighting and
so forth and the great thing about that
is that the representation of these
extrinsic images like pose and lighting
is totally decoupled from which model
you're using we've managed to learn a
model with some of the same properties
with our model we're able to directly
make changes in the latent
representation and then re render images
under different conditions so what we're
doing is we're showing the network one
image of the face and then basically
asking what would this face look like
under different lighting conditions or
what if it were tipped down some and the
network can generate these images for us
so the model we're using to achieve this
is based on the variational auto encoder
first we have an encoder which takes in
a grayscale image it has several layers
of convolution and pooling and then
produces a distribution of our latent
representations unlike in an ordinary
network with distributed representations
we're going to interpret certain
components of this representation as
having specific semantic meaning then we
have a decoder which takes in the latent
representation and renders a new image
it has three layers of nearest neighbor
on pooling and convolution so it's
basically the encoder Network flipped
around backwards and we use a special
training procedure to learn a
representation with this semantic
structure that we're looking for what we
do is during training we have short
video clips with only one of these
variables changing at a time so its face
rotating up and down or with the light
sweeping from left to right but not all
of them then intuitively what we want is
throughout each of these training videos
all but one of the components of our
represent a
should stay the same one component
berries representing the action in the
video but all the others should be
constant and by enforcing this
constraint on the forward pass and then
add ingredients that encourage the
encoder to produce these representations
on the backward pass then we can
actually get this behavior that we're
looking for where one component of our
representation closely corresponds with
the one variable that changes during the
video I think that's about all the
detail we have time for right now so if
you'd like to hear more we'll be posting
number six at tonight's session and by
the way i'm also applying to PhD
programs right now so if you're looking
for grad students definitely stop by the
poster thank
so my name is sue Tanja and this is work
joint work with my long-term
collaborator area so we are actually
looking at the text categorization
problem trying to understand whether
neural net can do better than what has
been done for a very long time the
linear classifiers like sem so we we do
something on the supervised and then we
are moving to the semi surprise so this
is mostly about the semi-supervised
learning but first I will talk a little
bit about what we did for the supervisor
text categorization so we probably all
know if you have been around in in the
community and I'm included I'm very
interesting text categorization has done
some earlier work in the maybe more than
15 years ago about linear method for
text categorization at that time it
achieve the state of art and it has been
that way for very very long time until
very recently so it recently there are
attempts to do the neural net on the
text categorization problem you know so
there are few works but including ours
so our the difference of hours we try to
use the tags original test as as input
directly to the convolution your net
it's a simple convolution neural net
structure but when we find out the
software's we couldn't find anything
which can do that so we we do that
ourselves implementation then so the
idea we we really try to play with and I
actually I can beat a linear method but
what it really matters we found that
it's a better use of angra basically
using reaching embedding so far for the
traditional linear method you use
engrams plus like linear method and svm
but here you're in bad and Graham were
are reaching into a inviting vector and
a dying you try to you can think about
us back of those so so
that essentially is the as far as I know
the state of art for the super eyes the
tax cut validation now if we move to the
semi surprise because when we compare
methods are like super eyes method the
two to the task is like like sentiment
classification we know that a lot of
people to employ how I sit here and plus
some surprise learning that can do
better so what we look at is that we try
to apply our methods on semi super
methods to will learn you which is a
quite a while ago which happened with
the linear method but then for the for
the non EMS we send it to CNN using the
2 vo learning basically what first use
to vo to learn are embedding feature now
added feature to a ZN and a time we find
that this achieve the city of ours as
far as i know so this is the best
possible until now and we also tried
some other methods like allison TM and
so on but this is best okay thank you i
am rupesh and this is joint work with my
collaborators Klaus Graf and you can
shred over at this we see our lab it's
here so deep networks are very powerful
models but they can be very difficult to
Train sometimes especially when the
number of layers becomes tense or in the
hundreds on your network is essentially
a successive nonlinear transformations
of data and when the number of such
transformations is very large the
gradient of the composition of these
transformations can vanish or explode
and this causes problems in training so
in this work we attack this problem
using a new architecture called hyper
networks in a highway network every unit
learns to smoothly very its behavior
between transforming information or
carrying information this idea is
inspired by the gating mechanism in
another cm network due to this design we
can easily bias the network initially
towards carrying information and this
way the gradients don't vanish and we
can train networks with tens to hundreds
of years with gradient descent
techniques
on the figure on the right we show that
plane networks cannot be trained well as
the number of players increases to about
100 with well-known initialization but
highway networks do not suffer from this
problem and we can still easily trained
them as training proceeds every unit in
a highway network learns to adaptively
transform the data as needed so some
units might transform later more others
unit other you might want to carry
information further so this has some
additional advantages one of these is
that units can easily learn to combine
information from multiple levels of
abstraction another advantage is that we
can easily measure the contribution of
every unit or any layer in a train
network so in this figure we measure the
contribution of each layer in two
networks they are each 50 unit networks
and they're both 50 layers deep and what
we do is for each layer we can force it
after training to only carry information
and then we can measure the resulting
loss and we see how much has it changed
and we find that on the simpler m nest
ask the network learns to use only for
the first 15 layers and changing the
rest of the layers doesn't affect it
much but on the more complex if 100 does
it learns to use more resources and uses
almost all the layers so hyper networks
represent a more flexible class of your
network models it has already been used
by people for things like speech
recognition and language modeling and if
you want to find out more please come to
a poster tonight and it's question
number four thank
hi I'm Dan Rosenbaum and this is joint
work with your vice so if we have a
generative mixture model and we use it
for inference every data point is
assigned to the mixture component that
best describes it this means that an
inference time we need to go over all
the mixture components as illustrated
here on the left in contrast if we use
the mixture of experts model which is
shown on the right data points are
assigned to experts according to a
gating network that was trained in
advance in our work we take this 25
years old idea and apply it to natural
image priors we show that if we have
mixture model prior we can train a
gating network to perform fast inference
without harming the quality of the
results so this illustrates how we use
our method given a noisy image or any
corrupted image one of the best ways to
do to restore the original clean image
is to have a mixture model prior /
patches of the image and use it for map
inference so this is shown here on the
left like I said before one of the steps
of inference is to assign every patch in
the image to the mixture component that
best describes it so this red win figure
shows the assignment for every patch
using a certain color coding on the
right we also show the assignment of
patches but now it was performed with
the gating network that was trained in
advance and this is done with much less
computation and we can see that the
results are very similar the bottom
figures also also show that the results
are very similar in when we it's the
restored images in both cases and
they're also very similar so the full in
fruits here on the Left can take a lot
of time to compute so typically to
restore this image will take about 30
minutes and using our method we can do
it in less than in less than 10 seconds
so this is can be pretty significant so
using our method with a gaussian mixture
gaussian mixture model prior we achieve
both fast inference and state-of-the-art
performance and this is true even when
we compared to two deep architectures
that were specifically trained for a
certain task
our model still has all the advantages
of generative models this means that we
can use the same model to do different
restoration tasks like image denoising
with with different noise levels image
deblurring with different blur kernels
and more so the middle figures here show
different deep architectures where each
was trained for a specific task our
model is that the results are here on
the right and it's the same model that
was trained only once for clean images
and used for different tests and we can
see the results are comparable and
sometimes even better so we have also
code available online so feel free to
use it thank you hi everyone I'm madria
represent and I'm here presenting our
paper entitled were they looking
decision work with Aditya coastal a
carbon Rican antonio de alba if you look
at these two images here it is actually
really easy to infer where these people
is looking in the image of the left you
have these two persons watching the TV
in the event of the right you have these
two kids looking at their foot this task
is commonly referred as gays following
and it has interesting applications and
understanding people attention intention
and activities the goal of this paper is
actually built a computer system able to
follow gaze and to tackle this problem
we first decided to build the largest
scale data set called the gays follow
data set with 130,000 people on it in
122,000 images the latest it is
available for download in our website
unlike everyone else in the room we
decided to use deep learning to solve
this problem we divided we actually got
inspired and how humans might perform
this task to be lower architektur so if
I I now ask you where the person next to
you is looking you would first look at
their head and then you will drive in
further gaze direction and finally in
that direction you will try to find the
Saline object that they could be looking
inspired by this idea we divided our
network into different pathways the case
pathway and the saliency pathway the
case path which uses only a crop of the
head of the person and the head position
we just information that we actually use
to infer what the gaze direction is
going furthermore the saliency pathway
uses the full image which is information
that we as human used to find salient
spots in images finally these two
different representations got multiplied
and the final output is computed what
it's really really interesting here if
we drainage Network end-to-end these two
different representations emerge
automatically so if you see in the first
row of examples we want to predict the
gaze of the girl in the right the Gator
opposite the gate representation which
is called the day is matched here
indicates us the gaze direction of the
girl the saliency representation in the
highlights the Saline spot which are the
head of the two girls and the hand of
the two girls and finally if we multiply
this two different representations we
get the product where we have enough
information to predict where where this
girl is looking so finally here we show
some results of our system in yellow you
have the real output of our system in
dread you have the ground truth
inundated by humans if you want to see
more of the results or you want to check
out our really really cool demo of the
system working please come visit us in
poster number two thank you very much
so hello everybody my name is Pedro and
this is a work that I did together with
run on Colbert and pyaar turtle are to
remain tionship at Facebook so out of
the most common visual cognition tasks
we are interested in object detection of
segments so basically given an input
image would like to generate the masks
which fully contain all object instance
in that given image we believe that our
work is important step 2 are dealing
with is that with this problem so here
we propose a object proposal algorithm
which is able to find strong regions in
an image which are very likely to
contain objects independent of their
categories in the past few years there's
been a lot of work on this area but they
usually rely on very low level vision
cues such as edges and super pixels and
which has a very small learning
component so in this case we try to a
model with convolution neural networks
which of course have a very very strong
learning component and we notice that it
performs quite well that being said the
question is how can we set this this up
as a learning problem so as I said
before the model is the convolution
neural network but in this case is
divided into different branches so the
top branch is responsible generating a
segmentation mask of the input patch
while the bottom branch is it's
responsible to give an object as a score
of how likely that input page contains
an object fully contained on the image
and it's located in the center of it so
in the scheme we have our training data
contains three different values the
first one is the input RGB another one
is the object nussie score information
which helps the information of the race
of object fully contained on the image
and located in the center and finally in
case there is an object a segmentation
mask of that object the train is done
jointly by maximizing by minimizing as
some of logistic losses one for each
pixel location in the image and also one
for the object Nussie score and there is
trained by stochastic right to dissent
as usual at test time we apply the model
densely so that you have a segmentation
mask in an object the score at its
location of the of the test image and we
also apply it in different scale so that
we can detect
of different sizes and then finally to
get the topic a proposals we simply at
the the masks corresponding to the top
case course generated by the mass by the
network so to conclude we propose an
algorithm that is able to learn to
generate object segments it's achieve a
very high recoil with a very low number
of proposal and it surpassed the state
of the art by a big margin at different
data sets so if you're interested please
join me at poster number eight tonight
that's what we can talk a little bit
more thank you so hello everyone I am
yawn and i would like to present work
done with my accent collaborators Dima
Dima Canyon in Yeshua on how do you are
apply soft attention models to speech so
why would you do it well mainly to see
how it breaks because you have those two
problems you have repeated contents
repeated sounds and then I you typically
want to scale too much longer inputs a
test time than you had drink training so
um first about choosing content the
classical model at each step would go
around the source sequence look at the
content see which frames it likes and
then select those obviously if you have
the same content it may not distinguish
between them so we add a little
location-aware mechanism that can also
tell frames are based on the location
relative to the previous selection and
this helps our with the repetitive
content problem so then maybe the more
important question are can you apply
this to something much longer than your
training data so in turns out that both
models fail but they fail in a different
way so the content our model the content
based one learns an increasing location
tracker and this location tracker is
very non reliable so when you ask it to
decode a long sequence it would first
define and then essentially loop around
and loop around
around and there is no way to fix it now
the location-aware one would do just
fine for a while and then refuse to do
anything we investigated the problem it
turns out that there is a softmax over
frames and then the softbox behaves
really badly when you apply it to
something much larger than your original
training inputs so we have a few fixes
for this softmax and this resolves the
problem in the end we were able to
decode a stuff which is 20 times longer
than the training data we got close to
the state of the art on timid like our
fruit fly for for speech recognition we
didn't applied it to Wall Street Journal
so if you are interested in the details
of how to do this a location-aware thing
which is very generic it's not only for
1d it's also valid full for 2d things or
3d things are or how to do this for
speech please come to talk with us at
poster number 5 thank
okay thank you very much now the morning
session was over so please join me to
thank all the speakers in the morning
sessions
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>