<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Microsoft Azure Machine Learning to advance scientific discovery â€“ webinar | Coder Coacher - Coaching Coders</title><meta content="Using Microsoft Azure Machine Learning to advance scientific discovery â€“ webinar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Using Microsoft Azure Machine Learning to advance scientific discovery â€“ webinar</b></h2><h5 class="post__date">2016-08-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uCggbLJ1f5o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to today's webcast using
Microsoft Azure machine learning to
advance scientific discovery I'd like to
turn today's event over to your
presenter Roger Roger Varga group
program manager for cloud machine
learning group microsoft corporation
roger you now have the floor great Thank
You Kim and thank you everyone for
joining I'm the presentation this
morning last week we had um two exciting
announcements the first was that we
announced the public preview for Azure
machine learning which is a fully hosted
cloud service for machine learning and
data science on on the Windows Azure
cloud this is something our team has
been working on now for the past two
years working with researchers across
Microsoft Research to pull together some
of the best machine learning algorithms
but also looking at what are the
friction points that people encounter in
using machine learning and more
importantly how can they deploy their
solutions into production as fast as
possible I'll be telling you more about
the service today another exciting
announcement we had last week is that we
announced the azure ml for research
program that's being being managed by
Microsoft Research and this is this is
very cool because what this provides is
there's two types of awards that one can
apply for and get from from Microsoft
Research one is a data science
instructional award we're in an
individual student will actually get an
account on Azure ml and along with 500
gigabytes of cloud data storage for each
student's which a university can use to
actually teach an entire data science
program or apply machine learning
program and again this program is
available to academic and nonprofit
institutions and there's a selection
process that goes on to actually choose
who gets admitted into the program and a
second offer offering within this
program is actually for large research
collaborations in which will offer a
shared workspace for a research group
can stick up can actually store a large
volume of data up to 10 terabytes to
enable a group of researchers to work
together to try to actually analyze the
data gain insights together and because
of azure ad because Azure ml is
collaborative the data scientist
researchers working against this data
can be anywhere around the world they
canna care the models and put a put a
mod predictive model into production as
a machine learning web service for a
community to offer now the first
deadline for proposals for this program
is sep tember 15th the next deadline
will be november 15th and every two
months after they'll be reviewing
proposals you can actually apply online
at the link that's provided at the
bottom of this page so again too
exciting announcements next week now we
have a diverse audience listening in
today and so before instead just diving
and I talking to you about and showing
Azure ml which I will later actually
wanted it to step back a little bit and
say what's going on a machine learning
why are we so excited about machine
learning why is the industry getting so
excited about machine learning what is
machine learning and how should we sit
be thinking about in its potential
impact so I wanted to spend just the
first 10 or 15 minutes just sharing a
perspective just an overview if you will
on to what we've seen the power of
machine learning then I'll turn the
conversation over into why did we build
Azure ml why did we choose the features
that we've chosen for this first on
version of azure ml what's motivated is
what we're we hearing from users
potential users it actually influenced
what we built then I'll spend about 15
minutes in a demonstration of azure ml
so you can understand what the heck this
is and then we'll close and open up hum
for broad questions now if at any point
during the presentation you feel that
there's a clarifying question that needs
to be addressed I would encourage you to
go ahead and actually send it in to the
moderator Kim will actually then surface
a tamiya will help you know in the talk
itself otherwise we'll leave plenty of
time at the end for for Question and
Answer so machine learning simply put
their computing systems that improve
with experience and experience comes in
the form of data or training sets now in
a world of big data one of the first
thing that's really changed is that our
customers are researchers are able to
capture enormous volumes of data about a
process about an individual that they
actually want to analyze so having
ubiquitous data and relatively cheap
compute it's actually really made this
possible for machine learning to
actually be applied the the potential
for machine learning we actually view it
as being one of them
most impactful technologies that's going
to shape I t-shape applications and
services of any other technology out
there over the next decade we actually
believe it they were largely
unappreciated and its impact is actually
going to be profound over the next
decade and this hasn't gone unnoticed I
mean for many years bill has been saying
this when I quote he was noted for if if
you invent a breakthrough in artificial
intelligence so machine can learn that
would be easily worth ten Microsoft's
and even more recently such an Adela
here's our current CEO made the
observation that over the next decade
computing will become even more
ubiquitous an intelligent will become
ambient and that's actually very
powerful it's we have so much data
floating around us we have devices
around us all of this information
imagine being able to analyze that and
make sense of it and any point in time
our devices or applications are going to
basically be powered by machine learning
to actually help us to make smarter
decisions and entered a world of
connected devices connected cars
wearables we have this opportunity to
basically be surrounded not only by data
but then augmented intelligence services
so it has again it has great breadth and
depth of its applications and we're just
starting to see the beginnings of it and
it doesn't help Stubing what was one of
the most first successful applications
of machine learning and how did it
change the game and one of the first
successful applications was handwriting
recognition now in post offices they
were early days in the in the late 80s
Early 90s basically to actually figure
out where a letter should be routed
there were systems optical character
recognition systems that were written
they were largely rule-based imperative
declarative code that basically would
actually analyze look for the letters
and actually run through a series of
rules now these systems were incredibly
challenged by different letter formats
different handwriting styles and they
fell over regularly and required a lot
of a large amount of maintenance to the
point where I just became untenable
actually the US Post Office actually
commissioned a large research grant
funding call for proposals to actually
try to find new ways to actually process
it and the actual way that that
real um into a successfully implemented
was to treat this as a machine learning
problem we talked earlier that machine
learning requires data requires training
sets to gain experience so how it was
handled in this case is it individual
letters from different he had writing
styles were captured and labeled with
the correct output and in fact lots of
training samples in some cases they use
computer simulation to add noise
rotation and look at many examples and
then labeled that data fed all that in
into a machine learning system and what
you end up with is a relatively accurate
digit classifier but something very
unique about machine learning they never
they never ship they're never done they
make a mistake that's no problem you go
in and actually let correctly label put
that back in your training training set
and you retrain and so this is actually
resulted in what the state-of-the-art
systems are today for optical character
recognition understanding documents and
document understanding is treating it as
a machine learning problem and letting
the data and a machine learning
algorithm actually build the application
no handwritten code it's actually
learning from the data with machine
learning and of course there's
statistics underneath of the covers
we're not going to get into that in the
talk but basically it changed how what
was a previously challenging problem got
attacked the fact that a modern
implementation that you can actually go
to the azure data marketplace and
download through the Bing language
translator and when you happen to be
traveling around of parts of the world
you can actually point your phone at a
menu in any language take a photograph
tell what language you want it to
translate it to and it will translate it
into the end of that language why
because behind the scenes we've used
machine learning from our microsoft
research technology stack to actually
build a language translation system
recognition and translation system so
why do you why do we do learning well
you learn it when you can't code it
recognizing speech recognizing images
written handwriting gestures you learn
it when you can't scale it
recommendation systems at the scale of
netflix or amazon this is not something
you could write a recommendation system
for by hand but you have sufficient data
on people's behaviors and what they do
that you can in fact apply machine
learning and build a highly accurate
recommendation
system similar with fraud and similar
with spam all hard problems that have
all been successfully attacked through
machine learning and you learn it when
you have to adapt or personalize it
predictive typing on on a small handheld
device is now being attacked by machine
learning actually learning my voice and
actually building a highly accurate
speech to text translator just for me in
the way that I talk so this is something
you could not write have a developer
write a custom app for every one of your
individuals but you can in fact learn
from individuals data and you learn it
when you can't track it I a I gaming
robotic control there's a myriad of
applications where you have the data
it's an intractable problem to route to
actually address by hand use the data
and machine learning to actually build a
solution and hey if it doesn't work the
first time if it's not highly accurate
that's fine because you're ever
essentially building a training set for
the next generation of the application
now let's look a little bit what's been
going on over the past years and talk
about what we can do today you know in
the 1980s when I was highly active in it
it was the connectionist or the neural
network period of machine learning and
we're really looking at how to actually
evolve neural networks we did Mulligan
that true proved to be a very tricky
algorithm at a very tricky technology to
get right with a lot of hand tuning and
then the 90s we started moving into the
symbolic era of machine learning
decision trees were like she'd the
dominant method that what was called
cart classification and regression trees
seed of 4.5 was the classic algorithm
and there were expert systems and rules
as well that started to surface at that
point in the 2,000th we actually saw
support vector machines and the rise of
so-called colonel machines to actually
allow us to tackle even more difficult
problems in higher dimensions we saw the
real rise of statistical learning theory
and actual scoring systems this is what
we saw machine learning Scott was being
put into production by fair Isaacs and
FICO scoring and risk scoring where
predictive models were put into
operations and businesses started to run
at scale over these over these models
that were making predictions in a 2005
we saw the rise of graphical models
where we're not just learning for the
data we're actually trying
statistically model the data was an
interesting fork and it's that that
exists today in fernet from Microsoft
Research Cambridge is a fine example of
that where it's actually understanding
the statistics of the data modeling that
in its dependencies and yes fine-tuning
as you understand the distributions of
data but not just trip not just allowing
a machine learning algorithm to learn
from the data you actually model it
effectively and more recently and what's
really been exciting rate lately is big
data and deep neural networks I'm not
going to cover it in the talk today we
will have support for deep neural
networks in fact we do and our current
and release for Azure ml but its
implications for understanding truly
understanding what's in an image truly
understanding language translation and
actually being able to learn multiple
languages at the same time is profound
all because of basically the work that
started back in the 80s which has
continued a neural network theory to the
point we can train arbitrarily deep
neural networks but the real turning
point here has been the fact that we
have large scale compute in the cloud in
particular or GP GP use for deep neural
networks for example we have a tons of
data and we can now actually apply this
data even the simplest of algorithms
when given large quantities of good data
can actually become highly effective
machine learners let's just look at a
couple examples here within Microsoft
before we turn our attention debt to
Azure ml the Xbox Kinect was it was a
you know clearly a commercial success
from Microsoft but it was also an
incredible technological success if you
look at and have played with the Kinect
you realize that it it watches your
gestures it figures out what you're
doing and if you think about the
diversity of challenges that had to be
faced there you have individuals and
different postures that you have to be
able to recognize you have individuals
of different body shapes and sizes
different ages of individuals so the
whole the relationship the whole ratio
is completely scaled individuals will
have have accessories attached to them
and you have to be able to figure out
still is that there are more is that a
purse you have to be able to pull
foreground out of the background you
don't have to clear your living room you
can have
plants in your living room couches in
your living room lights yet that Kinect
sensor can still actually figure out
where the individual is in the room and
what to ignore and in fact when you can
do multiplayer you have multiple people
in the room and guess what it actually
tracks each individual independently and
is able to understand what they're doing
this is clearly one of those problems
that could not have been tackled without
big data and in fact how we built it
again it's a machine learning system so
we're going to have to build a training
set we had individuals where these
sensor suits in which we could actually
record their gestures label what they
were doing building a label training set
to feed into a machine learning system
and this was no small task actually we
had 800 million training examples from
individuals and that was kind of
expensive to do to spend that time with
that individual in that suit and capture
that data so we actually and we needed
to to get the performance actually
simulated using both machine learning
and simulations to add variations
adenoids do rotations of these
individuals to create another 800
million training samples by the time it
was done we had roughly 2 billion
training samples that we had to fit in
this is the era of big data and big
compute meeting machine learning that
was fed into a machine learning system
so the Kinect sensor that you see today
and are interacting with when you pay
Xbox is basically being driven by
machine learning systems the same
machine learning algorithms are actually
in Azure ml to allow you to tackle
equally hard problems and there's a
couple other cases you may not be aware
of that we've actually applied
large-scale machine learning and search
we've actually treated search as as a
complete machine learning problem in
fact all the results that you see no
less than a dozen machine learning
algorithms have shaped influenced what
you see on the screen everything from
what links are most likely to get
clicked and will actually put down those
closer to the top with something
misspelled did you really mean to type
something else in in which case we'll
actually go ahead and correct the
spelling and do a search on what we
think is the correct spelling but also
put a message
they're saying did we get this right you
can see if you should be connecting the
dots we're actually going to build a
training set from all this if we got it
right what was at what language were you
speaking in what was your intent we've
been watching we watch the series of
searches we look at the actual words
that were used in the query to say are
they trying to find information are you
trying to make a purchase are you just
serendipitously surfing and based on
that bring up related searches which we
feel are contextual based on the on your
search the words that you've used behind
the scenes before we serve up a page
we've actually run an ml algorithm to
see if the page is malicious and
filtered it out so you only see what we
believe to be safe pages and we're also
we're serving up ads and we're asking
the question what's the probability of
each click on each ad and that often
deals with what your context is what
you're doing aren't what's your intent
are you trying to buy are you just again
looking for more information what ads to
show and in what order and afterwards
the end of the day what pages should we
re index to actually improve the
performance and your clicks and your
choices that you make in the search and
how far down you have to go to make that
first click all become information for
us to actually retrain and recalibrate
our models every night and really
machine learning enables just about
every value proposition out of web
search in fact the more people search
the more people use the better the
system gets it learns over time here's
another problem which actually some
might find interesting and applicable in
their day domain is a really challenging
problem you know we in our data centers
this is what you're looking at our logs
coming off of our office 365 hosted
exchange from our double in data center
now our data center operators are
actually very smart individuals are
looking at dashboards that are
monitoring their servers the services
the networking the temperature and the
services themselves are giving us
feedback and they attacked it in a
traditional manner they wrote a series
of rules handwritten rules and code that
would monitor look for KPIs and spikes
but you know darn it it never really
helped them with finding the root
and so after several months of not being
able to predict failures not being able
to rapidly diagnose a failure excited it
treated as a machine learning problem
and actually captured all of the sensor
logs that were coming off the machines
the network cards the services
themselves other ambient information
within the data center and started
capturing them as logged time series log
and when an event happened we just
simply had the operator press a button
saying this is the type of it a service
failed a disk drive failed the whole
rack went down or had to reboot the
service itself and let machine learning
actually analyze the data and figure out
what the root cause was so hundreds of
thousands of machines hundreds of
metrics and signals per machine and
really allowed machine learning to
actually find out what correlated with
the real problem because again just
because it happened once doesn't mean it
was actually strongly correlated we had
logs from months and also how can we
basically extra extract effective repair
could we do predictive models that says
this disk drive is going to fail and the
next week you might as well replace it
now or or the server is going down you
might want to move load off of it to it
to other servers and so now our
monitoring is to a large part shouldered
by large-scale machine learning so
hopefully those of you are thinking
about how you might actually apply this
technology I've got a little bit of a
better sense to the kind of applications
to which you can apply it to sensor data
actually user interactions services even
actually predicting what individuals are
doing off of off of images so if we
think about machine learning it really
allows us to solve extremely hard
problems better it allows us to extract
value from big data and drives a shift
in data analytics and there's actually
kind of something a little bit more
subtle going on here it's really about
changing your way of thinking about a
problem it's about getting the data
getting labeled data getting good data
and actually trying to let an algorithm
build the solution for you and again it
doesn't have to be perfect but as long
as you have a feedback loop you have a
chance to actually iteratively improve
the performance of an ml solution over
time which is really compelling
so a large number of scenarios just
we've got and had exploratory projects
in almost all of these all the way from
predicting who is going to basically
return to a hospital after being the
readmission rate doing telemetry data
and analytics for connected cars
predictive maintenance and manufacturing
churn analysis life sciences researches
research targeted advertising smart
meter monitoring maybe we've had a
number over the past year with with
customers and and researchers we've
given access to of these applications of
applying machine learning there's
there's simply no limit to the kind of
applications to which this can be
applied all you need to have is data the
ability to label that data or give
feedback and access to machine learning
at scale to actually start making sense
out of it so this is where Microsoft
Azure machine learning comes in and
we've made some intentional choices
about the functionality we're releasing
in this first version so I kind of want
to walk you through that and why we made
the choices that we made then all I
should give you a demonstration we'll
spend a little bit of time wallowing
around with the service since you have a
really clear idea of how you could use
it on what it can do for you the first
was you know we could have focused
machine learning on a lot of things
image analytics speech analytics
winstead said look where we see people
getting the most value is the ability to
develop and deploy predictive models and
not just build them and keep them on
your workbench but actually put them
into production as a machine learning
web service with the REST API and we'll
do it together today in a quick demo
will actually grab some data set will
train it will evaluate it and will
actually publish it as a web service
within minutes nobody else can do that
out there today and the reason we did
that again is just the breadth of
applications and another issue we wanted
to address is that the two impediments
to why people aren't embracing
predictive analytics is as much as they
are today the first there's actually a
handful of reasons but we're not one of
the main ones is the time it takes to
put into production so I talked about
you know that we could put a model in
production in a few minutes out of the
real world it can take three months to a
year to put a model into production
because it's called a code porting
exercise you build it in our studio and
then or
the other favorite tool and put in
production you have to roll up your
sleeves and start sitting down with a
developer to write code our target user
is a data scientist and specifically
what we're calling it an emerging data
scientist this this tool does not
require a PhD to use we really wanted to
make a tool we're seeing a new
generation of data scientists coming out
of certificate programs self-study
they're moving over from their bi
profession or their technical computing
profession and getting interested in
exploring data and we wanted to make a
tool for these individuals to be highly
productive and build models that are
just as good as somebody who may have
been doing this for the past decade and
using SAS and again we're optimizing for
the fastest time to a deployed solution
and something else which was inspired a
lot by our work with with researchers in
the sciences early and understanding
that businesses was moving that way is
its support for collaboration we'll talk
about it in the demonstration and we'll
talk about a little bit more but we
support collaboration and sharing it is
meant to be an experiment platform you
can share data with researchers anywhere
around the world you can share your
experiments with members of your group
or other research groups you can even
share the finished web services that you
build and maybe make calls to them to
augment your web service so again
support for collaboration and sharing
whiskey to this something we heard from
researchers but also from business users
one of the things that we heard when
talking to people access to high-quality
ml algorithms is hard it's either
expensive but the reality is one package
might have very good decision trees
another ml package has great neural
network support another one's got some
pretty darn good support vector machines
and you need them all for various
applications but having to learn four or
five packages it's it's it was a mess
and the affinity group was about on the
order of a dozen packages you had to
have access to and it's not just ml if
we're trying to actually we're doing
data science we're reading raw data in
we're cleaning the data we're selecting
features which have the most information
we're trying out different ml algorithms
and running experiments we're going to
evaluate which one is best so you really
need to have intense
you shouldn't have to use multiple tools
and then finally that ability to put a
model into production which I've
mentioned already some of our guiding
principles was there should be no
software to install if you have a Chrome
browser running on Linux or if you
happen to have IE or two other browsers
will be testing over time you're good to
go there's no software to install it's a
fully managed cloud service accessible
through a browser so nothing that
basically prevents you from getting on
and doing data science it is
collaborative will actually show you
today how you could actually invite
somebody to join this workspace which is
well I'll talk about what a work spaces
in the demo and we tried to minimize
coding to the extent possible you'll see
that we've got modules which have
actually there behind that box which we
call a module there's actually hundreds
maybe thousands of lines of code
implementing an algorithm to you the
user the data scientists they don't have
to think about that they simply say I
need to read this data source and they
drag a module onto the pallet that
actually reads the data source up I've
got to clean missing values well you
drag a module on that cleans missing
values and connect your data set to the
missing values replacement set the
parameters on how you want missing
values replaced when it comes time to
create a training set you drag a module
on that does that you tell it whether it
is stratify it or how to split it
between training and testing set so
you're really thinking about your data
and your experiment you're trying to run
you're not thinking about writing code
something else which we're not going to
get into today is that those modules you
don't know one of them could be a Hadoop
job one of them could actually be
reading and running its execution on
sequel one of them is actually running
an r package on a pool of our servers
that we have behind the scenes so it's
something else we've factored out you
don't need to learn all these different
systems again you're just thinking about
what you want to get done we are running
an orchestration and coordination engine
behind the scenes that moves the data
transforms it from an our data table for
our services to a our internal data
format or maybe back over to HDFS and
it's extensible so we have support today
that you can write arbitrary our code we
will have support for Python there's
over 400 roughly today 400 our packages
that we've uploaded and we're going to
allow you the
to upload our packages I'm not in the
current release fitness and an update
coming soon so that basically if you
happen to find an r package that looks
very compelling to you the few clicks
you could actually add it to this your
workspace your private workspace that
you and your team have access to we also
have an SDK coming out later this year
which in fact you might say hey I'm
going to actually wrap some of my own
code I've written I've written a
simulation engine or I've written a
rules engine or optimizer you'll be able
to create your own modules with the SDK
and then finally what you'll hopefully
notice in the demonstration it's an
experiment platform data science and
machine learning is really about running
experiments there's no rules on what ml
algorithm is best so we want you to
experiment an experiment fast and be
able to do side-by-side comparisons roll
back to what you're working on two days
ago and stand for cough and start from
there so basically you can try things
out but once a model has been built and
put into production it becomes immutable
so if your research team has built a
model for making a prediction about
whether about earthquake whatever and
it's that the team has decided that's
the model that they want to run on it
becomes immutable but it's not invisible
people can actually search for it they
can discover it and they have the
ability to clone it and create a copy of
it and start from there so that that
allows you basically to protect your
valuable models but also allow other
people to actually reuse them then again
we keep hitting this on because it's the
key differentiator that nobody can do
today quickly deploy it as an azure
service to our ml API service that will
host it might make sense to say a couple
more words just a few words about the
API service because this is pretty
powerful before running the demo
increasingly we're moving into a
services world you know sometimes we
think about big monolithic services when
it in the future apps are going to be
made out of hundreds maybe thousands of
micro services and so this ability to
say I've just made a predictive model
that is going to predict whether this
was spam I've made a predictive model to
figure out the intent of the user and so
when creating a web page for example our
web page the result making calls out to
these dozen you know ml services makes
perfect sense and actually
shape or influence the final result so
and having an API service that you can
call and say what models do you have in
your repository what's their API and
then you can make a rest call to it
allows for incredible agile composition
of these web services but when you put a
web service or your model into
production on our API service actually
we don't spin up a vm and start charging
or you know just have it setting their
Idol instead we intelligently keep it
indexed and hot if a request comes in we
pull it into a hot vm pool and start
serving up requests for that that model
when there's no more incoming requests
week we asked the model or if somebody
that maybe the whole research community
is pounding away on a model we can
actually start scaling it up across
multiple VMs to satisfy the SLA or four
that's been set and we have two modes
that we support today so when you put a
model into production you actually get
two instances of your service the first
is a request response which means you
send a record and you get a record back
that with the scored result so is this
transaction fraudulent is this patient
going to readmit or batch mode we can
send in an entire file saying hey here's
a hundred patients that checked out last
week please give me a list of which ones
are high risks that I should actually
follow up on what you get back as a file
of scored results and there's some
monitoring and telemetry we provide to
the person who's hosting these models so
they can see which ones are getting used
by who by whom and how much data is
going through them so there's there's
capabilities there as well so i'll be
spending most of my time talking about
mo studio or showing um else to do but
there's that second back end model
management service which is really
powerful so with that I'm going to stop
and go over and actually run a demo Kim
sometimes when I when I change from
PowerPoint over to browser sharing stop
so please let me know if sharing is
still going on okay I'll let you know
thank you so really what at what I just
did is I just open a browser I just
logged into my workspace and I could
have opened it with chrome as well and
I'm in what's called my workspace which
I mentioned to you earlier
and usually on first-time users what you
land in is basically hey welcome to ml
studio preview which has a forum that
you can ask questions we respond other
users in the in the community can reform
can respond as well we have all of our
online documentation for how to use the
service updates of what we've to what
has changed and samples both experiments
that we've built a gallery of sample
experiments with sample data sets to get
you started videos they'll actually step
you through how to use it but really
when I do my predictive modeling work
what I land into this is my work space
and what I see here are samples that
Microsoft is ship to help you get
started everything from doing a time
series forecasting with arima which are
all our packages well back to how to use
different functions such as how to read
a data set how to do cross validation
let me just expand this out these are
all out of the box to help you get
started using regression binary
classification multi-class casts of
classification some UC irvine reference
data sets that we've built models for
for breast cancer detection flight
dictated German credit these are all
either from UC Irvine or former kdd
competitions down to basically how to
get make best use or best practices in
using our service sentiment
classification for example to allow you
to do sentiment analysis and you can see
you can actually build arbitrarily
complex tags all these r amenities are
documented what we've done each step so
you can actually read how we've used the
service to help you get started I can
also see I've obviously not been busy
because I click relate recent work and
it says Roger you haven't done anything
recently I can actually go and see the
experiments that I am working on right
now these are models that I've built
over over the months but I'm not limited
to just my own workspace I can actually
say hey I want to go join one of my
colleagues Mona and go work with her on
an experiment she's given me access to
her workspace and then go work with her
on a problem or I can go work over with
my colleague Manish so I can bounce
around between different workspaces and
literally change the projects I'm
working on so for example for the
academic program an entire class can
have a workspace or they can have their
own workspace and students can work
together or researchers can work
together let me go back to my workspace
and I may decide that this is our videos
here I may decide that I need some help
on a project and so i can say simply go
over and say want to invite somebody to
join me this is where i would type in
their live ID which is her email address
i would basically set them as an owner
which means they could delete and do
whatever they wanted to in the workspace
or just a user which means i want them
just to work on models with me hit Send
they get an invite they open up their
browser hit the link and they can
immediately join me here in this
workspace and we can work together let's
actually let's actually take a little
bit of a tour OPA first stop them let me
say new data set somebody could have
given me a data set from my local file
and you can see here that we can upload
to the cloud from from ml studio from
the desktop CSVs tab-separated files
text files a zip file and our our data
object at our file format that's how one
way of getting data in now this is the
authoring experience I'm I said let's
create a new experiment we're going to
create a new model together what you
notice is a few things have changed here
this is the authoring palette which is
kind of a hint to hey drag stuff on here
to get started but you'll also notice
over here on the Left I have drawers
available to me of functions these are
models that I've trained and put into
production these are data sets that I've
uploaded data input and output a generic
reader I pop this guy on open up the
dialog box over here I can read from
blob storage as your table storage I can
read from a sequel database on the cloud
and HTTP sites such as the date UC
Irvine or maybe you set up an HTTP
server to serve up day
run a hive query over Hadoop run a power
query all from this module you see as I
make these choices the dialog box
changes it this is the necessary
information this module needs to run
that's the extent to which we have to do
coding let me just take a data set in
fact well let's do one other thing let's
just get started with some there's
actually a very simple demo I like to
run and it's it's this bike buyer I've
got two years of customer data of people
who have been buying product from me and
in fact if I just click visualize
regenerate html5 so we can visualize it
on any browser there are 10,000 rows in
this data set 13 columns there's bike
buyer whether you've bought a bike from
me or not I click it I can actually see
the frequency go ahead and close that
that's interesting oops for me to close
that completely but from this pop-up I
can actually get all the metadata that I
need marital status it's pretty pretty
good mix of single and married genders
balance i can see income is is not
children well that's interesting it's
like most people don't have children in
my data set different education levels
if i scroll down also don't have any
missing values i have to deal with so i
get a lot of lot of information from
this visualization at any point in the
end in the workflow we have this
information available i just have to
click the pin and it will actually show
me what's inside my data set so i don't
have to deal with missing values but if
I did everything searchable over here
not only is it in a drawer but if i just
type in missing well there's my missing
values scrubber module go ahead delete
that I don't need that the interest of
time let's just go ahead and create a
training set and let's just split it and
we're going to send the data set down to
the splitting routine open up the dialog
box just a couple quick things to note
i'm going to use seventy percent for
training but we also have support for
stratification those an applied machine
learning will know what this is I mean
if I'm trying to predict in the future I
would use a timestamp column and
stratify it only seventy percent of the
data that was actually behind some time
stamp would be used for training the
other thirty percent for prediction so
we can stratify just with the click of a
few buttons we will we don't need to
stratify this
this point let's go ahead just click run
now this is the first time I've really
done any execution on the cloud I just
sent the dag up to our service to a data
center it's in our south central us it's
done I open this and say visualize and
sure enough seven thousand rows the
distributions look look good nothing's
really changed that's my training set so
at this point let's just go ahead and
train we have a generic trainer that can
train any ml algorithm needs to get
retaining set I'm going to need to score
which is basically I got to test the
fish for minh sub the model on data it's
never seen this hold out data which is a
great proxy for how it's going to work
in the real world and because i like to
look at things in run experiments i'm
going to need to have something to
evaluate different experiments and we
have that as well this evaluate model
let me just shrink this down a little
bit it's getting a little cumbersome and
close that so at this point it's like
okay well well what ml Agra then we're
going to use let's go ahead and close
this up and just give you a sense of
what we have available to us again we
can do filtering cleaning transformation
everything that you need for doing data
science and end is here open machine
learning close these drawers that I had
opened we're doing a classification are
you going to buy my bike or not and so
you can see we have a large number and
growing daily of classification
algorithms so I'll go ahead and I'll try
you know i'm going to try a two-class
logistic regression I don't wanna do
regression I'm doing classification will
actually say we're going to go ahead and
do to class average perceptron simple
neural network that's the model we're
going to train now at this point we've
got a red bang here that says hey I need
some help this guy needs to know what
column we're trying to predict well
we're trying to predict whether you're a
bike buyer or not that's it everybody I
mean minus the narrow Thibodeaux taking
me all of a minute to author a machine
learning experiment I click run this dag
is being sent up to a shiur for
execution and I'll start giving me some
progress
bars so it's split was already done
we're loading the perceptron modeling
and configuring it we're now training it
till it until it converges on on the
best-trained model that can with that
training data strained now we're scoring
it on data it's never seen before
and we're done we've actually done the
evaluation as well now of all the
metadata here for this experiment let's
see how it did and what I'm looking at
here we're pulling out a lot of data
information I'm feeling a lot of people
are hitting the service this morning cuz
it's running a little slow Roger well
that is loading we do have a question
yes please it says what is the
technology to capture the data how would
you capture the data for an IT
environment let's say for an IT
environment where we want to plan for
capacity based on GPU memory disk i/o
etc or predict issues okay so we've done
an application similar to that and so
what we what we did is that they put in
the Indian while this is running
extremely long we did we did um
interception at the devices itself
basically intercepting and writing to
local file systems for the local
machines that data was then moved over
to Hadoop cluster where we ran hive
queries from azure ml I'm actually
stopped us so basically the bottom line
is is you put logging intercept and
logging data close to the service that
you're trying to do predictions on move
those into files then we can run hive
queries over them to actually
aggregating create the necessary
information so that is just not
rendering there we go yeah so it didn't
one of them failed when I'm succeeded
sorry about that folks that's a cranky
model right now cranky visualization I
think i wedged it when I when I clicked
it let's go back over to my recent oh
don't tell me I lost my experiment I
didn't so if the answer the question
again we've actually done local
interception written them to local files
move those local files up to Hadoop ran
a hive query over them and then from
azure ml and pulled that that hive the
results of that hive query in and that's
we started our model to build predictive
modeling so let's let's see if this is
going to be a little bit it should be
much snapper there where that's the way
it should work I'm gonna set a
Heisenberg so what we're seeing here
folks before the studio's do a quick
experiment this is an ROC curve for
those of you we're familiar with it you
know that that basically it's the it's
the true positive versus the false
positive rate for that hold out set so a
perfect model would have had all true
positives no false positives a model
that was flipping a coin would have ran
the diagonal you can see we've done a
little bit better than that and in fact
if you see if you scroll down a little
bit you can actually see the confusion
matrix you can see a number of
performance scores but the short answer
this model isn't all that good it's
better than guessing let's see if we can
do better and what again we're going to
try to push this experiment paradigm a
little bit i'm going to Train another
model really quick and I know I'm going
to need to train its score so I'm going
to clone those those modules I don't
need i'm not going to Train another
perceptron but i am going to train a
boosted decision tree I'll feed the
boosted decision tree into the trainer
I'll train I'll feed the performance of
the model to the same evaluate model to
do a side-by-side I'll click run again
only the work that you have changed get
sorry executed everything else stays the
same and again we've done lineage and
provenance tracking so that all the data
from the previous run is retained as
well so while this guy is training
scoring it's already done and I gave it
kind of a let me just let it let it
finish and then show you something so
right now it's doing the side by side
evaluation which is generating a lot of
html5 it's done this one I wanted us
click the boost decision tree if you
looked at it we've just trained a
hundred trees in this ensemble and for
those of you are understanding decision
trees each one of these is a machine
learning algorithm we've trained 100 of
them it together to actually help us
make a decision so that's an ensemble of
machine learning algorithms in reality
we hope it does a little bit better
let's see click visualize
and sure enough significantly better the
ROC performance is much higher is click
it in fact a good proxy for how accurate
a model is our previous perceptron an
area under the curve which is just
literally the area under this curve is
sixty-eight percent so it's a
sixty-eight percent maybe sixty-nine
percent accurate algorithm this one on
the other hand eighty-six percent almost
eighty-seven percent we clearly have a
winner here what if we wanted to put
this in production we wanted to actually
run our business on this so let me show
you how fast we can do this let's try to
clear some room out here come on Mouse
is being finicky today but that's okay
so I save this train model so I'm going
to save the machine learning algorithm
model that I just trained it's saving it
new experiment bring that train model
back on the pond to the palette there it
is we overload the use of our scoring
module to be a harness for building a
web service now web service has to know
what wet data oh no don't do that what
data what the data incoming data looks
like so I say look for the web service
this is what data incoming data is going
to look like for the web service this is
what the output should look like now
folks we're really building an
arbitrarily complex web service here if
I had modules that might write to a
database run a rules engine I could have
dragged them on and made that part of
the scoring model I'm about to put into
production as a web service I'm just
doing something very basic today but
literally you can build an arbitrarily
complex mission web service that uses
machine learning and publish it I click
run so what's going on behind the scenes
is that it is looking at the data
building an eight REST API it's done I
say publish would you like to publish
the service
yes it's now talking to Azure and we
have just published a web service not
just one web service but to web services
to Azure a request-response web service
a batch web service I could actually
test the model running on Azure right
now just enter the metadata here and say
well this person by a bike or if one of
you said yeah I'll tell you my marital
status my gender my income my children
my education would I buy a bike or not
and we could test our predictive model
right here and it's in the actual
environment which it's going to run in
production another cool thing is if you
click this and there is the URL the
OData endpoint for this model here is
the URI for it scroll down a little bit
further if you wanted to write some code
in Python well we've given you all of
the rest headers associated with that
for what what the request body should
look like what a sample request should
look like scroll down a little bit
further the response coach or model is
going to give you going a little faster
because I wanted to show you this we
even generate the c-sharp code to call
this model so you could copy paste into
your IDE compile and you're good to go
we generated in Python as well we
generated it in our as well so the time
not only we've put the model in
production but we get you give you code
to help you get it up and running and if
I go back here and I say okay let's do
configuration I like my service put into
production now the rest of the world can
actually see this model and start making
calls to it that's the speed at which we
can basically go from experimentation
with different ml algorithms writing a
model putting it into production as a
web service and people can immediately
start calling it so I'm going to stop
the demo there and just kind of go back
and just say a few closing words before
we open for questions I mean we're doing
this on the cloud because we see a world
where devices vehicles and other other
devices will be sending data to the
cloud for for learning over but also
trying to make informed decisions
there's no reason why why your vehicles
can't tell you in advance when they're
going to fail we have customers right
now monitoring fleets of vehicles we
have university groups who are doing
predictive models for when it when a car
is going to fail or win it
energy for electric vehicle so a lot of
applications there and it is the pull
back a little bit what's really making
this happen I mean it really is the
power of the cloud to support
collaboration of crowd sourcing and
collaboration the ability to have
large-scale machine learning and
analytics because you need it when you
when you're learning but when you're
done the machine learning models so
small so fast it could be integrated
into small devices so it really is the
cloud where we think everything comes
together scalable machine learning cheap
storage of data all the compute you need
when you need it for machine learning so
just a couple closing comments again I
wanted to remind you about the azure for
machine learning award program the
Microsoft Research is is hosting or as
is operating for academics and nonprofit
institutions the first call for
proposals will be 4 sep tember 15th the
next round for november 15th and they've
also got this part of a broader azure
microsoft azure for research program as
well which you can actually get grants
for using Azure for your research
project as well not just as your ml and
a few things to point out you can
actually sign up for a free trial of
azure ml and play with it if it is so if
you don't want to wait for the academic
awards or if you're not if you're into
industry and get access to it we've
actually put up a machine learning
center which basically has tutorials on
how to use the service samples and a
community we're trying to build around
machine learning at it we also have a
blog that we try to keep in contact with
our users and our customers as well so
with that I would like to stop and see
if we can just take questions in the
time that we have remaining okay great
we do have a few more questions
regarding IT question is there any
documentation available so there's
documentation on the entire service the
modules the the of all if you're asking
about that particular scenario of how we
built it that would require a white
paper we but we will be preparing a
white paper on that but but the actual
service itself if you go to our ml
machine learning center you'll find full
documentation on the service the modules
samples videos to get you up and running
okay thank you and with the large
variety of different tools and
information moving between between
different platforms how much information
will there be available on performance
impact of each component on the palate
that's a good question so we actually
get micro data you can actually click a
module in an experiment run it's
something I didn't show you by the ways
you can actually go back in time and go
back to previous versions of your
experiment but you can click a module
and you can say hey that module took
this much time that's a bottleneck in my
machine learning let me try another
algorithm that were might run faster so
we give you performance data on each
individual module as well as the entire
experiment how long it took to run so
hopefully that answers the question okay
and will the presenter be covering how
the pricing model will work for as your
ml ok so our current public preview
pricing I'll tell you what are so with
the spirit of it is actual numbers we'll
have to pull out auto out of some file
somewhere but the current public preview
pricing is literally we're doing cogs
recovery for EM else to do that is we
only charge you when you're sitting
there staring at the screen you're not
you're not incurring a dimes worth of of
CPU or couch or cost when you hit run
and your experiment actually starts to
consume CPU cycles then and only then
are we actually charging counting helmet
how much CPU got burned and that's
basically what we're charging a
chargeback for just cogs recovery for
the CPU cycles that were burned while
for experimentation which really brings
the cost down now when a model gets put
into production we have a per prediction
price and if your model happens to be in
Ted incredibly in compute intensive we
actually charge for the compute as well
so that's the rough outline of what the
pricing model is at least for the public
preview until we understand how people
use it how much compute they use and
what you know if we could have a package
deal where we sell blocks of compute
time so we're still figuring that out in
our public preview okay and it looks
like we have one last question is there
support for probabilistic programming in
as your ml not at present we do have
technology here within Microsoft and
Microsoft Research that will be pulling
it in and
offering that functionality as modules
as well as well as well as support for
things like simulation and optimization
as over time as well modules that do
that but again with that SDK that we're
going to be making available later this
year if you find a good probabilistic
learner or probabilistic our simulation
package or rules engines that you want
to incorporate the ability to wrap that
up as a module and put that into your
workspace for your team to use is always
going to be an option for you okay and
one last question what is the best
internal contact so we have a support
alias somebody can start with me for it
for internal just barge a VAR GA at
microsoft com and i'll make sure you it
gets routed to the right person or
address the question myself if i can
okay and one more when will as your MLB
available in the dublin data center
that's that's top priority on our list
we r we we will GA with with with it in
the dublin data center in the amsterdam
data center and there are other data
centers i don't want to get into but our
G a roadmap like all azure services is
to make sure we roll it out to all of
our data centers by GA and so won't talk
about a ga roadmap but I certainly don't
want to so we don't want to set on the
service for too long waits it'll be
moving forward fast okay that's looks
like that's all our questions I will
thank you everybody for for your time
this morning I know for some of you as
early some of you as late so I
appreciate everyone I'm tuning in for
the talk and I look forward any feedback
you might have on our service and how we
might help great Thank You Roger we hope
that you found today's information
helpful if you enjoyed today's webcast
or have feedback on how we can provide
you with a better event please let us
know by completing our survey you should
see the link to the survey and a pop-up
window on your screen and all materials
from today's presentation will be
available on the archive page within 48
hours I'd like to extend a big thank you
to our presenter Roger this concludes
today's webcast you may now disconnect
from the call
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>