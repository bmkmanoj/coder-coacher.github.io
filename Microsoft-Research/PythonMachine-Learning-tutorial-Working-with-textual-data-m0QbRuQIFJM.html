<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python+Machine Learning tutorial - Working with textual data | Coder Coacher - Coaching Coders</title><meta content="Python+Machine Learning tutorial - Working with textual data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python+Machine Learning tutorial - Working with textual data</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m0QbRuQIFJM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
let's
get back to grid search so as we've seen
previously we we did the feature
imputation manually it means that we
took the original data frame and for the
missing values we replace them by the
median on the whole data set the program
here is that we have actually cheated
because before we did the Train test
split we have used all the data set to
compute the median so maybe the
information that was transferred by
computing the median even on the test
set was actually helping our model in an
unfair way so this is called what we
call data snooping so it's using data
that should not have been available at
modeling time so to check whether or not
we actually this cheating was actually
helpful or not we can use what we call a
pipeline in scikit-learn where we will
combine different stage as part of a
single model that will do both the
pre-processing the feature extraction
and the training and then we will cross
validate the whole pipeline so that we
are sure that we are not cheating
when we do the future amputation because
we will do the future imputation on each
validations on each training set for our
cross validation loop so let's try again
to do the to extract we still need to
work with a numerical representation of
the data as an umpire race for
scikit-learn
so we'll do it exactly the same of
chartres extraction as we did previously
but this time instead of doing the the
feature imputation using the median we
will just use a fixed value of -1 for
all the missing values it's just a
marker because the person with an age of
-1 doesn't mean anything so it's just a
marker to tell scikit-learn that oh
there is a missing value here because
the natural representation doesn't work
so we we use exactly the same feature
extraction but this time we use minus
one for the missing values so we are on
memory have the numerical data frame as
previously and now we we do the Train
test split before doing the actual
imputation before computing the median
and we can use from scikit-learn
pre-processing on the cycle on purposes
in package we import the the imputed
class and this impute our class has
different strategies to replace the
missing values so we use the medium
strategy as previously you could you can
also use the mean instead of the median
and in the future we plan to have better
strategies and we we tell it that any -1
value should be treated as a missing
value so we call fit on the training
part of the model and we then display
the statistics that it's it's collected
on a training part of the data set not
the model on the data set and so here
you can see that the second column is
the age column if I go back up for age
and so this time I have a median value
of 29 so it's actually slightly
different but it depends on the trends I
split that I did so so it should not
it's very close to the other value so it
should not help us too much but we can
check by going on so in once we have
fitted the impute err on the training
set we can do the actual imputation by
calling the transform method so
transform on this class takes the input
with the missing values and coded as -1
and we'll return the same data and
replace all the -1 values by the median
for each column and in this case there
is only missing values in the edge : so
we can check that in the original x
train
there are some values that are -1 but
once we go through the imputation there
is no no longer any minus 1 value in his
data so it has been replaced with
American and we can do the same on the
test so here you can see that the
imputed object was fitted on the train
path and was then used on to transform
both the training set and the test set
but at no point we used the test set to
learn anything because we we learned
only on on the training path
so this way we split the the estimation
stage from transformation when we have a
transformation that depends on the
estimation so now that we have this
impute or object we can combine it with
the classifier itself in a pipeline so
we create a new instance of our object
with the same parameters we recreate our
new instance of boosted trees with the
same parameters as previously and we
combine the two together using this
object the pipeline object actually in a
recent scikit-learn versions you could
also do make pipeline instead and I will
do pipeline to equals make pipeline it's
a it's just just a shortened in pewter
this way it's also create a pipeline but
it's a nicer way to do it
when you create a pipeline manually
using the pipeline class you need to
give a name to all to each stage in the
pipeline so as a string parameter when
you do that it will use the class name
as the name of the pipeline and it will
add the suffix if they are to processing
stage with the name same name so here we
call Crossville score on the pipeline
object that we created and we pass the
the feature values that have the -1
marker for the missing data and for each
cross-validation fold each of the 5
iteration it will call fit transform and
then fit on that and then predict on the
whole stuff individually and it will
iterate this process over and over again
five times and this way we know that the
model has not cheated in any way because
it has never used the test set values to
to do any pre-processing and we can
compare this the score and we see that
the score is actually the same as
previously and it makes sense because a
median of 28 or 29 doesn't in fact match
the model and so it's not a lot of
information to know the the median age
of the person to help make good
predictions so but it's a good sanity
check to in general to avoid their a
snooping to wrap all your pre-processing
into a pipeline to make sure that you
are not cheating so once we have a
pipeline what we can do is also do
cross-validation and grid search on the
whole pipeline at once so we did
cross-validation previously and now we
can do grid search
and you can see that this time we define
the rate of possible values for the
parameters by using names that are
compound using this underscore
underscore connector that connects the
name of the pipeline stage and the name
of the
and so if you had another a pipeline
inside a pipeline you could repeat this
underscore underscore separator yeah so
here I do not greet jobs the learning
rate I could add it but it's just too
slow too yes if you don't include it if
you don't include a parameter it will
use the parameter that you set here for
all the iterations and so there we will
try to evaluate a gradient boosting
trees with 0.5 of the features and max
depth of 3 is the mean strategy then
with the median and we'll do all the
combinations of possible values so we
can do that and display the results so
you can see that it's quite slow all
right let me change the resolution so
now we can see that the the
top-performing models they have mean and
median strategies so apparently in that
case the kind of imputation that we do
if we impute by the mean age or the
median age doesn't change much because
anyway the the age of a population is
approximately Gaussian distributed so
it's not a big difference for all the
distribution that might have a much
wider impact yes you have a question
I'll come to you after
for make pipeline or for you need to
import it so the errors that you you
imported it okay so if you get an
importer or here it means that the
version of an icon that you are using is
likely holder older than it has it does
not have the last release of
scikit-learn so you can just come down
update scikit-learn and then we start
ipython and you should be able to get
the latest version
this was the make pipeline helper was
introduced in all point 15 I think all
right so so you can see that this way in
this session we used tenders to do data
a normalization pre-processing and we
can combine it with other pipeline build
a machine learning model that we
actually we haven't actually use pandas
inside the pipeline itself if you want
to go further and do that you can have a
look at this project that makes it
possible to have like pre-processing
stage that you write with custom pandas
code and it will wrap them into
something that psychic on accept in a
pipeline and that makes it possible to
to have like a more maintainable code
over time so it's called a scalar
intended and provides you with
additional helpers like let me show you
an example like this the data frame
mapper object here let me make it
slightly bigger this class is a
transformer it takes as arguments a list
of names and an transformer object and
basically what you will do is that
whenever it has it receives a data frame
as an input if it has a column with the
name that it will transform that column
using this transformer cycle and
transformer or you could write your own
function to transform the column
individually so it means that it makes
it possible to have like transformers
for different types of
columns like if you have a column with a
dead type dead time type like for a time
series you can transform that time that
timestamp into different columns like
for instance the day in the week the day
in the month the month in the year and
this way you could extract different
columns that would encode different
periodic events so if you have got a
recall values you can use the label
binary analyzer if you have numerical
values you can you stand out scalar to
divide by the center now to subscribe
the center and divide by the standard
deviation that normalizes the the
feature and so on and once you have this
wrapped as a as a data frame mapper you
can call fit transform on your data and
it will output an empire raise so it's
very useful to to wrap your own
proposition code in a cyclotron pipeline
in the future we plan to have better
support offenders in scikit-learn but we
will probably never introduce it as a
dependency but cased it should be more
natural to do this kind of operation
directly we stack it now
alright so that's it for this for this
part and now I for the the rest of the
this workshop we could either go further
into analysis of how the model work and
fail and cycle like most logical
concerns like overfitting and
underfitting other otherwise we can have
a look at how to do text mining text
text processing we stack data let's do a
vote who is more interested in doing
text classification and text data for
processing so
let's say 15 and who wants to do the
more theoretical analysis of a fitting
and a fitting slightly less I would say
no so let's let's do tech stuff anyway
in all cases if you go to my github
account agrees all I tell it now because
otherwise I'm not psychic parallel ml
tutorial so or wizalll /param element
tutorial so you have the notebooks and
all the solutions for this session if
you scroll down you will see this which
is a video recording of the same
tutorial that I gave at PyCon and
actually the parts that I cover in the
three hour session that we did at PyCon
exactly the stuff that I didn't have
time to cover today so it's very
complimentary so if you have the two
videos there the one from this session
and this one you will have most of the
of the netbooks covered so it's the
website is parallel ml tutorial it's the
name of the zip file if you google it
you'll find alright so let's switch back
to number of seven the book seven
first a couple of import statements by
now as you should be familiar with them
you can note the the first one from
future import division it's to make this
notebook behave the same and the Python
28 and 3 so if you want it's a good
practice to do that to write code that
works for the two languages so that in
the future you can migrate all your code
to Python 3 if it's in production and
you don't have to rewrite everything
from scratch I would advise you to run
Python 3 from now on but sometimes you
have dependencies that are not yet
perfect Python 3 you can also put that
to have the same behavior for divisions
and and print function alright so text
classification and clustering the
problem is with text documents is that
they are text documents and cycle only
except numerical description of the data
so how to do classification of text
document like spam on that spam or
classifying the topic of a news article
or trying to for instance if you have a
support system helpdesk and you have
incoming requests from your own user
with stack traces of errors of bugs how
to classify automatically the issues to
route them to the correct expert in your
organization to the person with the most
expert in this problem you can do that
by using a representation that we call
the bag of words representation so we
will discuss this feature vector
extraction for text data then we will
see how to train a simple text
classifier on this bag of words
representation and then we see how to
wrap the feature extraction part and the
classification part into a pipeline to
of the two together as a text
classification model and finally we will
discuss a cross-validation and model
selection for this kind of beast so here
is the quick summary of
all of the contents so this is a piece
of Python code that does everything for
text classification
so if first it loads a bunch of data a
set training and testing for four
possible categories for possible topics
so those are data from 20 newsgroups
data set and some some discussion rooms
are about religion or they're about
atheism so it's very related another
room is about computer graphics and
average room is about space and science
so we collect this data we load them as
Python strings and then we apply this
tf-idf vectorizer to extract back up
front representation and convert that
into a numerical representation matrices
then we train multinomial naive bayes
classifier and we then transform the
test data and apply the classifier on
that test data and compared to the true
labels and to see that the score of the
classifier so let's execute that so it
takes some times because passing the
text data I'll take some time and you
can see that on the training set the the
error is 95%
it's a 95 percent accuracy and on a test
set it's slightly lower it's 85%
accuracy but it's already quite good
because just guessing by chance will
give you 25% accuracy because you have
four possible categories so the regular
visit session is to detail all the stage
so if we come back to the previous
diagram you can see that we have this
raw native representation of the data
which are just text there are text files
or Python strings loading in memory the
first stage is to call a vectorizer and
call the method transform of this
collection of text documents it will
extract a memory
representation so Syfy's pass metrics
metrics I will detail what is this past
metrics layer then we call fit on the on
that feature extractor extracted
features and we get the model and for
the new data the new documents we
extract the same representation using
the same vectorizer as we did for for
the training oh and we can call other
classifier on this new representation of
the data so let let's see more detail so
you will have the problem of and the
windows you need to put quotes here so
let me zoom a bit it should work better
on the windows and you might need to get
rid of that so if you'd unzipped the
data from from the zip file that I gave
you at the beginning of the session you
should have the data already downloaded
in this data assets folder if it's not
the case you can always do run fetch
data and this this tiny Python script
will check whether or not you already
have the data and will download it if
you if you don't have it so it will also
unzip the the 20 newsgroups data set so
in in this folder you can see that we
have the original data training
downloaded from the public website and
it has been uncompressed into two
folders training test
this sentiment 140 and and this other
zip file for the next notebook so you
can ignore them for now
so in this folder in the train pot you
can see many subfolders and you have
each for each subfolder you have
files that have numerical names and no
extensions but those are text files that
were extracted from the archive of
public discussion groups on the internet
in the 80s on 90s and so those are the
topics so each folder has a specific
topic and in each folder you have many
files from post on that we are posted on
these forums from the same topic so
let's use this utility from scikit-learn
datasets to load the files by using the
the folder structure as categorical
information so it will treat the the
folder name as the target categories and
it will dig decode the text content
using the latin one representation which
i think is approximately correct for
those guys
in general it's much better to know
exactly the encoding rather than
guessing so it's better to ask the
people produce the data which encoding
they use to save it so you can see that
when we load this data we have this
object all twenty trained so it's all
the twenty newsgroup training data set
and we have this attribute target names
there are the names of the possible
categories they match the folder names
if we if we look at the target values we
have integers encoding so the twelve
integers means that the number twelve in
that list might be electronics for
instance we can have a look at the shape
so it's the target variable here so we
have more than eleven thousand posts in
this collection in this training set
text document and in the test set is
slightly lower it's a bit less than
eight hundred and eight thousand text
documents
have a question okay so if we look at
the data attribute this is actually a
list of Python strings that have been
loaded in memory and so if we look at
the first element you can see STR on
your machine and the Python - you might
see unicode instead yeah because I'm
Python - we have the the STR type means
byte string
whereas in Python 3 the STR type its
unicode string by default and the bytes
are a different type so it's in any case
you need to decode and have a unique
representation that I'm Python to its
role as gr is just the name that has
changed so I can display so this utility
function will take the index of one
document in my collection my data set
and it will display the name of the east
element of the the class of that
document and the the text content of the
document so I can do that for the first
document you can see that it's a
document from the news group on science
electronics and the text contents are
some headers and a discussion about
software simplicity purchasing stuff
from electronics and then on the second
document they have been shuffled so you
can see that it's an attitude to sell
something they are speaking about I
don't know that it's for sale it's for
advertisement so we we know that the
data is is in Latin one representation
we can we can encode it in Latin one so
we by looking at the length of all the
strings in the in that collection we can
compute the
memory size that is required just to
store that and we can see that it's a
hundred megabytes of data just to have
like an idea of the size of the training
set we are dealing with so it's not that
big it fits in memory there's no
particular problem if it in memory we
would have crashed by now so on the test
set it's much smaller because there are
fewer samples so now how to extract the
features from those text documents the
main class that we were going to use is
called tf-idf vectorizer vectorizer is
just a name that we use inside to say
that we you start from some input format
and we extract vectors of features from
from from that data tf-idf is the name
of a normalization scheme that is very
useful for text document so let's create
an instance of that guy and you can see
that it has many many hyper parameters
many ways to tweak the behavior of the
extraction of the of the features so we
will just create we just use the default
values for most of them and yes it's in
a recent version of psychically I mean
DF equals one is the default value it
wasn't the case in previous version and
just I've fixed it to make sure that we
have the same behavior for everybody so
if it's the same you can just get rid of
it so we will use the default behavior
and call the fit transform method on the
training part of the document the text
documents and what we get as a result so
here you can see the person time magic
is just something to time the execution
of that statement it's actually quite
useful and you can see that less less
than a second process megabytes and the
resulting data structure that we get is
a Syfy's past metrics so sparse matrix
is a weird data structure that we
represent logically an umpire
ray it looks like an empire ray it has
two dimensions some 2,000 rows and
34,000 columns the values are floating
point values but there are only much
fewer stud elements in that data
structure because it does install the
zero values so let me show what we
should have gotten
so if we have if we had used an empire
array and we had stopped the zero values
we would have used 79 million entries
and here we just have a couple of
hundred thousand entries so it's much it
doesn't use a lot of memory compared to
storing all the possible zeros that we
would have yet so to understand the back
of fertilization what it does internally
this vectorizer is that for each line
each document it will split the text
into individual world and for each world
it would assign it a column and so in
this matrix I have 34,000 columns so it
means that there are thirty four
thousand individual unique world in the
whole corpus and for each document it
will put a nonzero value for the words
that appear in the document so it's
likely that you have a short document
with a couple of hundred words for
instance so out of the thirty thirty
four thousand columns just a few hundred
we will have nonzero values and those
nonzero values they are basically based
on the frequency the number of times the
world appear in that specific document
so you can see that the number of rows
matches the number of documents that we
have in this sub sample of the data set
actually I'm working on a sub sample
here to which we make it faster so not
the full training set and the number of
features is the number of unique words
that we found in this text collection
the matting width between the column
name and and and the column indices
integer indices is taught in a know
Python dictionary so it's a very big
Python dictionaries because it has as
many columns
possible world so I won't displayed on
the netbook because we crash it but you
can have an idea of the future names so
the all the possible unique names by
looking at this gate feature names that
returns an array of our list of all the
strings that it found and you can see
that at the beginning its dotted in a
lexicographical order so at the
beginning there is a lot of garbage with
a digit at the beginning and if we have
a look at the middle of this collection
we have actual English names and English
world and they have been normalized they
have been put in lowercase and there is
no punctuation anymore so what we can do
on this new site pass pass matrix is try
to do to find that a compression a
two-dimensional projection of that data
into a two-dimensional space to
visualize it and to do so we use an
algorithm which is called singular value
decomposition and we'll just look at the
first two component of that
decomposition and project each
individual document of our corpus into
that space so we turn we start from the
big high dimensional space with 34
dimensions
thousand dimensions and we compress the
data into a two dimensional space and we
try to preserve the layout from the
original space into that space
of course it's we are losing a lot of
information so we are compressing so
it's not a lossless transformation and
but you can see that even if we compress
a lot by doing so there they are still
like a layout where document about
atheism and document about religion are
in light blue and dark blue respectively
you can see that there are in the lower
hand of this space and size science
space and computer graphics inter wind
more in the upper side and it
makes sense because religion and made
them they share a lot of common worlds
to speak to talk about God whether you
believe it in it or not you are using
the same word and electronics and
science there are common words the the
topics are more similar and this is
actually reflected in in this
representation but you can see that
trying to draw a line in that space it
would make it possible to separate
document that speaks from religion from
document that speaks about space by
drawing a line here quite quite quite
well actually
but there are still a lot of overlap in
the middle so we wouldn't a good
prediction accuracy by working in that
space but it's good to use SVD
projection just to have a look at the
data and to get some intuition about how
the different classes are related so
let's try a first model so we extract
the target variable and so we call it Y
and here we just have four classes it's
a subset of the data we check that the
the the the cyberspace metrics that we
extracted as the same dimension as the
target variable for the first or the
number of samples and we can train a
multinomial naive Bayes model so this
model is very nice because it's simple
it has very few parameters and basically
the only Alpha is really important and
it's some kind of regularizer
regularization parameter that makes it
possible to adjust a trade-off between
memorization and simplicity of the model
expressiveness and simplicity so we
train the model as usual by calling fit
so it's exactly the same API as random
forests or logistic regression and this
model is really fast to Train
and we can call we can transform the
data set using the same vectorizer the
tf-idf vectorizer with the default
parameter and we check that the ass
metrics that we get by vectorizing the
test set as the the right number of
documents as we have for the labels and
we can compute the score of the
prediction on the test set and we see
that on this small data set we have
close to 90% accuracy actually but this
is just a random a treinta split so it
might depend so we would need to do a
cross-validation to make sure that this
is an accurate estimation we can also
compute the score of the model on the
training set and you can see that the
the the score the accuracy on the
training set is much better and this is
the case because the the model was able
to memorize all the training set almost
by heart so in practice we never
evaluate the score of a model on the
training set but it's good sometimes to
to see if this score is not 100% admits
that the model is to constrain and it
might suffer from under fitting if there
is a big gap between the two training
score and test score we say that there
is overfitting so this model is actually
overfitting a bit because there is 10%
difference between Newton so we can have
a deeper look and howdy tf-idf
vectorizer works internally so here is
the list of parameters if you want to
read the documentation for the meaning
of the individual parameters you can
access the doc string of the object in
ipython you can do that also by doing so
using the question mark and you will see
the documentation that opens up in a new
window and you have the the meaning of
all the parameters and there are default
values
okay I will just put back
okay so you see important parameters
like the kind of inputs that we feed
either we can pass a list of finance and
it will open the files or we can just
buy a pass already file object open file
object or we can pass the string content
that we fetch from once a JSON API that
is not a file on the file system point
and there are many many parameters we
will explore some of them oops sorry
so the first component of the vector
riser that we look at is what we call
the analyzer I don't know if you if you
have a prior experience with text
indexing for full text search in text
engines text search engine if especially
if you use the leucine or solar or
elasticsearch libraries or tools they
have this notion of analyzer an analyzer
something that takes your original
document is always stronger presentation
and will tokenize it and simplify it and
extract a normalized way to deal with
the data just so this is basically the
kind of transformation that it's doing
on the data so I can call the analyzer
on a string on an individual document
and see what it's doing to it so here
you can see I left I keep an eye on this
is a cool Python lip that first it gets
rid of the first the eye because it's a
single letter word so well that are two
shots they can be too noisy so we will
get rid of them then you can see that
Python is now low lowercase and the
exclamation mark at the end has been
stripped and there's a columnar so so
the punctuation is removed it's
transforming to lowercase and accurate
as splitted circuit and learn as two
different in
visual world so this is the default
behavior we can customize this and to
customize this you can pass a custom
preprocessor function which is actually
taking the original text and keeps it
this way to disable the lower casing
because the default preprocessor does
lower casing and then the token pattern
parameter is a regular expression that
says which characters should be
considered as world characters or not to
extract the world and here you see you
see that I'm using backslash W and the
if--and sign which means that now I will
not no longer split on the iPhone and
now I see I love scikit-learn this is a
cool library if I execute it psychically
on is a unique word and I haven't get
rid of the Y here because I do not
exclude a single letter tokens by
default at this regular expression only
considers two letters world all right
so so for instance in lung in languages
that don't have the work separators like
Chinese where the I don't know D what's
the name of the letters yeah so the so
yeah basically the Chinese words they
can they can be stuck together and you
need actual actually I never machine
learning algorithm to know where to
split so there is a specific processing
that is required for China or Chinese
for segmentation of the sentences for
Latin based languages
you can just click on the white spaces
and on the punctuation yes so you can
actually put indeed India token pattern
you can actually customize it even
further let me see if I do maybe not but
basically you can you can stick any
Python call a ball for the preprocessor
the same for the tokenizer and
additional steps also as arbitrary
Python functions so it will be possible
to call a NLT K stammer as an stage in
this and to pass it as a stage for
processing the data before tokenizing or
after tokenizing
and so nlgj is a Python library for text
processing basically now the natural
language processing and in particular it
has models for stemming so stemming
means extracting the root of a word for
instance when you have a verb you can
have different endings and beginnings
depending on the tense and so you might
want to normalize that to get rid of
that just to get the root of the verb to
get the meaning of the verb and ignore
how it's used in the context so it
depends on the cases sometimes it's
helpful to do stemming most of the time
it's not for classification so and it's
costly to do also so sometimes when you
want to be quick at processing you don't
want to do stemming just because it's
too costly yes so an ltk does does also
tokenization and stuff like that and it
also does text classification it's also
able to Train machine learning models
but the internal representation that
it's using its Python dictionaries
instead of cyberspace matrices which
means that it's going to use a lot more
memory and it's actually slower process
dealing with Sipan matrices like just
numerical representation is much more
compressed and faster to you to train
models and actually an LT cake and use
psychic long models to train the models
so it's possible to just like a plan
inside of a mck and it would also be
possible to plug an LT case timing or
pre-processing inside the tf-idf
vectorizer here so you can mix and match
the two I can show you the documentation
on the website the scikit-learn
scikit-learn so if you go to the
documentation and you go to use our
guide and then you look for feature
extraction the next one
this is where oh yeah I don't know why
it massage didn't work so if you look
for feature extraction on the
scikit-learn website you have a stuff to
deal with a Python dictionaries you have
stuff about feature arching which is
very interesting also but we won't have
time to cover today and there is a
section a big section about text feature
extraction that goes into more details
that wanna present me today and there is
actually an example here to use an LTI
thing and ok so here it's it shows how
to import the word tokenized the
tokenizer
and the limit the limit riser based on
walnut from an LT k and to plug that the
custom Python class into the con
vectorizer of psychic Nam Kham
vectorizer is an alternative to tf-idf
vectorizer
so you can actually combine the two
together if you need you and the best
way is just to try and you cross
validate and see if it improves the
accuracy of the model and if not just
get rid of it because it's it's too
expensive it depends on under data
honestly there is no generic rule so
there is an exercise but I would like to
go on because it's should we the time
should we stop by the way okay so yeah
so I prefer to go yes question I think
yeah actually this is the exercise so
you can extract engrams
and I'm just educating it okay I just I
just demonstrated before so I'm
scrolling back up to the previous
example so I'm scrolling up until you
will reach the first training model that
we've seen so here it is I'm pretty sure
it's gonna is going to be covered later
so the question was how to extract
engrams the the answer is just to pass
engrams range equals one and two but I
think we were gonna cover it later no
not here but later okay so before after
we will see how to cross validate the
parameters from the feature I backed
vectorizer and the classifier bed before
going to that point I would just have a
look at the the multinomial my night
bias it's off the model itself you can
see that it has a parameter which is
called alpha it's called a smoothing
parameter and if you put it to zero it
has not moving when you smooth a lot it
means that it will try to find simple
models and we don't when you don't do
any smoothing it will mean that it will
have more freedom to adapt to the
training set but if you give it too much
freedom it can over fit the frequencies
of the data of the training set and fail
to generalize correctly so this
smoothing parameter is important to a
grid search and we can see in the
solution for this exercise that we can
use the grid search object as we did
previously and by passing different
values of alpha and and and running
cross-validation to find the best value
of alpha so here this notation log space
numpy log space
is a quick way to generate an empire
array that goes from 10 to the power
minus 3 to 10 to the power 3 and extract
seven-step between those two on the
logarithmic space so I'm trying
different values of alpha between that
and that and you can see the result that
there is a trade off if alpha is too
strong the score is really bad if it's
too small actually the smallest value is
the best in that case solid let's
explore further 1 for instance it's
still the best so apparently smoothing
is not helping in that case all right
this is weird this is really it's a log
space it's really well
oh no actually the best core is is
actually here yeah it's not other was
one is it wasn't ordered so if I go back
to my knife three and three
so the best core is this guy so the
second one so as your open G one okay so
if you if you don't smooth at all the
model starts to overfit okay that makes
sense so you can use the learning curves
the validation care of from scikit-learn
to do a plot of this actual dependency
on of alpha so here this is doing
exactly the same as previously for
different values of alpha it's computing
the cross-validation score and as the
red line at the green line and it's also
measuring the training score as the blue
line so you can see that if you don't do
any smoothing like very low alpha the
training score is 100% accuracy so it
means that the model is about you
perfectly memorize the data set it will
make no error on predicting a document
it as seen in the past but on new
documents it's only like 94% accurate
and if you do some smoothing you can see
that the training score is decreasing a
bit but the test score which is the only
score that we are really interested in
is increasing a bit so it's actually
good to constrain the model a bit so
that it generalize better to new data
but if you constrain it too much you can
see that this is decreasing the training
scores is decreasing a lot stronger and
the test score also because this is a
kind of upper bound on the test score so
when you have a large gap between the
two we say that the model is overfitting
and when the test score the training
score is decreasing too much we say that
the model is under fitting so the two
are problems so we need to find a
trade-off between overfitting and
underfitting
overfitting is also sometimes called a
variance problem in statistical parlance
and underfitting is it's called a bias
problem the model is too constrained so
here we can see that there is a
trade-off and it's around that point
so alpha equals point zero zero one so
now that we have seen that alpha needs
to be cross validated we can also study
the impact of the hyper parameters of of
the vectorizer on the model and we will
instead of using a naive Bayes
classifier we will use a passive
aggressive classifier which is another
kind of inner model that works well on
text data so just to show that you can
switch the classifiers it shouldn't
impact too much the results so here we
actually building a pipeline with min DF
equals 1 max DF equals point 8 and we we
enable IDF vector
normalization of the data the weighting
so max DF means that we were gonna drop
all the tokens that occur more than 80%
of the time in the data set so those are
typically what we call stop words for
instance in English the a what those
words are not very informative because
they are too frequent and you can see
them in more than 80% of the text that
you would see so you can get rid of
those and don't extract features for
those words and min DF is the opposite
is that we can strip words that occur in
less than 3 times for instance in the
whole dataset because if you see it only
3 times you cannot really know whether
it's correlated to a specific topic or
not so it's a way to get rid of features
that are too noisy so it's it's good to
trim stuff that is too rare and stuff
that is too frequent when you are doing
tekzilla and this actually is very
important to do in in practice so I will
put it back here and so my pipeline here
now I can cross validate it directly as
a whole
and here you can see that I am reaching
an average 86% accuracy and this is the
standard error of the mean you could use
the standard deviation if you prefer
this way so now we can evaluate the
impact for instance of this guy if I put
India figure 3 here does it for does it
improve it actually decreased a bit so
it's an important not to drop too many
words if I dropped 2 this is better so
instead of tweaking the parameters
manually like I did just there I'm doing
cross-validation manually we can use a
grid search TV again and this time we
can adjust the parameters of min D F max
D F whether or not we use IDF
normalization and whether or not we use
n grams
so here the question the first question
on how to use and grams is there is an
Engram range parameter for the
vectorizer and this parameter takes a
tuple a pair of two integers and this is
the the minimum size of the engrams and
the maximum size of the engrams so an
Engram is for instance if you have N
equals 2 it means that when you have a
sentence like the cat sat on the mat you
will treat a pairs of consecutive words
as token so the cat you will consider
this as one feature cat and sat cats at
together will be considered as one
another fish another feature and so on
and each of those possible pairs that
you observe in the training set you will
map them to a specific dimension in your
space and so by doing this 1 &amp;amp; 2 means
that individual words we count as
features and pairs of two consecutive
words we also contacts other features
independently and so by doing
one one here I mean that I will just
consider individual world and this will
consider individual world and pairs of
world unique crimes and diagrams so
let's run this grid search here I used
the verbose mode to you to monitor the
progress the actual output of this is
not very useful but at least we we see
that it's doing something
so a grid search can be quite slow
especially when you're doing with a lot
of pre-processing because this is quite
expensive
I think only on this model training the
model is actually very fast but doing
the pre-processing is the the slowest
stuff so you can see that the best model
that we found is a a 96% accuracy so
it's very close to the default values
that we found so it said that using IDF
is actually useful and they're using bar
grams it's not necessarily useful and
using stop words it's not useful on this
data set it's so it depends in general
you can what you can do is also do yes
grid course and you could see all the
possible combinations I mean you see
that they are very close to one another
so there is not a big difference so they
are not very impacting on this data set
finally it's very interesting when you
train a text classifier to introspect
the model to understand how it makes its
decision so to do that with the linear
model that we've used like
passive-aggressive classifier only in
our models in scikit-learn they have
this clef attribute underscore F
attribute when you feed them so first we
fit the whole pipeline and we can access
the the different stages of the pipeline
the Beck object is the vectorizer and
CLF is the classifier from the web web
Beck object we can access the gate
feature names arrays where we have the
mapping between the feature order and
the names of the tokens and from the
classifier we have the weights of the
tokens of the features so by using
feature weights and feature weight we
can see which features are the most
discriminative world so this is just a
Python function to order by weight and
display the names and so for each class
it does that so far I am you can see
that the world
itself is very important at least is
very important keys I don't know there
is a kiss on the mailing list who is
very into face them that might be him
but there are also stuff like the Bible
and get silly silac what anger Hagar
yeah and the negative ones are stuff
that are completely not related to I say
them but more to the other topics of the
training set in particularly you have
graphics space Christian parent Lee the
idea is I'm don't speak about Christian
so much for computer graphics graphics
image animation is very important if the
file format videos and stuff that are
not related like Chris Alaska Augustana
me but so here you have to understand
that each classifier classifies graphics
for instant versus I am religion and
space so there is a bias because the the
neutral class is not that neutral in
that case so it would be interesting to
add additional data from from the
internet like you hold the whole
Wikipedia as junk and you classify your
text of interest versus that junk to
have like some more or less opinionated
negative weight because otherwise they
are they really reflect the the other
classes if you have a friend whose name
is Macbeth that might cause problems
because you know because there are their
frequencies in not representative of the
neutral ground you can just collect
random stuff from the internet or or
Wikipedia and yeah we keep it guys nice
it's it's not that big about it it
covers a lot of different stuff okay so
far region Christian the Beast
Hudson good and
Saturn is not really religious
apparently so
this way you can you can understand what
your model is how your model is working
because basically when you make a
classification it will just pick up the
world and multiply the number of times
you see the world by those numbers and
and see if it's below or above some
threshold that you can access from so as
we did previously we can also compute
metrics so precision and recall in this
in this case this is more interesting
because we have more than two classes so
here you can see that it's highly
precise for computer graphics and space
and less precise for for religion and
NASA because those two classes are very
close to one another and we can actually
check that by plotting the confusion
matrix and so the ordering is the same
so the first and the last one are I see
them in religion and you can see that
those element is those of the diagonal
element here our confusion between
religion and I see them because they are
overlap a lot whereas the others of
diagonal elements they are not as strong
all right so yes this is the target
names to be able to understand the
confusion matrix all right so if you
have questions
yeah so the question is what if the data
doesn't fit a memory in that case you go
to the next one large-scale text
analytic and then it's another one our
so basically this covers what we call
the hashing trick which makes it
possible instead of using a vitam
dictionary to map the tokens to the
dimension of your of your matrix you
just use a hash function and you will
have collisions but on average it's not
a big problem and this is a really nice
because this ash function it doesn't use
any memory whereas the mapping the
explicit mapping from the text
representation to the column numbers is
very big if you have a big training set
and furthermore this is stateless which
means that you can transform your data
into a numerical representation in
parallel and then aggregate it also you
can scale to a large cluster and compute
hashed features in power on a big
cluster and then aggregate all the data
and Felina models at the end the second
part of this it shows how to train on
line models linear models that that can
fetch a bit of data increment their
internal states the statistic estimates
of the frequencies of the weights and so
on then through every that did I fetch
the next one and do that over and over
again so that you have a fixed memory
usage when you do that and finally it
also show how to Train independent
linear model out of core in power on a
cluster and then to merge the
coefficient from one time from time to
time and at some point you would have a
unique classifier that represents what
you did distributed on a cluster
yes when you are using the question is
when you are using a hashing can you
understand what the the model did the
problem is that the hashing is a one-way
hash function and so you cannot invert
the meaning so you would have to trace
the the mapping and store it somewhere
to be able to interpret the most likely
interpretation of each column but you
can have collisions one column can can
be mapped from different worlds
but you would have to store that in a
database or a file or something we don't
do it currently but it's not very
complicated to hack the code which you
wake up that somewhere I know that
whopper rabbit does that I think yeah so
whopper rabbit is a microsoft c++
project forfeiting this kind of linear
models on a extremely large scale and it
does the ashing trick and it does the
trace of feature mapping and it's very
fast so if you are working with text
data or streaming data it's very nice
but there is no good Python API because
it works directly with the file system
or with the streams because it's very
low-level to be fast but this is an
inconvenient because you cannot use
numpy and sci-fi it's protocol yeah it's
also in CMS and I don't know if germline
court will see that video I've tried to
reread the C++ code and there is very
few commenting it is very hard to
understand maybe it has improved because
last time one more question and then
after that whoever is interested just
huddle up here and we'll continue the
discussion because I know some people
want to go one announcement I'd like to
make is first of all obviously thank
Olivier and Brian and everybody who
spoke yesterday if you liked what you
saw yesterday
and today please go to http kudos and
give us some feedback so we can get
Joseph and Scott to pay for events like
this it really helps we have to
basically show them proof that people
wanted if you didn't like it don't go
there don't don't don't but we will send
out a short survey Survey Monkey give us
feedback because we'd love to do this
again and we'd like to improve so give
us all the feedback possible so with
that let's take one more question and
after that let's huddle up here for
people who want to go a question is so
what is the maximum size the size of the
data you can work with psyche long so
have you seen like most emotive model
that we've been that we've been dealing
with today we used it we load the data
as an umpire array or spend as data
frame or cyberspace metrics and so we
expect the whole training set to fit in
memory and it is there there are a
couple of exceptions some of the models
support partial fit so on line modeling
online learning with incremental data
but then you have to do a lot of
boilerplate code to to connect your
database or your hard drive or your
network to stream the data but yeah they
did a nice sweet spot for psychic
knowledge to work with data that fits in
memory so I would say depending on the
algorithm from a couple of megabytes to
a couple of gigabytes or tens of
gigabytes is the typical sweet spot if
you go above okay you can have like big
machines with a lot of RAM nowadays so
and and and use that and it's typically
the switch but if you want to go to
scale beyond you have to move either to
another framework or to combine
scikit-learn with another from our plug
spark or something but it's not that
easy to do yes
think just another comment about that in
terms of Python is a general tool for
large memory situations last night
Olivier and I were playing on a server
that had 24 cores and half terabyte of
RAM and without any problem we could
create numpy arrays with hundreds of
gigabytes of memory and it worked
perfectly fine
the Python garbage collection detects
when those go out of scope and they get
cleaned up basically instantly and on
this particular machine the Blas
implementation was open blas that had
multi-threaded support so we could you
know diagonalize a massive matrix and
instantly all 24 cores were being used
and so I think the real answer is what
machine do you have right I mean it's
also very often the case that the big
data that you have is the raw original
format and that you you would do a lot
of processing to do aggregate feature
extraction for instance if you have a
transaction log of individual user
interaction with a web page typically
you want to extract features that
summarize a session of 30 minutes of
user interaction into a feature vector
and so this we can do using Hadoop for
instance or SPARC and then from this
session eyes data that it's much much
smaller you can load it on a big big
machine with a lot of memory and do
interactive data modeling there so there
is a trade-off between being able to
quickly iterate when you do the modeling
quickly visualize the data trainer model
try something throw away an iPod edges
and the ability to use more data to
build bigger and better model so usually
if the data is too big your your speed
of iteration that's pretty building the
predictive model will be lower and you
won't be able to good to create big
models a good models because you would
have not explored all the possible
strategies to build good models so it's
it's good to be able to quickly iterate
and so to do so you can also just sub
sample the data and plot what we do the
learning curve so that for instance you
use 1,000 samples 10,000 sample hundreds
hundreds other samples and you plot the
cross-validation accuracy of your best
model for each of this data size and you
see our T increases and it might be very
well the case that with the feature that
you have there is no no way to go beyond
some some plateau and that rather than
adding more samples you should better
build better features by aggregating and
reaching your data with external data
set like geographical data set from
OpenStreetMap or stuff like that try to
enrich rather and then add samples so
it's good to do this kind of analysis
before spending too much time on
computation for nothing all right other
question yes thank you very much again
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>