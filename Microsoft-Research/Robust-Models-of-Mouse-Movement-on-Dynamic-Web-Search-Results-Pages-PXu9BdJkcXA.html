<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust Models of Mouse Movement on Dynamic Web Search Results Pages | Coder Coacher - Coaching Coders</title><meta content="Robust Models of Mouse Movement on Dynamic Web Search Results Pages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Robust Models of Mouse Movement on Dynamic Web Search Results Pages</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PXu9BdJkcXA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so this is joint work with folks in
Redmond although to be to be honest a
lot of this work also happened at Yahoo
before I joined Microsoft but that
allowed me to reproduce results on to
production data sets which is kind of
convenient ok so the premise of today's
talk is going to be all about
interaction logging and interaction
modeling now what do I mean by
interaction log in here by interaction
logging I mean detecting exactly how
users interact with or engage with some
interface now I'm going to be relatively
abstract here because I want the I want
to sort of suggests that these
techniques are general so when we're
talking about user engagement here I'm
talking about not just clicks but also
things like dwell time clicking on the
back button you know you could think
about touch interfaces or in this talk
will be discussing mouse movement and
when I'm talking about interfaces I mean
anything the user might interact with so
this would include things like the
search results page it would include
things like portal pages or mail pages
or office documents now interaction
modeling has to do with being able to
predict user engagement on some
arrangement of items on a page or some
interface that perhaps i have not never
seen before ok so it's exactly the same
definitions of engagement and of
inferred interfaces i listed before so
i'm interested in being able to predict
how users going to are getting click i
want i'm interested in understanding how
they're moving the mouse or touching the
interface without actually ever having
presented this interface to real users
so what's the value here so obviously
for interaction logging i guess most of
us in the room should should should
understand that logging is very
important for evaluation of system
so the Richer are logging the more
accurate are logging the better we can
evaluate an interface that we have
actually launched into production
furthermore this beyond this evaluation
we can we can in an automatic system
roll the feedback we roll this in as a
feedback signal or as some sort of
feature that can then lead to better
system performance in an automated way
so you could imagine if our metric is
computable automatically it being fed
back into some sort of learning system
which would then adapt as it observes
more and more feedback now interaction
modeling what is the value here the
value with interaction modeling has to
do with what i'm calling the pre
presentation phase so what what what
what value does the model have before
I've ever actually presented this
interface to any users so for one it
gives me an automated system performance
prediction capabilities okay so if I
know that there's some relationship
between the users interactions and
system performance then being able to
predict users interaction given a
certain presentation or interface I can
I can then make predictions about how
well that that decisions can actually
perform relatedly you can use this
performance prediction signal as a
feature for your ranking algorithm or
your arrangement algorithm so if i have
an automated page layout algorithm the
predicted performance of that layout
maybe should influence my decision
whether or not to actually execute on
that interface and finally a little bit
different this is a little bit different
style of use of this data we could use
these predictive models as a design tool
so imagine that I could given an
arbitrary interface designed by some
designer tell you where the users are
going to look how they're going to
interact with that page that's going to
be very valuable for a designer because
a lot of times they don't observe that
sort of information until they've run a
laboratory study or launched a thing in
production so giving the design or the
ability to
I anticipate how users are going to
react to pages or interfaces allows them
to iterate much more quickly with the
design part so like I said before
today's presentation will be focusing on
the mouse as the type of interaction
we're interested in and search results
as the specific interface we're
interested in some of these techniques
should be generalizable to the other
interactions and certainly to a lot of
these web-based interfaces okay so since
I'm going to be talking about Mouse let
me try to motivate mouse tracking a
little bit so here's here a couple of
heat maps of this is i fixate or
distances I visual tension so gaze on a
search results page on the left we have
a search results page which does not
include the task pane or entity pain and
on the right we have a search results
page which includes the task pane or
entity pain and but the heat map is
telling it with the heat map is giving
us as some idea of where users are
looking on the page and it's telling us
all so that users tend to be attracted
to the task pane winston actually
presented on the page so this is nice a
designer would be very interested in
this sort of real data if they were
designing the search results page to
include the task pane now what does that
have to do with mouse well there are
also a lot of results this is a recent
result but other results dating dating
back I guess early 2000s that have found
a very strong correlation between I
position and mouse position on a page so
those of us who know about JavaScript
know that Mouse is pretty easy to track
down on an HTML page if we encode the
right javascript so what this allows us
to do is get some surrogate for visual
attention on the page given a really
crude sensor so the mouse so if we can
capture them
we can get some idea of where the users
are looking on the page the figure here
is presenting the same task pane and
without the task pane interfaces with
the eye tracking with the eye tracking
trajectories as well as the mouse
fixation so the green i believe is eye
tracking and the blue is the mouse
fixations and what they find is that
yeah they're not perfectly correlated
but there is in general a tendency for
the mouse and I to be attracted to the
same parts of the page this sort of
result has has been studied also within
Microsoft so Ryan Jeff and jörg have
looked at the correlation between I and
Madison search results pages and have
found that you can get within a within
with it within about two hundred pixels
of the true position of the I given just
Mouse data they've also found that this
actual correlation is non stationary
with respect to the page render time so
as with within the first second I
believe there's there's there's less
correlation but as the users struct
interact with more and more the page or
stronger correlation between I and mouse
ok so today's talk we are going to be
discussing search results pages we're
not going to be discussing the exact
relationship between the XY position of
the eye and the XY position of the mouse
we're more interested in the general
regions that the users are looking at ok
so here's our SERP what we're going to
do is chop that SERP up into different
areas of interest ok so here I've got
all the algorithmic results as separate
areas of interest I have the image
answer as a separate area of interest
the advertisements the navigational bar
and the search box and the left rail now
the type of data I'm going to be looking
at is like I said before this mouse
tracking data so I'm going to be
recording we're on the page the mouse
has been visiting throughout the course
of a user session so user issues a query
I start I
chopped up the page into the areas of
interest and I start recording where the
mouse is so how is this implemented like
I said before the instrumentation is
based on JavaScript so the JavaScript
can actually record the geometry of the
different areas of interest it can also
capture the the mouse the mouse
trajectories and in practice how that's
beaconed up is you're sending up the
header information with all the Aoi and
for geometry information which is a
which is stationary more or less with
respect to the session and then every
time you observe mouse movements you
fill this buffer that you flush out as
the buffer gets full all right so for
the purposes of this presentation I'll
be referring to a page arrangement as a
configuration of these modules on a page
ok so these modules are items on the
page are things like algorithmic results
image answers advertisements etc so one
arrangement might be 10 blue links
without any advertisements a second
arrangement might be 10 blue links plus
2 advertisements on the right and so the
set of our arrangements are the set of
all legal slot ians of items on the page
so this this is a just a plot of the
distribution for over page arrangements
for to search engines the two search
engines you can probably guess but the
more important thing to notice here is
that relatively relatively long tail the
relatively long tail of arrangements on
this page or on these search engines
which is to say that a lot of queries
observe these weird configurations that
we haven't seen before it just kind of
makes sense right because in when a
query is issued to a search engine you
might have a very different distribution
of intense than a query you've seen
before and because a lot of the page
layout is automated these days we're
going to see novel configurations of
advertisements and answers on a page so
you're going to begin to observe
all these all these really strange
configurations that we haven't seen
before that said we we have a lot we
have a lot of things at the head of the
of the arrangement distribution which
are things like 10 blue links or 10 blue
links plus advertisements okay so let me
let me move on to the modeling part of
this talk so this is this is sort of a
graphical representation of one of the
ways we might think about how users
interact with a social research results
page so here I've labeled several
different modules as m0 through m6 okay
for the purposes of this graphic I'm
going to say there are five search
results labeled m1 through m5 m0 might
be an advertisment m5 might be search
assist the unlabeled note at the
beginning is just a start state and the
way that people often think about
modeling users interacting with the
search results page is that they started
the first at the first algorithmic
result so here m1 and then linearly scan
down the page so m1 through m5 the
premise of this presentation is that in
practice that's probably not so
realistic what's more realistic it's
something like this that the user is not
starting at m1 they could start at any
of the arbitrary elements on the page
they might start on the advertisement or
on position for and then they could make
it they can jump around arbitrarily
between the different modules so you
might start it you might you might start
at a algo result for then your visual
attention might jump up to image which
is slotted at 0 and then jump around ok
so that's the premise of the model we're
going to be proposing let's see if the
data actually gives us some intuition
about where we're going so like I said
before the classic model assumes a
linear the linear traversal of the rank
list so this is a this is a vision with
visualization I'm going to be using for
the next few slides and so I think it's
worth explaining a little bit so this is
a square matrix where
each each index of the matrix represents
one of the modules on the page so here i
have n which is a navigational bar at
the top of the page which allows you to
select a vertical i had then have the 10
algo results and then i believe as
search assist our is related queries
ages historic queries and then
advertisements is a so the way to read
this is that for forgiven rows so let's
say from position 1 the the entry is
proportionately white according to the
probability of going to that column so
in this case the probability for the
classic model because it's linear the
probability of going from 1 to 2 is
biggest and it's 1 &amp;amp; 0 pretty well so
everything else is black and so forth
down the done the rank list there's no
yes searches it so this is I believe
from yahoo data so i think it was
something like query formulations that
sort of thing okay so yeah there were
there were at least in this in this data
said there were three ways that you
could suggest queries one is recommended
queries one's search assist and one is
this these historic queries okay and for
advertisements oh so this would be I
believe in this example it was right
rail so it's advertisements on the right
yeah it will be because it's look at the
next slide so here's here's reality so
this is the map basis this is the Trent
the actual probability is based on the
mouse tracking data and so what do we
notice here we notice that the
transitions are going not just linearly
down the page they're jumping over L
they're jumping over entries on the page
they're going they're going back up the
page they're going over to the right
rail and transitioning down the right
rail they're going up to the nav bar etc
so the actual reality about what's going
on with the users is a little bit more
complicated than than is suggested in
the linear but by the linear model now
it
so two zones the mills right so it has
to do with the fidelity that's of this
sampling so it's almost like we're cat
we're capturing something like fixations
just because the masses would mean so
fast okay all right so right in general
that in general it is linearly down the
page but you do have some complex
interactions going on here so so that
means that the linear model perhaps
should be tweaked a little bit but it
gets more interesting when we start to
look at pages that include instant
answers okay so here on the left this is
only for this first six positions but I
have on the left I have the six blue
links okay and on the right I have one
weather result at position 1 followed by
followed by five blue blue links okay so
it's a little bit hard to see on this
plot so what I'm going to do is I'm
going to essentially divided
element-wise the matrices here okay and
I'm going to next slides can present two
matrices one where the probability of
transitioning is higher on the weather
presentation and one where it's higher
on the non weather presentation so
here's here's the result of that
division so on the left hand side I have
the cases where whether it has higher
probability of transition than the non
weather results so one thing we notice
here is that there's a higher
probability of transitioning up to page
towards the first position and over here
where we have the presentation where the
over the probabilities are higher in the
non whether versus the weather case we
notice that we're actually getting all
this probability mass from the
transitions down the page so the mouse
at least tend to be attracted to this
position one this is kind of interesting
because weather at least at the time of
these experiments was something that was
not clickable so this is this is this is
this gives us some some feedback signal
for the actual weather answer
now if I present exactly the same thing
for the image answer we get we get
similar results so here I have image at
precision for and this on the left is
the case where the probability of
transition for the image arrangement is
larger than for the non-image
arrangement and we what we notice here
is that the transition to image from
almost all positions are actually from
all positions is higher than in the
non-image case so the the mouse tends to
be attracted to the image and over here
where we were trying to find out where
that probability is coming from we
notice that it's it's being removed from
the positions below image actually going
above image so the things that things
are no longer going from positions 6 up
to position one they're stopping at
position 4 ok so this the image vertical
tends to act as a sink in this case ok
so that's that that's sort of a
high-level description of of the data I
don't now I want to talk about some
experiments that we're running in order
to try to model what's going on so the
problem definition we're going to be
looking at is is sequential modeling so
we assume that we observe in the mouse
tracking data a sequence of modules that
the user has moused over ok so it might
be in a linear scam case you know algo 1
i'll go to i'll go 3 etc or we might
observe a user starting at image at four
and then going up to Al Gore three and
then jumping down to algo at five so
this sequence of modules is going to be
the data that I'm interesting interested
in predicting
now the actual way I'm going to be
studying this data is to try to estimate
a that matrix we saw earlier so that
black and white matrix I want to try to
estimate the size of those white squares
I want to estimate the transition
probabilities between all pairs of
modules on the page okay so before I
discuss the actual model and how we do
that and all the features in the course
of writing this paper I came across some
some really interested interesting
related work having to do with shopping
okay so that's some scientists in the
60s studying grocery stores which is
exactly what Farley and ring did in 1966
so in their case just to relate it to
what we're looking at the set of
arrangements they're looking at is
arrangement of aisles in a grocery store
okay so what are the different
configurations of aisles i can look at
in a grocery store the what they call a
consumer is a person interested or not
in buying products so it's somebody
who's navigating this grocery store and
the paths in this case are shopping pads
so how did the person navigate the
grocery store so this is what their data
look like this is this is data from 2005
when they used RFID tags but they have a
grocery store as your layout and then
the paths were the paths that the
shoppers took through the grocery store
until I think they finally exited so it
starts and then goes all around and
comes back in practice what they do is
they don't looked at the actual XY
position of the user or the consumer
they're looking at the regions in the
the relevant regions in the grocery
store so each of these regions is a
collection of related products so this
is similar to our areas of interest in
this case they're looking at things like
cereal or produce etc and then they're
interested in also modeling or measuring
the transitions transitions between all
these regions so what is the flow of the
traffic in a grocery store given a
certain arrangement of aisles and things
in those aisles Farley and ring in 66
probably something that looks exactly
what it was exactly like what would I
would what I was proposing earlier which
is a transition matrix between all areas
in the grocery store okay so it so it's
always nice to find something like this
in a paper from 1966 because it means
maybe you're on the right track and yeah
so they had said they had certain
constraints on their model but more or
less it's the same thing and I'll be
returning to it later on too okay so I
have this this transition matrix how do
I actually estimate those little white
squares well if I have a certain
arrangement like 10 blue links and no
advertisements I've certainly observed
that in the data a lot so what i can do
is i can get all the data which here is
represented by this matrix D and count
the number of times that I've observed a
transition from algo want I'll go to
I'll go one tail go three et cetera and
just compute the maximum likelihood
estimate of the transition between all
pairs of modules okay this is pretty
straightforward the advantages here are
that it's easy to estimate given a lot
of data you're going to have a pretty
reliable estimate the disadvantage is
here is that well it requires a lot of
data to get those reliable estimates and
secondly more importantly it's unable to
generalize to novel arrangements right
so if I've if I've only observed 10 blue
links with no advertisements that tells
me nothing about the transition
probabilities for 10 blue links with for
advertisements because the
dimensionalities of the matrices are
completely different and it just doesn't
make any sense and introduce the of the
Farley ring model so here are Farley and
ring a little bit older than when they
proposed the model but they said ok so i
have this this p IJ which is the
transition probability from different
parts of the grocery store what i'm
going to do is
not going to I'm not just going to count
the number of times I've observed that
transition I'm going to feature eyes the
the start and the end of the transition
so I'm going to feature eyes the produce
and dairy sections and say well given
the arrangement of these two things and
the identity of these two things what is
a probability of transition so I'm
trying to predict the entries of this
matrix P given the properties of this
starting and ending states of the
transition okay so they looked at these
three features here which were inspired
some by physics and we're going to look
at a few more so again the intuition
here is we're going to feature eyes the
start and end states and then make
predictions based on those features here
are the features that we're going to
look at in the mouse tracking example so
there are what I'm going to call module
level features which are just things
like the geometric properties of the
module so how what is height with area
the Dom class so is is this something
labeled as an algorithmic result in the
Dom tree the ID which also contains some
information about what's inside the
object notice here that we do not have
any sort of image features so these
features are all computed based upon
properties of the HTML this is this just
this is this is only done because it's a
little bit faster than I doing real
image analysis now we also have these
interaction features which looks at the
relationship between the elements on the
page so so let's say I'm considering
making a prediction on algo one and I'll
go three do they have the same Dom ID
the same dumb class what does a
geometric relationship is one to the
left is one on top of the other one what
is the distance in what is the area
ratio so these this is an exhaustive
list of features we can aw we can always
think about more but these these were
the ones that kind of made sense at the
beginning then we have page level
features which
again are just properties of the page so
how big is the page how many how many
results are there on the page time of
day and then finally one that's sort of
important which is the viewport so
what's the thing actually on the page
right so the viewport is only going to
capture if the viewport only captures
results 1 through 4 transitions down to
10 are very very unlikely now in order
to mean the model itself is not super
important we we did because we had a lot
of data we rehash the features but we
also looked at quadratic features which
allows us to look at the relationship
between things like a you know were the
relations between pairs of modules so if
the if the if the product for example of
the two areas of the modules is
important then that would be captured by
the quadratic features then it is at the
Farley's remodel our that it allows you
to generalize to these new arrangements
of items so if I have trained my model
on a bunch of diverse data I can
actually make predictions about
arrangements that I have never obscene
observed in my data set so if I've only
seen images at positions 1 3 &amp;amp; 5 I can
make claims about what the probability
of transition is for position images at
position 2 again it requires a lot and
careful use of data when you're training
the models and unfortunately it's unable
to explicitly incorporate the data from
the target arrangement so let's say I'm
making predictions about the arrangement
of elements that includes images at two
and I've never observed that if I begin
to make observations I want some way of
incorporating that data at least in the
model as its defined now it's not
possible but we can always update
estimates and the way we do this is we
let each row of that transition matrix
be a multinomial with the dirty prior
and then we just set the
parameters for the deer schley according
to the predictions from the Farley ring
model then when we observe target data
we can just update the the probabilities
in the usual way here so again was duly
prior so the posterior is just going to
be the function of the counts that I've
observed in the and the in the new data
as well as the predictions from the
Farley ring model so here okay and new
here is a hyper parameter which is
proportional to the trust in the Farley
ring model so if you crank up Mew it
means you're going to have to observe
more data in order to adapt your model
ok another tricky thing here is how we
how we use training data for training
this model so what we want to do is
select instances so that the model is
robust so that it can actually
generalize to brand new arrangements so
one things we one of the things we do
not want to do is just a sample template
links because if we sample Justin
luelinks we're never going to observe
things well you won't observe images in
there but you also maybe be giving too
much weight to things just because they
are not diverse enough or they haven't
been randomized in the presentation so
we're going to use one of three
different sampling methods one is just
use data from the top arrangement so
this would be equivalent to just using
10 blue links second approach is just
sample k sessions so here I'm just
sampling from that distribution of
sessions I saw at the beginning of the
presentation and finally round-robin
which is going to essentially sample
without replacement sessions or
arrangements and then a session within
an arrangement over all arrangements so
that i get i try to get a diverse set of
different arrangements into my training
set experiments so in the Farley ring
paper they had five arrangements with
five grocery stores
and it's kind of cool that they looked
at the number of aisles this here this
at the number of areas is is equivalent
to our number of modules on the page and
actually about the same size i think in
our data set we have on the order of 20
different things we have on the page
they have all these other properties
other data set which are fun to look at
including like the meet location bakery
location which unfortunately i don't
have on my status it now this is this
this is the arrangement where how did
they actually get the paths well
according to the paper they the models
predictive power was tested with data on
actual supermarket traffic flows
collected on maps drawn by research
assistants who secretly followed
shoppers around the supermarkets in
Pittsburgh yeah we didn't quite do that
but it's always fun to see how people
used to do tracking and this is I don't
know if you could get away with this
these days at least Microsoft might not
be able to maybe they would in academia
okay so what's our data look like like I
said back at the beginning of the
presentation we hit we're collecting
data from two different search engines
they were they were they were they've
collected over actually in two different
times type of times of year here's some
just high level statistics no personal
data was used for this experiment so we
didn't look at the query we didn't look
at at the user ID or anything we just
looked at the transitions between the
mouse be between between the modules
certainly you could think about
incorporating those things as you begin
to come up with more personalized models
about how individuals use the mouse for
evaluation we looked at two different
metrics one of which was the likelihood
the second of which is just the
reciprocal rank of the of the observed a
target module in your in your prediction
so this is this is just just sort of
like an IR metric which to replace the
probability and it just to augment it
they're going to track each other pretty
closely so I don't think it's worth
discussing up a lot okay so experiment
one experiment one's sort of a straw man
experiment just to make sure everything
is working how well does a model perform
on the most frequent arrangement so this
would be equivalent of saying well I've
test I've trained this model all using
all sorts of different sampling
strategies how do I actually know how
well performs on the 10 blue links so
the way we operationalize this as we
sample em sessions from the most
frequent arrangement so this would be m
m sessions from 10 blue links for
example and then evaluate the
performance using our two metrics in
this case what we expect actually is a
queue it happens is like if you train
the model using the most frequent
arrangements you're going to perform the
best on the test set which has the most
frequent arrangements it's kind of makes
sense sampling from Rennes definitely
randomly from all of the arrangements
obviously it gets you not quite as
strong performers but still pretty
strong performance because that
distribution is pretty skewed and
finally round-robin performs the worst
which again is to be expected because
you're not you're investing more
robustness than you are in making
predictions about the specific
arrangement yes there's some kind of
target when you and for that one you
want to know how like in the users to
transition right so it would be so let's
say I'm at in the data I met I'm a tell
go to okay and then in the data I've
observed I'll go three next my model is
going to make predictions about it's
basically again I mean the multinomial
is going to give me a ranking over all
the other modules on the page this is
the position of that module in that
ranking I mean they're gonna they're
going to track each other throughout the
rest of the presentation to its we can
discuss what they are differently later
no self transition you could because I
mean self transition is interesting
because you could that's one way you
could mod model the amount of time users
spent on page or on a specific module
which may or may not be a good
evaluation signal
but that's that's an experiment that we
need to do yet okay so that's experiment
1 it sort of confirms that things are
more or less working as expected now
getting into something we're more
interested in which is how well how well
do the models perform on the least
common arrangements because this is
really what we want we want to be able
to make some predictions about
arrangements we've never seen before so
in order to test this we took the least
frequent arrangements of items on a page
and items on a page and use them as a
testing data so in this case everything
reverses so of course if you only train
your model using the most frequent
arrangement you're not going to be able
to generalize to the least frequent
arrangements because maybe you've never
observed images in your data set before
and in your dataset in the top for the
most frequent session arrangement round
robin performs the best because it is
focusing its you are focusing the
evaluation of the model on the
robustness you want it to be able to
generalize to brand new arrangements
random is performing okay but it's still
not as good as round robin because of
the fact that you have this skewed
distribution so it's going to be
focusing a lot of the modeling effort on
the most frequent arrangement yes higher
absolutely yeah as in as in points of
clothing earth or 4.58 on a previous
slider there miss remember that so it's
actually more accurate on the so the
round robin is more accurate on them on
the least frequent male all surrenders
as well top is not top on top is doing
worse than round robin on bottom because
top vision just have a lot of top under
service since why would that be worse so
interested ooh with the number of items
on the page affecting well the
reciprocal rent yet so so so it will
affect these metrics right so if you
so I guess my claim is that and I think
I'm wrong my claim is that you will have
fewer items in experiment 1 so the upper
bound on the I guess no the the lower
bound on reciprocal rank is actually
lower jaw is higher right well for top
yeah it doesn't explain yeah yeah yeah
yeah so that doesn't it actually should
be the opposite if that's true yeah yeah
yeahs for specific queries and that as
you get towards the tail is also a less
frequent yeah I I don't know i think i
think it's worth looking into but uh
because okay it's rather you have an
image onto any video answer but when you
have them it's very obvious that
everyone's going to look at what i have
young a lot more races if you just have
their reign if you've only trained on
the razor mins top you're just gonna I
mean you can't choke yeah but I mean you
would expect it yeah maybe so yeah i
think i think it's worth looking into
okay experiment three like i said we can
adapt these far leavering models so the
way we tested this was to take I've
lived random things random arrangements
not sessions random arrangements from
the from the torso of that distribution
and sampled adapt adaptive sessions in
order in order to see how well we
improve with performance as a function
of the training set size so here
training size means the number of on
target examples that I get and and so of
course everything everything gets better
with with with more examples and
everything sort of converges to the same
thing the more
thing to notice it more important thing
to ask is well at what point does a do
they all converge because the because
the point where they all converge is the
point where you can actually throw away
the model and just start estimating
things using the mle so the mle remember
is just counting the number of
transitions so what we observe here is
roughly for both metrics around 150 or
200 they all converge so what that just
means that the value of the offline
model or the value of the the Farley
ring model is that it will let you avoid
essentially 200 sections in your user
study ok if we assume that the only way
we can get that one way we can get this
data is through an eye-tracking study or
mouse tracking study we can actually
dodge that cost by by by running the
offline model so that's one way of
quantifying the value of the model ok
experiment four which is kind of kind of
fun which is to ask question well you
know this is cool for frenching
questions about arrangements we've
already seen now let's let's ask
ourselves well how well can you predict
just crazy arrangements so what do I
mean here this isn't so crazy because
things like images look like this the
image results look like this but it is
crazy because what I'm telling the model
here is that these are all algo results
so imagine I have 10 or 25 text results
arranged in a grid I want to be able to
make a statement about how users are
interacting with that page ok so here
this is not this is not the diagram but
the matrix black and white matrix ahead
before this is actually each each square
here is a different result they're
shaded according to the probability of
the user looking at that item first ok
so this stuff at the top left hand side
of the page is getting more attention
than this stuff at the bottom right hand
side of the page so this is nice
diagnostic exercise to make sure the
things are looking right and we know
that
or we have evidence that users tend to
browse top to bottom left to right now
let's say I replace one of those things
in the middle with an image so that's an
image of the same size as the rest of
the text results the models predicting
that the users will look at the image
first and then look at the rest of the
text results in the same order it would
have before let's say we replace four of
those results with just one big text
result again we have that we have in
general the same left right top bottom
scan path but the the larger elements
going to get more attention and then
also when we have one large image you're
you're getting all the attention on the
image and you're getting a little bit
more you're getting a little attention
on here but certainly not as much
attention as if it were alt text so this
is this is certainly not an evaluation
but yes no no no absolutely absolutely
and no I thought I did what's that yes I
agree I agree working on it right so so
so right this is not evaluation this is
just sort of a qualitative demonstration
of what the model is actually learning
right I think that wraps of presentation
so so what do we learn here that you
know we can learn these robust model so
to make predictions about what people
people are mousing over on the page at
least and if we if we trust correlation
between i and miles tells us what
they're looking at if we want to
quantify the value of these things the
farley during models about the
equivalent to two hundred lab sessions
and in terms of future work you can
think about the obvious things which are
to extend this to non sir pages so how
do people how come how can we build
models about people reading other types
of pages such as document page
documentation pages or mail pages etc
and finally since a lot of interfaces
are becoming more and more touch base we
can we can start to think about
generalizing these models to other
devices okay so that's it and I'll take
a question
yes the optimal order given a set of
objects it's I believe that's I believe
it sent people complete because you need
to basically you have to go over all
different arrangements and then you also
need one thing that I didn't talk about
here was which you would need for your
metric is a vector of relevance values
right so this Allah put the order user
will look at things if you essentially
do the combination of the relevance
values with that you would get in sort
of in a dcg world you could get the
accumulated gain as their iterating over
this this arrangement one thing that
doesn't capture is the probability of
transitioning out of the session as you
begin to observe more more relevant
information so it's definitely something
that that's one of the reasons I
developed this model was was for exactly
that problem because with things like
federated search or vertical search you
need to you need a more complicated
model than just a cascade model
your transition probabilities will draw
normalized right yes I imagine the
column normalized one especially for
grocery stores as is somewhat
interesting where people tend to buy
south after buying crisps I'm changing
some layout on my page i need to know
what else i can change bring people here
i said it said that once more I i'm not
sure why that's why that would be
interested in in in well i'm interested
in bringing people to my area of
interest so what else do i need to
change in the page to bring people here
oh the right right yeah transition
problems especially good i don't
understand how that would be either the
column normalized version of it yeah i'm
interested in milling for metal people
right right right so you're right Izzy
what you're saying is you want to
maximize essentially the sum of the
values in the column yeah yeah what are
your mean yeah it's worth asking like
what are the degrees of freedom and I
think you have to use you have to do
something like what malad was suggesting
would just incorporate the relevance
value so if nothing is relevant on the
page and you're the only relevant thing
on there then the column sum is going to
be relatively large whereas if the first
thing on the page is relevant even if
it's the same arrangement it's an user
might just stop there so imagine for
navigational query but yeah I think it's
an interesting problem but it also
suggests that it's that's an adversarial
situation where were you know I'm an
advertiser and I want people to look at
my thing first as opposed to letting the
algorithm decide based upon what did
what it thinks people should look at
first
about using graphical interestingness
type features just to go beyond blue
link to blue link with a bad blue link
with deep links blue link with so some
of the right so some some of the Dom
level features capture that because when
you when you say like there's deep links
in here it'll be in the in the in the
class of that div so some of that is
captured in their image next to search
resolved she said I've not an image on
sir that's the way back so with the
picture but you may even that is encoded
I mean the people who work on front-end
are oftentimes just dropping that stuff
in there so I'm capturing a lot of that
pardon me recent if you care to name of
the class that's right that's right
which is just cheap it's not it's not
exactly what you want but it gets you a
lot of the way there what I'm not
getting though is say I have an image
answer i'm not doing not getting any of
the analysis of the image itself what
color the image is are there other faces
in the image which will affect the
attractiveness of it sighs i'm just
wondering how much of the gains can be
chewed by a simple baseline that only
considers the size or the size of the
dumb discounted by the position or
something like that do you have a new
results on that yeah i mean i think
you're right that's what that's worth
looking at that was I mean there's a few
things to strengthen the baseline to be
honest like one of things that Phillip
pointed out earlier is is that you're
you're not very likely to see you know a
transition from you know three two one
without going through two so imposing
some sort of a planar graph on this
thing kind of makes sense or doing what
you suggest which is which is like don't
even worry about data just just look at
the relative sizes of things but then I
mean what would you I mean so so I have
let's say had the relative size of
things how do i turn that into a
probability it's not clear
or the orders I mean we can sit around
and take away about ways to hack that
hacked that out but uh I think as its
defined here it's just like we have this
bucket of data here so this just pour it
into the model as is and let it go what
I'm trying to find out the model is very
elegant but I'm just trying to see how
much if the gains could be simply shaped
by something very basic and how much
they sell like it model is adding on top
of simple heuristics so the problem with
heuristics is that as your as the types
of things you're adding onto the page
become more more complicated you have to
add more more heuristics so for example
now I want to model deep links
differently than non deep links well now
I need to I need to go up with a
heuristic or how that will turn into a
probability or let's say there's an
image in this algo result how does it
how does that change things so I heard
the reason the baselines sort of simple
here's because I didn't want to invest a
lot of effort into the baseline when
there was a lot of modeling work to do
but i agree with you if we SAT around we
could certainly come up with with
stronger baselines but i don't know if
it'd be worth implementing um I think I
i I don't I don't have them here I think
I ran that analysis a long time ago but
I don't remember them off by Hannah
results no I mean this is one of the one
of them I mean that's where the data is
now if we could if we could run these we
do if we could run the data collection
on non serve pages that would be great I
would love to do those experiments the
problem is that or I mean the good and
bad thing is is that search has the most
sophisticated login infrastructure that
I've seen across properties so that's
probably one of the reasons why that
restrictions are
hope this one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>