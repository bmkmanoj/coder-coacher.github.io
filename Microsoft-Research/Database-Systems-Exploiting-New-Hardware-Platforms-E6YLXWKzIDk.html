<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Database Systems Exploiting New Hardware Platforms | Coder Coacher - Coaching Coders</title><meta content="Database Systems Exploiting New Hardware Platforms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Database Systems Exploiting New Hardware Platforms</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E6YLXWKzIDk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
welcome you to the session on database
systems exploiting new hardware
platforms this is going to consist this
session will consist of four talks given
by distinguished practitioners in the
area all of whom have a slightly
different perspective on on how we
should we should do this this happens to
be a very important ongoing area and we
will we will be talking about that I'll
have a couple of preliminary remarks
which gives us a little bit of time for
people who are stragglers to come in
before the first really substantive talk
so it turns out that that that the
hardware platforms have changed a lot
and our ability to deal with the new
platforms is very much in sort of a
catch up days an example it's in fact
very different to have all data in main
memory versus having only having all the
data that you happen to have on disk in
the main memory cache that happens to be
a very different kind of problem and in
fact by knowing that data is all in
memory you can in fact make a system
substantially faster than it would be by
simply putting it all into a main memory
cache from a normal displace this this
new hardware setting requires a change
in in many areas among them access
methods concurrency control and recovery
and you'll see all those areas discuss
today as well as others the underlying
technology that on which database
systems are built has changed
substantially over time one of the
interesting new things is that we have a
gap filler technology in the form of
flash memory which which in fact has
some interesting properties it's around
10 times the cost of disk storage but it
has the significant infinity of being
around a factor of two hundred faster in
terms of the number of i/o operations
per second that it can support and and
so both of these technologies
are confining a place in modern systems
and and both of them are interesting we
are moving to a strategy where they
don't do any update in place which
fundamentally changes how you want to to
to use them and in particular it works
makes rights right into the storage
medium very costly indeed we have
memories that are enlarged servers can
go up to a terabyte in size three levels
of cache and l1 cache which is around
100 factor of 100 faster the main memory
so how you use cash and whether you use
cash effectively is in fact a very big
deal in terms of what kind of
performance you see with single core
performance seems to be now in the two
to three gigahertz range and and not
going up instead we're getting more and
more servers and so so we have to figure
out how we're going to use these
additional members of servers
effectively and again that dramatically
impacts the way in which which you use
modern hardware so this session we'll
talk about about that we'll talk about
the problems encountered the performance
of the of the systems that are result
result from this and and we're we're
going to go going forward the i'm happy
to report that we have we have three of
the real leaders in this area Paul varsa
from Microsoft Research Justin
Lewandowski from from Microsoft Research
Thomas Norman from the Technical
University of Munich and jignesh patel
from the University of Wisconsin in
Madison I'm going to now turn the
microphone over to Jake desh who will
start the session
great thank you everyone and thank you
for giving me the opportunity to present
to you on some of the things that we are
doing in the space and the big Direction
change that we need to make as we try to
build big data systems that run on
modern hardware so the first is I'm
going to break this up into good news
bad news the first good news is big data
by definition is about data analysis and
management and database systems are at
the heart of it viet descansen have been
screening loud databases rule so it's
finally the whole world is paying
attention to us but there is some bad
news which is that the big data software
that we deployed today to run these big
data systems are largely based on
software and technology that were
designed for a bygone era so what we've
delivered looks very much like a pig
with some lipstick to do these types of
processing and now hoping I can convince
you why I feel that way the other good
news is there's a lot of growth
happening in this area people are very
distracted with this growth and things
are sort of working okay right now and
so they're not quite paying attention to
this big mismatch we have between
hardware and software but the last bad
news I have for today after that it's
all going to be good news is that there
is a significant mismatch between the
hardware and software and the hardware
changes are accelerating in a way that
we can't hide in this mode for a lot
longer so what are these hardware
changes these this is the familiar io
hierarchy where as you go down the
spectrum you pay higher latency also you
have higher capacity but you pay a lot
more per byte higher up in the hierarchy
and a couple of disruptions that I want
to focus on in this talk of course
they're disruptions happening all across
this entire ecosystem I'm going to focus
on 21 is at the upper level which is
where we have lots of multiprocessors
things moving towards larger main memory
systems and then down to this area where
we now have nvram technologies like
flash storage that has really brought
down the gap between drm access and
stable storage access by a couple of
hours
of magnitude okay in the past what we
got across this entire spectrum was
every generation which was about every
two years we got processing or storage
that was twice as more than the previous
generation so Intel kept giving us
processors that are twice as fast for
about the same cost with every
generation and they did that for many
many decades but now what has happened
especially over the last five or six
years is we've started to see that
design of hardware systems across the
entire spectrum is governed by power and
we are no longer seeing this
high-performance low-cost we are still
seeing the double of transistor density
doubling as we did before but even there
there's been a significant change that
the next generation every generation now
has increased from about two years to be
about three years right so you're seeing
a slow down even in this very
fundamental traditional growth of
Moore's Law more interestingly what's
happening if we just look at the
processor is that we've hit essentially
a Powerball and we have of now forced to
design processors that have to stay
within a certain thermal limit and so we
no longer can keep giving doubling the
performance that we did in the past but
instead what we do is we start having to
play around with tricks such as adding
more cores and if you ask architects
there are all kinds of numbers they'll
give you roughly the answers they'll
give you is that we are going to give
you about forty percent more cores every
generation they might give us some other
hardware mechanisms to do stuff they're
trying to figure out how best to deliver
the computing in these transistors that
they can put into these processors now
data is often growing at a rate that is
that is far faster than this 40-person
growth every three years and the big
problem we have in the database
communities as we look ahead if my data
is growing by forty percent every year
and this is what Gartner's thinks is
happening in the big companies growth of
data is about forty to sixty percent
every year if i'm only getting forty
percent more cores every generation
which is now three years i have a
fundamental problem if I've got a data
set and I wanted to do the waltz in
operation of just scanning the data set
from start to end to do an analysis I'm
building up a deficit of about 2x every
generation right I include basically
what this means is I'm going to need
bigger and bigger computers or more and
more processors to do the same type of
analysis I need to keep up with the data
growth rate and that's clearly
unsustainable so the central message I
have today is there are lots of things
we could do including playing with
algorithmic tricks and doing
approximation all of that needs to be
done but I want to focus and the fact
that we need to do more with less by
paying closer attention to what the
hardware and software does I'm going to
focus on one very simple operation the
scan operation i'm just going to scan a
data set from start to the end maybe i'm
scanning it to select some results and
feeding it into a machine learning
algorithm turns out the scan is very
very common in fact companies build
appliances that are essentially scan
based and keep the data mostly in memory
so it's a very important primitive today
and the way we have solved this problem
at least in this post cut off it is with
this technique called bit waving just
presented that at sigma at a few weeks
ago and it pays real close attention to
this hardware software synergy what's
going to do is exploit what we think was
a missing dimension in processor
hardware today which is if I look at
just a simple operation of adding to
64-bit numbers really what happens is in
a single cycle at the circuit level the
processor is computing on the 64 bits in
parallel to compute the answer to this
64-bit number so the circuit level in
the processor there's a 64 way parison
that we have the question is could we
exploit that processor register words
are getting wider and wider in fact
architects are dabbling around with
processor designs in which they could
give us kilobit words and that's fairly
easy for them to develop with the
technology ship that's happening so
there's at least an order of magnitude
today with this intro cycle parallelism
in processors that preview it from the
circuit level and maybe an order of
magnitude more beyond that as we go down
the future so I'm not going to tell you
a lot of the technical details of what
we did and I encourage you to read the
paper or look at the blog post that we
have on it the only the short
introduction
do is if you're trying to scan data fast
today you would typically take things
that are in relational tables and stored
that in these things called column
stores so that each of the columns get
stored in a separate file so if I'm only
interested in looking at the quantity
field in my query I'm only looking at
that file and not the entire database
which has a lot more stuff and it beyond
what i need and bit viewing works by
thinking about the parallelism at the
circuit level and exploiting it it comes
in two flavors one is called bit weaving
V which will actually take the each of
these columns we convert that into
dictionary compressed codes so if a
column can be represented as three bits
we're actually going to stab her those
bits across multiple register words so
what you see in yellow is the actual
code for a column it's across three
consecutive registered words and so the
most significant bit of all the codes if
it's a 32-bit example will be together
and so on and so forth and you have a
whole algebraic framework surrounding
between which will then that is compute
essentially on 32 or 64 tuples at a time
in a single processor cycle so that's
what gives us this int recycled
parallelism exploited at the database
level the other format is between H
which is a more traditional bit packing
format but it has a couple of additional
things you can read about that in the
paper including the use of an extra bit
that we'd store that makes all the math
works so we can use that extra bit as
overflow bit to register whether we have
computed the answer all of this works in
the framework in which we can have data
stored in either one of these formats so
if I've got a sample t PCH type of query
over here we'll compute a predicate tree
some of the columns like ship date are
in between V format others in between
each format all of a max generates these
result bit vectors with one bit per
input indicating if that corresponding
trouble satisfied that predicate and we
can combine all of these things together
to produce the final answer so it's a
full framework that lets us answer
sequel analytic queries and the bunch of
results that we have in the paper
we do close to an order of magnitude
better than the state-of-the-art methods
I want to spend the last few minutes
going down in the spectrum a little bit
if you think about how we've been doing
computation and a in the traditional
computing world we've traditionally
taken data that comes from the i/o
systems moved it to various buses to
compute answers and today what we do is
we do sort of the same thing maybe we
replace things the hard disks like flash
but essentially a processing is moving
data through multiples levels of buses
before it gets to the processor where it
computes if you think about the
long-term implications of storage and
computing the boundary between where you
can compute and where you can store is
changing processes compute fast but they
also store data and caches similarly
storage devices store stuff but they
also compute the cost of storing and
actually computing is free all of the
Costas isn't actually moving and
powering these devices so this is a
collaboration between Microsoft and
Samsung where we took one of Samsung's
SSDs which essentially looks like a
small computer inside it's got a lot of
processing power and what we did is we
said if you try to pull data out of the
flash device through SAS a pci or
through SATA interfaces the growth rate
of that relative to 2007 is that yellow
line as these vendors try to put more
and more capacity in the devices
internally they have got an internal
network to pull data from the chips that
are on the right hand side and that
growth rate they showed by the red line
and there's already an order of
magnitude difference between those so he
said can we actually take sequel server
production quality code in a complete
prototype so this is a hacked up version
of sequel server and we ran t PCH
querrey 6 where we selected push
selected computation down into the
storage and if you use just a regular
SSD that's that red box if you use a
smart SSD that's that a circle over
there the response time is about two to
four x better and the total energy
consumed by the entire box is also much
better okay so you can start to see
where you can generate a lot of these
efficiencies of scale and with that i'm
basically going to end
I hope I've convinced you that
transformative changes happening at all
levels hardware and software needs to
work a lot closely together and like to
thank the quickstep team that did most
of the hard work here thank you thank
you so our next speaker will be Justin
levendusky from from Microsoft Research
and he's going to be talking about
something called the BW tree which I
know a little bit about next Dave so I'm
Justin from microsoft research and i'll
be talking about how to architect access
methods for this new hardware era and
this is joint work between david
loamians to dip this in gupta also at
microsoft research so an alternate title
for this talk could be the BW tree a
latch free log structured be tree for
multi-core machines with large of eight
memories and flash storage so if you're
wondering what be W stands for it's
actually buzzword because this title is
fairly buzzword complete right so a good
would introduce the BW tree is to talk
about the buzzwords the first is b tree
so the BW tree is the Bugbee tree of
access method or data structure we know
and love and a bee tree provides keyword
or key range access to records provides
efficient point and range lookups which
is why it's a popular access method in
database systems and operating systems
it's self balancing and the version
we're talking about here is a be linked
tree so intermission to search pointers
down the tree right we also forms a
linked list with each node pointing to
its direct sibling to the right and of
course is a nexus method if it runs into
memory pressure we can cash the hot
pages of the index in memory and spill
over the cold pages essentially to to
flash or disk the BW tree starts to
differ from more traditional be tree
architectures in the following ways that
we we target two main hardware trends
the first is multi-core machines with
large main memories
so the BW tree is completely latch or
lock free the database community uses
the word latch where the rest of the
world uses the word lock so there are no
critical sections providing a single
thread to get in and an update a piece
of memory so so work of threads do not
set latches for any reason this means
that threads never block in memory and
we don't and an important point is that
we don't rely on data partitioning to do
this meaning we don't assign updates to
a single thread for our partition of the
index any thread can come down an update
any page it sees fit we also avoid
updates in place so instead we use delta
updates that describe kind of the new
change to the page and this is important
because it reduces cache invalidation
which is important for large multi-core
machines as well we also target flash
storage flash is as good at sequential
i/o and relatively good at random reads
but relatively bad at random writes due
to the flash translation layer that's
needed to to on a random right so here
we use flash is essentially an append
log we implement a novel log structured
storage layer over flash to take
advantage to always write sequentially
back to flash door so the BW tree is
layered in kind of a traditional manner
at the top we have our key value store
interface the so-called crud interface
for creating reading updating and
deleting records in the index we also
support of course range scans which of
this being a b-tree this layer of course
provides all the BW tree the beach
research and update logic and all the
operations are completely latched free
on at this level it seems everything is
in memory if it needs to bring a page
essentially from log structured storage
from flash to disk every relies on a
cache layer that will do this for it to
provide this mapping which also provides
a logical page abstraction for the
b-tree layer and blow the cache layer we
have our persistent layer our flash log
structured storage built over flash and
here's bright again provides sequential
writes back to its flash and also
provides log structured garbage
collection
given the more the main memory tilts for
this talk I'll mainly be discussing for
the rest of this talk kind of the
in-memory parts of the of the BW tree
which is the V tree layer and the
in-memory version of the cache layer so
the video feature is rather versatile if
we don't need a kind of a persisted
layer we can use the BW tree as a very
efficient in memory latch freebie tree
and in fact the BW tree is the ordered
index in the hackathon main memory
database system which Paul will be
talking about next I can also be a
high-performance standalone persistent
key value store you can think about this
is no local storage and in a
multi-tenant cloud setting something
akin to Google's leveldb or if
transactions user transactions are
needed it can also be what we call data
component in a Deuteronomy style
architecture and Deuteronomy refers to a
a project we have at MSR to essentially
component I Xindi couple the the
transactional functionality in the
database kernel into a transaxle
component which is in charge of
transactional functionality that
interacts with a data component which is
in charge of data storage and atomic
updates so the VW tree can can support
multi-step transactions in this way as
well so now I'll talk about a little
about how we made the BW tree completely
latched free in memory and it's been
said that you know an extra layer of
indirection help solve all problems in
computer science and this is certainly
true in this case so all pages in the BW
tree are logical they all have a logical
ID which is an entry into the mapping
table which translates its logical ID
into some physical location so the pages
in memory this can be an in-memory
pointer if the page is currently on
flash it's an offset into our log
structured store in the in this is
important also that the the pointers
held within be tree nodes are logical
IDs so any pointers held between nodes
on a traversal much first go through the
mapping table let's read the ID to get
the the physical location of the page
all right this is important for both
electri and behavior and and log
structuring another important point is
that it isolates updates to a single
page
recall that I said you know we we we
don't update pages in place this means
that it creates a new address on every
update to a page so to avoid having to
propagate this address change all the
way up to the root of the tree the
mapping table essentially isolates this
update to a single page so to update
pages we use Delta records so say in
this case we have the mapping table with
our entropy pointing to a base page P so
this is you can think of this as a
traditional be tree page with sorted key
vector is very efficient for search and
so we want to update the page by
inserting a record with key 50 we'd
first create a new Delta record make it
physically point2 p and install it in
the mapping table using a compare and
swap which will exchange essentially the
old pointer to the base page p now to
the Delta record right and we can keep
on doing this say we want to delete
record with key 48 weed again install it
with a compare and swap and it's
important to note here that multiple
threads might be racing to update the
same page but the compare-and-swap being
an atomic struttin struction allows only
one updater to win and the rest of the
update is that lose must essentially
retry right so it's also important to
note here that that if we use the
persistence layer log structuring pages
on our log structured store much look
very similar to how they do on memory
that is if we flush say the base page p
at some point to our log structured
store and we had a few deltas and we
need to flush the page again we only
need to flush the deltas back to log
structured storage and make them
essentially point back to what was flush
flush previously in the log right this
greatly reduces write amplification back
to back to flash storage so using delta
updates we eventually can get a very
long delta chain prepended to some base
page and long doubt the chains
eventually degrade search performance
right so we use delta consolidation we
occasionally consolidate a page by
creating a new search optimized page by
applying all the
Delta updates to some page pig in this
case and again install it using a
compare and swap right so this now
installs a new search optimized page in
the mapping table and this consolidation
is piggybacked onto regular operations
as are most other modifications
maintenance modifications in the index
another point point here is that the
this being a latch free data structure
the old version that we swapped out of
the mapping table might still be
accessed by other threads right so to be
able to to not d allocate the memory
before or to be able to safely
deallocate the old version of the page
we have an epic garbage collection
mechanism that ensures that no other
threads can be accessing that page
before we elevated so updates to regular
pages are relatively simple what's hard
to make latch free is allowing the the
index to both grow and shrink right so
allowing pages to both split and to
merge right so I'll describe how splits
work in the BW tree fleets work in
somewhat similar manner I'd be and happy
to talk about it offline so splits are
installed in two atomic steps the first
step is to say we want to split page 2
in this example the first step is to
create a new page for I'm allocating it
in the mapping table and copying over
the upper part of the key range from
page to page four so the purple and blue
records in this case we then install the
split using a split delta onto page two
and this essentially says that you know
the upper part of the key ranger page
two is now being taken taken care of by
page four we make the split live in the
index by performing a compare and swap
to install the split Delta in the
mapping table and at this point we have
a valid search tree since all searchers
coming down through the parent and
accessing page two will see the split
Delta and immediately know that the
upper part of its key range is now being
taken care by by page four and their can
traverse the side pointer over the page
for the last part of the split the next
part is to install the index term at the
parent so this is essentially the new
search key and pointer down to page four
and again we use an index
entry Delta to describe this change and
install it using a compare and swap
right and eventually all these
maintenance deltas are also consolidated
into a page much like the record updates
are as well so finished by just
providing a few performance highlights
we these experiments are in memory
experiments only and we compared against
the Berkeley DB standalone key value
store btree we took out user
transactions for for fairness and this
represents more of a traditional btree
architecture with page level latching
for updates as well as updates in place
for for be three pages and we also
compared against a latch free skip list
implementation so up until the BW tree
let free skip lists we're one of the
state-of-the-art kind of latch free
range indexing methods we use three
workloads one from xbox and this is
updates a performance trades updating
state and a multiplayer online game for
Xbox Live we also used a real enterprise
storage deduplication workload from
Windows Server and also a synthetic
workload that has a read/write ratio of
roughly between the two wheel workloads
that were used so compared to Berkeley
DB the BW tree you know it shows much
better throughput performance and
especially if node is the xbox workload
that at any one time upstate it's a
really small set of keys so only focus
is on a small set of pages and here's
where we see on the latch free nature of
the b-tree do very well so whereas a lot
of threads would be lining up behind
latches to update a page in berkeley DB
there are no latches in the pw tree and
we also see some of this better
performance is also due to avoiding
updates in place for better cache
behavior versus skip lists the BW tree
is roughly 4x faster and and on the
synthetic workload and in both indexing
methods here our latch free right so
where the BW tree really does better is
in is in the l1 and l2 cache behavior
right so if we look where accesses are
going for both indexing structures the
BW tree spends most of its time more in
common
vietri pages searching you know compact
be three pages whereas the Skip list
needs to essentially do pointed reversal
on every point of the search as a
searches for the next hour down the
search so with that I'll wrap up the
talk and hopefully have given you some
insight into constructing access methods
for these new hardware paradox Thank You
Justin the next speaker will be Paul
Larsen from from Microsoft Research who
will tell us something about i believe
about hackathon so well for those of you
who came here to learn all about that he
won't be disappointed thank you Dave
let's see so evolving the architecture
of our database system let's start with
a little bit of history some of you may
remember this all the major database
systems commercial database systems are
out there were essential designed in the
early 80s and if you look at what a
typical machine looked on da days you'll
be astounded you wouldn't imagine today
that you could actually write a database
system I could run on a machine like
that but it was done and the older all
the system basically have the same
architecture you have data stored on
pages on disk you have a buffer pool
which essentially is a caching layer we
have a lock manager and so on in about
two thousand eight in 2009 it became
pretty clear to many of us that this
architecture wouldn't take us forward we
couldn't continue using relying
completely on this architecture for the
next 20 30 years we had to do something
because here you see the hardware trends
this goes up to around 2009 memory
prices are going down until an A and D
keeps on giving us more and more course
and so on
so what are we going to do well
revolution is out if your commercial
software vendor you have to evolve your
system customers do not want to change
their applications there is no question
about it so the conundrum was what do we
do with sequel server architecture how
do we move it forward and i'll talk
about to briefly mentioned two projects
that that that we undertook to move this
forward one is Apollo which is
integrated columnstore technology in the
sequel server that started in about two
thousand eight deliver the first hour
installment of that is equal 2012 and
there are further enhancements in sequel
2014 this particular technology the
issue was not so much the technology was
well understood how to build column
stores the issue was how do you
integrate it into sequel server and we
decided to do it to expose it as a
separate index type it actually worked
really well but that's not what I'm
going to talk about I want to talk about
hackathon which is our main memory
engine which also is fully integrated
into sequel server and it is targeted
for oltp workloads it will ship in
sequel 2014 the first beta is available
you can download it and try it out if
you want to and I'll talk a little bit
about the architecture and because
primarily about the considerations in
the design so here is what we call the
for architecture pillars of hackathon
down at the bottom here we had a
hardware trends that you are all
familiar with declining memory prices
many core processors and so on and over
there here are the business drivers
which is really really important if you
are selling software commercially first
of all this engine is optimized for main
memory
the data structures are so on that we
use everything is optimized for main
memory that's that's the key assumption
in disease support two types of indices
hash indices and arranged in disease
using the pw tree they exist only in
memory we do not maintain them on a disk
or flash at all there's no buffer pool
there's no caching layer because we
assume that these tables or fit
completely in memory now for durability
of course we have to put something out
on external storage but all of that is
completely stream-based no random access
whatsoever because bandwidth external
band which is a lot easier than then
then random access given the fact that
we are getting many many more course we
had to design a thing to achieve very
very high concurrency so two things to
that one is we use a multi version
optimistic concurrency control which are
going to talk a little bit about the
core engine itself uses only lock three
data structure there are no locks
latches or spin locks in sight that's
all good but individual course are not
really getting any faster so we had to
reduce the number of instructions that
we execute per transaction very very
significantly in the way we do that is
we compile TC equal which is the
standard interface for sequel server and
everybody uses into machine code finally
as a commercial vendor we cannot know
how should I put it the decision was to
integrate this into sequel server
because as I mentioned customers do not
want to change their application but
really do not want to do that
and that's where a lot of the work was
so it's integrating both queries and
transaction integrates high availability
and backup restore and so on all right
let's talk a little bit about
non-blocking execution one of the issues
in sequel server and in any system that
Luis uses locks or various kinds it's as
sometimes those are going to limit the
level of concurrency that you can that
the system can support it's fairly
common a sequel server that the limiting
factor is not the CPUs not the disk it's
the latches that you have you have too
much contention on various latches and
so on so what we wanted was highly
concurrent execution with full CPU
utilization no thread switching if we
can avoid it no waiting no spinning and
so on just make sure that the CPUs run
on full speed because the data is
sitting in memory so that led to three
really important design choices large
free data structures only in the core
engine multi version optimistic
concurrency control and the fact that we
allow a limited form of speculative read
sometimes we have to allow a transaction
to tentatively read something that
another transaction has written even
though immature is going to commit or
not so we allow some of that so what's
the result well the absolutely great
majority of transactions in hackathon
run up to the final log right from
you've got to write the vlog so we don't
lose things without ever blocking or
waiting they just run straight through
right to the log and then wait for the
log right to finish
the internal the in-memory organization
of the data is quite straightforward
it's a multi versioning system so we
have time stamps to in to indicate the
valid time of a version we use embedded
pointers so indices I use embedded
pointers so we can always refer directly
by a physical pointer to a version two
index types are supported hash indices
and range unisys using VW trees I should
point out that we do not rely on
partitioning any thread can at any time
try to update a record or access any
record okay so why multi version
optimistic concurrency control this may
be obvious to too many of you but
perhaps not everybody first of all multi
versioning breeders don't block riders
and vice versa essentially we've
separate out the readers they don't
compete with with themselves or with
writers and then the writers just have
to compete with with other writers no
lock manager lock managers had huge
overhead especially in the main memory
setting it was just not acceptable to
him that high overhead that you would
have to take out the lock for every
record access it's actually more
expensive to take off the lock then to
to to access the record well you can get
around a little bit but in any case no
led locks so we don't need any deadlock
detection I can be made highly parallel
in the implementation that we have of
this optimistic multi versioning there
is only one synchronization point and
that is each transaction has to take out
an end time step and those are to be
sequentially ordered
that's one single atomic instruction
everything else happens in parallel
lower isolation levels are run very very
fast the end result is that this
performs very well even under high
contention and handles a long read only
transactions well there you see an
example of the kind of scalability
improvements that we see compared with
locking much much higher scalability a
little bit about where a hackathon fits
into sequel server first of all heckle
itself is just a storage engine compiler
in memory storage engine compiler that
comes compartir sequel into native code
and then a lot of supporting runtime
routines but it relies on it leverages a
lot of the features in sequel server for
example I doesn't have a catalog it uses
the standard sequel server catalog and
it gets its metadata from their high
availability is integrated with a sequel
service high availability story doesn't
have a separate log it uses the sequel
server log and so on what actually took
most of the work are those arrows here
the integration with sequel server not
the heck of that engine itself as i
mentioned transactions and queries are
fully integrated with sequel server so
regular see you can access it using
regular sequel server queries you can in
a single query you can access hackathon
tables and regular sequel server tables
updates can go across multiple tables
and so on this is absolutely crucial
from a commercial point of view and from
customers point of view here you see
some idea about how much throughput
improvement we can get this was a
workload where a sequel server had a lot
of content
and we gone what we did was we converted
as the key table there to an in-memory
table without changing any other code at
all that got us a three-point 3x higher
throughput in this particular example
then there was a stored procedure that
did the work who converted that to a
native stored procedure in Diagon owes a
throughput improvement of almost 16 X so
a number of customers are running this
summing production some in tests bwin
has been running in production of for
quite a while and their true put maxed
out at 15,000 requests per second using
sequel server and throwing hard we're at
it didn't help partitioning did help a
little bit but not enough so with the
heck of an engine they got that up to
250,000 requests per second which was
more than enough for them they have
other bottlenecks that they hit much
earlier than that edge net is an
unaccompanied where their problem was a
data ingestion rate they need to ingest
data daily very very fast they maxed out
at 7,500 rows per second and we'll hang
it on up to 126,000 which allowed them
to change their business model instead
of batch ingestion they can do it
essentially online that's all I had it's
available you can download the beta and
try it out if you want to it will ship
in sequel server late this year thank
you very much Paul
our final session speak possession is
Thomas Norman from the University
Technical University of Munich and
Thomas will tell us about the hyper
system oh and one more thing is we will
open it to questions after Thomas's talk
yeah so today I will talk about the have
a system so a research database system
that we bid at our University and the
main goal of the system is to build a
combined engine for OTP and all up
processing which is something that
traditional systems don't usually do so
if you look at it two different kinds of
workloads you'll see in an OTP setting
where you have very large number of
relative simple transactions and I cues
us all our products or user by something
or users make phone calls right if
simple transactions but they have a very
very high rate they have been mainly
care about updates performance and on
the other end of the spectrum you see
your lab processing we might have much
fewer transactions but they aggregate
and look at very large volumes of data
and this has five different application
scenarios in the particular as they are
conflicting now we have if you want to
do both so for example you want to
analyze our current business data have
the problem that is very high update
rate conflicts with our analytic
experience because of course the data
must not change for the whole of
transactions yeah and traditionally
database systems are not very good at
wooden box as they concentrate on one of
the two things so standard things like
to PA locking would not work here but if
he assumes that our data is in my
memories and we can suddenly do things
we could not do before so then we have
new options and in fact this is what's
the innovative founding idea of our
project so we said okay if we have data
in memory memory memory we can use this
fact for transaction isolation so how
does it work in a very high level
overview we have one process who owns
the data and all the updates a short
updates are running against this big
chunk of data and the executors as
quickly as possible and now if you need
stable state of the data for longer
times before long running transaction
what we do is we copy the database in of
course only conceptually I physically
cannot copy a mighty gigabyte of data
conceptually we copy the database by
forking our process you make a virtual
copy of the thing what happens it
happens then is that this virtual copies
are initially the same physically about
once a of you perform updates it's a the
analytical transactions not allowed to
see this update is captured by the
memory management unit to get a trap
then he do a copy on write mechanism so
you replicates a change data and then
these two are perfectly isolated as a
good thing about this that this is very
very cheap because we get the checking
for the shared as a right to share data
by how to support the very little
overhead and this allows us to execute
bows and orgy piano lab concurrently is
very little open means there's a tiny
bit of aware DC some numbers which is
very little okay so this is what's the
innovator originally at the end now when
you build a system you noticed some more
things and particularly notice that the
traditional way to execute queries for
less also mentioned this is not so very
good he'll like the classically iterator
model where you interpret operator trees
this is fine if you wait for this care
because disk is so slow it doesn't
matter what you do in your operators but
if your data is in memory then suddenly
these expensive yo alters the coat
itself is not very nice you have for
many branches you have a lot of function
codes this is not not so nice on a
in-memory execution so what we do
instead I speak also compiler is and we
try to compile the careers in a way that
we touch data as rarely as possible so
in a way we think about like memory
access is an i/o operation as it's an
exaggeration and sometimes you assume
you have to go to memory but the idea is
that we don't want to access memory if
you don't have to and so we try to
minimize successes and all that we try
to do as you try to get charged loops
over small pieces of code because this
behaves more nicely in your branch
prediction in your CPU so you want to
perform a large number of operations in
a tight loop
this whole execution model is in
particular important if you run overlap
curious because they are you aggregate
millions of Chopard so you really care
about but all turn the OTP side which is
not that depend here you'll notice the
differences in its execution model okay
now how does this work in a one slide
summary so what we basically do is we
take our cleary BL shone like a small
edge retreat we take our query enzyme be
inelastic you to figure out what as a
pipelining fragment of security so
pipelining means there can be pests data
between operators without materializing
for example of this is here and a Hester
on top you assume you have a scan this
will load mattila said of course only
performs an in-memory operation for
example selection and then we
materialized into the hash table of the
headstrong so the the idea that in every
pipeline fragment you always pop load
the data into your CPU registers and
ideally really are registers if you can
it's not always possible if you can you
do this then you perform in CPU whatever
you want to do and then you materialize
into the next pipeline breaker and this
is about the minimal number of memory
access at you can do it because you by
definition you must miss Phyllis at this
point so this is conceptually very nice
and now if you combine this with an
efficient way to generate machine code
and we use the llvm a toolkit to lose
this then we get extremely compact and
efficient code in fact we are currently
closed to and always humans always very
smart but we are close to matching
performance of handwritten code for very
large classes of cure so this is very
nice now if you look now go back again
with from the all upside to go to look
to the ATP side we notice that the oil
TP performance is very much dominated by
index excesses so in x access is an
absolute most impact on the performance
of OTP processing and the question is
what should we do if you look at what we
teach to our students in introductory
courses how do we index data in my
memory the classical things we teach our
hash tables and trees inverse cos even
binary trees and we see easiest both
have pros and cons hash table
of course are nice because if you are
lucky you need one excess you get your
data very good not guaranteed that you
need one but could be danced at is first
you cannot arrange clears created by
definition but it has also some other
very unpleasant properties a hashtag
words not very well if you have
non-unique keys you have a supplicates
you have problems updating linux and
hash tables an absolute nightmare to
grow means they are hash tables that can
grow but you pay a heavy price for this
growing hashtag which is not a very good
operation and so this is every we don't
like them if you look at trees really
feature wise good of course you can
arrange clears or good things but trees
are slow binary trees are terribly slow
be trees are not terribly slow but also
they are they have not perfect in
performance in particular as if you look
at what happens if you navigate this
tree what you always do is you compare
two elements then you branch depending
on the comparison well clearly but this
is not so good because you are you
cannot predict this very very CPU and so
this is a relative slow operation you
know terribly slow but relatively slow
we therefore at least by default use
different kind of inuk's are two namely
erratic strip so radix trade as a small
example shown to the right uses at every
level of the tree one bite of your index
key to navigate to the next chart so you
don't compare but it's conceptually at
least you have one big area which edge
you aware of this byte value go to this
node conceptually no you don't implement
it like this but conceptual guesses so
your you have direct access quite a bit
similar to a hash table but of course
only for one by it so but you directly
jump you don't compare this is good and
fast problem is if used in a permutation
of heretics tree you have a problem with
space utilization as these things can
get very large and if you are not
careful you can also get your deep yeah
because you have to index your key so if
a key as long as this could be a very
unpleasant yeah but fortunately both of
these problems can be solved and in fact
we use a radix index by default and here
try to give her
you have a limited time you but I tried
to show you that Lisa high-level ideas
how you can make a red X 3 practical so
basically what we do is you say okay
this is space problem is because your
domain is utilized heavily in some parts
and underutilized in other parts and how
can we solve this because of this by
using different types of notes so here
shown you the four types of motive
implemented so depending on how many
child notes you use more widely almost
or more valid or more narrow nose
another usual example you have 10 type
if you have only for chart notes to one
node where you have 256 side notes so
this varies you adapt to your data
distribution and the note i made so that
you can find you the right now by
without comparison so this is crafted
like this and then you get both the
direct access behavior and still compact
representation now to makes is really
practical you have to do some more
tricks you have written like here this
past compression a lazy expansion so in
particular you try to avoid generating
notes that don't carry information and
so if you if they don't help you to
distinguish tied notes don't don't start
so skip everything that you don't need
and if you do this you can get a fix to
an upper bound on space utilization pair
entry which you usually don't have the
classical red x 3 is bound by key
lengths and not by number of entries but
here you can implement it in a good way
so this is behaves quite nice now one
final implementation about to mention
you said if you look at the database
system you realize your database system
in general can be huge yes very large
but in practice it's set of data you
really excessive a few small here's a
working set is not so not to last you
most of the time are you accessing a
small number of chopper that you really
update now unfortunately these tablets
are not necessarily close to each other
in a physical sense and what you would
like to have some clothes for several
reasons for example for our
copy-on-write it's important but in
january you might want to store your
read mostly data more aggressively yeah
like you compress or an extreme case
even move it to disk and also you want
to store differently
and every in our storage scheme use such
a hot coal partitioning of short pipe
say what is a cooling partitioning so on
the top you see the hot items means
items that are accessed all the time
these are stored just as they also on a
classical in memory pages on the lower
end you have the frozen page data items
that have not changed for a long time
there's ever most likely never change
and most of the data that's all they
never change and we can store them
differently that we compress them put
them on yuge pages and so on and then he
moved tabloids up and down as a
hierarchy based upon observed access
patterns also via via trap excesses to
the pages in between then we decide if
you can go up and down now I said we
build in a combined system at
traditional people said this is not
feasible at least not with good
performance so what kind of performance
can you expect here are some numbers
when we have a very small server machine
and so we see on a remotely more like a
desktop on a very small ad server we can
see something like 138,000 tvcc
transactions ootp transaction per second
which is a very good number so on a
single call so this is very fast and if
you look at the Euler processing here is
et bch scare factor 10 if executed
sequentially single core again we need
something like 14 seconds also very fast
now that's the most interesting number
actresses heard one namely what happens
if we do both so you run T PCC and
simultaneously we run the tpc HQs
adapted to the feast kima so we do both
on the same data set at the same time
and we see if you do as we see get some
122,000 transactions per second slightly
less and without curious but not
terribly much less and so all up part is
nearly unaffected me we have some
overhead to do copying right but in
general we get extremely good
performance both ODP and all up and
boasts so to conclude I hope I've shown
you that my memory is more than a fast
disk here and memory you can do things
you cannot do on the disk random access
while Otto he Rodriguez is a memory a
memory management using the
and and not only can we do those we
should do this of course yeah we should
for example use different indexing
techniques our index would not be that
great for displaced system for example
yeah so this is aim from in memory and
also our execution model has made for a
memory processing and the system that we
have built actually demonstrates in a
practical sense that you can get very
decent behavior both already being all
up and this in a really a full-fledged
system so we do AC transactions you
offer your secret support we don't
require partitioning all those kind of
things so we get a very reasonable
system and thank you can at least soon
hopefully try this out yourself if you
want so thanks thank you very much
Thomas so before this session over the
last couple of weeks have been
interacting with the speakers and I've
been emphasizing that that they should
not exceed their 15-minute budget I must
congratulate the ball they they came
through with flying colors so that
leaves us with 15 minutes for question
answering so so what I'd like to do is
to sort of it just remind you that that
there is a person in the back of the
room with a wearing a red shirt carrying
a microphone and if you're interested in
asking the question so that we can get
the recording correctly we would ask
that you that you ask it through the
through the microphone that are our
helper here is providing also if you
have a particular person in mind that
you want to ask the question of please
please indicate that too otherwise I'll
open it for everybody
comment and rest of the last speaker it
seems like that networking which is
filled island has come on these problems
where the hardware couldn't quite keep
up and make them even hear an issue and
so I just Thomas especially I think
there is a number of residences receive
you price of our rating strings you spot
compression because we are going for
compaction and there the issue is
between we want to try to run registers
as much as possible and you're never
really going too slow for us and so it
might be just a bit we make things like
that too I think that we have to
recurrence intrapsychic algorithm it
might be an interesting exercise to
compare differences to your transactions
we want to find lecture you have what TP
was while we do this you want to say
anything Thomas of course you're all
right yeah yeahs are they kind of see
much about this yeah he means is they
probably probably bc has clearly true if
everybody has to process large volumes
of data quickly the only thing is
traditionally databases did not do this
because if we wait for disk you have a
lot of time for your cpu yeah but of
course you're right yeah and there's
some interesting parallels where people
sometimes in the networking world from
what i understand you will sometimes
burn some of your logic into FPGAs and
put it really close and I think the
database community is also starting to
explore some of that for specific
processing now so there's a lot of
peridot there that question so I was
interested to know your opinion about
the use of GPUs and the cpu GPU combined
which is appearing into various machines
and how it's going to affect not only
the oltp workloads but also the OLAP an
OLTP combines that you were discussing
so I can take that I think we are very
excited by especially with what's
happening with integrating GPUs and CPUs
on the same memory bus and effectively
it gives you a massive parison ivory
excessively about the the joules per
cycle that I can extract out and that's
looking very good for the GPU compared
to where the CPU cycles are going
especially the non risk cycles are very
expensive the interesting part is can we
build a lot of the problems going
forward at least from the analogous I'd
is is getting data into the processor so
memory buses are not scaling as fast as
the rest of the ecosystem scaling and if
you can get that memory architecture
going and scaling out on the 10-year
horizon that would be great it's not
there yet and I think this considerable
work to be done in terms of whether we
can feed the beast at the speed at which
we can we can consume it nice thing
about databases we can go and adapt our
computation like we did for example with
leaving we can change kernels there just
a handful of them to work at the speed
of the hardware but it's also the rest
of the ecosystem and the buses is where
the next set of problems is is after we
make the algorithmic changes that are
needed so sorry of a general comment on
that one that one of the one of the
problems with with using a special
hardware and it's indeed sort of a
universal problem with respect to things
like the partitioning approach to to
doing databases which is sometimes
you're trading you're creating a sort of
micro performance for for organization
and staging and and and managing your
data in such a way they can you can
actually exploit these resources so the
real problem for these things I think is
is being able to figure out a way where
where those those sort of techniques can
get integrated smoothly without enormous
overhead because because if you don't do
it right then you lose more by simply
staging it incorrectly than you do gain
by it so Alex ally johns hopkins and so
the much of the discussion today
revolved around in memory
databases and it reminded me of an old
paper Jim Gray wrote about 25 years ago
called a five-minute rule where he
claimed or where he stated that
essentially a business argument can be
made for in-memory databases that how
much memory should we buy to actually
basically make up for the I ops of up
hard disk read the same price and then
he so he claimed this crossover time was
five minutes then God's great fairy did
it then a few years later where he
claimed when flesh showed up it was 15
minutes but you cannot have changed so I
have redone this recently just for my
own amusement and we are back at five
minutes and the memory that basically
takes up five minutes of I upset around
900,000 I ops per machine is about six
seven hundred gigabytes which is very
much in line with a run-of-the-mill sir
where these days or sort of at the top
of the line so I could comment or I
could ask some of the panelists to
comment so what are the things that is
pointed out I think by jignesh and by
Thomas as well is that you can do things
in a fundamentally different way if you
know the thing everything is in main
memory whereas most of the five-minute
rule type of stuff was predicated on the
fact that you were using a traditional
architecture and so I think the game has
changed and wondered and wondered that
you get enormous gains by knowing that
that the that the data is in main memory
so so so i'm not sure that the
five-minute rule continues to apply in
such a such a strong way i think the
other part is the no communication is
where a lot of the bottleneck is not
just buses in the system but people
increasingly deploy these massively
distributed systems and it's not just
for the googles and the Facebook of the
world you're forced to do that even at a
much smaller scale and again it's that
networking speed you know we are at 10
gig Ethernet right now it's not changing
as fast as we can density pack storage
and compute and single silica right you
can't today build system-on-a-chip you
have those in your phones in your
pockets today you can pack incredible
amount of storage compute together
but getting that in and out is a huge
problem in fact this is what the quantum
computing guys worry about to a lot they
can do a lot of stuff inside a quantum
bit but communicating with the outside
world is where all the challenges are
and I think that's the longer-term
perspective is how we're going to build
systems where we excessively pay
attention to communication which
traditionally we haven't done in the
database world is very important yeah
but even this were the I hopes that how
we can page sinks in and out or from one
system to another will be important as
long as we will continue to have data
sets which are larger than a contiguous
memory of a single system but I think
there's another dimension that we
haven't looked at you know we always do
replication for fault tolerance we
haven't fully exploited the equations
between high availability mechanisms you
use for that performance related things
about which replicas to use I think
there's a huge field ahead of us if we
start putting all of that together in
cost-effective ways and balance not just
performance but these other aspects that
people worry about in a solid framework
Paul would like to comment yes I'm it's
clear that that that it's more cost
efficient to move some of your cold a
doubt on cheaper storage whatever if
storage form that has but what is not so
clear exactly how to organize that
because you still want to have very
rapid access to it and integrated access
to it and so on so that whole issue of
how to integrate the cold storage
management into it is an active area
research right now and the the solar
solutions are not clear they're probably
going to be multiple solutions but we'll
see how it works out in over the next
few years yeah so just like they point
out that you're Thomas talked about one
approach and we actually redid the the
five-minute real calculation for one of
our Paul and I did for four work that
recently appeared in ic de and found
roughly the same numbers as you did this
is an active at this Paul said an active
area of research in the database
community and all the major systems
projects have it h door hyper and a coat
on as well there is one thing that
happens when you when you
or store data on a secondary storage
device which which changes things and
perhaps substantially you tend to have
to go to an async programming model
where we're in fact now threads can wait
and things can block and more things can
be active at the same time and and this
is this is sort of the complexities that
you avoid well I strictly a main memory
approach but what people are as we
indicated here people are in fact
looking at this and so maybe maybe the
best comment is to say is to stay tuned
so what is the mechanism typically used
in the memory database for resistancy
and for torrents so you mentioned you
use a duplication is a typical way to do
that oh you use some fast secondary
storage to to maintain the position C
and for tolerance well I can say how we
do it in inherit on honest to guarantee
sort of persistence in a single machine
we use logging traditional logging be
right out to log and then we have a
novel check Ponzi scheme to to speed up
recovery but it's also integrated fully
with sequel servers high availability
but it basically maintains multiple
secondary copies and any one of those
can take over if the main fails so
that's those basically the approaches
that are currently being used for the
locking mechanism so I are logging into
the secondary disco you're logging in
the memory so if the node called failure
so what happens no we are we are logging
we do log in to guarantee persistence of
the data because we can lose the memory
image at any point in time which means
that we have to log to some kind of
persistent medium which today would
probably be flash because we need the
rates that flash can provide and there
are other options there are products out
there that will do k safety we're in an
in-memory transaction they will send
that update to K other sites and it'll
be in memory and so you can
numbers and calculate how how much a
what the risk is of losing the data
people are playing around with battery
back ramp for fast log storage and
startups in the bay area for problems
like this but the thing I want to
mention it's just not main memory right
I think the fundamental hardware is
changing in such a way we focused a lot
on main memory systems over here but you
can start to envision systems which
maybe if you're doing analytic workloads
for example in between we really don't
need a main memory system for that type
of stuff I just need processing coming
out at the speed at which the bus can
deliver and it may be that is sitting
inside a flash storage and my future
appliance looks like slow spinning hard
day's hard disks lost of them being able
to feed the processing thing at scale or
flash storage that just feeds directly
into the process may not be a high
memory system right I think we care
about joules per query and a sustainable
10-year path to keep up with the data
growth rate and there may be very
different architectures and what we see
today so it's very interesting to
explore that appliance design with that
goal in mind there is a community more
or less consensus of building replicas
for high availability as opposed to
doing fast recovery and and there are a
number of number of techniques that
people use the details are different but
most of it involves all glogs
replicating the log and shipping the log
and having re executed on secondaries on
one form or another I are not from an
estate so I wanted to touch upon
something that jignesh just mentioned as
well so and actually most of the
architecture so far that we've been
talking about is main memory followed up
by SSDs as a trend I like to spend some
time trying to envision what I can buy
on newegg for you know a dream system of
mine and the latest configuration that I
built was twenty five thousand dollars
for a terabyte of memory across I think
three machines which is fairly commodity
and in the fact that you know you can
actually do this on a single credit card
if you wanted to is kind of cool and so
you are you are have a very good credit
rate
sure dave is looking at his credit does
already maxed it out what's that number
again so the way I see it is that if
you're reaching this level of commodity
where we can buy hardware at this level
distributed query execution becomes an
interesting possibility there are lovely
data structures that we're talking about
so how about thinking beyond and you
know just 11 core or one node sorry and
then I'm going beyond that and and I
understand that okay ethernet might not
be the solution to this so there might
be some other hardware requirements
maybe throw in some InfiniBand which is
a few hundred dollars but it does get us
to the terabyte of memory threshold
which is I think a good significant I
think the key part is commoditization of
that high-performance hard hardware the
networking level which is getting there
not quite there in fact for stuff like
you're saying you don't even need to buy
the machines you can go and rent hi
instance in instances on amazon and the
networking maybe a little bit slower a
lot slower but you can get if you wanted
single box high memory instances you can
pay as you go right amortized that be
the reason was essentially multi-node so
multi-node reaching that level of memory
at especially easy to the variability is
crazy enough that Franklin smiling there
because I think he doesn't buy hardware
anymore he just rented out on I mean I I
would not like to but the network
variability on cloud right now is
something that I cannot guarantee
especially if there is just Ethernet and
ya know I work for Terra dinner for two
years they make a lot of money by
selling really essentially what is a
fast hardware interconnect with
commodity process Thomas Norman has
wanted a comment yeah I want to mention
but you should realize it that you can
get the turbine on a single machine for
nearly the same price and it's not much
more expensive and of course depends on
what you do if you're right just running
analytically query scale out is not an
issue if you're running or ATP scalar
does an extreme pain and you can think
about how many people have more than a
terabyte of active or DP data not many i
would guess so because it could be for
example it could be
it says we look at this what we looked
at was concentrating the updates on one
node and then scaling out to differ
notes for clear processing this makes
perfect sense as you try to centralize
the idea or DP transactions because this
video transaction an absolute pain yeah
Thomas makes a really good point which
is which is that that what what happens
in systems is you end up partitioning
them if you have to scale them over
multiple machines and and the more you
can run on a single machine the less
hardship you incur by doing partitioning
commercially this is definitely the way
it's going that that you offload a lot
of the the reed work mode on the
secondary copies and you had the update
activity on on your primary node that's
a lot easier then actually trying to
distribute update work load across oltp
update workloads across the cluster of
machines it is a royal pain by that so
actually I want to push back on running
everything on a single machine the
problem there is that the memory becomes
larger and larger and the memory bus
actually is not increasing at the same
speed so if you look at the numbers if
you have you know you have 500 mega
gigabytes just scanning that memory it
will take 10 seconds so if you want
something very fast again you will
depend on your computation you'll be
better off actually to distribute this
500 megabytes gigabytes say in on 10
machines 50k bites on each sorry can I
interrupt you we have no probably agree
with you completely we are talking about
all up in analytical work loads because
those can be distributed we were talking
about oltp workloads those are really a
royal pain to distribute across multiple
machines so all up wherefore we agree
with you okay we have one more question
back there because which I think will
have to be the last one because we're
already a bit over time I can also sit
down but I'm and so Intel's Haswell
supports hardware transactional memory
now and it's it will be available
hopefully soon not soon enough in server
sizes that are interesting to people in
this room so there was a whole lot of
interesting software transactional
memory style thinking in the BW tree we
just heard about so does HTM make you
say rats I wasted a lot of work and i'm
going out to throw it all out does it
make you say heck yeah I did something
that's going to be better than what HTM
can do anyway or does it make you say
whoo I think we've already laid the
right foundation for taking advantage of
that and that's going to make us even
better be well the answer is yeah I mean
we've looked at hardware transactional
memory and essentially porting to bw3 to
transactional memory certainly creating
these types of data structures gets much
easier conceptually wait if you have
that crutch we'll have to see how well
it performs I mean the from a commercial
point of view there are issues is first
of all we don't know how well in those
transactional hardware transactional
memory actually works we just don't know
we'll have to see the other part is well
unfortunately the world is not on the
running harsh world machines as well
sockets everywhere there's going to be
lots of machines for a long long time
that don't have hard with transactional
memory and I was also got a run on all
those machines as well third thing is
does it give us sufficient gain compared
for example with the lock free data
structures that were using in hanging on
that it's worthwhile doing it or not and
we don't know that any of that yet so
we'll have to see but I think we're
going to have to wrap up now I want to
thank you all for attending and I want
to particularly thank the speakers today
who did a fine job in exploring what is
in fact a very large space of database
systems on new hard way thank you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>