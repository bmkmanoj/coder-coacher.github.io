<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>mSDA: A fast and easy-to-use way to improve bag-of-words features | Coder Coacher - Coaching Coders</title><meta content="mSDA: A fast and easy-to-use way to improve bag-of-words features - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>mSDA: A fast and easy-to-use way to improve bag-of-words features</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jrLVGdVNx7A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so thanks for coming to the talk
this morning we'll hear from Killian
Weinberger um
formally at Yahoo and now at Washington
University in st. Louis
Killians an expert on metric learning
and transfer learning multitask learning
he's also interested in brain decoding
problems and this morning he's going to
tell us about something a way of
improving bag of words features one of
his co-authors min min Chen is also in
the back and is an almost intern the
summer and just a bonus question for the
audience in that in the in the subtitle
of the talk you will notice that Killian
has a middle name that starts with Q
that is apparently not his secret agent
code word handle it is a real Bavarian
middle name free lunch to whoever can
guess what it is
thanks Alice good luck hey so thanks
very much for inviting me and a lot of
fun to be here and actually I recognize
a lot of people in the audience I'm
shooting actually some people just took
my course at Wash U so I'm talking today
about MSD a marginalized stacked
annoying autoencoders it's an algorithm
to improve bag-of-words features and
this is joint work with min min Chen who
was my student who's you know on the
back here lady with a purple sweater and
my student Eddy and facia who's actually
also coming to Microsoft I think for
next month or something okay my talk
actually is there a way to turn down the
line a little bit or is that gonna screw
up the video because my slides are all
black that's gonna screw up the video
okay so my talk and the 90% of my talk
will be on modernized technology
auto-encoders Oh
all right well at the a let's keep it on
the trolls will be presented at ICML in
two weeks and then I will give a very
sneak preview at the end mostly just
because you know there might be some
people who get Microsoft in particular
and they might be interested in this
where I would like to talk to about cost
sensitive training in that that's also
paper that we have it ICML in two weeks
okay so let me get started yeah just a
quick review on bag of word features so
you know probably most people are
familiar with bag of words but just as a
reminder like your word is basically a
way to represent text documents and
machine learning so basically let's say
you have a text document here this is a
product review and basically what you do
is you take the words in their text and
you represent them as a vector and the
way you do this is you take the text
then you have some dictionary and the
dictionary Maps any word to a particular
entry in a vector like the dimension you
can also use a hashing function and then
you represent the entire text document
is basically a vector that has as many
dimensions as you have words in your
dictionary and each dimension tells you
how many times that word is present in a
new tag so for example here one that
means the word Kindle occurs exactly
once in this text and the word Nook
occurs twice etc so in this case baby
it's called bag of words because your
bases just throw in all the words in the
bank you the order of the bird air words
is lost
alright so this you know just machine
learning 101 in some sense and then you
know if you want to have a
classification problem or something you
take these you know bag of words the
vectors and you put them in um space
nuts to tell you train an SVM classifier
or something or whatever you want to do
with it this is used in many different
settings for example in image
classification where people use Hogg
features and sift features as interest
points and then these become you know I
use this bag of interest points or of
course in text classification let's take
the domain maybe I will focus on today
for example
classifying documents or webpages based
on topic or you know classifying product
reviews or blog entries you know based
on sentiments and that's actually the
you know kind of the example that I will
use throughout the talk right given any
review is it positive or negative but
you know everything applies to all of
these settings okay so let me just
illustrate you know why bag-of-words
vectors you know some sense to limited
here are three sentences you know a is
recently Obama signed an important will
be Sunday our president mentioned the
game-changing law these are probably
about the same topic these two sentences
and see last Sunday Manchester United
Football team and Britain won a game I
mentioned and so you know clearly if I
asked you which two sentences are more
similar about the same topic a and B are
similar and C doesn't really have
anything to do with these two but if you
look at the bag of words representation
you know that actually turns out that B
and C have a lot of words in common so
you know that if you take the inner
product between these two vectors you
know they might actually appear pretty
similar whereas a and B actually don't
have any words in common and why is this
well they talk about the same things but
they actually use different words so
here for example Obama and B says
president if you have important and
game-changing leave you have bill and
law so these are actually you know
various you know similar concepts but
because we use different words you know
in our representation this is completely
lost and it looks like these two are
very very dissimilar so just sum this up
so bag of word vectors basically very
often your problem that your vector said
your vectors are just too sparse you
know too little about the text document
and you might just have very little
overlap between documents so for example
you know you know just because you use
different birds you know there's no
overlap between two documents you can't
tell at all you know if they're similar
or dissimilar and particular if you
train data is small then one thing gonna
observe that you know if you use words
that are not that common in fact
actually in the English language you
know the majority or almost over its are
very rare because they follow SIPPs law
it's a power law distribution so if you
use rare words then you know you only
see the once or twice in your labeled
data and so the classify doesn't really
know what weights to put on them
and you know the main problem here is
basically that bag of words just capture
the word by themself but not the meaning
of the word so if I use President or
Obama Obama these are two different
things
two different entities it does not you
know there's nothing in their
representation that tells me these two
actually the same thing and therefore
two documents might be similar if they
use these two words and so usually these
you know none of these problems are
really too bad if you have infinite
amount of data but you never have that
especially our labeled data is usually
very limited you have a small amount of
labeled data and then you know these
problems are pretty severe so what we're
proposing is to but we can't really do
much about the little labeled data but
we assume that we have unlabeled data so
you can get this basic justice Emmie's
request setting you have unlabeled data
from the same kind of distribution or a
similar distribution and the use
unlabeled data to learn a representation
of the documents and then take the label
data and learn our classifier
that's the two-step approach that'd be a
proposal and so later on we'll also show
this for a domain adaptation where the
unlabeled data actually can be from a
different domain so we can say well you
might not have an unlabeled data from
the from the problem that we're
interested in but we have a similar
problem that actually has unlabeled data
and we just use that and that you know
might still work okay any questions all
right so our algorithms called
marginalized IQ noising auto-encoder and
i will explain in a few slides where
that name comes from and why that makes
sense and this probably the most
important slide is the basic building
block of our algorithm is a linear de
noising auto-encoder
and so let's say we have some text
documents as in our one our examples in
our unlabeled data set and this here is
the c has the text document some product
review of some book at Amazon and this
here is the bag of words representation
so this here is a vector and yellow here
means that basically this word you know
might be present and but why it means
it's not present this is a zero and so
each word here each entry here in this
bag of word vector corresponds to some
dictionary entry this might be the word
favorite and this might be the word best
written etc all right so what do we do
the first thing we do is we take our
text
and we duplicate it and let me take the
duplicated version and be corrupted how
do we corrupt it the goes through it and
we take a look at every single word and
we remove it with probability P so you
easily roll the die for every single
word and you know if it's the right
thing comes up then we just remove it
and just blank it out and the intuition
is that well if you understand your
domain right then even if you remove
some of the words you should still make
sense be able to make sense of the
document so for example here I read this
to you I read two to three blank a week
right well if you look at the look at
the stacks of the blank you can probably
figure out that this should be books
just because you understand the domain
pretty well and so that's our tuition we
basically take the corrupted version and
from this corrupted version of the text
you're trying to reconstruct the
uncorrupted version and so in some sense
what we're doing here is saying well be
simulating the test case in the test
case you know what are the hard examples
during test times are the ones they're
basically important words about the
domain I'm missing write something
Arthur used some different words if you
don't know right and now we don't know
what you know the classifier hasn't
trained on those words right so if
you're simulating this by taking our
train data and just removing words
randomly and now basically making harder
test cases out of it and all right so
how does our algorithm work EBC take
this corrupted version and this is the
bag of word vector of the corrupted text
here which is strictly sparse so it's
strictly more zeros in it because we
removed them words and we try to
reconstruct the original text document
and we do this with the linear mappings
I mean we just take the corrupted
version here and we try to minimize the
square loss between the reconstruction
and the original document any questions
sorry so W is lazy popular they're just
learning this name mapping mapping W so
we have the original document we
corrupted we try to find some W that
basically you know if you if we map this
corrupted version with with this linear
mapping and these two are very similar
we just make it a square matrix yeah I
get that
absolutely you're right good point
substitutions
because it's really simple so that you
know everything Minh definitely do
fancier stuff but you will see later on
that basically if you just do this
deletion then actually turns out that we
can in some sense is the noise model
that we can handle very nicely later on
yes and actually one thing I mean one
thing that's actually you can definitely
do this and actually have a you know
have different kind of probabilities for
every single word in some sense we try
to keep it simple and just have mom
probability but you can definitely do
this
actually there's no reason not yet very
good point they actually really remove
it from the bag of words yes absolutely
but that's because of the same word
appears several times read remove it in
all locations who removed the entire pin
so we just said it is you right and so
and what what this mapping what he does
basically it it you know it learns to
reconstruct from other words that
co-occur right so basically it would
learn that president you know career
occurs together with White House you
know an Obama or something then later on
a few just the president but will do it
will basically assume well you maybe you
removed White House and Obama and it
will start hallucinating those words and
adding them to your representation
that's kind of the idea so it makes the
representation richer yeah
look at my student and they use to try
to that's that's that's exactly perfect
yes thank you this aspect annoys me
auto-encoder
and we have the marginalize
technological encoder that's exactly
what our work is building on absolutely
and I will have a slide that baby puts
those next to each other yeah whether
you do illusion or you know like it
might be
would you say they might be errors which
are might be worse to Mesa were than
such as to word oh I see then the
symmetrical
and not entirely showed mean so did you
mean that different words might be
definitely reduce your work instruction
prediction yeah and you can either
pretty to work there or the opening like
reduce the weight of the word which
sector you how I seen I see you saying
yes about the same type of we stayed
right hey that's that's true that's true
so yeah we just we just handlers on
square loss let me just keep it simple
that sense yeah
yeah that's right
yeah that's right okay all right let's
start avoid that in some sense
absolutely right and and you can view
this you know just a different spin on
this it would be that you basically say
or people and actually you can talk
about this maybe more offline because it
goes a little bit more into depth but in
some sense that you know proposing a
little bit of different noise model they
basically say well you know people use a
lot of Gaussian noise for example
etcetera but in text documents right if
you have you know words full of power
law distribution so a lot of words
either present are not there or not
right they only appear once or twice in
the document right so a good way to
model it in some sense is to say well
they might either be there or you know
sometimes you're saying while doing the
in the test case you have some documents
and you know some words are just removed
and that's why they are not there does
that make sense they get it's it's some
sense an approximation of basically you
know off of the tail of the power law
distribution
yeah yeah not nearly enough okay good
and let me just move on to a few aside
this is great okay so this you basically
minimizing this this objective here we
basically learned this in matrix W to go
from the corrupted version to the non
corrupted version and this is just a
square loss regression right says
ordinary least squares so ordinary least
squares you know of course very nicely
behaves as a convex function and in fact
there's a closed form solution so we
just have a little matrix inversion here
and let me jump right to the minimum of
the function right so this pops out in
closed form and well the one thing I've
done in the previous slide I just you
know took the text document and they
made one corrupted version of it and of
course that's you know that's only you
know if I would do this again I would
get a different corrupted version
because I moved every document with
probability P so instead of just having
one corrupted version it would actually
be better to be kind of you know to tree
my classifier with many different copies
of corrupted versions so let's say we
have M different corrupted versions
right so we do this m time for every
single text document in my training data
I unsupervised data I corrupted M times
and then I can actually do exactly the
same thing so now I want to learn a
mapping that does you know reconstructs
the original text document well across
all of these corruptions and that's the
optimization problem that you get is
exactly the same thing except that the
average over all am corruptions so
across all of them you should do well
and turns out well you know it's not
surprising there's also a closed form
solution and actually this very
analogous it's basically just the
average here you basically average the
covariance matrix of these we average
these outer product matrices there are
other products of vectors okay
and so one questions house how large
should we set em and in this case and
sometimes that's iterating over the data
set but in this case we are not
overfitting right there's no we're
fitting because we're not having not
using the label anywhere so the larger m
the more robust we get against this kind
of noise right the more examples we see
that are corrupted so ideally you would
like to make em as large as possible so
DeeDee would like to make em in fact go
to infinity and in fact that's exactly
what we can do we can just
that M go to infinity and in the limit
these terms here actually just become
the expected values of the outer
products so n goes to infinity and we
can just stick that into you know our
closed form solution and turns out these
expected value is actually really easy
to compute and why is this because our
noise model and that answers your
question that you asked for earlier why
did we do this simple noise model
because actually if you just remove
every word with probability P that's
just a binomial distribution so this
year we can just compute in in Khost
form is just the scatter matrix
multiplied by the probability that every
feature survives the corruption that's
all there is that's the expected value
and turns out it so if you want to code
this up it's actually it's really pretty
straightforward the whole thing in
MATLAB is just ten lines of code so this
is the actual code that we use ten
months pretty tough thanks we're that
good so I said oh you didn't see the
average over all possible sums over
these squares and so the here the
average of all the all the possible
corruptions oh oh oh I see so he have
something about I just wanted some
summing over all my entire course
relationship so yeah that's right and
that's a sometimes you can imagine bass
if you get you take the entire corpus
and you replicate at em times and you
corrupt it should be taking you should
be taking inverse you could be taking at
the top and eigenvectors ya know and it
is okay wait let me just see how much
warts are averaging over uh there's two
covariance matrices one which is your
trade a maximum yeah yeah but it's
actually this is half this is basically
this is fully corrupted but this year's
can over scatter matrix of the
uncorrupted version and the cropland
there's kind of a mix between these two
does that well if I just big all of my
doctor data and I dissident PC on it
they are actually construct my day
that's a whole and it's quite different
right so basic you you finally the
veterans to basically in some sense
you're projecting onto in two directions
of maximum variance right which is not
what we're doing here and some obviously
minimize the variance overeager truck
so yeah I didn't I don't know what
happening but linear discriminant
analysis always goes to you you can
always take zero one targets and boil it
down into
the corruption will be different this is
a very special chemical maybe yes yeah
you're not either corrupting it no OPC
Oh No okay so yeah I know you can talk
about this I don't know I don't know no
no get one 1w4 all samples we summing
over all samples yeah okay any more
questions so they can you know compete
this in 10 lines of MATLAB and so here
someone asked earlier there was a paper
last year I see mail and it does
something similar well that's actually
the paper that we based in some sense
actually the paper that we have it I see
mail this year is actually what we show
that we can take their version and make
it much much faster so what what you
were referring to is paper by your
banjee Oh screw up in Montreal like
Laura Lord at all and they're basically
they were the ones who actually you know
inspired about this world they had this
idea of this corruption corrupting data
input vectors and then reconstructing
them so they base it took text documents
and randomly removed
you know Ruth buckets with probability P
exactly the same noise model that we use
and then between the newer network to
reconstruct the original and the
original bag of word vector and the new
network had a hidden layer that's over
complete and they trained that with back
propagation and that gets really really
nice results and that's kind of actually
what started our work that this is
amazing to have an encoder here in a
decoder and this is their loss function
the only problem is that they see this
is use back propagation and you have to
iterate over the data if aider training
said many times and our idea was
basically how can you make this possibly
faster and so ideal spacey why we remove
the hidden layer in the middle and this
way we can make this linear and then
instead of going over the data set many
times and corrupting and over and over
again they can actually marginalize out
the noise and do the whole thing in
closed form so that's yeah
so the so the in are you talking about
this money right the SDA based it is so
the reason why they get over complete is
because of the corruption if it wasn't
corrupt an apostrophe trivial right but
because of the corruption they can get
away of making this there's actually
over complete they only use daily use
and this is actually going back to what
Lynn said earlier is they only use the
five thousand dimensional at five
thousand words takes another problem
Reese doesn't scales another high
dimensions because back publisher is
really slow right and they they have you
maybe
then you better find a good detective
periodically when you do these
autoencoders it does you might as well
just remove the nominee or you do
nothing for two million years yes and so
I guess that that's that's what we are
showing are they actually getting
similar results the Linnaean only the
only thing not much good if you do it
girl since anyway unless want to use the
natural languages
the net okay
yeah yeah let me just move on so
basically this is but this is related
and the reason why we call it
marginalize technology an autoencoder is
because we if the kind of our algorithm
boss was inspired by the spec you know
he marginalizing okay so what one thing
is because they're training it on the
anthropoids corpus that's usually a lot
larger right so there's many one
document then once you learn learn the
W's that when you see that one word you
generate the entire you might and in
those cases you might run a little bit
into ponds and actually let me address
that later on yeah well so you actually
do a little bit of memorization
here's that there's a little Ridge
regression there yeah so I put that
under the rug a little bit and the
vegetation but at the same time so every
time so ten to the minus five it's the
main you turn it on just that it's not
as busy well-defined
yeah
oh it's an imaginary we use cross
attributes for the science might be 11
yeah you use cross attributes instead of
AC actually amendment would now is that
somebody
okay so I feel it's different like for
me the symmetric case since you do only
condition then there's a lot more some
of these to us I agree
but bear with me for team effect we can
also talk about sulfide an exact
relationship reduce driving okay so one
thing that's nice about DMF though is
that they can be stocks take it and make
it deep right so that's that's in some
sense their claim is that they approach
to deep learning is the stacked you
noising or encoder that's the you know
the montreal version of deep learning
and so you're basically here you have
another hidden layer and then you have
another hidden layer and so on they make
this five six you know layers deep so
what we can do the same thing with our
method and so let's say you have alright
input X and we learn a matrix W and we
apply W onto X and we get some hidden we
get our output here in some sense it's
basically the reconstruction you know
reconstruction a sense that basically
take our X C and we hallucinate new
words to it right and then what we do is
we apply a non-linearity to it so we
basically take a squashing function some
sigmoid function that basically squashes
these these outputs here and this is now
in some sense the output of our
algorithm and now they can use that as
the input of the algorithm against we
can apply it again and so this in some
sense we call this layer one we can now
use the output of layer one as the input
again of secularizing on area of an
autoencoder and get layer two etc so an
each one of these basically solved in
closed form so we just solve it in
closed form apply a sigmoid function and
then so if the next layer in closed form
a plastic might function in settlement
and let me take these input layers and
that's actually the same thing that they
see Josh of NGOs group does you take the
bag of reg word vectors and these hidden
layers and make those the new
representation of our data so instead of
just using the bag of red vector you
busy now have these hidden
representations as the bag of word
vector the first reconstruction the
second reconstruction and so on
that's our representation of our data
and now we trained svms on that data and
these here are basic ompletely learned
on the unsupervised
part of our corpus unlabeled part of our
corpus yeah
what you need to station so you have
their every scene no bike engine but I
need a good intuition and years ago to
church the what is this firstly I do
license sometimes it basically takes the
back of were directors and for every
word
it adds words that basically Corker with
that word right so for example let's say
I have the word you know Obama right it
might might add the words White House
President government or something right
it wouldn't add Clinton because Clinton
and Obama don't really occur together
right another Clinton okay that's wrong
clean bill clinton and but then if you
you know in the second layer right now
you have actually white house government
you know president right and secondly I
would actually add Bill Clinton to it
because that also occurs in the same
context so you can kind of view it as
actually Ronnie point that's nicely when
I talked with him about yesterday so you
can kind of view it as a graph they may
say what words are connected you know
corker with other words and you kind of
each each layer here takes one step in
that graph stationary how do you know
the scale of the signal it seems like I
mean if you multiply H but well I mean
your target search you can and I don't
think that important at least a reversal
are you trying
yeah actually amendment tried a whole
bunch of different smashing functions
and in fact even if you don't use the
sigmoid it doesn't make the sigmoid is a
good idea but it even if you don't use
it it it still improves to have multiple
layers because of that effect
forces varsity's and the purse we don't
go out the sparsity does not spokes is
not first but the graph goes into small
number of neighbors do suggest that you
are after parsing that you want to have
somewhere long straights
not really Melina's
then sometimes basically what you're
doing is you're exaggerating if a
participant enough that you actually you
know make it more binary yeah
integrating it out using every possible
word noise lyrics yes but then the w is
linear the table is in you I mean if you
introduce any apparel you might not be
able to be back there might not be s so
you might not be a perfect in
construction is that really saying there
might not be a transformation from age
five to X well there but you're just
removing right so basically just have to
represent if you remove a word right you
can just make it if it's a linear
community just have a linear combination
of other words to reconstruct any word
that you removed and so you move this
word here energy-efficient well in this
case is about negative force but you
could basically say well that's a linear
you know like for example there's a word
you know in the product review if it's
energy-efficient right that might appear
together with good you know but great or
something so you know you reconstruct
words from from those Corcoran birds so
kind of answer you
the reconstruction model is linear and
the loan model is totally random so I'm
not sure if that can go back to this all
the time but we don't have to write
something you just have to find a new
representation like maybe still you're
still taking the original input in our
representation so if you take our SVM
this style we're not corrupting this
maybe not corrupting a test example we
take a test example the way it is and we
take these basically additional you know
these hidden representations where you
basically added more words to it so be
filling out the sparsity of the back of
one but he didn't SED right another
presentation it's very dense
that's right that's right because I
think that I think W is and that's
exactly we don't PC that so PCA actually
finds the low representation of the
repeat that thing they'll allow you to
read in specs you don't actually have to
compute reconstruction you're computing
sort of the square root you want me to
continue the square root of W matrix to
convene that you're computing this is
equal to make me weeks explore things
with a direction every day you know what
I don't want to get too much into it
because I just have a couple of times I
do like this discussion a lot actually
that's great okay and so if you do this
whole stacking business and then
actually the whole thing is twenty-ninth
of MATLAB so what I'm trying to drive
homie is that it's really really easy to
use right if you've any kind of bag of
words model any kind of Battenberg data
this is the entire code that you need to
get this new representation and the only
parameters if you have here is buddy use
your input data P is based the
probability of removing a word an Ellis
number of layers that you stack so it's
a really really simple model and this
isn't big contrast to these new networks
that people use right but you basically
had a number of layers the step size
instead of a number of epochs you go
over the data and so on mean leaves on
her team
okay let me talk about some results so
the domain that I focused on was
actually domain adaptation in particular
actually because the paper by a sham NGO
previously an IC ml used exactly that
domain and so here that the task
basically is given a review a product
with you try to predict if it's positive
or negative so the sentiment of the text
and the data set that we have is we you
know actually John bit said created that
dataset in 2006 he scraped product
reviews from amazon.com and he said
everything has five reviews or four
views is positive and anything that has
fewer and sorry five or four stars is
positive everything has fewer stars its
negative and so given the text our goal
is to predict if it's a positive or
negative review and the way we do this
is we do domain adaptation so we now a
training data is from one domain so for
example could be you know electronics or
something and a testing is a different
domain so in this case actually had four
different domains so for example you
could train our classifier on book
reviews and be tested on a electronic
equipment or kitchen appliances and in
this kind of case in this kind of domain
annotation it's really really important
that you have a good representation of
the data so this is why this is a good
application a good good way to test our
algorithm in some sense and so if you
just do this naively that's how you just
do bag of words and we train a
classifier on book reviews you get a
test error of 13% and you take the same
classifier and apply it on kitchen
appliances there you get a test error of
24% so you're basically doubling your
error by going from one to the other and
that makes a lot of sense right because
the way you describe a good book is very
different from the way you describe a
good toaster right so you might for
example here you might say that book is
best written you know it's one of my
favorite books were favorite you might
use in both but eloquent story or you
would never say it toasters best written
or eloquent or something right
so for you know a coffee maker a toaster
you might say it's solidly constructed
or easy to program it so these are
things you would never use he said words
you would never use for a book so if I
train my classifier on book reviews the
classifier would have
no idea what to do with these words but
what we do is a basic assume that we
have you know unlabeled data from both
domains and we run our MSD a
representation learning algorithm over
both domains and get a joint
representation that we then map our data
into okay so here's the result so
basically B these are different domain
notation tasks so these are the same
means to be going from DVD to books
electronic to ebooks kitchen appliances
to books and so on so that's what you
train it on this is what you test it on
and what you see here is that transfer
loss transfer losses defined us the
error that you get from the transfer
minus the error that you get if you
would actually if you had actually state
within the domain if you had trained on
kitchen appliances and tested encryption
appliances there's a few trained on
books and so one thing is just this year
is basically just to compare this this
years be basically the error on bag of
words first this years on MST a and so
one thing you can see we compare against
a bunch of different algorithms this he
has the baseline he be FPC ASM I
mentioned PCA earlier so the blue line
here is PCA ever you busy project
everything into one common subspace then
you have couple subspaces paper by John
Blitzer and give a couple what baselines
and the red line years stas that's the
stack the noisy auto encoder that's the
newer Network the base he does the same
thing and MSD a is our dark red line
yeah and so one one two you can see here
I think I love those twelve charts and
ten me basically get this best results
yeah only two we there's this algorithm
does a little worse you have to be to be
fair though SDA was only one layer and
MST a was five layers and the reason is
there's only one layers because it's so
slow takes a really really long time to
train yes
what have hard examples do things like
ironing or reviewers were always super
hard or super soft and is there like is
there a notion of what the reviewer
correlation would be if you were to
relay Bolivia is it how was the National
noise that or the testament and wait am
i quite understanding what's the
question one what is there just what is
the error rate what is the point what is
the noise level and their label and the
other one is that what is besides the
better numbers is there anything you
think you're solving conceptually that
that you couldn't get before things like
what is irony or whether it's a bias of
their reviewers the answer to both
questions I don't know so the yeah I'm
not sure about the national error level
I'm sure there's some right and I mean
that some who use you just can't
possibly get right and in terms of I'm
not sure if there's some so though it
just really helps you with making use of
words basically and really what it does
it kind of connects words from the
source domain to the target to me that's
really what it does so if you have a
review that before and was written with
a lot of words that you would never use
in books you couldn't get that right
before because you just don't know what
these words mean and now you can
the tire core personally yeah that's
actually the same MSDS that's the entire
purpose like a kind of protest to not
eat more
and none of these methods do that oh so
it's okay to just train you I don't
think there's a benefit for these
methods to only use one class I think it
always helps to throw in more data so I
think that that's at least what we got
empirically yeah so it seems like you're
getting the negative number on this
so there's that right I can tell you
exactly whether this and there's also
Katie became their kind of traumatic
pairs here their friends so the the
reason why that seems odd right like the
Train indeed you do better you know that
and the reason is that you get this he
has some bag of words right this he has
the baseline a bag of words because this
is the baseline of MSD a so just does
that make sense
well electronics and so I actually I
don't know too much about the individual
domains I mean could have been to what
words to use to describe a DVD but words
to users grab a book
so maybe electron
like that MSD a composed two books I
would have done better actually view the
GMC and the holdings so they took all
the reviews that we have and then one
representation for everything and then
we just train the classifier yeah so the
basis there you know
yeah goes representation changed right
decimated summer is right
because he'd fight winners so the left
hand one actually has much more passive
so that then that explain why you get
negative three so let's explain why
negatives are but yeah if I give you
that I give you the do that afterwards
did you look at the domains it has
roughly the same size they all have
about 2,000 I think but I don't know
what he knows it
in how hold large is your vocabulary
logically trying to
so this adhesive is not only 5000 versus
the 5,000 members
Zebulon x1 as at the Asian and alien
shape of a class primitive pergolas
25,000 simulates in early 2011 samples
yeah but it's regular meister SVM's ofme
dude ESPN we do you know cross
validation over the regularization
and good question I don't know you would
get something I'm sure
always have a grad student in your talk
anything that works so you said the sta
even though man
or if they bought the same order and not
significantly better within the limits
when you transfer it
no I mean the lemons are still within
the same domain you just classified
actually by givin up the middle X that's
a few more sides when its builders-- you
right means better than back upward
that's why I told us to be good servants
but ya make sensitive negative not good
yeah so I think the most the most
important thing though is we look at
speed and so now I'm gonna look and
actually average results across all of
these different adaptation tasks and
then average them actually we do
transfer ratio so they actually divide
the two because otherwise is dominated
by like these small ones have washed out
by the large ones so anyway that's just
transfer rate is the same thing that
actually been Gio's group does so and
here's the classification result that is
the transfer ratio so Louis better and
this he has the time in log scale and so
one thing it sees that all these other
baseline survey see bag-of-words SCL and
we have coda and then s da that's in
some sense like over the years basically
i didn't have the year when these are
published but basically you know the
kind of goes in that order
so you know the results got better and
better but also the time that it was
took to train these classifiers who
exponentially write an SDA now it's now
at five hours on that dataset and what
we managed to do is basically push the
results of the SDA which are really
awesome you know to the left they raise
to say but we get the same results as
SDA representation but instead of five
hours of training we just have a few
seconds right so there's like two it was
two minutes of training in five layers
and so here here's some words just to
illustrate this little bit what gets
hallucinate it Soviet visit to a
document that only has one word the only
the word great and so here's kind of the
document that's that's generated from
this and these are words are basically
in order of their strength of how in
other their regenerate so we have great
is great highly highly government
excellent perfect fantastic waste year
two great ways and so here's a bad one
bad reconstruct dead worse sorry
you know please the worst a bad hope
horrible and so on right
yeah 20,000
I think that's right there so that's a
bit of the last year Network 2020
I was in some
yeah this is what the report is to us
what they hope they must use punch cards
or something so you know but nobody crap
abilities little bit be corrupt over and
over again so you have many iterations
because you keep up all right make it
two hours to compare against two minutes
I mean even is now actually use their
code we actually have a 340 thousand
data points that's actually the large
data set that John created back then and
nobody really used because it was too
large for most algorithms so we ran it
over SDA and we took two days
make it six hours we can do it in six to
twenty minutes depend on many layers
yeah and you get the same result
yeah I don't know that might be amazing
speeding up so thanks T they have a code
that actually also puts it on on a CUDA
graphics card but that that doesn't help
in this case yeah
I don't know it might be use when when
you do sterile package we know the well
just here I just I just agree that MSDS
to be fasting
ouch
just that I'm surprised by the ISTE
yeah
like maybe
so they don't really stop it they don't
ever early step this way they can make
it three times faster but that's still
you know that's still document
artificially create a document that only
has one entry in it only one word that's
this one here just bad and then you run
it to MSD a and you look what is the
document that I get at the end like what
are the words that basically
reconstructs and Sobeys are basing the
word said that like if i take put him
bad the most you know the strongest word
that he constructs is dead death by
Graham's tattoo man just like the way
John created the data set in 2006 was
just with diagrams too so we just used
it the same way you didn't yeah thank
you yeah so okay so far I've been you
know I kind of put something a little
bug little it has you know Rosa
mentioned earlier on that we have this
matrix inversion and that's a d cubed
operation and these number of words in
my vocabulary right and so so for all of
these results actually done with five
thousand words but is the same that
actually the SD a paper uses but that's
a little bit lame right because text
documents usually have much much higher
vocabulary a much larger vocabulary so
what if you have very high dimensional
data and and then actually so here's
kind of what we do and it's based on
intuition so the intuition basically is
if you have very large vocabulary right
that's if you use hundred thousand words
then most words in you know you know in
the English language you can get pretty
far by just using five or ten thousand
words right so the other ninety thousand
words or in some sense words that
describe some concept that already
exists in the first ten thousand words
most of the time right so you might have
a word tasty right but instead it can be
fancy you can say delicious right
that's a rare word but it really says
the same thing that's tasty for all
means and purposes all right in terms of
our classification so what do we do the
busy take this really
vector and we can't make the inversely a
hundred thousand a hundred thousand
right so we just randomly divide it up
into chunks and let me take the five
thousand or whatever ten thousand most
common words and mean some since we do
that we learn MSD a transformation for
each one of these chunks so what are we
doing here and sometimes we're trying to
reconstruct common words from rare words
and so you basically say well if there's
delicious try to reconstruct tasty for
example or something like this so be
translating you know big words or
something into common language yeah
like yes so in this case we have fibers
yes I want one reason you know five
grams is here it's a good question I
don't know but I would I would actually
think it's the Meister work but I don't
know the take and no it's not the same
it's not you thought about this the
differences are fine
and so then xD one thing you do is you
can then know the stacking now we have a
mapping into the low dimensional space
and I'm yeah and this is about the
algebra here but it's straightforward
you busy just add up all these these
reconstructions into one reconstruction
and now you can just stack it just the
way before all right so now actually the
subsequent layers are now in the low
dimensional space and so we tried this
here's the the data set the Amazon
review dataset and the tried at five
thousand dimensions 10,000 20,000 30,000
I think this is 40,000 and this here's
the SDA curve this is the result that we
get with the originals technology auto
encoder and basically as you know one
one thing is very clear trend as you
increase the dimensionality the error
goes down so it really helps the error
and our MS da basically matches this
very very nicely right so for every
point here there's a parallel point here
you know that's just much much faster
but that gets roughly the same same
accuracy and so actually at some point
at some point you start overfitting if
you get 40,000 you just include really
bad words yeah it actually does and now
it also doesn't happen this actually
doesn't rectangle
you seem easily happen item is important
and then hidden layers and she then just
trying to help but what I'm saying is
what the SDA like tries to compute its
gradient does it actually reconstruct
every output or just so we do the same
thing that the only reconstructed the
top M or whatever top k most common
words so we don't reconstruct everything
in the SDA paper where they came
very large sparse binary automotive
problems they can approximate the
gradient very very quickly
all right a little try to ascertain and
be probably everything that I'm not that
I don't know I actually have enough but
it probably will use exactly the
implementation okay and basically you
have you have an optimizer which work in
a specific setting which is this small
dimensional target and MSE loss and you
use Sdn this exact same condition right
why SDA the additional flexibility that
you can switch to MSE four percent of
the end
and use a large target space yeah maybe
that helps I mean you still yeah that's
definite you so in some sense we're
locked in and you know because of our
little tricks right that makes us really
efficient we are locked into exactly
that formulation right because they can
change things around much more flexible
absolutely feasible what you use to
actually yes I don't well maybe I do
okay I can show to you the I think it's
at the end of the talk somewhere so it's
very little but yes we did this okay
and so here's also someone I think Leo
asked a question earlier on but if he
just in domain so this is just that
space is just semi-supervised learning
if we just stay within the domain so he
also did semi-supervised learning
experiments CSD mas and he has reuters
data set and so here we also compared
against LS I was basically PCA and
latent Ashley allocation but David Bligh
and tf-idf and so here basically here's
the graph there's the accuracy so here
higher is better and this is as you
increase the number of trained DSS so
and MST nicely nicely on top and a month
you can see I guess here here you can
you know the benefit is more pronounced
when you have little train data which
makes sense then as you keep getting
larger at some point actually here
there's not that much benefit here's
some benefit on these still email to
7000 data points
and notice I specifically is that
their tricks you can use malasada that
you see blog TMI
yeah so we didn't do that actually but
there's also a lot of tricks that you
can do the MSDN but you can actually for
example you know the optimization
problem you can actually also put on
what waits for the different you know
add weights for the different words
right
for example IDF score and we have some
result in this that I actually that's -
I don't know I don't know what they are
so in our case is just TF right so we
just reconstructing the TF score but for
the other algorithms and the comparison
early on they use tf-idf or whatever
work best for those now if you just
kissed yet so immense documents have
basically more optimization
meant as an SDR is waiting more for long
documents with Thank You hours per
person that's right but then these cases
don't marry too much that we nothing
else EHR it's always zero it's just that
we think the variance is the same as our
shift it's I think it's I'm saying cuz
old target of one has an error in a
target of zero innocent it's not like
it's the same guy as the same applause
Evan is more there's more words okay so
they're the covariances yeah and this is
oh by the way I didn't mention the
optimization be able to have a a
constant term in optimization you know
square loss and I don't know if that
matters in this course
much more interesting
ghost notes and that's exactly our
listeners the solution notes are think
about it is nice because if you have the
world that's very rare
just like you came to the same
realization you know is just something
about mitosis works nice yes they let
you 2000 is that if you did blogged here
in your reconstruction of your Gaussian
see it actually better echo p ha
yeah I might be what find out yet so on
the MSCI you stop the original in this
experiments of the original work still
or you did just MSD a representation yes
maybe actually we add the original
representation and yes yes and we
actually tried both for those and the
old of these we cross validated all the
parameters know what I think it might
have actually not I thought be yeah I
don't think it but by a DA I think it
didn't know that's one of you this
through squeezes that's right inside of
either cross mine age for those
okay let's see the last slide and so in
summary I talked about marginalize
technology hard encoder the you know the
marginalized stands for me busy
marginalize out the corruption and we
keep the high you know high accuracy of
the sta features really feature
generation right so it depends on what
algorithm to use afterwards but the the
nice features the SDA creates just much
much faster and that's because this
layer wise convex and there's a layer of
ice a closed-form solution and you know
one thing and I hope people I know who
works with bag of words features might
just experiment with it it's really
really easy to implement actually min
min is here so you can just ask her for
the code and you know it might just
might just lead to better results in
different applications I always have a
second part to talk but I feel like
we've already talked a lot and you know
there's been a lot of discussion so
maybe I just leave that and if someone
wants to talk with me there's another
ICML paper we have that's about
cost-sensitive learning when you assign
costs to features I don't know
unless someone really wants to hear it I
think it's a spin you know I've always
talked for an hour so maybe I just skip
this you wanna hear it I say okay so you
have one pot so you have this deletion
procedure to generate extra data
fairly standard technique use with
learning methods like when you're going
to image work for example because I
think the virtual whistle and yeah this
kind of loosen a ting the examples
support by consetta so you're loosening
some examples and then you also this
cost function japanese sense of what to
do if you sort of figure this you loosen
a lot of examples like you're doing and
had a beatitude of LBA those the spirit
dataset to lva or LSI or whatever yeah I
don't know that I don't know the it
might be very slow LDL assign was very
very people look at those results
because it across all via date of all
these different settings yeah okay but
then everything is a covariance so there
was an ad a earlier yeah Ellie a yes to
have to you know when he was online
still takes a while right it I don't
know so basically saying you know let's
take this noise model and just use the
other I wouldn't spit that noise mom
yeah whatever you know which you're
proposing two things the cost function
and the solution ation process and I
wonder well there the customs is not in
the cast yeah I see butane yeah I don't
know to be honest so one thing we've
tried and that's basically and follow-up
paper is that we basically said well
let's just take this noise model of
prescription and just put them into the
classifier that's an SVM or something
the amazing set up you know but it's a
scaled up slide if you have an l2
regularization that's really just
Gaussian noise that you assume yeah
let's through this corruption noise and
then you know busy derive the update
rules and stuff and that also helps like
so for text documents is this a lot
better than than Gaussian noise or
something so there's definitely some
benefit from you know the noise model
it's actually it's generally the case is
actually learning these representation
that you layer and then stick them into
s3 and that seems to be better than just
putting the noise model into your
classifier but I mean we're just at the
beginning of exploring that in some
sense yeah
does that that may have we use
cross-validation and that's the one one
parameter that it is sensitive so the
layers in some sense you know
it improves if you keep make it deeper
and there isn't really you know just
Peters off at some point so they can't
be counted to wrong but with the
probability P that actually depends on
the data set and so actually one thing
that was quite surprising is that we
found on some data sets that give us up
to ninety percent actually noise was the
best setting so nobody actually just
basically take everybody take a holdout
dataset then do apply MSD a on the chain
set the different and chain SVM on the
you know and then test on the holdout
set so so that takes a little bit but
actually this is pretty fast of it
because SPMS only takes a few minutes we
say something I say the best keys
percent but using the same
you know different ladies or something
as it is the best piece it's yeah that's
right XM is not surprisingly high visual
II my intuition would have been that it
should be pretty low because the 90% to
removing a lot of the document right
like basically have very little left but
because you're integrating out all
possible corruptions and maybe you get
away with having pretty high P it's not
always 90% though I could you know it
did vary that's so if you plotted the
curve actually in the curve there is a
nice trend like it kind of looks like a
bucket but it depends on the data set
where the best so yeah actually yes and
it actually makes perfect sense it is
slower because mas da can only go over
so many iterations of the train data set
right because they go over all possible
ones that's SDA you know you keep
corrupting your de Indias set in every
Airport you have a new new corruption
right so even if you do 100 hundred air
pox right you only have a hundred
corruptions of your data 3 Millions
Oh minutes yes which is hotter or
unbalanced case what you mean
specification so you have your very
stupid oh I think I think I don't think
it matters actually so the one thing I
should be did so because well your train
is completely unsupervised right so
labor doesn't really matter
yeah so I bet you that that she still
works there and so so one thing I should
be did so this is nicely by hi Ben David
and John Blitzer who actually it's um
you know they've proved some nice nice
downs on on domain adaptation and and
they have this metric crazy same
amenities should work if you and that
claim is well the you know that the
target and the source have to be
surprisingly it has to be a have to be
similar and the way they measure
similarity being targeted sources by
saying well we trained a classifier to
distinguish between the two and if you
can't really do this very well if you
can't distinguish between electronics
and DVDs then they're very similar and
therefore they help each other and so
one thing we show is actually that then
you run MSD a representation then
actually you're better at transfer but
you're also better at separating between
the two classes so it's actually you
know it actually helps in both cases
like these are very different
classification tasks right and it's just
it just seems to be a better
representation
yeah maybe maybe can course minivan
running you let me know
maybe I switch to the very last slide
just thank my co-authors so here I just
want to thank Eddie women and fair who
will be here and also a levy Chappelle
actually who helped with this and
questions it does actually one punch
graph that I promised here's the graph
that you asked earlier so if you take
basically there's a sixteen hours'
dimensional data set it's not so high
and he basically reduce the
dimensionality and this is the
computation time see you know here from
15,000 16,000 he sees as a big gap
because that's the the cubic scalability
of the inverse and so if you go down to
10,000 it's a very small hidden accuracy
but actually a drastic reduction in time
then 5,000 years it's noticeable at some
point you know you notice it because you
remove some some concepts
but okay my pretend Elsa words you'd say
a lot in ten thousand words right
naked 15 hours so that's that's
because any more questions alright thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>