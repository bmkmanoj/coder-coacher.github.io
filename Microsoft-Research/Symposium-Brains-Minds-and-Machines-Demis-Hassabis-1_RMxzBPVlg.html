<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Demis Hassabis | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Demis Hassabis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Demis Hassabis</b></h2><h5 class="post__date">2016-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1_RMxzBPVlg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let me start have the pleasure of
introducing very briefly Demi's as a
piece and this is continuing a tradition
that i started with christophe is a much
better known that i am and it was a
postdoc with me davis and and of course
is ideal for today because it's a
mixture of a neuroscientist and a
technologist and an entrepreneur next
week so it's great to be here okay won't
hear me here cool so so I'm going to
talk a little bit about because of the
symposium is about neuroscience and
brains and minds as well as machines
I've gotta focus sort of most of my talk
around systems neuroscience and how it
can be may be helpful towards Earl in
helping us progress in our quest for
building AI so I'm the CEO of deep mind
deep mine was founded in 2010 and then
we joined forces with Google in 2014 to
sort of accelerate progress towards our
mission of solving intelligence and one
way you can think about deep mind is a
kind of like an Apollo program effort
for AI so we have over 150 research
scientists now so i think it's the
largest collection anywhere of sort of
machine learning researchers on this
single topic but also one of the things
we try to do with deep mind is come up
with a new approach to organizing and
doing science so we've tried to combine
and take the best from the best academic
institutes and combine that with what's
best for you know that the greater sort
of Silicon Valley to style startups and
we try to kind of create a hybrid
environment that maximizes and it's been
sort of optimized for research progress
to try and make it as efficient and
productive and collaborative as possible
so what we do it deep mind is we're sort
of interested in algorithms we call them
general-purpose learning algorithms and
we're only really interesting algorithms
that can learn automatically for more
inputs or directly from walk experience
so not pre-programmed or handcrafted in
any way and we're also interested in
this notion of generality so the idea
that the set s aims to single system or
single set of algorithms can operate out
of the box across a wide range of tasks
and in fact this relates to our
operational definition if you like of
intelligence that we use a deep mind
which is we define it as the ability to
perform well across a wide range of
environments so while i agree with tommy
and i think i'm some of the other
speakers today about the intelligences
quite an amorphous term we found that
operationalizing it like this and
putting generality at the center of it
works very well for us and you could
argue well what about all these
different properties that doesn't cover
you know creativity cetera but actually
then it just depends on the tasks that
you include in the set of tasks that
you're going to try and attempt so the
wider and the more diverse the better so
we call this type of AI artificial
general intelligence or AGI and the
hallmark of this kind of these kinds of
algorithms is that they're flexible and
adaptive and perhaps even one day
inventive and they're built to deal with
the unexpected from the beginning now
contrast that with a lot of what's
called a I out there which we mostly
term narrow AI which is often handcraft
doodle special case for specific problem
and therefore can be very brittle if
something unexpected happens it will
just catastrophic you fail so one of the
other sort of philosophies I guess that
we commit to at the start of deep mine
was the idea of grounded cognition so
this is the idea that all the notion
that a true thinking machine has to be
grounded in a sensory motor data stream
now this kind of gives rise to notions
of end-to-end learning agents you know
the perhaps start with raw vision so it
may be pixels on a screen and go all the
way to making it
about what action to take and we're in
stood in that entire stack of things
that's involved all the way from
perception to decision making now
usually when people talk about grounded
cognition on body cognition people start
working on or thinking about robots and
of course that's the ultimate grounding
robots in the real world but actually we
decided to start off with simulations
and specifically games and we think that
game saw a pretty perfect platform for
developing and testing a ir grams
especially these kinds of grounded
algorithms so firstly obviously you can
create unlimited training data by
running the game as much as you like
there's no testing bias because the
games were designed to be challenging to
for humans obviously they were designed
by game designers for the purpose of
entertainment not for testing a is so
often I think one of the things that is
held back the AI field is that usually
is the algorithmic developers who the
algorithm developers who also develop
the tests and that can lead to a kind of
unconscious bias about and what the
tests really test for of course compared
to robots you can parallel test these
algorithms you know run millions of
agents at once all playing the games and
it's also quite convenient in most games
that have scores so it's very convenient
for us to measure incremental progress
if you make some small change to an
algorithm you can quickly see and
quantify the difference it's made so we
really love games and of course we also
like robots and we're very interesting
robots as a as a sort of application of
AI but not at you know in terms of
development we mostly focus on
simulation so I guess our most sort of
widely known work to date is I work on
deep reinforcement learning where I
think we showed for the first time quite
an impressive use of the reinforcement
learning when we used it on the classic
Atari games from the 80s Atari 2600
games I'm just going to run a quick
video of some of the of the agent
playing these games but just to be clear
for those you haven't seen this video
before you know what the agents getting
here
are just the raw pixels as inputs around
30,000 pixels per frame and the goal
here is to maximize the score everything
else is learnt from scratch and here
it's a single system we insist on a
single system that plays all the
different games without any tweaking of
the hyper parameters so we called the
system DQ n after DQ DQ network and
here's a sort of little medley of the
same system playing of a mere half a
dozen games and you can see for those we
don't know about Atari games how
different in style they are visually
also in terms of their objectives
incredibly diverse and and the same
system could you know it effectively
master all of these games now when we
sort of published our nature paper on
this we were better than human at just
over half the games and now with
superhuman level at all but five of them
so if you if you're interested in a so
here's the book the boxing's on /
where'er you know here that alyssa it's
the red boxer on the left hand side
playing the inbuilt AI and so if you're
interested in in the details of this so
you know it's all them in the in the
nature article including our code which
is available online so you couldn't you
can play with DQ and yourself so given
we're here talking about brains and
minds as well as machines what about the
brain and in fact DQN and why one of the
reasons it works so well was partially
inspired by neuroscience and
specifically hippocampal replay the idea
of replaying your experiences so that
you can maximize the use of that
information and now it's critical the
use of replay was critical in making DQN
tractable in a reasonable amount of time
when it was training on these games so
I'm going to end the rest of the talk
now just discussing my own view on you
know how systems neuroscience should be
integrated with AI so first the thing to
talk about is you know often I get asked
well why bother with the brain or
neuroscience at all you know there are
probably many other ways of building
intelligence and that might well be true
but I think there's a couple of
arguments to take neuroscience seriously
first of all I think it's it's likely
that the
were possible solutions is actually very
small compared to the size of the search
space so if that's actually true then
it's probably worth honing in on trying
to reverse engineer at least to some
extent the solution that we know exists
and after all the brain is the only
existence proof we have that general
intelligence is possible at all now that
might all be very well but if it would
be no point talking about it if we
didn't have amazing now new techniques
and data streams to actually analyze and
to give us information about what's
going on inside the brain and pretty
much yearly now amazing new techniques
get developed so away from opto genetics
kind of tommix two-photon microscopy you
know that the list them goes on and on
now and you know there's a huge
proliferation now of amazing techniques
to get ever closer to what's going on in
the brain and this is all resulting in a
kind of exponential increase in our
understanding of the brain of course
we've got a long way to go you know I'm
not saying we have a good four or even
close to full understanding what's
happening with the brain but there are
many clues now many interesting nuggets
of information that if used in the right
way I think can be helpful for AI
development so I see there's sort of two
you know kind of two buckets if you like
of you know the purpose of neuroscience
in terms of how it can help AI
development so firstly and maybe more
obviously it can provide direction so
research direction and inspiration for
new types of algorithms architectures
and even analysis techniques for
analyzing these machine learning systems
and I think we should be looking to
neuroscience where we have the least
idea or the most uncertainty if you like
about what to do in machine learning to
solve a particular problem or to have a
particular capability and also I think
something that some we're pushing very
hard on at the mind is building and
taking a inspiration from the analysis
and visualization tools that are now
quite mature in neuroscience say for
analyzing ephemera images and then
applying that in some analogous way to
analyzing machine learning systems and
another interesting thing is actually
the experimental techniques in biz
techniques that are kind of standard in
things like fMRI which in terms of
controlling for what you're looking for
in an experiment and that's something
again that in machine learning you know
hasn't come across yet and I think could
be very useful this sort of idea of
designing these kinds of experiments
that we do in neuroscience in the second
way I think neuroscience can-can are not
being useful is why call validation
testing so if you have some idea of our
notion of you know your favorite type of
algorithm maybe it's reinforcement
learning you know and you're sort of
arguing with another machine learner
that actually this should constitute or
could constitute a viable component of
an overall a GI system you know how much
F how do you decide if this is you know
this is reasonable conjecture and you
know so maybe you go away and you start
trying to build a system and probably it
doesn't work straightaway so then you've
got to decide like how much more effort
should we put into that you know is it
just a question in another few years and
then something viable will happen and
something interesting will happen so
it's these very difficult decisions to
be made especially if you're running a
large group or you have a large team of
where should you put your effort and I
think if you can point to a system in
the brain that that analogously does
that sort of mimics that algorithm or or
indeed the algorithm mimics that part of
the brain then I think that can give us
confidence that we should put more
effort into that area of research and
that ultimately it will will yield some
fruit and in fact that's what we thought
about with reinforcement learning and
why we committed to that so heavily
because the brain indeed and most
biological systems use forms of
reinforcement learning like TV learning
in order to learn about their
environment but as you know the next
question then is you know if you're
going to take neuroscience seriously
there's so much neuroscience so what
parts of neuroscience should we be
paying attention to and so here I like
quoting sort of david mars three level
analysis or it should be called Tommy
Tommy's a three level analysis as well I
think tommy was involved in heavily in
this and david marr used to say in the
70s probably one of the founding fathers
of computational neuroscience that to
fully understand a complex biological
system you need to understand
on three levels the computational level
which is what the the what if you like
the goals of the system and algorithmic
level so the how so the representations
and algorithms the system uses and the
implementation level you know I either
sort of physical substrate that realizes
the system so those are the three levels
you need if you want to fully understand
say the brain but actually I think for
machine learning an AI development
really the top two levels are the most
important so at deepmind we focus on the
kind of computation on algorithmic
levels when we come to analyze
neuroscience in the brain so
collectively I refer to that as systems
neuroscience and really what we're
interested in then Ben is the algorithms
and the representations and the
architectures that the brain uses and
we're working on all sorts of sort of
cutting-edge areas where we're using
systems neuroscience ideas as well as
the course machine learning ideas to try
and make progress on so here's just a
sort of a kind of summary of some of the
areas that we're looking on at the
moment and currently focusing on
representations memory attention
concepts planning navigation imagination
and in fact in terms of parts of the
brain Christoph was talking earlier
about you know prefrontal cortex and and
and high-level cortex but actually one
of the old parts of the brain the
hippocampus which is this bit of the
brain in pink here in the middle it
turns out to be quite critical for a lot
of these capabilities so let's talk
about some of those in in order that
still talk about memory first so we've
been experimenting a lot with memory and
adding memory to neural networks so one
way you can think about the work we're
doing is you take a classical computer
we implement a sophisticated recurrent
neural network perhaps with some LSD ms
and then we give it a huge memory store
that it can learn to control and then
that whole system is differentiable end
to end and then what we end up with is
what we're dubbing this neural cheering
which is called annual chewing machine
in the sense that it's it has all the
components now that a Turing machine
would need but its neural and the sin
so that it's learnt by from input and
output examples so here's a sort of
cartoon diagram of the of the
architecture so you've got the in the
center there and the CPU the controller
is the recurrent neural network and then
there's the input and output tape you
can think of and it learns by example
input output examples and it learns to
control this a very large memory store
on the right and it learns to read and
write from that now haven't got time to
go into this in detail today and in fact
I think Alex graves is giving a talk on
this right at the moment but also Greg
Wayne is giving a talk about this five
o'clock tomorrow in another one of the
workshops so he'll be going to much more
detail about this work if you're
interested I'm just going to give it to
show you a couple of videos of sort of
some of the latest things we've been
able to do with your two machines over
the last year so we looked at this one
of these classical problems from
old-fashioned AI I guess back from the
80s shrewd allu which is a blocked world
problem and the idea is that you're
trying to manipulate these blocks you
know put the green block on the red
block or put the blue pyramid inside the
white box and you can also answer
questions about the scene as well now
we're not ready to tackle for shrewd
allu yet but we created a kind of mini
version of shrewd aloof you like so a
mini blocks world where now we're
looking side on on to this block world
and the sort of task we created was
imagine a configuration of blocks here
on the left you have this starting
configuration and you've got a goal
configuration on the right hand side and
the system has to convert the stock
configuration into the gold
configuration by moving one block at a
time so the only moves that are allowed
are moving the top block to the block to
any of the columns adjacent columns so
it's a little bit like a complex Tower
of Hanoi problem and it's actually quite
difficult for humans to do in some sort
of you know reasonably optimal way so
I'm just going to run this video and
show you how the new machine does so in
the middle here you see it trying to
change the stock configuration into the
goal configuration and no obviously it's
never seen this type of configuration
before so obviously the stock of
aggression the goal configuration are
totally knew it
learn this from example seeing other
examples of solutions from other
positions you can see there it solved
the problem and then we have actually a
language a sort of mini language version
of this where we describe now the
constraints in terms of little language
instructions rather than an example n
board and so we if you see down at the
bottom here I hope you can see that the
little constraints so like block three
needs to be down from block five block
for needs to be up from block to block
one needs to be up from block 4 and so
on and so we give a bunch of these
constraints and then from the start
position the euro train machine has to
reach an end position that satisfies all
those constraints so I'll show you that
again here so it reads in the
constraints letter by letter and then
when it sees the ? it knows that it's
supposed to execute a solution so we can
see there at the end there that that
final ball configuration actually
satisfies all those constraints three
down from five four up from 21 up from
four and six down from three and it does
that inner inner in a in an optimal way
so that's certain memory so what about
concepts so we would like to have
abstract concepts because it's going to
be key for so many things including
transfer learning which you know I think
is one of the keys to flexible general
intelligence so we defined transfer
learning is the ability to apply
previously learned knowledge
appropriately to a new situation to help
your performance in that new situation
and that breaks down into at least three
sort of sub steps one is you got to
identify the salient features in your
current environment and more importantly
ignore the irrelevant features then you
need to re represent those features as
an abstract concept so sort of divorce
from the perceptual features that you
learn them in and then once you have
these library of concepts you need to be
able to select and appropriately apply
those to any new situation that you
encounter so probably step 2 of this is
the probably that I mean all these
things are very challenging but
prospector is probably the most
challenging and the most critical
and really I think from the beginning of
deep mind we've identified learning
abstract concepts as one of the key
break foods that are needed towards
getting us to AGI and you can see this
diagram you know this is sort of missing
gap on the left-hand side you know you
can think of information in the brain
crudely split into three levels like
perceptual information this abstract
conceptual information and finally
symbolic information like words and
there's been a lot of work obviously
amazing progress the last decade on with
things like deep learning in terms of
dealing with perceptual information and
obviously there was a lot of work in the
80s on logic networks but there's this
missing piece in the middle which would
allow the the symbols to be grounded and
also form to go from perception
information up to truly abstract
information now I think is a gable show
Dan Gable and you know when I was doing
my PhD in neuroscience in London I was
this was one of the key results that I
noticed that were quite amazing to me in
terms of clues about how the brain might
do this so these are Jennifer Aniston
neurons I think gay will cover this
earlier you know these in Europe this is
a neuron that only fires to Jennifer
Aniston pictures and only when she's on
her own not when she's with Brad Pitt so
so so you know so this is very specific
and here's a Halle Berry neuron you know
we're here it can be you know drawings
of her not particularly good drawings in
fact and you know it one way she's
dressed up as Catwoman so and you know
not only whets just heard the text of
her name and so you know these are
really abstract neurons in some sense
that are representing the concept of
Halle Berry Halle Berry so then you know
we've done a lot experiments I did
someone hit my PhD with my colleague and
kumaran where we looked at conceptual
learning in the brain with fMRI studies
and so in this study here we had people
learning about fractal patterns so they
saw two fractal patterns on honest on a
on a TV screen and they had to predict
the weather outcome of the next day
whether it be sunny or rainy and they
learned this by trial and error and
eventually you find what you learn is
that there's some underlying pattern so
certain fractals you can ignore where
their position
and others the other factors you can
ignore their identity and it's just
important where their position on the
screen so these are kind of underlying
rules that were independent of the
fractals themselves and we scan people
learning this task while they were
learning the first task and then once
they mastered that there was a second
task where now we changed the fractals
to new fractals but the underlying rules
were the same were kept the same and we
scan them learning the second task and
they were much faster at learning the
second task and what was really
interesting is that the part of the
brain that correlated with this
increased learning in the second task
when we were scanning in the first task
was actually the hippocampus so the
amount of activity in the hippocampus in
the first task predicted later transfer
to the second task being improved on the
second task so it seems quite strong
evidence that then that the hippocampus
here it's quite difficult for me to
point at it from the state podium here
hippocampus here is very critical in
learning concepts here's another famous
study actually of a sleep study also
related to probably hippocampal replay
and the learning of concepts so in this
one the concept you're trying to learn
is this linear hierarchy that a is
better than B that's better than see
that in the E and F but you don't see
that whole hierarchy at once you only
see the individual pairs so you see that
you get shown B&amp;amp;A together B and C
together and so on and you have to learn
by trial and error which one of the two
is better but you never see the whole
hierarchy and the joins between so your
brain kind of has to infer this from
those individual premise pairs and
what's interesting about this study is
that when they did this and they tested
people 20 minutes later on the separator
pairs so maybe like would be better than
B then people are at chance level 20
minutes later but after 12 hours or 24
hours and a night of sleep they're now
up to eighty percent or ninety percent
successful on the these sort of
separated pairs the inference pairs but
even after 20 minutes there's no
difference on remembering the premise
pairs so it's really this in this sort
of extraction of
actual information that's happening
during the sleep so what about
representations so here's another thing
that I saw that made me think about this
is a huge clue to how representations
are structured in the brain so this is
very famous effect in psychology called
the drm effect and what happens here
just briefly is that you get shown as a
subject lists of words so maybe let's
take that top list dark cat white and
coal and you get you get shown all these
study words and you're told to memorize
them and then later you get tested on
these words and you get tested on like
did you see the word cat and you say yes
no but there's also critically some law
words so like hear that word black or
the word river or the work cold which
are related to those lit word lists but
were not shown during the study phase
and actually people get and fooled very
reliably to say that they see they've
seen the word black or they seen the
word river when they didn't actually see
it during the study phase this is an
incredibly reliable effect so it's been
known since the 60s it was rediscovered
in the 90s and it's been repeated like
you know in thousands of psychology
experiments it's one of the most cited
studies but interestingly no one had
thought about doing this with fMRI so
what we decided to do is look at this
with fMRI where now you know you can
think about what's going on in the brain
and why is this happening and one of the
reasons we thought was that a partial
priming effect so the idea is that you
know these the neural representations
underlying these words are maybe
partially overlapping perhaps they will
learn because of the way they were
learned through similar experiences and
we thought maybe the degree of overlap
would predict how how much the brain of
that individual would be fooled into
thinking that they see in that world
because perhaps all these partial
priming of these partial overlaps would
end up creating a full a priming effect
on the world which is maybe the way the
brain judges whether you've seen
something recently or not and that's
what we found is that actually once we
started scanning people looking at these
lists these word lists and being tested
on a drm effect we found that afterwards
there was one part of the brain that
predicted weather reliably whether
individuals were going to get confused
about whether they saw
vic word and the part of the brain that
kept that was predicting that was the
anterior temporal lobe here highlighted
in the yellow and that's actually known
to be the area that's involved in
semantic dementia so that's the part of
the brain that goes wrong when you have
semantic dementia so it's actually where
we predicted these types of conceptual
semantic representations would be and I
think this study now shows a little bit
about how those representations might be
organized so with imagination now then
as the final part you know we this is
something I also did in my PhD was
looking at how people imagine and plan
for the future and one thing we decided
to test on was hippocampal patient so
patients without hippocampus errs damage
to the hippocampus could they imagine we
know that there are music we know they
don't have episodic memory but they
could they imagine things about the
future and so we tested always imagined
things like cues like imagine you're
lying on a white sandy beach in a
beautiful tropical Bay describe what you
can see and when we looked at the
patient descriptions that was hugely
impoverished specifically in their
spatial coherence so what that means is
that they couldn't bind the disparate
components of a scene into a coherent
whole and you can see their performance
here there the dark bar this is a
measure of kind of richness of their
description and they're massively
impoverished compared to age and
education matched and verbal matched
controls on the right-hand side we then
took this in the scanner and we found
that of course the hippocampus is
critical but there's also a whole brain
network a very reliable one that
mediates imagination and is also
involved in episodic memory so then one
ish anything you can think of and we did
we haven't touched on this is also about
animal intelligence you know there's not
only human intelligence I think we can
also learn a lot from animal
intelligence so I was fascinated to see
whether rats can actually imagine and so
we designed this study but i think was
quite elegant and quite simple to show i
think categorically that rats can
imagine so we first of all we looked at
play cells so for those of you don't
know play cells are neurons in the
hippocampus of a rat that signal where
at is in at location so here we got
top-down looking at a box that rattles
in and then these are two cells a and B
that fire only in that position in the
box what we can also we also know from
previous rat studies is the sequences of
these cells play when a rat runs along a
trajectory like a linear track so here
ABCD in the in and it replays in the
order that the rap moved in we also know
that when the rat goes to sleep these
trajectories are replayed a very speeded
rates probably to aid learning so what
we did is we designed a teammate and so
now again we're looking top down the rat
there's a barrier on this teammate that
stops the rat reaching the arms but the
barriers see through so the rat can see
the arms just can't move there so
initially in the first phase the rat
runs up and down the stem of the
teammates and we give it a reward we
show it a reward on the right hand side
I can see the food pellet but it can't
reach it then the rat goes to sleep and
we're recording from the rat's brain at
this point and it's very important this
because we're this is this is the
critical data that we come back to to
analyze then in the second session we
remove the bat that the the barrier so
now the rat can freely move up and down
the teammate and so now on the arms of
the t-maze we can find its place else so
here I and D and what we find is that
when we go back to this analyze the
sleep phase that in fact these place
cells were being replayed or pre played
if you like during that sleep even
though the rat had never experienced it
yet so it only had only seen it maybe it
was thinking about moving towards the
food pellet so this is the first time
that anyone's ever shown imagined place
else and what was very cool was that
there were significantly more pre plays
if you like to the right hand on where
the food pellet was then to the left
hand on in every one of the four rats so
I think that's pretty categorical that
this was actually behaviorally relevant
as well so we're now looking at
imagination based or model based
planning in for our atari agents and
this is a very this is an early version
of some of the models that were trying
to build in and use in planning so i'm
just going to end with integrated agents
so if you now combine all the things
i've been talking about into a single
agent then perhaps we have something
that could be deemed rat level AI so be
able to do unsupervised vision attention
have memory episodic memory navigate
round mazes and
haps even do imagination based planning
and I think you know rad is pretty smart
so if we can get to this level that'd be
pretty spectacular and maybe you can
think of the atari agent as a sort of
lizard level so here here's a little
sneak preview the kinds of things we're
working on so this is now true feed 3d
environments called labyrinthus we've
extended it from an open source 3d
engine and this is our agent navigating
around finding rewarding apples and and
also exits out of the maze just form
pixel inputs so just from from the raw
pixels so we're on doubling down on
systems neuroscience a deep mind and I
think there's an incredible wealth of
information and ideas and clues if you
look if you know where to look and I
think we're actually just at the
beginning of the influence of
neuroscience and AI on each other and
one thing with especially excited about
is developing new tools and techniques
borrowed from neuroscience to help with
the analysis of machine learning systems
so I think we're sort of entering very
exciting here an hour and we're doubling
down on these efforts and part of that
is actually I'm you know very excited to
announce the professor Matt bought for
Nick who many really know from Princeton
is joining us full-time from februari to
head up the neuroscience team and join
our incredible thought in-house team and
you'll know a lot of his work I'm sure
ready from decision-making and control
and working memory and we also have a
bunch of collaborations with Oxford and
a call to MIT with CP mmm program
Harvard UCL and so on and obviously
we're expanding the neuroscience team
we're putting more into this so if
you're interested in the work that some
of the work I've been talking about then
come and have a chat with me or Matt so
I'm just going to end by saying you know
that actually we've talked a lot about
how neuroscience can help AI but i also
think equally interesting maybe we can
discuss in the panel is how a oh i might
help us better understand the human mind
especially questions like their
christoph is interested in like what
actually is consciousness and is it
necessary for example for intelligence
and as which would fireman one of my all
time here i said what i cannot build i
do not truly understand and i think
that's true of intelligence to thanks
thanks a lot Dennis um we have time for
a few questions so um thanks a great
talk and I was intrigued by your you
guys are now looking at imagination
based planning I guess one of the
problems you have with with that kind of
system is how does the agent an agent
who acts as it know when it's thinking
about something that is not currently
doing versus when when it should be
thinking about things that it is
actually doing and it seems like in
humans at least we have some sort of
cognitive level access to our own sort
of attentional mechanisms so we know
what we're thinking about and we know
perhaps the context and why we thinking
about it you know in space and time so
if I think about my my birthday a few
days ago and I'm not thinking about you
know I'm not confused in that sense yeah
sure I mean we work on attention both
the sort of internal and external
attention I think that's going to be
very important I think you know actually
the biggest issue with our model based
planning stuff is that and what our our
genitive models are pretty good so if
you measure them on compression or other
things measures but the small errors
compound very quickly and so if you're
trying to imagine you know hundreds of
steps out the errors get quite large I
think part of the issue is is probably
you know you don't want to actually be
imagining on the pixel level ideally you
want to be sort of measure you know
imagining on some high level feature
level which again comes back to sort of
concepts so let's say that's the bigger
problem than not knowing whether you're
imagining or not I mean in an artificial
system we can make that distinction
quite clear so does that work yes it is
a quick comment and a question you
mentioned in the beginning this blocks
world problems that yeah you hide em
it's how you doing that that you were
able to solve and you also mentioned
that you think this is a more complex
version of the towers of annoying
problem I think that's not the case
simply because in the cause of and away
the problem you ask this additional
constraint why you cannot place a larger
disk on a smaller one which means that
if you have n disks you really need to 2
2 to the N moves which means if you have
30 disks you need a billion moves to
the optimally which is not the case and
that particular reduced black swirl and
setting where is maybe more important
for the topic of the symposium here is
in the beginning you mentioned that you
were inspired by neuroscience as you did
the atty game thing and and and you
mentioned the action replay as an
example but that of course goes back to
Lin 1991 know where that PhD thesis on
the enforcement learning about action
replay and which we did saw it with your
car you sure yeah I've guys that's
that's great let me do it and at the
point is however is that this is what I
struggling by engineering considerations
and so the questions do you have
something where you really can show
neuroscience inspired us to do something
better than standard engineering which
would maybe address exactly that's a
very difficult bar because i think what
you find is that neuroscience ideas have
sort of seeping around everywhere and
help formulate certain ideas and of
course you should be doing both at the
same time so i think engineering should
go hand in hand with the neuroscience
and lots of ideas have been thought of
in the machine learning and lots in the
neuroscience and i think it helps they
help each other sort of confirm that
you're going in the right direction i
mean even converse with you know hooble
and weasel you know which ones
influenced who i mean i'm sure you know
you could say that the commnet sort of
came up you know with little influence
format but actually i think these ideas
are kind of all mixing together and help
with this progress so i don't know if
you're asking for a specific example of
exactly a neuroscience thing that was
done and then interpreted in machine
learning is that we were saying well a
countenance is actually a great example
because that was in my point of view
more or less the last time when your
science really produced an important
inspiration for AI which because
fukushima's conclusion annette works and
the 70s were inspired by a human and
weasel whose item of the 50s yeah i
think this is actually
question for the panel awesome well
other people might comment so let's do
this letter and maybe one two quick
questions we're here I try to be very
quick I'm surprised that you rule out an
eight structure more or less all
together and I wonder whether that's a
methodological commitment you said at
the beginning you want to do everything
kind of from raw pixels you don't want
anything built in there's a lot of
evidence from biology that the genome is
spending half of its effort so to speak
trying to build a very carefully
structured grain so why not as I'm sure
so in fact I should clarify that so
we're interested in sort of learning as
far as possible sort of end-to-end but
we're not we're not against having
modules or I mean clearly evolution is
given some initial constraints so we're
looking at those kinds of things as well
but we would like the prior knowledge
that we build in to be as minimal as
possible and as generic as possible
that's maybe another longer way of
saying it's an empirical question with
the right amount should be ho yeah very
brief collaboration and competition have
some neural basis in the frontal cortex
so like this the agents are very
independent what role do you think
competition and collaboration play
between learning systems so we arranged
in things like meta control where where
there's you know more than one type of
control system and then you've got to
decide which system to trust or which
system to switch to so the brain you
know definitely does that between mobile
free model-based perhaps between
episodic controllers as well so we're
looking at that and you know
implementing something similar to that
maybe you know deciding around the
uncertainty of each of the controllers
so we're looking at that moment Oh
should say thanks to all my colleagues
of course to deepmind all the amazing
people have done all the work on on the
screen here and many more thanks thanks
that's in clemmons again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>