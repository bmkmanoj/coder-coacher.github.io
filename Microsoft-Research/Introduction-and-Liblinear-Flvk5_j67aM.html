<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction and Liblinear | Coder Coacher - Coaching Coders</title><meta content="Introduction and Liblinear - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction and Liblinear</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Flvk5_j67aM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so originally I was actually not
thinking about the open machine learning
workshop originally I was thinking about
a vocal workshop on vocal Abbot alone
but I started wondering something the
thing that I wondered is New York the
center of open machine learning and this
is serious I mean so we have four local
projects which will be pretending into
remote projects duty people flying in
from outside to present but some
evidence that New York is the center of
open machine learning but the evidence
is that New York is the center of open
machine learning is that we could have
had four different projects that are
local and and had a very credible
workshop so Thorsten worked on SVM light
and variance David Min know is running
Mallett his factory at Amherst and in
liemba too is worked on this STD and
variance so these are all very credible
open source machine learning projects in
addition to the ones that we're not
talking about today right and I I don't
know if there's another place in the
world where there are this many projects
with which are within easy driving
distance of each other and in terms of
other people who are really working on
them that's it's kind of I think unique
and interesting about the New York area
okay so the schedule for today basically
there's some parts to presentations as a
break with refreshments and there's
project presentations and then for for
lunch this is New York so you go down to
the street you pick a random direction
you rock walk one block you find the
food you want and then you should return
by 1:00 p.m. for the afternoon we're
going to have a summary so everybody who
cares about the project should be here
because otherwise we might lie about
your project and then we're going to
have a panel where all the speakers will
be upfront and leon batu will be
moderating and then we're going to have
a break
and then we're going to break out into
six different groups one for each
project and the idea here is well this
is a remnant from me wanting to do a
vocal workshop I wanted to talk to
people who are using VW and discuss sort
of directions that I want to go in the
future and get some understanding of
things so this is a time for individual
projects to talk to individual people
and figure out what they want to do and
then there's be one hour at the end with
a brief presentations by each of the
projects or people discuss what their
future plans are for the project okay
and then five hopefully everything
finishes up on time
all right so Microsoft this is an
advertisement Microsoft is hiring we
really wanted to go to applied
scientists and research engineers in New
York City and Yahoo where Alena works is
also trying to hire so they want a top
research engineer okay I think we should
begin the presentations
so I should mention we have 25 minutes
for each of the presentations but of
course that includes all the
question/answer and transfer time so
individual speakers should try to if you
want discussion you should make sure you
a lot of time for that okay okay so so
here's the first project presentation of
this workshop so my name is CJ lien I'm
from National Taiwan University so I'm
out so I'm going to attain the kdd hung
Sunday so so so why I'm here that is my
pleasure to visit Microsoft Research for
a few days okay so so in this talk I
want to describe some project status of
live linea which stands for a library
for and linear classification so here's
the outline of the talk so first I have
an introduction and I describe servers
in this software and there's some other
details of this package they have some
discussion so let me keep the history of
this software so before about 2006 at
that time my research groups actually
focused on developing a general Colonel
our support vector machine package
called the base via that was our focus
at that time but about using two
thousand six and seven I spent several
months at Yahoo research so you know
idea who are all those engineers they do
a lot of web document classification
then I applied this libous VM the kernel
SVM package to Las dataset and I found
out that unfortunately to train a large
set of documents but biker knows is
actually very time consuming but for
Curnow SVM you can of course use linear
kernel
so 404 so so I tried to compare with
between linear and say Koshien or RBF
kernels and I found out that the
accuracy the accuracy does not differ
much well here I show you a table to
compare the results of using linear and
the Gaussian kernel on a bunch of data
sets here at timings training time and
accuracy means test accuracy but the
training time here for linear is already
low training time by using lib linear
that we developed later and therefore
RBF kernel the training time is actually
by the best VM as a Carnot SVM solver
let's look at least a cough type data
set it has more than half million
training instances if you are using
linear kernel by a linear classifier the
training finished finishes in 1.4
seconds by your taster accuracy is only
76 percent now if you spend a lot more
time say Health Day to do the training
then your taste test
accuracy dramatically increases to 96 so
this is probably one example that you
want to spend more time and not to use a
nonlinear classifiers like kernel SVM to
get better accuracy however if we look
at SRI other data sets here the first
one is called a news 20 so you can
imagine that this is a set of new star
comments and there is also one called
Yahoo Japan so this is actually
something related to web documents now
in one point one second when the tester
accuracy is 96.9 now if we use a kernel
solver then you need about 400 seconds
but the tester accuracy is about the
same okay so the same situation happens
for all those three data sets so one
reason why this happens is
for document the data set we use the so
called the bagel for its motto solo
statuses are very large and sparse so
you have a large number of instances and
also a large number of features so for
example for this Yahoo Japan dataset so
we have about 140,000 instances but
almost 1 million features because each
feature corresponds to our oh I see that
people sitting in the end couldn't seal
a button of less slides ok I can try
this let me see yeah well so I have been
teaching for many years so I know how to
handle this ok so now okay so we see
that lost datasets are fairly large I
mean that the in terms of the number of
features they are a so-called large in
the sparse data assets because each
feature corresponds to one English word
okay that's the so called bag of words
model and if you have already if you
already have so many features probably
well you don't need to make data to a
higher dimensional space okay by a
nonlinear mapping so that explains why
for Low's document datasets the
performance of using a linear classifier
is about the same as using kernel
however the fact is because we because
of without using kernels then our
training and testing speed becomes much
much faster well this is very very good
okay so once we realized this effect we
started developing this software lib
linear in around 2007 so that's about
the history or the original motivation
of this package okay so now I will I
will go to some details of this package
we need a brief introduction about a
binary classification suppose we are
given a set of training instances so lay
our label and the feature pair a feature
vector pairs called Y I and X I so we
have i from 1 to L so that means we have
L training instances and why I is the
label here and assume we are considering
two class situations then Y is either
plus or a minus 1 so each X I in n
dimensional space so that means we have
n features then the standard
optimization problem to do a binary
linear classification is by solving this
optimization problem you minimize so you
minimize the summation of the
regularization term so this one is the
so-called arrow true regularization Len
plus a parameter C and times the
summation of trending losses so this
function indicates a kind of training
losses you can see some simple loss
functions here if your function takes
this log phone then that means you are
doing regularized logistic regression
and if you take this hinge loss function
then this is the standard of support
vector machines and there's a parameter
C to balance between the two terms
between regularization and the training
losses that means you don't want to
overfeed the training data I also don't
want to under fit the training data
okay so let's the standard formulation
for binary classification a linear
classification and of course we can
extend lat to multi-class situations
lately this is supported in the software
even though I don't I won't mention that
in detail so currently what kind of
solvers are included in this library
first actually
menacing is for l2 regularized
classification because that covers the
standard form of logistic regression and
the support vector machines so I think
the majority of our users they use this
type of the softest type of problems so
for l2 regularized a classification
currently we have two types of servers
the first one is Newton method for
solving the primal form where primal
forming the optimization problem that I
just mentioned of logistic regression
and l2 loss support vector machines and
this paper indicates the some technical
details of this of this method and
another type of solvers is a dual
coordinate descent method for a dual
problem of either SVM or logistic
regression so this covers suppose L 1
and L 2 lost support vector machines
then another type of problem that we
solved is for L 1 regularized
classification so that means you replace
the auto regularized regularization here
with 1 norm of tableau so let's call L 1
regularization and you get a sparse
vector of lamanno vector W currently our
survey includes a primal coordinate
descent method for L 1 regularized
arrow to loss support vector machine and
also a hybrid method as the combination
of Newton method and a coordinated
descent for L 1 regularized logistic
regression the third type of solvers is
for recreation solar covers the standard
least square regression
so it includes a Newton method for the
primal phone of l2 loss support vector
regression okay so uh and so this is
actually a more generalized to form of
least square regression and we also have
a dual coordinate descent method for a
dual form of support vector regression
so here I list a bunch of solvers so in
addition of proposing loss optimization
methods we also include a bunch of
implementation techniques
well of course those details are in
their corresponding papers so he may ask
a question about a last election of
solvers so how did we come up with
putting loss solvers in the package of
course in the process of develop
developing a Lib Linea we study a lot of
other optimization methods so for
example our for l2 regularizes
classification we have also studied like
primal coordinated descent so right now
it's dual coordinated descent included
Inglot package and we also study like
quasi Newton method for solving the
primal form actually all those methods
are reasonably good but in the end the
reason we choose the current solvers of
but I mean for l2 regular rise to
collect a vocation we have two types of
servers one is coordinated descent for
the dual problem and another is renew
the method the reason of doing such a
selection is because now we choose one
first-order method and one second-order
method so sewing optimization if you
have a first order method like ordinary
decent or stochastic gradient this
gradient then you can quickly get a
model but that's mainly for easy
situations so if you have easy problems
then those first other methods can
quickly give you a model and they are
very fast however sometimes you have you
faced difficult situation
when you need more robust optimization
measures
so in those situations you want to
switch to hire other optimization
methods so for example we provide an
intent here okay so that's about a
selection of solvers then we check our
design principle so based on our past
experiences in developing the software
libous via so our setting actually of
live linear mainly follows lepage libous
via they share the same sparse data
format alright so so we only accept a
sparse input urge training and a test
data so we provide two ways to use a
package one is vice ruing vice ruler
recommend the line mode okay so you can
type in a comment type of comment to do
the trending and a doula prediction we
also provide a library function
interfaces so if you write your own code
okay so you have prepared your data but
of course you need to transform your
data into our or our data structure then
you can go our library functions for
training and the prediction and the why
important this thing about libous via is
a lot simple ECT and the easy of use
that's the reason why it becomes a
popular support extra machine package so
in designing lib linear we try to follow
the same principle however they also
different differ in several aspects my
important thing is that later talkie the
users are slightly different so when we
design when we developed the best VM
somehow our users are those general
users they may not know let me not know
much about machine learning so in
particular liebe sphere has been used
for teaching machine running horses a
lot so lost users they actually don't
know much about machine learning so this
can be seen from layer questions but
liberal linear because we omit
training large-scale data
data sets so of course we have to assume
that probably lay lost users know more
about machine learning yeah and about
the usage of Lib Linea so currently it
is mainly used by Internet companies for
training large-scale document documents
then people have also developed
interfaces of Lib linear to other
languages so for example we have met lab
and octave well this is actually done by
ourselves we have also done pison but
for others like a Ruby pearl Waker and
others they were done by our users an
issue I want to mention is about
parameter selection now for a linear
classifier the only parameter is C okay
the regularization parameter so we have
an easy script for users to select tool
to use cross-validation to select lower
the parameter C but in fact we don't
write a new tool which will the same
script provided in the base VM can be
directly used but beyond the local
package we have also done a lot some
extensions so right now in the website
of there is a website called IBM tours
this place includes a lot of extensions
of live SVM and the Lib Linea so we
provide some extensions so here are some
examples the first one is large-scale
linear rank s via then we also have live
linear for incremental and decrement or
landing and to handle very large data
sets
yes so this means that if now you have
removed a subset of your dataset okay
then after that you want to do the
retraining to get a new model that's
ahem Hank called the Freeman's of
learning yeah well not sure if this is
the correct way to describe that
scenario but but but we know how the
incremental learning is I just used
another word
Oh in the for large-scale data set we
also tried different ways one is a disk
a level linear classification okay so
instead of loading the whole date huh -
to memory then we store on data only in
a disk to disk level linear
classification and now we are also
working on distributed deep linear well
I have three minutes left so I won't be
able to finish the discussion but let me
try to say something okay so so now
let's have some discussion about issues
that I don't think we have soft for this
software I want to go back to the issue
of a parameter selection where users
usually have problems about the
selection of parameters well of course
we provide a default value but we know
in this regularization parameter C goes
to infinity our optimal solution W star
actually approaches the solution of this
this optimization problem that means you
don't use the regular regularization so
you just try to fit the training data
and you can prove that this w star will
converge to the solution of this yeah
and in fact for things like L 1 loss
support vector machines you can prove
that after a certain number C star then
your optimal tableau stock will remains
the same that means if you draw a figure
to show the relationship between
cross-validation accuracy and the
parameter the parameter C
you see a figure like lease so in the
beginning because you on their feet your
training data so you're across the
radiation rate is very low okay so you
don't want to use C in this area but
gradually then your CV accuracy improves
until respond well this is good well you
can stop here but a lot of users they
somehow they tend to believe let their
CV accuracy can just keep increasing or
something okay so they try larger
parameter C well not only is that not
useful but also the training time may
increase that's a big problem first for
some solvers in the package then the
training time may dramatically increase
so this is not very good yeah okay so
that's what I just said that when C is
large then a default server currently
Pro the default server engli in a
package that's a dual code in entity
sent because it is a first order method
it can be slow yeah yeah so so we know
doing a very large C is not very
necessary however users may not be aware
of that so we are hoping to have some
settings somehow to guide users so that
they don't need to they easily know let
well now my model has already stabilized
so they don't need to keep searching for
other C values I think I'm running out
of time so let me actually I do it five
minutes but as for for questions okay
okay so let me quickly do this so theta
scaling is another issue I want to talk
about linear classification so we know
for Colonel learning okay ah data
scaling is very important
well if your feature values are very
large and you are you are doing say
Gaussian kernel so this is the
formulation of Gaussian kernel length if
your if you have two different engine
instances X I and X J and their feature
values are quite large then after this
exponential function this value is going
to be zero
so if you don't do any feature wise data
scaling then you
testa accuracy can be very bad okay that
means your motto is very bad for linear
classifiers actually this problem does
not exist
however okay so we don't really have
problems about accuracy however the
range of feature values they still
affect the training speed okay so so if
you know your feature values are very
large but now you use mostly low order
optimization methods such as stochastic
gradient or coding and descent they can
be slow they can be slow so usually
people normalized normalize each
document instance to have unit length so
that's very common in document
classification it turns out that so far
from our experiences if you have done
such normalization then actually
accuracy is about an asset or you don't
lose accuracy but less may not be the
case for an undocumented data but of
course our users
let me not apply linear for only
document datasets
yeah so unfortunately we don't have a
career guide a knife for users about
that yet I think I want to skip this so
go to probably well conclusion so in
conclusion in this talk where I briefly
summarized in the pasted development and
also the current status of liberal inia
well we have learned a lot from users in
different areas so the experience is
quite good and here are some
acknowledgments so our users they helped
us a lot and also our my students they
contribute a lot for the development of
this package so that's about my
presentation so I welcome your questions
and the comments thank you
yes
oh yeah okay okay okay okay so so the so
the question is so I put two types of
optimization is a low order one and a
high order one lower the one is for easy
situations so is there a way to say when
to use which okay so that's roughly the
question oh yes
actually if you go to check our FAQ or
there's a practical guide we clearly say
for what kind of situation you should
use which okay so in general if you are
if your feature values have been
normalized and your parameter C is not
very large they want to use the default
server which is a coordinate descent
method but if you're a number of
features is not very large and you or if
your parameter C is actually quite large
okay because I mentioned that Quincy's
larger that means you try to fit the
data okay that's a more difficult
situation then you know in those
situations you want to use a higher
order you mean a starting point okay
okay
for Newton miss Newton method usually
okay so okay
so right now they both start with the
zero vector for primals will still yeah
but when we did a study on incremental
learning if your solution it is if your
initial solution is better then the
convergence is of course much better
yeah yeah so you know says well of
course it is sensitive but okay I think
it's still difficult to say yeah
probably we can discuss one more yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>