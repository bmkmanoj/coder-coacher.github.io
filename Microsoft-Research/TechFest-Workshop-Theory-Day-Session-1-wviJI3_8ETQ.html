<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TechFest Workshop - Theory Day - Session 1 | Coder Coacher - Coaching Coders</title><meta content="TechFest Workshop - Theory Day - Session 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TechFest Workshop - Theory Day - Session 1</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wviJI3_8ETQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
all right good good afternoon everyone
welcome to theory day we have I'm sure a
bunch of great talks coming in fact when
when I so we have a combination of some
grizzled veterans and some young
researchers and postdocs and when I told
our post doc says they should not feel
too pressure that if the talks don't go
well then you know in a few years they
can recover from it somehow this
reassurance didn't work so well because
I saw them at the 11 p.m. yesterday
practicing their talks so anyway I'm
sure they'll all be fantastic and we're
starting with I talking looking forward
to the Bobby Kleinberg will tell us
about incentivizing exploration thanks
so much you've all for organizing this
workshop and inviting me to present this
paper I'm going to be telling you about
some work I did on incentivizing
exploration with Peter Frasier David
kempe and jon kleinberg this picture you
might be wondering about this is
christopher columbus receiving his gifts
from ferdinand and isabella for
discovering north america so this is a
depiction of how incentivizing
exploration used to work at some time in
the past we had all powerful monarchs
who would command their loyal subjects
to go out and explore the subjects would
obediently do it and then if successful
they would earn their rewards uh you
know in the online world also there are
many powerful entities
that depends on their subjects now
called users to explore a world of
possibilities for them you know so
Amazon for example is a store where you
can buy almost anything you can ravit
read reviews of almost anything but it's
not as if Amazon employees full-time
reviewers to figure out which products
are go to which ones are bad they just
depend on the autonomous activities of
users going about their business you see
the same pattern repeated in many many
other contexts both in the commercial
world and elsewhere you know social news
readers need to recommend articles to
you but in order to do it they need you
to recommend articles to them there are
these collective citizen science efforts
where you know amateurs at home with
their telescopes are mapping the sky but
there's no you know global governing
organization that can command them which
part of the sky to aim their telescope
at and it's a bit of a stretch but you
could even say that the same types of
issues play out when we talk about
things like national funding of
scientific research we have an
organization like the NSF that may have
priorities for what research projects it
wants people to undertake but in the end
the research is being done by individual
scientists who will pursue their own
aims that may align but are not obedient
to the dictates of the funding agency in
all of these situations what we have
really is a misalignment of incentives
where we have principal whose goal is to
explore a broad space of alternatives
and collect information about all of
them and we have individual users whose
goal is to select the best alternative
for them to state it in a more pithy way
we have a principal whose goal is to
explore and individual users whose goal
is to exploit stated in this way it's
very tempting to model this dilemma of
Miss
lined incentives as some type of
multi-armed bandit problem so on the
next slide I'm going to introduce you to
or recap for you what is the multi-armed
bandit problem and the version of the
problem summarized on this slide is what
generally gets called the bayesian
multi-armed bandit problem so this is a
problem where there are K alternatives
conventionally called arms the strange
name stems from the metaphor of K
different slot machines each of which
has an arm that you can pull and get
some random payout and we'll assume that
the payout distribution of one of these
slot machines is determined by some
unobservable parameter called its type
and so the K arms have independent
random types but the type itself is
unobservable all you can do to find out
the type of an arm is to pull it observe
one random sample from its distribution
and as you accumulate more and more
samples you accumulate more and more
certainty about what its type yes ok in
the model of a decentralized
crowd-sourced exploration of a set of
alternatives I'm going to assume that
you have in sequence of users and this
talk it'll be modeled as an infinite
sequence indexed by time so the user
that comes in at time T will participate
in one and only one round of decision
making in which it chooses one of the K
arms I'll call it I sub T think of this
as a user coming to amazon.com to buy an
8-megapixel camera and there's some
finite set of eight megapixel cameras
that are sold at Amazon it selects one
of them afterward it observes you know
how good that alternative was maybe on a
scale of one to five stars
and reports what reward it got from
pulling that arm the sequence then
repeats in the next time step and in
this paper where observe we're assuming
that the history is fully observable so
um the user that comes in at time T can
see all of the star ratings that were
provided in all the time steps one
through t minus one and then to
reiterate with a bit more formalism what
I said on previous slide the principal's
goal is to maximize the long-run average
of people's payoffs I'm going to do the
standard thing in this field and assume
that there's some geometric discount
factor gamma less than one and payoffs
that people get t time steps in the
future contribute with weight gamma to
the T into this weighted average that
the principal is trying to optimize the
user at time T on the other hand has a
goal of just maximizing their individual
constituent of that weighted average and
so the policies that would maximize this
quantity and this one respectively I
call the optimal policy and the myopic
policy so the optimal policy chooses a
sequence of IT s that maximizes the
expectation of this average reward the
myopic policy is a completely greedy
algorithm that in each step T chooses
the IT that maximizes expected RT given
the history encoded in the public log
file of the past t minus one
observations what's known about these
two alternatives so the optimal policy
in principle could be very very complex
it's a function that map's every
possible history of t minus one steps
into a decision at time T so they're
exponentially many possible histories
and so you know encoding the truth table
of this policy could potentially take
double the exponential space and in fact
for a long time
this problem of finding the optimal
policy for the multi-armed bandit was
thought to be so hard that statisticians
who were working on it on the Allied
side during World War two were joking
that if we wanted to defeat the Germans
what we should do is we should drop
leaflets containing descriptions of the
multi-armed bandit problem over Germany
and their scientists would become so
preoccupied with this unsolvable problem
that they would get distracted from the
war effort and we would vanquish them so
then it came as quite a surprise in the
1970s when this man John Gittens came
along and solved the multi-armed bandit
problem with amazingly simple and
succinct solution so he defined a number
called the Gittins index that you can
compute for each arm that depends only
on the past history that you observe
observed for that arm I'm not going to
tell you how this number is computed
because it won't be relevant for my talk
but the optimal policy is simply at
every time compute the gettin's index
for every arm pull the one with the
maximum index question what's the model
how are these RT distributed do you need
I mean if you notice you don't know this
this could change your right so you need
to have a prior belief about how the
reward distribution of an arm depends
upon its type parameter the gettin's
index policy is generic in the sense
that no matter what your prior is it
explains a procedure for computing a
score such that the optimal thing to do
is to pull the arm with the highest
score um the specific function that that
procedure computes will depend on the
form of your prior so this might be easy
or hard to compute depending how complex
your prior is for standard priors like a
beta prior a common one that's used in
reality is you believe that the arm has
some unknown parameter theta between
zero and one independently drawn from
the beta distribution when you pull it
you see a binary reward which is either
0 or 1 and you do a bayesian update that
gives you a beta distribution with
different parameters so that like the
that would be a typical prior that
people would use in practice
essentially yeah in the specific
instantiation of the model that I just
talked about with beta distributions
outcomes are 0 or 1 I forget ins theorem
all he needs is that there exists a
bounded sub interval of the real line
and outcomes are distributed on that
bounded interval so the result is really
quite general okay for myopic users the
policy is even simple to simpler to
describe I described it on the last
slide it's a purely greedy policy that
always computes the posterior expected
reward of each arm and pulls the one
with the highest expectation a lemma
that we proved in our paper that i'll
skip the proof in the interest of time
is that the value of the myopic policy
is always at least one minus gamma times
the value of the optimum and in this
multiplicative factor 1 minus gamma
can't be improved in the worst case
gamma is the discounting factor that's
right so if the principle is very
patient gamma is close to 1 it might be
0 point nine nine nine and then this is
a one over 1000 factor which is not very
desirable if the principle is very
impatient so they're less consider gamma
equals one half this is a principal
who's so impatient that a reward of 1 in
the present day is as good as getting 0
in the present day and one every day
from tomorrow until eternity and so in
you know if you're discounting so
steeply that the present period is worth
as much as the future combined this says
that being myopic is half as good as
being optimal which is actually not so
surprising right if myopic is doing
exactly the best thing you could do in
the present and if the present accounts
for half of all the value you could get
over time okay um so the proof of this
lemma is not quite as brief as I just
made it out to be because you know
neither of these policies is getting a
stationary reward sequence who's
expected value in each time period
is the same the expected values are
probably increasing over time so you
need to do a little a little bit more
work to prove this approximation but
trust me that it's valid so the trouble
is that when the principle is very
patience this approximation factor is
very close to zero and we want to know
if we can do something better to do
something better we're going to
introduce a new feature unto our model
we're going to allow the principal to
pay the agents a bonus for exploring
alternatives that are not myopically
optimal for them so you know in the
simplest implementation of these bonus
payments you might just post a sign that
says you know here you know four arms
won through k here's the bonus that you
would get if you were to pull each one
of them right now that changes the users
decision problems that they're
maximizing posterior expected reward
plus bonus and in these applications
that i talked about you presumably would
not put up a sign that literally says
i'm paying you to do sub-optimal
explorations and here's how much I'm
paying you but you might instead for
example if you were amazon just silently
offer a discount on the you know the 8
megapixel camera that does not yet have
as many reviews as its competitors and
in that way you might hope to accumulate
a more diverse set of trading data
without the users knowing that you were
making them do your experimentation for
you in some of these other environments
like social newsreaders where they're
not actually exchanging money with their
users then you can think of these
payments as being in some kind of
artificial virtual currency like your
reputation points as a user of the
system and indeed you often find on
these systems that they give their users
some kind of reputation points and when
you reach certain milestones you get a
badge and other people can see your
badge and um there's a research
literature that I have not contributed
to but people like my brother are quite
interested in
having to do with how to design these
virtual reward systems to encourage the
maximum amount of participation okay now
in my model where we have this publicly
observable log file of every transaction
that's ever taken place um if the
principal and the users are correctly
doing Bayesian updates on the evidence
in that log file they will always have
the same posterior beliefs about the
arms at all points in time which means
that I'm if I'm a principal that's
trying to get somebody to pull arm I
instead of arm j I know exactly how much
bonus i need to pay them to bridge the
difference in expected rewards between
those two alternatives so an equivalent
description of what the principal is
trying to do is it can adopt any policy
at once for selecting which arm to play
at time T based on past history but if
it makes us election other than the
myopically optimal arm at any time it
needs to pay this much cost to bridge
the gap in the users utility between
doing what's myopic and doing what the
principal is asking okay this is a good
so so that specifies the model of the
problem that we're investigating this is
a good time for me to take a break and
tell you about a bunch of very
interesting related work much of it
coming out of MSR ah that has to do with
models that share the same motivation of
incentivizing users to explore a space
of alternatives for you but don't make
use of this assumption that you can pay
people to pull arms that are not
myopically optimal for them okay so in
the absence of that assumption what
other mechanisms do you have for
manipulating their behavior well you
could withhold certain information from
them so in my model I'm assuming you can
see the full history of every
transaction
happened in the past but in many
recommendation systems they don't show
you every review that's ever been
produced they just sort of say here's
one or a couple of alternatives that
we're recommending to you and you know
maybe here is some small amount of
evidence why we think this is a good
recommendation think of netflix for
example netflix doesn't make public the
set of all star ratings that all of its
users have ever given to movies okay so
in those models an exemplary one being
this 2013 paper of Kramer monsewer and
Perry which with apologies to you shy
who's in the audience I have the
conference citation but i think it's now
in some journal jpe yes it's I thought
so so not just some journal but top 51
they have a multi-armed bandit problem
where the rewards are privately observed
and the principal controls the
information channel by which you know
information about this past history is
funneled back to the users and their
paper for the most part focuses on the
case of two arms one of which is opera
already better than the other and which
are collapsing in the sense that after
you pull the arm once an observer reward
you have no remaining uncertainty about
its type so your your prior collapses to
a point mass distribution the very first
time you pull the arm and in the in
non-strategic settings it's trivial to
design an optimal policy for these
problems but in the strategic setting
where you have to you know give people
advice and they have to be willing to
take your advice it turns out to be
quite challenging to solve for the
optimal policy even in this setting
their papers primarily devoted to doing
that they have some follow-up results on
dealing with a greater number of arms
but then there's this paper from two
years later where you shy Alex and
Vasilisa ganas
did an extension from to too many arms
they allow for much more general prior
distributions over the arms and they
give a policy which while not being
optimal achieves a regret that has the
optimal scaling in terms of the number
of time steps up to a constant factors I
haven't defined regret in this talk but
for non Bayesian analysis of the
multi-armed bandit problem this is sort
of the gold standard for how you
evaluate the quality of algorithms and
in this discussion of related work I
skipped over another important reference
which is this working paper of young
cuche and Johannes Horner two economists
who are looking at a continuous-time
model that's very similar to the Cramer
months or perry discrete-time paper and
obtaining qualitatively similar
conclusions okay um as I said all of
these papers are on mechanisms without
money that try to achieve the same
effect as the mechanism in art paper but
now let me return to our model in which
rather than withholding information
about the past we are paying people side
payments to get them to do things that
are not greedy in the present time step
so to state our main results I need to
define a term called the achievable
region and the way to think about it is
that if you're the principal in this
problem there are sort of two measures
of costs that you want to simultaneously
minimize what is the opportunity cost
relative to the first best policy the
gettin's index policy that you would use
to explore the space of alternatives in
a perfect world where everybody's
incentives were perfectly aligned and I
can normalize this to be a number
between zero and one so if we scale all
of the rewards so that the expected
geometric discounted reward of the
optimal policy is exactly one and the
principal
adopts some sub optimal policy that gets
one minus a then I'm going to call that
number a the opportunity costs and
plotted on the x-axis okay and you know
the other thing the principal would like
to minimize is the amount of bonus is
that it has to pay out to the users ok
so I'm also going to express that as a
multiplicative factor B times the value
of the optimal policy we now have two
parameters a and B that we'd like to
drive both of them down to zero but in
general you can't do that ok and so um
just to make sure the meaning of the two
axes is transparent to everybody let's
take that result that I presented a few
slides back that said the value of the
myopic policy is at least one minus
gamma at times the value of the optimal
one ok so the myopic policy never has to
pay anybody any bonuses so it's at zero
on the y axis and the theorem says that
it's getting at least one minus gamma
times the optimum so that says that on
the x axis it lies somewhere between
zero and gamma ok so that's one example
of plotting a point in the achievable
region that is a oh right i forgot to
define achievability so if a policy
satisfies these two inequalities we say
it's achieves the lost pair a B and then
we say that a lost pair is achievable
maybe i should say universally
achievable if for every instance of the
multi-armed bandit problem no matter
what priors you have on the arms there
always exists a policy that achieves
that lost pair so previous result with
the one minus gamma in it says that the
point 0 comma gamma is universally
achievable and the objective of the
paper is to map out the entire
achievable region and not just its
x-intercept expectation you'll pay more
money random this is it yes yes yes very
important question
both of these should be interpreted as
expected values in a lot of these
problems it would be interesting to be
able to solve for example for the
maximum expected reward under a hard
constraint on the payment right like it
you you know I give you a budget of a
hundred thousand dollars of bonus
payments that you can pay out to your
users and I don't want you to satisfy an
expectation you should never exceed the
budget we don't know how to solve that
problem but I think it's really
interesting one okay all right you know
once we had formulated the problem this
way one thing that struck me is really
appealing about it is that this is a
model that in some concrete sense lets
you plot or depict the exploration
versus exploitation trade-off inherent
in multi-armed bandit problems so I've
written a lot of papers in my life about
multi armed bandits I can't tell you how
many times I've stood in front of a
projector screen saying you should think
of the multi-armed bandit problem as a
theoretical construct that abstracts the
exploration exploitation trade off the
decision makers often face okay but
here's a model where if you want to
explore you have to pay people to do the
exploration okay so the cost of
exploration is very nicely encoded on
the y axis and if you let people exploit
they do something that's in general
suboptimal and so the cost of allowing
people to exploit is nicely encoded on
the x axis and so the shape of this
curve is the shape of the exploration
exploitation trade-off curve for the
multi-armed bandit problem okay so once
I you know like had conceptualized our
problem in that way I became very
curious to know what the shape of this
curve is and our main theorem tells you
exactly what the curve is ok so the
achievable region is the set of all
pairs a and B that satisfy this
inequality square root of a plus the
square root of b is greater than or
equal to the square root of gamma let's
pause and reflect on what this results
tells you about you know this the
incentive dilemma ok so the the first
qualitative take away from this main
theme
is that the achievable region is convex
closed and upward monotone meaning that
if i have any a B and I increase one or
the other coordinate I remain in the
achievable set okay um except for being
a closed set these other two properties
are actually obvious in hindsight if I
have a policy that achieves a B and
another one that achieves a prime B
prime for example I can achieve their
midpoint by tossing a coin at time zero
and deciding whether to use policy one
or policy to the achievable region is
set wise decreasing in gamma as I
increase gamma toward one the set
shrinks this is also consistent with our
intuition that as the principal becomes
more and more patient its incentives are
less and less aligned with those of the
myopic users and so it should become
harder and harder to achieve the points
that are close to zero a more
interesting qualitative takeaway is that
there are certain points even ones that
are not very far away from zero that
belong to the universally achievable
region no matter how patient the
principal is so anytime Rue de plus root
b is greater than or equal to one even
as the principal approaches infinite
patience it still remains possible to
achieve that loss pair so you could
state this more interpret ibly as saying
you know this point one point five
corresponds to the statement that the
principal can always run their system at
ninety percent of the optimal learning
policy while giving back only fifty
percent of the surplus to the users of
the form of these bonus payments and
that holds even in the infinite patience
limit um a final thing that I should say
although I forgot to put it on the slide
is that I told you about the result that
the x-intercept of this region is at
gamma gamma 0 ah
and I said that that's a pretty easy
lemma although I skip the proof the
results is that the y-intercept is also
0 comma gamma as far as i know that's a
hard result and even proving that the
y-intercept is finite as far as i know
is a hard result no no so the
y-intercept means so the opportunity
cost is pinned at 0 so you have to run
the kittens index policy and pay them
whatever it takes to get them to keep
doing what the kittens index policy
wants them to do um there's no reason
that so you keep paying them the
difference between the myopically best
and the best according to the kittens
index are there's no reason that those
payments have to be bounded above by the
value of the arms that get insist
pulling um you know I guess maybe if I
could depict it using a whiteboard
marker that works you have the payoff
sequence of the optimal policy which
presumably looks like this and you have
the payoff sequence of the myopically
best arm which is also increasing it
must eventually increase more slowly if
it was better than this one out to time
infinity then this one wouldn't be
optimal ok so these two eventually cross
each other but initially this one might
be way more than twice as high as that
one might be a thousand times as high so
the gap so the right the the policy that
achieves the y intercept is paying for
these gaps and the optimal policy is
only gaining this amount and so if you
chop at any finite initial time there's
actually no bounded approximation factor
between what the optimal policy is
getting and how much you have to pay
people to play it it's only if you let
time go out to infinity and take
advantage of this geometric averaging
that the approximation kicks in okay um
I want to devote the remainder of my
talk to giving you some sketch of how we
prove this result it's an if and only if
so there's an unachievable ax t parts
and an achievability part and the one
that's easier to talk about is the
unachievable ax t so let me do that
first um there's a particular type of
stereotypical hard instance of
incentivized exploration that we call
diamonds in the rough and this is
searching through a bunch of risky
alternatives that are probably worthless
but have a huge upside if they pay off
when there's an outside option that's a
safe bet that everybody would rather do
instead okay so to link this back to the
citizen science example where you know
like you're trying to get bird watchers
to go out and record observations of
what birds they see in order to collect
ecological data this is like everybody
in Ithaca wants to go to sapsucker woods
and look at the birds there because it's
the most beautiful place to go birding
but you know maybe if someone would go
and sit next to the airport runway for a
while and watch the birds there we would
actually learn some very ecologically
relevant information about the ways that
air traffic interferes with bird
migratory patterns okay so you want to
somehow get people to do these risky but
potentially very valuable bets but none
of them want to do it on their own okay
so how does that work quantitatively
we're going to have infinitely many
collapsing arms each of which is you
know think of it as like a sealed
envelope that the first time you pull
the arm you see a payoff which
completely reveals its type either it's
an arm that always gives you some high
payoff capital M or it's an arm that
always gives you 0 the probability of
giving the high payoff is 1 over capital
M times 1 minus gamma squared
this magic number is chosen so that if
you spend your whole life opening these
sealed envelopes until you finally finds
the one that yields the big payoff and
then you always play that one the
expected value of that policy is
normalized to be equal to one and
there's an outside option whose pay off
as always fee x 1 minus gamma and this
is normalized so that if you always play
that one then you're you know geometric
discounted reward is going to be fee
okay so there are two obvious policies
to pursue here I just told you what they
are one of them searches through the
blues until it finds a winner the other
one always picks the yellow fee is less
than one otherwise the problem is not
interesting and yeah so the the optimal
policy gets a reward of essentially one
what cost does it pay it pays the
difference between this and the Opry re
expected value of one of those and you
know it does that from time 0 to
infinity which gets rid of the one minus
gamma factor okay so there's an
opportunity cost of zero and there's an
incentive cost of fee minus 1 plus gamma
if you work it out for what the optimal
policy does the myopic policy has no
incentive costs and it only gets fee
which is less than 1 so there's an
opportunity cost of 1 minus fee okay and
it's not obvious but it's true that the
achievable region for this instance of
the multi-armed bandit problem in the
limit as the number of these blue arms
goes to infinity is just an unbounded
polygon with two corners at these two
points and everything in the first
quadrant that sits above them and so as
I very this parameter fee I get a bunch
of unachievable point all the white all
the white points lying below this line
are unachievable and as I very fee I get
a bunch of different lines and a bunch
of unachievable points that lie below
them ok so the union of all those white
regions gives me a bunch of unachievable
points the theorem statement that I had
on the previous slide exactly says
those are all the unachievable points I
want to take a second to point out to
you something cute about the form of
this curve so if you look at all these
tangent lines that I derived by varying
the value of fee you'll notice that the
sum of the x intercept and y intercept
doesn't depend on fee it's always equal
to gamma ok so the envelope that you
trace out is the one that you get by
starting with a point at the origin and
one on the y-axis at zero gamma and
letting them move at equal speed until
the y axis point drops down to the
origin and the x1 is a gamma 0 that's
actually an art project that lots of
people do when they're elementary when
they're in elementary school ok so uh I
found on the web somebody who had done
it with string there's the curve I
remember doing this when I was in
elementary school my brother did too and
it's mysterious to me you know like I i
loved the beautiful shape of the
envelope that came out of that and at
the time it was unimaginable to me that
i would someday write a research paper
where the answer to the question was
that curve there are other people
besides me who are captivated by this
curve here's somebody who constructed it
between a fallen tree and a standing one
ok on to the achievability result so so
now I want I need to take a point that
lies inside the purple region and I need
to show that there is a policy that
achieves it and so it's going to be a
proof by contradiction suppose that
rather than the purple region the
achievable region was some other subset
denoted here in yellow I explained to
you that trivial reasoning justifies
that the achievable region is convex ok
so if there's a purple point that
doesn't belong to the achievable region
then there's actually a separating
hyperplane a line that passes through
that point that separates it from
everything that's achievable and that
line has some slope lambda
and so to say that this line separates
the point from the unachievable I say
that this line separates this point from
the achievable region is to say that
there's some instance of the multi-armed
bandit problem where no policy can
achieve reward minus lambda times cost
as large as the value that that
objective function attains at this
putative Lee unachievable point a B okay
so reward minus lambda times cost I'm
going to call that the Lagrangian
objective or I'll abbreviate it as the
lambda objective it's a it's an
objective function parameterize by this
parameter lambda that measures kind of
the trade-off between learning and
earning that a trade-off between these
incentive payments that you have to make
two agents and the value of the rewards
that they reap okay and in our proof by
ok so our theorem that the achievable
region is this purple one can be
reinterpreted as saying that for every
lambda you can always guarantee that
there's a policy whose lambda objective
value is at least some approximation
factor times the opt to extract what
that approximation factor is you would
merely have to calculate you know for
the for the curve rude a plus root B
equals root gamma where's the if I draw
a line of slope lambda I guess negative
lambda where's the point of tangency
what's the value of one minus a minus
lambda B at that tangency point ok I've
done the calculation for you the value
is 1 minus lambda gamma over 1 plus
lambda and our theorem is simply
equivalent to the assertion that not
only for diamonds in the rough but for
every multi-armed bandit instance you
can find a for any specified value of
lambda
find a policy whose lambda objective is
at least this big that's what I need to
prove for you and okay so let me check
in with you fall it's 120 in a minute
will be 120 which is when I was
scheduled to stop talking I've 10 more
minutes excellent um ok I'm planning to
spend less than 10 more minutes ok so so
proving that the answer to this question
is yes is the bulk of the technical work
in our paper and I just want to present
maybe two slides that give you a sketch
of why this is true um so rather than
computing the policy that optimizes the
lambda objective which we believe is
probably a very very hard maybe p space
hard problem we design a simple but sub
optimal policy that we're able to
analyze the policy incorporates two
features that are crucial to its
analysis time expansion and random
censorship so i'll call it the Turk
policy for time expansion with random
censorship ok and how does this policy
work at initialization time before it
pulls ending arms it tosses a biased
coin for each arm and with this
probability marks the arm as censored
censored censoring an arm basically
means I'm never going to try learning
that that arm is the best ok after
initialization in every time step it
tosses an independent coin with bias 1
over lambda plus 1 you'll see why this
one over lambda plus 1 in a second if it
gets heads it plays the next step of the
optimal policy limited to the set of
uncensored arms if it gets you shot yeah
honesty it's a finite number k and
I mean the sudoc the parameter K is not
represented in the pseudocode because
it's assumed to be finite yeah that's
right so when I was informally analyzing
the diamonds in the rough instance I
said that there are an infinite number
of those blue arms but it's really a
limit of a sequence of finite instances
and so the proof of unachievable atif or
the white points requires actually doing
it for every finite number of arms and
taking a limit I okay if the coin toss
comes up tails you just don't offer
people any bonuses and they pull in our
myopically I didn't say it on the slide
but you also ignore whatever observation
came out of that arm okay so your
posterior in this policy is actually not
conditioning on the entire history but
you only do Bayesian updates on the
histories that you saw when your coin
toss came out heads okay so I ok so so
this trick where we interleave the
optimum in the myopic policy that's what
we call time expansion why does it work
and why this one over lambda plus 1 and
the bias so here's a simple calculation
in any step if you look at the expected
value of lambda times the bonus that you
pay out that's lambda times 1 over
lambda plus 1 because you only pay a
bonus in the event that the coin lands
on heads times the amount of the bonus
which is the payoff of the myopic arm-
that of the one that you told the person
to play instead of the myopic up now in
the remaining time steps where you
tossed tails you actually got some
surplus reward above what the optimum
policy on the unscented arms thought it
was going to get but just to say that it
had you been playing this restricted
optimal policy you would have gotten
this payoff but instead you told people
to just play myopically and they got an
expectation a higher payoff ok so
terms of the Lagrangian objective that
surplus when you toss tails exactly
cancels this deficit that you get when
you toss heads because you know tails
comes up with probability lambda over
lambda plus 1 and when it comes up the
surplus that you get in your lagrangian
objective is this thing in parentheses
ok so the time expansion trick brings
about this cancellation we're from now
on we can ignore the bonus payoffs that
we're paying to the agents because
they're in expectation getting exactly
canceled out by these windfalls that
happen when the coin toss comes up tails
um okay so in other words what I've said
is that the lagrangian objective of the
Turk policy at time step T is equal to
the expected payoff that the restricted
optimal policy gets in the same time
step ok now this analysis has a bit of a
something for nothing sound to it you
know we're giving people these bonus
payments and yet I just told you that on
expectation our reward minus the bonus
payment is equal to the reward of the
optimum policy so why are we censoring
arms at all we should just leave every
arm uncensored and always get as much
pay off as the opt in every time step
and then we would have you know we'd be
achieving the magical point zero zero on
the achievability plot ok I so the
reason that there's something for
nothing reasoning doesn't work is that
the Turk policy is learning more slowly
than the output the opp policy gets to
pull an arm and see a feedback in every
time step the Turk policy in step T is
getting the same payoff that the OP
restricted off policy was expected to
get in step T based on the knowledge
state that the Turk was actually in but
this policy always gets a new data point
and advances his knowledge state this
one only gets new data with this
probability and the rest of the time its
knowledge state remains paused so it's
basically trying to
simulate this OP policy but it's playing
more slowly okay so notice the
Lagrangian objective for the Turk policy
is the same as the expected payoff for a
stuttering opt that every time it pulls
an arm it stays on it for some random
geometrically distributed number of
steps and then advances okay stuttering
slows down the rate of learning and
reduces pay off the censorship is
designed to compensate for this slow
down so in other words a conditional on
arm one remaining uncensored I the
slowdown in learning delays the time
when i get to pull arm one for the
hundredth time but censoring the other
arms accelerates the time when i get to
pull arm one for the hundredth time
because it didn't have to wait behind
all those poles of all those other arms
that got censored okay and the
censorship probability is designed to
exactly trade off the slow down with
this speed up and that's about as much
as i can say about the analysis of the
censorship um okay so I'm moving on to
my concluding slide now so in summary
this paper presented a model of
crowd-sourced information discovery the
models phenomena like reviewing products
online or articles or you know
crowd-sourced scientific exploration we
saw that situations like this involve a
misalignment of incentives that has to
do with explore exploit trade-offs and
the analysis that I provided allows a
surprisingly precise quantification of
that trade-off as a function of the
principles patience it allows you to
make statements like the principle can
always achieve at least seventy-five
percent of the social surplus while
paying back only twenty-five percent to
the users the structure of the Turk
policy lets you say that you know simple
policies that randomized between just
letting people do what they
hunt or providing incentives for them to
do something optimal are simple policies
like this are sufficient to achieve the
approximation guarantees in this bullet
and the analysis of the lower bound gave
us some insight into what the structure
of the hardest incentivized exploration
problems it looks like and so this was
going to be the last night of my talk
but yesterday I was waiting in the
overlake transit center for my bus and I
saw a quote from Mahatma Gandhi that I
had never seen before uh so he said live
as if you were to die tomorrow learn as
if you were to live forever and I hope
you can see the relevance of that quote
to this talk but you know maybe you can
also see that it uncharacteristically
for Gandhi you know his his wisdom was
sort of incomplete here he was
overlooking a fundamental trade-off in
life that would we learned by living we
learned by living so you can't adopt a
different policy when learning from the
policy that you adopt when living with
dolly the randomization ah yeah so
that's right so replacing Gandhi's Maxim
with the much more wise you've all Perez
Maxim when he should have said was live
and learn as if you were to die tomorrow
with probability 1 over lambda plus 1
and live forever my probability lambda
over the end of this one and with that I
conclude and sorry if I understand
totally then for this bubble to really
make sense in fact is it very hot but
everything this happens before with
public right so as an Amazon user in
principle according to you having
particularly I have to see the whole
history of all the thousands of
thousands of reviews does it not but all
the discounts that you've replied
through this day right sure because
otherwise you wouldn't have to see that
because if we all have a common prior
and you have a correct belief about what
discount policy i'm using okay then you
couldn't i compute for yourself what
discounts we look like but
wondering whether there's a total two
factors as British you have better when
we were there's any sort of middle
tranza where you know you could be
publicly announced for doing X which is
quite help moving but it's very easy for
girls done why'd you care about the
discount I mean you don't care why the
person's incentivized or even how you
just care about you you get to see the
results that's all the information there
is about about the arm you know actually
care why people Djokovic signature
Minnesota like this so given a and B
which is in a feasible region how can I
find a policy ah it's risky sorry are
you prison are you are you portraying
that as a modification of Anders
question or are you just really it's
those things make you want to do with
like a fly the factors but I just want
to say is it clearly easy to see them
only one possibly two merchandising I
wanna fight yeah so no given a MP it's
not easy to define a policy that's in
the jewelry achieves a TV what's easy to
do is given a lambda i showed you how to
get a simple policy who's lagrangian
objective achieves 1 minus lambda gamma
over lambda plus 1 approximation to the
optimum um but the business of achieving
a specified a and B depends on knowing
more about the parameters of your
multi-armed bandit instance so the camp
simply be it can't be described as a
simple transform of restricted optimal
policy and ER I think your question is a
good one and a perceptive one I don't
really have it I haven't thought about
the question enough to say anything of
substance so I think the one thing I'll
say in response to your question is that
I object that I was here for an entire
morning of the algorithms for technology
transfer workshop and I did not once
hear a question that was prefaced by you
know your theory is all fine and good
but bringing it a step closer to
practice good
okay I think we have to move on so sank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>