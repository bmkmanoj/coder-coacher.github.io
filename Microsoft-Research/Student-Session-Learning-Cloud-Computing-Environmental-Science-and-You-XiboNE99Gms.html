<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Student Session: Learning Cloud Computing, Environmental Science, and You | Coder Coacher - Coaching Coders</title><meta content="Student Session: Learning Cloud Computing, Environmental Science, and You - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Student Session: Learning Cloud Computing, Environmental Science, and You</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XiboNE99Gms" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
you
we go so let me introduce myself first
my name is when flipping ye and I am a
senior program manager at the microsoft
research connections so I have been
working at Microsoft for about five
years so it also i would like to make
this session fairly Interactive's if you
have any questions you're very welcome
to ask them and you can ask them in
chinese and i can i'll repeat them in
english so there's no problem there's no
problem there so ask me any question you
like the second thing is that i am
actually from boulder colorado so i went
to high school college and graduate
school there so this is basically in the
middle of middle of the country in the
US so what's also interesting about Olga
Colorado is that it is the whole to a
lot of environmental agencies and
government agencies they're one of the
most notable ones are the ones that I've
listed here the University of Colorado
we have a lot of environmental science
and also organizations that are funded
within within the university and then
there's a UNIVAC so that particular
organization is funded by National
Science Foundation and what they do is
figuring out their physiques of the
planet earth and they also collect large
amounts of data the ones that I work was
but a bit is you car university court
for almost pure research there's also
something called national center for
arms on the sphere research that's in
Boulder Colorado um so these are the
guys that do what
forecasts and they have quite a few
supercomputing centers that they own
there's about a thousand people there we
actually have a lot of researchers from
China that business them very often and
the other places called Noah stands for
National Oceanic and Atmospheric Oh
agency and the this particular agency
does also does a lot of weather
forecasts and ocean studies ocean
currents so a lot of the environmental
science done there then there's the
neural the corner L stands for National
Renewable Energy Lab and is about ten
miles or 20 kilometers away from Boulder
what they have is really really large
wind turbines so these study different
types of renewable energy and then
there's a max big maps is one of the
largest organizations at Microsoft jerz
currently I think three or four hundred
people there were like located right in
Boulder Colorado so what we do there is
take fly over data or we use these
really really high resolution cameras we
put them on
20 and we go around the world and
actually do certain passes and get
really really really high resolution
images so a mass has an ordinance with
160 petabytes petabytes of data that
they've collected just you know he spent
about two or three years just flying
over the United States and now they're
doing Europe and other places and
there's last sorry last stands for
laboratory for austral space physics and
it's part of NASA and it's especially
within the university so there's a lot
of environmental agencies by where I
live and actually so my wife Ashley used
to work for some of these organizations
and this is where a lot of the
environmental science is done I think I
missed one which is Aniyah on this sense
for national ecology Observatory network
so they look at Ecology's and they
collect huge amounts of data so they
also have a lot of collaboration with
Europe and also I believe of China as
well so that's a my reef introduction
and here's the agenda when I look at the
armlet or comet challenges so some of
the things that are becoming really
important to our ways of life and then
there's the white cloud computing can
actually help to solve these really
challenging problems that are getting
harder and harder to resolve in here and
I have some common cloud computing
scenarios and
so those are the things that
architecture is that you will be using
that are very common in the cloud so
there are certain things that work
pretty well in the cloud there are other
things that is not yet optimal running
in the cloud so I'm going to help you a
little bit to understand and look at
that large-scale compute and also big
data in a more you know practical
fashion and then we have some hands-on
materials that you can download have a
link at the end of the class which you
can download we also have a two-day
training class that starts tomorrow so
is any of any of you guys signed up for
that class anyone oh you're not okay
well I have the content here so I'll go
over some of them so you might be able
to use these samples to start working on
projects at relevant to year so there's
also we have a Windows Azure RFP program
where we're giving away huge amounts of
Windows Azure hours I think it's any
orders with have a have a million four
hours equivalent of storage so it's
worse a very significant amount of money
and we are starting to do about a
hundred of these projects and from what
I've seen is that till we have fantastic
researchers here in China and we would
love to see some innovation and
collaboration with
research to really think about
interesting ways of using the cloud that
no one has before so let's go to the
next one so I am a I would call an
enthusiast of environmental science so I
like environmental science at work with
a lot of environmental scientists
however I am a you know computer or a CS
computer science major by training so I
do not do science necessarily but I do
work for I used to work for the
Department of Energy as a one of the
subcontracting company so I do a lot of
plasma physics and particles cell
simulations to very similar to
yesterday's keynote essentially really
really large amounts of physics data and
do parallel simulations of them so here
are some of the issues that we currently
see and everyone sees that it's all over
the headlines in newspapers so I picked
up a newspaper and the headline is air
pollution right so a lot of these
problems are becoming harder and harder
to solve so there's the air pollution
and also there's over fertilization
their soil pollution a lot of these
problems are becoming won't bite so in
fact in California there's you know
there's a significant
soil pollution because of the over photo
edition and then those fertilizers going
to rivers so it's a problem worldwide
not just necessarily in China and also
there is a waste and toxin management so
that includes nuclear waste and other
types of the voice that are toxic in the
landfill and they get into groundwater
so there's a lot of things that are
happening that we don't yet have the
control over so these are I think very
big challenges which I'm hoping that
environmental science to take off and
we'll fix them and then there's energy
and natural resources I call them the
energy security oh by the way I call it
environmental debt what that really
means is that when I when we write
software I'm a tougher person by
training when you write bad software we
call them technical debt meaning that
you are not paying to your owing someone
the good work right so so this I call
the environmental debt because of all
the solutions and problems that we've
created because of this healthy economic
growth rate but it is necessary you know
for people to get to certain Linux
dangerous
and pollution is becoming a very large
problem so the second thing is really do
Energy and Natural Resources security so
yesterday there was a headline saying
that China will become the world's
world's number one oil importer so the
fact is that china is what you produce a
lot of oil itself as during port a lot
of it and the rest it has to make up by
coke so there's 3.7 billion tons of coke
being burned every every year so this is
actually a dinner conversation that I
had was a friend of mine who is who is a
see who works for one of the co agencies
in China I think he regulates the
production and one of the regulation
agencies for a production of CO so some
of the things are fairly interesting so
in the US every year there's about on a
third amount of coping part which means
there's significant amount of the air
pollution if you've earned that much Co
so we actually have an example which we
can show you're given the pollution
pollutants meaning if it's you so2 or
any type of pollutants you will be able
to figure out what type of source where
the pollution source is coming from so
it's for example could be coming from
diesel it could be coming from burning
coal or burning wood so there is
different types of properties when you
earn certain
type of materials so and also there's
natural disaster so earthquake and
hurricane so we we I lived in voters so
we just had a flood i think it was
making national headlines so what
happened was i lived here for 20 22 23
years it's a very dry place suddenly
there was 20 inches of rain down on in
boulder at once so you walk look down
the street is ocean so basically you
couldn't even see the streets so every i
think most of the houses were actually
flooded in voter there's actually
several people that told me that it went
to neighbor to make a phone call and
when he came back the house has gone and
we're just will got washed down the
street so so the natural disaster is a
huge risk in that and have you do
predictions of these problems so so
these are the challenges that I see are
very important for us and we might um so
how might a cloud computing view that
help us these types of problems one is
that
so one of the things is that we Kyle
computing allows you to scale out on
demand meaning that a lot of these
really large problems cannot be solved
on your desktop computers anymore so
normally you would bring up a
workstation open that love and love that
but the problems are becoming larger so
there is a lot more as being being being
ingested so you can't really just say
okay I would like to ship one petabytes
of data to my laptop and we'll let's
analyze that so all the end of analysis
has to be on centralized server the
other interesting thing is called he
always on and reliable computer you
always on so I had machines that are
running for 23 years that you know I
deployed a service and they have been
running there for 23 years I haven't
ever shut it down so it's just running
there so anything that happens to the
hard work under underneath anything bad
happens all that is taken care of for
you so it will be moved to good hat work
if something happens to that work so we
have some demos of that and how that
works another interesting thing is the
abstraction for hardware and IT
infrastructure so for example I have a
service that's running in a cloud right
now has been about two years and I don't
really know what hard work they have
changed is significantly faster now but
I don't have to worry about that because
so normally when you run a program
shut down the server install the
operating system get it on you know come
to the network again reinstall yourself
or it may be a different version so all
that is essentially taken care of for
you within that stretches so if you
think that cloud is expensive or cloud
is slow at the moment six months down
the road it will be twice as fast and
twice as cheap so there's a significant
amount of resources being devoted to
cloud computing so there's also a lot of
competition so it's going to become
cheaper and more economical than
building your old servers and also
centralized compute and data storage so
for a university probably have no fast
Network for companies may be right but
in the cloud we have some of the best
networking and the many also you know
many other commercial vendors also will
have that too what that really means is
that you would be able to connect at
hundreds of hundreds of times faster and
it's always going to be running their
right and convenience that collaboration
is also important you can share data and
you can run live simulations and you can
share these instances on the cloud so
that others can run your simulations
let's go there so this is actually one
of the very very like a one quarantine
so one of the smallest machine and I
just this morning I just ran this so in
my hotel I get wait maybe one megabit
IGAs I think it got two maybe three
that's the best
it goes and then you look at that
running out of 14 so if you run an
eight-course it actually double or
triple the amount of speed so it would
actually go to almost 10 times as much
speed if you have a machine with a lot
more course so here this is going at a
hundred times faster than anything you
have so that's why you really will need
cloud computing because it's always
going to be there it's going to be
reliable right so it works very well for
streaming type of applications so for
example if you have internet of things
or smart building sensors that are
streaming data in of you would not want
you to have a server sitting your house
probably not so you really want
something along the lines of maybe it
may be a little server in the cloud that
streams of data and do some of the
process in there okay so here are some
of the common cloud computing pattern
scenario has been havent so this is just
from my you know personal experience
from working with customers so I work
with both commercial customers as well
as customers that are the science and
research community so here are some of
the common patterns one is the
workstation in the cloud so for example
if you just wanted to take something
that that's runs on your desktop you
want to put it on the pile it works
perfectly fine so you just wrote a
remote terminal service and you go into
the cloud and you can run
you are pretty much anything you want
you can go on that machine and install
anything you want turn it off you're not
paying for it once you turn it off and
when you're using it again turn it on so
that's one of the scenarios and the
second one is to always down Kyle
service being at once you did the boy in
cloud service it could be a website plus
some compute in the back end it's always
going to be running because we guarantee
certain levels of SLA or service level
agreement so we'll say oh it's going to
be 99.95 so for every year it will be
down only four hours or you get some of
your money back right so that's the kind
of things that you can't really do
yourself and even you know some of the
companies they cannot do that themselves
and here's some other scenarios there's
some HDC so how many of you actually do
hbt or high performance computing MPI
applications not very many right okay so
HP see what that does it does numerical
modeling so you usually have these non
linear linear solvers that you have to
run right using MPI so there's a lot of
communication between different machines
in a cluster so that doesn't generally
work very well in any of the public
clouds but we are going to be having a
very fast interconnect so 42 pts per
second interconnect with low latency so
those however are going to be in cloud
in the near future the second thing is
that clustered parametric sweet meaning
that you run a program higher times so
you can run it so I have a weather
simulation
as a weather forecast I can run it on
under cities right so because each of
these unit of work is it independent of
each other right so that's how how that
works and it works very well because in
a cloud you can pretty much scale out to
as many machines as you like if you know
that your account of the house it and
then there's the big data processing so
in big data i divided up into three
categories one is batch processing so
meaning that you're using the hard drive
on each of the computers to do the
process and it can process a huge amount
of data so essentially a jew is called
batch processing so and the second one
is interactive type of compute so that
would be something called high or our
shark what that means is that you can
run see poor light flurries against
really large amounts of data and it will
be turning that into a MapReduce call so
underlying it will be turning it into a
MapReduce call and then there's the real
time processing so for example if you
have a lot of sensors that are streaming
data in so this would be a great way of
doing things so there are tools that can
do these for you so I have a demo here
which I you know wanted to show you guys
actually have it running live so what
it's called is the weather research and
forecast is the classrooms so every day
you watch The Weather Service your dream
and how do they know that in three days
it's gonna rain or not so they have a
finite-difference model essentially they
have data points that they've collected
at certain point and then they run a
finite-difference model too
the prediction so this code is really
old is probably 15 years old and it's
called w RF probably some of you have
heard of it so what this does is it does
assault on really large matrices and
then what you do is we can take that
result and plug on Big Macs so and one
of the things the same thing is that I
can run a hundred cities all hundreds of
cities on the same dodo so especially
I'm running these simulations on a
course it takes about eight hours so the
service I have been running here it has
has not died after about two years so
people have one about 2,000 simulations
on here and each of them are about eight
hours so that's a lot of simulations and
also have a lot of ADA that my
simulation produces I have 13 million 30
million objects that are sitting in the
blog search so that's a lot of data and
I would normally not be able to score
that data so so it just automatically
scales that for you also have something
called platform-as-a-service so the rule
of thumb if you want to scale you if you
don't want to manage your machines you
want to keep everything stateless so
stateless means you don't install any
software on your machine afterwards so
you don't keep any state so anything
that you did it's going to get lost
afterwards so what you do is you can
automate that process and creates
thousands and thousands of machines that
just does one thing or you know does it
sir
but you're not going to be keeping
hydrides with these machines so anything
that happens to that machine it would
just get rebooted and it will be
installed he'll be the installing
software that you asked it to initially
and it will keep that particular State
okay so for example if I have a machine
that I have matlab that I want to run
right so in the beginning when I
provision it when I start this machine
it will install matlab and if something
happens to that machine muster say that
word is broken all you will do is to
deploy it on to another machine and it
will be exactly where it was so it will
it will install the same amount of
software on that machine so the nice
thing about it is that you don't have to
keep hydrides around to attach to them
right so let's go get this particular
demo so the first thing is that it does
a forecast so you can also look at the
website it's called Weather Service the
cloud app.net so it's actually being
running live so I can actually go there
and show to you so let's go through
these slides for first first part has
been about two thousand two simulations
so far and what i can do is i can it can
run a forecast on one city 180
kilometers binary plumber it will take
you about seven hours and then you know
it will do a forecast for three days and
you can do up to four up to about eight
days so the more days you predict it's
going to be less accurate so I just put
down three days as a default so you can
just go on that website
the forecast and here's how actually how
it's done so this is actually the
website we're going to look at the
website after I finish the slides here
so what is that it shows you it can show
you all the forecast at we have done and
you can zoom in and then you can see the
animation because the animation over
being mad and i also have a gallery view
which shows you the snow and temperature
data and it's animated gift they're just
animated gifts so so this is the
architecture first first of all when you
want to do a wetter tradition you have
to have some boundary data so that
boundary data is actually from NOAA so
just in a reminder nor stands for
National Oceanic and I'm skirt reserve
agency so I get the data and then what I
do is I keep them in the blocks or so
every six hours this data source of she
gets changed so it's very simple I just
copy the data from the FTT website from
NOAA and the next thing you do is you
run the simulation do you run a
simulation against that data set and it
take you about eight hours to do it just
very compute intensive work and then
once it's done and it will take all the
information and store them in a blob
storage so it's so we do we get the data
we view the computation and we keep the
processed data into a blob storage and
then we just show them on a web page and
that's all there is to it as you can see
it's pretty simple process of course
there's a few more things that happens
in between and this is the windows as
report
can see that I start running a job it
will have monitoring decals to about
ninety percent so and I have running I
am running about 100 quarters out of 128
course in this particular account that's
available and you can get up to probably
a few thousand of course if you really
wanted to the other interesting thing is
that you can look at the the instances
when you're when you're not running them
you can just scale them back you can
just move a little slider and it will
just use less course and that's how it
works so here's the little slider so
right now if I wanted to do weather
forecast about twelve cities I just
moved a slider and that's all I have to
do so what i will do is will gold spin
up those machines it'll start those
machines and it will run the forecast
for you and here's also auto scaling so
what that means is that if I my cpu is
busy one of a few of these machines it's
going to start running the rest of them
so it's going to add more machines if
you if the first few machines are really
really busy so there are a hundred
percent and will spin up a few more
machines here's how it works um so you
can see there are three types of the
machines here one is the head node which
dis controls the cluster right and then
there's the front end which is the web
page that you see and then there's the
compute node which is eight cores each
so these are really really large machine
they have 16 megabytes gigabytes around
and they have eight CPUs a cpu cores on
them and then you can just move your
slider to add more or remove machines
you me too so this is a screenshot of
the Autoport that are running so you can
see in this particular instance we're
running at ninety-eight percent or
ninety nine percent I mean that's a lot
of computing intensive work so let's
just say any of these machines by I
don't have to worry about it because
we'll get reprovision automatically so
we'll go to another hard work another
piece of backward and it will redeploy
them on to that machine and then it will
just pick up the work outwards so the
nice thing is that I'm not keeping
anything on that machine's hard drive
and I'm storing all the data back into
the blob storage which is centralized
storage service and I can easily copy
data 100 under 30 megabytes per second
so if I want to copy data into the bulk
storage into to storage I can do about
130 if I do it properly all rights
between 100 bag 24 it is very fast in
terms of being put on that so if you
have a few other machines you would be
able to run it even faster you know you
would do any move faster so for things
like a dude would be a full data from
multiple computers make it even faster
so let's actually show you at them home
so I have a machine running here see ya
so I have a machine learning here so
this is live by the way so if i go to
the home page there stood there's the
home page there I have a gallery view so
the gallery view basically shows you all
the simulations that have been done and
I'm going to move it into my dear
so let me do that again so I can go to
my homepage right so there's your
homepage it is a little gallery and the
URL is weather service cough net and I
can go to the gallery view so the
gallery view shows you two recent cities
that have blended forecast so I ran one
on Lhasa and a few other cities in China
right so you can see that that was from
yesterday so I can essentially become my
own you know whether man I wanted to
obey so then I can click on one of them
and I can animate it at much higher
resolution so let's look at the
temperature so you would probably think
that last thought would be really cold
at night so that's specially animate
that a little bit so it does actually
gets to be a fairly code on the
outskirts you can see the color map
there right so it's a little jumpy
because of the internet is not very fast
year but essentially moving ahead of
animate a lot of these process because
he actually gets to that maybe even the
low zeros in some of these places so let
me go back and you can see a cointreau
is really really hot right you can click
on it you see can I click on it
alright and that work is a little slow
and the other interesting thing is that
what you can do is you can so as I was
moving the slider just go to compute
instances I can also move that for the
web bitches instances so the webos is
that I can add more so let me click on
the play button here it's all JavaScript
this is all being mapped by the way can
see Guangzhou is really really warm it's
but I don't know 70 degrees art is
Fahrenheit I think it's about 20 degrees
or so so you can see that it shows you
certain places are really warm and some
other places a little colder is probably
because the geolocation of Guangzhou so
there you go so this is how the system
works and I can also go back and look at
the gallery view again so I can go to
gallery and I can't click on things like
math
oops
wait
so what I'm trying to do is try to click
on one of these icons where it says
timeline voices timeline and Max so I'm
having a little bit of trouble pointing
to it because as you can't see it on my
screen here so you see if I can click on
that a little bit here it's not quite
working
so what it will look like is basically
showing you the yeah there you go so
what it will actually show you is this
particular beer i'll show you when these
simulations are runs you can browse
around the globe and see all these
thoughts and click on them and also you
can't browse by timeline let's just say
the last five to ten hours I ran 50
simulations you can see that it ran 50
simulations so I think the problem is
that it's actually the because with the
keenest you're coming okay great I move
this a little bit we're going to
duplicate this finger
this is ie so ie ie works a lot better
for this particular demo so I can go to
gallery here again and I can go to map
and shows you all the US relations that
are running so you can see now right so
there's all the simulations that people
ran it's not necessarily just two people
from Microsoft there's a lot of people
that ran simulations so this was all the
ones that ran in the last few months the
other thing can do is look at the
timeline so here's also a little bit of
time line here
yeah still taking a little bit of time
to look so you can see the bigger dogs
at a time where somebody ran a lot of
simulations so today we ran all these
you know simulations for all these
cities but if you click on one of these
other days that there was nobody doing
anything right so then that day suddenly
ran and on one two three three cities
and then you can see you know this
particular day people read it on a few
other different places so it gives you a
history of all the simulations that has
run out of your service okay so that is
the demo for running a cloud service so
some of the things to remember is that
one it's tailless meaning that i am not
keeping anything on the vocal high drive
so if i'm moving this slider it just
means i can spin up more instances right
and the other thing to remember is you
can add any combinations of compute
resources you can add multiple websites
you can add multiple compute nodes so
it's just simply you know a
configuration of a configuration file
change so we went through all this and
let's look at the file copy we did we
did I shouldn't do that so I have
another example here which it actually
ran a few years ago 23 years ago so what
this does is it does a an analysis of
the pollution source as I mentioned
earlier so for example if I detect there
are certain amounts of pollutants in the
air so I can figure out where you're
coming from using a color
negative matrix factorization it's a
fairly standard EPA code so this is
essentially scaled out you can see that
I have a one machine here interactive
shell I have a little controller and
have all these little compute engine is
all done using Python so you can
download ipython notebook somewhere if
you via I'll be what if you like we also
have a lot of these these things in
trainings you can learn about them from
the training materials so here's how it
works one is that you have an eye so
essentially here's your PM 2.5 so if you
have too much of it basically it will
cost a lot of death so they actually did
one in did the study in denver denver is
a city that isn't badly so basically
pizza valley so there's a lot of
pollution that sits on top of it so what
happens is that when the pollution is
really bad there's people that goes to
the hospital so they want to find a
correlation between you know when the
pollution is bad versus how many people
actually goes to the hospital that have
lung issues and dessert second thing is
they want to find out is if you want to
know what pollution source this is
coming from is it coming from the power
plant is it coming from too many cars is
it coming from you know diesel diesel
engine running or is it also it could
also be coming from forest fires because
you know Conrad attends to have forest
fires and it will have you know these
smokes it positive
and it makes the error we're pretty bad
faith so there's also essentially it's a
matrix Petra trituration that you can
run from EPA so here's the the positive
matrix factorization rate so this is one
of the laptop this one's on my laptop so
it takes about an hour and a half to be
able to run so taking the same code i'm
using 48 course so instead of two cores
I'm use 48 course on the other Asher
chain and I ended up having about 40
seconds so it goes from an hour and 45
minutes on your laptop to about 40 40
seconds so you could imagine that I
could take these measurements as they're
coming in and I can run these analysis
and tell you how much diesel pollution
there is how much how much is from
forest fires are burning wood or how
much it is from ashley from just
gasoline engines right so these are the
things that you can pretty much figure
out it could also be we also have a like
a oil refinery so we have an oil
refinery that also is being regulated by
the EPA so if the file the oil refinery
is putting out too much pollution they
may shut it down for a while so this is
the way they do this calculation so
basically what we did is simply take
this code and parallel that's it I using
Python and then the runtime goes to
about 40 seconds right so that's an
interesting thing so the second thing I
want to talk about here is that those
people no sequel here so no sequels and
for not only sequel meaning that you're
doing a structured data so the first
thing the first property and actually no
sequel kind of goes
with big data so essentially it's non
structured data that's not you know data
that you store in your sequel server and
then it's a structured data you can have
Jason Jason MCS be key value pairs is
the big thing about a dude so it has
everything has key value pairs so you
have a key and have a value it's pretty
simple but sometimes you might ask me
how do you have a table when you have to
ski down two pairs well that's pretty
simple because you can have multiple
keys so let's just say if you want a
table you can have an X and a y as your
key and then you have a value right so
essentially that represents a table in
fact this is how HBase works so HBase
it's on top of to do and it rip this
table structure the second thing is that
you would be able to scale out in the
cloud to overcome I've come you know how
to limitations that you have about
Prentiss so the feature of big data is
really true beauty partition though on
too many many machines so you can have
hundreds of machines that have 20 Tony
high cards in them and you can run them
in parables using that this processing
speed to do it so each machine has
assisted 2020 disks and you multiply
that by 100 megabytes per second you're
then you multiply by a thousand machines
and suddenly it becomes very fast then
the other thing is the scale out in the
clouds would be cloud allows you to have
multiple machines you can a thousands of
machines without having to purchase
first right and you can turn them off
when you're not using it and the second
thing is that commodity meaning that
they're really cheap had word or not you
know
network storage systems that cost
millions of dollars these are just
regular machines that you put some
hydros in them and finally they have
very high throughput okay so you
basically we're using the i/o from
multiple machines that have multiple
hard drives so that's the idea of doing
big data and no sequel so you really
utilize many many machines to work on
the same problem and the reason is
scales is because it doesn't have a
schema right doesn't have a schema and
it makes it easily dividable into
smaller chunks of work that can be
worked in parallel right okay Hadoop is
for big it is how many of you actually
used Hadoop or nobody so far okay how
many of you want to use it or think
you'll be interested and still nobody
come on somebody's going to be
interested me but anyway I will let you
know you guys are just being shy the
first thing is that it's a distributed
batch system so what this does is a lot
of these jobs from for a long long time
the second thing is it uses MapReduce
paradigm which is really really simple
simplicity really scales meaning that if
you want something to be able to scale
linearly you you want them to be simple
so MapReduce is a very simple protocol
so it takes a huge amount of work by
adult onto each of the machines and let
them run so we have some nice little
animations so one of the applications of
big data is concept called collective
intelligence so suddenly you would have
everybody's information all in one place
so in the US we have facebook and you
know in the here you guys have one of
the
network that's equivalent I think I
heard run unoriginal things like that so
basically have a place where you have
everyone's data they're all in one place
so you can do a lot of interesting texts
one of the things you can do is to
collective intelligence so when you have
all everyone's data in one place you can
figure out consensus you can't figure
out what people are trying to do or
another example is Twitter they have
great um another thing is predictive
analysis so given certain things are
happening what are you know what what's
going to happen in the near future given
the data that we currently have so for
example a big one of the big 50 no
problems that we're trying to solve here
at Microsoft is to predict something
along the lines of when a when Windows
Azure is going to possibly have a
problem right so you can fix it ahead of
time so we may look at the data and say
okay we have the most trouble maybe at
eight eight o'clock in the morning what
this company is running really really
large amounts of data in this data
center right if that happens we could
possibly change the parameters of our
cloud services to meet that demand so a
lot of that it is going to be very
useful when it comes to you can have the
data in one place so here's how to
Hadoop works after two layers one is the
MapReduce layer and then the second one
is the HDFS layer so HDFS stands for
distributed file system so this is what
I was telling you about essentially you
have these really commodity hardware
which each of these have word has lots
of hard drives and them hey and then you
can build it fast that's sitting on top
of it but it has to be redundant what
happens one of the machines dies so it
keeps three copies of the same file in
one place and the second thing it does
is that when you do a right who is she
right altamonte once to make sure that
it guarantees that you have three copies
at once so and it keeps two copies on
the same network and it keeps one in a
remote location on a different track
right and then there's the MapReduce
function a MapReduce is pretty
straightforward all you're doing is you
are doing a scanner and gather right so
you're taking a large amounts work and
dividing it up to let most polka sheets
to work on it and then you collect them
back together so that's how it works so
I call this dis dis is the assembly
language you know the most basic role
level language of MapReduce oh okay so
here's how it works the first thing is
that you know you take order so there's
there's a bunch of data that's
distributed out all these servers and
see we have a piece of code so the most
important thing to remember here is that
you're moving the code to compute so I
don't want to move data around because
the data resides on each of the machines
as they are and when I want to do some
computation I would take a piece of code
and the scheduler automatically figures
out where the data is put for that
particular job and it will go and work
on that so it doesn't have any i/o that
happens so that's what that produce is
actually working it's called moving the
code to where your data is hey um so the
second thing I want to talk about is the
something called interactive big data
analysis so what this does it allows you
to run scripting language that looks
like she bolted at the sequel language
so Spartan shark is actually a Berkeley
project so that came out of working and
I would encourage you guys to go take a
look at that if you're doing a large
data analysis one of the things that's
really nice about
is that it has a memory data storage via
the Brazilian distributed data sets it
can be searched times faster than the
view the reason it can be that much
faster is because it caches a lot of
intermediate state in memory so it
doesn't do a computation keep everything
to the hard drive and then read it back
in and do the second iteration so what
it does it keeps a lot of the
information in memory and then there's
work to make it redundant so you're not
going to lose any information when one
of the machines go die right um here's
how it works so MapReduce is very
iterative right so into one MapReduce
and then it gets feed into the second
step and the third step so here is the
animation one is that it doesn't write
too well this is actually the original
diagram when you run MapReduce it goes
does the computation right sit down
storage and then the second one and then
writes to storage again but if you use a
working Sparkie why she catched a memory
so that it doesn't have to do that I oh
um and then just that's why it can be
about 220 certain times faster rate or
sometimes it will say 10 to 100 times
faster if it's done properly and oh
here's something that's interesting this
is called something called hive and a
shark what this does it allows you to
run this looks like sequel right this
looks exactly like sequel so you don't
have to learn how to build MapReduce in
you know Java code which is really
really long probably all you have to do
is to open a command prompt and run
type of queries I'm good data but
underlying it has the execution engine
that will build an execution plan and it
will still do map reviews for year it's
just that it can't do it much faster and
here are some of the operations that you
can do just giving you an example of
them so these are all standard sequel
code and you can just run of course it
has it's a subset of sequel so it
doesn't have all the functionalities of
it so there's certain joins that you
cannot do so here's another interesting
thing called the real-time streaming
across the stream processing so this is
becoming a very popular topic to talk
about it's related to Internet of Things
and especially sensor data that are
coming in I've actually built an example
for you in the training kit which you
can use to start doing your own projects
we have a link at the end to this thing
is it's developed by used by Twitter and
the reason you might want to use these
software is that has been tested in
these production environment by these
really really large social networks
right so someone has already built this
software and has tested it properly
right so it's running in production is
fault tolerant so you don't want to
write your own probably and it can't
scale so meaning that if you want
there's more tweets coming in or there's
more data coming in
I want to do processing in real time
it's not a problem I can simply add more
computers to it so that's a very
important it's called horizontal
scalability it's also a fault-tolerant
as I just said there is a fail over
mechanisms at them so I'm not going to
go into too much details with this
because we have about 20 minutes lapier
the the other interesting thing is
something called a kafka which allows
you to do message passing and I'll go to
that later so storm is dead underlines
underlying a scale engine so as you have
streams that are coming in so these are
just data that's coming in so we have
these structures that are meant to work
together one of them is called spouts so
essentially it used a source of this
dream so for example if we had sensor
data that are coming in it will be going
to that about right so it will be taking
that and streaming it and then there's
bolts think about a sink of them as
workers so a spout can be fed into
multiple bolts and they can't do filters
aggregation drawings into DB read and
write so what happens is that you have
data coming in and these workers can do
different things to them so you can add
a bunch of numbers or you can write them
to a database so this is how Twitter
works so it might want to find out you
know how many people are tweeting about
specific topic in real time and then it
could also say okay I want to store all
this data right so there are multiple
things you can do using different
workers so Oh went backwards so here's
how it works you have a little source
that's coming in and you have workers
that pic work out of them so you can
stream that
together into a network essentially and
here's something that works was a storm
the way it works is it allows you to do
streaming in real time is developed by
linkedin so it works in a combination of
storm right so here's something
interesting is that you can send almost
was a two hundred thousand messages if
you wanted to for a second to a true
machine just like one machine to a
cluster that has just more machine so if
you have multiple machines you can
handle millions of the messages using
this combination meaning that I can't
run hundreds of thousands of Stan sirs
uh sensors that sends data into the
system using the message protocol so
their way it what work is that you
implement a Kafka client so essentially
it's a messaging protocol messaging is
pretty simple you do sand and you didn't
and that you do just oversee but and
then you can use storm as a base or it
should be able to scale so we actually
have samples that you can run yourself
that takes in gps data and bought them
on a map and you can also store them
into a database and do all kinds of
interesting things with it oh so this
would be a very interesting project for
people who want to do data ingestion and
want to do some real time data
processing so here's a very simple
example all this is doing is that i
built a Kafka storm combination and then
as the data as the data is coming in to
conceal years ago
modeling for the client so i can send
probably four or five thousand messages
easily to it using one client and then i
can scare it out to many many different
clients hitting it from different
machines and it can't handle up to a
hundred thousand messages in this
particular instance so if you have lots
and lots of data that are coming in and
you want to do real time processing duty
aggregation first and start up in the
database this is a really good piece of
software to fewer the years so here are
the possible applications one is that
you know you when you have a sensor
network you really want to keep these
machines running you don't really want
them to go down right otherwise the data
is just permanently lost that's why
cloud computing works so well here is
because we have much better network than
anyone else that's out there and the
Internet of Things so so regimes fell
asleep I know some people killed sleep
but machines definitely don't sleep so
it could be sending your messages any
time so we really can't afford that
downtime to be able to do things and
also yeah I could have you know 100
sensors today but tomorrow I could have
10,000 and all I really have to do is to
go on the backhand cloud and move the
slider and say I want 10 more machines
to handle the workload it scales
linearly because you have certain amount
of workers that are advocated for for
processing so if you move the slider the
work number of workers just go up to do
the same day Nate ingestion smart
buildings as marceline is essentially a
great application for this so we have a
microsoft project called the
at 88 acres you can go online and come
and find it 88 just 8 28 acres acra yes
so what this does what this project does
is a monitress everything that happens
on microsoft campus so we know exactly
how much electricity we're using at a
specific time we also know which
building has a bad air conditioning so
everything is wired so you don't have to
have someone so used to be that we send
a lot of these maintenance workers to go
to each building and check everything so
it takes about five weeks for someone to
go through the entire campus so now it's
two minutes especially you can get an
overview of exactly what's happening on
microsoft campus in two minutes or so um
so here's that like a diagram of what's
happening so right now at that
particular moment Microsoft is using 64
megabytes and that's actually a lot of
power if you think about it so we have
another 100 buildings over on Seattle
campus we have 40,000 people in Seattle
so it uses a lot of power I mean we
definitely do so for example if if we
just you know happen to use a bit more
power the Seattle grid what actually
feel it so so you got to be really
careful not to have those spikes
actually happen so if you look at this
diagram so this is the power consumption
during each day so you can pretty much
figure out that people go to work here
at 5am or six a.m. so somebody to start
going to work at 6am so by the time it's
a couple 8 9am it really goes up
and then you see this little dip here
take a guess what that is people going
to lunch so when people go to lunch do
not under computers you can see that
there's a little dip of energy
consumption just a little bit and then
the lights are on you know just
computers start working for a while and
then in the evenings it just goes down
and at night at night you can see that
is a little spike around that time so
these are probably air conditionings or
other things that started turning on or
maintenance work that happens so you can
really see how these things work in real
time oh so and you can open any building
and take a look at what's happening at
that time so how many people there are
in there because you know it's hooked up
with the badge system and how much power
is being conserved and we can navigate
through this map and click on any of the
buildings so all these red dots are
saying there's a problem so there could
be a nice bucket of regulation or
something happens there's a little red
dot there and then you can draw in and
say hey what are the problems that we're
having and you know this particular
building and what are the problems of
this is the xbox lab and it shows you
how much power is being used and you
know what it advanced doing and you can
look at the building and then find out
in which location we're having problems
so for example right now we have a fault
it's a level 5 which means it doesn't
really matter into a small problem we
don't care it's going to cost us about
our $69 if we don't fix it so there's no
problem if it's a big problem for
example
office is on fire don't I come face it
right the what's interesting here is
that there is a 2 million data points
being ingested there two million data
points being sent to the service is this
actually using sequel server and other
things so let's just say if we really
start going crazy we have the entire
city of Beijing each building is wired
to a sensor how would you be able to do
that so one possibility is that you
would use that storm kafka combination
that I showed you okay so being able to
really scale out linearly as there's
more data coming in and it works great
in our cloud because we provide that
underlying hardware service hardware and
software service that ensures that it's
always running so we have a training
class that that happens tomorrow and
Thursday so if you're not signed up
that's okay i have a link on this deck
just get your camera's ready and you can
take a picture of it and it will have a
link to it and to not here yet so let me
explain what we do what I teach in this
particular class and eventually we might
bring it online I would really want and
you know welcome you guys to you know
take that class online if possible in
the near future and in the meantime you
can study that on your own so in day one
we have an introduction to Windows Azure
and then we have
as your website's lab so websites allows
you to create a website and you can
create a blog you can create anything
you want asp.net PHP anything anything
you can think of that's a web
application very easily there's 10
websites as free and then you can also
create these as your virtual machines so
virtually machines are essentially a
workstation in a cloud or a vm just like
how just like how your servers worked
right you just have a machine that seems
sitting in the cloud you will save
everything you do as a hard drive that's
remote so it's sitting in a blob storage
so as you're reading and writing that
data is chemistry kompy's meaning that
your hard drives you have essentially
you have three high porosity in a cloud
so one of them dies that's ok so it's if
you did reliability of that that's
virtual machines and you can have linux
and windows on it so and then there's a
Windows Azure storage so this is a
remote storage system that we built
which allows you to be able to take
advantage of stateless applications
meaning that you can you should probably
not right thanks to your local hard
drive if you want to keep it scalable so
you will copy them to the blob storage
and also for for the cloud services we
have a lab that shows you how to do
communication between multiple computers
between multiple machines and then
finally we'll you know that's concludes
day one and then we go to day too so
they do is mostly about compute as scale
so we have some HPC labs and the HPC lab
shows you how to use matlab on a cluster
is multiple machines to run
apples and we also have a linux cluster
lab and shows you how to build just you
know type of a command and create a
cluster that consists of multiple linux
machines and then we also have a cell
and data visualization in the afternoon
will show you how to have a run hit
event of your type of no sequel they
database systems so allows you to
process your data in parallel right and
then there's a streaming data this is
what I discussed already which is cough
cottage storm right so you really ingest
data and do the real-time processing and
also store the data in a scalar
old-fashioned so that's the training
materials and then you know get your
camera's ready this is the URL if you
want the training materials about 4 140
megabytes so it's and it's also you're
going to change what i will do is i will
update that URL when it changes so you
can still keep that link that will
always work ok so let me go to the end
so we also have a windows azure for
research program so each of you have a
one of these will templates so take them
home if you like to so there's a URL on
the SS asher for research so you go to
that URL it shows you what we actually
do training i think there's going to be
seven trainings in china at some point
it's completely free just have to sign
up for it l and the other thing is that
we have
did you are else so oh can you actually
access linked in China does anybody know
okay then you can you can thank you
probably can I think to whether you
cannot access but I think you can do
that in the cloud but that would
actually make it happen the so the
program we're avoiding a significant
amount of windows as your hours for
researchers so we have a really good
project please do apply um and we you
know we really want to see some
innovative uses of a cloud computing so
using Windows Azure in ways that other
people haven't really used so we would
like to see projects that are always on
and running for a long time I would like
to see you do science that you weren't
able to do with your resources that you
just had on premise or your laptop hey
um so please do take a look at this and
there's a URL which you can submit a
proposal there is Dennis Dennis runs the
program for sure for research you can if
you have questions you can ask the
gentleman in the back there's there's
Dennis he's waving at you and I think
that's that's all I have as any
questions and you know don't be shy and
you can you know ask you a Chinese auto
translate human yeah any questions no
okay great thanks everyone for coming to
this session
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>