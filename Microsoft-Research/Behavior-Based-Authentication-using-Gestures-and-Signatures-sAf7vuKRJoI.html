<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Behavior Based Authentication using Gestures and Signatures | Coder Coacher - Coaching Coders</title><meta content="Behavior Based Authentication using Gestures and Signatures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Behavior Based Authentication using Gestures and Signatures</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sAf7vuKRJoI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it is my pleasure to introduce introduce
Shahzad he's a PhD candidate at Michigan
State University his research interests
are primarily security and protocol
analysis and behavior based
authentication so today he will hear
about some of the work he's been doing
while here at MSR for for the summer
should add Thank You h1 all right so
I've been working here for like 12 weeks
now this is my last week here and I've
been working on behavior based
authentication using signatures and
gestures on touchscreen based devices
alright so why do we why are we trying
to find new authentication schemes
because currently in computer world we
have mostly passwords and pin codes and
if somebody else finds out your
passwords and PIN codes they can access
your account so we want to get rid of
those we want to not be dependent on
those anymore research community has
been doing a lot of research on
keystroke rhythm that the typing rhythms
of people to use those for
authentication authenticating the people
that kind of eliminates the need of
usernames and passwords so you enter
something the computer finds out your
typing pattern typing behavior and it
finds out if it's you or not but the
problem is that this it is still not a
very efficient way of doing it because
it has very high false positive rates
still and the thing is that whenever you
have to log in use and if you want to
use your typing rhythms you have to type
something and consider I mean usually
it's considerably long like 300
characters or some like that so that's
not very convenient so anyway so we were
thinking that can we port such kind of
scheme to touchscreen devices
the problem with touchscreen devices
when you try to port such schemes on
touchscreen devices is they're
touchscreen devices do not have a
physical keyboard so you there is
nothing really to guide your fingers
physically like in keyboards there are
keys so they kind of guide your fingers
so you have a typing rather more typing
pattern and another thing is that this
kind of scheme does not work for people
who are not
adept at typing so you have do you have
to be very experienced so you have to
have a pattern only then they can work
so we were thinking that now that these
devices have touchscreen so can we use
something else and of course the first
thing that we thought about was
signatures so can you few signatures to
authenticate yourself on the touchscreen
devices and why not because signatures
are very prevalent these days they have
been in forever in financial
institutions and the reason being it's
even if you know somebody's signature
it's not easy to copy them like why
don't these banks and all these
financial institutions use passwords
because if I know your password I can
log in or I can access your account but
they use signature so signatures are
more secure compared to passwords but
the problem with the reason why
passwords are still more prevalent in
computing devices is that there was no
way to get signatures on your
touchscreen devices but now with on your
computing devices but now with these
touchscreens and with very fine
granularity of resolution that you can
obtain off the points you're touching on
the screen I think this thing can be
ported I mean signatures can be made to
work on these touchscreen devices and
you can probably shift from passwords to
signatures because if somebody knows
your password he can log in but if
somebody knows your signature he cannot
login because the way you do your
signature is kind of very distinct and
that's what we are going to explore
today so for signatures I'm not the
first person doing any work on this
thing especially people have done a lot
of work on signatures especially the
static signature analysis scheme so you
have an image of the signature you want
to match it to another signature over
your database the problem with this
thing is that the static image analysis
is that what happens if somebody copies
your signature exactly like he'd spent
some time and then he is able to copy
your signature exactly so that signature
won't be that signature will be
authenticated by the static image
analysis the other problem is that these
static image analysis techniques which
just use image processing to get to a
authenticated signature require a large
amount of training data I'm giving you
an example so there isn't a database MN
is T for handwritten characters it has
like 60,000 training samples and
have been doing a lot of work on
automatic handwritten digit recognition
and the best accuracy is somebody was
able to achieve was about ninety nine
point six percent on that data set and
that is using about six thousand samples
per digit so digits you can see digits
are very simple compared to signatures
so if you read if you need six thousand
samples for each digit to get it to
trained so well that you get 99%
accuracy you probably need a lot more
than six thousand to train on your
signature and you do not want to ask
your user to give you six thousand
training samples and that's just
ridiculous
so we were kind of thinking like so
that's when you're using touchscreen so
you're doing your signatures and the
touchscreen can basically tell you the
behavior of the way you like it can tell
you each point what time you entered
what time you touched which parts of the
screen so can we use this information to
reduce the training samples and also
eliminate the problem of perfect copying
of signatures so I'm going to talk about
that so the other so another thing that
we thought about was so can these
signatures be feasible for all kind of
touchscreen devices so I have this one
here I mean probably it works pretty
well on this thing but can you do it on
a phone you cannot actually so I will
talk about that as well so we wanted to
look for something else for phones again
with not pin cores and passwords not
something that you have to remember who
it's just something that you do on the
screens of the phone and then that is
what you use for authentication so you
guys have seen that in Windows Phone
whenever you have to unlock the screen
of the phone you have to make a swipe
for you first press the unlock button
and then you swipe upward to unlock the
phone on iPhone you press the unlock
button then you swipe right to unlock
the phone so we were thinking like can
we use just these gestures to find out
if it's the original person or not so
that's what we worked on we kind of make
this gesture is a little more
sophisticated not just little swipes but
something a little more sophisticated
and we use those to authenticate the
user I will show that that's it that is
what we can do I have the demo
applications for both these things and I
will show it to you guys and we will see
so the hypothesis behind all this work
is there everybody has a distinct way of
doing signatures so
signatures as well as gestures so if I
can do my signatures anybody else even
if you can copy it or the gestures that
I perform on the phone even if somebody
else can do it they will not be able to
do it exactly the same way that IU is
III do it and that is our basis of the
work that is how we distinguish between
people so rest of this talk is divided
into kind of two parts I will first talk
in detail about what the signature
authentication scheme that we have
proposed is I'm gonna go and detail off
not a lot of technical detail but I will
still give a very a lot I mean I will
give details about how this thing works
after that I'm going to switch to the
gesture based authentication on the
phones the technique that we use behind
gesture based authentication is kind of
a subset of the technique for signatures
so I'm not gonna talk about that
technique in detail I'll just mention
what would put parts of the technique
from signature based authentication we
borrowed for the gesture based
authentication and then I will give you
the demos and you guys can try that as
well
okay so signatures what is the objective
here so objective is for us to find out
if this signature is visually correct if
it's structurally structurally correct
or not like does it look the same as the
original signature that you are
comparing with and the second thing is
once that is done the second thing is
that you want to find out if this has
been done by the same person or if that
has been imitated by some other person
which mean who made it look alike but
it's not the same way that they did the
signature okay so I'm gonna show you a
couple of videos here just to give you
an idea so you will see here two
different signatures being done by two
different people and likely not two
different people there my signatures but
anyways so you will see that so you will
see four sample of those signatures and
on each of these screens and you will
see that they kind of look very similar
when you do it the behavior that could
which you do it you will see it
so these are four samples of the
signature from the same person being
done you can see that kind of look
exactly the same when you do it I have
slowed them down a bit for you guys to
see it so
so off all four are different signatures
but they looked kind of the same when
you were actually doing it I'm going to
show you another sample here
and you will also see that there are
some differences sometimes a little lag
in some particular part of The Strokes
but there is still a behavior with which
a person does the signatures so so we
will use this fact for authenticating
the signatures so from this particular
thing we will extract velocities at
different parts of the signatures from
velocities I mean the magnitudes of the
velocities as well as the directions I
will talk about this in detail later the
next thing that we should see is if
there is a so we analyze this thing that
does a person have a behavior when of
the pressure when he does the signature
like the pressure on the screen you can
fly I don't know if that's very clear
not but you can see that there are some
parts always the same parts which have
lesser pressure so the darker the dot is
the higher is the pressure so and there
are these dark parts and then there are
lighter parts so they're always the same
in the same in signature this is this
probably will make it more clear so
these are the pressure plots of these
four samples and you can see that you
have very distinct peaks at different
locations so this is one here and then
they to here to here to here and then
there are these at the start so there
are distinct peaks so this kind of again
is a behavior and the signature of the
person so you can use this information
for authentication this is another
sample and this is another pressure plot
of the thing you can again see that you
have the same distinct peaks here here
here here and so you can extract this
information out and try to use that for
authentication and another person who
imitates your signature exactly the same
way will not be able to follow this
pressure plot and so this is so
velocities is one thing that you can use
your no it so the thing it for so maybe
I should have done that so if you do
your signature to try to do it a little
differently and then or maybe little
slowly or something your pressure plot
will automatically change so pressure is
not based upon the shape pressure is how
much pressure you put it so if you have
a behavior you will always press hard
press little I mean pull up your pan or
finger whatever you using and then you
so that kind of makes it this thing
so we so right now I've talked about
velocities that you can use the
magnitudes of velocities the directions
and the pressure so the magnitudes of
the velocities kind of give you the
behavior and behavioral information the
pressure also give you the behavioral
information the directions of the
velocity give you the structural
information so do you use the directions
to find out if the structure of the
signature is correct like if it's really
correct or not you use the magnitudes of
the velocities and the pressures to find
out if the behavior of doing the
signature is correct or not but there
are other features as well I will start
I will talk about well let us see how to
extract these features because the thing
is that you don't really know which
exact so there are parts of signatures
which do not have consistent behavior
among different samples so you don't
want to use any information from that
particular part of the signatures
because that is going to increase false
positives imposters will be able to
login so you want to not use that
information only use only that
information from those particular parts
of signatures which have consistent
behavior so how to extract those
features so an easy way is to divide the
whole signature into several sub strokes
like so each signature has several
strokes this particular signature has
eight strokes so this is 1 1 2 3 4 5 6 7
&amp;amp; 8 so you have 8 strokes you divide
each stroke into several small parts and
then the thing is that you also have to
figure out well how small should this
sub stroke be if it's too small it will
kind of become instantaneous and there
won't be any consistency among different
samples if you make it too large it will
average out all the interesting
information in it and you again won't be
able to find out what is if there is any
distinguishing information in it so you
have to have a particular is a correct
size of the sub stroke so there are two
challenges challenges first you have to
find out what parts of the signatures
have consistent behavior and in those
parts what should be your sub stroke
size which will actually extract the
consistent behavior if the sub Stoke is
then too large averages out too small
it's in turn instantaneous you don't get
anything out of it so to find the
consistent features if you divide the
sub strokes and D by divide each stroke
in two sub strokes of certain time T I
would say here we use T equals 20
milliseconds 30 milliseconds 4050 so you
divide each stroke into several parts of
small several sub strokes of different
time periods and then take one
particular stroke I have like made this
one black hair so take this one
particular stroke from all the
signatures that you have and then just
extract the average amount of feature
Everage feature value from this
particular sub strokes for example from
this particular sub stroke you can if
you want to find out pressure take the
pressure values among all the points in
this sub stroke and take the average
from the above the this from the other
sub strokes from all other signatures
take the pressure values take the
average so you have for example if you
have a hundred training samples so you
have 100 of these sub strokes and you
have 100 average values of that
particular feature that you're using it
could be pressure it could be velocity
it could be direction once you get the
hundreds values of those signatures
those sub stroke features from all the
signatures you take their mean and take
the standard deviation and then try to
find out if the standard deviation is
large or small if the standard deviation
is large that means this feature is not
consistent among different sample of the
same signature if that standard
deviation is small then you can probably
use it because the user has the same
behavior every time so we use
coefficient of variation which is just
simply the ratio of standard deviation
to mean so we use coefficient of
variation of 0.1 and 0.2 0.1 and 0.2
means so 0.1 means the standard
deviation is just plus minus 10% of your
mean value which is very restrictive the
reason why we are using such small value
of coefficient of variation is because
we do not it is okay to reject the
legitimate user sometimes to not be able
to log in but it's not okay to let an
imposter be able to log in even once so
you use small value of coefficient of
variation this will increase your false
negatives kind of might become a bad I'm
in annoying but it will never let an
imposter get locked in because you have
your like your along such a small amount
of deviation in the feature values that
an imposter may never be able to do it
so we take the coefficient of variation
of each sub stroke in the
signature and see and get the feature
values and see if the coefficient of
variation is below this threshold for
all the feature values where you have
the coefficient of variation below this
threshold you use those feature values
for training your classifier if you if
if a certain feature has a coefficient
of variation greater than this threshold
you don't you just don't use it you
ignore that one
so using this strategy this is what we
get so let's talk about directions so
this is the entire signature so it is
divided into several parts and from each
part we calculate the value of direction
and then take the coefficient of
variation from each sub stroke and then
you see that these particular areas the
green ones are always consistent their
coefficient of variation of the value of
direction is always less than 0.1 so you
extract features out of these and you
ignore rest of the stuff so whenever a
user will whenever somebody will try to
log in he will provide your signature
you will extract the values of direction
from only these parts of the signatures
and then you will try to compare it
because these are very consistent they
are always the same for the legitimate
user so if they match then he's probably
the legitimate user but these are not
the only things that we use so if you
increase the coefficient of variation to
0.2 0.1 was this 0.2 means you can use a
little more information out of it
0.2 is still acceptable it's not too bad
so so this is the other signature these
are the areas from where you use the
values of direction if you increase the
coefficient of variation of 0.2 you
extract values a little from a few more
locations of the signature from
velocities the magnitudes of velocities
if you use coefficient of variation of
0.1 this particular user does not have
enough consistency no we didn't use any
values here if you go to 0.2 so you have
quite a few places from where you can
extract the magnitudes of the velocities
same is the case with this user
coefficient of variation 0.1 no velocity
is extracted 0.2 yes you have enough
consistency that coefficient variation
is less than 0.2 when you use these
velocity values if if if somebody finds
out that there is even for the
coefficient of variation 0.2 you do not
have any extra any feature values that
you can extract like nothing is
consistent you can increase keep on
increasing the coefficient of variation
it will only so it will let the
legitimate user log in eventually but
again if you use high coefficient of
variation there is good chance that you
will let an imposter login as well
so all the users that I have studied all
the volunteers that I had we were able
to do it with zero point one as well as
zero point two so we didn't have to go
about zero point two and I think
generally zero point two gives you
enough features for the pressure we saw
the pressure was kind of very consistent
among different signatures who you V use
a lot of parts from the signature for
the pressure so from both these
signatures actually so there are other
features that you can use for example
total time of the signature so for this
particular user the total time was
generally 3.9 seconds varies from about
3.7 to 4 seconds for the second user it
varied from 2.8 to 3.2 seconds so it's
not too much variation you can probably
use total time of the signature as well
but we are not really using the total
time of the signatures we are actually
using time of each stroke that gives you
enough I mean that that is consistent
enough so these are the other features
that we use duration of each stroke in
the signature this gives you behavioral
information inter arrival time strokes
that is a time between two strokes like
how much time you take you how much time
it takes you to lift your pen or finger
from one stroke and start the other one
so we use that and then you also use the
displacement between bounding boxes of
the strokes I will talk about what
bounding boxes are and so you extract
all these features from like you you
take all the stroke times you take all
the into stroke times you take all the
displacements of displacement between
connected bounding boxes and you again
run the same test of coefficient of
variation 0.1 or less 0.2 if it's less
than that you use that feature otherwise
you talk so this is one signature one of
the users we can see that inter stroke
times like the time between two strokes
and time bit of and the time the strokes
the coefficient of variation is pretty
mean reasonable less than 0.1 in most
cases here less than 0.2 in most cases
they are sometimes in some signatures
some stroke times do not have
coefficient of variation less than 0.2
we just do not use those stroke times so
this is what I meant by bounding boxes
so bounding box of
each stroke means the smallest rectangle
rectangle with smallest area that bounds
that stroke so you take that box and
like basically join the Centers of all
the consecutive boxes and you can see
that each user so all these four
signatures have kind of the same pattern
the the location from at which a
particular stroke is located compared to
the other stroke is kind of usually the
same
so this previous thing showed the
coefficient of variation of direction
and the distance so distance means this
distance and directions means which
direction so coefficient of variation of
direction is always very very less so
that is a very good feature to use so we
use that feature and in some cases you
can also use the coefficient of
variation of that the distance between
bounding boxes so that's also usable so
till now I have talked about what
features we use that include velocity
the magnitude of the velocity the
direction the pressure stroke times into
stroke times and displacement between
bounding boxes so these are the features
that we use to identify a signature from
the other signature feature in a
classifier I will talk about that and
then we do the testing and then that's
how it works
but before I do that there's one last
thing that I want to talk about is this
I'm not going to go in the exact details
if somebody's interested we can take it
offline but so sometimes when you're
doing so this is kind of the signature
that generally looks like from a user so
it has eight strokes here so you can see
so these two signatures have seven
strokes this particular stroke is
combined and these two strokes here
they're combined so there's you should
so we came up with a way to basically
separate them so if you have seven
strokes there is no way you can make the
sub Stokes and then correlate and try to
find out which sub strokes you should
extract features from so you first need
to make the number of strokes in the
signature consistent and then after that
you extract the features so generally
what I mean what effectively we do is we
take information from this to all the
strokes of the training data and
whenever it
Mattox signature comes in we take this
all the strokes and then we try to find
out which stroke you should split which
stroke is not consistent with the
training data and once you find that out
then we again use the training data to
find out which exact parts of this
stroke you should split from and then
you remove that certain parts from from
the stroke and then you get your desired
number of strokes like this so for this
one we just extract we just remove a
little bit of thing from here and it
divides it into two for this one we
remove this part and then it makes the
number of strokes consistent alright so
that is all the feature extraction part
after that it's simple we take the
support vector distribution distribution
it's a one class classification so you
do not have any information from the
other imposters class so all you do is I
mean so there is open implementation of
SVM one class SVM it's called support
vector distribution estimation that's
available online so we use lab SVM for
that we took these features strained our
classifier did some great search for a
parameter optimization I'm not going to
talk about that because that is a
standard machine learning part so we did
that and it works out pretty well so
let's see I'm gonna give you a demo of
this thing now and after the
presentation anybody is welcome to try
to do these signatures and try to log in
so I'm not sure if I'll be able to do it
here or not but generally I am able to
so please trust me
let's see
so I told you here that it's a
legitimate user these are done by me I'm
gonna do it again I will make them look
exactly like this thing I mean I cannot
make it look exactly like but I will
make them look similar enough that a
person if you just visually sees them he
will say that this is from the same
person but I will try to do it
differently and then it will pan tell
you that it's not the same person I mean
these are kind of hard but if you want
to try it you're welcome to
the reason why I'm not getting yes it
matters if you have visual feedback we
have not done that but I think it's
gonna be very much more convenient for
the user the reason why I'm not doing
that is because I need to have very good
sampling rate and if I try to draw the
thing on the screen it kind of reduces
my sampling rate and when it reduces my
sampling rate it reduces my accuracy so
see this so this one's imposter this
one's legitimate and they look exactly
the same two naked eye right so this is
this was yeah I mean of course there
would always be differences you can
never do your two signatures exactly
like but I mean if I send it to the bank
probably they will accept it so this
this is the idea behind this thing
okay so that was a signature sparked now
let's move to the gestures all right so
why don't why don't we just use
signatures on the phone I mean how hard
could it be this hard so this was the
best that I could get out of my
signatures I do both of these so this is
what it looks like looks like on the
phone maybe maybe you can have some
accuracy here as well we didn't try that
but I don't think it's gonna be very
good because it's hard to imitate your
own signatures on the phone the screen
is small you don't have any place to
rest your hand and then it makes it hard
to your own signatures on the screen so
and again I mean if you are driving you
probably do not want to do your
signatures to unlock your screen right
so there should be an easier way for the
phones so what do we mean by gestures so
these are the gestures that we have you
have ten gestures I started with 39
gestures so I gave few phones to some
volunteers we've got like 40 volunteers
so they perform those gestures and we
analyze them we saw which gestures have
distinguishing features which just says
do not have liked the swipes on the
iPhone or the Windows Phone they do not
have distinctive enough information but
if you make it two fingers or three
fingers in different directions or if
you make it like some curves and pinch
like zoomin and zoom-out
that has a distinguishing information in
it and you can use that for
authenticating the person so we so so
these are the ten gestures that I have
in my final demo application so what we
do is we ask the user to do the training
provide at least 30 training samples for
each gesture 30 is the minimum but the
more the matter so we take that
information and then we train our
classifier I'm gonna talk about how you
do that so the basic thing behind the
whole scheme is almost the same as
signatures here each finger basically
represents a stroke you divide each
finger stroke into several sub sub
strokes and then extract features like
velocity time bounding boxes
displacement between bounding boxes
we're not using pressure here because
our phone could not give us the pressure
information so that feature is not being
used but I think that if we can use if
the API for the pressure right now is
not available but it will be available
soon I read it somewhere
once that is here once you use the
pressure information it's gonna make it
much more robust I think because
pressure is something you probably
cannot figure out by just watching a
person to a signature you can see how he
does it you can see all the thing but
probably not the pressure so once the
pressure information is available we
will use that probably and see how it
improves our accuracy and again the same
thing we took that we took those
features trained our classifier tested
them so as I mentioned we have data set
from like 40 users so what we do is we
take all these all the data of these ten
gestures from any user who wants to
train and then use all those 40 users as
imposters and try to rank which feature
for this particular user is most
distinguishing and once we rank them
then we give send that information so we
do this on a cloud service we send that
information back to the phone the phone
then asked she automatically which
signatures you which gesture you should
perform and you perform those gestures
and then it uploads the information back
to the cloud and it does the
classification and tells you if that is
you or not if you can use so if you want
to be more secure you probably want to
do like three gestures or four gesture
or five gestures if you want to be if
it's just okay you can do one gesture
and so two gestures three gestures are
probably going to be are actually going
to be more secure compared to one
gesture so and that brings me to the
damn off the phone this is what you guys
can try it because so I have already
done it once
it tells the legit made and then you
guys can try it whoever wants to do it
take the phone and do it just press the
test again button number of speeches
selected three and do that and see if it
if you can imitate that or not so the
phone is here the last thing that I want
to talk about is how to do the training
one interest interesting thing that we
observed while doing this work was so I
sat down and I did 100 signatures as
training and I try to test myself again
up so I did that 100 signatures and
training I I trained them I did all the
simulation in MATLAB and found out that
I'm getting like 99 out of hundred
signatures being identified correctly
and same is the case with some other
volunteers that date in the same way sat
down gave me hundred samples so
apparently looked it's going to work
pretty well but when we actually made
the application and the user again tried
to sign and we give
that information give-give that
signature to the to our testing server
it wouldn't recognize that person so
that was kind of strange because so if
you have we had hundred samples we are
training on 99 samples and testing on
one sample working perfectly fine so we
are taking on another 99 samples working
on testing on the remaining one sample
working very good I mean so it was able
to recognize them but why not
the signatures that you were getting on
one time and we found out that once you
are trying to your training all once you
are giving all your training samples in
one row you kind of develop a pattern
which is not your natural pattern and
you cannot imitate that again so if you
start doing your signature 100 times you
will actually see that you are doing it
in a very distinct way which is very
very consistent gives you very good
accuracy and simulations but you cannot
imitate that after half an hour that
becomes hard so we did again so what we
did again was we I asked several users
including myself we did training 10
samples at a time and with several
breaks half an hour hour days of breaks
and once we did that then it started
working so this is so that that's kind
of thing that you have to do in the
training and that's kind of thing I have
seen in some of the earlier work they
have done this kind of work on phones
and some of the nuns tablets as well and
they have very good accuracies of
simulation but I think that's not going
to work in real life so you have to do
the training like this just don't take
too many training samples at once
because you develop patterns which are
not your natural patterns takes even I
would suggest like five samples at a
time and then take breaks and then train
over a period of time like maybe a week
and then do it and then it works pretty
cool so that was about what my work here
was any questions yes I was also
wondering about accuracy when you want
to
use your tablet in the morning or any
evening after you come home for partying
instead probably so I so I was kind of
worried when I came here because when I
when I was going to do my signatures
while standing I have done all my
training around my testing on myself
always sitting and I'm when I was trying
to do it here so I kind of thought that
I may not be able to but I was able to
if you're doing your signatures they are
you have I mean if you have been doing
them for a while you can you can do it
so that doesn't really make too much of
a difference you always have a pattern
in our scheme I did not talk about all
that stuff it does take into account
like for example if you increase your
actual time that you so if you do
generally in four seconds and if this
time you do in five seconds because
you're a little lazy it can still figure
out using those five seconds as
reference that well even if you used
five seconds does your rest of the stuff
has behavior in it and it it will have
so you normalize with your total time
and all that stuff
so it works on different things for the
gestures yes so the prop in in gestures
you do see this problem and we plan to
work on it in future that for example if
you are driving or if you are nervous
probably or something that kind of makes
it hard for you to log in but if you are
in a normal state of mode or like if
you're walking it's probably gonna be a
little hard if you are driving it's
gonna be hard so the best way to deal
with that is train it in all different
scenarios and it will work
yes if you add more and more in training
sessions to it over time it becomes more
accurate it does but that depends what
kind of training are you doing if you
are just putting in random samples it's
probably gonna get worse like if you
just think that you're gonna do it again
and again and again and don't care about
being consistent then it's gonna make it
worse but otherwise yes it makes it
better
so the training part is tricky you have
to be very consistent in that and you
have to be patient with that because it
becomes kind of tedious so frustrates
frustrating a little bit but if you stay
consistent in there it works pretty well
after that
yes so you will get full signatures
verification we do anything shorter like
so just the first part of your name or
sometimes with a made-up gesture um
made-up gestures no we didn't ask people
to do random I think we did but we
didn't do that analysis so we have a
list of gestures right now we just
analyze those again the thing is so our
the main idea behind our work was to and
to understand the behavior to use the
behavior of doing something if you just
make up something right now and if you
do it right now train it right now and
do it afterwards you probably won't be
able to do it again so in order to get
rid of the passwords you need to have
something which is consistent
maybe handwriting I don't know I have
not worked on that maybe just write
something and it kind of a good nice
well it's tank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>