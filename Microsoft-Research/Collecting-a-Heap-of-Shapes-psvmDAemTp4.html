<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Collecting a Heap of Shapes | Coder Coacher - Coaching Coders</title><meta content="Collecting a Heap of Shapes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Collecting a Heap of Shapes</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/psvmDAemTp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so they
for coming so introducing mark Mark's no
stranger to rise in fact his paper here
is joint work with Chris Berg oh shut up
so I'll let him take away okay great
great well hi everybody thanks for the
introduction and thanks everybody for
coming and I guess people watching
remotely thanks to so today I'm gonna be
talking about some work that i did with
the Earl bar who's a postdoc at UC Davis
Chris bird who is here at MSR and we
basically did sort of an empirical study
of the structure of the program heap and
so the motivation for this is I mean it
well okay the slides work online click
on that hey there we go okay the
motivation for this is um you know for
me I've most of my work has been on sort
of you know tools and techniques for
either static or runtime analysis to the
program heap and aliasing these sorts of
structures applications for this and
optimization memory management and so
forth and in doing this I spent a lot of
time sort of looking at well you know
what properties do people say are
important about the heap what properties
do they try and extract in these
analyses what properties do they try and
leverage in these program optimizations
and and how do these things go and can
we sort of distill you know the
properties that are used in this wide
body of literature down into a couple of
core principles that are that are sort
of general applicable important things
and in this process you know I sort of a
sort of process of discovery for me
because a lot of things that I looked at
and I said oh this is you know looking
at this code this is an important
property this is going to be fundamental
to understanding what's going on in the
program heap just turned out not to
matter and some things that I thought of
this is you know I mean it might be
interesting but it's fairly incidental
they turned out to be fairly important
into getting sort of the right structure
and predicting things correctly and so
really what we wanted to do in this
study is we wanted to sort of look it's
sort of a step back and say well look
can we can we look at sort of these
concepts that people are using that I've
been using and look at some real large
programs that represent real world
workloads and say look are these
concepts you know perhaps too simple to
describe the reality of what these
programs are doing are they more complex
than is actually needed to get useful
information out of these and so they're
extra sophistication is really
not giving us any value it's just taking
as computational cycles or you know
increased implementation complexity and
not I'm not getting us anything useful
and you know the hope was that okay
maybe there's a nice simple set of sub
concepts that categorize the majority of
what's seen in practice right the hope
is that most heat structures and most
programs build sort of this consistent
set of heap structures and maybe have
small specialized structures that they
use and not every program builds an
entirely different sort of looking heat
so what did we do well we wanted to try
and not bias this study too much and we
want to look at this in just sort of a
general from base principles way so we
said look we're going to we're going to
pretend these heaps are created by some
unknown process we're not going to look
at the source code at all we're just
going to run the program we're going to
take a heap snapshot we're going to
start out with some set of you know
features we think are important we're
going to express relationships between
you know sort of this graph structure we
got and these properties we're going to
do some measurements some things will
say okay we're able to categorize this
structure in this direction this
structure and some kind of you know
structures will see will say well you
know we don't have any way to express
these they fall into some unknown thing
that unknown set is big will sort of
look at these heap structures that are
in the unknown set hypothesize some new
features repeat again until we sort of
feel like we have a good coverage of
general things and then we'll see where
we are and hopefully this at the end of
this this you know set of features we
have is fairly small fairly reasonable
and this uncategorised set you know is
negligible so we'll see how that goes
rezian yeah can you go back so if you
ignore the source code
the type declarations give you some yeah
so we don't fully ignore this I mean
we're going to hypothesize features that
you know are relevant to pro you know
some programmer related concept we don't
want to just be too general so we're
we're trying to minimize our bias here
so we're going to look at types we're
going to look at field identifiers we're
going to hypothesize things that are
going to be relevant to a programmer
such as aliasing or you know immutable
types will say okay you know let's base
our definitions on these things but we
don't want to start looking at the
source code and say oh we're you know
we're building this with you know
recursive calls or we're building this
with whatever so we want to just be as
general as possible and yeah I think
it'll be a little more precise as we go
on and introduce the measures and stuff
so what's nice about this well you know
doing this runtime study we can easily
experiment with a range of sort of
definitions and identify these features
just by looking at what pops out that we
weren't able to categorize earlier we
have high confidence since we're running
these on large programs and over a
fairly reasonable benchmark corpus that
they actually are useful in practice
we're sort of not consents we're not
looking at the source code we're not
constrained with issues of you know did
I get a lot of uncategorised features
because I didn't have a way to
categorize them or because my analysis
was in precise we sort of avoid that
whole problem by using concrete
execution and you know I mean going back
to my experience with static analysis we
avoid all the issues with reflection and
threads and all that good stuff we just
run the program take some snapshots and
we're good so what is the artifact we're
actually going to use here I mean how do
we how do we you know snapshot the heap
and then turn it into this artifact that
we're going to measure well there are
two ways to sort of look at this and
previous work is explored both of them
but the the majority of the previous
work is sort of said we're going to
compute metrics on individual objects so
we're going to take an object we're
gonna measure its in degree we're going
to measure whether or not it's alias
whether or not pointers it has in its
fields alias other things is it part of
a linked data structure whatever but you
know might and this is sort of a
hypothesis we're going to make my
experience is that developers aren't
really thinking in terms of individual
objects when they're building the code
they're thinking about the roles these
objects play in a structure how these
larger structures fit together and how
these composed
deserve you know be the representation
of some concept in the abstract problem
space that the program is attempting to
solve and so we're going to sort of
hypothesize that these roles that the
objects play are encoded by how the
objects their types of course and how
these objects are stored in various data
structures and can be referenced in
various ways by the programs so we're
going to you know attempt to sort of
examine the validity of this hypothesis
later but we're sort of making this
assumption from the get-go and how we
how we do these measurements so what
we're actually going to measure is an
abstraction of the concrete heap and
we're going to take it's based on this
storage shape graph approach so we're
going to take the concrete heap we're
going to group the objects into you know
equivalence classes each of these
equivalence classes is going to become a
node and then all the pointers between
objects in the same equivalence class
and objects another equivalence class
going to become the edges in the graph
right and this is a nice natural way to
represent a lot of these sharing
properties it's been used a lot before
the only thing is there are some
properties that we may be interested in
that can't be encoded just in the graph
structure so we're going to add two
extra sort of annotations to the graph
that allow us to express a little bit
more information that's interesting so a
big question is how do you how do you
group objects together we say well we
have a bunch of objects in the heap we
need to produce equivalence classes for
our graph how do we do that if we have
too many equivalence classes basically
we're going to have objects that all are
really equivalent from the programmer
viewpoint but in multiple equivalence
classes in our abstraction and well
we're basically having a bigger graph
with more complexity that actually isn't
capturing any information so that's not
too good we have too few nodes we have
lots of you know objects that are in two
actual distinct equivalence classes from
the programmer viewpoint we're merging
together so we're losing precision so
the question is well how do we how do we
pick this this trade-off that works well
a lot of previous work has sort of done
this partitioning by looking at the
allocation side of objects the type of
objects maybe some object sensitivity so
you sort of generalize this allocation
side information and I'm not a big fan
of these fixed naming schemes because
you sort of have to before you even
start you have a finite number of names
and you have to assign every object to
one of these finite equivalence classes
we want to have as many
Evelyn's classes as we need to express
all these sort of conceptual components
that the programmer has in mind and no
more so we want to basically dynamically
generate as many equivalence classes we
needed based on some you know the
runtime structure and so we're going to
have this notion of indistinguishability
between two objects right and we'll say
okay it has three parts two objects are
going to be indistinguishable if they're
part of the same recursive data
structure based on type information so
here's where we're using type of
information a little bit if they're
stored in the same collection or array
and they have the same type so two
objects in the same array of the same
type are sort of indistinguishable to
the programmer or if they have
indistinguishable predecessor objects
and they have the same type and the same
predecessor field plus there now they're
distinct under this definition they'll
be distinct under this definition as
long as they aren't you know connected
together somehow price to Justine linked
lists are distinct in you're
representing right right so so basically
the idea here is that you know if you
think like a point object in a ray
tracer right you know you'll have many
point objects they're all the same class
they're all is a you know instance of
points but they'll all be you know you
might have some representing the verts
vertices of some triangles that are your
rendering you might have some that are
representing the bounding boxes of the
space decomposition tree you're using so
they're all you know representations of
distinct concepts in the problem domain
right so we sort of what I want to
capture that so here's sort of an
example that I'll use is a running
example through this we have a simple
program that manipulates arithmetic
expression trees right and so we have
some you know expression objects they
all inherit from some expression class
we have add multiply subtract we have
some constant objects so these little
cons hanging off there we have some
variable objects and you know something
you might want to do a lot in this is if
you have the same you might want to in
turn these variables in some sort of you
know tables so that if you have multiple
references to the same variable they all
share the same reference and so in our
case we have the variable objects in its
little variable array and sure enough we
see that too
terms in our expression tree both have
their left field pointing to the same
variable object here so this is sure
like that and so if we apply these
indistinguishable any principles while
we see the first one says okay here's
this recursive backbone structure we say
okay look at the variable objects
they're both in the same array and they
have the same type so it'll be
indistinguishable and the constant
objects are of the same type and they
have indistinguishable predecessors so
we'll group them into the same
equivalence you know class conceptual
component as well and so the resulting
sort of abstract graph model that we're
going to do the measurements on looks
like this where we have one node for the
conceptual component that represents the
backbone of the expression tree one node
that represents the constant objects one
node that represents the variable
objects and then our variable array
sitting off there yeah the work that was
done with data structure analysis who's
the static males who said it was not by
a crisp under an admin yeah so does it
look the same distinction to or is it
different um it is a little different
they basically do a local I mean if I
remember correctly I believe they do a
local points to analysis first to sort
of compute a sort of partial points to
graph for each method and then they do a
sort of top-down bottom-up
context-sensitive at flow in sensitive
approach to unify these various local
points to graphs into the global points
to graph or you know the global points
to grasp that they need so again this
goes back to the fixed naming versus
sort of this ability to generate
partitions on demand so I mean will
generate as many const partitions as we
meet even if they're all allocated at
the same site but they end up being used
in different places right we'll just
look at the structure of the graph and
the types in the graph not necessarily
the allocation sites or anything like
that so you'll get I mean depending on
the exact code you'll get similar
structures but I think this is in
general going to produce a more refined
result but I can another track
objects are like in the abstraction of
add multiply subtract well for this
study we don't I mean we're just I mean
in this study we just want to look at
the structure here but certainly you
could in the abstraction and we do that
in some other work where we actually use
this for doing some memory use analysis
yeah okay so um any other questions or
okay so you notice here that well okay
so we did a nice job of sort of
identifying the components that are
going on in this heap and that are you
know the various parts of the heap that
the programmer is using four different
things but we've sort of lost some very
important information right so now for
instance we just have some sort of self
edge on this ad multisub right doesn't
really give us information whether or
not is a tree or a list or some sort of
cycle right so we probably like to
recover that so the first additional
property we're going to have is some
notion of layout for these you know
regions with self edges and we're going
to have a really simple way to do this
you can either be atomic there's no
internal connectivity you can be a tree
so you're a binary tree or a kre tree or
you can be a cycle right and we're going
to order these so you know a cycle or a
tree is a site as sort of a degenerate
cycle okay um so really easy notice an
atomic layout has ok so I said this and
yes so that's a pretty simple set of
shapes to label these things with and so
our example here we say ok well now
we'll say this has a tree layout so this
means this is a kre tree over these
objects the other thing we sort of lost
some information on was this sharing
between pointers that end up being
grouped into the same edge right so each
of so this edge basically represents
some set of pointers that are stored in
the ad multan sub-objects in this
component that point to const objects'
in this component right and if you
remember back from our original
abstraction go back a little bit here
well we see that the variable our intern
array has you know each each index in
this array of points to a unique
variable object right although these
variable objects might be shared by
multiple elements of the expression tree
and
here we've sort of lost that information
we say well there's some pointers from
the expression tree to variable objects
they may or may not share and there's
some pointers to the variable array the
variable objects they may not sure so we
want to refine this a little bit and
we're going to find this in a very
simple way yes sure if you have a bat
pointers then everything becomes a cycle
so it'll actually we the way we detect
recursive structures is a little subtle
but it actually allows us to distinguish
between let's say things that have
recursive types because they're building
some unbounded recursive data structure
versus things that have recursive types
because let's say you know each variable
needed to know let's say there was some
larger context about where this
expression was defined in each variable
needed to know about its definition
context or something so you had this
back pointer from the variable up to the
definition context that's sort of the
the example you're thinking of would you
like label inside of each of those had a
bag pointer do blable it cycle or would
you still light lit tree in a regular
cycle so this is one of the things we
looked at we started out we said let's
be really simple it's a really simple
tree or it's a cycle and maybe we could
be find this to be doubly linked list or
a tree with parent pointer but we found
out and we'll get into the details here
but it really didn't make much of a
difference or we sort of left it is that
so this this notion of injectivity so
we're each edge represents some set of
pointers and we're going to assign it
either injective or not injective and if
it's injective that says all pairs of
pointers that this edge represents do
not alias they point to different
objects okay or they may alias so there
may exist a pair of pointers that might
point to the same object now I want to
point out that this is a really really
strong sort of definition in a sense
that the notion of injective is about
the strongest property you could assert
for non aoac it's saying all pairs of
pointers don't alias at all or you know
a very very weak statement there might
be some power pointers that alias
somehow but I'm not giving you any more
information so this is a very strong
property that we're asserting their so
if we look at how that sort of refines
our our structure here we basically
split this one edge to now
distinguish between the are pointers
which are injective pointing to the
variables so each our pointer in the
expression tree points to a unique
variable and the L pointers which
actually may alias some of the variable
objects okay so then we're going to use
these properties and we're sort of going
to think about as we go through these
measurements from general questions well
okay how many components are used in
practice right it's just sort of a
general structural question you know and
does this number correlate to the number
of objects or types used in the program
right is there some sort of one-to-one
relationship between types and
components you know how prevalent are
some idiom specific things like our
immutable types and components used a
lot how often we say the singleton
pattern you know in degree distribution
is too simple graph structural property
tree cross back edges we want to
classify these and then we also wanted
to look at some more sort of application
specific questions a little more
motivated by the research so you know
are heaps built up as sort of recursive
data structures or they built primarily
by aggregation of sets of objects how
prevalent is local ownership there's a
lot of work on ownership type systems
ownership annotations and we're going to
look at sort of local ownership and this
is where as ownership is a transitive
reachability thing we're going to define
local ownership as just does this
pointer is this point of the unique
reference to the objective targets so
basically global ownership implies our
local ownership and it's a little easier
to measure how prevalent is injectivity
this is a very strong property I mean
maybe we don't see it very often maybe
it's too strong and you know when we do
see things that are not local owned well
why is this sharing occurring can we
categorize this in a nice way and and we
want some sort of not just qualitative
information we want quantitated
information about sort of the prevalence
of these various things yeah this
methodology
yes previous slide
so just happened that was help owners
were not injective me or reddish orange
and it was just luck right and if we ran
it with a different input data set they
might both be non injected right yeah so
yeah I'll get to the methodology and
just just a second that was the next
line question on this slide this one
felda when you went you went from there
sorry for 16 what's the difference
between local ownership and injectivity
so in this example here right so here
this our field this this edge represents
injective pointers right in a sense that
we're saying it's it's thin and light so
basically every pointer in all that love
using this represents subset of pointers
right and we're certain this is
injective so that means that any no pair
of pointers in here alias the same
object in here what for the whole point
over saying that there exists some
parent pointers in here that may a
liaison the same variable object whereas
the ARP our field no pointer is alias
the same variable object are the bolt
isn't the gold the bowl and bold is not
inject oh sorry yeah so so the
difference between so in this case um
this set of edges okay is injective
right and there are only one incoming
edge to the set of constant objects so
that means that well there's only you
know none of these pointers alias and
there are no other pointers that refer
to need these constant objects so we
know that each constant object is
referred to it moat by at most one
pointer in this edge right whereas in
this case well this edge is injective so
each pointer in this edge refers to one
variable object but there are multiple
incoming edges so there might be sharing
between pairs of sets of pointers in the
two edges what let's local person is
closed yeah so const so this edge is
yeah it's gonna be the local ownership
of these but these variable objects are
not going to be
lyon by anybody right and i'll show some
measurements and we can talk about it a
bit more in a second so the methodology
so what are we going to do so we're
going to run time extraction we used the
I kvm cross compiler to take a number of
the benchmarks from the dacapo sweet and
convert them to Donna bytecode they're
fairly large programs we didn't take all
of the dacapo sweet we took six of them
so like the XML parser there's program
analysis text indexing document
processing they're reasonably well
written object oriented code they were
selected because they sort of represent
a variety of real-world workloads they
have nice inputs that are you know in
theory exercise large parts of the code
so this goes back to sort of the
coverage issue we felt pretty good that
these were going to you know produce
some interesting very deep structures
and exercise good chunks of the code now
what we're interested in is we wanted to
sort of simplify our lives rather than
trying to find these do these
measurements on sort of universal
quantified overall points in the program
and all objects in the program we wanted
to look at these properties when objects
were in their invariant state so we're
going to do our sampling at the entry of
every public method we reduce some
randomization on that but it you know we
did some experimentation and it doesn't
really matter in the results we're going
to produce heaps where all the objects
are in their invariant state so to
public method we're going to start with
the local variables and any in scope
static variables we're going to walk the
concrete heap take a snapshot of this
and then transform this into its
abstract representation and then
accumulate so we're going to do the the
join of this new snapshot with all the
previous snapshots from this public
method and do this for each method in
the program and accumulate all of these
over the run and this is what we're
actually to perform the measurements on
you're going to keep ya ankle one for
math and then we're going to do it okay
so basically here's here's our workflow
so we take the program his room at the
bytecode we execute it take the snapshot
we do this in simulation we might adjust
the sample rate you know if we're not
seeing any new state will back it off a
little bit but as I said you know we
tried running it with full sampling on
some smaller inputs and it didn't really
change the results at all and then we're
going to take this accumulated set and
do a measurement on this right now the
interesting thing with the measurement
is what do you measure right well we can
either compute the rates of these
various properties on the nodes and
edges we see or we compute it on the
types and fields we see right and the
nodes and edges is obvious right we just
look at each graph count how many nodes
satisfy a given property out of all the
nodes and add that up right the types
and fields is a little different because
now you sort of have to again look at
each type in each node that occurs in
each method and basically take the or or
the end of each property over all of the
measured values you see right now
depending on the application you have
you might be interested in like if I'm
doing program analysis why I care about
the number of regions in the number of
edges that satisfy a certain property or
how prevalent this property is it means
because I'm dealing with nodes and
regions in the abstraction if you're
designing a type system well then you
care about these measures in terms of
you know the number of typed fields
because that's what you're trying to
specify so we measure in terms of both
and we see slightly different results in
the two so it's kind of interesting a
couple of you know sort of the default
boilerplate slide about some warnings
there are course some limitations in
what we're doing here right this is a
runtime study we're looking at object
oriented programs written in a memory
managed language and so the results we
get here you know mayor I mean there's
some things that will generalize some
things may not but you know some care
should be taken this probably isn't
representative of low-level OS code
might not be so representative of how
functional programs build heaps and even
if you look at how C++ programs build
you know these data structures they
don't have memory management so the
notion of ownership and sharing and
lifetime might be fair
different than in an object you know a
memory managed language like Java or C
plus C sharp sorry where you don't have
to worry about these things further okay
we're doing an evaluation over one suite
of programs with the specified
abstraction so you know it's Mike
pointed out earlier we might miss some
interesting behavior if this is in
exercising code or isn't exercising
certain programming idioms that are out
there and you know we might be masking
some interesting information by the
abstraction now you know we picked the
dacapo sweet because in theory this
should be a fairly representative set of
benchmarks and inputs and if we do mask
information with the abstraction that's
just going to reduce the fidelity of the
measurements we take it's not going to
introduce any incorrectness just might
reduce the precision a little bit and
we'll see that in fact the we seem to
get very precise information so this
does not seem to be a problem so now I'm
going to just sort of talk about some of
the results the measurements we saw what
I think of some interesting features
that we pulled out and some implications
there now we meant this to be sort of an
empirical study for people to use and in
their research look at find out what
they find is interesting so I'm going to
sort of point out things that we're
interesting to me in my research and my
bias is a little bit but if anybody sees
something on in some of these slides
that they want further explanation of or
you know think looks interesting feel
free to interrupt and I can answer
questions or comment or whatever so the
first thing is just sort of some of
these miscellaneous properties so we
have our six benchmarks here antler
chart fop oh um decks oh you index Kim D
and Xalan we have the number of objects
that the largest heap snapshot taken we
have some information on the types that
are seen so the total number of types
that we saw instantiated in the heap
this differs from the total number of
types declared well include or multiple
in like
yeah woman the the i guess the default
medium input on for all of these yeah so
the number of types that we detected is
being singleton the number of types that
we detected is being global in turn type
so this is like our variable array so we
basically had a static field that points
to a collection that has objects of this
type in these and we only see these
objects in this static collection and
then we have the information on the
components so how many components were
identified for the largest snapshot the
number of components that consisted only
of immutable types so that would be like
system dot strings system dot in this
sort of thing and the number of
components that consisted only of system
types so that could also be like you
know hash set arrays whatever so well
that's not was sort of interesting here
is we one of the questions was well how
does the number of types relate yeah
distinguish between the libraries in the
application so anyway yes so we in the
abstraction we treat the libraries a
little carefully rather than basically
exploring the internals of how hash that
is represented we recognize it as a as a
hash set so it's an opaque container
from the programmers perspective so we
basically just rather than sort of
saying this is a object that is a half
set with you know a hash table with
buckets hanging off we say this is an
opaque thing that represents a hash it's
like a half set object its atomic
sultans for all the built-in contains
yet right the JIT compiler come into
play
in what sense I mean if the jib were to
run while you were doing your
measurements right you'd be measuring
the gym as opposed to the application
again we're using got Nets they don't
have that problem we use CC I to see my
Jack's okay after ya know it's but if
the JIT were like a Jegs the chin is
another job program so it's all sound
state right so that's so here we're
actually sampling only at public we're
using CC I to rewrite the bytecode
program to add instrumentation code at
public methods right and we freeze all
threads we make sure that nobody's doing
anything and we only walk objects that
are met yeah that are visible for that
method right in the application space
yeah so you're sampling the application
space for the local objects and
everything the local variables and
everything reachable from those so but
there's the other part of the heat right
which is up the call staff which we're
not looking at because those objects
might not be in there you know invariant
stage yeah and so we want to basically
compute the sort of invariant properties
for the objects you have a rough sense
of the fraction of the heat that you're
actually looking at
throw them the largest I mean so we
should get near the max-heap size
because we sample it every public method
right so each part right and how long
again it depends on how much state the
runtime hands yeah so I mean like just
just I mean rough ballpark figure we
seem I mean the max reported mem usage
for the dacapo benchmarks we seem to be
about three quarters of that so we're
missing something something floating
around out there that we're not getting
but we seem to be getting you know quite
a bit of stuff percentages are with
respect to objects not the percentages
so these percentages are with respect to
components and these percentages are
with respect to the types so basically
what we did here is we said we found 606
you know nodes conceptual components
after we did the abstraction right we
looked at each of them we found out that
twenty-five percent of the 606 were only
immutable objects in that component okay
be interesting to compute that as a
percent of objects in the whole need and
also a percent of bytes right yes yeah
yeah I mean I I believe p I mean I could
swear I've seen work that's done that
sort of study before but I could be
totally mistaken yes okay well that
metric so it's interesting for you to do
it right yeah okay fighting for watch
 40 first one very singleton
right so basically this says okay we
looked at all the time so there are six
hundred and six components right these
were created using 63 types and we
looked at the number of components that
were Singleton's right and there were
none of them and so there were no types
that we're being used as singleton tight
okay so what I think interesting is here
is particularly an antler here you see
that there are sort of 63 types makeup
606 components so obviously each type is
being used to sort of represent more
than one concept from from the the
application space on the other hand PMD
here we see 162 86 instantiated types
but only 146 components were identified
so in this case we actually are using
well more than one type you know to
represent a single concept right and we
similarly see some fairly wide variation
so far pair has fifty three percent of
its types that can instantiate adore
Singleton's right and they use the
antler with this nutin as well also I
thought it was interesting the very high
percentage of components we identified
that had a mutable types only so I mean
that really was like hey programmers
really like immutable things it's much
easier to reason about than other stuff
and we're actually under counting the
immutable percentage because we don't
actually look at user-defined types that
are immutable we only see system types
that we know to be immutable so this
might be an undercount as well so the
next thing we sort of looked at was one
of the more directed questions which is
ok how do programmers use a notion of
building recursive data link to
recursive data structures to sort of
organize their there you know program or
do they try and use aggregation I have a
set of objects I have another set of
objects that a pointer to this object
and I sort of aggregate and compose up
so in this example we have you know one
component that is marked as being
recursive this tree we have three
components that are marked as being sort
of atomic or just simple aggregations
and then we have you know since this
component has three types we have three
types that are marked as being part of
recursive data structures and then since
each of these has one type we have three
types through are marked as being
aggregate types so when we did these
there
Germans over the program's this is the
ratios of the shapes per per node or
component so we see that atomic is
incredibly dominant like well over
ninety percent of all the components end
up being just simple aggregations now
this is a little biased because of the
way we do indistinguishable ax t we say
Oh everything that's part of the same
recursive backbone gets collapsed into
one component right but still this is
the mean this is not this is saying that
even in this case you don't have the
case where the programmer has lots of
linked lists floating around the program
right the programmer is building one
recursive data structure or two
recursive data structures they might be
getting collapse down but they're not
using them all over in the code they're
using them in one or two places so when
we wouldn't looked at types this was
kind of surprising because now
especially like PMD here right I flip
back you say oh okay there is some
cyclic structure component identified in
PMD but you know it's not heavily used
but here over fifty percent of the types
in PMD are involved in this recursive
structure and so we were a little
control I was confused by this I was
like oh great I got a got a great butt
all right let's go find out what my
codes doing wrong so we looked at PMD
and it turns out that ok so PMD actually
is a java source code analyzer if I'm
remember correctly and so it builds a
big recursive data structure for the
entire java source code parse tree and
so it has some class i think it's like
you know java you know statement or
something like that you know java node
and then it inherits from this in sort
of a classic object-oriented style one
subclass for each type of note you know
no door statement that can occur in a
java program so you'll see you know java
while statement java if statement java
expression statement java conditional
statement etc and these are all linked
into this is one big data structure
which is the recursive structure that's
being processed and then each subtype
will have some additional information
that's specific to that particular type
of abstract syntax tree node and then
these are built using sort of
aggregation and they'll hang off this
tree in various ways right so this is
kind of shaken solar things in an antler
and Xalan we tend to see tended to see
you know there would be one very
the application-specific recursive data
structure that the programmer had built
because it was a very good
representation of their problem domain
but in general if there wasn't this the
strong need for building the structure
they much preferred just using
aggregation and using built-in container
libraries so that was those quite
interesting to me avisa have UI
components and there they like
frameworks of a irregular I know that
they've been pretty much stripped of
their UI so they're nice and easy to run
from the command line I I can't really
answer confidently about the the
framework stuff although I didn't see a
lot of framework code in there I mean
stuff like zan Lanny is sort of
framework code it's a xml parser right
and LU index as well does text indexing
so they're sort of designed to be used I
think by other applications but they
don't really depend on large amounts of
sort of infrastructure or other
framework themselves from what I saw do
we don't have to solve the problem
interactive input test me right fine you
saw the tongue against it if there's
code in there it's not exercised right
okay one thing I think that you can you
might conclude if you look at that
Beckman code is that be those kind of
complex deeply nested and sort of
highlight object re frameworks we have a
lot of types basically and they likely
would be sort of it to do it the same
way that p.m. de you know has these
abstracts and textures that have all
these different you know sometimes you
might see this
with the UI yeah okay so the next thing
we looked at were sort of these these
properties of you know the indegree and
local ownership so in this case you know
obviously in degree it's pretty easy
this node has an integrative three this
housing degree of one not a problem we
discussed the local ownership a little
bit and so local ownership is defined as
this edge or field depending on how
you're measuring holds the pointer that
is the unique pointer to the target
object right so in this case the none of
these edges are going to be local owner
edges because one there are multiple
incoming edges to this so that might
mean there are sharing between pointers
in different edges and this edge is not
a local owner edge because well it's non
injective so that might mean there might
be atheists in between two pointers that
this edge represents whereas this edge
is going to be a local owner edge
because it's a unique incoming edge to
this and it's non injective so that
means every pointer represented by this
edge points to a unique object right so
if we go ahead and let's look at the in
degree distribution first so this is the
in degree distribution per node or
component and as you expect you sort of
see you know these are violent plots so
and it's also logarithmic we labeled so
basically you see that in this case
there you know in degree of one or two
is that is very very prevalent and it
tails off rapidly as the indegree gets
higher what's kind of funny is it over
here if you look at the indegree you see
it isn't quite so so clean you see fop
has this funny little bulge here at
about three or so right and this goes
back at ok and then so this this is the
issue I don't know if you remember from
FOP it had a huge number of singleton
types right fifty-seven percent or fifty
five percent and so what it ended up
doing is it declares all these singleton
types and then it places them all in a
dictionary so that it can look them up
quickly through other names and so that
means that each of these types basically
has one incoming edge from the static
reference and one coming incoming edge
from the dictionary so it gets this
funny little bulge here PMD has this
goofy bulge here about seven or eight
and this is from the fact that you have
this large num
of types building this abstract syntax
tree and then there are several
references coming in to this abstract
syntax tree so that I'll get a slightly
higher reference but other than that you
see it mostly as you'd expect tail off
pretty quickly you see a couple of types
have just a sort of weird outlier number
of references coming to them these are
usually singleton types so this is this
was nice this sort of made sense and was
you know consistent with what
expectations would be then we have some
other stuff so here we look at the the
number of pointers and this is measured
in terms of the fields that have this
property that our internal that is they
are building up one of these recursive
data structures and again we see sort of
even for PMD here which had that large
number of types a relatively small
number about ten or fifteen percent of
the fields are actually used to build
this very large recursive data structure
and if we look at the source code we see
that basically this recursive data
structure is built mainly by the fields
in the base class node which has an
array of sub expressions and then each
sub expression has a parent expression
so the actual data structure and code
that builds this very large recursive
structure is fairly localized in this
parent class right and then there's just
a lot of code that does simple
aggregation and manages these other
auxiliary structures non nullity I mean
it's not too surprising it sort of
matches i think what people's
expectations are it's pretty prevalent
that this this field is never null in
its invariant state and then the local
owner property right and so we see okay
here that this is I think very
interesting because it says ownership is
something that we would expect
programmers to strive for their local
ownership makes it easy to reason about
things makes writing program safe makes
the reasoning local and we see that okay
yes it is important you know over fifty
percent a lot of benchmarks but it's not
Universal there's all there are a lot of
fields that are not always the local
owners of their objects there the object
are pointing to is getting shared
somehow right and so we need more than
just this simple ocean notion of
ownership to really express what's going
on on in the heap and keep in mind this
notion of local ownership is sort of
implied by the
the classic notion of ownership types
right so if this doesn't hold then on
this is definitely not an owner field in
the ownership type sense so that means
that your ownership type ownership is
even less than this right so the
question is where is all the sharing
occurring yeah so i'm going to say or
feel you mean that in classified
particular source field say no if every
and yes right to them then I'm not sure
about the internal measurement interest
as you're building up a recursive
structure
once you find that if it only consists
of one or two nodes yet and maybe
they're different type that you will not
get classify you will not collapse it
and not classify one of these edges as a
maternal if it's a small and only once
you start seeing more notes will you
start classifying internal which means
your goals
use that number are officially very much
right actually we do it the other way so
if we ever see it involved in recursive
data structure we say it you know it's
involved in some internal yes yeah so
the quantification goes both ways it's a
little weird but we always try and pick
the one you know that sort of bias is
the right way so yeah okay so back to
his ownership is there are two ways that
the the local owner something could be
shared of this local owned property
could be violated again because you
could have non injectivity or because
you could have multiple incoming edges
so the easy question answer is how
frequent is this injectivity remember I
said this is a really strong property
right so here's the rate of injectivity
/ edge that we see so blue is the number
of edges that are injective right it's
over eighty eighty-five percent in an
ivory benchmark right and in the cases
in most cases where we see an edge is
not injective it's because it points the
target objects are immutable so like
strings or something in which case if
you're sharing strains who who really
cares they're immutable and if we
include just this then every you know
over ninety percent of the edges in
every benchmark are injected so that
means despite being a really I mean
about the strongest aliasing property
you could assert on these edges it's
almost universal right it's just fairly
surprising to me now the question is
does that hold for fields as well and
yes it does I mean again we see that
this this sharing on immutable is a
fairly common idiom right here like 30
or 40 percent of the the fields in
antler are not injective but they're not
injective on on immutable objects mainly
strings but once you include that still
almost ninety percent of the fields end
up being classified as injective or
injective on immutable so this is a
really strong property in the type sense
as well so this means that the local
ownership is almost never violated by
non injective edges it's almost has to
be violated by this this high-end agree
so I I really like this I thought this
was a great result and if in I'm curious
to see if anybody doesn't believe me
okay
clear that if you have aliasing to
different parts of the list that that
that's confusing I guess a data
structure and then if you're going to
use some list in multiple places you
would alias at the top right so is that
the intuition here I'm not quite clear
so basically what this says is that you
never see like you always end up sharing
from you know this list and that list
both share the same objects very rarely
do you have one list and you have a
bunch of sharing from the pointers just
in the same list for example your
indexing some lists two different ways
to speed up search right but mostly
you're not going to share right so what
you're saying is if I'm going to index a
list in two different ways to speed up
search I probably won't make two copies
of the same elements the first half all
indexed one way in the second half all
index the other way that share I'll take
two lists and made two copies in two
distinct lists that have two different
roles in the program and do the sharing
that right yes exactly and so when we go
and we actually do this edge
categorization so we're gonna look at
how many cross edges right we see that
okay the tree edges which are sort of a
prerequisite for this this local
ownership are high but again not
universal but the cross edges are pretty
prevalent right and this says yes this
sharing comes from you know two
different components that have two
objects playing two different roles that
are sharing some common state that they
need to either both reason about or
reflect changes through so we see that
okay this cross edge thing occurs fairly
frequently and this is where the non
ownership comes from and so the question
then is uh you know is this these cross
edges or they just created arbitrarily
or their general sort of idioms that
programmers use okay and we see by field
you know about the same same ratio or
are there some idioms that programmers
generally used to sort of do this
sharing in a relatively controlled way
or a standard way to make it easier to
reason about so here I show the number
of cross edges we found in each
benchmark with sort of the min/max there
for easy civilization we looked at the
number of crime
edges that were cross edges on immutable
objects so we see that again you know
programmers like immutable objects
because it's safe to share them they
don't have to worry about how these you
know they can't change they don't have
to worry about sudden unexpected changes
we also see that you know a fair number
of programs end up doing a lot of
sharing on singleton objects and also
these objects and global in turn tables
which is good because I mean that's sort
of the point of a singleton object is
everybody to share it we also looked a
little bit at this notion of local
escape right and so this is we saw a lot
of programs which I think go back to
your question where I have an object it
has let's say a list of things its
stores and then it also wants to be able
to look them up quickly by keys so it
also has a hash table of things it
stores right and so really the sharing
is all reason about locally in the
parent object which just has a vector
and say a hash table or an object that
has a vector of possible pen colors and
a pointer called current pen color and
they share but it's very local and so we
saw this as a fairly common idiom
especially like you know you index like
ten percent in though the cross edges
run this and so when we sort of looked
at all the all the sharing that occurred
in these categories we ended up with a
relatively low percentage of things that
were sort of remained unclassified or
that the sharing was just sort of
general or done in a ways it was looked
fairly specific to the particular
application that the programmer was
implementing so um this was pretty nice
they said hey look a lot of the sharing
is sort of idiomatic it's well-behaved
it's done in a standard way so that it's
easier to reason about if we looked at
it the number of fields the cross edges
/ field we saw a relatively sort of
similar behavior right leaving about
four to 16 percent of fields having some
sharing that couldn't be classified by
one of these fairly basic idioms so the
question is we made this hypothesis at
the beginning about these roles and
components right and the issue is if we
were this hypothesis was invalid right
we could be grouping objects and just
sort of arbitrary ways that mixed
objects that had different behaviors
together in the same equivalence class
and was going to cause us to lose
fidelity in our measurements but we see
that in most cases we have a very high
information classification injective
that gives us an incredible amount of
aliasing information and non injected
which basically gives us no aliasing
information we saw that in general we
were very highly biased towards the high
information content measurement fairly
consistently and this high information
content bias was usually the sort of
more natural one from a programmer
perspective so we feel fairly confident
that our our notion of roles and
components is probably a good way to
sort of map map well to programmer
intent but fairly certain that it also
is not destroying a lot of information
and causing us to lose a lot of fidelity
right so we're fairly confident that the
results we have here are you know
representative fairly precise and and
meaningful and this hypothesis is not
throwing in a lot of spurious
imprecision um so the conclusion well my
conclusion at least oh sorry did you
have a question one question about this
objective and property is so obviously
you want to take advantage of it right
right you want to look if you want a
reason naturally about that property
right so how does that power that work I
don't know that's the problem right I
mean that thing it's very strong yeah in
I mean this notion of roles Maps well to
sort of programmer intent but there's no
nice token in the source code that says
all these objects have the same role so
they should be in the same component
right i mean that's that's not clear so
the question is how do you take this
property which is on sort of an
aggregation of objects which play the
same role and map that into the
Declaration of each object so that you
can say you know this guy when he's in
with objects that the other the same
role is going to be very well behaved
but something something and I don't know
what that is yet I mean you can declare
it in the type system until ownership
tribes
yeah well and I'm not clear same either
necessarily because I think with your
lie for my I see it here easily locally
say look you know I have a bunch of
pointers here locally I know there may
be other point or somewhere else
acquaintances but locally I know that my
point is i'll never do and thats a nice
apartment is not the same as ownership
and also it seems to be sort of organic
right i mean i don't think the
programmer has in in mind this set of
objects is you know the same conceptual
component i think it happens organically
through sort of the grouping of you get
through containers they say i have a
container this container has pointers to
blah and they're sort of implicitly
making this statement but i don't think
it's necessarily clear so i think if you
want to attack it from the source code
perspective adding annotations to
containers is the way to do it but i
don't know the answer so I'm all
mentioned that a little bit in my my
slide after this one so my conclusion I
think there are lots of conclusions that
people can draw and I'd be interested to
hear you know what other what
conclusions other people draw but I
Zaillian look this is really cool i'm
really excited about this this means the
heap is not this wild woolly horrible
object at least from a certain
perspective but you know recursive
structures and not that that comment i
mean a lot of reasoning is aggregate so
if you really have a decent way to
reason about aggregate structures you're
in good shape and the the decomposition
we do does a pretty good job of
isolating these recursive structures so
that you know they they're in a small
little chunk and maybe you can bring
some more powerful reasoning directed to
them that's great ownership is important
but is not universal so it's important
to have some way to reason about this
but the nice thing about the
non-university universality is that
common idioms cover a lot of the stuff
that isn't there so you don't need this
rich description system talk about all
of this with a few simple properties you
can say well let's let's put these in
and we get pretty good coverage of the
sharing that does occur and especially
for the static analysis perspective this
this idea that the conceptual component
of abstraction plus injectivity is a
really effective sort of way to capture
a lot of this information expresses a
lot of stuff isolates these more complex
structures and and that's really i
thought was really great from a static
an hour
perspective so what does I think the
implications of this study are for
various bits of research well I think
they're clear applications for static
analysis because I like that that says
you should favor set based reasoning and
you only really need a simple set of
properties and I've already incorporated
this into some recent research and you
know not to be too modest but I think
the results are amazing and I'm just
thrilled with them so you know they're
phenomenal but I think they're also some
solid applications for type and
annotation systems right that you really
you know you don't need to design at
least to get some good an initial start
over being able to specify these
structures you don't need a huge rich
complex system you can pick a small set
of simple properties that are fairly
natural to programmers and hopefully
give them these tools in a way that they
can easily use them and express things
and not be overwhelmed by I don't
understand the semantics of what this is
trying to express I don't understand how
this is should work ah something should
be a singleton that's great right you
know sometime sharing this thing but
it's only on immutable types I
understand that as a programmer it's an
easy thing for me to write and
understand and and if the system spits
that back out at me I know what it means
um you know other areas this is stuff I
I don't have so much experience with or
I haven't thought about in such detail
software quality metrics right this is a
big question you know we measured these
structures over programs that are
relatively bug free that are relatively
mature that are relatively well tested
do these metrics vary greatly if you
have lower quality software higher
quality software does a high degree of
sharing correspond to hire defect rates
and something I'm not clear on this
memory management um you know there's
some interesting structural information
here that might be useful in coming up
with some some different memory
management stuff program optimization of
course if you know that a lot of these
structures don't have aliasing maybe you
want to try and be a little more
aggressive in doing some optimizations
it would benefit from this so let's see
thanks everybody for coming i think
that's that's it and i have a little
time for questions and we do have a tech
report up it's an MSR tech report
collecting a heap of shape so if people
are interested i mean i can email it to
you or you should be able to find it
pretty easily
other than that thanks a lot for your
attention and I'm happy to answer any
questions or comments or whatever yeah
the surface it reminds me a lot of the
this kind of surprising result with the
data source data structure analysis work
where they basically said they could
build these separate keeps you know they
can put stuff in them the point there
wouldn't be pointers except from certain
places into those heaps yeah and it was
all thru static analysis which it was
even like wow in C right so that is
really consistent i also have a surprise
so but but i have to say i never really
trusted network and part of it is that
you know it was only for a certain set
of programs it was i don't think i don't
think they had a good results say for
GCC which you know is kind of like I
think it's kind of like that's a great
program yeah um and I think I think the
same thing you know I this is very
interesting and I know it's so
surprising to some extent I think the
big question I would have this you does
it if you look at bigger programs you go
beyond you know those are interesting
programs but I don't necessarily think
they're common that net programs a lot
of the data programs these frameworks AZ
wise they use you know well you know I
would say I you close with a result for
eclipse would give me would make my
heart feel better for me mark wouldn't
the larger program fee group are sort of
boring consolation I mean oh what a
stink at any other time so maybe it
would be a direct result yeah but I'm
just know i think is the issue you'd
have in order to get that information
you have to drive the benchmarks
differently you'd have to drive them by
hand
because otherwise you're not using the
user turns right you don't want a
glimpse right you have to pull epilogue
lips but anything that uses all these
frameworks usually has a user interface
with it that is making all those things
happen right right sure how much slow
down did you see uh uh a lot and the
game is so many minute version no I mean
okay so the very fortunate thing was is
I was a little nervous we started this
you know I mean because basically we're
doing daikon style invariants right
we're computing likely class invariance
for these objects and you know my member
might might I when I read the daikon
work or my take away from it was yeah it
generates a lot of invariants and a lot
of them aren't that meaningful right it
just generates a lot of stuff and so I
was concerned that we would really have
to sample these with a really really
high rate to sort of get rid of all this
you know spurious invariant stuff but
what was really I mean kind of
reassuring to me is that these
structural invariants were pretty much
universal like we would take two
snapshots of a method call and then
every snapshot we would take after that
would pretty much not be changing right
the answer calories in versus you can
produce the estimated of these programs
and let people use them
it wouldn't it wouldn't be that fast I
mean it still took me like you know six
hours to run some of the larger things
right but you know it wasn't as horrible
as I was excited is expecting to set it
for a week and go home you know take a
vacation yeah okay really that you use
for positive line teen babes i think the
original ipod work they were using
arithmetic and my choice very explosive
the number of terms you can construct
yeah whereas here I think you have
deliberately we put signature vocabulary
yeah but I also it's a reflection that
programmers don't I mean they want to
have objects they're very invented they
don't want to have a lot of exciting
stuff going on in general so you know
they say this is the structure and maybe
we have two or three variations on it
you know the sort of type state thing
but really we try not to be too creative
you know because then the reason it
becomes all nonlocal for the developer
and that's just gets unpleasant quickly
um you've had a result where you've had
the number of times greater than the
number of instances just how can that
happen if you're
so that was um 4.png right and this is
where you have that huge tree that's
made up of all the subtypes right and
these all get compressed down into one
component so this component has
something like sixty two types
associated with it right so the
structure all in the same data structure
yeah okay
so is there some like okay I now get
that ownership is different from what's
the property you're saying if you're
uniquely pointing out all right sorry
the injectivity injectivity so it's not
attractive so non-objective is this edge
might have some sharing the objective is
it it's basically a function we're
saying that the function from one set to
the other on this edge label is
injective injective is a property of the
outgoing corner where / ship is a
property of the incoming point uh no
ownership would be the outgoing pointer
as well right it would say this edge
consists of pointers that always point
to objects that they're the only point
or two but but damn you're saying object
has no other pointers so that's saying
that coming in is one object and and and
so on a definition only one person can
point to you if you only have one right
so so any activity is basically a
property of that the pointer and the
source object whereas the ownership is
you know requires multiple objects in a
little bit wider context yes yes
measurements observation about oh well
it was partitioned into things that have
between top-level things so with even
here by 11 forest is emotional yes you
mentioned that I think we were supposed
to measure that and I didn't why didn't
I do that Chris favorite dinner yes so
we I think we initially we wanted to do
this basically look at sort of this
notion of sort of cleats size or graph
span or whatever but from my personal
experience what I can tell you is that
you tend to see things chunk apart
fairly well into modules and then what
you'll see is down at the bottom there
will be the singleton types or strings
that everybody ends up sharing so you
sort of see this tree like structure
where everybody continues to isolate out
little boxes and then merges at the
bottom and what you'll also see
sometimes is sort of some scary back
references that create huge connections
between large chunks of destruction
which sort of terrifies me like an
antler I mean I guess it's part of the
way that the program has to work but it
certainly was scary to be like you know
in DJ ml you can say show me the
strongly connected components and it
highlights them in red I don't know if
you've any buddies played her out of
that so I say show me this right and it
just highlights the whole graph and I
was like oh god this is this is not good
looking yeah it's just not enough I I
think that's the other thing we need to
look at this back edge issue
right so so if it takes us a simple why
are we so stuck in the sky absolutely I
don't know I mean that's sort of what
this was this TAS about and so I think
one of the questions that people come up
with all these properties right i mean
people come up with all these properties
right and they're very enriched
properties and very sophisticated things
right but my feeling was and this is
what i did i sat down and I read these
papers and then I looked at source
examples and I said oh what do I need to
read about these right and I feel like
the things that I was coming up with
these these tuyo examples were more
complex amples could be more complicated
than what programmers did in practice
because they just wanted to write the
simplest thing that was obvious and
worked right the deeper answer to that
because in snack analysis you have to
looks inside the methods and the
intermediate state is very confusing
because of willy nilly references from
local variables to disobey that's right
because you like basically throw all the
pointers up and then you put them back
the way they're supposed to be put back
together and so it's that local internal
state that screws up static analysis and
he was not measuring that that's real
only at the pub so the middle deck
students okay so i want i want one
common tonight so i definitely agreed
that the local state is more complicated
but one of the questions you have is in
c++ you see the idiot you mean it has
you have to you have to know who the
owner is and someone has to dispose of
the object so there's a lot more I'm
going to take the object I'm going to
move it from this container to that
container and you now become the owner
right in these object oriented programs
you know there's been some interesting
work saying the behavior of these is
actually mostly functional right I don't
transfer ownership so much as I create
copies and I create new objects and I
assign you know the fields once or twice
or I don't move this object from here to
here I just create a new object and
overwrite it so they're sort of mostly
functional and so this was the other
thing that we used in the static
analysis working we said look if we know
these structural properties and well
behaved and if we assume that most of
the updates are these sort of mostly
functional type of updates that means
weak updates in general are fine right
we don't need all the
strong update machinery we don't need
case splitting we don't need to do past
sensitivity we don't need to do all of
this and how much precision is this
going to cost this in practice on these
programs and so we went and ran this and
basically we see very little information
is lost by just using week updates in
these object oriented programs right so
we're almost as precise as we were when
I had the case splitting and the strong
updates in the past sensitivity and it's
much simpler and much faster work it's
nice that I analyze object oriented
languages of garbage collection rather
than C++ which is painful right to take
the local state and have an assertion or
something like so what we did in the in
the collections work with teenagers
frontosa poeple this year we use three
valued logic to try to capture
properties of collections to get rid of
synchronization right that was our
optimization and what the huge
difficulty turn was always the internal
state and so in some cases we just
serted after that poor for sub method
that confused the internal state what
what was true at that point and then the
pointer analysis no shape analysis work
and I think this is really important I
mean in the static analysis I would work
or work i was mentioning earlier we
basically took containers and we treated
them as ideal algebraic data types so we
don't actually have the analysis explore
the internals of the container
representation which is you say it's
very complicated they do lots of pointer
swapping and whatnot and we just say an
ad takes a pointer and puts it in
magically right and in other words an
opaque thing to the analysis just like
it is to the user and that was really
important as well and it makes it much
simpler this is the static analysis work
that you've done yes
that was his step yeah and so that this
is a continuation we just finished it up
this summer and so it's studying an
allocated only everything's to get you
approved like you want to prove things
about the programs and that's the only
evidence allowed in here you sample the
deeper grantham you look at the actual
thing you can't prove anything about it
yeah apparently don't think there's some
I mean I think there's more interesting
locations hear from you know the
annotation is looking at the heat all
the time right so you have you have
assertions on the heat because it right
okay but I really like your trick for
doing it at method entry and then
knowing what is I think that's a problem
we never solved in Maria's work is we
are you ran out of memory because we
took keeps not snapshots based at the
garbage collector so we stopped at
random points when the heap could be
thrown up in the air and we just
statistically kind of ignored that right
yeah pump I think there is still some
more to support them there's a lot of
good news but your percentages for the
cross ledges that were classified we're
still pretty high when you compared them
as a fraction of the min-max at the top
right boss Yeah Yeah right so tight you
know and I think that's the issue when
you're like talking to people who are
designing time systems well you can say
oh we can capture these patterns but
then the things that you don't capture
they can cause just unfold pain if they
don't if you're types is register I
think I mean I think this is the
interesting thing between type systems
where I need to have a consistent type
at every program point versus this sort
of i think which is interesting is the
annotation system which is i can want to
specify as precisely as possible leaving
this sort of unknown value permissible
at invariant states right and i think
this is a really nice from the static
analysis point where you say look I I
can basically tell you that really if
these you know method injury and method
exit you're in a
track can sort of i mean what i think is
basically this informs like if you you
have designed my contract what things
should be really easy to express in the
contract language it if you can express
these simply then it becomes much easier
to use this contract language for a
programmer because they have simple
things they can write down that are very
important to them right you still have
this I mean this isn't not i mean if you
want a complete system this is a
non-trivial percentage that you still
need to fight but that means that you
know in a relative the time does the the
you know the architect or the person who
really thinks about the way the program
is designed have to sit down and write
some non-trivial specification of what's
happening you know usually it's one line
ah boom you know its shares on a mutable
easy they don't want to do it that way
to some sensitive oh that's a good place
to look to fix the program because
ultimately you know it's just hard to
reason though right right so if you add
paying where you don't want them to do
that then that's good right then they
don't do that is there a related work on
finding annotations you know that go
along the lines that you a very say have
this component you know
which is this type of object also I mean
in my static analysis I've mind these
specifications I'm still not happy with
a way to output these I think this is
the next thing I really want to work on
is what's a good way to express these
usefully right that I can I mean I can
extract lots of information but what's
actually useful to give the program or
what's useful to specif you know provide
other tools and so I mean that's the
other thing I was interested in talking
with some people here is I say look
here's a good way to categorize these
things in my static analysis I can
actually quickly in real programs
identify these things and as this
information sufficient to give you the
ability to do modular reasoning in you
know given methods for the kind of
things you want to reason about or is
there stuff that's missing or you know
what what else do you need this kind of
you know I mean I think this is a really
interesting thing you have a this
analysis which makes gross
simplifications for in a sense right
weak updates everywhere right so
obviously I'm going to lose information
in certain places there's code that does
strong updates that I'm lose information
on but I can propagate pretty quickly
throughout things I can do a pretty good
job so I can get you these sort of
global invariants so you can then do
local reasoning about sophisticated heat
manipulation right and what's a good way
to sort of coordinate on that and what's
useful here I I don't know the answers
to that so you know I really like to
talk with people about it okay anything
else all right great thanks everybody
appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>