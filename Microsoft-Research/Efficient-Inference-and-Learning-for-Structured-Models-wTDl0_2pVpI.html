<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficient Inference and Learning for Structured Models | Coder Coacher - Coaching Coders</title><meta content="Efficient Inference and Learning for Structured Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficient Inference and Learning for Structured Models</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wTDl0_2pVpI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um crafternoon so it's a pleasure to
have Alexander schwing here with us
today Alexander is a fourth-year PhD
student at ETH Zurich where he's doing
his PhD under mark polyphase in the
visual geometry group and he's also Co
advised by Tamir Hassan and rocky Luther
soon in TT Toyota technical
technological institute TTI Chicago and
he has been doing some really
interesting work on efficient structure
prediction and applying it to 3d scene
understanding stereo matching and easier
to talk about it today thanks to the
stuff for the introduction and so what
do I mean by efficient inference and
learning for structured mods so suppose
you are given this indoor image on the
left hand side where you're interested
in predicting the location of the walls
you could go for a pixel wise
independent prediction approach and
obtain a recite similar to the one
illustrated in the middle obviously
obviously that's kind of noisy so
wouldn't it be kind of cool if we could
jointly yet efficiently reason about
physically plausible and hence
structured configurations and obtain
recite similar to the one illustrated on
the right hand side or to give you
another example suppose you're given
this rectified facade image and we are
interested in finding the locations of
the windows the balconies the ball and
so on so again you could go for an in a
pixel wise independent prediction
approach but I would argue that
reasoning within the space of physically
plausible configuration actually gives
you results that are visually a lot more
pleasing so what does it require and to
do reasoning in those physically
plausible in those structured spaces
well first of all we need to do this
inference task somewhat efficiently and
one way of doing the inference tasks
efficiently is by distributing it on to
all the computational resources that you
have available and this is going to be
the first part of my talk where i'm
going to show you one way of doing that
not only do you want to do inference
somewhat efficiently you also want to
earn in those structured models and
those structured models are usually
fairly general and so the second part of
my talk I'm going to give you some ideas
on what you could do if you have general
graphical models also with latent
variables and how to learn in those and
in the third part I would then want to
give you some details regarding the
motivating layout application before
concluding with also some hints on
possible future work so let's look
around us a little bit and let's see
what happened with technology in the
past couple of years so if you look into
your cell phone for example and you
Stefan probably has two cores for course
or even eight cores these days so mighty
core environments are everywhere and
obviously we want to leverage those for
the problems we are looking at so we
want to distribute our tasks in addition
our cell phones acquire information that
is larger than ever before we're having
sensors and rates that acquire bigger
and bigger data sets so we want don't
want to throw away some of this
information so we want to reason with
this within this large-scale setting and
obviously I already mentioned that we
want to recent and physically plausible
that is somewhat structured spaces and
it would be waste if we wouldn't
leverage the structure which usually
constraints and the resolution somehow
so the task order the question I'm going
to ask is how can we formulate this
reasoning task this inference task to be
distributed with respect to both
computation and memory so I need to
first start with telling you what do I
mean by this reasoning what do I
actually mean by this inference task so
by inference I mean a standard maximum a
posteriori or like a score maximization
task so we interested in a set of
variables S sub ones through S sub n and
we want to maximize some scoring
function this scoring function consists
of a couple of terms local scored terms
theta sub V and higher-order scoring
terms theta sub alpha where by higher
order I mean terms that depend on two or
more variables I'm also going to assume
that
scoring term and that the variables we
are interested in are actually discrete
variables so if the variables are
discrete then this local scoring
functions this theta sub with our
nothing is then simply look-up tables
and the structure of the problems the
structure I've been talking quite a bit
about at the very beginning and is fully
encoded in those higher order and
scoring functions and a nice way of
visualizing this structure is why I what
is known as a factor graph so I gave one
example of a factor graph here on the
slides and we have four variables in
this example as sub 1 through S sub four
and two factors one being dependent on
three variables and the other one being
illustrated by green rectangle depending
on two variables and we draw an edge
between those rectangles and the notes
the variables if the factor depends on
this variable and since all the
variables are discrete also the factors
obviously just depend on discrete
variables and therefore or just look up
tables as well so now how do we solve
this maximization task the first thing
to notice is probably that this
maximization task is equivalent to an
integer linear program and to show that
I want to walk you through a small
example namely the example of those two
notes being connected by one factor and
in order to rephrase the problem we are
going to introduce some variables I
denoted them by B sub D and B sub alpha
for beliefs so now suppose we are going
to multiply every belief variable with
the corresponding score and we maximize
this task obviously we don't quite get
what we'd like yes the value of that
cost function would be unbounded we
would simply select I believe to be plus
infinity or minus infinity depending on
whether the score is positive the score
Thea is positive or negative so we need
to introduce some constraints what can
we do where the first thing to be done
is we could ask for the beliefs to be
either 0 or 1 if you're doing that and
then the cost function is at least
bounded we are still not quite where
we'd want to go because we could either
my table or none of the scores for a
variable so we need to somehow constrain
the fact and constrain ourselves to
selecting at least one variable / and /
factor and one very reaper local scoring
function how are we going to do that
where we're going to enforce and that by
saying that the beliefs both the local
beliefs and the factory beliefs have to
sum to 1 and considering the above
constraint we are required therefore to
pick at least one and also at most one
of this course we are almost at the
place where we want to be the one thing
that is still missing is that the factor
beliefs should be somewhat consistent or
should be consistent with the local
beliefs and the way to enforce that is
why what is known as a marginalization
constraint which I'm going to illustrate
here in its most in a general form so
and the cost function is linear the
constraints are linear and there are
some integrality constraints hence we
obtained an integer linear program which
we need to solve so here's the integer
linear program again and now let me
actually rewrite that and somewhat
simplified so instead of always writing
this in a product for the cost function
I'm going to just use the some notation
it might look a little bit messy but
it's nothing else than an inner product
actually and then I'm going to refer to
the marginalization constraint by its
name and I'm also going to say this
requirement that the local beliefs and
the factory beliefs have to sum to 1 and
I'm going to say that I want those
beliefs to actually be local probability
distributions I'm also going to include
in this local probability constraint the
fact that i want the beliefs to be
larger or equal to 0 and then if I the
next step throw away the integrality
constraint what I'm going to end up with
is what is known as an LP relaxation so
now we are ready to state our initial
problem the problem I post or the
question I asked where said how can we
distribute inference we are now in a
position to describe that
a little bit more or make it a little
bit more specific so what is the goal
what do we want to achieve so first of
all we want to optimize this LP
relaxation objective and obviously we
want to leverage the problem structure
the problem structure that is encoded
within these higher-order scoring
functions and from the initial task
description we want to distribute the
memory and the computational
requirements and obviously and
importantly we want to maintain
convergence and optimality guarantees of
existing algorithms how are we going to
do that where we are going to go for
something and that is known as do a
decomposition approach so let me try to
give you the intuition of what do I mean
by dual decomposition in this case so
suppose we are given this grid like
graph of two by four random variables or
two by four nodes where the edges are
now that the factors that the
connections between the variables and I
want to distribute that problem this
graphical model on to two machines
copper one or on the two computers
copper one and copper two so now a
computer and has to hold a belief only
if the variable is also assigned to that
computer so computer copper one doesn't
need to know anything about the various
bv also be three to be eight for example
it's just they're just not assigned to
that computer similarly a computer has
to hold a factor belief if the factor
depends on at least one variable that is
assigned to this computer so that
already tells you or shows you that some
of the M that there is some distribution
with respect to the memory going on we
don't have all the beliefs on all the
computers but obviously there is a catch
so on every computer we naturally need
to enforce marginalization constraints
and local belief constraints the 1i have
been introducing earlier but and we can
see that there are mighty factor beliefs
that a cure on multiple computers kappa
1 and kappa to independently naturally
we need to enforce we need to make sure
that those beliefs are
consistent I'm actually going to or I'm
showing that via those two parallel
lines here so um the beliefs that'll
cure on computers independently are
illustrated by those parallel lines and
we need to somehow make sure that upon
convergence the value on both computers
is actually equal so with this
additional consistency constraint we are
ready to state our program we want to
solve again it's a linear program which
we can write in a way that it's um
parallel in terms of the computers kappa
just by rearranging the linear cost
function and also we're going to have
local probability constraints on every
computer we're going to have on every
computer marginalization constraints and
we're going to have consistency
constraints on every computer so now you
might say I'm wait a minute this looks
like everything is d covered now that
cannot be the case and obviously a right
and we cannot completely decoupled this
problem it was originally copied and the
coupling occurs only in those
consistency constraints those were the
constraints and which tie together the
individual problems the nice thing now
is that this program allows us to UM to
derive an algorithm and I want to show
you the intuition of what does the I
room look like in fact the island
consists of two parts the first part of
the island is standard message passing
independently on all the computers kappa
and therefore possibly in parallel so
you can do message passing in parallel
on all the computers your help but to
make sure that upon convergence the
variables are consistent you need to
exchange information occasionally before
then going back doing some more message
passing and iterating between those two
steps so you iterate back and forth
between message passing and exchanging
information and again some message
passing in fact you can see and that the
exchange of information is nothing else
then another type of message passing on
on a separate graph actually
and so now that poses a few question
first of all can we really do
large-scale with that approach second
and how often do we have to exchange
information and third how does it
compare to other state-of-the-art albums
so back then when we did this work that
was in 2010 and there was a library for
discreet approximate inference which we
could compare against and there was an
early version of the GraphLab framework
and it was an early version and it
wasn't yet distributed so it only
operated and shared memory environments
that's why we couldn't really compare
our distributed algorithm and to
GraphLab at that point in time and the
numbers are from yeah 2010 or 2011 so if
you look at the runtime and the library
for discreet approximate inference I
divide that by four because I had a fork
or machine and therefore that the
library for discreet approximate
inference didn't really consider all
those four course in terms of comparison
to graph lab we can see that our convex
belief propagation approach the general
version of it actually performs and in
the same order of magnitude so it's
equally efficient and efficient see is
measured in how many nodes does it
process per microsecond yes this
experiment are you do this experiment
you assume that all the theta variables
are pre computed and store so this
every use it to speak'st across all of
them yes evaluating the function they're
like order one each theta B and theta
alpha is one who come yeah exactly so to
repeat the question and the question was
whether am the cost function was pre
given that is whether all the look-up
tables the theta visa and the theta
alphas were em were given and pre
computed and yes that's the case when
doing this comparison I don't include
times for computing those and those
function values that's correct and so
this upper part of the table therefore
is like a comparison like kind of a fair
comparison where you can then see that
the primal energies and are in about the
same order of magnitude so now if you go
about and derive or code a dedicated
errand for the task we were looking at
dedicated by dedicated I mean and an
evident that worked on pairwise mark of
random fields only and so if i take this
dedicated algorithm then i can see that
i can get some further improvement
naturally because i don't need to be
that generate if you now take this
algorithm and further distributed onto
multiple machines then i can get any an
even higher m speed up the one thing to
notice is that the primary energy is not
quite an ad the one that was not
distributed so why is that and to see
that we look at what how often do we
have to exchange information between the
different machines so we're maximizing
the primary function and maximizing the
primary means we have an equivalent
problem that minimizes the dual function
and when we measure the doer and with
respect to the number of iterations we
can obviously see that exchanging in
from information every single iteration
which is illustrated by the yellow line
and performs better than when
illustrator and exchanging information
every 5 10 20 50 or 100 iterations but
the story is different if if we actually
plot this same curves with respect to
the time naturally because when
measuring with respect to the number of
iterations we didn't really include the
time into the mess
mint and the time for the distributed
item comes also from the fact that we
need to exchange that we need to
transmit information between computers
so in our experiments you could see that
exchanging information only every 5 10
or 20 iterations was actually better
than exchanging information every single
iteration naturally that depends on the
connection between the computers which
was in our case a standard local area
network connection and it also depends
on the problem we were looking at and
the problem we were looking at was a
standard muck of random field type of
problem like a grid graph and for this
example the question was how many
machines were we using and for this
example we're using 9 machines so the
problem we were looking at was a
disparity map estimation problems you're
given a couple of air you're given two
images in this case and we wanted to
compute the disparity map the images
were larger than 10 megapixel and we use
250 discrete States per pixel so the
graph was about 10 million notes 24
million edges and due to the 250
discrete States we use the disparity map
was actually kind of smooth very smooth
actually and we could capture small and
deviations and with that i'm also going
to conclude the first part of my talk
where we looked at how can we distribute
m the score maximization task where the
scores were those theta functions those
local theta reason those higher order
theta alphas so where did those Thetas
actually come from well in this case and
someone gave them to us or we computed
them from an image and but often times
and we have some data which we want to
leverage in order to get to those Thetas
so within this second part I'm now going
to show you what you could do in order
to get those scoring functions and I'm
going to assume that we have a training
set of data pairs where X is an image
and s is for example a segmentation so
the inference task from before was this
maximization of
a scoring function consisting of two and
terms theta local terms and higher-order
terms instead of now maximizing this
scoring function we can equivalently
rewrite it as a maximization over some
inner product between a weight vector we
are interested in and a feature vector
Phi and now what we are interested in
when learning is this weight vector W so
how do we choose the weight vector W
well we are given some training set
right so we should leverage this
training set somehow and what we can do
is will be or what we can ask for it and
we can ask for a score w transpose x phi
that should actually score that should
be smaller for any possible segmentation
we can find then the score for the
ground truth segmentation that is given
to us or put differently when maximizing
the scores we should always have a lower
score then the ground truth
configuration that is given to us lower
or the same in case the ground truth
segmentation is within the space of the
segmentations we are maximizing over or
put the other way around we want to
penalize within a cost function whenever
we find a maximizing score that scores
larger than the ground truth
configuration and then exactly use to
what is known as the hinge loss when we
also include some margin so we want to
linearly penalize whenever the maximum
is within a margin el of the data score
or in equations we maximize over the
space of segments patients some inner
product between our weight vector and
the feature vector plus a margin and
whenever this score is larger than the
ground truth configuration we want to
linearly penalize and that eh to the
cost function that is known as maximal
from the max margin mark of networks or
the structured support vector machines
where we minimize with respect to W some
regularization term in this case I used
just standard a to regularization and
then I some the difference of those the
maximization minus the ground truth
and score for all the training for all
the pairs in the training set so for all
the excess but we are not quite where we
want to be yet why because that would
require us to annotate every single
image in our training set and obviously
you know that annotating every single
image is very time-consuming it's costly
and sometimes might not even be possible
so we what we ideally want to do is we
want to work on training sets or on data
sets that are not fully labeled and I'm
going to RI illustrate that via the
black pixels in this image for example
where black pixel means we don't know
what labour this pixel should be taken
so the data we are looking at the
complete data we are looking at consists
of two parts the annotated part which I
denote by Y and the latent or hidden
part which I denote by H so again and
the complete data consists of those two
parts the annotated part Y and the
hidden part H and now what do we want to
achieve what one what do we want to
optimize and therefore we are looking at
the weekly labored hingeless we want to
penalize whenever the best overall
prediction so the best prediction over
the joint space over the complete data
space exceeds the best prediction with
the annotations being clamped in
equations that means whenever the met
the score when maximizing over the joint
space y + h which is the complete data
space whenever that score is larger then
when we just maximize over the latent
space and clamp the annotation to be
whatever the user gave to us so whenever
that and that the difference between the
two is actually um larger than zero we
want to penalize and that leads to what
is known as the latent structured
support vector machine framework and
where we are having cost function which
again has an eye to regularization term
plus and now we sum again over the
entire data set will give
and we max we have the difference
between those two maximization scores so
here's the cost function again I didn't
change anything I just rewrote it
slightly and what I want to do now is I
want to generalize it a little bit the
first thing I'm going to do is I want to
introduce the softmax function the
softmax function is a one-parameter
extension of the max function and the
parameter is epsilon whenever epsilon
approaches zero the softmax function
smoothly approximates the max function
and whenever epsilon equals zero the
softmax function is equal to the max
function so we have a difference of max
functions now I can just plug in the
softmax function and we have one
additional parameter epsilon right sure
oh what a friend once and this will in
this in turn up all in this structure
prediction
but this is not this is not like saying
that your output space is structured
right so what is the I'm trying to
understand in what sense is the
structure prediction latent variables
okay let me try to rephrase the question
so the question was am in what sense in
what way is this structure prediction
whether the output space I could
consists of a certain structure or
something it's not like you showed an
example of a place where you want to do
structure prediction so I understood
what you meant there by saying that you
want to constrain the problem by using
them the known structure but in the
setting here where you are learning we
try to do the segmentation and able to
leave valuables are today explain how
this is a structure prediction problem
okay I see so the question is rather in
the beginning of my talk I was saying
and we want to UM to infer or want to do
estimation in structured spaces so what
means structured spaces with respect to
the structure I'm looking here because
like a segmentation problem is not per
se like a structured problem i was
talking about the very beginning let me
actually come back to that question the
at the end of the second part of my talk
in the beginning of the third part where
i'm going to explain more rigidly what i
mean by am the best structure and i was
talking about so in this setting i'm
just talking about like pure
segmentation for now but i will get to
that question good question so now we
can't just replace those max functions
and with these soft next functions and
this is what we would get why would that
make sense why is there a reason of a
thing to do well and the nice thing now
is that we're having two seemingly
different frameworks within one cost
function namely the latent structured
support vector machines and the hidden
conditional random feeds so for
absolutely equal to zero that's how
did the derivation and that's how I went
through my talk perhaps don't equate to
0 we used the hinge loss or the weekly
labored hinge loss and similarly we
could say we try to go for a max margin
framework and what we got was the latent
structured support vector machine
formulation now though would use instead
the log loss and go through the entire
derivation using the log loss and go for
a maximum likelihood approach I will get
the hidden conditional random field
which would be equivalent to setting
epsilon equal to 1 and in addition to
having those two seemingly different and
frameworks within one cost function we
actually have a whole set of additional
and cost functions for different epsilon
in between and beyond one and obviously
what we could also do is we could again
include the margin and which I left out
for clarity but when including that you
get now I mean the cost function looks a
little bit more complicated but nothing
fancy happens so now we want to optimize
this and this cost function what are the
challenges so there are two challenges
two problems we need to solve first of
all and we are having these summations
your the summation fear over
exponentially size sets so for like a
segmentation problem we need to sum over
all possible segmentations with just
exponentially size summations and in
addition we have the difference between
two softmax functions the difference
between two max functions and that means
we're dealing with a non convex problem
so what can we do well let's look into
what people did in literature so to
address those difficulties let's look at
how to address exponentially sized some
or max operations where we could assume
them to be efficiently and exactly
computable and there will be the case if
you're looking at foreground background
segmentation for example and would have
submodular energies then we could use
graph cuts and it would the maximum
operation would be exactly computable or
if you want to solve matching problems
or if it's a tree-structured problem and
but often times in particular oftentimes
when talking about physically plausible
spaces we don't have those structured
problems so we need to go for other
approximations and other approximations
could be local type of approximations
entropy type of approaches in fact this
is exactly what we're looking into we're
doing local approximations the
difference with respect to previous work
is that we're using convex duality based
entropy approximations and I'm going to
explain in a minute why this is
beneficial for the non convexity that is
for the non convexity the difference
between the max operations we are going
to go for a standard approach like
expectation maximization or the concave
convex procedure type of approaches how
do the aliens look like or how do the
errands with convex duality based
entropy approximations compared to
existing albums so existing items for
hidden conditional random fields and
layton structured support vector
machines here illustrated on the left
side and they are contained or are kind
of a double loop type of structure you
have an outer loop and within the outer
loop you first need to solve the latent
variable prediction problem that is for
everything that is not annotated you
need to figure out what is either the
maximizing state or what is the
probability distribution over then the
given very much and then once you solve
this latent variable prediction problem
which is an inference problem that you
need to solve until convergence you can
update your parameter vector and the one
thing to make sure is that you need to
update your parameter to convergence
before you go back and resolve the
latent variable prediction problem and
you iterate between those two problems
basically if you do convex entropy
approximations we can get rid of this
second requirement to converge so
instead our we also have to solve the
latent variable prediction problem as
before but now it's sufficient to just
do a singer step in the parameter vector
direction before then going back and
solving the latent variable prediction
problem I'm saying it's sufficient
because obviously you could also do more
than a singer
parameter update and obviously that's
this is the question we have the
advantage that we have less inner loops
we got rid of one convergence
requirement but um how many updates
should we do or how often do we should
we update before again solving the more
complex laytonville prediction problem
that's that's a question that we didn't
answer and importantly using those
approximations those convex entropy
approximations we got rid of the
assumption to solve those subproblems
like matching problems or tree
structured problems exactly so we don't
we don't have this requirement in the
heirloom anymore which contrasts for
example latent structured support vector
machine so is there a benefit or can we
show that there is a benefit for using
those entropy approximations and I guess
I wouldn't be talking about that if you
couldn't and the benefit first of all is
since we have less inner loops the am
training time the average training time
is lower compared to other algorithms to
r2 standard I loops and importantly we
could also show that in terms of
performance it pays off what and what we
did was we increase the amount of latent
variables and like two ninety percent in
this case and we try to train a simple
segmentation problem here and if we
compare our approach to the latent
structured support vector machine we see
that late instruction support vectors we
actually learned the wrong model in this
case and we wanted to learn that
neighboring pixels should have an
equivalent labeling and and but Layton
structured support vector machine
learned that neighboring pixel have the
opposite name so why did that happen
what was the reason for that to happen
well the reason was that the due to the
ties during the latent variable
prediction problem the empirical means
vector pointed in the wrong direction or
had the different signs that were just
opposite to what they should have and
therefore late instruction support
vector machine learn the wrong model
that doesn't happen in in the approach
we used and/or we propose to use and it
also didn't happen the hidden
conditional random feeds framework it
didn't happen in the hidden condition
random feeds framework because we use
the convex version of it that way and
there we achieved equal performance at
the trade-off of having a higher or
longer training time so naturally that
is kind of a setup example and what we
want to show is that these
approximations are actually also useful
in in the real world and and one way to
show that they're useful in the real
world is that if we can extract
information from weekly label data so
what I want to show next is I want to
look at an example where we are given a
fixed set of fully annotated sample and
if we throw in additional weekly labeled
samples then we can improve the
performance so the task i'm going to
look at in this example is the initial
layout prediction task so um the task
was given a single image we want to
predict the 3d parametric box that best
describes the observed room layout so if
you look into this room here for example
you'll notice that I'm box a cube like
structure is a very good approximation
for this room for example and that holds
for many other rooms and that we're
living in so how can we find this and
the parametric box and that also hinges
a little bit down to the question of
sudipta where we're asking what is now
the these physically plausible
configurations and the problem there is
that we need to find an adequate
parameterization on how can we actually
parameterize the problem such that we
don't reason about labelings general
labelings in a structured manner but how
do we reason I'm about configurations or
just the configurations we are
interested in so how can we constrain
that problem and the question is to
design a parameterization that it makes
sure that this is the case and that is
often the key problem that's the key
thing to look into and what
parameterization did we use so in our
case we assume the vanishing point
to be given when I'm saying we assume
them to be given I mean we ran a
vanishing point detector we didn't hand
labor them we ran a vanishing point
detector and assume those to be the true
vanishing points obviously they were
they were also errors in the vanishing
points but we assume them still to be
correct because we wanted to assess what
is the real work performance now
assuming the vanishing points to be
given that prediction of a box is
equivalent to predicting four variables
as sub 1 through S sub for those four
variables essentially and correspond to
predicting the angles or the deviations
and between the connections of two
vanishing points and the Ray we are
interested in so ray 1 through ray for
in this example so given those four race
we know what the 3d parametric box looks
like so now the next question to answer
is once we have a parametrization that
encodes these physically plausible
configurations what are the features
what are the measurements we are
interested in and the features we are
looking into our have been designed by
other people and those are orientation
maps and geometric context and
orientation maps are shown in this
illustration here and what our features
are going to do is and we're going I'm
going to do that with the hypothesized
left wall which is shown by those brown
raised as an example so the left wall
will be something like this thing and so
our feet are going to count what are our
image cues what our orientation map and
geometric context and giving us as
estimates for for for example I left
wall for a front wall so we are counting
effectively colors how much yellow is in
hypothesized left one how much green is
in a hypothesized left wall how much red
is in the hypothesized left board and so
on so the model we are going to get with
those variables is then a loopy
graphical model in this case luckily
were either higher order in our case
even a pairwise graphical model or we
could decompose it such that it was a
pairwise graphic
that's so now we have this pairwise
graphical model loopy which is neither
submodular nor does it possess the tree
like structure so that's why I talked in
the second part about like learning in
those general settings and and for
simplicity I used the segmentation
problem although I could have just as
well used this is an example here so now
and what I wanted to look at is how does
this how can we extract information from
weekly labor data and in order to show
that I'm going to compare to a fixed
approach a fixed approach that trains
only on fully annotated samples on five
fully annotated sample 10 20 and 50
fully annotated samples and I'm going to
measure the performance the prediction
error in terms of pixel wise
classification error so obviously we can
see with the fixed approach in blue that
the more samples we throw in to the
training procedure the better the
performance in this case because we only
have few number of fully annotated
samples so now if in addition I throw in
a twenty-five fifty or a hundred weekly
labored Santa's with the number of Angus
being thrown away being either two or
three on the right hand side so fifty
percent latent variables or seventy five
percent latent variables we can see that
the prediction error goes down so
despite the fact that we did some
approximations we're capable of reducing
the prediction error so we were able to
extract some additional information
green one
50 annotated examples and you have an
additional twenty five examples which
are weekly labeled it'll to get certain
words that it because it's doing is
really think is actually something to
that okay so the question was why is
like for the green and curve where we
throw in an additional twenty five
samples in case of the fifty why is it
slightly worse than when just training
with the fixed number of samples and and
all those curves are averaged over ten
runs i believe and because it within the
data set we obviously had to pick a
subset of them and a i would say it's
it's mostly noise that happens I mean
this is then the error shown on the test
set so you can train on the training set
yes but what then happens on the test
set is kind of a different story right
and so maybe if I would ever reach over
and 20 runs it might be better the
important thing is that the differences
are are actually minor so well with that
curves I then also want to conclude the
second part of my talk where I am wanted
to show that certain approximations can
help to train em also if information is
not fully provided and in the third part
I want to give you some more details on
this estimation in physically plausible
configurations meaning and others or
other specific inference items or are we
capital or how are we capable of
designing specific inference items for
certain problems so again the problem is
the 3d endorsing understanding task I've
been looking at and what we want to do
is now given a linear model w so in this
part i'm going to assume someone did the
training for me and i'm given this
linear model w given this linear model w
and some image x how can we estimate the
layout using a feature map five where
this feature map was again this counting
em of how much yellow how much green and
so on what was
within the image cues the parameter
session we're going to use again
consists of those work for variables
because this is a fairly structured
configuration that allows us to do
exactly what we wanted to do and further
on the layout is fully specified if we
know the four variables and if we know
the vanishing points obviously what we
are going to assume is that the
Manhattan world assumption holds meaning
that the walls are parallel the floor
and the ceiling are parallel and so on
and what we are going to do is instead
of working with or reasoning with singly
hypothesized layout we're going to work
in interval product spaces so we are
going to assume we're given a set of
layouts I'm going to specify the set of
layouts via the letter s and the set of
layouts is denoted by a minimum angles
and maximum angles so for every of the
four random variables we're going to
have a minimum angle and a maximum angle
and the minimum angle is illustrated in
this graph on the right hand side down
here via the black race and the maximum
angle is illustrated by the blue air by
the red race so we're considering all
the possible layouts and for every angle
being within one of those intimacy so
what could be a possible inference
procedure well we could go for a
branch-and-bound approach how would that
work well suppose we are taking a set of
layouts we're going to compute them and
upper bound for the score of the set of
layouts and we have put it we're going
to put that into a queue in addition to
the set then we're going to take from
this cue the highest scoring em set
we're going to split it into two parts
we're going to score the two parts
independently put them back into the
queue and keep going until the highest
scoring element we retrieve consists
only of a single hypothesized layout and
then the eigen terminates because there
is no way of splitting a single lay out
any further when would that work and
we're guaranteed to obtain the global
Optima optimum of of our scoring
function if
two properties hold for this scoring
function first of all the scoring
function has this upper bounding scoring
function f bar has to be a true upper
bound for the cost function so for any
possible layout within any possible set
if we scored this set we have a true
upper bound to this cost function and in
addition and what has to hold this
exactness for a single hypothesis so
whenever we throw a single hypothesis
into the bounding function we get the
exact score given those two properties
the above branch and bound approach
gives us the exact and the global
optimum so then the next question is can
we actually find such a bounding
function such an f bar being an upper
true upper bound and what do we need to
do in order to find one where we have
this scoring function being this inner
product right this inner product between
our weight vector W and our feature
vector Phi we can divide this in a
product into two parts a positive
scoring part and a negative scoring part
and the positive scoring part f plus
consists of the weight vector or the
weight vector element multiplied by its
corresponding feature vector if the
weight vector element is positive and
the negative scoring function f minus
consists of and the weight vector
multiplied by a feature vector if the
weight vector element is smaller or
equal to zero why are do we have to and
a positive part in the negative part
that way the reason that works is that
we have that our feature vector is just
counting counting colors and counting is
always positive accounting is positive
the sign is uniquely determined by the
design of the weight vector so now
having split up this function into two
parts how will do we design and this
bounding function f bar well what we
need to do is we need to sum the maximum
positive contribution of the consider
interval
and subtract the minimal negative part
and that that's up for me that is a
pretty important statement so i iterate
over that again so we want to sum the
maximum positive contribution of some
interval and we want to subtract the
minimum negative part so now consider a
front wall consider this like this front
wall here where we are given maximum
Angus by red race and minimal angles by
BlackBerry's and now we want to sum the
maximum positive contribution where the
sum of the maximum contribution is just
summing up the quadrilateral everything
that is positive within the red
quadrilateral right we can't get more
positive than everything within this red
quadrilateral here what is the minimum
negative part well the mini- part is
everything negative within the black
quadrilateral we can't get places yet
this is the minimum negative part that
we we always need to subtract for all
the other cases we need to subtract more
so this bounding function fulfills
exactly the two properties from before
its exact for a single hypothesis if you
have a single hypothesis then the red
quadrilateral and the black
quadrilateral are actually equal so
we're going to subtract exactly this say
the positive part and the negative part
are exactly considering the same areas
so the cost function m is exactly
recovered and also it's a true upper
bound because we're something we're
taking always more positive and less
negative or the least amount of negative
and it works for the front wall case and
similarly it works for all the other
boys and I'm not going to go through it
there is an illustration for the left
wall there as well but I skipped that
part so now and we designed this
bounding function and we need to
efficiently compute that we built on
prior work and for that and I'm going to
skip that part and I'm going to show you
next yeah a little bit about the
performance so I said
ready what we use as image cues as
features our orientation maps and
geometric context and what we are going
to measure I'm going to show some images
about of orientation maps and geometric
context later and what we are measuring
is the pixel wise prediction error on
the images on some specific data set
which is the layout dataset there was
obviously some previous work on that
task and people usually use different
image cues like orientation maps
geometric context or both of them and we
obtain quite decent performance in f
actually a fraction of a second so the
branch and boundary is quite fast in
this case and it achieves a reasonable
performance so how does that those
results look visually we're here are
some some examples so I show some images
and in the middle column of i'm going to
show the orientation maps on the right
hand side you can see the geometric
context which are some feature cues and
the feature cues on its own at least for
geometric context you can see there the
performance would be twenty eight point
nine percent so on the images you can
see overlaid our prediction result in
red and in blue we are going to show
what would be the best possible
prediction if we would have the ground
truth available so I'm showing the blue
because that illustrates a little bit
the discretization artifacts we're
discretizing everything and so there is
not really much to see much errors to
see with respect to descriptors ation is
it your result the rivers the right
we'll rightmost is like this oh no this
is this is an input image like this is
geometric context which we're using as
an input feature yeah so the colors here
exactly that the red is the country and
yeah you could see that ballistic
so I hope this just setting I don't know
if it's to this particular data set or
if there are more like this but in these
datasets there are objects in the scene
yes and in some sense the objects are
like distractors rival the drum true
that I understand does not mask all
these except they're all the pixels are
are just included in the three walls
right excellent yes so then when people
do learning on these on this data second
round trip they will do the sex how how
is the learning not getting affected by
all these objects destructors in some
cases they are everywhere lined up with
the orientation of the box of the room
but in many cases they are not and do
you have a sense of okay what happens
okay so to repeat the question what
happened the question was in many scenes
there are objects there and objects kind
of distract also the learning algorithm
and so how how does the learning not not
get affected and the question is the
learning does get affected in fact it
does get distracted and also inference
does get distracted as you can see in
the to failure cases on the bottom here
where we're actually predicting the red
layout which is kind of obviously
distracted by a by the objects in that
case so and there is at least if you
don't model the object then um it it
will distract you correct and another
source of failures is wrong vanishing
points so for this image on the right
hand side you could see that the
vanishing point should be somewhere in
the middle of the image but it's
actually far to the right outside the
image cell and since we assume those to
be given and fixed we cannot really do
much about that and yes it's um I'm
actually also i can show you also some
video here for some of the results so
again what we assume is given a single
image like this one here what we want to
do is we want to predict the
layout but if you then just have the
layout as illustrated here on the right
hand side and you want to render scenes
from a new few point what happens is
that you're going to obtain recites or
renderings that are actually more
destructive so the girl or an
interesting additional thing to be done
could be too kind of also add model and
not only the object itself but also the
layer add the not only the layout itself
but also their the object on top of it I
haven't talked much about that and but
yeah it's kind of not not really
published work yet but again on this
example you could see that given us you
can see that given a single image you
can see that you get significant
artifacts if you and just render the
scene to a single box and then on this
last image and what you can also see is
that obviously for some of the pixels we
don't have any texture information so
what happens if we're predicting objects
on top is that we do get some black and
regions for which we don't really know
what's happening obviously we could do
something painting there but we haven't
yet looked into that task so with that
video i also want to conclude my tart
talk and basic quickly recap so what we
what the mahalo motivation was to work
in am physically plausible configuration
spaces what we need to do is because
those can get fairly largest we need to
do inference somewhat efficiently one
way of doing it efficiently is to
distribute the task on to as many
computers if you have publicly available
in the second part I then tell you that
those models generally I possess a very
general structure so we we are required
to learn in general graphical models
also if you have don't have everything
annotate
because annotation in large environments
is time-consuming and costly and the
third part I then showed you some amp
research regarding a 3d indoor seen
understanding task so thanks for your
intention and I'm looking forward to the
questions any more questions sure thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>