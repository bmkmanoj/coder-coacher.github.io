<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Turnstile Streaming Algorithms Might as Well Be Linear Sketches | Coder Coacher - Coaching Coders</title><meta content="Turnstile Streaming Algorithms Might as Well Be Linear Sketches - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Turnstile Streaming Algorithms Might as Well Be Linear Sketches</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pezrZR52C84" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay welcome everybody it's a great
pleasure to have David Woodruff from IBM
Research here David works on streaming
algorithms sketching algorithms low
dimension embeddings and related to
ethics he did some great work in this
area and got several best paper award at
top CS conferences he was also recently
awarded very prestigious Pressburger
word and today david he'll tell us about
some of his works as involves feminine
Griffin's and and sketching okay so
thanks for inviting me to give this talk
the title of the talk turnstile
streaming algorithms might as well be
linear sketches so the goal of this talk
is to try to characterize all optimal
algorithms on a certain natural for
certain natural streaming model this is
joint work with ely and hui new in so
the turnstile swimming model which is a
well-studied three model is the
following so you're given an underlying
n-dimensional vector X which is
initialized to all zeros and what
happens is you see a long stream of
additive updates to the coordinates of X
in the form that X goes to X plus e I or
X goes to X minus C I for standard unit
vectors AI so you can think of this as
items coming and going in a stream where
the ice coordinate of X corresponds to
the number of times item I is there at
the end of the stream your promise that
this underlying vector X is inside of
some bounded integer box so all the
coordinates of X are between a minus m
and M for some parameter M which for the
sake of this talk let's think of em is
being polynomial and n what you would
like to do is to a computer function or
an approximation to a function f
with high probability so usually for
efficiency reasons you want to you need
a randomized approximation algorithm and
the goal is to use this little space in
bits as possible to do this so the
memory required of the algorithm is one
of the main goals so of course you could
just store x but you want to do with
much better space complexity ok so this
is the turnstile sweeping model what are
some example functions f that people
have tried to approximate in this model
ok so one example one well-studied
problem is the RDL p norms so typically
what you wanted these norms is to
achieve relative error right you want Z
to be within 1 plus or minus epsilon of
the pin or more equipment Lee for
approximation the piece power of the P
norm of X and this is a well study
problem with many applications of course
the Euclidean norm has applications to
geometric and linear algebraic streaming
algorithms the one norm has applications
to measuring distance between
distributions pretended in the stream
which is useful for things like network
monitoring etc ok so these are some
problems now let me give you a flavor of
algorithms in the streaming model and
you don't have to really understand why
the algorithms work so well I just want
to give you sort of a structurally what
do these algorithms look like and I want
to try to abstract some generic features
from these algorithms and prove a
characterization so what is an algorithm
for the Euclidean norm in a stream ok so
the algorithm is quite simple so let the
parameter R be 1 over epsilon squared
what you do is you choose a random r by
n matrix a of I ID sign random variables
so each is plus or minus 1 independently
with probability 1 half this is a very
wide fat matrix and what you do is you
maintain a times X in the stream
which is easy to maintain under additive
updates to the coordinates of X because
I just add the corresponding column to
the sketch and note that ax you know the
dimension of this vector is only r which
is one over epsilon squared so we're
doing quite well and after processing
the stream all you do is you output the
euclidean norm of a times X rescaled by
square root R and the proof that this
works is basically this I mean that
these bounds work this is I mean it's an
immediate consequence of the Johnson
lindenstraße alema and yeah so one
thing is regarding this sign matrix you
can actually generate the entries from a
four wise independent family so that can
be stored succinctly so this is a simple
algorithm and I just want you to see the
sort of the structure of this algorithm
because for the one norm there's a very
similar algorithm due to in dick so
again set the parameter are to be 1 over
epsilon squared you're going to do a
very similar thing this time you choose
an R by n matrix a of iid koshi random
variables so these come from the cosi
distribution it's a continuous
distribution doesn't matter so much what
it is I'll just described the property
we need of it but all you do is again
you maintain a times X in the stream
which is much lower dimensional and at
the end of the stream this time the
estimator is a little bit different you
up with the median of the absolute
values of the coordinates of a times X
and you can show that this that this
works so and basically the proof it just
uses the only property of the Cauchy
distribution that it uses is the one
stability property so basically if c1
and c2 are independent cosi random
variables arm then if I look at a times
C 1 plus B times C to wear a and B of
scalars this is equal in distribution to
another cosi random variable C 3 times
absolute value of a plus absolute value
of B
so so basically each of these a excise
is just the one norm of x times the
coast vendome variable and you can show
that this estimator works again it's low
space you can show that these can be
drawn from family random variables which
has limited independence so that so a
can be stored succinctly so both of
these algorithms have a very similar
form and the goal of this work is to try
to abstract that form so what are some
generic features of these algorithms for
norms so what you do is before the
stream begins you choose a random matrix
a independent of X then you maintain a
times X in the stream which we said was
easy to do and at the end of the stream
you up with some function of a times X
maybe the Euclidean norm or the median
in those two examples and a natural
question that one sort of can't help to
ask is does the optimal algorithm for
approximating any function in the
turnstile model have this form so not
necessarily a norm just any function of
the underlying vector X that's being
updated and I should note that actually
for all problems studied in this model
all known algorithms do have this form
can be cast into this framework yeah
well i mean i can always maintain the
store x and then i can compute any
function so it depends like how much
space you ask for so different f require
different amounts of amounts of space i
should note that you know some functions
may be quite weird and it's not clear
that intuitively the best approximation
is to maintain a linear sketch for
instance these pointer chasing type
functions where i read the first
coordinate of x which has some value
which indexes into another chooses
another coordinate an eye out put that
value of that other coordinate why
should the best algorithm for that bale
in your sketch ok so what we show is
that actually the answer is yes up to a
factory of log n the optimal space
algorithm does have the the above form
and let me qualify that by formally
saving the theorem because there are
some some caveats so the theorem is is
following is a false so for computing a
relation f4 underlying vector X which is
promised at the end of the stream to be
inside of this bounded integer box then
there is a correct with high probability
algorithm which does the following so
before the stream begins it samples an
integer matrix a uniformly from a small
number of matrices and log M which are
hard-wired so there's some amount of non
uniformity here in general we don't know
how to represent these matrices in small
small space but the matrices they have
poly n bounded integer entries and then
what we do is we maintain a times X in
the stream and then we output a function
of a times X and you know what we show
is that the logarithm of the number of
states of AX so what a state is is as I
range over X inside of this integer box
I get potentially different images a
times X if i look at the logarithm of
the number of distinct images i get
together with the amount of space you
need you to store the randomness
which is actually just log in bits
because I'm the randomness is only used
to choose one of these hardwired
matrices uniformly at random then this
is optimal up to a log n factor in this
space okay so before I get into some of
the details of this theorem in
particular one thing that we need to go
over is the non-uniformity let me just
state an application where the
non-uniformity doesn't matter ok so the
application one consequence of this
theorem is to proving lower bounds for
data stream algorithms so the standard
way of proving lower bounds is via
communication complexity and most often
people look at two player communication
complexity so what you have are these
two players Alice and Bob with inputs a
and B say n bit strings to some
communication problem what they do is
from their inputs they locally create
streams s of a in SF b in some arbitrary
way and the lower bound technique is the
following basically that there's some
streaming algorithm out Alice runs it on
her stream sfa and transmits the state
of that algorithm to Bob then Bob
continues the execution of the algorithm
on his generated stream sob and at the
end what you'll have is that the
algorithm will have been run on the
concatenation of these two streams and
if Bob can solve some some problem G of
a be these inputs to the communication
game then the space complexity of the
algorithm the streaming algorithm must
be at least the one-way communication
complexity of solving this problem g
right now i'm not going to formally
define communication complexity but
basically one-way communication
complexity is sort of the so it it's one
way Alice sends a single message to Bob
and what we look at is sort of the best
protocol with the the minimum message
length maximized
all inputs and so in this case what
happens is you know the message that
alice is sending is is the state of the
algorithm and this is why we get this
lower bound and so one way communication
complexity it's often easier to prove
often but not always easier to prove
communication lower bounds than for
general communication complexity when
both players can interact so what our
main theorem implies here is that
actually we can look at an even more
restrictive communication model instead
of one-way communication complexity we
can do the following so again they do
the same thing of creating these local
streams sfa and s of B and if the
players can solve this problem g of a b
then the space complexity of the
algorithm must be at least the
simultaneous communication complexity of
g and let me describe what that is
that's an even a strict more restrictive
communication model and so it allows for
proving lower bounds in a new way it
gives an an avenue for easier proofs and
so what is the simultaneous
communication complexity of G so in that
model you have these two players and
instead of Alice sending a message to
Bob what happens is they share some
public coin and what they do is they
locally create a message based on the
public coin and their inputs and they
send this message to the referee so
there's a third player called the
referee who sees these two messages and
then he decides the output based on
those messages so it's more restrictive
because you know of course in the
one-way model Bob could simulate the
behavior of the referee but our main
theorem implies that it suffice is to
look at this and it's not obvious you
know how to reduce a streaming algorithm
it's not obvious why this lower bounds
in this model give lower bounds for
streaming algorithms but what our main
theorem says is that the optimal
streaming algorithm up to a log n factor
is a linear sketch so basically the way
this proof works is that the players use
the public coin there's some shared
randomness
to choose a common sketching matrix so
as we said there are you know our main
term implies there a small number of
sketching matrices order n log M so if
you sample one uniform a random that it
works for your input with with high
probability so they choose the
scattering matrix on the public coin
Alice applies that sketching matrix to
her stream Bob applies that scheduling
matrix to his stream and they send these
sketches to the referee and because of
the linearity the referee can add the
two sketches so a times s of a plus a
times s FB and therefore well that's how
you do the simulation in this case using
our theorem and so this has some
consequences were able to prove some new
lower bounds using this framework for
for estimating p norms I should know if
you're familiar with things like
multiplayer set disjoint is a lot of
proofs become a become easier in this
model than in the one-way communication
model and for instance there were lower
bounds for multiplayer set the stillness
which were first in this simultaneous
communication model and then later
people were able to get it for the one
moon model okay and note that the
non-uniformity doesn't really matter
here right because communication
complexity it it doesn't really I mean
we're not looking at players space or
time complexities and so so basically
they can there's a constructive
procedure to generating those matrices
small number of matrices but it's not
efficient but it doesn't matter for
approving communication lower bounds any
questions on that
okay so yeah so that's one application
so let me describe the non-uniformity
now in more detail so as I said what you
do is you sample this integer matrix a
uniformly from a small number of
hardwired matrices with poly an integer
bounded entries and not only is the
non-uniformity there so there's
non-uniform that we don't know how to
store these matrices a and low space but
actually we don't even know how to store
the output function of the of the
streaming algorithm in low space so this
needs to be hardwired as well so what
that means is that for each a times X
what I do is I am I non-uniformity I
hardwired the the output that I should
produce if I have a times X so note that
this doesn't cause any sort of indexing
problems into the non-uniformity because
if I'm storing a times X already then
that sort of indexes into what my answer
should be in the non-uniformity also I
you know I'm storing the identity of a
with these log in random bits so I can
index into this Mon uniformity if you
don't like non uniformity you can
alternatively allow the algorithm to use
more space to process the stream update
provided it only retains a times X and
its randomness after processing each
update so basically what I'm saying is
that there is a constructive procedure
for which what you can do is during each
stream update you can run this procedure
and you can regenerate this matrix a you
can generate these n log m possible
matrices you can use your randomness to
choose the one that you've been using
and you know if you're updating one to
the i'th coordinate you can add the I to
coordinate of a to your sketch a times X
and then you can throw away all that
additional space that you use to
generate these matrices a so basically
in between stream updates all you do is
maintain a times X and the randomness
but when processing a stream update you
you can use a lot more space so this
could have applications in sort of
multiplayer communication where
or the communication is what matters and
locally if people have updates they can
use more space yeah thanks only in these
hot while three phases um that's also
stay a part of the thing you have to
know so the algorithm is non-uniform so
it has these hardwired matrices and
there are n n of them right so that that
would already be n bits of space so so
the non-uniformity is not counted
towards the space and it's also like for
these applications like communication it
doesn't go into the communication
because as I said like you non
uniformity is not important for
communication lower bounds because the
players you know use the public coin to
choose one of these matrices they just
have to agree upon one of them and yeah
that make sense ok no so the you mean
how hard is it to generate these
matrices a yeah so it's no it's a very
it takes a lot of time to generate these
matrices a not even in polynomial time
that I love the idea of all time
actually the space yeah I mean Oh in
peace ash bowie yeah this step is it
sort of came ganger story point orlando
this output the second Oh for each state
no so because basically what you're
going to get is for every a times X you
have an output and just sort of the
number of outputs is who could be super
polynomial depending it depends on the
number of outputs is basically
exponential in your space bound so oh I
see because you're moving only the
communication though so if you do priest
or everybody both at is involved we saw
like a bunch of matrices yeah it is
little comp yeah exactly so it doesn't
play any role for proving a lower bound
for upper bounds I mean in general we
don't know if it's possible to represent
these matrices a and low space and it's
probably not because
claiming this for an arbitrary arbitrary
functions so so okay so another comment
on the model is the following so for
each random seed of the algorithm the
algorithm becomes a deterministic
automaton with a finite number of states
so I'll describe more what that means in
this context in a few slides but let me
say the following so that the main
theorem it only requires correctness
under the promise that X is inside of
this boundary integer box at the end of
the stream so you're given this promise
that you know X undergoes all these
updates to its coordinates and at the
end it's inside of some integer box so
while processing the stream it may be
the case that some of the coordinates
may be outside of this box right you
have some long stream who knows what's
happening in as you go and one
restriction is it the algorithm is not
allowed to abort if the if the
coordinates leave this box at some
intermediate point in the stream so
basically if it yeah so if if if one of
the coordinates leaves this box but
later at the end of the stream it's
inside of the box and the algorithm must
still be correct so this is sort of like
you know so there's a feature here which
is that let me just say this so what
we're going to do is we're going to take
the optimal algorithm we're going to fix
its randomness and we get an automaton
and what we're going to do is we're
going to count the number of states as X
varies over this box so it's going to be
some automaton with lots of states but
some of these states might not
correspond to any X inside of this box
you know the only X that might reach
this state in the automaton might be
from some X which is outside of the box
we're not going to charge that algorithm
space for the
States and this is a feature in the
sense that what we're going to do from
the automaton is produced a linear
sketch and the linear sketch the the
number of states of the linear sketch is
about the same as the number of states
of the optimal algorithm when measured
in this way so so basically you know
it's it now if we produce a linear
sketch what we're doing is we're
counting the number of images a times X
is x ranges over this box instead of you
know a times X for all possible X with
with large coordinates but we can always
take a times X mod each of the
coordinates poly n and if we do that
then that means like the total number of
states that that you would ever see is
actually the same as the number of
states proportional to the number of
states when X varies in this box and the
algorithm is correct at the end of the
stream because you know a xmod poly n
will equal a x given this condition so
it's just a technical well it's a
condition so ok so another comment is on
the log n factor loss so as I said like
what we do is we take an arbitrary
algorithm and we show without loss of
generality can be a linear sketch but
the linear sketch might have a login
factor more in space than the optimal
algorithm now this log in factory loss
is actually necessary if we're producing
a linear sketch over the integers one
way to see that is just to consider the
function f of x equals x 1 mod 2 so what
is the optimal algorithm for this
function well I just maintain one bit
which is you know the parody of the
first coordinate so it has two states
but if I'm trying to compute this with a
linear sketch if I look at a times e 1
the standard unit vector with the one in
the first position zeroes elsewhere then
that should be nonzero if the
is correct and then if I scale you won
by two I get a new state if I scale by
three I get a new state etc so if i
scaled n times i get n states right so
basically you know i went from two
states to end states so there is a log
in fact or loss that is that is
necessary if you want to linear sketch
over the integers that makes sense yeah
so yeah so am i'm saying is poly n so
it's a log m yeah ok all right now
before just describing how this
characterization works let me just
mention a simulated work of gangly so
what gangly did is he tried to
characterize all loft more algorithms
but for a specific problem called the
heavy hitters problem in data streams
and he also only looked at deterministic
algorithms but nevertheless some of the
notions he introduced are useful in our
characterization theorems so okay so in
the reign of the talk I'm trying to I'll
try to give an overview of how the proof
works so basically what's going to
happen is you start with the optimal
algorithm for each fixing of its
randomness you're going to get some
automaton and what we're going to try to
do is pretty up the automaton we're
going to try to make it have some
special properties which I'll call being
path independent and once we reduce the
automaton to a path independent
automaton then it's not so hard to
reduce that to a linear sketch and so
that's that's sort of how the poof of
work ok so yeah what is an automaton
first of all in this context so yeah we
have some streaming algorithm for each
fixed randomness it looks like the
following so we have
some states and we have a transition
function so we start in this state where
the underlying vector X is initialized
12 zeros and we see updates so suppose
we see the update you know add one to
the first coordinate that we might go to
this state etc so subtract one from the
first coordinate I go to this state
those are just a general generic
automaton and what i said is we'd like
to get some control over what this
automaton looks like so a property that
we want is the following we would like
that the streaming algorithm the state
that it's in it only depends on what the
current value of this underlying vector
X is all right so this vector X is
evolving we don't want to sort of to
depend how it got there so so so note
that that this particular automaton does
not have this property because if I
start in this state and I'm my vector X
is initialized to all zeros if I add one
to the first coordinate and then
subtract one from the first coordinate
the all zeros vector also goes to this
state so this is something we'd like to
to avoid okay yeah so 0 to the n is in
two different states so this motivates
the definition of a path independent
automaton which is for each underlying
vector X there's a unique state for
which in the automaton which contains X
now note that most states should contain
many vectors X if you want a small
amount of space so if you have this path
independent property you can think of
this underlying automaton is being an
undirected connected graph in the
following sense that if I'm in some
state and I add one to some coordinate
and go to some other state and i
subtract one then i should go back to
the state i started with i mean that's
what it means to be path independent and
so in some sense it's undirected you
know that
have parallel directed edges everywhere
so a natural goal is for each fixing of
the randomness of the algorithm we get
an automaton and can we modify it to
make it path independent without
reducing without increasing the number
of states and while preserving the
overall correctness of the algorithm
okay yes this would rule out algorithms
that for example we remember the last
five stream updates and try to do
something with them okay so the strategy
is as follows we're going to introduce
the notion of a frequency vector of a
stream so for a stream Sigma let
frequency of Sigma be an integer vector
which is that the net update to each
coordinate in the stream Sigma so you
know Sigma is a long list of updates and
for each coordinate I I look at all the
positive updates and i subtract all the
negative updates and I get my net update
that's my frequency of Sigma and the
idea is the following that if I'm in a
state s and I update by a stream Sigma
which has zero frequency then and then I
get to some new state then the answer in
the new state ought to be similar to the
answer in the old state right if the
algorithm is typically correct then this
should you know on most inputs according
to some distribution then this should be
what happens and so the idea is maybe we
can collapse all states s and s prime in
the automaton for which with some abuse
of notation I started say s and I add or
execute the stream of updates in Sigma
and get to s Prime and Sigma has zero
frequency
no so this is intuitively making things
path independent right okay so yeah so
the issue is how to formally define the
new states the states of the new
automaton the transition function and
the output function and to guarantee
that overall we have a correct algorithm
okay so we're going to introduce the
notion of a zero frequency graph which
is a directed multigraph G the nodes of
this graph are the states of the old
automaton for a fixed randomness and
what we're going to do is we're going to
draw an arc from state s2 state t if
there's a stream Sigma for which if i
start at s and i add Sigma and I get to
T and Sigma has zero frequency and figma
has zero frequency so I connect all so
it's a multi graph so what's going to
happen is I fix the mall priori length
bound on the streams it's going to be a
super large length bound but finite and
what I'm going to do is for each stream
in that length bound which has zero
frequency I'm going to draw an arc from
s to T if it connects esta t ok so it is
a multi graph right i have many I could
have many arcs between smt
oh okay yeah the frequency of the stream
Sigma is I look at all the it's an
n-dimensional integer vector and the ice
coordinate is the sum of all the
positive updates to the ice coordinate
in that stream minus the sum of all all
the negative updates all right so being
zero frequency means I I added one to
the each coordinate I the same number of
times i subtracted one but but the
ordering is completely arbitrary say the
zeros no update at all right yeah so
basically all the updates the net update
on each coordinate is zero okay so what
we're going to do in this zero frequency
graph is define a terminal equivalence
class now what this is is it's just a
strongly connected component in this
directed multigraph with the additional
property that there's no way you can
escape so there's no outgoing edge of
the strongly connected component okay
and basically the an observation is that
suppose I'm in this zero frequency graph
G and I take some arbitrary walk along
the edges then the observation is that
eventually my walk lands in a terminal
equivalence class for a long enough walk
and and just to remind you that what is
a walk in G so we're not walking along
the transition function what you're
doing is a walk and G consists of a long
sequence of zero frequency streams right
and just all concatenate it together
this long sequence ok so what we're
going to do is for the old automaton
we're going to look at at zero frequency
graph and we're going to define the
states of the new automaton to be the
terminal equivalence classes of the of
the old automaton ok
yeah isn't there a natural underlying
group structure and then you can just
divide by the commutator so this is an
arbitrary automaton so okay ah yeah sure
but fur for our setting isn't there
always a roof structure because the
updates are just from updates of X and
wiser okay worry about this man get out
of nacht opinion but a group structure
that's keep your budget of this zero
frequency steam right you you may lend
yeah this just means the group is not a
billion but I don't see why there is not
no underlying group okay well I do my
stuff too yes then for this process if
you zoom to this graph this multigraph
is a fun character then can assume that
it's under actives I if if you if you
replace about one a steam mop or
frequency 0 and if you have players are
now now you can't assume that I mean
that that's kind of what you sort of
want to prove right you I mean the only
thing you want to prove something
stronger than that but that would be
implied by by that something what is
that part in the region i know that's a
little because you can always i guess if
you mr. oh whatever okay no visit you
that we have middle class yvonne a sense
no you can't
change there yet helium of a few
thousand peculiar have go on the line is
your instructions any means that we can
we change again what happens one
gentleman all these but how do you oh
you want to change the algorithm change
the graph so that it has the right
you're a senior how do I do that in a
global way that I mean an end to
preserve correctness and this is and
this is sort of what what this proof is
going to give maybe ask again after the
this proof okay on the phone so okay so
it's clear with the states of the new
automaton are right terminal convince
classes of the old um what is the trance
re yeah that so I didn't later said what
do you mean like a walk g eventually
introduced a terminal in Scott so it
doesn't I mean it doesn't have to write
it just end it
no because like if you if you're at some
node and you look at like what it can
reach and one of the things that it can
reach if all the things that can reach
can go back to that node then it would
be strongly connected but if there's
something it can't reach then if I sort
of take a step from that node then
either I can't go back to that original
node or that thing that original note
can't reach can't I can't reach the new
node because because otherwise right so
basically you can you can peel off
vertices and argue that eventually you
need to land inside of one of these a
some old paths or I mean with the long
enough with a long enough walk but they
need air can grab right no no but it's
always right in any it doesn't matter
what but it's a finite graph as a
function graph you invention you know I
you guys that might not be put this is
disconnected coaster and I hope you know
opponent right then I'm reversing what
they are paid this merely one vertex as
well but the number versus doesn't
depend on the number of updates so it's
a family- I mean you mean you will
always have edges out to another
communicating class with you no no no
because the number of vertices is a
function of n right so like I mean I
fixed my underlying graph with some
function of N and then I take these
really long walks and so basically i
will eventually land in this from
oaklands class it's not like i'm getting
new vertices we're missing is just the
picture cause they have some cycle that
they can directed cycle they can go
around and they're also edges going out
from that to some terminal states but i
just don't use them thank you walking
along this very good size oh oh you're
saying yeah okay uh what do I mean by by
by walk right is that the question yeah
so I mean I get one thing I can say is a
random walk will eventually get there
with high probability yeah so that's
what you make
typical yeah so I mean it's of Isis for
a random walk to have this property but
you'll see how this is used in the proof
like basically I'm saying yeah that what
you then or random walk yeah what a
property point what properties do you
need of this in the proof if you just
take the definition of a tonal movements
but you don't need anything else yeah
that's it there is some walk which will
land there in fact that's all we need
actually that some walk will land in a
turbulent always fun I mean so basically
I started some node and there's some
walk to a terminal equipments class okay
you'll see like exactly what is needed
on the next slide and should be more
clear so okay so in fact you'll see it
here so we need to find a new transition
function okay so suppose I'm inside of
some terminal equivalence Class C and
I'm given an update AI I need to define
a transition function what do i do so i
let a V in CB an arbitrary node this is
a state of the old automaton right the
states of the zero frequency graph the
vertices of zero frequency graphs of the
graph are the states of the old
automaton now what I do is I compute the
new state v + e I using the old
transition function right so the old
transition the old automaton comes with
a transition function which has that are
in some state v and I a DI I go to some
new state so this is a you know a state
is also a node in this your frequency
graph and so what I do is I take a walk
from v + e I and this is getting to your
question to a terminal clovens class c
prime yeah isn't it every nor bartow
songs of analyst for known
no because I could sort of started a
note and walk away from it and never go
back right i mean now you have a zero
frequency update you go back to that
class yeah but i mean i could start at
some node which is not in a terminal
equivalence class and i can execute a
you know a bunch of updates i get to
some terminal equivalence class in there
if i update with zero frequency streams
i'll stay inside of it but why do I ever
get back to my original probably okay so
basically you're pregnant you're
definitely relation that you are
defining that you're connecting but not
to another it's a zero frequency ok it's
an equivalence relation so there is an
equalist partition and no that's really
I mean it's sort of what we're trying to
prove in sometimes but like like there
could be a city it's not symmetric so
it's all it's not so many more okay so
yeah I mean ultimately we want to
statement of that fun yeah but see plane
is late it's not uniquely different over
here ah so that's a great question it
actually it is uniquely defined and I'll
show why that is so that's why it
doesn't actually matter like you just
need a walk that gets you to a term
local dance class c fine so uh yeah
that's your answer what so what I'm
doing is I'm saying you know how do we
define the transition function I CEI I
choose an arbitrary node and I take an
arbitrary walk until I get to a terminal
equipments class and the claim is that
it doesn't depend on the choice of
original of note that I chose in the
terminal influence class and actually
there's only one terminal equivalence
class reachable from any walk
and this second property it's crucial
that i started in a terminal equivalence
class so so so you'll see the proof now
pictorially why it's unique okay so yeah
what's going on here so I'm in some
terminal equivalence class some state of
the new automaton and I had multiple
choices it's a U and V of how to do my
update right suppose I chose you and I
did the update AI according to the old
automaton and I got to this state and
now what I did is a zero frequency
stream walk so it's a walk in the in the
zero frequency graph and I eventually i
said so there's a walk which gets me to
a terminal equivalence class and suppose
if i were to choose v a different vertex
in the original terminal evans class and
do the same thing i would get to a
different terminal convince class now I
claim that these two terminal coombs
classes are the same yeah maybe I should
I was asked to ask Emily can't you clean
the graph like kind of discrete I saw
your update and create a directed graph
where if you take them a PDO go there
and if you subtracted update your book
and sorry what you mean my discretize
the graph search iskra dies all the
updates until AJ so like if you can have
Delta one less than your discrete the
smallest possible discrete okay so let's
say all the updates or 1 or minus 1 or
my ship so you can have like if you take
a 1 you go there if you take a minus
when you go back and that way you will
know I mean that's what we're trying to
prove that that we can take this
automaton and get it into that form I
mean so that so that is implied by being
a path independent automaton if I'm in
some state and I a d1 and i subtract the
one I should go back to the same state
yes you should but but I mean I don't
see how you can point in the original
model it's you know you don't like just
returned to the same stages because you
add 1 and subtract one yeah I mean all
boys do is you know it's time to bend so
the whole point is to kind of prove
yeah and I mean we're trying to show the
automaton has what you said and I don't
see how to do it by sort of locally
adjusting things that is what your
question is okay so why are these two
terminal equivalence classes actually
the same so okay so suppose that I
started at you and I executed this path
and got to a terminal equivalence class
and I got to some note w then what i
could do is from w suppose i have the
update minus CI then let's look at this
path it's actually a zero frequency path
because i start at you I addy I i think
is zero long sequence of zero frequency
streams and then i subtract TI so the
net update is actually 0 so what that
means is that w when i subtract a I I
must return to some node X inside of my
terminal equivalence class alright
because it's a zero frequency update
stream and there's no outgoing edge I
can't escape this terminal govans class
now also this terminal covens class is
connected so there's a zero frequency
stream path from X to this other node V
that we said we could have used instead
and now we said from V suppose if I were
to a DI and get to some state and do a
zero frequency we walk I got to some
other terminal equivalence class but
then these two terminal equivalence
classes are actually connected by a zero
frequency path right from wi do minus CI
i do a zero frequency path i do plus e I
and a zero frequency path but that means
i need to be the same terminal
equivalence class because you can't
escape I mean that's the definition okay
so so that actually shows that it
doesn't matter if you chose you or V to
do the update based on and it also
doesn't you can show that you can only
reach a unique terminal equipment class
from a node for the same reason
okay um yeah so good okay so we define
the transition function now we want to
define the output function of this new
automaton so what we're going to do so
in each terminal kuving's class c we're
going to do the following we're going to
sample a node you from the stationary
distribution from a random walk inside
of this terminal equivalence class ok so
it's strongly connected we're going to
add self loops with some positive
probability to each of the nodes so the
stationary distribution exists these
self loops they correspond to empty
streams and what we're going to do is
the output of the new automaton on this
term of equivalence Class C is going to
be the output of the old automaton on on
this vertex you that you sampled this
way ok so so what doesn't work or this
what I don't know how to prove works is
if you were to instead choose an
arbitrary vertex in this terminal
oaklands class and output with that
outputs I don't know how to prove that
works intuitively what's going to happen
is you're going to want some
distributional correctness and some of
these arbitrary nodes might just be bad
nodes give the wrong answer you could
try to choose a random node but what
does random really mean like are you
really at a uniformly random node or not
like maybe that's also a bad note so
I'll explain why this choice works on
the next slide um we also need to choose
a starting vertex of the new automaton
what we do is we take the starting
vertex of the old automaton and we take
a random walk in the zero frequency
graph until we get to a terminal
equivalence class now you could get to
multiple terminal equivalence classes
from the starting vertex because the
starting vertex need not be in a
terminal equivalence class itself so
there's some mixture some distribution
on these terminal convince classes that
you get ok so we get to some terminal
equivalence class and we let the
starting vertex of the new automaton be
the terminal equivalence class
that we got to from this random walk so
what we did is we defined the starting
vertex we define the output of each term
eloquence class so we've basically
characterized we said what the new
automaton is we take an automaton we get
this new automaton now why do we have a
correct algorithm okay so what we're
going to do is we're going to show that
the following we're going to do is for
every distribution pie on stream Sigma
we're going to show that we have a path
independent automaton which succeeds
with high probability over streams drawn
from this distribution pie and then but
from the strong version of the minimax
principle what we're going to do is say
that therefore we have a randomized
algorithm which works for all streams
okay so let's see how we can you know
argue that with a path independent
automaton for an arbitrary distribution
Payan streams and so here is a trick
what we're going to do is we had our old
algorithm which is a randomized
algorithm so it works for every stream
with you know for any fixed stream with
high probability so what that means is
there's a fixing of its randomness so I
get this deterministic automaton hey old
which is correct on the following
distribution pipe rhyme now pipe arm is
not going to be the same as pie but it's
going to be related to pie so let me
just say what that is you sample a
stream Sigma of updates from
distribution pi
and you're going to create a new stream
Sigma prime of the following form so
Sigma prime is going to look like a long
sequence of 0 frequency streams tau 0
followed by the first stream update
Sigma 1 followed by a long sequence of
zeros streams tau 1 followed by the
second stream update Sigma 2 all the way
to the end until you get to Sigma R and
then followed by a long sequence of zero
frequency streams so this distribution
pipe I'm is derived from the
distribution pi and the key property is
that the output of the new automaton on
distribution pi is statistically close
to the output of the old automaton on
distribution pipe rhyme and note that
you know the answers are the same right
if I'm putting a bunch of zero frequency
streams for a fixed stream the you know
the underlying frequency vector X the
underlying factor X is the same
generated from sigma sigma prime no no
so there's just some arbitrary
distribution pie on stream Sigma and and
these are its updates yeah how how you
really choose as your dreams uh so
they're just long enough so each 0
frequency stream corresponds to an edge
in this 0 frequency graph we choose a
large long sequence of these things
along enough sequence so that and I'll
say it okay so that basically a random
walk is what you get is statistically
close to the stationary distribution so
so let me just say that more uh yeah
zero frequency streams
so so basically these are all zero
frequency streams of a long enough
length and you choose enough of them for
each of these in between parts so that
it let me just say why the proof is why
it goes through and then you'll see
what's going on maybe it's easiest to
say it as follows so what happens is you
first execute a long sequence of zero
frequency streams and you end up in some
a terminal equivalence class now how do
we define our starting state for a new
what we did is we took a random walk
from the starting state of a old in the
zero frequency graph so basically the
terminal equipment spots that we get to
the distribution of on terminal
equipment classes that we start at it
statistically close to the distribution
of terminal covens classes we would get
by taking a long frequency of zero of
long sequence of zero frequency streams
ensure that this walk is longer if
you're close how long how long do you
need it I mean you can just get some
finite bound which is enough so so this
is where the stream length gets really
long yeah wrath is directed so the this
mixing thank you the inspiration oh it's
reports an explanation yeah yeah I mean
whoa okay yeah yeah it's not a small
number of it's not a small stream it's
just some finite bounded yeah
and so exponential length here is
acceptable yes what we're saying because
note that the the streaming algorithm
the state space doesn't depend on on the
length of the stream but it depends on
this underlying dimension end of the
vector can depend from WV there some
kind of calendar you know how many
elements it has seen over there then
sort of its space stage will depend what
was so what we're doing is we're only
counting states for which an app for
which an X in the in this bounded
integer box could reach so basically if
I'm just counting a lot of things and
just throwing the first coordinate and
the first cordon is increasing
arbitrarily I don't count states for the
old algorithm once this County's
exceeded em and I design a new linear
sketch where the number of states
measured the same way is about the same
as the old automatons number of states
and now if you wanted to sort of measure
all possible states you could get with a
new linear sketch I'm claiming you could
just take each of the coordinates mod
poly N and then you'll never blow up in
this way because if at the end your
promise you're inside of this small
energy box then you'll get the same
answer if you take everything mod
paulien right so I mean we don't want
sort of lower bounds that come for
trivial reasons right I just you know
somehow the state depends on I mean the
space depends on the stream length and
we just make the string length long
enough ok so what I didn't describe ok
so basically what I'm saying is if I
take Sigma Prime and I execute a long
sequence of zero frequency streams then
this is like taking a random walk in the
zero frequency graph and that's exactly
how we chose our initial state of the
new automaton we took a terminal
equivalence class that you reach via a
random walk then what you do is you
update by Sigma 1 you take a long
sequence of zero frequency streams you
get to another terminal equivalence
class etc
so basically what you can show is in
Sigma Prime at the end you're going to
reach the same terminal equivalence
class as you would if you were to
execute so you execute the old automaton
on Sigma Prime you reach some terminal
equipments class and that distribution
on terminal coons classes that you reach
is very close to the distribution that
you would get on states of a new if you
were to execute pie as stream drawn from
PI okay and sort of the the key idea
here is that at the end you get to some
terminal cohen's class and you execute a
long sequences zero frequency streams
that's that's taking a random walk
inside of the terminal equivalence class
and so that random walk is statistically
closed where you land is statistically
close to a sample from the stationary
distribution and the output of a sample
from the stationary distribution is
exactly what the output of the new
automaton is what does it make sense
it's sort of like sort of the trick is
to you know relate this old this with a
one distribution PI to a distribution
pipe rhyme where we take long sequences
of zero frequency paths and we argue
that this gives us something which is
statistically close to the stationary
distribution inside of each terminal
events class yeah why I can't be too
mystic ligue 1
pretty good
yeah so the pike could be a singleton
could be a mean pie is just puts all of
its mass on the single stream so i guess
it means seems like this
I single
leather
oh sorry I guess yeah we're running way
over time okay so that's sort of the
main trick and basically so what you can
show us for every distribution pie there
is a new automaton that's generated via
reduction which is correct on with good
probability on streams drawn from PI you
can show the way that we created this
new automaton its path independent you
can get a randomized algorithm now that
you have a correct algorithm for all
distributions and you can reduce the
number of random strings you have in
your algorithm by standard techniques so
similar to newmans theorem basically you
know if each randomness in your
algorithm is correct for a constant
fraction of your inputs in the integer
box then if I take so I how many inputs
so I have minors your box I've M to the
N and if each randomness is correct for
a constant fraction of them then my turn
off bounds if I choose n log M such
random nas's then you know they'll be
correct and so basically I can reduce my
number of random strings to something
small and the number of random strings
is exactly the number of linear sketches
that you get in the hardwiring ok so I
don't have time to talk about the second
part a sort of like yeah basically at
this part is somewhat easier in the
sense that once you know that your path
independent you can define em to be a
free module which is sort of all the
vectors X which go to the same state is
0 so basically now that this now that
each vector X only goes to a unique
state you can do that you can show that
the state of the automaton of the cosets
of this quotient module and using some
lattice theory the Smith normal form you
can show that the state's can be
represented as ax where you take each
coordinate mod a different value Qi and
then there's some tricks to reduce this
to get a sketch over the integers which
I won't talk about so
just to conclude so one application of
this is to proving lower bounds as I
said it gives simpler proofs of existing
lower bounds and also if the optimal
algorithm is a linear sketch then you
just need to prove that there's no good
linear sketch for your problem which is
an easier task than necessarily going
through communication complexity and
this has led to some new lower bounds
and all another thing is that there are
many dimension lower bounds known for
sketching various norms over the reals
like if I want to sketch some matrix
norm you know via linear measurements
there are lower bounds for that and the
question is if you can use this
framework to translate those lower
bounds to get lower bounds in the
streaming model and there's a little bit
of an issue there which is that the
lower bounds assume continuous
distributions over the reels and it's
not clear if you can get those in this
sort of finite precision case so that's
all thanks
questions were social David Nogueira
under tomorrow so you want to meet
offline</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>