<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Theory Day Session 5 | Coder Coacher - Coaching Coders</title><meta content="Theory Day Session 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Theory Day Session 5</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/64c0lPx7BTA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
afternoon so for those with the with
stamina who have stayed with us this so
so far we have the great great closing
talk by Medusa Don an imperfectly shared
randomness in communication ok so thanks
you also thanks for arranging this
theory day it's really great to be
celebrating theory within Microsoft even
if we are a little less strong than they
used to be or maybe not in a strict
sense we've done well since but we
definitely lost a few people the recent
past so ragu thankfully is now at UCLA
and this was joint work when ragu and
Claremont and think it for all it MSR in
New England and we started thinking
about this question so what's this talk
about I'm going to talk about
communication complexity very quick
introduction to those of you who haven't
seen this you have two players Alice and
Bob there's they have some private
inputs x and y and they want to compute
some joint function f of X and typically
0 or 1 valued functions are the ones
that we are thinking about and they are
allowed to do some interaction and the
way I want to think about it Bob wants
to compute this function so Bob outputs
f of X Y idea and the question that we
are usually interested in asking is over
all the possible interactions that we
could consider which is the one which
minimizes the number of bits the day
exchange and today it will be very
important to talk about randomized
communication strategies so here there's
some randomness which is shared between
Alice and Bob and they're now allowed to
output the correct value with
probability at least two-thirds so
that's the change in the model and now
once again we can still continue to ask
questions about this so now wipe do
people usually study communication
complexity with normals not here anymore
he wrote the book on it I would suspect
he may agree or not but by and large if
you look at the bulk of the literature
on it it's focused on proving lower
bounds and communication complexity if
you ever give a lower bound which is
little less than n then you know your
paper is rejected so you want to come
together the the Holy Grail is to get
good strong lower bounds and then these
things to have lots
of application circuit complexity
streaming data structures etcetera
etcetera but today I really want to
think about it as a positive concept I
mean you look I mean I found out here
trying to figure out how to design a web
web form or some such thing I want to
understand what is the user likely to be
I know and care about what are the
things that I should tell them about all
the services that we I have to offer
what would I like to do at the end is to
minimize the total amount of
communication the user is not going to
sit around on this website for ever and
ever they're going to do something
quickly they're going to go away what
kind of a website should I offer them
what's the right way to study this I
would say look actually this
communication complexity is the right
model to be studying these kinds of
things a particular thing that I want to
in the long run thought talk about but
not something that will get to today's
you know when we talk about human human
communication or computer to computer
communication these are often cases
where these are you know inputs are huge
the X and the wine so on that you're
thinking about are huge what is the
context for today's talk of mine well
all the knowledge of English that I have
all the you know current socio-political
things that we might want to invoke all
the mathematics that I know and the same
on to your end but these two are not
exactly matched okay so do i really need
to know all the words in English that
you know in order to be able to design
the stock and if I do it's going to fail
miserably so there's going to be a large
amount of context the communication is
relatively brief I really want to think
of communication that's extremely short
you know rapid fire communications tweet
twitter is a good example and on the
other hand the contexts are always large
and the context are not going to be
shared perfectly this is roughly the
kinds of things that we want to talk
about this is the kinds of things that
I'd like communication complexity to be
able to address I'm not going to get
there today today we're going to talk
about one very simple idealized
mathematical problem in this space but
you know just this is the general
picture that i want to start thinking
about so what is in this talk two things
that I wanted to do one which is
basically just a little bit of pedagogy
we've had lots and lots of I mean
there's a book as
other surveys on communication
complexity most of them start off in
there say here is one protocol and now
let's start doing what we really want to
do lower bunks I want to change that
thing I want to give you a few different
problems which have low communication
complexity protocols and I want to just
sort of tell you about these that they
exist mainly because I want to say that
actually low communication complexity is
very interesting and this is the context
in which I want to talk about some extra
levels of uncertainty and how to
overcome you know how to make
communication protocols more reliable
under a new notion of error or
unreliability okay all right so this is
one talk one slide which I hope will be
very very useful too many of you there's
nothing here that's new however it's
very things that you might not have all
put together on the same page in the
past one of the most basic problems I
mean there are a few problems where
communication complexity is very small
deterministically but that's sort of not
the interesting ones the interesting
things happen when really I cannot take
all the input in my head and put it in
some small number of equivalence classes
and still managed to communicate very
little to you and get you to figure out
the computer function one of the
simplest problems of this type is
equality I have a big string X in my
head you have a big string Y in your
head we want to know if these two
strings are equal or not the
communication complexity of this problem
under this shared randomness model is a
constant fixed constant depends only on
the error the error we fix to be
two-thirds so it's constant what is this
protocol it's actually very very
elementary but i will mention it anyway
because it starts introducing some
things that i will use later anyway the
way you should you know one way to check
for equality is i could look at it the
randomness tells us to look at a
particular coordinate of x and y and we
look at it that's clearly going to be a
very bad idea if x and y happen to
differ and only one coordinate so what
do you do it's the very most obvious
thing you don't talk about x and y you
talk about the encoding zuv x and y in
some error correcting code so use an
error correcting code now these strings
differ in a constant fraction of the
places your randomness tells you to look
at a particular coordinate you just
exchange
black particular bits of x and y if the
encoding of x and y and check and see if
they're equal clearly a constant number
of bits and you get as small an error
probability as you want okay that's
protocol number one now there are a few
other problems I mean actually for most
people I think it stops right here
that's the constant communication that's
the landscape of constant communication
complexity no it's not the few other
problems which are all generalizations
of these Hamming distance but in a
restricted sense we are given two
strings x and y and we want to say one
if the Hamming distance between them is
at most K where case think of K is a
constant five or ten and you want to say
zero if they're having distance is large
this is an extension of the previous
problem the previous problem was k equal
to zero if k equal to zero we had the
identity problem here it turns out you
can actually do some thing fairly
reasonable we shall get us to K log K
bits of communication I will tell you
some weak approximation to this in a
minute that's one other problem where
you have constant communication
independent of the length of the input
here is another problem we are looking
at small set intersection so think of X
as the characteristic vector of some set
why is the characteristic vector of some
set both sets are of size at most K okay
the universe is massive there's two sets
x and y of size at most k you want to
know if the intersect or not here
there's a very clever protocol do to
host our than witnessin which gets k
bits of communication if you don't want
to be very clever then there's a very
simple way of getting some pauline k
bits how does these how do these things
work very simple we have lots of common
randomness we can design a pick a random
hash function pick a random hash
function which maps the universe to
something like K squared bits there will
be no collisions now if you use this
hash function in some appropriate way in
each one of these cases you will be able
to reduce the universe eyes to something
like K squared you can just exchange all
the bits that you want to exchange in
this universe you get self party in K
communication that works out quite
nicely you have a
in constant communication protocols
independent of the length of the input
Ravi this is order log in this in all
these the shared randomness is order log
in bits and it's not a coincidence it
comes without loss of generality there's
a theorem due to Newman which says that
any problem you want to solve with
shared randomness you only need to use
log n bits of randomness it's a it's a
sort of it's a variation of the same
proof that went into edelman's theorem
but when you apply it here you still
need some randomness now comes the nice
problem which I don't think has been
sort of studied in the communication
complexity literature itself however
it's it's actually well studied in a
different literature where they actually
offer a solution which looks like a
communication complexity solution and
this is what i call the gap in a product
question so in communication complexity
there's an inner product but here we are
talking of real vectors and rail in the
product so I have in my head a unit
vector in n-dimensional real space you
have in your head a unit vector in
n-dimensional real space what we want to
know is there in a product but we can't
estimate it exactly so we are happy to
estimate it to within a gap of plus or
minus epsilon something like that or in
general you want to say yes if the inner
product is sufficiently large at least
some number C for completeness you want
to say zero if the inner product is less
than ss4 soundness can you do this and
it turns out that you can actually do
this with communication complexity which
is something like 1 over C minus s
squared so if C and s are separated by
epsilon epsilon 1 over epsilon squared
bits suffice to solve this problem and I
won't give you a full explanation of how
this is done but a very quick insight as
the following I have a huge vector in my
head I don't want to send you the huge
vector I will be able to reduce it to a
single real number by the following
observation if you pick a random
Gaussian vector in n dimensions so in
each coordinate it's a normal mean 0
variance 1 and it's independent across
the coordinates
and I look at the inner product of g
with X and then a product of g with Y
and take their product this expected
this is the expectation over G of this
quantity this is an unbiased estimator
of the inner product ok and it's even
got pretty good variance and if you now
use this thing I will use the shared
randomness to define this vector g i
compute G times X discretize and send it
to you that more or less gives you this
protocol ok so it's a I mean if you want
to talk about communication complexity
and applying it in the real world
tomorrow this is a pretty interesting
problem i mean i could imagine large
number of communication problems being
estaba in terms of i have some objective
in my head you have some offering in
your head we the gain that we're going
to get by collaborating or interacting
is probably this inner product I mean
it's a pretty reasonable thing and the
fact that can be solved very efficiently
is important this is probably damaged
yeah maybe I mean maybe I have the wrong
reference here but there for whatever
reason this the differences but you
subtract out the lifespan and so on
everything will work out I think so
right yeah yeah exactly it's the lengths
of the right things but you put it in
your semen is right over here I just
normalize everything and so right yeah
good ok so i should thank my students by
the end pretty issue sort of you know
put this taxonomy of what was known
together for me alright so those are the
examples of problems which have short
communication complexity that was not
the goal of the talk but i just did want
to emphasize that there are many of
these problems and these are the
problems will be coming back to to
motivate the rest of the talk the
general question that i want to talk
about is reliability in communication
and to model some notion of uncertainty
now what could uncertainty mean modeling
this is a lot of effort lot of work goes
into it I will not be talking about any
general settings but I just want to draw
this picture you know you have this
thing Alice and Bob are talking to each
other there are lots of things are
supposed to be common to the two
anything that's common to the two you
can sort of ask you know you can put it
under a lens and say is it really common
so you can look at this wire that they
are using to interact with each other if
you decide that it's not a reliable wire
then you get Shulman's problem of
interactive communication and there's
been a lot of work on that in the recent
past now what we're going to do I mean
you could put any dividing line between
the two and say look okay what about
this function Bob wants to compute F of
XY but what does Alice know about f I
mean maybe she doesn't know everything
it's probably a pretty reasonable
setting too we don't get to it what
we're going to look at is this middle
arrow the randomness be saying this
common randomness which is shared
between both the players what happens if
that one is not really perfectly shared
ok so that's the question Alice and Bob
don't share random is perfectly only
approximately what can you do so I'll
tell you what the modulus I'll tell you
about some positive results and maybe
couple of slides on the negative results
alright so here's the model what I will
sort of go out and make come up with a
very simple clean model of shared and
imperfectly shared randomness and this
is all will work within the talk there
can be other variations that can be
looked at but not in this talk all right
so Alice gets a sequence of random bits
these are going to be plus minus 1 bits
just so that i can write everything else
in nice formulae and Bob gets a sequence
of bits the pair are I si are all going
to be independent and they are all
identically distributed ok so over I
their independent RI is correlated with
Si and each one of these is going to
have a marginal which is 0 sorry
marginal distribution expectation is 0
and the two of them are correlated which
means if you look at the product of our
I x SI each one has the same
distribution the expected values row ok
so it just says so when Rho is positive
these are positively correlated if Rho
was equal to 1 these are identical if
row 0 these are independent I'll use is
our may
we not really used but anyway maybe in
this slide I will use is our to denote
the communication complexity of a
function with imperfectly shared
randomness and where the correlation
between the individual pairs is Rho
extreme cases perfectly shared
randomness is Rho equal to one private
randomness is Rho equal to zero ok
starting point for boolean functions now
we'll come back to when I mentioned this
already earlier clearly you know
communication complexity of a function
is upper bounded by its communication
complexity with some correlation this is
perfect correlation some correlation no
correlation but even this is upper
bounded by the communication complexity
plus log n why is this because of
newman's theorem which says that in any
problem the total amount of randomness
that you need is log n bits so this
already gives us a pretty good
approximation to how good communication
complexity of a function can be up to a
narrative log in but I don't like
additive logins this talk yeah yes
everybody knows this row it's fixed
envision you need to know a lower bound
of the row and protocols will work but
yeah sorry i'm just going to show that
it's less than the private random
message i guess you don't even need to
know the more you could have alice is
the odd bits and they Bob used to even
beats and and then it works out exactly
and in general actually there's a even
stronger monotonicity you can take
anywhere you can look and roll wow I
mean I can just rerun demise every bit
with a little bit extra thing and then
all right so but what if the
communication complexity of some
functions are much much less than log in
like we've seen a few examples where
it's a constant can you get a constant
in those cases so a little bit of a
history we started looking at this
question in Clemence friend to this talk
at I kalp and he sees this paper by
bavarian kavinsky and ito with exactly
the same model now this wouldn't be such
a dramatic surprise except bavarian is
my student and i'd had no idea that he
was working on this problem
anyway so fortunately they didn't have
the same collection of girls and
questions so once coping as completely
but that's the only other paper that I
know in the literature which has really
talked about communication complexity
with this correlation in general in the
literature and probability as well as
information theory in signal processing
people have been looking at what can you
do with some correlated randomness even
cryptography quantum key exchange etc
etc what can you do but not in the
language of communication complexity
which is what we're going to do today
but waiting at all said well this plus
log in makes this problem uninteresting
so we'll look at it in the sense of
simultaneous communication which is a
different model of communication
complexity for us really I am interested
in tu parte communication complexity but
I'm really interested in constant
communication protocols so here for us
it still made sense one of the things
that they show even in their sort of
their model is actually you can do less
in their model but even their model they
show that equality testing actually has
a constant bit communication protocol
even with imperfectly shared randomness
it's not completely trivial if you look
at the protocols we have they are very
very sensitive to the randomness that
you are using you can't just convert it
to a different one so and one of the
things that I'll try to do is show you
this what we did was actually came up
with a very general result which says if
you have a problem with communication
complexity k its communication
complexity with imperfectly shared
randomness is actually at most
exponential in this game will be one for
every constant Rho which is strictly
positive yes and we were very unhappy
with this result and try to improve it
and finally we were able to prove that
we couldn't improve it so there is a
function it's not a it's not a function
there's a promise problem for which the
communication complexity is at most K
but with imperfectly shared randomness
its communication complexity bumps up to
2 to the K for every choice of K there
is one such thing so I guess
okay so there are some constants in
front of all of these that depend on row
constantly since it's not in the
exponent yeah yeah I mean this is a
theoretical computer science notation
constants can be suppressed Oh can be
suppressed all right all right so so in
the rest of the talk I'll try to tell
you a little bit about the positive
result how it's obtained then hint at
the negative result so the positive
result comes from thinking about inner
products and now again I'm talking of
real in the products what we're going to
do is encode my vector X in an error
correcting code but error correcting
code which is sort of where bits are
plus and minus one yes so that if you
think of it as a real vector it's a real
vector of constant length I mean
constant ok so you encode X into this
big string capital X Y into this big
string Capital y and x and y are going
to have this thing if x and y are equal
little X and little wire equals so our
capital X and capital y the inner
product between these two is n ok if
little X is different from why then if
you are encoding it's good enough then
the inner product here is at most n over
2 ok I think goes from anywhere from
plus n 2 minus N and over to safe enough
ok so what we're going to do is try and
estimate this in a product and the way
we're going to do this is based on this
sort of the sketching protocol that I'd
already mentioned earlier but we do it
slightly differently what we are going
to do is the following Alice will pick a
collection of gaussians each one of
these is an capital n dimensional vector
and t of these and what should go to
send to Bob is she's going to pick the
index I which maximizes the inner
product between GI and X so if you've
seen analysis of the coloring algorithm
based on semi definite programming and
so on this is very similar you're
looking you have a vector in n
dimensions you pick n run Katie random
vectors and pick the near ask I ok
and Bob does not have these gaussians on
these random gaussians Bob does not have
them but he has the ability to
manufacture gaussian switcheroo
correlated with Alice's gaussians okay
just take a collection of bits and
average the mountain that gives you
Garcia's okay and they will the
correlation is preserved at least
positivity of the correlation is
preserved and now what she's going to do
is look at the i'th coordinate over here
of her eyes girls in the she has because
he sent a bob sorry Alice sent I so Bob
is going to look at the i'th coordinate
and he is going to check if this in a
product between GI and y is positive or
not and if it's positive he accepts
turns out that this works out you have
to pick T to be something like all right
okay so some constant T works for this
example but in general if you are trying
to measure in a products to within plus
or minus you know epsilon when x and y
are normalized to unit length then this
would you would have to pick exponential
in epsilon squared different gaussians
and then you send the index of one of
them which is log of exponential of 1
over epsilon squared bits 1 over epsilon
square base ok so this manages to work
out and this is what we call the
Gaussian protocol and it turns out to be
very very useful how do you go from this
very simple in a product problem you
know equality testing problem to a
general case turns out inner products
really capture everything in
communication complexity so this is
quite standard if people have looked at
it if not I'll just give you a very
brief idea why this is the case I mean
what does a simplicity let's think of a
one-way communication alice is going to
send some message to Bob and Bob is just
going to spit out the value of the
function what kind of what does this
protocol look like well alice is going
to send some message some number between
1 and 2 to the K K bits of information
so she's sending some information and
Bob is taking some function which says
given whatever Alice sense what should
he do accept or reject alice's input is
represented by an integer between 1 and
2 to the
a bob by a function from 2 to the K 2
101 okay and what we know is that they
have a perfectly shared random thing I
mean so if you look at all the different
random strings on which they're doing
they can use for each random string
they've come up with a possible message
and a possible function to work with if
you look at a random choice of our this
value fr of IR is the right value that's
the definition of having a communication
protocol with K bits of communication
and perfectly shared randomness we want
to take this problem and just solve it
without having the perfectly shared
randomness first thing we were going to
do is convert their strategy alice's
strategy is a collection of integers I
sub R over ranging over our Bob strategy
is a collection of function f sub R
ranging over are we going to take both
of these strategies and represent them
as vectors so very natural
representation of the vectors so that's
the same thing that I had earlier the
representation is simple I sub R is just
going to be represented by a unit vector
of length 2 to the K 1 in the coordinate
I sub r + 0 everywhere else f sub r is
going to be represented by the truth
table of this function both of these are
vector for vectors of length 2 to the K
their inner product is the value of the
function okay the only thing that's
happening over here is that there is a
certain normalization these this vector
is of length 2 to the K norm 2 to the K
L 2 norm 2 to the K and you want to
distinguish this inner product within
plus or minus 1 so you want a 2 to the
minus K approximation to this in a
product if you normalize that's the
approximation that we will eventually
have to compute and the Gaussian
protocol can do it with about 2 to the K
bits of communication yeah so all of
that ok that's it all right so this is
how you do one way communication two-way
communication is again just the same
thing again you just look at the
definition of what is two-way
communication extract vectors and
represent strategies as vectors and
acceptance as in the products and do it
it turns out to be all similar
I won't do it in the stock because it's
so at the end you get a theorem no
matter what function you have if it has
K bits of communication with imperfectly
shared randomness you get 2 to the K
bits of communication this may be
actually you know I'm not so sure this
may be actually four to the K because of
some some some form of to the epsilon
squared I don't know what happens to it
all right now my all right so the main
technical result in the work which I
might not really get to is a lower bound
says that there is a promise problem
which can be solved with K bits of
communication but the imperfectly shared
communication complexity is
exponentially large in k it's actually
some constant times 2 to the K but again
and once again this lower bound works as
long as rho is less than 1 so for any
row less than 1 you get this lower box
first thing I want to tell you is what
is this problem because I think even
this problem is actually a natural and
interesting problem and we should
probably think about it occasionally
this is what I call the gap see the
problems that we've been working with so
far are the gap in a product problems in
a product is large or small distinguish
between them there's a gap between the
good case in the bad case now we're
going to have an additional feature
which is varsity what's going to happen
alice is going to get a very sparse
vector in 0 1 to the N the vector is
sparse in the sense only one out of
every two to the K bits are ones are the
others as zeros ok and now Bob gets a
vector in 01 to the end they want to
estimate the inner product now the
largest that the inner product can get
to be is 2 to the minus K times n right
because that's the weight of the vector
X so cannot be larger than that these
are just just looking at crude in a
products no normalization so the promise
that i will give you is in the good case
in the yes case the inner product is
ninety percent of the maximum possible
in the bad case it's almost like x and y
are uncorrelated almost like 50
and we have to decide which of these the
problems so this is the problem we want
to do two things about it we want to
show that it has small communication
complexity with perfectly shared
randomness and we want to show that has
very high communication complexity if
the randomness is not perfect I'll do
one of the tools all right said that
sort of animal just to remind you what
the question is hopefully it'll stick
around for the next slide so if we just
decided to do the girls in protocol we
would have communication complexity of
about 2 to the K for this problem okay
and that would be ignoring sparsity so
we want to do better by exploiting
sparsity what are we going to do the
main idea is the following if you can
discover a random coordinate I such that
X I is nonzero then why I is sort of
positively correlated with the answer
right because ninety percent of the time
if X is 1 Y I will be one in the good
case but it will only be one with
probability sixty percent in the bad
case that gives us enough of a
distinguishing advantage so we want to
discover a random coordinate I such that
X I is one how do we do this well they
share randomness they are sharing
randomness perfectly that's the model
that we have here so what do they do
they have a sequence of random numbers
that's what the shared randomness is
which is uniform over N okay just pick a
random sequence and Alice will send to
Bob the index of the first J such that X
sub I sub J is non zero ok clearly it's
a random well this is this is generating
for us a random coordinate where X is
non zero ok and how large is j going to
be well for each one of these
coordinates the probability that X I sub
J is non 0 is 2 to the minus K so
amongst the first to to the gay k guys
you're going to find a non zero
coordinate I have to send you the index
which means I have to send you K bits so
we got an exponential advantage on the
trivial protocol ok
so by looking at this problem carefully
we are able to get a significant
advantage over the round the knife pro
protocol and now the question is can you
give a lower bound for this yeah I mean
point 60 points for with make no
difference would make no difference I
think point 6 we just use to make sure
that in the bad distribution we will
actually pick x and y uncorrelated so we
just want to make it be correlated
negatively you negatively and this the
result foothold yeah actually just have
to say things like native yeah great
okay so that's the protocol tells you
why this works now I'm so in the lower
bound I won't tell you give you the
lower bound but I will tell you about a
few interesting things about it first
you can't do the usual strategy in
communication complexity what's the
usual strategy in communication
complexity lower bounds you fixed a
distribution and then you say well on
this distribution the communication
complexity is large ok so you don't look
at the worst-case instance and the
complexity you look at the average case
complexity over a distribution you can't
fix a distribution anymore if you fix a
distribution there is a deterministic
strategy which has K bits of
communication ok why because it's just I
mean for every input there is a strategy
which works with K bits of perfectly
share in a communication with perfectly
shared randomness which means that for
every distribution there is a randomized
strategy which works with K bits of
communication with perfectly shared
randomness but then it's a distribution
you can sort of flip the minimum the
maximum or whatever and you now have a
deterministic protocol so you really
have to do the things in the reverse
order you have to look at the strategy
that you're going to employ and then fix
the distribution later the distribution
has to be tailor-made to the strategy
this won't come out and the proof but it
is I mean in the proof that i will
describe but this is the susan
ingredient that has to be there one
thing that we can we do need to do is to
say look I mean there was this perfectly
shared randomness protocol and you might
try to implement something like
that with your imperfectly shared
randomness this should not work why will
this not work we need to somehow rule
out the following somehow or the other
Alice and Bob make managed to agree on a
common index I which is uniformly
distributed between one and n where X I
is nonzero we need to rule this out how
do you rule this out we show that look
this immediately implies that they're
agreeing on some randomness with high
probability ok large amount of
randomness randomness of entropy log n
bits with very little communication we
prove that if you want to agree on a
large amount of randomness perfectly
when you only have correlations to start
with this trick takes amount of time
which is amount of communication which
is proportional to the entropy that you
are sharing so you cannot agree on login
bit strings without login bit on an
index between one and then without login
bits of communication even with
correlations that you are sharing so
this is sort of you know once we phrased
it this way the lower bond was in the
literature but that was now the other
thing is I showed you a protocol which
ignores the sparsity and achieves 2 to
the K bits but is that the best possible
we proved that yes if you ignore this
part City and so on so these are some of
the ideas behind the proof the
unfortunate thing is a protocol could
look like anything so this is not an
exhaustive list or doesn't look like an
exhaustive list but the remarkable thing
is it almost does and the formal proof
of this goes through something called
the invariance principle and it was an
amazing miracle when I saw this you know
the way this principle can be applied to
get a reduction take any protocol and
either Alice and Bob are really agreeing
on some variable very with high
probability and then you know sort of
this way strategy has some common
influence in it and then you know you
get sort of get to this setting or it
turns out that it's actually ignoring
all it is not placing too high and
influence on any coordinate
in which case you can actually replace
bits by gaussians and gaussians there's
no notion of sparsity anymore so you
know you're really sort of you can't and
then you're forced with the two to the K
lower bot so this was sort of a
remarkable discovery comes in no
perfectly suited for our setting and we
were able to use it so summarizing if
you have K bits of communication with
perfect sharing 2 to the K bits of
communication with imperfect sharing
suffice this is tight and along the way
we you know we the invariance principle
is sort of very nice very cute but I
feel again for the sake of utility
especially in either PCP literature or
in the probability literature there
would be a different articulation of
which it would be very very useful and
nice and we have in our paper like what
I think is a very very convenient way of
you stating it and once we were able to
do it we sort of had a lower bound
saying if you have this thing the in the
one-way communication would be at least
2 to the K and without almost any change
in the whole proof we were able to get
it to 2 to the K just by making the
invariance principle sufficiently
general so I encourage you to look at
our paper for that version of the thing
so back to the big picture I think this
imperfect agreement of context is
important this is sort of something that
we've been wishing away but it's not
true and communication complexity in the
classical sense we try to abstract
things as either you and I know this or
you and I don't know this a very firm
division between me and you but most
natural communication is not along these
lines there for everything that I know
there's some chance that you might know
it and some chance that you don't and we
managed to leverage this uncertainty and
still work with it in you know
anecdotally in common context Commons
settings can we sort of make it more
rigorous and prove that this happens in
mathematical sentences this is where
we're going here and the other thing
that I do want to say is that yeah you
know large context and short
communication is certainly sort of
interesting range of parameters to think
about okay thanks very much i'll stop
here oh just some time of your first
slide or motivation okay oh it doesn't
seem that at least in human to you in
communication um in fact i mean that
tries to minimize the lunch does the
information this past but there are
certain types of information that are
much easier to comprehend another for
example pictures versus words right ok
so the measure of complexity may not be
simply it's also a function of what is
easy to achieve right i mean i can
certainly point to an object in the room
and that's maybe large number of bits of
communication if you sort of look at all
the different things so but there is a
certain i mean we do want to minimize
the total time it takes to get this
communication done and so some of us
maximize i agree yeah only I guess a lot
of these things lower bounds are used to
let's say your vows for Steven
algorithms like if you have losses but
morally like if you have a good his
example that you chose right now product
right here or just the scheming also
uses team but there are other algorithms
receiving so morally on some of those
examples where you actually a blow
communication protocols for calculating
certain functions as well so like in
streaming Gianna say let's take example
from Steven causal effect because you're
seeing there are very few examples which
are known which have a low communication
and then your competition function right
should show up to an exponential factor
in the communication the inner product
problem that I described is everything
in fact we proved it in the during the
course of the talk almost
so everything else reduces to that but
then within that scope once again there
are other things i would think I mean
Yamin you know small set intersection
seems like a very reasonable thing to be
thinking about I mean we've sort of
narrowed our interests down to something
reasonable small set where we might have
an overlap and we sort of play the game
of you know and now can we find it
within this space so I mean in that
sense I do think that it makes a lot of
sense to model things in terms of
communication is that the question but
I'm done privately okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>