<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Explore or Exploit? Reflections on an Ancient Dilemma in the Age of the Web | Coder Coacher - Coaching Coders</title><meta content="Explore or Exploit? Reflections on an Ancient Dilemma in the Age of the Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Explore or Exploit? Reflections on an Ancient Dilemma in the Age of the Web</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p5QurcU0Zaw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so we're very very happy to have Barbie
Kleinberg here from Cornell we've been
wanting him to come for a long visit for
a long long time and it finally happened
so he's gonna be here throughout the
year until June and he's gonna talk to
us about explore exploit reflections and
an ancient dilemma in the age of the web
thanks for the introduction yeah I'm
absolutely delighted to the MSR this
year and I'm I've already met a lot of
you and I'm looking forward to meeting
some more of you so by way of
introducing my talk I want to tell you
about one of my favorite children's
books toot and puddle which is the story
of two pigs that live together in the
woods toot is an adventurer who loves to
travel and see the world and puddle is
the first to stay and enjoy the comforts
of home and in the book the entire story
is told through a sequence of letters
that they exchanged as two explores the
Arctic and the Arabian desert and go
scuba diving and puddle meanwhile writes
back to him about you know enjoying the
maple syrup from their trees and jumping
in the pond and you know part of the
books appeal lies in its message that
friendship can conquer this dilemma and
you don't have to choose between whether
to be adventuresome or whether to enjoy
the simple pleasures you can do
whichever one comes naturally to you and
your friends will let you vicariously do
the other but you know life outside of
children's books is more complicated
than that and often times you have to do
one or the other and in fact you know in
the judeo-christian tradition the main
plot point in the story of the first
people on earth is that they had to
decide whether to enjoy what was safe
and comfortable or to gain some more
knowledge about something different from
what they already knew so
at least from the standpoint of someone
giving this talk under a very
self-serving interpretation of the
judeo-christian tradition what it's
saying is that the quintessential
defining feature of humanity is to face
this dilemma whether to explore or
exploit and so it's kind of amazing that
you know given the this has been with us
since the first people on earth
mathematicians have not been treating
this as a serious question of research
until about my grandparents generation
and since then there's been a line of
work on so called multi-armed bandit
problems which will be the star of this
talk that attempts to quantify this
dilemma and make predictions about what
you should do in quantitative terms when
faced with explore or exploit the limits
and but you know to say that this is a
fundamentally human dilemma is actually
framing things too narrowly because as
computers have grown more autonomous now
they're facing this dilemma all the time
- and so in my generation computer
scientists have also adopted this batch
of problems as a serious batch of
challenges that they must face in their
research and that's the main theme of my
talk my first exposure to these problems
was actually in a computational context
and I should tell you a little bit more
about that to situate the issue for you
so a dilemma that computers often face
is that of finding the closest server
there's some service that's replicated
on more than one server worldwide and
you know my computer may have access to
a directory that says it can obtain this
service from any of these three servers
it now wants to request service from the
nearest one and this is something that's
going to be doing repeatedly so it wants
to have a policy for choosing one server
every time it asks for this service and
minimizing the average response time at
experiences so I became aware of this
type of problem in distributed systems
when I was working for Akamai the
Internet content delivery company across
the street from us which is in the
enviable position of being able to be
simultaneously toot and puddle this the
Akamai mapping system which is toot and
is continually measuring the Internet's
and finding out what's out there and
finding out about new properties of
connectivity that were not the case one
minute ago and then there are the users
who get to be puddle and whenever they
make a query they instantly get routed
to what the model currently thinks is
the best server for them but you know my
laptop does not have the luxury of
operating a network of a hundred
thousand servers distributed worldwide
that can be constantly measuring things
instead it has to run crude algorithms
like this so here on the right side of
the screen is what goes on when my
computer looks up whitehouse.gov it
finds out that there are seven
authoritative nameservers for that name
it has to ask one of them okay what's
the address of whitehouse.gov
and a commonly used algorithm is you
keep a vector of scores for each server
whenever you have a query you ask the
server with the minimum score you
measure how long it takes to respond in
milliseconds increment the score by that
number of milliseconds and crucially
decrement everybody else's score by one
millisecond so what is this algorithm
doing it's doing some crude trade-off
between exploration and exploitation
because you know if one of them is five
milliseconds away and another is 100
milliseconds away then I'm almost always
going back to the one that's five
milliseconds away but every so often the
decrementing the counter for the other
one means that I'll try it out and I'll
find out if in fact that server is only
one millisecond away from me and the
hundred was an outlier measurement that
just
and sometime in the past because
whatever there was congestion on the
line and it took a while to respond to
me okay so this is a crude algorithm for
doing both exploration and exploitation
in the context of looking up servers
once you become aware of this issue you
see it all over the place in computer
systems so for example what does Amazon
need to do when they serve up their
front page to you they they actually
face a lot of Explorer exploit dilemmas
just in creating this page they have to
decide which products they're going to
advertise to you they have a choice of
whether to display the Columbia men's
double-barrel camouflage handbag there
or some other product for that matter
who says the price has to be $29.99
maybe they could make more money selling
it to you if they said the price was
$34.99 why don't they explore other
prices a little bit and so almost
everybody operating an ongoing service
in the world of computers now needs to
be running an algorithm that figures out
how it's going to do this kind of
experimentation to improve its service
and keep track of changing conditions
over time and this is a very rudimentary
algorithm for which I was able to give a
hand waving justification but I haven't
even told you a mathematical model yet
so we don't even have a framework for
asking a question like is it the best
algorithm well it turns out that at
least one version of that framework
existed decades ago around the time of
the very first computers or even a bit
earlier that is the multi-armed bandit
problem in general terms a multi-armed
bandit problem concerns a decision maker
who in this talk I will often refer to
as a gambler repeatedly choosing one of
n actions which in this talk I will
frequently call arms and the metaphor
comes from array of slot machines each
of which has one arm you can pull a slot
machine is called by some people have
one
and it because it takes your money away
from you as surely as a bandit does and
every time step the gambler picks one of
these actions it produces a random
payoff from some distribution that's not
known to the gambler you get to observe
that history of payoffs and the goal is
to design a policy which overtime
maximizes the expected total payoff
so that is a sketchy definition of the
multi-armed bandit problem I'll be more
formal about it later but first let me
delve a bit into the history of this
problem so I like this quotation from
Peter Whittle who was one of the early
theorists of multi armed bandits in the
decision sciences so it says the
multi-armed bandit problem was so
baffling to allied scientists during
World War two that they made the joking
suggestion that it the multi-armed
bandit problem itself should be dropped
over Germany as the ultimate instrument
of intellectual sabotage interestingly
this quotation leaves out any mention of
what the heck the Allied scientists were
doing with this problem during World War
two and I will follow in that tradition
by also leaving out any mention of that
during this talk multi-armed bandit
algorithms were also proposed a little
bit after for medical applications so
there instead of traditional randomized
clinical trials where you pre partition
the subjects into groups of predefined
sizes which will be given different
treatments you know here you make a
sequence of decisions where initially
some small subset of your subjects are
given the blue pill the red pill the
green pill and afterward when you know
something about the effectiveness of
those three treatments you may choose to
adjust the numbers of people who are
given each of the treatments in the next
round of the trial in some cases may be
eliminating one treatment altogether
from the trial because you have
accumulated enough statistical evidence
that it's known to be in
effective and it would be considered
unethical to continue testing it on
patients interestingly although these
methods were proposed as far back as the
1950s it took on the order of 30 years
before they were actually adopted in
clinical trials and it's still not the
case that the standard practice in
medical clinical trials is to do
adaptive experimentation using something
like a multi unbanded algorithm I
imposing on this slide the question why
and promising to return to that although
I say that a little bit sheepishly with
some awareness that there are probably
people in this room who know more about
the subject than me and could provide
better answers than I will provide I'll
attempt to say something on that subject
anyway but you know nowadays multi-armed
bandit SAR absolutely everywhere
and unlike the 1950s when these things
were being proposed but took 30 years to
be adopted now if you want to run a
multi-armed bandit experiment on your
website to see whether it's better to
display the ad in the top left top right
or lower right corner you know you just
have to go to google and press a button
and they'll start running the experiment
for you it's extremely easy to apply
these algorithms and they get applied
for all sorts of things on the web ad
placement price experimentation they get
applied in crowdsourcing search and many
other applications that I'm not listing
here and the theme of this talk will be
to bring forward some of the
mathematical challenges that come up
when you try to apply these algorithms
in these domains to themes that will
occupy this talk are the loss of
autonomy that results when the platform
is not directly doing the experiments
itself but instead calling on users to
autonomously decide to pull arms and
report what they see and also the supply
and
limits that are kind of ubiquitous in
the types of applications I was talking
about with Amazon for example where as
you're performing experiments you're
consuming resources and when they run
out the learning process has to end okay
so this this multi-armed bandit problem
that was so baffling that the allied
scientists didn't know what to do with
it except potentially inflicted on the
Germans how was it finally conquered it
was conquered by John Gibbons who
developed a beautiful type of solution
called the Gittens
index policy that in computer science
terms we would say was a reduction from
the many armed bandit problem' to a
modified one-armed bandit problem
so what geddens looked at is a version
of the multi-armed bandit where you have
your n arms each of which has some
unobservable random property that i'll
call its type which completely
determines the payoff distribution that
you see every time you pull it and the
gambler has some prior belief about the
types which is any you know it's a
distribution that's independent across
arms that's the only assumption that
we'll make and the type of average
payoff that this person wants to
optimize is a geometrically time
discounted sum with some discount rate
gamma okay and what I mean when I say
that Gittins reduced this to a question
about a single arm is he said here's how
you should decide which of the n arms to
pull at any point in time consider
instead a decision problem where you
only have two alternatives pull arm one
or retire from the game take a one-time
payoff of size X and the entire process
ends okay and okay so in this game we're
going to set it up so that you can pull
arm one as many times as you want at any
time you can opt
get your retirement reward of size X
it'll be scaled down by gamma to the T
if you retire at time T and you know
okay if X equals zero the retirement
reward is useless and you should always
pull arm 1 and if X is sufficiently
large effectively infinity then
retirement is so attractive that you
would never pull arm 1 you would just
immediately retire and as I increase the
value of X there's some thresholds where
the gambler becomes indifferent between
retiring immediately or pulling the arm
wants to see what happens and then
revisiting the decision whether to
retire that threshold value is called
the Gittens index of arm 1 and what
gettin's showed is that if you now have
a problem with any number of arms to
solve it all you have to do is compute
the index for each one of them and
always pick the arm with the highest
index it'll produce some payoff now you
have more information about that arm so
you have to recompute its index but
that's all you do you just pull an arm
recompute its index again maximize the
index pull that one recompute it doesn't
say anything about whether the gettin's
index of one single arm is easy or hard
to compute but it does say that the
complexity of the entire problem boils
down to that one computation that you
have to do and it also provides us I
think with a sort of beautiful
structural insight into how to quantify
the trade-off between exploration and
exploitation which is you know sometimes
you have to do something non myopic and
try an alternative other than the one
that you currently believe to be the
best and the amount of boosts that you
have to give to the suboptimal
alternatives about what you would like
to learn more is completely
characterized by this retirement reward
notion ok so that's why this is a justly
famous result there's one other
development in the history of these
problems that I have to tell you about
before I can get down into the thick of
telling you about the research that I
and my co-authors did and that is
a different model of the problem
introduced by Lyon Robbins they were
trying to contend with the question of
kittens needs to assume the gambler
knows so much it has a complete prior
about every arm that it might pull what
if you just know that you have a bunch
of arms each of them has some payoff
distribution but you know nothing at all
about that distribution except maybe
that it's normalized so that the payoffs
are always between 0 &amp;amp; 1
what can you say okay so on the slide a
little bit in accurately I read no
probabilistic assumption about arm types
they make the probabilistic assumption
that for any arm it produces rewards
that are iid over time other than that
no assumptions ok yeah you're right
you're right the boundedness isn't it is
an extra assumption they don't need to
assume independence so you get a
two-pole from the arms which is
independent across time so now the
question of independence across arms
never even comes up in this problem
because if you don't pull an arm it's as
if it didn't produce a payoff so it's
almost immaterial whether the model
defines the payoff to exist at a time
when you don't observe the arm
okay so in this model where you're not
making any assumption about it arm types
it's actually quite subtle to try and
capture what it means for an algorithm
to be optimal
well the optimal algorithm is the one
that has the highest expected payoff
well yeah but we can't compute expected
payoffs if we're not assuming a
distribution on the arms okay so they
did something quite ingenious they said
we're instead of asking what's the
optimal algorithm we're going to ask for
a comparison between two parties an
uninformed gambler that is an algorithm
that doesn't know what distributions
underlie these arms and just has to find
out by experimenting with them and an
informed gambler who knows for every arm
what payoff distribution is going to
produce and plays optimally and just to
spoil the least interesting part of the
mystery for you if you knew the payoff
distribution of every arm playing
optimally means you compute the expected
payoff for every arm one of them has the
highest expected payoff you always pull
that one
so the informed gambler has a very dull
strategy in this game and they chose to
frame the question in terms of how much
worse is it to be an uninformed gambler
and do learning versus being an informed
one who's omniscient about the
distributions at time zero the
difference in payoff between those two
parties they called regret and they
supplied an algorithm whose regret at
time T grows only logarithmically in T
just striking result because you know in
particular the informed gambler knows
strictly more information than the party
playing the gettin's index policy in
Gittens result on the last slide and so
you know they're uninformed algorithm
which has log t regret against a gambler
that's omniscient about the
distributions it's also doing within an
additive log t of the payoff that the
gettin's index policy would get even
though it knows nothing about the prior
so in some sense this result is saying
you
that getting's depended heavily on the
prior but if you're just willing to
tolerate a small additive loss you don't
need to know the prior at all
what is this upper confidence bound
algorithm I'm not going to write it down
for you but it's based on a very simple
intuition for every algorithm you make
the most optimistic possible guess about
its average payoff that is consistent
statistically consistent with the
evidence you've seen so far that is to
say you plot a confidence interval such
that you can say with very high
probability that the actual average
payoff lies in that interval and then
you optimistically guess that the upper
endpoint of that interval is the true
answer you then always play the arm with
the highest upper confidence estimate
and then you know writing down and
analyzing the algorithm boils down to
figuring out exactly how wide a
confidence interval is statistically
justified and that's an easy problem
that was solved long before these people
Nicole did you have your hand raised you
mean is it also behind by a log T I've
never gotten that question and I have
never asked myself
it's not easy enough for me to answer on
the flight so I'm just gonna say great
question gamblers you know Pryor yes all
right
that's right yeah I I suspect that the
log t gap exists even if you're
comparing kittens with the informed
gambler but it's a great question and
I'm curious now to the Lord T is an
asymptotic result yeah I was going to
delay this until later in the talk but
that's a great question there's a worst
case found that this algorithm with n
arms does not have regret worse than
constant times root TN log in at time T
it's also the case that for any profile
of arm types as you send T to infinity
the actual growth rate of regret is only
order log T but the inside that big o
there's a constant that depends on the
properties of the particular profile of
arm types on which you're running the
algorithm so the hard instances of
multi-armed bandit have a very small gap
between the best at second best and
ironically although that means that
there's a suboptimal alternative which
is close to the optimum so in some sense
it's easier to find things that are near
the optimum it's so expensive to
discriminate between the optimal and
suboptimal one that in the short run you
end up accumulating it's sorry and look
in the long run you end up accumulating
a lot of regret trying to do that
discrimination yeah so the actual bound
is like log T over epsilon squared where
epsilon is the gap between the best and
second best on
yes distributions how different is by
Robins
it also depends on the discount right so
for reasonable distributions and very
slow discounting they differ in their
earlier in their early decisions they
say after you've played every arm a few
times your priors become sharp enough
that there's almost only one sensible
thing to do and both of these algorithms
do it and then the log T sort of comes
from the fact that Lai Robbins roughly
every time T is a power of two goes off
and does something crazy just to make
sure it's getting data about the other
arm whereas Gittens
exhibits incomplete learning in the long
run where you know arm a may be better
than arm B and Gittens might end up
playing arm B forever
because of some initial data it sees
that sent it off in the wrong direction
so even sample path of live Robin
sources kittens could be quite different
because of this phenomenon to be
incomplete learning but you know sort of
their average pay their average behavior
over many sample paths will look quite
similar after the initial time slice
okay um oh you had a question
infinite regret it has to do with the
fact that lion robins are computing and
undiscounted regrets and kittens is
computing geometrically time discounted
so what happens is kittens can run off
and forever play the wrong alternative
but the sum of regrets that it
experiences when doing that adds up to a
finite sum because it's few metrics
County you can send gamma to one but I
don't think the the sequence of kittens
index policies converges to a
well-defined policy as you do that ok so
I promised you a couple speculations on
why it took so long to adopt bandit
algorithms and medical trials and why
the adoption is still incomplete I've
presented you with sort of the two main
families of bandit algorithms the
kittens index policies and the upper
confidence bound ones and kittens index
policies are beautiful but fragile so it
depends on every assumption that I made
on that slide the geometric discounting
instead of some other discounting scheme
independent priors over arms in practice
people doing a medical test probably
don't have precise priors about the
success probabilities of their
treatments if they did they would
probably be correlated they also don't
get instantaneous feedback in the
multi-armed bandit problem it's
important that you pull an arm of the
slot machine you instantly see its
payoff in a medical trial it could be
months or longer and in medicine the
objective is not to maximize the
discounted number of successes the
objective is somehow to advance science
and eventually make medical discoveries
that lead to life-saving breakthroughs
and so for all these reasons it's not
obvious that you would want to apply
that getting's index policy
is there any reason to believe that any
of those objections that you said bring
us any closer to anything that they're
currently doing rather than even further
away from what they're currently doing
in the direction that this suggests you
were one of the people in the room who I
anticipated might have more nuance to
answer to this question than me and now
I'm feeling very on the spot I'm not
sure I got your question is there any
reason you said to believe that these
objections if overcome no no no it is
there any reason to believe that these
objections move you towards what is
current practice relative to what is
suggested by Giddens
rather than moving you even further from
current practice then is suggested by
kittens in other words like well I don't
know so so so let's talk about delayed
observations if we're gonna run a trial
and I decide beforehand that 500 people
are gonna get treatment a and 500 you're
gonna get treatment B that completely
removes the problem of delayed
observations because I've already
predetermined who's going to get which
treatment I don't have to wait until I
see any observations correlated success
probability in the in the really
stereotyped case where I every time I
run a trial with two alternatives I'm
gonna pre partition into fifty-fifty
groups that also takes away any have any
having to reason about priors because I
hate I already know I'm gonna do 50/50
because that's what the handbook tells
me to do so now
I thought your question was going to say
do you have any reason to believe if
these four objections could be overcome
that would bring us any closer to
bandits being used in medicine I think I
have to stay mute on that subject there
are lots of other things like privacy
concerns and also just the fact that
it's not as if medicine
is you know a single organized entity
like Microsoft where you can say from
the top down we're going to do things
this way from now on
but okay in the interest of getting to
the remaining bullets on my slide you
don't want your clinicians to know how
the trials are coming out during the
time because they may leak that to the
patients and make the placebo effect
worse and so something that completely
secret has another advantage whereas
giddons requires somebody to know yep
that's a very good point yeah in fact I
guess we just learned about that one on
Monday at the workshop on
experimentation in the web okay so we
also have this other class of algorithms
UCB that needs to make much fewer
probabilistic assumptions and in that
sense is more flexible but as I said in
answering the question that came from
that corner of the room those policies
suffer from slow convergence so they're
more suitable for experiments where
you're doing a million trials let's say
rather than a thousand so on the web
oftentimes these objections go away it's
very easy to run an experiment at N
equals 1 million you don't have humans
in the loop making the decisions so the
sorts of worries that Preston for
example just expressed about humans
deviating from the protocol once they
know the outcome of earlier experiments
go away although hold on to that thought
because it will return when I get to
talking about the first paper in a
second um also the on the web the stakes
are much lower than in World War two or
in the practice of doing medicine if you
make a mistake in what price you quote
to a user on Amazon nobody dies okay so
in fact I think you know the the
circumstances are right in our
generation for these algorithms coming
widespread use and in fact that seems to
be happening and this talk is going to
be about two pieces of research about
what comes up when these algorithms do
start coming into widespread use okay so
the first one is on a paper called
incentivized exploration I have your
picture of Ferdinand and Isabella giving
Christopher Columbus his incentive for
exploring North America but unlike in
the days when monarchs could just order
people to go on voyages of discovery for
us nowadays we have many platforms that
want users to do the equivalent of
voyages of discovery for them but they
don't have monarchical power over their
users so Amazon for example depends on
you to review products so that it can
provide good recommendations to other
users it's therefore in Amazon's
interest for its users to provide
reviews on a diverse set of products but
any individual user just cares about
getting the product that's right for
them and you see this all over the
social news reader sites also depend on
you reviewing articles in order to make
good recommendations to other people
citizen science platforms like Galaxy
Zoo or eBird depends on a user base that
is going out birding or looking at the
sky just for fun but their project only
succeeds if that user base collects a
diverse set of observations and you can
even see this phenomenon playing out in
the arena of funding for science where
you often hear things about the
difficulty of encouraging resource
researchers to do high-stakes
potentially transformative research and
how should an organization like the NSF
provide the right incentives for them to
do that in all of these situations
what's happening is a case of misaligned
incentives that you have a principal
whose goal is to collect data about many
alternatives and you have users who each
individually just want to select
what they perceive to be the best
alternative or in even pithy er terms
you have a principal whose goal is to
exploit and use a principal who wants to
explore and users who want to exploit
and the question is how much efficiency
loss results from that misalignment of
incentives or how to bring them more in
line with each other
okay given the preamble of this talk
it's natural that this problem can be
modeled as a multi-armed bandit and so
you know we have k arms with independent
random types and you know the only thing
that changes from the multi-armed bandit
problem I presented to you before is
that for every time step we have a
separate user user t participates only
in time step T chooses an RMIT and sees
a reward R sub T we're gonna assume
history is fully observable so user t
sees everything that happened in rounds
one through t minus one before making
its selection and we're going to assume
that everybody has a common prior okay
so you know in this model the principal
is trying to do exactly what the kittens
index policy wants to do maximize
discounted rewards and a user just wants
the user who participates at time T just
wants to maximize that component of this
information that would allow you to
update the prior about the nature of the
kittens index
yes you're asking if users have private
signals in addition to the common fryer
no no there are no private signals in
this model
everybody's initialized to a common
fryer and they share a common history so
they always have common posteriors so
the the only reason for treating the
users as distinct at all is to remove
the intertemporal incentives between
user s and user t for a different times
it just might be kind of interesting if
there was some way you were trying to
exploit the private signals of the
agents as well ok so right in this model
the principle has some optimal policy
the users left to their own devices
would pursue a myopic policy we've
already talked about the optimal policy
it's given by the Gittins index the
myopic policy is simply at any time step
T everybody has a common posterior
there's one arm which has the highest
expected reward under that posterior and
that's the one that would be myopic we
pulled a first question you might ask is
how much worse is it to just play the
myopic policy than to follow the optimal
one and you know one of the things our
paper does is it gives a tight answer to
that question so the myopic policy
always gets at least one minus gamma
fraction of the optimal policy and this
bound is tight that means that if the
principle is very patient so this
counting time very slowly gamma is close
to 1 and this is saying that the myopic
is greater than epsilon fraction of the
optimum but that's not a very compelling
guarantee and so if you want to do
better than the fully myopic policy in
one of these crowd-sourced investigation
platforms you have to provide the users
with some incentives to be non myopic
and our model is going to incorporate
that by saying that the principle can
post a vector of rewards at time T
saying if you choose alternative I at
time T in addition to whatever random
you get from pulling that arm I'm also
gonna pay you an extra see sub-ti and
that changes the incentives for the
agents so that now they're maximizing
expected reward plus this incentive
payment and you know crucially CIT is
under the control of the platform
designer so they can try to use those to
bring the users behavior closer in line
with optimal exploration at this point
it's relevant to go back into a sanity
check and ask whether this model has
anything to do with the applications
that I said we're motivating this
research so we're not envisioning that
Amazon would actually post a bounty you
know and say we're giving you this
reward you know we believe this digital
camera really sucks but if you buy it
we'll give you this reward for taking
one for the team but you know so in the
product reviewing context you could
imagine that the reward is silently
being applied as a discount so the user
just sees a lower price and doesn't know
that that price actually incorporates
this reward in other context like
citizen science the reward might be a
social psychological reward we're going
to give you a badge for being the first
person to explore this hitherto unseen
part of the sky or okay or in the
scientific funding context the reward
might be an implicit reward in the form
of a promise of greater funding
probabilities if you choose to work on
this problem where you have low
probability of success will incentivize
that by promising that a certain
proportion of our funds have been set
aside for funding exactly that type of
research okay so now that I've told you
our model of how the principal is going
to incentivize agents to choose arms I
can talk about the main question that we
sought to solve so if the principle
applies some incentive policy by
determining the payments CIT that it
offers at time T and the agents best
respond to that policy there will be
some expected time discounted reward
which will be some fraction of the
optimum call it one minus a and you know
think of a as like the opportunity costs
of not following the optimal policy and
the principal will have to pay out in
expectation some other amount B times
opt okay so these a and B reflect two
different types of costs that the
principal takes on one is the
opportunity cost of suboptimal
exploration and the other is the
incentive cost of bringing people's
behavior closer in line with what would
optimally be done and okay we'll say
that policy PI achieves loss pair a B
and then the crucial definition a pair a
B is achievable if for every instance of
the multi-armed bandit problem there
always exists a policy achieving lost
pair a comma B okay so now the main
question should be clear we want to know
what is the set of achievable loss pairs
and one thing that I find charming about
this question is it actually lets you
plot in a visual way the trade-off
between exploration and exploitation so
I I've been working on multi-armed
bandit problems for a long time my PhD
thesis was on multi-armed bandit
problems I cannot tell you how many
times I've given a talk where in the
first few slides I said the multi-armed
bandit model is all about the trade-off
between exploration and exploitation but
if someone had confronted me and said oh
there's a trade-off can you plot the
curve of you know how much more
exploration you get to do if you do less
exploitation that not only would I not
know the shape of the curve but I
wouldn't even know what the question was
asking but this is a model where you can
very cleanly say you know you have to
pay people this much to explore and this
is how much you lost because they were
exploiting and okay so once I realized
that this curve is in pictorial form the
exploration exploitation trade off I was
come first very curious to know its
shape and our paper answers that
question so loss pair is achievable if
and only if the sum of the square roots
of a
and B is greater than or equal to the
square root of gamma well it's
interesting that it's such a simple
equation but let me try to highlight you
know some of the more enlightening
qualitative features here so the achieve
okay
this bullet consists of things that you
would expect if you just thought about
the problem for a second so like if a be
an a prime B prime are both achievable
then any point on the line segment
between them is also achievable because
I could to get the midpoint for example
I could just toss a fair coin at the
beginning and decide if I'm gonna run a
policy achieving a B or one that
achieves a prime B prime okay so we knew
that the achievable region was going to
be convex we knew it was going to be
upward monotone because you could always
just pay people more so it you know if
any point is in the achievable region I
can pay them more and get some point
that lies directly above it it's
intuitive that it is set why it's
decreasing in the discount factor so as
the discount rate gamma gets closer to
one and the principle becomes more
patient the misalignment of incentives
between the principal and agents becomes
more severe and so you would expect
fewer points to be achievable it's
satisfying to see that the theorem bears
out that intuition and I think the most
interesting aspect that was certainly
not obvious to us at the start before we
derive this theorem is there are some
pairs that are achievable
no matter the discount factor so you you
can make it infinitesimally close to one
you can make the principal almost
infinitely patient and still there's a
policy where you get at least
three-quarters of the payoff of the
optimal policy while giving back only
25% of the optimal reward to the users
in the form of incentive payments
similarly if you want to get 90% of the
optimal policy you never have to pay
more than 50% of the optimal reward back
to them
okay so I now have a sequence of slides
talking about how we derived this result
let me see I'm going to have to skip
over some of them um to delineate the
achievable region exactly you have to do
two things you have to show that every
point in it is achievable and every
point outside of it is unachievable okay
the easier one is the unachievable 'ti
there's a hard type of instance for this
problem that we call diamonds in the
rough where there's a very large
effectively infinite number of arms that
you just have to observe them once and
as the first time you look at them you
know what their payoff is we call those
collapsing arms and each of them
produces a gigantic effectively infinite
payoff but with a tiny probability
otherwise payoff 0 there's also one
option that's a sure thing that produces
a payoff of fee and now as you vary this
parameter fee you get a one parameter
family of instances that are hard
because there's the safe option that the
users would like to always pick and
there's the unsafe option that has is
this lottery ticket that has a tiny
probability of paying off but you really
want to get them to do that because in
the long run it's going to be worth more
to the principle okay for each of these
diamond-in-the-rough instances the two
extreme points of the achievable region
correspond to an optimal policy that
always picks one of the risky
alternatives until the high payoff is
found and a myopic policy that always
plays the safe arm so for any value of
fee you get an achievable region that
looks like this polygon everything in
the white triangle is therefore
unachievable and as I move fee around
the union of these white triangles
completely maps out the compliment of
the purple region so now we're just left
with showing that every point in the
purple region is achievable that's where
it gets hard and the basic idea is to
use the separating hyperplane theorem so
a point a B which is in the purple
region and I want to reason by
contradiction suppose the achievable
region instead where this yellow one
that does not contain that point because
we know its convex there must be a
separating hyperplane a line that goes
through a B but lies completely outside
of the achievable region okay so so now
that line has some slope negative lambda
that can be used to make a one parameter
okay so right the I've stated
incentivized exploration as a Pareto
optimization problem you have these two
numbers a and B and you're trying to
make both of them small okay but the
benefit of focusing on this line of
slope lambda is it gives me an exchange
rate for converting opportunity costs
and incentive costs into a single number
that an algorithm can try to optimize
and so you know the main part of an hour
analysis then goes into figuring out for
a given value of lambda what's the
largest value that we can guarantee a
policy can achieve for this mixed sign
objective function of payoff minus
lambda times cost all right and our
paper defines something called a time
expanded policy that provably achieves
the value that you would achieve if you
were facing the diamond-in-the-rough
instance okay so in conclusion in this
part of the talk I presented yeah I
presented this problem of its end
devising exploration that asked how
cheaply we can incentivize selfish
myopic users to explore a set of
alternatives
the main result completely characterized
when you can say with certainty that a
certain opportunity cost is achievable
at a certain price and you know there
are many open question
from a practical standpoint one would
want to enhance our model with various
features like hard budget constraints
right now I say that a B is achievable
if there's a policy where the expected
amount that it pays out is the times the
optimum oftentimes the learner would
want to know that they never pay more
than B times the optimum on any sample
path of this learning process you know
very interesting question is about when
users are non myopic you know it's like
in the Amazon product reviewing setting
our model is making the unreasonable
assumption the people are going to use
the system exactly once contribute one
review and then go away forever and one
would like a model where users return in
the future and may make non myopic
decisions accounting for the future
state of the system when they are likely
to come back also I don't not from a
practical standpoint but just from an
intellectual curiosity standpoint the
answer to our question turned out to
have a spooky symmetry that if a comma B
is achievable then so is B comma a and
that symmetry shows up nowhere in the
proof that everything we do is
completely a symmetrical with respect to
exploring and exploiting and then at the
very end some cancellations happen and
we get an answer which is symmetric
under interchanging the two axes there's
something dissatisfying about that
because you know symmetries like this
usually don't just happen by accident so
be interesting to know if there's some
duality principle that says for every
multi-armed bandit instance there's a
dual instance that exchanges the costs
of exploring and exploiting okay but I
want to reserve five minutes for telling
you about this other paper bandits with
knapsacks that addresses a completely
different aspect of the multi-armed
bandit problem which is how to deal with
supply or budget constraints during the
learning process so you know typical
scenario that motivates this problem is
you're selling seats on an airplane
using price experimentation so maybe you
don't know how much this flight is worth
to people and you're going
to offer varying prices to users in an
effort to converge in on the profit
maximizing price for airplane seats ok
but this problem is not like the
ordinary multi-armed bandit because
every time someone accepts a price that
you offered one seat on the plane is now
reserved and when the number of seats
for sale drops down to zero the learning
process is over and you can't do any
more you see the same thing in
crowdsourcing where you may be using
Amazon Mechanical Turk to collect image
labels and you may have been given a
budget of size B for this project every
time you asked someone to provide label
for your image you have to decide how
much money to offer them but when your
budget is used up you can't ask for any
more labels so you're just trying to
maximize the number of labels you get
before exhausting your budget okay so
there's a general model that encompasses
all of these problems and many more
where we have n arms as before we now
also have D supply limited resources and
every time you pull an arm now two
things happen you get a reward R which
is between 0 and 1 and you get some
random vector in d dimensions which says
how much of each resource was consumed
and the learner has to stop when some
resource is fully consumed ok so you
could plot the learning process in some
you know in d dimensions maybe there are
two dimensions the money spent and the
time until the experiment ends and any
sample path I'm not showing rewards but
I'm showing resource consumption it adds
up and when we hit the boundary of this
region that means that we've exhausted
one resource in this case money and the
process is over ok and it's easy to
represent you know like the Mechanical
Turk example or the airline seats
example in this model and because we
allow for a multi-dimensional resource
constraint you can actually represent
much more such as selling seats
multi-hop itineraries where there's many
planes and any given user you know you
may have you know a thousand planes that
you're operating so you're in a thousand
dimensions any given user may use up
three of those dimensions at once
or you could have you know a web
advertising context where you have a
thousand advertisers and every time you
decide to place an ad you use up some of
that advertisers budget but there's
actually a thousand separate budgets one
for each advertiser okay one of the
things that makes this problem hard is
that unlike the ly and Robin setting
where the ideal was to figure out the
best single arm and always pull it in
these problems the best thing to do
might be to pull a mixture of arms you
can even see that in this
two-dimensional picture if you have one
arm which shoots off a long array that
hits this wall and another arm who's
expected resource consumption and shoots
off a long array and it's the ceiling it
might be better to do a mixture of those
two arms that takes aim at that corner
and thereby gets to keep playing for a
longer amount of time before exhausting
any resource okay so you see defining
regret in relation to the best fixed arm
is actually the wrong thing to do for
this problem and I'll spare you this
entire slide the right thing to do is to
define regret in relation to the
informed gambler someone who knows the
distribution of rewards and resource
consumptions for every arm and always
picks you know the optimal solution of
the gigantic dynamic programming problem
defined by that set of distributions you
know your state is whatever remaining
supply you have of each resource and in
principle you could solve an
exponentially large dynamic program and
figure out that optimal policy that's
the informed gambler and you want to do
nearly as well so our main result is
that there is an algorithm who's regret
is small with respect to the informed
gambler it's small to the extent that
the fraction of the optimum payoff that
you must sacrifice is bounded by this
expression
which tends to zero as you hold the
number of arms fixed and send both the
budget and the value of the optimum
payoff to infinity it's intuitive that
you need both of those to be
asymptotically large because if you have
tiny budgets then it could be that in a
single trial you consume all or almost
all of your budget of one resource and
then you never have time to do learning
and similarly if the optimum payoff is
tiny then getting the optimum payoff
depends on what happens in just a very
small number of rounds and the learner
has no time to do learning before that
number of rounds elapses although its
analysis is difficult our algorithm has
a very simple structure it's always
pulling an arm to maximize a
cost-benefit ratio so now that you have
multiple resources you don't just want
to pick the arm that gives you the
greatest reward you want to get pick the
arm that gives you the most bang for the
buck given how much resource you expect
it to consume okay the difficulty is
since we have many resources the cost is
not easily expressed as a single scalar
and yet you want to be picking an arm to
maximize the ratio of benefit to cost so
what our algorithm does is it
simultaneously tries to learn two things
the best arm to play and also a vector
of exchange rates P that tells you how
you should be trading off consuming the
different resources against each other
okay and the way that's done is we
estimate rewards using upper confidence
bounds and resource consumption using
lower confidence bounds in each case
that's the most optimistic estimate you
could make about that parameter so it's
in keeping with the live Robins
philosophy we pull an arm that minimizes
the estimated or maximizes the estimated
ratio of benefit to cost is actually the
easier way to think about it and then we
update our vector of exchange rates
using a multiplicative update that'll be
familiar to any algorithm theorist who
has used algorithms in this family
before it's a familiar update rule for
approximately solving fractional packing
problems in an iterative manner okay so
the intuition for the algorithm is LP
duality says there exists a price factor
session if you just knew the right
vector of exchange rates the optimum
policy would only play cost-benefit
minimizing arms and would fully consume
every resource with positive price what
we're doing instead is we're
simultaneously running two algorithms
interleaved a learning algorithm that
uses multiplicative weights to try
learning the vector of exchange rates
and then a youcb algorithm that is doing
something like live robins given current
estimates and the difficulty in the
analysis is from analyzing the feedback
loop between those two algorithms and
showing that errors rather than
accumulating tend to shrink to 0 over
time fast enough that we get the regret
found in that main theorem okay I have a
slide about subsequent work I'm not
going to tell you in less than a year
since our paper came out two very nice
follow-up papers have come out and since
this is the Microsoft Research
colloquium I feel I ought to at least
advertise the two papers for you because
of the five combined authors all of them
are at Microsoft Research in one
location or another
Ashwin was an intern at Microsoft New
York City where these other two authors
are cheaper anna kiel are at Microsoft
Bangalore and Redmond respectively so
this is a problem that has attracted
great interest and follow-up work within
Microsoft Research and these two papers
taken together really generalize our
results in exciting new directions and I
was very glad to see other people
picking up on this problem and improving
our algorithms in various ways so in
conclusion I hope I've convinced you
that the web has created a fertile
environment for doing high throughput
low stakes algorithmically mediated
sequential experiments and that this is
creating exactly the right conditions
for these algorithms that have been a
wellspring of great theory since my
grandparents generation to finally start
getting widespread use in practice in
this talk I have told you about research
on two of the issues that I think will
come up as they start to be applied in
practice and if you were to ask me to
summarize what lies ahead in the form of
a one-sentence grand challenge I would
the multi-armed bandit provides a great
theory of explore exploit trade-offs in
stateless environments where what you
did in the past may influence your state
of knowledge but doesn't constrain your
future decisions or influence your
future payoffs at all
so that very cleanly separates learning
from all of the other you know sort of
past future dependencies that happen in
online algorithms but now that we
actually want to apply these things the
interesting challenge is lie where you
have problems that are stateful these
two examples actually fit that mold so
in the bandhas with knapsacks problem
you have a state vector which is your
remaining resource endowment and that is
you know some form of weak state
dependence between the past and it's a
way that the past is constraining your
future decisions and incentive ice
exploration the past is also
constraining the future payoffs that you
have to give people if in the past
something happens that revealed one arm
to be myopically optimal now you're
going to have to pay people more in the
present to get them to try other
alternatives okay so you know both of
these papers illustrate that it might be
possible to make progress on this grand
challenge I think now is the right time
to be trying thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>