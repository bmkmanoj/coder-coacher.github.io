<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>(Re)Designing Storage Systems for Fast Non-Volatile Memories | Coder Coacher - Coaching Coders</title><meta content="(Re)Designing Storage Systems for Fast Non-Volatile Memories - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>(Re)Designing Storage Systems for Fast Non-Volatile Memories</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3hrHGV8KYjs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright so it's my pleasure to welcome
Adrian Caulfield Adrian got his
bachelor's degree here in Seattle at the
u-dub and he's getting his PhD from UC
San Diego advised by Stephen Swanson and
he'll be talking about designing or
redesigning storage systems for fast
non-volatile memories thank you for the
introduction and thank you for inviting
me up to interview here at MSR so over
the last few years in the non-volatile
systems lab we've been working on
integrating emerging non-volatile memory
technologies like phase change memory
and spin torque transfer memories into
systems and what we found is that as
these technologies drive Layton sees
from seven milliseconds or so like we
would have with disk drives down to a
couple of microseconds the software
overheads that we experience are
actually skyrocketing so we go from
about 1% software overhead going through
the kernel stack all the way up to about
ninety seven percent with these very
fast non-volatile memories and so I'd
like to first set the stage a little bit
with some sort of background on where
storage systems are going why this is an
interesting problem and then I'll walk
through a couple of different iterations
of the prototype SSD storage system
moneda that we've created and then I'll
talk a little bit about what direction I
think some storage research should be
going and some future ideas that we
could look at so we're really living in
the data age now the world's collecting
data an astounding rate just give you an
example of how this is going in 2008 we
processed about 90 bytes of data so this
is 10 to the 21st is a phenomenally
large number and we have huge scientific
applications that are generating data
very fast rates Large Hadron Collider
for example generates terabytes of data
with every experiment they run large
astronomical surveys are you know doing
nightly sky surveys that generate
petabytes of data and we have to be able
to process this information websites
like being goo
youtube facebook are all collecting lots
of user-generated content as well as
large indexes of the web and so we
really need to be able to start
extracting a lot of knowledge from this
data that we're collecting and it turns
out that storage performance is one of
the major bottlenecks holding us back
from being able to do this but new
storage technologies like phase change
memory and spin torque transfer memories
can actually help solve this problem as
long as we're careful to not squander
the performance that they offer so if we
look at the trends in storage
technologies over the last couple of
years starting with hard disk drives
this is these numbers are for an array
of four hard disks but we can get Layton
sees of around seven milliseconds random
access bandwidth reading four kilobytes
of data time gives us bandwidth around
two-and-a-half megabytes a second and
this was sort of the case for the last
four decades or so up until about 2007
which saw the introduction of flash
based PCI Express SSDs and these devices
significantly decreased Layton sees to
around sixty eight microseconds they've
increased bandwidth significantly to
about 250 megabytes a second so it's
about a hundred x improvement sort of
overnight from what we had with hard
disk drives and if we continue down this
road devices like I'll talk about today
might be commercially available around
2016 have Layton sees of around 11
microseconds bandwidth goes up to about
one point seven gigabytes a second
mostly constrained by the interconnect
that we're using and so we can get 650 X
improvements for both of these latency
and bandwidth
yeah so this is an SSD attached to the
pci express bus so think fusion-io or
something like that this is a little
misleading because if you're really
doing big data you're not being a 4k
time and long sequential reads or writes
there's only about it 2x difference
between this guy in archive sure so for
sequential access is the the gap is
smaller but these new technologies are
still going to give us significant
improvements but you know for for random
accesses which a lot of applications
require especially if you have very
large data sets that you need to sort of
query and pull various bits of
information out of you know these
numbers are certainly things that we've
measured in the labs with workloads that
we have so depending on your workload
they're going to change so if we do the
math here between 2007 and 2016 it works
out to about 2x a year in terms of
performance improvements for both
latency and bandwidth and so this kind
of scaling is actually better than what
we've seen for Moore's Law with cpu
performance improvements during its peak
so the types of memories that I'm
talking about are faster than flash
non-volatile memories these are devices
that have interfaces that are as fast as
dram or nearly so maybe a factor of two
or three off sure this all seems great
there must be some downside to all this
right
what is it so I'm going to get to that
but one of the big downsides here is
that software overheads are actually
going to limit the performance that we
can get from these devices unless we do
something about it and so I'm going to
walk through some of the ways that we've
been able to tackle that problem with
our prototype SSDs so the the memory
technologies that I'm looking at are
things that are as fast as DRAM there as
dense as flash memory or they will be
soon they're non-volatile they're
reliable and they have fairly simple
management requirements we don't need a
large thick management layer like you
would with flash memory or something
like that so phase change memory spin
torque em Rams and the memristor are all
pretty good examples of the kinds of
memory technologies that we're looking
at and at least one of these is going to
be commercially available within a few
years at the sort of performance
characteristics that we're looking at so
but the the challenge here is that the
relative cost of the software overheads
that we have on top of these devices are
actually staying roughly constant or at
least they are at the moment so this
graph on the y-axis shows you latency on
a log scale in microseconds and then
four disks flash and our fast
non-volatile memories we've broken down
Layton sees into file system overheads
operating system overheads if we wanted
to share these devices over a network
with something like I scuzzy that
overheads there and then the red bar
represents the actual hardware latency
for these devices and so as we go from
disk drives we have a situation where
the hardware overheads are about two
orders of magnitude larger than the
software overheads that we're
experiencing so something with like
flash where we're actually fairly well
balanced and when we get to fast
non-volatile memories the hardware
Layton sees are actually about two
orders of magnitude less than the
software overheads that we're
experiencing and so this works out to
something like four percent software
overhead for disk all the way up to
about ninety seven percent software
overhead for our fast non-volatile
memories as I schism
it does but if you even if you remove
the eyes Ghazi the the software heads
are still pretty high and it's also a
lot of the software limits the
parallelism that we can get from these
devices as well and so it you know the
performance is more than just the
latency numbers so to help us understand
this problem a little bit better we
built a system called minetta the sort
of cartoon representation of the
application stack looks like this
applications run up the top in user
space we have a file system linux ios
stack in the middle Monette a device
driver underneath that sort of knows how
to talk to our device and has some
optimizations included and then this all
runs on a stored of standard x86 64 bit
host machine and we have a PCI Express
connection connecting this device to the
host machine and a several banks of
non-volatile memory technology inside
minetta itself every visit yeah so it's
all an fpga-based prototype and all go
for what this looks like so the minetta
architecture is runs on an fpga board
and just to sort of explain this i'll
walk through a read request as it sort
of travels through the hardware so once
it's issued by the driver the request is
going to show up as a Pio right to a pci
express register it goes through a
virtualization component which allows us
to have the appearance of many
independent channels so that
applications can you talk to them once
it's gone through that we have a
permissions jacking block so that we can
actually verify that applications that
are issuing requests are allowed to make
the request that they're generating and
from there the request will get placed
into a queue once the space is available
in our scoreboard we'll issue will
allocate space there and allocate space
in our transfer buffers for the request
as well we can track 64 in flight
operations in the scoreboard at the same
time since this is a read request will
send out a message across our ring
network to one or more of the memory
controllers and they'll send the data
back to the transfer buffers once it's
there will issue a DMA request out to
the host machine and the data will show
up in the DMA buffer allocated by the
operating system and we can complete the
request there will set a bit in the
status registers and we can issue an
interrupt to notify the operating system
that the request has been completed so
it's a fairly straightforward
architecture we're trying to just move
request through this as fast as possible
and keep as many of the memory
controller as busy as we can so this
whole design runs on top of the b3 FPGA
board and so this is actually designed
by in part by microsoft research uses a
pci express 1.1 an 8x connection to the
host machine this gives us 2 gigabytes a
second of bandwidth in each direction
the whole thing runs at 250 megahertz
and we're actually using ddr2 memory to
emulate the the non-volatile memories
that we're looking at and we can adjust
the rass and cast delays and the
precharge Layton sees that the memory
controllers are inserting to sort of
match the PCM latency projections that
we want to match so we're using
projections from iska of 2009 which said
PCM Layton sees will be around 58
nanoseconds or 48 nanoseconds for reads
and 150 nanoseconds for rights and this
board actually has 10 Gigabit Ethernet
connectivity as well and we'll use that
to sort of look at how we can extend
storage out onto the network as well a
little bit later so as we've been
developing minetta we've come up with
sort of three principles to help us
guide the development of these hardware
devices and keep software overheads
manageable so the first is to reduce
software io overheads we want to get rid
of as much of the sort of existing io
stack as we can that's been optimized
heavily for disk drives over the last
four decades and then the second
principle is to refactor critical
software across the hardware and
operating system and user space so
things like the file system that we
can't get rid of we want to split those
across all of the layers of our i/o
stack and put them where they can be
executed most efficiently we also want
to exist recycle existing components so
that we can reduce the engineering costs
as well as make it easier to adopt the
systems that we're going to develop so
this graph on the right gives you some
idea of the latency reductions that
we've been able to
leave I'll go over a few of these in a
little bit more detail starting with the
reducing of software I overheads so the
first optimization that we made is
actually to completely remove the Linux
io scheduler component and so in Linux
you can have basically pluggable io
schedulers that will do things like
reordering requests as are issued to
disk drives to make accesses more
sequential turns out if you have very
fast random access storage that's not
really beneficial so you should remove
that so what we did first is set it to
the no ops scheduler and this
essentially takes in a request and
immediately issues it again but the
problem is even the no op scheduler puts
all of these requests into a single
queue and then they get issued by one
thread sequentially to the driver so
you've got a huge roadblock to
parallelism that exists here so as we if
we look at the graph on the right you'll
see a lot of graphs that look quite
similar to this throughout the talk the
y-axis is bandwidth I neither megabytes
a second sometimes it's gigabytes a
second later the x-axis is transfer
sizing kilobytes from 512 bytes up to
about 512 kilobytes and these are all
random accesses so in this case we're
doing reads the blue line represents the
performance of our system with the no op
scheduler in place and once we remove
that IO scheduler and allow much greater
levels of parallelism going into our
driver and issuing multiple requests at
the same time from a number of threads
our bandwidth obviously increases
significantly and that's what the red
line represents so we can actually chop
off about ten percent of the actual
latency of the i/o access and increase
our bandwidth substantially with this
optimization underneath what's happening
here that gives you that increases it
because the device is
of servicing multiple requests in
Maryland where is it this season yeah so
that's part of it so we could get
increased parallelism at the device
level and we're also able to essentially
reduce the number of contexts which that
are happening as we're issuing requests
so if I have you know eight threads
running in my application and they're
all issuing i/o requests with the
scheduler in place each of those
requests ends up inserting an element
into a queue and then a single kernel
level thread will pull items off of that
and issue them through the driver if you
remove the scheduler what ends up
happening is that thread will actually
go into the kernel and then the thread
itself will call a function in the
device driver and hand off the request
at that point so we actually get much
you know a lot more threads in the
kernel talking to the device driver able
to actually issue these requests so you
need the parallelism at the device level
but we also need to be able to get that
somehow by allowing multiple application
level threads to talk to it again
multiple-course your paralyzing the work
of talking to the device driver / course
yeah i mean that's certainly another way
so get a single core machine we're just
dude see the same video um you would see
some of it certainly the latency
reduction is still going to be there
when we do all these latency
measurements it's obviously with one
thread issuing you know a single stream
of accesses and that's because we're not
contact switching between threads to
issue the request so the application
thread is actually the one that talks to
the driver and issues request down to
the hardware
access transactions can the device
support so we in this part of the talk
there's basically 64 threads so we can
reissue tags we have 64 tags available
they're assigned in the driver to each
request as its issued and the hardware
can actually track 64 outstanding
requests at the same time as well later
on when we have a lot of applications
talking to it independently they each
have their own set of 64 tags and so
there's a queue in the hardware that
will basically get filled up by those
and then we can track 64 in-flight ones
at the same time okay so this is the
first optimization that we can make the
next is actually selectively using spin
waiting for smaller requests so what we
found is that with very fast storage
devices it actually makes sense to
hijack the thread that's issuing that
request and sit in the kernel spinning
and waiting for notification that the
request is completed and it for our
device this works out to be about 4
kilobytes in size so anything smaller
than that it's actually more expensive
to switch to another thread try and
start doing some work and then have to
switch back to complete the request at a
later time and so we've set it up so
that for a small request will just spin
in the kernel for larger requests will
actually allow context which is to
happen so that more work can get done
and so this change actually saves about
five microseconds of latency and you
know there's a little bit of an
efficiency trade-off here because we are
actually wasting CPU cycles waiting for
requested to complete but we're also
getting much better performance and so
maybe if you have one application that's
performance critical you'd rather have
the five microseconds then an extra
thread or something like that how much
is this how important is five
microseconds I mean you has ninety seven
percent over
software how much of that percentage is
going to account for uh I don't actually
have it I don't have that broken down
but it's take well yeah the base column
the software is only about fifty percent
the ninety-seven percent was with a much
bigger denominator and the dark blue bar
between 15 and 20 is the weight bar and
it disappears when a highlighted column
biography partisan years yeah so we've
gone from about 18 microseconds or so of
software latency down to you know 10 or
something like that with all three of
these optimizations together so a lot of
the time that were you know spinning we
would have been just contact switching
between different threads and so you
sort of weren't going to get that much
useful work done anyway certainly for
the very small request that's true but
as you the request get larger you sort
of start to see this trade-off so the
cutoff is around 4 4 kilobytes to
optimize for the late right not
optimized for the analogy PU overhead
presumably they cut offers of you all
right of these smaller um yeah I mean it
depends on whether you're actually like
it takes about two microseconds to do a
context switch so if you do that switch
you might get a micro second of useful
work done in your application before you
have to switch back so you know maybe
twenty percent that we clapping is cute
little wait for it so that's basically
that's the amount of time that we're
spending in the in the kernel waiting
for a request to finish reading
societies and send the application
if it requests a small the application
for it comes down issues it and expects
that the results of back sofa there's no
point you're giving up the CEO yeah
disable the septum ization with a
request queue is full because if if
you're going to have to wait for not
only the four kilobyte rate but also at
home so the actual implementation is not
quite as efficient but we will basically
allow it to wait for as long as a 4
kilobyte request would generally take
and then at that point it'll actually go
to sleep it's yeah it basically it'll it
optimistically thinks it's going to be
very quick and then if it turns out then
not be the case we'll go back and let
some more Cap'n world's all right
because you spun the cpu for a while and
then still did a good double contest
yeah like i said it's not it's not the
ideal case and obviously we would you
know take based on how many requests
cute and how many play make a decision
yeah so one of the challenges there is
that you would actually have to sort of
keep a count of how many outstanding
requests are and we've actually worked
really hard to make it so that we don't
need any locks in the driver we don't
have like state that's shared across a
bunch of threads because the cache
misses actually take a significant
amount of time so yeah uh well since you
asked slide enhancement oh that's right
okay so this is another graph with
bandwidth on the y-axis and access eyes
on the x axis so the turquoise line at
the top is our maximum PCI Express
throughput in one direction clearly
we're never going to quite hit two
gigabytes because there's overhead for
PCI Express but the blue line represents
the baseline implementation so we start
off with something about like 20
megabytes a second for very small
accesses and eventually at around 64
kilobyte requests we can saturate our
pci express bus when we remove the io
scheduler we shift this curve to the
left fairly significantly and as we
continue adding our optimizations this
is removing a bunch of locking that
happens in the colonel so for require
that are about four kilobytes or bigger
you know this has a significant impact
but for a much smaller request there's
still a lot of overhead going through
the kernel for accessing these data
structures mostly because we're contact
switching and this is you know when we
add in the spin weights we can actually
recover a lot of that performance that
we were losing and so we've gone from
about 20 megabytes a second to 500
megabytes a second for very small
requests we get pretty good improvements
for larger requests as well up to the
point where we saturate the pci express
bus so we had a faster connection we
could actually sort of continue these
curves out to the right farther pci
express yeah that's pci express overhead
the cost of you know issuing DMA
requests and waiting for the responses
from the memory controllers and things
like that there is you know there's a
certain amount of overhead with PCI
Express connections anyway that you just
can't get rid of ok so all of these
accesses you know we've done a pretty
good job of removing a lot of the
latency we've significantly increased
our concurrency so we get better
performance but all these accesses are
to a raw device so basically there's no
file system sitting on top of a lot of
these accesses and it turns out that
file system performance is actually
fairly critical so we have a system that
looks like this bunch of applications
running at the top our kernel level
interfaces moneda underneath the file
system actually hurts our performance
quite significantly especially for
rights and this is because we're
spending a lot of time updating metadata
and so we go from something about like
1.5 gigabytes a second down to about 200
megabytes a second for bright access and
this is with XFS on the Linux kernel and
so we need to find a way of actually
addressing this performance discrepancy
how can we make our file systems as fast
as the accesses that we can get to the
raw device
so this is where the next couple of
principles that we have come in in
refactoring critical software across our
hardware operating system and the user
space environment and so in this case
we're going to refactor the file system
and the sharing a protection information
that needs to exist to make that work
and we'll out we'll try really hard to
not break compatibility with our
applications as well so that we don't
have to go and rewrite a bunch of
software to take advantage of these
systems yes so your y-axis there maybe
that's per second is that megabits per
second of user data being written into
the final system or is that maybe is per
second of actual data going out to the
storage device including the overhead of
the file system design so this is user
data so we have a micro benchmark that
essentially is using the the right
system call to update file data okay so
let's look at refactoring and recycling
will eliminate the file system and
operating system overheads so we want to
take a system like this we have
applications kernel level in the middle
the red box denotes trusted code so this
is stuff that normally runs in the
operating system and you know we're
willing to say okay yeah it's right it
has access to everything and then
minetta lives at the bottom and it turns
out that forty-one percent of the total
latency and seventy-three percent of the
remaining software latency goes to
supporting protection and sharing in our
advices so this is like the file system
costs with the operating system context
switches and things like this so we're
going to take this picture and we're
going to split it into a couple of
pieces well you know maintain the
application level at the top we're going
to shift some of the driver interfaces
up into the application will split the
trusted code into two pieces so that the
file system in the i/o stack live on the
side and the permissions checking block
we're going to move down into the
hardware and the key here is that we're
separating the protection mechanism from
the policy so in the on the left side
the colonel does both it sets the policy
and then it enforces it on the right
side the colonel is still going to
maintain the policy and decide who has
access to what but the permissions
checking blocking the hardware
is going to end up actually enforcing
that policy this allows us to actually
give applications direct access to the
hardware so that we can eliminate app
operating system context switches and
all of those costs so to be able to do
this we need four pieces the first is a
virtualized interface to minetta so that
every application can sort of talk to
their own device without having to
compete with the other ones yeah this
seems to be orthogonal dude I would have
been raised right what happened you this
this thing could have been dead dealing
with the classical storage yeah it could
but the the performance gains are
basically not going to be as useful so
like there's no if you're waiting seven
milliseconds anyway it doesn't really
matter if you spend some time in the
criminal right so we need to users face
library that's going to sort of take
over some of the functionality that we
had in the colonel we need our
protection enforcement block and then
there's some operating system and
application level implications as to you
know what changes and what we need to do
to be able to actually make the system
work so the first piece here is moneda
direct virtualized interface and the key
here is that we're virtualizing the
interface and not the device so all of
the applications are seeing the same set
of blocks but they each have a separate
set of registers and tags and you know
their own communications channel with
the device essentially that channel
contains a unique section of the address
space exposed by the device which
contains a set of control registers they
have their own set of tags I said we
could track 64 tags per application
earlier we need some way of signaling
interrupts back up to user space then
they have to have their own set of dma
buffers for transferring data back and
forth between the applications as well
so I'm going to mineta direct actually
supports a thousand channels and the
point here is that we actually want all
of the applications running on your
system to be able to take advantage of
this not just one or two specialized
applications so it's really not a
boutique interface so the second piece
is adding this user space library on top
of our underneath our applications and
we're going to transparently intercept
our file system calls
so if you normally would write read &amp;amp;
write calls to a file descriptor we're
going to intercept those and translate
them into direct rights to the registers
in the device and do all of the copying
of data in this user space driver so
this library has to provide a couple of
things that the operating system
normally would have so from the file
system we need to be able to translate
file offsets into actual physical
addresses that we can ask for data from
the device and we do that by retrieving
and caching this information from the
operating system so the first time you
access a block of data you still have to
make a system call and during that time
we essentially say okay I want to access
offset 100 in some file it's going to
return what physical blocks hold that
data and it's also going to send a
message down to the hardware that says
this application has permission to
access this set of blocks applications
directory to share across applications
in this yeah so they can it gets a
little tricky if you have applications
that are using one application using
this interface in one application using
the operating system interface just
because the block cash tends to
annoyingly get in the way and so we have
some coherency problems that you have to
deal with because rights will sit in
that cash for 30 seconds before they
actually show up in the data whereas
applications using limo neta see only
the data that's in the hardware because
there's no cash one of the rights just
get notified so that word so that but
it's not cashing any of the actual file
data so if one of them rights and the
other one issues a read immediately
after it it's going to see the new data
see the file later since the advice but
it seems like the metadata
like to extend the file or something
they're like you know I know if it
changes so as I can share it across
applications so if so for example if
you're extending a file those
applications you know one application is
going to extend that file the metadata
is actually still managed by the colonel
so you're going to make a system call to
do that extension and the other
applications don't know about whatever
region was allocated so when they asked
for information about that file they
were given information about what
currently existed and so if they want to
then you know read that new data they
have to ask the operating system to
update the permissions entry and make
that data available to them so it it
does actually work but if you're if
you're doing a lot of metadata intensive
updates this is probably not the right
interface for that kind of workload this
works really well if you have files that
you're updating in place a lot or that
have a lot of read and write traffic
that update the file data but not the
metadata so from the file system I said
we need to be able to translate file
offsets we need to be able to
essentially implement POSIX
compatibility as best as we can we get
pretty close to full POSIX compatibility
but they're a bunch of interesting sort
of rarely used features like
synchronizing file pointers between
processes that we don't necessarily
implement and then we also need some
aspects of the driver to be able to talk
to the device itself an issuing complete
request so the third piece that we need
to update here is to be able to actually
enforce protection in the hardware
itself and the key here is the file
system is still setting the policy
whenever an application wants to access
a file it asks the operating system to
update a permissions entry and install
it in the hardware so that that
application will have permission to
access it and the hardware is really
just cashing all of these protection
entries that the the colonel is
generating and we'd use we do that using
a permissions table this is an extent
based system every channel has its own
set of mappings and at the moment we
share 16 entries across all the char
16,000 entry
across all the channels we found this is
pretty good if you have one or more
applications running if you start
running huge numbers of applications you
might actually start running out of
entries if this was in an ASIC we could
make this memory a lot larger than we
are able to with the fpga based design
ok so there's also a couple of operating
system level implications with this
system we haven't had to make any
changes to the file system we don't have
any changes to the applications because
we can just dynamically insert ourselves
under the applications but there are
some open questions your question hinted
at you know what happens if you have a
bunch of applications running at the
same time and you know it's fine as long
as one of them isn't using the old
interface one is using the new interface
and file fragmentation can actually be a
bit of an issue for this system as well
because we're using extents based
records if you have a bunch of disjoint
sections of a file they're going to use
a lot more entries in our permissions
table than one contiguous region of
memory would change to the file system
giving you taking the position check out
of the final system and put it into the
aardvark so what we've done is
essentially there's an interface that
most file systems implement in Linux is
you can use it on any file system that
will basically allow you to query where
it and translate a file offset into a
region of physical addresses in the
storage device and so what we've done
it's basically when you open a file
through lib minetta we query that
interface we can get the range of blocks
that are represented or that are storing
that data and then you can just go and
read and write that data without needing
to communicate with the file system
anymore so we've already done the
permissions check to make sure that you
have access to open this file and to
read and write from it and then we've
told the hardware that this application
now has access to this region of data
interface by which you can query the
state of the file systems
yeah i mean it's it's more the actual
layout of the data on the disk that
we're interested in the permissions
table is is also part of that but it's
sort of the smaller piece so when checks
the permissions and what she wants to
open his past then the fact that you can
forget that those extent stack is what
it is is offering open right so and then
this is coming back what you said about
mixing interfaces you can't mix
interfaces because if somebody's going
to the file system interface they might
be changing the Senate extents involved
in this is summarizing previous yeah
essentially i mean the the big problem
with mixing the interface it actually it
varies depending on which file system
using and like whether it's actually
relocating data or something like that
but the big issue is that if I issue a
right through the normal interface it's
actually going to sit in the cash for a
little while and once it's been read
once it's not actually going to go back
to the disk and ask again so you could
have stale data sitting in the cache
consistency but you'll also have an
issue destructive security which is a
little scarier right i mean if if the
other interface freeze some blocks then
the interfaces fabulou minetta can now
look at this extent that might get
filled with some with the new data from
another application that's not using
them in that interface and you shouldn't
be seen that data yeah does the current
system is that just a security flaw on
the current design a little bit and we
actually have some future work that is
looking at actually using some caching
like using the same interfaces for
caching data and using minetta as a
cache and we so we actually with that we
have a way of shooting down the
permissions entries I mean certainly we
could insert hooks in the kernel to
remove the entries in the hardware
before you move data around and that
would would solve this problem it's not
in the actual implementation for for
this work but it's certainly solvable
yeah really good Charlie new file system
with the mapping of file offset to
physical blockers note
so as long as you're not moving those
extents around underneath yeah so like
if another application is updating the
the data and moving it sort of writing
into the journal then those extents are
going to be moving and we're going to
have to keep refreshing what permissions
entries are installed for the other
applications but once the data is on
disk you know it's not going to move
until the file system gets another
update or you know you sort of right at
the end of the log with the same file
offsets I can say this works for reading
that for every faucet there's a block as
it going this message look for writing
the translation of where you're trying
to write to the physical block is not
something you can create in advance and
so it doesn't it doesn't seem like this
would work with the journal so it
doesn't it doesn't maintain the
journaling aspect of the file system but
you can go and update the data in place
right big if you've already written that
block of data it has a physical location
on on the disk right like it reading it
there that was going to be exactly the
right go to the journal so the German
will return the deluxe football system
which I'm sorry yeah okay there's
nothing about journaling in a
dimensional cause system but a log
virtual file system so communication is
function
when you never suggest yes where the
date is located so Jeremy sayings and
Audrey profile system data moves yeah
right and so what I'm saying is as long
as right we're we're basically breaking
the log structured aspect of the file
system right you if you update the data
through limo neta we can find where that
data is currently living in the device
and we can update it may be missed what
your model is in other words are you are
you creating a new file system or are
you creating a new interface to existing
file systems we're creating a new
interface to existing file system but if
you're updating against look again along
structured puzzles and it expects a
right to happen at the end if you just
substitute randomly in middle and if you
later try to read it using the
traditional file system codes and the
kernel or something to wear Department
know it's going to be exactly where it
would have looked for it before as long
as it's not in the buffer cache already
it's going to go and read whatever the
latest copy of the data is and that's
right i mean if you do a normal if you
do a read in the log structured file
system yes mam structure file systems
are there to reducing some time I
understand I understand that they're not
just I am I trying to say got it for
direct and I I mean even in SSDs you
have a reason to use a lot of structure
file system to avoid over right there
are clean that's a portion I've been
waiting to hear if we are doing anything
on the busy ftra of that
so we I mean devices like minetta don't
actually need an FTL the the memories
have very simple where management
requirements and so we can actually get
away without needing to store sort of a
map for the whole device you assume the
type of memory you use pcs time yes yes
and so a system like this would work for
flash but the the latency difference is
perhaps not enough to sort of require
that we start moving towards you know
different interfaces to the storage ya
hear me they say kezia mysticism let's
continue this that was a big jump or two
that the question was moving because you
wouldn't use a lot of such a thousand
fear we're about to answer the question
anyway and i was about to gain some
understanding okay how your system works
can you just finish answering the
question how what would happen if you
wrap description file system right and
so the short answer is it will work but
you're sort of losing the log structure
semantics of the file system so if
you're going to you know if you were if
you write data in a log structured file
says normally it's going to move you
know change where that block is located
on the disk and it's going to write it
at the end of the log and update a map
that says here's where the latest copy
of this data is in my journal or in the
log and so when we do a read from that
we you know you ask the filesystem hey
where's this data it's going to tell you
that it lives at the end of the journal
at location 5 or something like that and
so we can get that same information and
when we do a right we can go and update
the journal itself perhaps not the last
entry in the journal but somewhere in
the journal this data exists and it's
the most recent copy so we can go and
update that data in place what happens
during a cleaning operation
I guess you I guess you just it just
stops working yeah so if if you move the
block of data for those files comes
along and reach the box yeah so we don't
actually like we don't cache the whole
you know set of extents for a file as
soon as we open it it's sort of on
demand so we asked for the extent that
contains whatever offset you are trying
to access and so what ends up happening
in that case is you'll remove the
permissions entry for that extent
because the data isn't there anymore the
user space library will try and perform
that access and then it'll have to ask
for permission again and it will tell
you where the new data where the data is
located it could be made a little bit
more efficient by having the right hooks
in the file system to essentially
preemptively do that but we don't we
don't do that at the moment okay so
let's look at what the performance
difference is or what performance impact
we can have from a system like minetta
direct so this again this is the same
kind of graph bandwidth on the y-axis
exercise on the x-axis the blue line
represents the file system interface
performance so if we're accessing data
through the file system performing a
bunch of rights you get that the green
line represents the raw device level
performance this purple line represents
the performance of accessing our device
through a broad device through the users
face level interface so you can think of
this as a file system that has one
extent that covers the entire device and
so basically we open the device and we
can read and write to it I mean you have
full access to it and what we'd like to
see when we add the file system back on
top of this is that the performance will
be as close to this purple line as
possible yeah
colonel 61 is one where the user
hesitated
yeah so yes so basically this is the
cost of the the context switch and the
you know the minimal does he have access
to this raw block device so you know
there's a bit of performance improvement
there we go from about 500 to 850 and
nine hundred megabytes a second and so
we'd like to see the file system level
performance be as close to this as
possible as well and so that's actually
what we get so basically we've
completely eliminated the the file
system overhead there's a one-time cost
when you open a file to sort of read in
the extents the first time you access
that block of data but after that we can
just read and write to the file in place
and we don't have to go through the
operating system or the file system at
all and so we've gone from the 1 million
I ops we had before for small accesses
up to about 1.7 so this is a pretty
significant increase in performance so
another thing you know we've worked
pretty hard to try and maintain the same
interface to our file system so you
don't have to change your applications
at all but sometimes it can actually be
a little bit beneficial to go and
rewrite your applications to take
advantage of new features that your
storage devices have to offer and so
liberal neta actually provides an
asynchronous interface to the device as
well so instead of you know capturing
read and write calls and intercepting
them and translating them into accesses
we can go and modify the applications to
actually perform asynchronous operations
so we'll start a request we'll go do
some useful work and then we can wait
for it to finish later and so this graph
at the top the red line represents the
synchronous one thread performance so
this is you know one thread doing random
accesses the dark blue line is one
thread using our asynchronous interface
doing random accesses and the turquoise
line is eight threads using asynchronous
interface and so we can get about a 3x
performance improvement for 32 kilobyte
requests using our asynchronous
interface we also you know change the
adpcm decode stage from media bench
that's basically doing some audio
processing to use this asynchronous
interface so we can be loading in the
next block processing it
writing out the the same you know the
previously processed block at the same
time and we can get about a 1.4 x gain
doing that as well versus using our user
space interface directly sorry what's
the scale what what does it stand for
for the inning PCM it so basically it's
a dick TM encoder I mean yeah we're
basically decoding PCM audio I think why
did what are you showing here basically
I mean you're showing volume files for
the adpcm format on the files and then
you try to read and decode it send it to
all the interview interface so we're
basically converting it from the PCM
encoding to another format and writing
it back to disk super you can move this
this data yeah basic so if you have a
stick size file say it's 100 megabytes
you process it takes a certain amount of
time with the synchronous interface and
with asynchronous one we can do it 1.4
times faster so basically the you know
we have one interface and you know we
have okay sorry we have what this
benchmark written using read and write
system calls to essentially load in a
buffer process it and then write that
out and start processing the next one
and so that's the baseline and then the
improved version does the reading of the
next piece asynchronously and then it
processes you know the currently loaded
chunk and it will write out the the
previously processed one at the same
time using asynchronous calls as well
Elmo speak to this benchmark you are
showing the
using the asynchronous interface that
you can basic issue and request and
there wait for the basically just ping
and wait for the data from you or yeah
well so basically I'm trying to show
that we if we're willing to change the
interfaces that we write our
applications with we can actually get
some fairly significant performance
improvements as well right so you know
liberal neta does a really good job of
speeding up normal read and write calls
by translating those into accesses
directly to the device but if we're
willing to use an asynchronous interface
we can actually sort of start pre
fetching data before we you know need to
actually use it and we can then do
useful work while the data transfers are
actually happening versus the sort of
normal synchronous interface that demo
for us a database like application for a
number
maybe probably is more simple the main
reason is this all media applications
linear and Kalinda clear division can be
heavily pipeline little brother Ed PCM
audio decoding you're basically just a
really need a bunch of any PCM audio
decode player out I mean there's no
reason between just as a stock sure I
mean it's it's just a little example
basically to say because you know the
existing benchmark in media benches
basically has done this as efficiently
as possible using a synchronous
interface so you know it's already
pipelined and it's it's doing the right
things to get good performance but if we
change the interface we can actually do
slightly better certainly other
applications are going to have different
benefits and costs and things like that
overgrown collocation specific so this
is just it's again it's a little micro
benchmarks so it probably depends a lot
on what processing you're actually doing
but basically you know we we can
actually issue more io request with one
thread and so if you have a thread that
can generate a lot of i/o before you
need it then you can get this benefit
one from 12 slides ago where he said
that waiting is a bad idea because the
overhead of setting up the weight is
longer than we expect to wait for the
request to come back and that that
suggested that we want to be synchronous
and not a synchronous but seriously
Justin tiongson it I I don't think that
they're incompatible basically if we
have a large number of threads it makes
sense to allow some of those to wait for
IO request but it again that's in the
synchronous I'm going into the kernel
and I'm going to you know I'm paying a
pretty heavy context which costs to
essentially get into the kernel and
issue my request and so the the penalty
there is when you have to contact switch
back to other threads right there's a
large overhead in this case we have one
user space level thread that's going to
issue a bunch of requests and
potentially be able to do useful work
without ever paying any of those
contexts which costs so we're getting
you know micro seconds of time back here
that we would have just otherwise been
spending switching into the kernel and
coming back so it depends on what work
you're trying to accomplish while your
your accesses are happening as well
certainly if you're if you're reading
tons and tons of data and you know that
you're going to need it a second before
you actually do you should do that
asynchronously and then do your work and
then the data will be ready by the time
you're actually ready to to access it
it's aight aight course
you could also have more codes that
these multiple trending set up instead
of asynchronous operation if you did
that but you want to turn on
optimization in previous life
of waiting
uh it depends I mean a lot of these
things depend on exactly what workload
you're running so if you have a large
number of threads if each of those
threads could actually be issuing you
know a number of i/o request at the same
time it makes sense to have as much work
as possible for the storage device to do
in the queue ready like already on the
device so if you can actually generate
those requests and then continue doing
useful work asynchronous interfaces make
sense if you can't make any progress
until the data comes back you really
want the lowest latency possible and at
that point spinning and saying you know
so that you're ready as soon as the data
comes back is probably the better choice
and you know I don't think it actually
really matters how many threads you have
for each thread you sort of have to make
that decision of do I want this request
the process as soon as the data is done
or am i okay waiting a little bit of
extra time but you know having 10
requests complete and the data available
for those so that's kind of the
trade-off okay so this is sort of the
optimizations for local storage we've
also looked at how we can extend moneda
like devices out onto the network and
sort of attach multiple devices together
in form distributed storage solutions
and so we've applied our three
principles to distributed storage as
well and essentially you know we're
looking at how we can reduce block
transport costs how much does it how
much extra time does it take to actually
go from one device to another one things
like fiber channel and I skazhi are sort
of the examples of what I'm talking
about here we can also refactor other
features into hardware if you wanted to
like replication things like that and we
want to be able to make sure that we can
continue using the existing shared disk
file systems that are out there continue
allowing use of our users face access
libraries and things like that so sorry
so basically we go from a situation
where this where we've done a very good
job of decreasing the latency for our
local
George requests but then we add in some
sort of software stack for sharing
devices out of the network like I skazhi
and we've basically gone back to
squandering our performance gains that
our memory technologies have to offer
and so we want to be able to do
something about this and so to study
this we essentially took a bunch of
minetta direct devices connect them all
to a network and now we have a situation
where there's a large amount of storage
distributed out throughout our network
with a little bit of storage attached to
each node so we have the opportunity to
take advantage of data locality as well
so if the applications are aware of what
data stored on what nodes we can take
advantage of that at the same time to
build this device we take our existing
Brunetta architecture and add a network
interface to it and so now whenever data
is in the transfer buffers we can
actually just generate a packet and send
that to another device or vice versa
requests can come in over the network
and we can process them locally so we
have a very low latency interface to the
network from within our storage device
and we also get to take advantage of the
permissions jacking blocks and the
virtualization components that already
exist so we can actually do the
permissions checks before the requests
go out over the network so the first
thing we looked at here is reducing the
block transport costs so these are sort
of the protocol and physical layer costs
for accessing storage remotely so this
is independent of what it costs actually
access to memory it's independent of the
operating system in the filesystem
overheads it's essentially if you take a
disk and you attach a network interface
to it you know how much extra time does
it take to to use that network interface
versus attaching the disk locally so
with quicksand we are able to use raw
ethernet frames we have a extremely
minimal protocol overhead we're
essentially just sending the destination
addresses and some just like about eight
bytes of data with each request to sort
of signal where it comes from and we're
taking advantage of flow control of
Ethernet level to make sure that we have
a reliable network so we don't have to
deal with retransmitting packets and
things like this and so you know we can
go from something like I scuzzy which
has a latency of about 280 mi
two seconds down to about 17 with
quicksand and for comparison fibre
channel adds about 86 microseconds of
latency overheads and this is doing all
of the packet generation and processing
and hardware check so the the local
storage side in this case is actually
handling the permissions so you have to
trust the the network and that you know
you're basically honorably it's free to
write requests not the words it Jake the
bitch you're located correct and you
know you could do it you could do it
both ways this way if if you don't have
permission you find out much sooner and
if you have a shared disk environment
anyway like they all see the same set of
permissions and metadata yeah just but
you're not flow control thing because I
remember like physical layer that's
Ethernet what's
so it's part of the 802 dot three
standard basically there's a mechanism
that nicks and switches can basically
send each other packet like pause frames
that say don't send me anything else for
X number of seconds or micro seconds or
so and so it's actually it's a it's not
as commonly used but other storage area
network technologies do essentially the
same thing like they require reliable
networks and so they have some form of
flow control built in but but you said
you weren't doing any packet
retransmission so is it a little higher
layer or is there um so in the the
systems that we've built basically where
you know we have a dedicated network so
we have not had to deal with any sort of
packet loss in our layers so you could
but basically we could deal with it by
essentially detecting if the you know
request doesn't actually complete and
reissuing it at the software layer or
something like that but because we have
sort of guarantees of how much buffer
space is available packet boss it's not
related for use at all like I check
something tails or something yeah like I
say we you know we we haven't run into
that particular problem I think the
right way to do it with this solution
would be to actually have timeouts as if
I don't get a response back in a certain
amount of time issue it again but we're
trying to minimize like acts that we
have to you know stand around and
keeping data around so that we can reuse
the buffer space for for other requests
I'm not a hundred percent sure on fibre
channel but I scuzzy runs on top of TCP
IP so they've sort of got the reliable
communications channel underneath so
RDMA or color changer
how I work basically yes she has some
similarities yeah so there are some
similarities with RDMA you know we're
doing a little bit different it's like
it's remote direct storage access
instead of remote direct memory accesses
so we're not actually you know putting
data into ram on another machine it's
going through the SSDs and things and
you know this the permissions jacking
pieces are certainly not there in RDMA
and things like that and as far as
converged Ethernet goes this is sort of
moving in the same direction right we
want to be able to use networks for both
normal you know Internet traffic as well
as storage traffic and you know one of
the pieces of that is having the ability
to support flow control and part of that
channel and things like that but because
we have dedicated network interfaces to
the storage we're not really dealing
with other forms of traffic at the same
time so okay so let's see what the
performance improvements we can get with
devices like this are so if we're
comparing against a nice guzzi stack
here and essentially I scuzzy
performance is pretty poor because of
those really high software overheads
that I talked about fiber channel would
be somewhere in between these two
numbers basically because quicksand has
this much lower protocol overhead we can
get fairly significant performance
improvements as well we also compared
this against a centralized version of
this configuration so this is a more
typical sand setup where you have one
large server in the middle that has a
bunch of storage attached and then you
have a number of clients that are
sharing that storage so we can model
that by having a bunch of moneda boxes
or quicksand SSDs connected to a single
storage server and they're all connected
to the network as well then we have one
or more client interfaces that are using
the same device but we're not touching
the storage or sharing the storage
attached to that device and so if we
look at this we get about a 27 x
improvement compared to the you know 32
or so that we were getting with the
distributed case and this is because
some of the data was available locally
in the other configuration and so we
actually get a locality benefit some of
the accesses that you're performing
don't actually have to go out over the
network I scuzzy is pretty bad in both
cases and that's because of the the soft
rubber heads and so there's not really a
huge benefit to moving the data out into
the network in at least in terms of sort
of raw throughput for completely random
accesses so if we look at a workload
running on top of this we've implemented
a distributed sort an external sort so
this is often used by systems like
mapreduce and other large data sets and
this is based on Triton sort which was
developed at UCSD by George Porter and
basically distributed sorts work in two
phases the first step partitions your
data which starts out at completely
random on all of your nodes we're going
to partition that into groups of ranges
of key values stored on each node so if
you have four nodes we split the key
space into four pieces and you move the
relevant pieces on to the other storage
device like the storage devices that
handle that set of the key space so that
part uses the storage area network
significantly because we're essentially
writing data from every node to every
other node once the partitioning is done
we can sort the data and this happens
locally on each of the nodes so wherever
that range of the key spaces we end up
with a sorted range of of keys so quick
saying can actually leverage the
non-uniform access latency because we
have some of this storage distributed
out into the network and this gives us
some pretty significant performance
improvements so comparing I skazhi to
quicksand we can end the centralized and
distributed versions so this is the
latency for sorting 102 gigabytes of
data spread across I believe eight nodes
and so it takes about eight hundred
fifty seconds to do this sort with I
skazhi in the centralized form we get
about a 2x speed up from using this
tributed and so in this case I scuzzy is
actually benefiting from having a lot of
the data locally as well and quicksand
does about three times better than the
ice guzzi implementation on this
particular workload and we get a 2x
speed up from using the centralized
versus distributed cases angee yeah so
the storage the the ethernet interface
is 10 gig Ethernet there hey LEM
machines i believe and eight nehalem
machines yeah politicians questioned by
her audio but it seems like a really bad
application bench work because this is a
classic big data application where you
know exactly what they an issue you
don't have to issue small rights or
reads you can issue huge ones and you
can do the Mason princely all the way
out to the end of the file is the moment
gap starts so there's no in other words
the casing after latency and we're
trying to reduce the latency of a
request only matters if your block until
the request finishes but that's not the
case at all this sword would sort you
know exactly what you're going to be
reading app that took quite some time
and so there's no I mean there's no
there's no point in to make small trees
and rights you've made over to really
large ones um that's true and you know
that's one of the reasons that the ice
guzzi implementation is actually not
that much slower than the quicksand one
right because we're actually able to do
these these large data transfers and you
know we've looked at other applications
as well and clearly if you're doing sort
of large random accesses or small random
accesses all the time a device like
quicksand is going to be much better
than the equivalent ice cozy that's guys
get slow at all because as long as you
have enough requests outstanding
that the network is always busy where
the underlying device is always busy
trevor is fascinated or slower and it
doesn't get isn't really matter with the
latency of interline requests are just
matters you all you care about is the
utilization is from cliff and not know
if I can see the underlying requests
yeah so is it that I scuzzy for some
reason underutilizing a channel there or
what's happening um it's possible I mean
we're you know we're reading at least a
megabyte at a time in this workload and
so sort of the the overhead that I
scuzzy has is actually just a lot of the
software stack that exists so even
though we're sort of using it in an
ideal case for this workload there still
are you saying that if the whole thing
is you you found that you're spending
what's your time processing eyes cuz he
read on to you um I you know I'm not
actually sure what the breakdown was in
CPU usage for for this I imagine that
there is a fair amount of overhead but
basically because the you're sort of
doing a lot of operating system context
switches and like the I scuzzy stack is
actually pretty involved if you look at
how these devices work like if you trace
the the flow through unless the CPU is
that is the bottleneck resource in other
words you can do as many contexts which
is as you like if what you're waiting
for is for the giant bone transfers to
come over the network or something so
I'm try to understand if you're making
an argument here that the bottle it
resource was to cpu and some reducing
some few over had stood up or is
something else happening so i don't i
mean i think we had available cpu cores
so i don't know that like actual amount
of processing power was a problem it's
possible that you know the one thread or
several threads that are you know doing
these transactions are actually not able
to get through all of those layers of
protocol overhead fast enough I mean I
imagine that's probably the case but you
know basically when you issue an i/o
request or a nice cozy request it goes
down and the colonel it's like a virtual
block device and then you know there's a
user space I scuzzy demon that runs that
you end up going back into users Bay
then you go back down through the second
so just do a lot of good sure you can
yeah but in that case then you know we
clearly don't have enough we don't have
enough CPUs to have a second of latency
absorbed by that many threads right and
so that's probably where the so so it's
literally a second of course I agree
sure yeah but basic you're not see if
you down or suggest you can keep
watching threads to do more than
incredible sigh hey guys I think the
short comment that sumbitch Jeremy
comment this sort is a heavy powerful
application yeah I'll see during the
time baby yeah yeah okay um so let me
conclude the sort of minetta section
here essentially an emerging
non-volatile memory technologies offer
huge potential for extracting sort of
more knowledge out of the data that
we've been able to acquire over the last
year's but it's going to require a
better system designed to be able to do
this effectively the key here is we have
to prevent squandering a lot of the
performance overheads that these
technologies offer and software
overheads are becoming the the critical
piece in that puzzle at the moment so
I've shown you these three principles
reducing software overheads refactoring
pieces across all of the system stack
and and you know recycling pieces of
that stack that we can like not
rewriting our file systems and things
like that so we've been able to reduce
overheads by about eighty four percent
we can get 30 X higher 512 byte I ops of
going from one you know 20 megabytes a
second to 500 megabytes a second for
those transfers and I've also shown you
that distributing storage add into the
sand is perhaps a good idea and that we
can build these very low latency
interfaces to network storage as well
and so let me just give you a quick
overview at the timeline of the Moon
etta work so in 2010 we had our first
prototypes we had papers in super
computer and micro I was the lead
student on this project basically
designed all of the infrastructure built
about sixty percent of the hardware and
all of the software stack to make the
system work in 2011 we released onyx
which is an actual phase change memory
SSD prototype built on top of minetta so
one of the students in our lab built a
dim with a bunch of phase change memory
on it and we drop those into the be
three boards performance is not nearly
close or not that close to the projected
PCM Layton sees but its first generation
PCM 2012 was the minetta direct work so
we're doing direct users face access at
that point and then in this year in a
sky will be showing off quicksand more
and I've also worked on a couple of
other projects at UCSD so we had one of
the first sort of wimpy data-centric
computing nodes in Gordon this is
actually published in ASP loss of 2009
and selected from micro topics as well
I've looked a little bit about Envy
heaps which is looking at how we can
attach non-volatile memories to the DRAM
bus and essentially how do we implement
a transactional memory interface to sort
of protect those technologies and make
them accessible and easier to program
and I've also built several hardware
platforms for doing flash memory
characterization and SSD research as
well but you know that was back in
twenty ten or so so I'd also like to
look a little bit about a couple of
future directions that we can move
storage research so one of these is sort
of whole system optimization the minetta
style work I'd also like to look at how
we can make compiler smarter at actually
interfacing with storage and what we can
do to sort of get more optimizations
automatically by adding features to the
compilers also I think there's a lot
more to be done looking at storage and
networking so the first piece here is
system-wide optimization so mu not a
style research where we're building
hardware and tuning all the layers of
the software stack on top of that are
kind of the things that I like to work
on and I think as fast storage systems
become more available it's going to
drive changes in other areas of the
system stack as well things other than
storage and I think this style of work
is a good fit for looking at a number of
problems I'd also like to look at
storage where compilers so I think that
you know this cartoon at the bottom
gives you an idea of what I'm talking
about at the moment when you write an
application you compile it you use read
and write system calls those are
essentially black boxes that the
compiler can't reason about they can't
do anything about and storage actually
has some pretty well-defined semantics
and so we should be able to sort of
promote storage up into the realm of
things that compilers can reason about
if we have a well-defined interface like
limo neta we can actually start having
compilers automatically convert calls
from synchronous interfaces to
asynchronous ones or to eliminate
redundant read and write calls to the
operating system and things like that
and I'd also like to look more at
storage and networking so quick stan is
really taking the first couple of steps
at how do we attach really low latency
storage to networks and you know what
are the right interfaces for those but I
think there's a lot of broader system
design questions you know we're not
really looking at what synchronization
primitives are we supporting for
distributed file systems to sort of keep
things in sync you know we're still
using the same locks that we had before
things like replication consistency
concerns our problems as well and I
think we need you know better interfaces
and protocols between nodes to really be
able to take advantage of these these
technologies so thank thank you and I be
happy to take any more questions that
you guys have no neta so it's actually
like the Roman god of storage or
something or goddesses storage something
there's if you go on the npsl web page
there's a picture of like a coin or
something that somebody has it has no
neta on it yep</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>