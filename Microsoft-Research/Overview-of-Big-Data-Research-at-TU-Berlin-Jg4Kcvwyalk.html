<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Overview of �Big Data� Research at TU Berlin | Coder Coacher - Coaching Coders</title><meta content="Overview of �Big Data� Research at TU Berlin - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Overview of �Big Data� Research at TU Berlin</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Jg4Kcvwyalk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
we are pleased to welcome Falcor Markel
from now technical university in berlin
me most of you know for quite well he
did his PhD with rudy buyer many years
ago then he got distracted actually
worked for IBM but finally he saw the
light and returned to Germany to the
technical university in Berlin pleased
to have him here and he's bringing
several students with him I must be
pushing them hard because one of them
got a cold and couldn't show up but 22
survived so they're going to give talks
and 4k is going to say a few words about
what's included in this presentation so
thank you for the introduction fall and
thanks for the opportunity for all of us
to be here with clusters and Stefan and
myself so I will not talk too much in
the beginning because this is really
about cost us and Stefan to present some
of the work just briefly the larger
context that all of this rug operates in
so at tu Berlin we are conducting
research in one major research area that
is large-scale data management massively
parallel data processing and we have a
relatively large research project there
were now in the order of 12 PhD students
are working on actually building a
system so we do existence building in a
project called stratosphere which you
also see here the idea is to build a
query processor paralyzed automatically
on a large cluster you'll hear a lot
about that from costes and Stefan what I
just want to briefly mentioned though
the original plan was to have two more
students of mine here which we actually
had some struggle of as Paul already had
mentioned so one of my students too much
bordner he got a flu or cold or
something just on the plane about not
and so he was in Seattle but he is sick
today and the other one Alexander
Alexandrov he actually is had some visa
issues and will only arrive tonight
visa hasta late so it's a bit
unfortunate so there's two talks which
will probably talk about next time or
later on after the talks I'll give some
brief summary on that so those talks are
one of them is in big data benchmarking
so a massively parallel data generator
where there's some challenges how can
you date generate now terabytes and
petabytes of data fast you have to do it
in parallel and how can you do that with
maintaining correlation so that's one
aspect and the other student Thomas his
workers on a use case repository for big
data so actually having some collecting
some use cases we're doing that together
with my carries group at the university
of irvine so that's the other aspect so
those are two things that i'll be happy
to talk about my talk after the
presentations but without much further
ado I would hand over and out to cost us
to give a brief overview of stratosphere
and then focus on a query optimizations
and MapReduce functions and then Stefan
I should also mention just briefly so
cost us is a so many of you may know him
he already he was interning at Microsoft
two years ago and he is a post-doctoral
researcher in my group and really
driving a lot of the efforts in the
stratosphere project Stefan is a now
fourth-year PhD student who will soon
graduated he's doing a lot of work in
query optimization and in particular
paralyzation of grey processing and so
cristata got his PhD with christian
jensen in al Berg and was a Microsoft
and I was working here with me or iced
been working with me for a little more
than a year and Stefan actually did was
a summer internships at all Madame
already and yeah we'll probably graduate
hopefully we graduate next year so with
that I'll hand over to clusters vulgar
for introduction and it's a thank you
for being here so i will give a brief
overview of the stratosphere system i
look a little bit about the architecture
and then i will dive in into some
specific aspects of the query optimizer
that are quite novel so they're quite
novel in the context of a system
good so so the motivation of this of the
project as a whole is big data so we're
seeing nowadays huge amounts of data
human generator was in generated and at
the same time we also see a need for
more complex and more more deep queries
so queries that go beyond the
traditional data warehousing queries and
this is used in a couple of contacts so
in big science in business so people are
doing this thing so as a result over the
last couple of years so a few years by
now we have seen a bunch of completely
new systems that are not traditional
relational dbms is but are used to do
this kind of complex analysis and so one
could argue that this really started
from Google's mob reduce system and then
we had the Hadoop open-source
implementation a couple of more systems
in this category is the Asterix hierarch
system at the University of California
Irvine so my car's group and the scope
system which is used here in pink and
the the stratosphere is also one of the
systems and this is the one I will talk
about today so stratosphere stands from
the layer of above the atmosphere so
above the clouds good so the
architecture looks like that so first of
all stratosphere is a layered system so
there are a lot of a few components in
the stack and it actually exposes a
programming api's to a few of these
components so the bottom on the bottom
level who have a parallel data flow ends
in which is called netherland so this
ends in essentially operates on Dougs
that are parallel programs consisting of
tasks and sandals and we have a layer
that does all the query processing so
its own algorithm certain classes and so
on and on top where we have an optimizer
so an optimizer you can the the second
entry poll the system is what we call a
packed programme and then on top of that
one can actually implement languages so
one can implement code
languages and we have a query language
called meter at the condition there is
some effort to port escala dsl on top of
that so a few words about its component
so the meter language is a is a language
squared language for semi-structured
data so the data model here is essential
ization it contains operators to do a
classic relational processing and in
addition to those it contains operators
to do information extraction for makes
integration kind of stuff so this is
actually developed at some partners at
the hospital Institute that are very
interested in doing data cleansing and
data mining a soul now that the pack
programming model is essentially a new
programming model that is based on and
actually extend MapReduce as a
programming abstraction so if you think
about MapReduce they're essentially to
second order functions and we extend
that to provide additional one so i will
talk in detail about that later the
runtime operators so the runtime
essentially manage this memory it might
say oh and because all the usual query
processing stuff the data flow ends in
gets gets an abstraction there is a dag
a parallel Doug and it does things like
resource allocation scheduling
communication and so on and finally so
stratospheric has a cost-based optimizer
and this cost-based optimizer optimize a
pact program or after a few parsing
steps and so on a meter script and in
the future a scholar program and it
picks things like the physical execution
strategies so for example a short
marriage or a hospice join it Peaks
things like partitioning strategy so how
will I partition my data and finally it
picks operator order so the the focus of
my talk will be on the other optimizer
before i get there so i have a few
examples of this requires that you can
run so as i said we have a JSON data
model and you can write small script so
too
for really for rabbit processing so you
on the top example we're reading a JSON
file and we're filtering on the bottom
example we are transforming some rows
you can do things like joins so down
here we're joining to JSON files and all
that and in addition to those you can
actually have user-defined functions so
you can you can call it a user-defined
function written in Java so this is the
top example over there and actually for
most of my dog will focus on how to
integrate this seller user-defined
functions in a cost-based optimizer it's
very significant yeah it's a science
it's very much that it's very much self
about national it's actually yeah it's
very much circle with a few extensions
for data cleansing its own now this
language is compiled to intermediate
representation that we call supremo that
stands for stratosphere operator and
data model which is essentially an
operator library with pretty fire
operators that you can actually extend
as a user and this so just to give you
the architecture you these are then
translate into these packs programs one
optimization that we can do there is how
to efficiently park the nested data into
records so you can process them
efficiently in the runtime so the random
works on a record data model now the
bucs programming model so the basic idea
here is that we have a programming model
where you so a pack is sort for
paralyzation contract which is
essentially a first-order function which
is written by the user in Java wrapped
around a second order function signature
which is provided by the system which is
so it is picked by a fixed collection of
second-order function so the basic
observation is that the second order
function will describe how the data can
be grouped into independent subsets
which can be processed together so the
reservation and then the user-defined
function will be called once for each of
these data groups so the observationally
was that in the MapReduce paradigm
what we have essentially is a second
order function that we call map right
and a first order function that operates
on the record what the map tells us is
that each of these records is an
independent subset and the UDF is called
one per subject so one per record the
reduce essentially tells us that every
all records that share the value of the
keys here the two records will have a
blue key will be an independent group
and the UDF will operate on these groups
so based on these you can actually
generalize that and we generalized it to
capture a few more relational style
processing so you can for example define
a close-packed which is essentially a
Cartesian product so you will get two
inputs and your second order function
will take every possible pair of records
and create an independent subsets for
those we can have something like a
liquid zone which we call mats so here
you will have a subset an independent
subset for every two records of two
inputs that much on the game and you can
call actually UDF on that pair of
records and we have a coke group with
essentially generalizes with use to two
dimensions or even two datasets you have
a reducer on two data sets yeah so these
are fixed these are hard cold in the
system so these are the five ones that
we have up to now but we're actually
interested in looking at other so one
can actually imagine for more packed to
do things like stream processing to do
things like similarity joints and so on
so the pad programming model then
essentially is a data flow graph
composed of these packets so we have a
direct a cyclic graph with input sources
output sinks and these packed operators
that contains a second order function
and a first order function so here for
example you have two sources that
extract two attributes from two txt
files for example we have we have
connected to map operators that do some
kind of processing and then those go to
a mud pack would go to reduce and go to
icing so it's a so one thing to note
here is that you don't you're not
restricted as in MapReduce to have an
input source and MA
read use and an output but you can
arbitrarily connect this in any kind of
form you want so as long as it's a Doug
and so another fine point here is that
this is a programming model that is
exposed to the user so essentially use
that to ride your udfs but it's also and
currently the intermediate
representation that is used by the
optimizer to enumerate plans now these
parks programs passing through the
optimizer create and met a plan so a
parallel planet is given to this novella
data fluency so how this looks like is
basically you know dab of tasks that are
going to buy channels so the you know
the fine thing here is that you have we
have three kinds of channels so you have
in memory channels you have network
channels and you have materialization
channels and what that allows for is
pipelining so you don't always have to
materialize data and then read it from
this guy here and this gets spawned to a
parallel program depending on the degree
of parallelism and so on and is executed
in a cluster so this slide gives sort of
a you know an overview of what's
happening so we have a pact program on
the left and some user code this gets
compiled to this into this dark and some
code so here a has join is wrapped
around the user code and then some
network communication goes wrapped
around that and then given as a task to
be executed in part good now so now
we'll move to the main part of my talk
so i will talk about the stratosphere
optimizer so we describe some aspects of
that in a vldb paper so 2012 that we
called opening the black boxes so we'll
probably see why so the stratosphere of
the miser is a cost-based optimizer that
enumerates plans so currently it is a
bottom electrodes that passes
interesting properties top down so
basically you are given a packed
programme and you generate an equivalent
back program that has a lower cost and
what I will focus on today is another
last bit which is
how to integrate the small produced Java
user-defined functions fully in the
query optimizer and the way we did that
here so basically the problem is that
you don't have semantics about this UDF
so what we did is to analyze the user
code of this gives using a compiler tool
extract a few properties that we can use
and based on these properties arrive at
safe transformations of the plunge so
the motivation for doing that is that
these so this this programming model
that described is actually quite general
so it's not only using stratosphere but
a variation of this model so a direct
the cyclic graph of operators that
contain this kind of style of udfs is
also used for example in scope and it's
also gaining traction in relational dbms
so both greenplum and a star trying to
integrate and have to some extent
integrate this fab reduce function so
it's a sit it's a timely problem the
problem as I said is that you don't know
what's inside these functions so you
have to do something about that and the
novel thing about stratosphere is how to
deeply embed those in the optimizer
including reordering them in the plan so
the first step is that you will take
with one of those and you will run a
static code analysis tool over them so
we use the Java tool called suit that
operates on some kind of intermediate
representation of the Java code there
are also tools for byte code so for
example if you don't have the original
Java source you can also do the right
code and what we're going to find is for
properties of the user of the user
function the first is the output schema
so based assuming that we know the
schemas of the two inputs we will infer
the scheme of the ultimate the second is
the read set which we loosely here
defined as what are the attributes of
the inputs that may influence somehow
the output of the operator the third is
the right said which is again loosely
defined what are the attributes of the
output that might be different than the
corresponding input attribute yeah and
the final is the emit cardinality which
says so per Coll how many records are
you going to emit
yeah so the way to do this is we are
static code analysis in our case so here
we see an example user function for a
mad sparked which basically copies out
the left input record and has a filter I
don't have to be the left record based
on that field based on that conditional
it either does something using some
algae the right record and then there's
some more things and it emits a record
so keep in mind that so this is a mat
right so this was this was modeling an
equal join so the second order function
essentially takes every two records that
matzen rekey however you can put
anything you want in there right so it's
not n equals 0 in the sense that you
just concatenate the records and I'm
eight but you can really write that one
there so the important part of this
particular Java code is that it's it's
not really general-purpose code so what
we have is essentially we have a record
data model that we expose to the
programmer and the program really has a
fixed API for manipulating these records
so for example in our case we have an
API to create a blank record to copy a
record to get set based on a certain
field number and so on so this is very
similar to the pig API also and I
believe also to scope some yes
differences and the final thing is that
so remember the parks program is a data
flow there is really no control flow
between to operate or so to operators
can only communicate by exchanging the
data set so you can localize your
analysis into its operator so when you
have cold with these properties what you
will actually can do is so to
approximate the Reid said you will go
and you will scan all the get statements
right so from from the statements that
get an attribute from an input record
you can find out with some tributes
you're reading to do to approximate the
right set what you'll actually do is go
and look at the other set statements
right so whenever you said something on
your route would record you need to take
is that something so you need to
basically reversely follow the data flow
graph of the program and look is that
something that is something that could
be potentially new
just something that I cope from my input
and if it's something that could be
potentially new then you can add this to
your rights it so in this example for
example if I know that my input one had
scheme ABC in my boot to hit scheme a DF
I know my output and i can approximate
by the the green statements here they
read set and by the orem statements the
right set and finally the emit
cardinality you can basically
approximate it by looking at the control
flow graph so you need to look you know
them so here the final amid statement
that the midst the output record how
many times is it going to be cold threat
is going to be called 021 the
conditional is it in a loop and so on
now what are the challenges here so the
challenge is how to do this first of all
correctly so you don't want to make
errors and how to do this for general
code so the challenge comes from
conditionals and loops so basically when
you have different code paths and
there's a very simple solution to that
so in the previous example who had these
statements in like six and seven and are
executed only if the condition line five
is true right however being that static
code analysis you have to become you
have to be you don't know if that's
actually if this code path is going to
be followed so you have to be safe and
to be safe you can be conservative in
our case we can prove that if these read
sets and write sets are bigger than what
they actually are then we will actually
get a subset of the other correct clamps
but but all of these plans are correct
so we lose an autumn ization
opportunities but we're guaranteed to be
correct so the safe strategy here is
when in doubt you will always add an
attribute to this rhythm to this read
and write set and you can probably get
correct transformations now having done
that you can do a couple of things so
first of all you can p you can pick
shipping strategies so this is the the
pact program that I had before so the
optimizer knows how to paralyze an
operator from the second order function
signature so it knows that for a mob
over a map operated for example
is a bar single parallel it can run it's
instant it in any node for reduce
operator you need to partition the data
based on the key for a mud super 8 or
you can do anything that the parallel
database does for an equal join so you
can for example do a partition zone you
can broadcast one sides if it's very
small and so on so based on this second
order function signature the optimizer
can pick these shipping strategies then
based on the read and write sit the
optimizer can actually look if the keys
are preserved through functions so it
can identify for example function that
preserves the key so it would not have
to do the double work the partition
price so here what we see for example is
that we have done a partitioning on the
same key needed by the mass and the
reduced so when we are at the other time
that we need to reduce we don't need to
do anything so contrary to the plain
MapReduce here when you run reduce that
does not imply a physical sort and it
use can come for free the second thing
you can do is actually reorder operators
so here comparing it to the previous
plan I have reordered the reduced and
the matchbox so basically how you can do
that is that you can you can reorder to
map operators if you don't have any read
write or right right conflicts so we'll
talk about that a little bit later but
the thing is you can do these things
correctly right and these things are
very important to do so reordering apart
from you know decreasing the data volume
can actually give you more opportunities
for paralyzation right or different
virtuous for paralyzation foot so in
this example here by pushing the reduce
down if they reduce really you know
reduce the data volume you can suites
the building probe side for example
right so very briefly we can formally
prove that we can reorder to map
functions if we don't have any right
write or read write conflicts on their
columns yeah so only if we have if you
have read-only conflicts this can be
safely or you can reorder them up and a
reduced function if that condition is
true and addition to that the map
function preserves the key groups so the
challenge there is that
user expects a certain number of Records
to be in the same group and that must be
preserved for example if you're trying
to reorder reduce with a filter this
filter must either eliminate a whole key
group or must preserve the whole group
so it cannot eliminate some records
inside the group yep and in the paper
actually we have you know these kinds of
theorems for every pair of packets so
basically the map and reduce are the
basic cases and then the rest of the
packs are derivatives of MapReduce plus
Cartesian product and union so you can
do that very easily for the rest and so
button based on this you can actually do
things like push down selections you can
do a genre or drink so the equivalent
zone your drink you can do limited
things of aggregation push down so you
don't have any semantics about the
aggregation than the reducer you don't
know if that's a sum or something
different but you can real if you have
some information about keys and foreign
keys you can reorder it joins and in
addition to this you can actually
reorder reducers with joins for reducers
that you would not normally or easily
write in SQL so actually for programs
that you would not typically use a skill
for you can emulate most of the
reordering that the relational optimizer
does good so the state of of the
optimizer right now is that it does the
automatic parallelization the reordering
is an instant in a prototypical states
so it was a prototype for the paper so
basically what that does is that it
enumerates all possible plan reordering
sand for its of this and for its order
it tries it finds the best physical plan
and so for the evaluation we implemented
so we hunt code it for tasks in this
packed programming model so we wrote the
Delta functions ourselves two of these
are to deviate squares the third is a
pipeline that does text mining and the
final is a single query that does click
stream processing that has basically
reducer that is not really a sequel
aggregation
so the first thing we saw is that the
static code analysis finds the fool the
correct written right sets for three of
the programs and so basically it found
using the static code analysis and then
numerate in all the plants we cover a
hundred percent of the search space for
three for three of those stats and
seventy-five percent of the search space
for the other task so it is a lousy
analysis but it mostly has good accuracy
and yep can you be a little guys framing
by accuracy when you say 75 hours and
its only producing correct results it's
just not covering in search space
exactly so so we only get correct
results right there's no way that you're
going to get the wrong result but what
we did was was so we run the static code
analysis and we also hunt coded what
would be the correct written Rises if it
misses some lumps and that's because why
because it in Swansea large we found a
superset of absolutely poetry than rats
eat right so what we also found is that
you know if you have an optimizer that
can reorder operators you can get
benefits and execution time so yeah so
in our setting so this is in a very
small cluster of four machines over
seven times different for TB c eight
square seven we got two times better
plan for clicks and processing and ten
times better plan for text final
decision is of course generating the
class face but
what is selecting the best plan right
right so do you have some kind of rough
cost model or do you do only
conservatively orderings of work right
so at this stage we have a cost model
that measures I or network io what we
did not have is a good selectivity
estimation component right so we're
working on that and we have a couple of
thoughts on how to do that and this is
the reason also that this is not shaped
in the open source version okay so some
related work so in the bigger context of
you know big data analytics and the
quarry optimization there are a few
systems out there I talked about a few
of them the technique that we used to do
this static code analysis and reordering
operators somehow resemble techniques
that were used in the Fortran compilers
du Louvre optimizations so there's a
small resemblance though so concurrently
two hours there were a few papers out so
there was a paper from from Wisconsin on
the data static code analysis will
reduce functions to basically find if
something is a filter that and then
recommend a p3 index and we recently
found out that people here in Microsoft
who have been doing similar things so
scope actually does a static code
analysis of these functions the
differences from our work is that you
focus motor goes from the partitioning
side and not on the reordering side
although I may be wrong here so you know
xingang can tell us but what's
interesting and there are a few things
that they do a reading log do so you can
actually do a deeper kind of code
analysis so this Sweden writes it is a
very rough thing so you can look for
example if a UDF preserves the short
order of a data set right and so on so
this actually so if you want so
basically to summarize the talk and so I
talked a little bit about the
architecture of stratosphere
stratosphere is a is a complete new
system for big data so it's written from
scratch
and it sort of has the complete stock
from processing to to query language it
is a it so it's open source under the
apache license and survival at
stratosphere w and what I focused on on
this dog is basically how can you deeply
embed this map review style user defined
functions in your query optimizer the
king size that we go is that so first of
all you don't need full semantics a few
properties of the UDF are enough to do
very powerful transformations second you
can accurately estimate this and safely
estimate this using static code analysis
and as I said we always correct and you
can emulate with this most of the
transformational power of a relational
query optimizer a few issues of future
work so you can do as I said deeper code
analysis you can do the transformations
that actually go inside the UDF code so
that for example slice a function in to
do or things like that we are working in
the context of the cost model and so on
we are working on doing this on writing
or optimized in a slightly different way
so we are working on an optimizer that
does not try to find the best plan but
tries to find a plan that is good enough
under a wide variety of circumstances so
the problem there is that in this cloud
setting you don't you can not possibly
have accurate estimates I mean even if
the in the traditional setting your
estimates are usually very off and hear
things in or even worse so what we're
working on is to to basically try to
find a plan that works well in a large
part of this uncertain space so robust
call optimization there are few people
that are working on adapting this plants
at runtime so as the characteristics of
the machine allocation so on chains or
as you get more information at run time
since the plan and Stefan after me we'll
talk about how to do it terror these
programs so how not to do dogs but
cyclic graphs and
how to embed that into the quarry
optimizer some other interests that that
I'm craving and um have talked about is
actually how to basically integrate all
this big data platforms with the new
stuff that is happening at the
networking layer so there's there's
actually a big revolution going on the
network's right now software-defined
networking and so on and this is very
actually very relevant classes database
researchers and how do you know how to
go to that space good so these are the
papers and thank you very much I'll be
able to bless so what version of
MapReduce is working
so this is not working this is not built
on MapReduce this is actually a new code
base so it uses HDFS the hot dog
distributed file system to store the
data and retrieve the input data so you
can use that but it doesn't have any
common code with Hadoop or need to find
out with yarn yet where is going to have
allowed different types of programming
framework so you thinking about yes we
are pleased about that we're thinking
about voting with the yarn you know so
that people can try it out another
reason so if I can follow up and then so
one one key aspect of that further data
prevention that was just a thought at
the bottom was that it has a very
flexible scheduling model and and it was
designed to run on infrastructure as a
service concerns cloth so it's actually
able to have just a single master
running and then allocate workers on the
fly for its its tasks to deallocate them
on wild parts of the data processor one
it so that was filled to the core design
and so we're at the moment our web to it
that is trying to replace that
connection to the natural search as a
service controller with a interface to
the yarn these managers so it's not
working on instances are young so
because it's more less following
examples
so when you did you to the java code
analysis so you use getting out of the
java code java cut the truth
throw them so we use the wizard duel we
didn't write our own static code
analyzer the tool is called suit so it
operates on three a dress code
abstraction okay thank you thank you all
right okay so yeah the stock is more or
less going to seamlessly follow up on
what what costs us did so it's taking
assets in the context of the same system
and as clusters mentioned it's um
basically the techniques we included to
UM make the system and efficient one
time for um for AG rodents that are
iterative in nature and yeah I don't
think I've true arm to do a big
motivation for that because I mean
iterative algorithms are everywhere when
it comes to machine learning or data
mining so the people that have have
played with Hadoop or with MapReduce wit
or if programs probably know that I mean
you can do it you can always write a
driver around a MapReduce program and
invoke a time after time and it's not
doing great and there have been
modifications of Hadoop they try to
overcome that and they've been doing a
little better but still not great so um
we um we're presenting the techniques
that we've developed to actually make
that a little better and the motivation
for the whole setup is the following
with with Hadoop and a lot of those
other general purpose big data analysis
systems not being great at running those
machine learning algorithms those
iterative algorithms in general there's
been actually am a variety of dedicated
or specialized systems for iterative
data processing so Google has published
the paper about pre goal which is this
bog synchronous processing adoption for
for graph analysis this of course is
always an Apache open source version of
that and there's been the graph
libsystem um yeah by people from I think
share from Berkeley and Carnegie Mellon
and um yeah they're they're dedicated
systems that that
ah try to overcome the problem that the
general purpose big data analysis
systems are not really doing great at at
those iterative algorithms but the
problem that a lot of people are seeing
today is if you're running that actually
in a in a in a production setting and
assume you're doing things like training
a spam detector or a recommender on a
periodic base what you typically have is
um the data is not in the format that
you can just throw it into your graph
analysis algorithm you typically have
something like Hadoop or you can replace
a dupe here with the stratosphere or
maybe your favorite data warehouse to
actually extract the the features or the
data from your from your sources
transform it into a format probably
normalize it then you throw it into this
specialized system for interactive data
processing then you take the result you
actually a post process it maybe map it
back to the keys using your production
system and so on so you actually have a
rather complex processing pipeline and
yeah this is basically all because the
system's use for the first in the last
step which are typically general-purpose
data flow systems or or MapReduce or a
parallel database are not really good at
those at whatever good and so the idea
was if we could just make a make the
make the data flow systems good at this
step in the middle then we can actually
simplify the whole process by a lot and
for a lot of reasons we think that just
making a data flow API able to process
an iterative algorithms efficiently is a
very worthwhile thing so first of all
you actually see that a lot of those
specialized systems are very specially
tailored towards certain use cases and
if you use a dataflow API you with some
extensions those are very flexible this
is a very very flexible model so um as
you will see in the later part of the
talk we can lay a custom api's on top of
that very easily the typically
correspond to just a very special data
flow so prague ilysm prig las a special
case of an iterative dataflow as we're
going to present it later you can also
address compartir el scripts to that
very easily oricon program directly
against that like people are programming
against against MapReduce so um I mean
if you think about making data flows
iteratively the first um the first thing
you need is actually not so not so
complete
so I'm let me motivate that with an
example this is an implementation of the
page rank as it looks like in yeah in
stratosphere so you can think of match
here or join and put John because it's
more commonly understood actually and
page rank is just an iterative matrix
vector multiplication where there the
vector is actually a vector of ranks for
each page and the matrix is though if
you wish the topology of the of the
network with transition probabilities of
the edges and what you do do in each
step is he actually join first the UM
you join the the current rank vector
with the matrix and then you aggregate
the you aggregate the other rank for the
page then in the reducer so we represent
that in a data flow the rank vector is a
pair of page ladies and rank Soviet page
the current rank and the transition
matrix as a pair of triples the major is
actually very sparse a lot of that pairs
of pages don't have edges between them
so the transition transition probability
is 0 and we just skip the skip the entry
from the matrix to represent it in the
sparse way so the matrix is here just a
set of triple sauce page ID target page
ID and some transition probability and
then the the algorithm is just joining
them and aggregating that okay so if you
want to make that iterative you can um
you can always say okay let's just run
that that one again and make sure that
the next iteration consumes the output
of the previous iteration you can
emulate that purchased yeah renaming the
input files or so or you can just say
okay let's let's include a really a
physical feedback edge in there in the
system so that's actually a pretty I'm
pretty simple idea and it helps a bit
but if you run that repeatedly you'll
actually see that it is a lot of
redundant work for example it's going to
yeah to do all the full processing of
the join of that input which it actually
doesn't need to that is something that
has motivated some of the UM some of the
specializations of Hadoop for iterative
processing for those of you who are
familiar for example with the halloo
bore the twister system what they did
was for example they gave you the
ability to to say okay here is a loop
invariant data set for the reducer that
would then be partitioned and sorted
only once and then
question that variant so that is
something that is actually very neat but
of course we have our general purpose
data flow system we have an optimizer so
let's let's just do have the optimizer
do that and the thing um the thing to do
that is actually very simple you just
have to identify in this data flow
difference between the dynamic path
which is everything that is basically a
successor in the in the end the cyclic
flow of the F the feedback edge and the
static path which is basically
everything that's not a successor and we
enabled the run time to actually
basically cache the data set where the
constant and dynamic data paths meet and
caching is just something like placing a
temp table or actually um it doesn't
really need to be a temporary table but
if the if the joint is for example using
a hash table you cannot just make that
hash table persistent across iterations
on something like that and um then we
just gave the optimizer I'm a notion of
this constant and dynamic data path and
gave the costs a little different weight
so the cost of the dynamic data part I
waited more than on the constant data
path and what comes out of that's
actually immediately very nice so we
will see that it comes up with plans
that just try to push work into the
constant data path and catch the result
so those are two different angry
execution plans for this PageRank
algorithm I mean the database people
will immediately see the difference
between a broadcast run and partition
join the interesting thing is um if you
look at those two variants they're
actually two very famous hand optimized
variants the left one the one that um
has been in my out for quite a while
optimized for um for computing the page
rank of off our setups where the rank
vector actually can be broadcasted and
the right one is the one that is close
to the execution strategy that Google
described and it's MapReduce paper so
the optimizer just gives you those two
variants depending on the size of the
rank vector so that's all fine that's an
that's an easy winner okay that's that's
the that's the summary so all you need
to do is actually make the optivisor
aware of feedback edge of a constant
data path dynamic data path and
different weights so there's a class of
algorithms that is still doing kind of
suboptimal in in that set up and um yeah
let me motivate that with the various
algorithm here that is an algorithm
taken from the Pegasus paper flute those
paper on graph analysis it's an
algorithm that finds connected
components of grass and just that in a
in a very simple way by saying okay um
each vertex is assigned a component ID
and after the algorithm is run all the
vertices that are in the same connected
component should end up with the same
connected component ID so um this
algorithm iteratively just takes the
component ID from each vertex tells it
to all of the vertexes neighbors and
each of the neighbors just says ok if I
find an ID that is smaller than my own
current one then I'm going to take that
one and I go on so you can actually see
that after a few iterations all vertices
in the same connected component end up
with a with the idea of the vertex with
the smallest ID and I've colored the
vertices in that example a little
differently and the reason behind that
is the following not in every step will
every vertex end up with a new ID and if
a vertex doesn't end up with a new ID
then basically for that vertex nothing
changed so there's no need to really
take the information of that vertex and
incorporate it into the next iteration
right so this is a phenomenon that is
often called sparse computational
dependencies it's especially present in
graphs but also in a lot of other
problems that can be modeled as graphs
so you really want to take those into
account um you can always run an
algorithm like that in in the fashion as
we had it before so you can you can
model this connect component algorithm
actually very in a very similar way
became model differently but I just
modeled it the same way with a joint in
the reduce a join again between the
vector that represents the state of the
vertices in this case just very exciting
in component dirty and a mage the matrix
is now even simpler it's just just the
edges right and you again join joining
meaning every vertex tells basically the
end of its outgoing edges about its
component edgy and then do a minimal
aggregation of all the the candidates so
if you run the algorithm like that
you'll actually see that it is a
constant amount of work in every
iteration right because it for
computes the entire next state it
consumes the previous model entirely
computes a complete new version of it
and and it will in a lot of case end up
recomputing just the same state as was
the input to the iteration if you look
at the at the green curve this one is
actually the number of vertices that
really changed ninja iteration so that
contributed something or that would
contribute some new information
potentially to the next iteration this
is the the ones that were colored were
colored red in the previous example so
this um this is very true for many
algorithms that you see actually that
the number of vertices they contribute
is going down after a few iterations
that is not only true for something
something that connected components or
finding shortest paths but it's also
true for a modified versions of PageRank
that try to adaptively schedule the
computation and the the difference in in
the work you do in each iteration is of
course huge so exploding that is very
desirable so how do we actually do that
in in our system because it's a it's a
property of data flow systems actually
that each operators result depends only
on its inputs right so they're kind of a
natural match for this bulk version of
iterative processing and this was the
main of one of the main motivators
actually for for Google's prego system
because there you can really keep the
stage of a vertex across iterations so
how can we try and encapsulate that
still in a very generic fashion to embed
it into a data flow and yeah one thing
was that we said okay wait we need two
representations of data flow analysis
and we have the bark representation
which was the one I just showed you
before the rather straightforward one
and we have an incremental type of
iterations where we try to encapsulate
exactly that nature that only parts of
the model change and we're later see
that there's actually a special case of
those incremental iterations that you
can even run without the typical super
step synchronization which you haven't
and most systems after after each
iteration ok so the abstraction that we
picked for for representing that and in
a data flow is called a work set set
algorithm abstraction and works at
algorithms our technique actually from
compiler construction and it's used
there for optimizing loops and so that
was actually are intuitively are good
matches it's used to optimize loops to
have them incrementally incrementally
compute the the next element that they
work on and you can think of that
actually as there if you know semin
evaluation and recursive computation
then works at algorithms is the is on
the same level for a tour edge of
iterative computation so it's it's doing
its that yeah it's a very similar thing
so I'm what we're what we're trying to
do is or what we're giving here the
system is an abstraction that is not
working on a solution set on a single
set that it's recomputing every time but
it's um it's actually working on a work
set that it's recomputing every time and
instead of computing a new solution set
its out of the works at computing a
delta for the solution set and then
using this data on the previous work set
to compute a new work said so you
typically motive model the algorithm in
that way that the work set is a is
candidate elements that go into the
Delta and with the Delta and the
previous works at your computing your
candidates so um if you want here's a
brief illustration of this connected
components algorithm in the work set
this is our solution set here really I'm
the state of the vertices and the work
set is is the candidate component ideas
for for the work set so I modeled it
like this that it's basically grouped as
it goes into the reducer by the target
target vertex or vertex one here really
gets from where takes to a candidate
component ID of two in from vertex 3
candidate component ID or three and
similarly vertex 6 gets candidate 5 from
vertex 5 so what you do with that is you
actually compute a delta for the
solution set which emerged into the
solution set and then you take those two
to compute a new work set with that you
actually get a new deal who emerged into
the solution set you actually see that
the works that is typically going down
so the termination criterion for works
at algorithm is really the empty works
that actually similar is for recursive
algorithm inseminate evaluation its if
no ruled every patient is added
okay and this really encapsulates the UM
encapsulate this dynamic computation
scheduling so if you we want to
represent this as a as an iterative
dataflow we actually see we have a
feedback edge from the next iterations
work set back into the system and a
delta which goes as not really as the
Union but is a modified Union operation
back into the into the solution set this
algorithm is now very similar to just um
the join is basically still to join your
head in the algorithm before between the
edges and in this case not the entire
solution but basically what came out of
that um of that operation which also
goes into the Delta and this is this is
the reducer so the co group as course is
mentioned as a more dimensional reducer
which is in this case not reducing
entirely on the work set but also
comparing that to the solution set to
actually compute a proper data and yeah
so interestingly you can more or less
take this plan as as being a template to
implement any pretty logarithm so I said
before if you if you use such a more
flexible abstraction you will actually
see that a lot of the specialized
systems are just special case data flows
on top of that abstraction so this is
basically the abstraction to do pray
glue on top of that um there's I
mentioned also earlier which is um which
might be interesting to note that that
in some cases you can eliminate the
super step areas here so this is the
same plan as as we did before for this
incremental algorithm but the difference
is now that we don't use a a coke group
here remember coke group like reduces a
group at a time operation replace that
now buy a tablet a time John operation
so what we're doing in this case is
really we're taking out the entire work
set and combine it with a solution set
but we're taking one element at a time
out of the works that if you wish and
comparing that to the and taking this
candidate and seeing whether it actually
should affect the solution set and the
reason why we can do that is because in
in this special algorithm we actually
have an idempotent operation in here
basically selecting the minimum one the
smaller one of the current state of the
solution set and the new candidate ID
and because you don't need to do this on
the whole group at a time we can just do
that on the elements individually so if
the algorithm actually has item
haute ind door it's doesn't even need
and doesn't necessarily have to be
idempotent but they're also weaker
conditions which hold um if the
algorithm fulfills that condition you
can actually often transform the crew
better time operations so tableta time
operations and then if you have only
tablet to time operations you can
eliminate the the super step boundaries
and let the algorithm run in and
basically unsynchronized fashion more or
less okay one last interesting thing
that I want to UM want to point out here
is with that abstraction you can
actually do algorithms that for example
preggers cannot do so I said earlier
that there is an interesting variant of
the of the PageRank algorithm it's
called adaptive page rank which tries to
exploit the dynamic computation and they
are built on the fact that actually the
the pages that end up with a very small
rank to be click on very well converge
very fast to the small ring and most of
the later iteration actually work only
on the both on the high rank pages and
on the pages that take actually special
roads like hubs or so between between
densely connected components that once
actually implemented on pregnant just in
a very hard fashion but in this case
it's just another special case of law of
a data flow so it's a pretty pretty
versatile abstraction that allows you to
do quite a bit I'm a few performance
numbers on on how we did with that so we
compared um we compared our
implementation of bug algorithms which
is there the first version just with a
naive single feedback edge with an
implementation of the iterative
algorithm and I'm also here one with the
elimination of the super step boundaries
we compare that to giraffe which is this
specialized system for graph analysis
and sparks or sparkles and a pretty neat
implementation actually of a data flow
system that that's very good at doing
those bike iterations and I'm rear under
done on four different graphs and so one
thing that's actually visible directly
is that the systems that can exploit the
sparse computational dependencies are
always faster than the other ones um it
seems that that our by processing in
spikes by processing are let's say in
the same order of speed and
the yeah the incremental processing is
actually able to essentially competitive
with the specialized implementation on
giraffe on the larger data sets we
actually could only do an implementation
of the algorithms against our own
difference between our own different
modes of execution and the problem is
the following so our cluster is actually
not that big some of the data sets are
actually pretty big so um with the
generated candidates and so on you
actually create create intermediate data
volumes that are exceeding the size of
the main memory and because this is
really just a special case of an of a
data flow with all the algorithms
implemented in such a way that they
operate well on a memory pressure i mean
the joint just degrades to disk right
the sort becomes external and so on it
it just worked where is both giraffe and
spark actually being being actually
special talent and not building on top
of ya of the previous existing
implementations of of those algorithms
as they're known from from literature
and those systems actually we're not
able to handle it at least in their
current point of implementation at their
core implementation state they were not
able to handle those larger sense yeah
where's the purple
this one here this one is the one
without em without the super set
boundaries this one is the one with
super step boundaries so okay I'm why
that one is more expensive let me go
back to to that plan here okay so if you
look at the way this one execute it's
basically taking the smallest of the
candidates comparing that against the
current and vertical component idea of
that vertex and only if that one is
smaller it it puts it into the Delta and
into the UM into their to the join that
creates the new candidates if you look
at this one here maybe this is taking at
first only second smallest one updating
it with a second smallest value and then
it's creating candidates based on the
second smallest value and then it's
actually updating the state of the
smallest value so it's creating new
candidates based on the smallest
component ID so the candidates based on
the second smallest component that you
will actually have no effect in the end
but they're still actually processed in
the system so depending on on the
structure of the graph that may month
sometimes be more expensive it may
sometimes be cheaper so this is
something we actually saw here that it
can make a difference sometimes in
general so we have to actually this is
some of the follow-up work we have to
investigate that little more the ability
to run algorithm I synchronously
actually opens a very different realm of
doing it um which you have not yet
explored we assume actually that there
are more robust to to processing and the
and the presence of let's say stragglers
and slow down notes because you don't
have to globally synchronize at each
step but that is something we were
actually currently exploring so at this
point um I just want to point out that
it's actually possible to run that Model
T the true benefit of that is something
we're currently investigating okay I'm a
bit of related work on that so I
mentioned before that there were
adoptions of of Hadoop and our MapReduce
in general to make it run by Gaudreau
iterative algorithm sufficiently and the
optimizations they put in there are
actually mostly subsumed by the by the
optimizations that the optimizer does
here there's um
bollocks ink runs parallel processing
model and it's it's adoptions pre Glenn
giraffe which actually just special
cases of the of the incremental
iterations that we showed here an
interesting I'm Carol L work on that was
by people from UM from the former yahoo
group so how cool am I Krishnan scoop
which is not here and I'm you see I my
carries group which they came on to
dissemble into very similar observations
of problems and what they what they did
is they they try to model it exactly in
the recursive way so I mean recursion
and iteration is theoretically equally
powerful so you can build from both
actually an abstraction for that they
are they compile recursive definitions
of the problems down to data flows which
is actually possible at the moment I
think actually that the abstraction that
we have here might be somehow simpler
even a bit because in the recursive rule
expression you have to include certain
tricks like temporal variables to
actually say okay this world every
vision is actually a new version of the
old rule derivation and so on but um
it's it's in general equally powerful
and then there's a base de specialized
system GraphLab which i think is of all
the specialized systems currently most
interesting one and so you can actually
model the graph flap processing almost
as a special case of a parallel data
flow the only thing that we're flap mum
at the moment does so GraphLab allows to
work asynchronously which we also do but
they have a they have a special way of
being able to relax consistency
constraints um because a lot of the
algorithms are numerically stable enough
to still work under a relaxed
consistency which we don't allow at the
moment so but there's um we actually
looking at how it's actually possible
tool to tell a system that only reacts
consistency is necessary here okay um so
yeah that's the the second part of the
talk and yeah true to conclude it
actually I'm integrating iterative
algorithms into a data flow is a very a
very worthwhile thing because it can
actually subsume a lot of specialized
system and
can lead to homogeneous data processing
pipelines and exploiting sports
computational dependencies and dynamic
scheduling is something that is that is
very important to do and works at
algorithms are works at the worksite
abstraction is actually a way to do that
without violating the principles of
dataflow programming like side-effect
free operators and if you integrate that
properly you can actually end up being
very competitive to specialized systems
okay so that's it thank you yeah what is
the duration standing yeah okay that's
very very very interesting so um the
feedback hu model doesn't necessarily
really have to be a physical feedback
channel you can just be logical and
you're I'm really unroll the loop
basically lazily in the in the
processing and that would actually
enable you to easily use different plans
in each in each iteration so we're not
doing this at the moment we're at the
moment optimizing with the expected
costs for the first first iteration
there of course case where the size of
the partial solution changes especially
if you have like in inflationary fixed
points or so and yeah it's definitely
worthwhile to look at that we have not
done that at this point so the system is
theoretically able to have that but
we've not yet done the the incremental
optimization aspect of it
cashed in what information goes
cash
okay so so how did you revoke you
responded we compute this in that is in
cash so it catches basically at the end
of the of the aesthetic data path so
okay at the moment when what we're doing
is when a note goes down we assuming
that we can actually allocate a backup
known for that so what we're not doing
is if we start on 10 notes and we hashed
everything into 10 partitions that we do
have rehash into nine partitions or so
because that would actually mess up the
cash that is true as soon as you can as
soon as you can allocate a new note
which actually takes over that partition
then you actually do the fault tolerance
recovery just as in the non iterative
park because it's part of the static
data path you redo some work actually
doing during recovery and fault
tolerance in the system where operators
have not the blocking property is it's a
very interesting problem of its own and
on which the people in our research
group that are building the further data
flow engine are working on so I can tell
you a little about that offline if
you're interested in that but actually
this work so transparently here for this
part with the limitation that you need a
backup note you don't redistribute it
among a smaller set of notes so at the
moment yeah chicken please try to
understand
for the ultimate looks like the ultimate
is just at the casual build strategies
over the place to cash
the the optimizer basically just what
exactly cost us described and in
addition it yeah it plays it uses your
wrist explains the cash and as I said
the optimizer doesn't really need to be
changed all that much except that you
need to tell it to identify the dynamic
and constant path and actually use
different cost ways everything else is
almost given for free because it's the
regular operation of what the optimizer
does anyways so um there's I think the
slight difference in the way that the
optimizer works at the moment in our
system compared to the classical volcano
style optimizer and that is that
interesting properties when they're
propagated top-down are not only used
actually in the pruning phase where you
say okay I'm I keep a plan that is
slightly more expensive around if it
fulfills an interesting property but
it's really also used to generate
candidates that actually realize
strategies further down the plan that
there then they're actually needed so
this is the way how the work actually
ends up further down below and the
constant data path that's but that's the
only difference I think to the classical
volcano model is based on heuristic I
assumed name is Albie
Sakura Sakura stick to consider whether
they had cash
yeah so at the moment we actually cash
always at the point where the constant
or dynamic data path mean and we just
place the cash either yeah you see in
that case actually after the build hash
tables so it catches the hash table as a
whole the hash table just becomes
persistent across notes and yeah this is
a probing operational in this this is
not really an operation it's just added
in this visualization to illustrate that
what the plan does so the cash is
actually after the latest yeah operator
that really changes the data at this
will give a little bit more way with an
unavailable but basically you can do
things like pulling things out over the
duration it's inside of negotiation so I
said but but I'm deck to the questions
already however it's there's heuristic
us that I used for generating the plan
candidates and that the service things
will decide where the cash operation me
and Jeff enters get bit simplistic which
is another the dynamic data path here
comes comes in meets these very data
paths that's where the Hashemite this
place I'm not sure if you may be
referring to optimization words there's
things like this magic set
transformation which try to actually
pushes filters from somehow successive
iterations into a previous iteration and
so there those are I mean this is
something that you could add it saw this
is this has been added to to the
optimizer of a relational database and
relational databases if you wish a an
optimizer similar to this one as well it
the problem that we're always having
here is that you are operating on the
restricted set of semantics or a smaller
set of semantics or sometimes those
optimizations are little high order to
apply but if you follow up the project
in the next you'll actually see that
we're moving through a semantically
richer model so i think then it'd be
able
quite a lot of interesting optimization
issues question but what some things
all right any other questions not relate
to optimization all right thank you so
much all right thanks Alysa</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>