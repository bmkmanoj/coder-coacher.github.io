<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probabilistic aspects of minimum spanning trees | Coder Coacher - Coaching Coders</title><meta content="Probabilistic aspects of minimum spanning trees - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probabilistic aspects of minimum spanning trees</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/85-XgM6qxps" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good afternoon everyone we're
delighted to have luigia dario Barry
tell us about probabilistic aspects of
minimum spanning trees thanks very much
you've oh thanks to the invitation so
I'm starting with this picture because
it's a nice picture that's somewhat
related to the subject of my talk and so
since there was some interest maybe I'll
tell you quickly just what this is so
here we have one hundred and thousand
uniformly random points in the unit
square we formed their minimum spanning
tree and say again the minimum spanning
tree according to Euclidean distance
yeah and we've picked some random root
vertex which is somewhere around here as
if I have a laser I can do that from
further away and then colored all the
vertices according to their graph
distance in the minimum spanning tree
from that root vertex so the closest
vertices are red and then orange yellow
green blue purple violet codes graph
distance in the tree from that root and
you see all these interesting curves
start to appear so this is a pivot
uniformly at random yeah so so so this
is it this is a picture that no one can
say anything about to date if if we used
some version of this on the triangular
lattice then the people to ask would be
a clear soft galbo gabapentin or the
tram who have a forthcoming paper on the
subject and the black lines so the
problem is that there are so many points
here that you can't see the most of the
edges of this tree only the colors so
what I've done is drawn the important
edges and what important means is that
so an edge is drawn in bold here if on
either side of that edge lies at least
three percent of the tree so if the edge
cuts at least 3,000 vertices off from
the rest of the graph okay so it gives
you some idea of the broad broadly of
the structure of the of the tree okay
but I won't be talking about two
dimensions today this is just sort of a
motivational picture and i'll be talking
about minimus pantries and high
dimensions so just a very very brief
reminder of what they are and how we
build them so if you trace back a bit
you find all the earliest reference that
I've been able to find is due to buca
and he asked the question in the
following sort of general form so you're
given the complete graph so n endpoints
with distinct edge weights which we
think of as distances between the points
and you're interested in forming a
network so buying a network which is the
unique connected graph that minimizes
the total length ok so you want to
connect up all of the cities and you
don't want you want to pay the minimum
number of miles of road ok so we've
we've chosen all the edge weights to be
distinct so that we had so that there's
a unique minimizer that minimizer is
necessarily a tree you're never going to
buy a cycle if all you care about is the
minimal connectivity and this is the
minimum spanning tree ok so the one of
the standard there's many easy
algorithms for building these things one
of the things I like about this problem
is that it's algorithmically very simple
but probably quite challenging so here's
one of the methods sort of simple greedy
method often called Chris Cole's
algorithm it's really essentially from
this earlier paper it just says or the
edges of your network by increasing
order of weight then consider the edges
one one at a time in that in that order
and add each edge and less doing so
would create a cycle so we're always
going to add the smallest weight edge in
this example okay second smallest is
three we add it don't add six because it
would create a cycle add seven and now
we're already connected
you know we're not going to add any more
don't add eight or nine okay and so this
is quite standard I'm going over quickly
but i want to emphasize one thing about
this process okay which is the following
any time we decide not to add an edge
that's because it would create a cycle
which means its endpoints are already in
the same component okay so that means
that if at every step if we're only
interested in the connectivity that has
been formed by the edges that we've
added okay so which at a given stage
which pairs of vertices are now
connected up by some path it doesn't
matter whether we add all the edges are
only the edges that this procedure it's
okay so let's just convince ourselves
with that quite quickly um here we add
an edge add an edge the third edge we
don't add right but if we did at it we
wouldn't gain any new connectivity there
was already path between those vertices
okay and so on here we add an edge and
now any edge that we the last two days
we don't add but we that doesn't change
the connectivity okay so so this gives
you a coupling between two processes one
where you only add edges that don't
create cycles and one where you add all
the edges and that couplings going to be
basic for this talk so it's hope it's
clear at all okay so what's um so say it
again that is not move oh that's close
but this really is crystals but I mean
in essence it in essence it's faarooq
asaad with it hey crystal crisco really
says one at a time but brief cassette
brooker more or less as do some greedy
procedure with possibly larger clumps
and single vertices ok so it's i think i
mean i would i would give the credit for
this super Africa but what blue car does
is everybody excuses this minimum cost
edge and then you certainly select all
of those
you see my right so so yeah so then then
well and it's not individual versus
right its its its components if every
component yeah yeah but I mean it's it's
essentially it's essentially the sort of
global greedy procedure it's it's
slightly different from crystal you're
right but it's okay so um so you can
think of various probabilistic models
that you might impose if you wanted to
understand the sort of typical structure
of these I showed you one at the start
given the sort of that the description
of the problem i just gave you a natural
probabilistic model it would just be to
say instead pick iid wait iid weights or
lengths okay and so that gives you sort
of two different probabilistic
structures one broadly Euclidean one
broadly I ID or mean field okay then
there's there's there's course so that
that's sort of one axis of division that
you can look at for research into
minimum spanning trees from a
probabilistic perspective another is
whether you study things that actually
have to do with the weights themselves
or you study the structure the sort of
metric or graph structure of the object
that you build okay and there's a lot of
research in both of those camps so on
the weight functionals so so maybe the
most famous classical result in this
subject is that the the total weight of
the minimum spanning tree if you use
independent uniform weights on the
complete graph convergys azada of three
so this is a famous result due to Ellen
freeze okay but there's a lot more
research in a similar direction okay on
the other hand if you want to study the
global structure then there's there's
also a huge body of work and there's all
sorts of questions you can ask one
natural probabilistic framework for
asking those questions is to try to
study some sort of distributional
convergence of your finite objects to
some limit object which might be an
infinite volume limit allah global weak
convergence or it might be some compact
limit something to do with with curves
in the plane if you're in a euclidean
setting or there's a there's a variety
of possibilities and a lot of
I'm work on the subject including by
people in this room okay so um I've
mostly been focused on trying to
understand the the iid picture and the
metric structure okay and so there's a
variety of ways you can do that you can
ask for local structure sort of global
structure which you might get at bye-bye
rescaling your treat if sort of maintain
a compact object as n gets large or you
might try some other things ok so
another question you can ask is how you
know if we manage to understand one
object into what degrees at a template
for other objects so if we can handle
one sort of some sorts of weights can we
handle other weights or can we handle
minimal entries of other graphs and so
on ok so that's I'll call that the
question of universality today I'm
really going to focus on I'm talking
about global structure ok global
structure for iid edge weights ok and
let me just quickly make one observation
that comes from the algorithm I just
described ok if i if i take the complete
graph on n vertices and i want to form
just to understand the metric structure
of the minimum spanning tree then as
long as I use iid weights with some
continuous distribution it doesn't
matter what the weights are why is that
well the procedure grrrrrr if I just
described starts by ordering the edges
in increasing order of weight and as
long as you have iid continuous edge
weights and that's that's just starting
from a uniformly random permutation of
the edges ok now and once you have that
permutation everything else about the
procedure is determined so the
distributional properties of the tree if
you don't care about the weights
themselves only rely on this fact so
you're really free to choose the weights
you want we're going to end up choosing
uniform 01 weights to get a nice
coupling ok so here's
and here's a description of the global
structure of this tree so if you take
the minimum spanning tree which I'll
call TN so this is a random tree and
rescale it so that each edge has length
n to the minus one third in mass one
over N so the total mass of the object
is is one and you call this rescaled
metric space measured metric space MN
then for some suitable notion of
convergence in distribution em n
converges in distribution to some random
limiting metric space ok so there's sort
of to fully explain this theorem I need
to tell you sort of a few things what
this notion of convergence in
distribution is there sort of some work
to make sense of what exactly this is
telling you for today I'd like to just
focus on one thing about this theorem
which is this rescaling here the fact
that the edges should be rescaled by n
to the minus a third so that the
limiting objects compact ok and and so
that statement is really equivalent to
this statement that if you look at the
diameter of the minimum spanning tree of
the complete graph with these ID
continuous edge weights then that
diameters of order n to the one third so
if you want to get a compact non
degenerate limit you should rescale buy
into the one-touch so so so this is this
is some older work I should mention
who's who I'm working with on this slide
nico de puta Christina Goldschmidt and
lego MMO and this theorem which I and a
focus on is joint work with Nikolai
burrata and Bruce Reid okay so any
questions about the result okay
so there was a conference paper or we
described the results in the journal
version appeared in 2009 we proved it in
2006 and we proved it again in 2009 that
was another possibility independently
and simultaneously um okay so here's our
setup we're gonna have the complete
graph I said now I'm making a choice for
my edge weights uniforms I do you know
from 01 edge weights okay and remember I
the point I made a minute ago which is
that in the in the process where you add
the edges one at a time if you're only
concerned about the connectivity
structure the set of components a set of
the vertices of each component it
doesn't matter whether you had all the
edges are just a sub or just the the
tree edges okay and that provides us
probabilistic Lee now with the coupling
with the classical era shrine your
random graph g NP so i'll remind you
that the irish on your graph process
uses these uniforms 01 edge weights and
then for a parameter P between 0 and 1
you only keep the edges of weight at
most p okay and then every edge in the
resulting graph average is independently
presents probability P okay and we can
do the same thing for the tree just
throw away all the edges of the tree
with weight bigger than P okay that
gives us some forest in general right
because we've taken a tree and thrown
away some edges and the point is that
the connective components of the forests
are exactly the same as the connected
components of the tree yeah okay so if
we want to understand the global
structure of this tree then the time at
which that global structure if we view
that p is a time parameter then the time
at which that global structure forms
will be the same time at which the sort
of a twitch GN P becomes hooked up into
into one connected graph okay so in
particular its classical that the
connectivity thresholds for GNP is that
log n over n P
login / in so we certainly don't need to
go past login / n to understand the
final ministry TN but in fact okay the
bulk of the structure is formed much
earlier around p is one over in okay and
I'll make that precise so I'm writing
everything the interesting curzon p is
around 1 over N so so here's a comic
sketch of what I mean if you look just
at just a bit less than 1 over N at
point nine nine over n then all the all
the connected components of GNP have
logarithmic sighs okay and so that will
also be true for the tree so if we're
trying to build something of size of
order n to the third which you can at
least believe you know I'm going to
convince you of that but if that is
indeed the case then nothing really
interesting has formed nothing really
substantial has formed on that scale at
this time on the other hand if we go to
submit past one over n then there's
already a unique component of linear
size and all the other components have
logarithmic sighs okay so you can
somehow imagine that if all the other
components have it most logarithmic
sighs then plausibly they're not going
to affect the diameter of the final tree
very much even after they hook on to to
to that largest component right we know
at this time then there's there's going
to be a component of this forest of
linear size and all these logarithmic
little pieces plausibly won't change
thanks very much I'll trolley I'll try
to convince you of that but but this
comic picture should at least motivate
why we want to study what's happening
around p is equal to 1 over n this is
really the time at which the structure
of the MST is going to be built okay so
i'm about to focus in on what's
happening around p equals 1 over N and
describe to you how we're going to
understand the structure of T and P at
that time before I do that I just want
to make sure that this this picture is
clear so this this is already an
important thing to understand right when
we're just below 1 over N they have
components of logarithmic sighs just
above we have one component of linear
size
all the others logarithmic let's look at
what happens in the am at that at that
break point one over in okay so it turns
out that at this point the components of
GN 1 over n are pretty tree like okay
this is good news for us if we're trying
to understand the minimum spanning tree
by some sort of comparison with this
graph process that in fact the graph
processes built is building us things
that are already pretty close to being
trees if you look at the largest number
of edges the largest number of edges
more than a tree that any of these
components has okay that's some random
quantity but it stays bounded in
probability as n gets larger in other
words that's it forms a tight sequence
of random variables that's in fact I
mean there's a there's a the expectation
states bandit of the largest number of
cycles okay so that's good and most of
them in fact will already trees okay so
a component of GNP that happens to be a
tree is also exactly a component of tnp
right it's the same set of vertices it
must be the same set of edges if it's
just a tree okay so how big are these
things and what's their diameter well
turns out that they have size around
into the two thirds okay this was this
was already as asserted in the classical
1960 paper of erudition Rennie on the
subject maybe 1959 hey but the fact that
that one over in the size should be
around into the two thirds is certainly
a classical result and more importantly
for this talk the diameter of these
components of these largest components
is around into the one-third okay so it
means that if you give me an epsilon i
can give you k such that the probability
it's at least n to the two-thirds over k
and at most K into the two-thirds is at
least 1 minus epsilon so so so you can
bound it in an interval
a multiplicative intro interval around n
to the two-thirds with probability as
close to one as you like lithography no
so thanks yeah yeah and I'm gonna I mean
basically everything I say in the talk
will hold in the sense of improbability
and also in expectation and I'm going to
sort of gloss that because everything
that in fact happens with very high
probability so for the most part I won't
be too careful about that but feel free
to ask if if something is unclear ok so
I'll sort of try to justify this
diameter into the one-third to you okay
but let me remark before I do so that
this already yields a lower bound on the
diameter of the resulting tree okay
because if I take a component of the
graph and it has diameter end to the
one-third well the tree has we know that
the forest has that same component right
and so the tree that spans the minimum
spanning tree of that component well
what is it do it takes all of the edges
of the graph and then it removes some of
them that can only increase the diameter
so that component of the tree must also
have diameter at least and to the
one-third and on the other hand that
that bit of tree is some subtree of the
final minimum spanning tree so the
minimum spanning tree must have diameter
at least 10 to the third ok so this
immediately yields the lower bound ok I
said that already um let me now explain
where the end of the one-third comes
from so let's suppose right here's
here's the vertices 1 up to n right and
let's suppose you give me some
particular set of vertices s and you
tell me that s is a component of GN 1
over N ok well we know that it's pretty
tree like I said it it has sort of it
at most a few edges more than a tree and
it has a decent chance of actually being
a tree okay so suppose so we're
supposing s is a component of g + 1 over
N and let's suppose that it actually is
a tree ok then the symmetry the model
tells you any spanning tree of that set
of vertices is equally likely to appear
under that conditioning right so so we
know it's a tree which tree is it well
any for any particular tree to occur
that gives you M minus 1 edges that have
to occur and the rest of the edges have
to not occur plus no edges to the
compliment that means for any tree your
conditioning on the same number of
you're asking for the same number of
edges and non edges and so any tree is
equally likely ok so if I tell you that
the components a tree it's a uniformly
random spanning tree of its set of
vertices and uniform spanning trees if I
give you a uniformly random spanning
tree of n vertices that's that's that's
the same as a uniformly random tree on n
vertices that and it has lambda root em
right that's that's classical ok so so
then the diameter that's probably too
low to see but it's written on the slide
as well diameter rudham here m is equal
to n to the two-thirds take the square
root you get into the one-third that's
where the end of the one-third comes
from ok and that's and I that's true
that's literally true if it happens to
be a tree if it has a few more edges in
it than a tree then its distribution is
not too different from what you'd get if
it really was a tree ok that takes a
little bit of thought but it's pretty
straightforward it's a tree with
probability yeah you found it away from
zero as intense
so that at least explains the lower
bound plus some idea of where the
scaling comes into the one-third comes
from if you already believe this okay so
for an upper bound we have to do some
more work right we have now we have we
said we have some components of this
size right and any given one of those
components is going to have diameter
into the one-third already to get an
upper bound we need to say that after
all those components hook up and all the
smaller components hook up that the
diameter doesn't get too much bigger
than this into the one-third so we need
to understand how these components are
going to connect together ok so the sort
of fundamental idea for how we do that
is is is a straightforward deterministic
lemma that I'll tell you in a second
before I tell it to you since we thought
we've already started talking about the
structure of the critical random graph
I'd like to say a little bit more about
that because now we're going to start
increasing the value of P I told you
what this looks like it p exactly went
over in ok and on the other hand my
comet what I call the comic slide said
that once we're at one point 0 1 over N
we've sort of already determined the
global structure so we need to
understand what's going on for P between
1 over N + 1 point 0 1 over N okay which
is called the barely super critical
phase and the following is a picture of
that so let's look at GN 1 plus epsilon
over N yeah epsilon small and order the
components by size then here's the
picture if epsilon is small enough then
you can think of epsilon as being 0
we're really at the critical phase and
small enough turns out to mean n to the
minus a third ok so if epsilon is this
small then then then we're back on the
picture of the previous page all the
components have size around into the two
thirds and diameter around into the
one-third okay the largest components
have to have those properties ok you can
you can easily convince yourself that
this value of epsilon is at least
plausible
because if you take two components of
size n to the two-thirds then there's n
to the four thirds non edges between
them and once one of those non edges
comes along and becomes an edge they'll
hook up and form a bigger component so
the characteristic time on which two of
this sort of largest components will
hook up is around 1 over N to the four
thirds and that's precisely what you get
by plugging n to the minus one third
over N in there okay so this is this is
this is reasonable okay once you get
much bigger than n to the minus a third
but still smaller than n the picture is
sort of in a way much more uniform okay
it always looks here we had sort of two
lines to say that this is you know more
or less up to constant factors once
epsilon is much bigger than n to the
minus one third a giant component a
unique largest component is already
forming that has size around 2 epsilon n
okay and it turns out this is sort of
more recent but also sort of well
studied and well known facts the second
largest component has size
asymptotically to over epsilon squared
log epsilon cube den okay and this end
so so we don't I'm not saying anything
about the diameter of this one but it is
classically known that the second
largest component has this size and the
second largest diameter which might not
be the diameter of the second largest
component yeah the largest diameter
outside of this giant guy over here is
is of order 1 over epsilon log epsilon
cube 10 ok so these you don't have to
worry too much about these logs they
disappear when epsilon is n to the minus
a third and for the purposes of this
talk you're not going to be in bad shape
if you pretend they're not there and
just treat this as 1 over epsilon
squared and diameter 1 over epsilon okay
so you can think of that kind of picture
I'll come back to some of this again in
a minute when I need it but but it's
it's good at least to have seen it right
okay so here's the deterministic lemma
that I promised okay that's going to
tell us how to understand how these
components hook up okay the picture
that's worth drawing for you is that
rather than just looking at all the
connections as they form we're going to
do the following thing we're going to
treat the largest component of the
growing tree as special okay and we're
going to look at how it increases in
size and diameter in stages so first
we'll increase p a little bit and see
what new connections form that are not
to this component so that that are
between other components okay and then
once we've raised p a little bit and
seeing some new connections come in over
here we'll look at how those new
components connect to this graph okay so
that's that's the rough picture and
that's the picture you should have in
mind when I tell you this this lemma ok
so I'm defining some quantity which is
just the it's LT for longest path the
largest number of vertices on any path
in a connected graph okay that's that's
certainly an upper bound on the diameter
and it's essentially equal to the
diameter if we're talking about trees
the diameter is usually in terms of
edges which is the reason for that minus
1 ok so if I take if if I take some
fixed graph G yeah and two sub graphs of
G but one bigger than the other ok so I
have to sub graphs and then spanning
trees of each of those graphs this is
the switch this is the situation I have
with my coupling between GN P and T and
P right I've got increase in graphs
that's GNP and spanning trees of those
graphs
and I say what's the diameter of T Prime
in terms of t1 upper bound is whatever
damage i starred with plus twice the
length of any the longest length of any
path outside of that tree okay so let me
this is this is this is quite obvious
when you draw the picture it's just
saying that you had some spanning tree
of of H right you now you have a
spanning tree of H prime how much could
the diameter have grown well at most
there's you take some path here some
path here right and any path in t prime
can be decomposed in this way into some
portion that's in in H prime and then a
path and H and then another path in H
prime that the fact that it's a tree
guarantees you that that you can't have
you can't have more bits than this okay
so that means that gives you this upper
bound immediately this twice the longest
path is just the length and upper bound
lengths of these two paths and this is
enough for bound on the length of the
middle bit okay is that is that clear
okay so how are we going to apply that
so let's go back to to our coupling and
order the components by size just like I
said before right then this tells us
that if if what if when we increase from
P to P prime for some values P and P
prime if what if which component is the
largest hasn't changed right then we can
apply the lemma to immediately to get an
upper bound of this form okay we've got
the the new diameter is it mostly old
diameter plus twice the longest path
from out here right and now you can
understand where why I was carrying
about the diameters of those those small
components because they're going to come
in as some sort of a band on this
quantity
ok so I'll keep that
so now I want to explain in a little
more detail that picture of the what's
happening in the barely supercritical
range that I just gave to you okay and
this is um they're they're sort of a
variety of references you could cite one
appropriate references as much of the
work in this area due to Thomas would
track in their early to mid-90s the the
particular result that I'm staying here
isn't is essentially from a paper of
1994 so in this range that we were
interested in okay the largest component
has size 2 epsilon n and if you remove
that component okay then what's left
essentially looks like a subcritical
random graph on the remaining vertices
okay so here the parameter was 1 plus
epsilon over N after removing the
largest component what's left looks like
a random graph with parameter 1 minus
epsilon over however many vertices are
left okay so if this if if we just
remove I'd like to explain this a tiny
bit if rather than removing the largest
component I just removed some fixed set
of K vertices well the independence the
model tells me exactly what the
distribution of the rest is it's
distributed like GN minus KP right and
if you want to write P in this forum
then now you have to use something over
N minus K instead of something over N
okay so if you just write rewrite p in
terms of n prime you get one you get
essentially 1 minus epsilon instead of 1
plus epsilon when this is the number of
vertices that you remove okay so that um
the content of this is really that even
so this would be true if we removed some
deterministic set of vertices and it's
still true even if you move around this
particular random set of vertices okay
good
the nice thing about this is that that
tells us that if we look at if we remove
the largest component we what we have is
essentially a subcritical random graph
so we can even increase p a little bit
on what's left and we'll still have a
subcritical random graph right so in
particular if i take p prime to be 1
minus alpha p okay then what's outside
of the giant component now is going to
look like a random graph on the
remaining vertices with this parameter 1
minus alpha times what we had before
okay and in particular if i take alpha
small enough that i'm still subcritical
right so alpha small enough that that 1
plus alpha times 1 minus epsilon is
still less than when I steps alone over
2 ok then I can apply the preceding back
the earlier bounds on the length of the
longest path outside the giant that came
from that picture okay so the point that
again what this is saying is here we're
already at the point where so this this
was my comic sketch right we're already
at the point where there's one big guy
out here we know the largest diameter
was around 1 over epsilon with some
logarithmic correction I told you not to
worry about and that will be true even
after we do the thing that I described
which is scatter a few extra edges out
here okay and these edges come precisely
from increasing the parameter from p 2 1
plus alpha times p okay so even after
you scatter those the diameter is still
small still of this you get a bound of
the same order good okay well that more
or less is is all we need in fact okay
and here's why so let me just this this
is really the key point so let me let me
keep it for a second
okay so here's what we're going to do
we'll start we know what's going on at 1
over N right I described to you that's
how we got a lower bound you know what's
going on at 1 over n we even know what's
going on at 1 over N if we add in a
little bit okay of this form and you
know if to be to be careful you need to
care about how this land the changes
things but you'll have to believe me
that even if I pick some large fixed
lambda the diameter is still only in to
the one-third and that's not that you
can prove using classical facts okay so
we'll start from here and then we'll
start increasing the value of P in this
way that I described we'll add some
edges out here see how they connect to
the largest component add some more
edges see how they connect to the
largest component and we'll do that in a
geometric way okay so all this is this
is my starting point epsilon zero and
then I'll just let epsilon I v 1 plus
alpha to the I times epsilon 0 okay so
each time i multiply epsilon x 1 plus
alpha to get the next one okay and I'll
keep doing that almost all the way up
through the critical window okay so I'm
thinking when I say lambda large and I
want you to think of that as saying
large enough that we can start
pretending that this picture is really
the correct picture that there's one
large component and all the rest SAR
sort of moderately well behaved and that
holds until we get almost 2 epsilon
equals 1 so it's only really epsilon
equals little o of 1 but if but let's
pretend sort of that one over lambda
this large lambda is a proxy for small
so we can continue until we get almost
up to 1 point 0 1 our new one point 0 1
is 1 plus 1 over lambda okay so what
happens this this fact and blew up here
plus the bound from the corollary on the
diameter outside tells you that when you
go from stage I to stage I plus 1 the
diameter increases at most this quantity
with the value epsilon I that we just
wrote okay which as I said if you want
you can pretend it's
one over epsilon I okay good because we
know what we start with and we know what
the difference is at each step right and
I've just rewritten that using the value
of epsilon its end to the one-third and
if you keep track of the log you get I
over 1 plus alpha to the I ok so just
some over the whole range this is this
this sum is dominated by some geometric
sum right and so even if we sum to
infinity these these terms we get
something bounded and so that gets us
all almost all the way through the
critical window and we still haven't
lost more than a extra factor n to the
one third in the diameter so the
diameter may have increased by a
constant maybe even a large constant
factor but not by anything more okay
great so so I mean the this if you if
you believe this it should already
convince you that n to the one third is
the right answer because we said that at
time 1 point 0 1 right 1 plus 1 over
lambda all the other components are
already logarithmic so they shouldn't be
surprising if they had any very large
effect on the on the diameter but let me
really prove that to you as well yeah so
no no no I mean alphas I think I think
you can take out those equal to five
quarters 47 it's really some all you
need is that each time you mobile
software I mean alpha is a quarter so so
remember didn't you haven't talked with
one
all 31 I had 1 plus alpha times 1 minus
epsilon is less than 1 minus epsilon
over 2 so
oh so sorry yeah so that was fast this
is what I meant so
so here we have so we have a random
graph with peas around 1 minus epsilon
over n right and so we can afford 1
minus epsilon over two over N yeah so so
i can take alpha yeah i can take alpha
like epsilon over 2 and
so let me let me write this say it again
well you're right this is 1 plus P is 1
over n plus lambda over into the four
thirds okay then then to get from
yeah so I can so that's fine so I so so
epsilon is so i can take epsilon like
lambda over to over into the four thirds
of the so i can like an anchor i can
increase right from from 1 minus lambda
over n to the four thirds 2 1 minus n
minus lambda over 2 and to the four
thirds in the first step yeah and when i
do that the parameter P increases right
so P prime then is like 1 plus 5 lambda
over 4 1 over n plus 5 lambda / for /
into the Four Thirds right okay so still
choosing to be excited
this one dies the end
so
so so that I mean the white is that
epsilon needs to geometrically increase
write epsilon is a geometrical increase
so that this sum can geometrically
decrease so I so I may have missed
parameterised alpha in this statement
but the point but but even if alpha is
epsilon over to right then we still get
an we still get a geometric increase in
the parameter epsilon I if we add these
if we add these epsilon over tues at
each step right so so so then so then
epsilon i plus 1 it's really epsilon I x
1 plus doesn't see I don't see how it
can work the way you wrote it with one
pass out if you got busy with in the
upper have to be too small and you love
pirates or so maybe I missing some yeah
but if you basically are choosing each
time your outfit to be the current caps
are over 2 yes then only the first you
out rather small yes and then they grow
geometrical in itself
because here you seem to fix our stuff
ya know yep you're correct so that that
that came from hastily preparing the
slides and not looking at my old paper
but no you're right so so this so alpha
needs to be small when epsilon is small
but if you take alpha I mean in the
first step epsilon gets a constant
factor bigger than you can take an alpha
constant factor bigger rain and the
point is that then these I mean the key
point is that these epsilon I if you do
that if you start with it with if alpha
I then is of the form constant greater
than one to the I / n to the Four Thirds
right then these then the corresponding
epsilon I are still decreasing
geometrically quickly and so the idea
the epsilon inverse of decreasing dimage
using so
so so now you want instead of alpha 1
alpha hi yes how do you define them
saying hi so I just want to do so just
psychic in the finishes so so as the
lobby let's take alpha i equal spiders
orders 5 quarters to the i alpha 9
that's let's
this is little else to increase charity
so I think this is fun perhaps Alonzo
what am I mean no and then each and then
don't know that s what is out
epsilon i plus 1 is is 1 plus alpha I
times epsilon I so that means that the
epsilon eyes are going like expertly
nice but no se because of the war plus
so yeah so so sorry for the confusion is
crawl but is that is a picture now clear
so so let's um let's go on from here
though I mean so so we can essentially
repeat the same idea right starting now
from the barely super critical phase to
go all the way up to the threshold for
connectivity ok so now right we want to
go for example from 1 plus 1 over 1 over
n plus 1 over lambda n 2 1 over n plus
five quarters over lambda n right and
that increase in the parameter the
duality principle still tells us that
what what is outside of the largest
component looks like a subcritical
random graph with parameter 1 1 over n
prime minus alpha over n prime ok and
the point is that once once epsilon is
of the order one is not much smaller
than 1 ok the sort of 1 plus epsilon 1
minus epsilon dichotomy can't literally
be true because for epsilon greater than
1 it's sort of trivially obviously false
ok but there's still a duality between
the super critical but between the super
critical graph and some sub critical
random graph after you remove the
largest component ok and in that sub
critical random graph all the components
will have logarithmic size even after
you increase the parameter a little bit
ok by some constant factor and so now
starting from 1 over lambda n we all
need to increase this the parameter by a
constant factor logarithmic Lee many
times to get all the way to one ok so
even if at each step in what's left we
we do the worst thing which is increase
the diameter by an additive factor log
in at the end we've only increased
things by at most log squared in ok so I
mean you can in fact right the
two arguments in that these two steps in
a unified way I mean in both parts
you're you're you're just applying this
diameter band for judiciously chosen
sequence of increasing sequence of
probabilities okay but sort of
conceptually things look a little
different when you're in this critical
window and when you're above the
critical window so I prefer to present
it like that okay so I think that's all
I have prepared for today Thank You mr.
Danny to lay some other no you look at
it in which is finally get a basketball
baseball yeah I think that you can you
can get logarithm log log n if you're
willing to spend a tiny bit more and I
think there's somewhere comment should
know i think i'd only point so then I
mean gonna be a poster that posters in
your office yes some other planted
Krishna subliminally absorbed it
so the this one so this is this is like
spring electro I think it's called
spring electrical embedding it's a some
mathematical package the the one on the
first slide is just uniform points in
the plane this one is really a minimum
value have the complete graph with some
embedding in the plane so the colors the
colors in this picture represent edge
weights so all the black edges have
weight at most 1 over N and then again
colors this time colors code the weights
of edges starting from red and
increasing to violet legs yeah just not
have the appropriate links the edges
have lights given by summer getting of
the complete graph after the innocent
left up into the plane disappears let's
say you have just met
do you also have concentration of the
Damned around
so you work through 0131 so you won't
have I don't expect you to have anything
to bid on the lower tail though I have
to think about it so what do you mean
but I mean there's no specific concept
in front of you I mean we are if we can
we can show in know we I mean we can
show in a week in a different paper that
in fact the diameter divided by into the
one-third converges in distribution and
that the limiting random variable has a
density so it's it's not it's not
appointments but the distribution is it
something something very calm yeah i
mean it's it's we don't find an explicit
formula for it
period of the tales
I expect that if you worked you could
get some sort of at least exponential
and possibly guess Ian upper tail bands
so we didn't explicitly state those
but I expect you to expect you could at
least extract Exponential's
so I mean the did I am the point is that
these at you know everything everything
that this the one of the one question
for answering that is how what sort of
tale bounds do you have on the damn it
the largest diameter outside of the
largest component and there you can get
quite strong bounds which I believe are
Gaussian so then you need to understand
how these have these increments
contribute to some final band but I mean
I expect with some work you could again
get a big se entail bandeau to that
so maximum degree
sure there no I don't
the Maxum degree
look
but what do you work with the
conductance is something right because
their centers but I mean there's some
there's some explicit formula for the
degree of the root which comes from some
mixture of Poisson distributions and and
and you should essentially have
independence between different parts the
tree so a guess based on that is that it
should be like log n over log log n um
but I haven't proved that
okay think we'll continue this</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>