<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Achieving the Full Potential of State Machine Replication | Coder Coacher - Coaching Coders</title><meta content="Achieving the Full Potential of State Machine Replication - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Achieving the Full Potential of State Machine Replication</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/b8HgFVwmK6o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I mean I'm a lot of things from having
me here this talk is going to be
essentially about revisiting Paxos and
I'm going to try to convince it that
that's indeed as good as it sounds but
just not in that way
so the general topic for my thesis work
is fault tolerance in distributed
systems and like fault tolerance in any
other domain this is mostly about
redundancy the way we achieve we've done
insane distributed systems is through
stable application the way we do it is
we have a process that's replicated on
modular machines so we know even if some
of these machines fail the remaining
ones will be able to handle client
queries and commands just as the failed
ones would have and to keep the states
of these processes in sync what we do is
we implement them as state machines that
means they change their internal state
donors or result of executing the
commands proposed by the clients of the
system and then we make sure that they
execute the same commands in the same
order and this technique is called say
machine replication I'm going to use
this shortcut SMR and sanction
replication is really one of the most
important primitives that we have in
distributed systems it is important for
example in local area clusters systems
such as chubbie box with smarter
zookeeper they use the admission
application to implement operations as
diverse as replicating data resource
discovery distributed synchronization
and because we've built larger and
larger clusters there is increasing
pressure on these implementations of
state machine replication to have higher
throughput and availability we also use
state machine application in the wider
area because we have databases that are
being accessed simultaneously by clients
on different continents we want to bring
that data closer to clients and at the
same time we want to be able to tolerate
full datacenter outages and because in
this setting
distances are so large any inefficiency
in extra unnecessary message delays or
on trips to commit is going to have a
high impact on line
so low-latency in this setting is very
important and there are many ways to
influence a machine replication they
range from simple primary backup
protocols to more complex protocols I
just backed those and visiting pol
tolerance now all the systems that I've
mentioned in the previous slides they
implement variants of faxes and the
reason is package is popular because it
has apparently the right trade-off
between safety that is what kind of
failures can tolerate and performance so
what do I mean by performance maxis is
as fast or faster than primary backup
protocols and because it does not depend
on external failure detectors it has
very high availability and in explain
what I mean by that by contrasting it
with a primary backup system let's say
we have a primary backup setup and at
some point there's a network partition
if you were to let clients talk to both
the primary and the backup there's
tastes will diverge say double primary
will divert say backup which is exactly
the opposite of what we want to achieve
so in this situation what we do is we
have this external entity an external
failure sector that decides which of the
two copies has officially failed and
chooses the other one as the official
Authority copy of the data but of course
it is going to take a sum is it going to
take a time between the primary or the
inner opposition setting in and the
external failure detector choosing the
backup to be the new copy of the data so
during that time the system is going to
be unavailable for new commands yeah
it's it's yeah exactly it's a taker
exactly now impacts us by contrast we
use more resources we have three
replicas instead of to tolerate one
failure but any network partition or any
replicas appearing to have failed will
not cause the remaining majority of
replicas to stall they will be able to
continue processing commands and in fact
we don't have to
your synchronously here we can just
continue operating with the majority of
the replicas that means that faxes and
systems like faxes have almost instant
failover which is to say very high
availability and this brings me to the
overarching goal my thesis work so in my
thesis work I want to improve state
machine replication focusing on packs of
style signature application that is to
say failures are non Byzantine and I
focus on protocols that that use quorum
consensus and I want to improve
signature application of multiple
practically important dimensions but
also in a way that's well anchored in
theory and and here's what I mean by
that so a batch of systems has
essentially two components
there's the practice algorithm itself
which is this core general nice
algorithmic core and then there are the
implementation considerations how do we
choose to implement a certain feature of
a algorithm how do we optimize a certain
performance characteristic that's
important to our application and I want
to improve state machine application in
a way that also expands this nice
algorithm core to include some of the
more practical implementation details
the reason why I want to do that is
because I want the results to be easily
applicable to a wide range of
applications so in this talk I'm going
to present the two components I've
worked on so far to achieving this goal
the first one is a new state machine
lubrication protocol based on facts we
call it a guy latarian Paxos it achieves
lower latency higher throughput and
better perform stability than previous
finishing your application protocols and
the second one is a technique that
addresses an orthogonal performance
characteristic of signature notification
which is how fast can we read the state
of these replicated state machines sorry
we get there in practice but it is
useful to go through a quick practice
overview first so patches at its core is
an agreement protocol used by multiple
processes to agree on one thing it
tolerates F failures with two flaws on
total replicas and that's optimal
because we don't depend on external
failure detectors replicas
can fail by crashing through because are
not sorry if failures are non Byzantine
which is say that replicas can fail to
reply for an indefinite amount of time
but they will not reply in ways that do
not conform to the protocol and finally
communication is asynchronous so with
any assumptions about the synchronicity
of communication for the protocol to be
safe to be able to make progress however
we do rely on there being periods of
synchrony so how do we use this protocol
that it's an agreement is an agreement
protocol to agree on a sequence of
commands that we execute because
remember that's the goal in same machine
replication and I'm going to show you
this through an example let's say we
have three right guys this is the
replicated state anything on of these
replicas has a copy of this but I'm just
going to show one on this diagram for
simplicity and this is essentially a pre
ordered sequence of slots of command
slots they're initially empty clients
will submit commands to these replicas
and they will contend for these slots by
running taxes as a result of running
taxes only one of them will win and
everyone will know which one that was so
everyone will put command B in slot
number one the rip you guys had lost
this slide which is a different store
they will contend for a different slide
and so on after as a contiguous sequence
of slots has been filled every rep you
can independently execute the same
sequence of commands thus their states
will be kept in sync so the takeaways
here are that with batches we choose
commands independently for each slot so
there is a separate instantiation of
taxes for each of these pre ordered
slots and in canonical packs it takes at
least two round trips to commit a
command the first one is to take
ownership of a slot and the second one
is to actually propose the command
because of this inefficiency we would I
really like to come to be able to come
in commands after just one roundtrip
practical systems implement something
that's called multi axis in
tobacco's one of the replicas is the
pre-established leader over all the
slides and then clients will only talk
to this one replica this one replica
which we call the stable leader will
choose which command goes into each side
and because no longer has to take
ownership of of each slide individually
it is the Polly stablish owner of
everything it can commit commands after
just one round chip that's a good
question
it doesn't need so it can be a
bottleneck for both performance because
it has to handle more messages for each
command then all the other non leader
replicas and also a bottling for
availability because there's a there's a
time between a leader failing and the
new leader being elected where no one
can make progress so the the complicated
parts of taxes coming here when a new
leader is elected yes so we can actually
use any any protocol we want to choose a
new leader but by running Paxos we are
guaranteed to be safe so we are we're
guarantee that even if two replicas
believe they are leaders at the same
time we are still safe and that's all
taxes gives us so the question that
motivated our research is can we have it
all can we have the high throughput and
low latency of multi Paxos but at the
same time keep the nice properties of of
canonical backs which are constant
availability can we also distribute load
evenly across all replicas instead of
having just want this one stable leader
hotspot that can be a bottleneck for
performance at the same time can we use
the fastest replicas in the system to
commit a command for instance there
might be concurrent jobs running on our
cluster that might make some of our
replicas slow as we would ideally want
to be able to avoid them serve you guys
and finally you can use the closest
reputa that be a geographically closest
replica so that we can get low latency
in the white area and canonical practice
has all these properties except for high
perform
multi-bank who solves that but it loses
the other properties in the process so
yeah they're impacts by contrast has all
these properties and implement them
efficiently so much so that has higher
performance in multiple axes and the
practice is all about ordering so we've
seen the previous strategies for
ordering commands were having replicas
content for slots this was the case for
canonical boxes having one replicas
decide and this was multi values but
also other variants of taxes called fast
axes and generalized axes and finally a
newer axis variant called Mencius where
replicas take turns in proposing
commands in this setting it is pre
established that the first replica is
the leader of every third slot starting
at one the second replica is the leader
of every third slot starting at two and
so on
mencius is effective at balancing load
and that's because we have many of these
commands being proposed at the same time
so every replica will simultaneously
have the same functions so every replica
is both a leader and accept her for some
commands unfortunately the problem that
mencius has is that before committing a
command for committing a slot we have to
learn what happened in all the previous
slots and remember the previous slots
belong to every replica in the system so
we have to learn some information from
everywhere in the system that makes men
see us running the speed of the slowest
replicas and it also means that the
system is going to be temporarily
unavailable whenever any single replica
is temporarily unavailable so it depends
on the exact setup if it's a single
cluster set up the client can just
choose arbitrarily and random if it's a
distributed if it's a dual replicated
setup the client can choose its closest
drive account so mencius works even if
clients choose a
and America so I can send the command to
one replica and then the next command
that's logically connected to my first
one to another replica and it works so
by contrasting access we take a very
different approach instead of having
this linear command slot space we split
it into as many subspaces as there are
replicas and we give default ownership
over one subspace to each of the
replicas so imagine the replicated state
now is this by dimensional array
everyone has a copy of this but only the
first 3pk is allowed to propose commands
on the first row only the second replica
is allowed to propose coincidence second
row and so on so clients can now choose
any replica to propose commands to and
that replicas will lead the commit
process in one of the slots that it is
the default leader of and that's good
because there's no longer contention for
slots but how exactly do we order these
commands right does become after or
before D so in the process of choosing
commands in slots we also choose what we
call ordering constraints and this says
that B has a an ordering constraint on a
which is say that command B should
follow a and we do that where every
single command and when we've committed
a slot we also committed with its
ordering constraints and every non
faulty replica will see the same command
in the same slot with the exact same
ordering constraints so therefore all
140 replicas can analyze this dependency
graph and come up with the exact same
ordering decision and the takeaways here
are that we leave passes we get a load
balancing because all the replicas are
leaders at the same time command leaders
we call them and very importantly if
axis has the flexibility to choose any
quorum of replicas to commit a command
so it's no longer the case that there is
one special replica like the stable
leader in multi pack so that has to be
on the decision path for every decision
on the critical path for the decision
and there's no longer the case that we
have to get some information from every
replica in the system right so we have
this flexibility to choose just any
replicas to commit a comment it
now we've got the problem of
establishing what these ordering
constraints are there might be N squared
so this might've made it worse
oh well it's not it might be in squared
of them it's not N squared of them and
I'm going to show you why so how exactly
do we get these ordering constraints
this is an example that I'm going to
show you it's a time sequence diagram
times flow from left to right we have
five replicas and let's assume that at
some point command a is proposed add
replica 1 now R 1 sends a pre except for
a pre accept is the way we call the
first round messages right so this pre
accept tells the majority of replicas
which is itself and replicas are 2 &amp;amp; 3
that a depends on nothing because r1 has
seen no other command at this point
right so the ordering constraints for
for a you know aren't the void set are
to the north you agree because they also
haven't seen any other command at this
point and because this 2 acceptor is
agree with each other
r1 can commit locally and then notify
everyone else asynchronously including
the the client and let's assume that at
about the same time command B is
proposed at reputa 5 now our five
doesn't know anything about a at this
point it hasn't received any message for
a so it says that B depends on nothing
and our 4 agrees but of course our 3 has
seen a pre except for a before seeing a
pre except for B so it says B has to
depend on a now because the two
acceptors disagree with each other our 5
has to take the union of this kind of
these ordering dependencies and then
tell everyone this is the final set of
constraints for B bead has to be pen on
a it does this through an accept message
now even if there were other concurrent
comments at this point accept messages
or the dependencies in accept messages
they do not have to be updated anymore
so it takes at most two round-trips to
commit a command in impacts us accept
messages are just acknowledged when a
majority has acknowledged the command
leader our 5 can commit B and then
notify everyone else asynchronously and
one last example let's say that C is
proposed at r1 C as depend on a says r1
because it only knows about a at this
point but of course r2 and r3 have both
same commits for B
and they say see depends on both a and B
because the two agreed with each other
the two acceptors are one can commit
locally and then notify everyone as easy
so a simple analysis of this protocol
would show you that it takes one ROM
chip to commit commands when commands
are non concurrent but for some commands
that are concurrent it may take up to
two runs to commit and that's not that's
not ideal we wanted to avoid having to
undergo two runs to commit so to our
rescue countries observation made before
us by generalized taxes and before
generalized practice by generic
broadcast protocols which is that we
don't actually have to order every
command with respect to every other
command consistently we only have to
order those commands that refer to the
same state so for example imagine that
we have a replicated key value store two
puts two different keys we don't have to
execute them in the same order on every
replica as long as we execute in both so
what this means for our protocol is that
it'll take one wrong chip to commit
those commands that are non concurrent
or they are concurrent but not
interfering and it'll take at most two
rounds to commit those commands or some
of those commands that are both
concurrent and interfering and it turns
out that in practice it's really the
case that commands are both concurrent
and interviewing so mostly we pastors
will be able to commit commands actually
just wrong object but the next logical
question is how do we know that two
commands interfere before executing them
because remember we first have to
determine interference to be able to
order them and then only after we have
ordered them can we execute them so the
the answer to this question is
application specific for no SQL systems
which are variants of key value stores
it is quite quite easy we just look at
the operation key if for two operations
the key is the same then they interfere
otherwise they do not or we can take the
approach that you can buy Google App
Engine which requires application
developers to specify the transaction
key and only those transactions does
that have the same key are guaranteed to
be ordered
consistently across all recognized
and finally it's easy or it's it's
feasible or rather to automatically
infer interference even for more complex
databases like in relational databases
because it turns out that most of ltp
workloads have these simple transactions
that we can analyze beforehand and we
can tell for sure for sure which table
and which row in which table they will
touch and so for these most frequent
simple transactions it is very easy to
determine that they can interfere or not
and for the remaining few transactions
that that are complex and we cannot tell
for certain one data they will touch it
is safe to just assume that they
interfere with everything else so the
more general definition is that we allow
commutative operations to be committed
in any order that's right
and of course what I said which was that
we allow non interfering or or commands
that refer to different parts of the
states to to be committed in any order
is subsumed by by your commutativity
story so we've established its ordering
constraints how exactly do we do we
parse the graph and and execute these
commands so this is an example let's say
we have five commands ABCDE and the
edges here represent origin constraints
and this is how a set of origin
constraints might look like now this is
a directed graph it's not necessarily a
cyclic so what we do is we find a
strongly connected components and then
this graph we're strong and components
are super nodes this is a dag a directed
acyclic graph so we can sort it
topologically and then in inverse
topological order we can execute a
strong IV components for a component
that has more than one command we
execute those commands in increasing
order of their approximate sequence
number this is a next role in constrain
that I haven't described but a simple
way to think about it is it's
essentially a lamp or clock
so what this gets us is these the whole
point was that you are establishing a
total order so where did this loot come
from fails right so when we when we
establish these already constrains we
can get cycles right that's bad so the
way we solve cycles is we have these
extraordinary constraints which are much
simpler they are just a lamp or clock
right so there's there's a they're
essentially a counter that's updating a
certain way for every command and then
inside strongly related components which
means inside cycles we execute those
commands in increasing order of that
lamp or clock that's a good question
that's a good question so we can get
into a situation where I guess the
simplest answer the simplest way to put
that is that the space the domain for
this lamp or clock doesn't have to be
completely covered so we can have a
single number of five or a command but
that does not mean that we have commands
that have Seamas numbers one through
four so we made we may wait forever for
something to have sequence number four
so that's why we need the first type of
ordering constraints these edges in the
graph if there's a cycle right so if in
in most cases a directed edge does mean
that the the command that the arrow
points to is going to be first in some
cases that will not be the case and
that's those are the cases where
commands are being proposed concurrently
and messages for some commands arrived
with some replicas first and for other
commands that are messages were of so
required the replicas there are messages
for other commands that are right first
that's how we get cycles and we have to
solve them and the way we sold them is
we use this the simple lamp or clock new
members of this cycle
no new members of this samba getting
invited to go to alive later exactly if
the command has been committed so if the
command has been committed we are
certain that a majority of replicas have
the sorry the the last version of the
lamp or clock is ever going to be
updated for that command right and what
that gives us is this nice property
called linear eyes ability so linearize
ability is a strong consistency property
in distributed systems it's assumed
serializability and furthermore it says
that if two commands interfere two
commands a and B interfere and one of
them is committed at some replica before
the other one has been proposed at any
replica then the one that the first one
will be executed consistently first at
every rep behind the system another nice
property of e Paxos is the it said fast
path quorums and by fast path curves I
mean what is the subset of acceptors
including the command leader that have
to agree on the initial set of ordering
constraints for that command to be
committed in the Fastpass so the size of
fast path chorim's is f + 0 5 over 2
where F is the maximum number of
concurrent tolerated failures now what
this means is that we are optimal for
the most common elements of boxes which
are 3 &amp;amp; 5 repeaters respectively and we
are better than previous versions of
optimized for low latency which are
fastened generalize factors by exactly 1
replicas now what this means in practice
is that if we do geo replication with a
setup like this or we have already came
in Japan to on the u.s. west coast 1 the
US East Coast and one in Europe the rib
cage Japan can commit a command after
just talking to its close to closest
neighbors it doesn't have to go all the
way to Europe or all the way to the east
coast as these protocols would have
would have to so we've implemented back
those along with the other practice
variants and we tested it on Amazon ec2
the workload that we used was a
replicated key value store this is why
Duryea a wide area set up is the same
set up that I described earlier we have
a replica in North Virginia one in north
California one in Oregon Ireland and
Japan and this is to give you a sense of
the wrong
trips between those sites and so we had
clients at every location proposing
commands and measuring how long it takes
for the command to be committed and this
diagram I'm going to show the median
latency experienced by clients at every
one of these sites for every one of
these protocols deep access Mencius
generalized access in multi boxes so for
a client of V paksas the latency that it
will experience it'll be that of its
co-located replicas to go to its two
closest neighbors jar Oregon and
Virginia and that's the same for a
client of multi faxes but only because
the multi patches client is in
California for a client in Virginia
however it first has to go to the stable
leader of multi packs was in California
wait for that command to be committed
and then wait for the leader to get back
to do it so the latency is going to be
higher mencius has to wait for some
information from every replica in the
system including the replicas that are
furthest away so it has higher latency
and generalized axis has larger fast
path quorum so it has higher latency any
taxes and at every location impacts us
has the lowest latency of all the
protocols and the reason is deep axis is
optimal has optimal commute latency in
the white area for three and five
replicas now of course this refers to
the situation when concurrent commands
not interfere or afford for those
commands that that you know interfere
with the other concurrent commands when
interference does occur for concurrent
commands then some of them will incur
latency it's double this however we
believe it that's rare enough that it
affects only tail latency 99 percentile
latency and also there are ways to
mitigate that latency that we we haven't
implemented yet but are pretty
straightforward
any passes also helps in the local in
your local area cluster this is a
comparison of the throughput achieved
with multi packs of Mencius and EEP axis
for various rates of command
interference zero percent means that no
concurrent commands interfere 100
percent means that all commands
interfere and what you can see here is
the deep axis has
I throughput in the previous protocols
when interference is low furthermore
when replicas are slow in this case one
replica was slow for multi vessels in
had to be the the leader otherwise the
throughput of multi faxes will not have
decreased significantly but so when the
multi biases leader is slow or any
replica in menses or e Paxos is slow the
performance of e faxes degrades more
gracefully than for the other protocols
and the reason is we have the
flexibility to simply avoid wear because
it are slow and you might wonder what
happens with batching because in multi
packs is where we have a single leader
getting all the commands there is a
higher opportunity for batching it can
just take all those commands and commit
it in one batch now this is a latency
versus throughput graph the y scale is
the y axis log scale in this graph it's
better to be lower because that means
lower latency and more to the right
because that means higher throughput
this is the the curve for multi paksas
this is a curve for you pack so suspect
was as much better throughput about
three times the throughput at about ten
times over
latency because the load of
communicating with clients with all the
clients that propose commands is spread
across all the replicas in the system
instead of falling squarely on the
shoulders of the single leader multi
faxes replica and this is he faxes zero
percent interference effects with 100
percent returns is essentially about the
same and the reason is the cost of the
second round of communication is
amortized across the many commands in
one batch second round messages only
have to update dependencies that don't
have to send the commands again right so
there are very small messages and
finally yes
and the latency right exactly so how
would those graphs change if you change
the matching window size doesn't have
any yeah so it's a good question so the
way we did it this minimum ladies you
would increase but media would point out
that that's not necessarily the case so
there is there's a there's a nice
straightforward way to implement
matching so that this minimum latency
doesn't increase and no matter what the
the batching window size you choose you
just simply send the commands that you
have a discerning point just you don't
wait for the batching window to expire
but it also mean that tail throughput
might increase a little bit now the the
batching we know that we use five
milliseconds was large enough that it
wouldn't increase too much here at least
not in our setup so a nice side benefit
of e practice is that it has constant
availability and to show you this I'm
going to show you a sequence of
throughput overtime graphs this is for
multi pack sirs and it has stable
throughput until the leader fails this
only happens if the leader fails if a
nonlinear replica fails throughput
essentially remains the same but if the
leader fails the throughput will
temporarily go to zero before a new
leader is elected and that's the same
for mencius but here are any replicas
failing will cause this this behavior
with EEP axis however the through P
never goes to zero it does decrease here
these are this is a setup with 3
replicas it decreases by about a third
because the clients that we're talking
to that replica that failed have to
timeout before talking to different
replicas of throughput decreases
temporarily the failure of the old
leader so I'm going to try now to
disentangle some of the main insights
into taxes why why does it work or why
does it work better than previous state
machinery education protocols and the
main
insight is that we deal with ordering
explicitly instead of having instead of
dealing with ordering implicitly by just
committing commands in this pre ordered
sequence of sorts we
you take ordering we put into our
protocol and this allows us to get
higher throughput because all the
replicas are command leaders at the same
time it gives us better performance
ability because we have the flexibility
to avoid wear because that are either
slow or far away and low latency because
into your application scenarios we can
just talk to because they are closest to
us yes
Oren couldn't you just run it up but
couldn't you just define a canonical
ordering on this table that you have
yeah and that give you Mencius which was
the previous protocol that I that we've
compared against so with Mencia is the
problem if you have a pre-established
order is that you depend on replicas
that own the previous slots for for you
to be able to commit have to wait for
the difference between having by
dimensional and array where we have a
pre establish order between between
cells and having a uni dimensional array
I believe there's no difference between
those booster variants it's just a
different diagram but it's the same
logical thing yes best contrast if you
look at main stream network in the
literature and it's all about the
latency as if people have been showing
that they are willing to give up on
average latency to prove the tale
because an average things are pretty
right so we haven't shown these I
haven't shown this results in the talk
we have them in the HP paper we're doing
we do improve IMC and we can improve it
even more with techniques that we do
described in the paper but we haven't
implementing the paper so they did not
affect the results
it will be worse than some of the
protocols it will be better than for
example generalized faxes it might be
worse than Mencius or more like faxes
but it depends on it depends on the on
the exact setup yeah so let's look at
yeah worldÃµs describing literature like
for example spanner chubby and we
concluded based on based on their
description of the workloads that
combine interference is actually very
low it's it's under it's under 1% so
ordering commands explicitly that gives
us these nice benefits furthermore it
allows us to only optimize those glazed
it matter because previous practice
protocols that optimized for low latency
like fast attacks and generalized
patches they try to do away with the
first message delay that between the
client and the first replica but of
course in your application scenarios
replicas are usually co-located with
sorry clients are usually co-located
with their closest replicas so that
first message delay does not matter
instead we optimize for smaller chorim's
which help us with loading and I will
point out that we have a formal proof of
correctness for paxos we also have a TL
e+ specification that's modeled
checkable and we released the
implementation of e p-- axis and all the
other protocols open-source so at the
beginning of the talk I was I was I was
describing how I plan to improve state
machine replication I believe that if
axis goes some way into achieving that
it has higher throughput optimally low
wider yeah latency it has better
performance robustness in previous
protocols and also constant availability
as and there's a nice side effect we
don't have to do leader election because
there's no leader but can we improve
other important aspects of signature
notification and
so so you there are multiple dimensions
- yeah well there are multiple
dimensions to what it means to have a
good statement chamber or a good
replicated state machine now these are
some of the obvious performance
characteristics throughput latency
performance robustness but there are
others well we are better on the ones
that we've considered so far we are for
example as you - was pointing out we
sometimes trade off tail latency for
media agency and of course if axis is
more complex it takes you know more work
to understanding it implemented
correctly it is more work to prove that
an implementation is correct so there's
that problem and of course you also have
to understand semantics about the
workload to be able to assess clan
interference so one other important
aspect of replicating machine is
orthogonal to the the things that
impacts us addresses is read performance
how do we read very quickly from
replicating state machine and I'm going
to show you why weeds are different from
normal commands so of course what we can
do is reveal is we can just simply treat
it as a as any other commands now we
have a client and now we have a multi
pack so system and the client tries to
read home replicas the replica will
forward the command to the leader the
read command the leader will simply
committed just as it would with any
other command it would then execute it
and send it back to the client perhaps
through the replicas that initially got
the the message right because maybe
that's the closest regular client this
involves a lot of communication so it's
not going to be super efficient a better
way to do it is to have the client send
the command to any replica that the
replica talks to any quorum of coming
column of replicas maybe the the closest
replica is they wait for the ongoing
commands to be committed and executed
and
they returned the result and the client
will take just the result that belongs
to the most advanced log of update
commands this is better it's less
communication than when we treat weeds
it's just in the other command it does
however involve some communication so
it's not perfect now practical systems
use variants of time leases the most
common Dainese is a leader lease here
the leader has a time lease for every
object in the system the client that
means that if the leader fails the new
leader is not able to update any state
until the leader until the the lease of
the old leader has expired and that
gives the old leader the guarantee that
as long as the lease is active it can
safely read the freshest version of
every object in the system right so
clients can now simply talk to that
stable leader it'll do it from its local
store and just get back the result this
is great for those times that are
co-located with the leader it's not
great for clients that are not
co-located with the leader perhaps in
other data centers it does make a leader
hot but so the discussion in the second
part of the talk applies to any variant
of taxes not just deep access right it
applies to multi packs or C packs or the
general effects and so on any process
you wouldn't have a leader but if you
were to implement this single leader
sorry single replica Elise you would
have the same problems it reads you
would be able to commit rights
everywhere but you wouldn't be able to
read except for at one location one
replica another approach is taken by
Google's mega store system instead of
giving the lease to one replica it gives
the list to every replica in the system
and now this has great right great read
performance because but we can be
service locally everywhere
however we pay the price for this better
read performance when we do writes
because now writes have to synchronously
reach every single replica in the system
that means that writes have higher
latency then if we didn't have them this
release and it also means that the
system is going to be temporarily
unavailable for rights whenever any
replica in the system becomes
unavailable so this is the price of a
here now if we were to look at the
design space for time leases we would
see that previous solutions essentially
are these two points there's the leader
lease which has almost as good a right
performance as when we don't use leases
at all but it's we performance is far
from ideal and then we have Megastore
leases which have great read performance
but they pay the price with a poor write
performance in our work we try to
explore the spacing between and the way
we do that is through something that we
call core laces and coordinators allow
us to arbitrarily move along this space
and furthermore more I will argue that
quorum leases give us most of the
benefits of the two previous design
points and the API is actually very
simple instead of giving the least two
one replicas or all the replicas we give
the least two any subset of the replicas
and we make the observation that the
quorum communication necessary for
committing updates impactors induces a
natural leasing strategy and that is we
might be at a sweet spot if we give the
lease to those replicas in a quorum so
if we have two synchronously update
replicas in a quorum before we can in
turn up before we commit a right we
might as well give those replicas the
ability to reload that's a great
question no so there's no there's not a
single reason this doesn't have to be a
single is we can have multiple leases
for overlapping currents and these
leases these leases will refer to
different objects if we have a
replicated key value store at least
might refer to a subset of the keys
another lease held by a different quorum
will refer to a disjoint subsets of the
keys
and the assumption that make this work
well is that different objects are hot
different replicas so imagine a
replicated social networking graph
clients or a data center in Europe would
likely see more demand for those objects
that pertain to theirs in Europe whereas
data centers in the US would likely see
more commands for those objects that
pertain to users in the US so that's why
we believe this assumption is realistic
now I'm stead of laying out our design
I'm just going to present the challenges
we're implementing the score Emily sits
and I'll be happy to talk about them
with you I want to ask me about them the
most important question is which quorum
should hold at least for each object and
it quickly becomes apparent that core
analysis should be dynamic when we had
leader leases or all replicas have the
lease that was a static assignment it
was very easy to say who holds the lease
for what right the single leader lease
holds leaks for everything or everywhere
we got house lease for everything here
however we should adapt to access
patterns
there's no point holding a lease at a
replica that will never try to read a
command us or an object and we need to
do this we need to establish maintain
and update lease is going to be able to
migrate objects from different chorim's
for many objects millions of objects
maybe many millions of objects
simultaneously with minimal bandwidth
overhead and we have to do all this
while maintaining the safety and and
strong consensus in guarantees of both
the underlying paxos algorithm and
maintain strong consistent guarantees
for our reads and the results that we
have so far for for our implementation
of core leases are encouraging we use
the Y CSV benchmark with a skewed
workload that is the workload
distribution is uses the European
distribution and for wider yeah the same
set up that I've shown you earlier we
get 60 to 90 percent of all reads at
every location our local and a similar
percentage of writes have the minimum
latency that can be achieved in the
system
with a bit of engineering I'm pretty
sure we can get this number to more than
90% for skewed workloads and leases are
also better in the in the local area
compared to single layer leases they
give us 4.6 X higher throughput on five
replicas this is not quite as good as
Megastore leases it's about 10% lower
than megasaur leases but it also with
coronation we also don't pay the the
price that megazor leases force us to
pay which is lower right availability
and slightly increase latency so in
summary I've described ways to improve
state machine replication and they
included ultimately low latency in the
white area higher throughput and higher
performance stability and all these were
due to a new station application
protocol we call it egalitarian taxes I
also described how we can get high read
performance without sacrificing write
performance and this was due to choruses
and I believe that there's extensive
room in this space for our future work I
want to explore reconfiguration
protocols that do not reduce the
availability of the system and also to
not impose high overheads and throughput
I would also like to explore further
uses a wall clock time beyond leases so
what happens if for instance we have
synchronous clocks I mean get better
implementations of you faxes and we
believe we can or conversely what
happens if we don't have synchronized
clocks but we want it consistency
guarantees similar to systems that do
have synchronize clocks like spanner we
want to get for example snapshot reads
can we do so with only quorum leases and
again I believe that there might be
wasted but you can do that and finally I
think it'll be interesting to put this
all together and have general
transactions spanning multiple impacts
those groups and increase the read-only
transaction performance with score
analysis so in the last slide of this
talk I'm going to point out that I've
also worked on other projects be beyond
the same unification I worked on
practical data structures
and also on redesigning systems around
upcoming non-volatile memory
technologies and if your interests align
with these please ask about them thank
you yes sounds wonderful right and as
you described it it's it applies to high
value systems
you know big data centers so small wins
generates lots of dollars so is
everybody abandoning their current
limitations and busily writing pen
source implementations that is our hope
but obviously it's not going to be that
easy first of all as I said if X is a
more complex protocol it takes longer to
understand it and implement it there are
efforts currently to to implement new
taxes in various places we we just hope
that it's going to go well yes right as
far as I know no right non Byzantine or
sometimes called benign failures is it
all non Byzantine absolutely yes so for
real to be honest I I'm not sure how
how relevant it is for real world
systems I I see that many of these big
Internet companies they settle for non
Byzantine five years for for tolerating
normies nineteen failures and that's why
that's what I address in my work I think
it could become important I think it's a
valid area of research it's not it's not
very widely applied as far as I know
well I have a couple of comments so
first I didn't think that the comparison
with things like nations and multi taxes
are fair because they don't really rely
on dependencies of finding dependencies
across operations which if you go to the
key value store example there might be
treated but I find that a bit naive if a
big if a big applications like zookeeper
or applications also keep they use they
use Z nodes in in very complex ways
if the dependencies are not always clear
so so it sounds like you're trading off
performance with the difficulty or or
best given it making the role of
everything we depended on everything
else how bad would it be everything
would be so not everything but most
commands would have to undergo two
round-trips commit to not not exactly so
in the graph that I showed you with with
batching the performance of V packs was
even when all commands interfered was
was about the same as when no commands
interfered and that is because we simply
do a better job of distributing the load
across all replicas even if we do have
to undergo the second round of
communication those messages in the
second round are so small that for
throughput they essentially don't matter
nobody but of you is writing that if you
if you really cannot differentiate
between different commands and you
cannot tell whether commands interfere
then there would essentially be no point
in applying this for example in the
white area because you probably won't
get that many latency savings unless you
have very few commands being concurrent
so so yes you do have to have more
application specific knowledge
absolutely if you don't then probably
you're better off you know using
mencius or multiplexers
the question I have is about recovery so
when do we make sure that I didn't fail
you and you have all seven that are not
leaderless
the delivery flavors didn't help to
recover is something about electing you
either when a server process crashes and
recovers he needs surgery when you have
a leader it's it's logically simply
because could you yeah so I think when I
talk about this I qualified it as you
know as long as a minority of replicas
fail then we have constant availability
obviously if we have more if we have a
majority of replicas failure failing
then you know the protocol is not going
to be live in your definition and and
indeed when we bring new replicas into
the system there is going to be a
challenge to do so without impacting
throughput very much right because we
have to update this in the state of the
reserve you guys and there are ways to
do it that have been exploring the
literature that we can do checkpointing
you know bulk data transfer and they
apply to keep access as well
you had the question all right let's
think again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>