<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microsoft Azure Storage: Blobs and Tables | Coder Coacher - Coaching Coders</title><meta content="Microsoft Azure Storage: Blobs and Tables - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microsoft Azure Storage: Blobs and Tables</b></h2><h5 class="post__date">2016-08-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gLQ9tU7gexc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in this video we'll look in more depth
at two specific asier storage services
The Blob service and the table service
if you would like background information
on Azure storage in general please watch
the precursor video as your storage
overview first we'll talk about blob
storage the blob service provides
storage for entities such as binary
files and text files every blob has a
name and it has a unique URL as shown
here the REST API for the blob service
exposes to resources containers and
blobs a container is just a set of blobs
and every blob must belong to a
container unlike a traditional file
system there is not an arbitrary
hierarchical nesting of containers
within containers however blob names
themselves can include the slash
character so you can produce something
that looks like a tree hierarchy using
this naming trick a storage account can
contain unlimited number of containers
containers are useful because access
policies and metadata are set at the
container level they are not set at the
individual blob level the metadata is
limited to 8 kilobytes of arbitrary name
value pairs there's a special dollar
sign root container which serves as the
default container for your storage
account the root container is also
useful when serving Silverlight and
Flash out of blob storage you may need
to store cross-domain access policy
files in the root of the domain the
Windows Azure storage infrastructure
strives to maintain a constant 60
megabytes per second throughput for each
blob this criteria is what all the
internal load balancing machinery
targets the blob service defines two
types of blobs block blobs which are
optimized for streaming and page blobs
which are optimized for random
read/write operations
page blobs provide the ability to write
to a range of bytes in the middle of a
blob they're handled very differently in
the azure API and they are handled
internally very differently by Azure
block blobs should be thought of as a
sequence of blocks there is a limit of
200 gigabytes for a block blob they're
intended for streaming workloads page
blobs have a much larger size limit of
one terabyte they are intended for
things like virtual machine hard drives
all blobs are read with a get blob
operation
which can specify what range of lights
to read writing content to a blob
however is very different depending on
whether it's a page or block blob to
write content to a page blob you call
the put page operation the rights to
page blobs happen in place and are
immediately committed updating a block
blob is more involved lock blobs less
than or equal to 64 megabytes in size
can be uploaded by calling the put blob
operation lock blobs larger than 64
megabytes must be uploaded as a set of
blocks each of which must be less than
or equal to 4 megabytes then a call to
put block list is called to commit these
newly uploaded blocks into the blob
itself if you do not call this function
then the uncommitted blocks will
eventually be discarded by the azure
system one nice aspect of this is that
you can upload many blocks in parallel
out of order the order of blocks in the
call to put block list is what
ultimately gets committed into the blob
this slide illustrates another aspect of
page blobs they are aligned on 512 byte
boundaries each right to a page blob can
overwrite up to 4 megabytes worth of
pages rights to pages happen immediately
unlike with block blobs now let's look
at the table service the table service
provides structured schema-less storage
in the form of tables it has a REST API
that is compliant with the a deonette
Data Services REST API as yours table
storage should not be thought of as a
traditional relational database table it
does have functions like query create
and delete but there's no fixed schema
and there are no relational semantics
the rows of the table are called
entities and they support functions like
insert update merge up search replace
delete and query every entity or row as
you can think of it in the table service
can have up to 255 properties the total
data in an entity cannot exceed one
megabyte each entity must have a
partition key and a row key it also has
a timestamp the combination of the
partition key and the row key forms a
primary key that identifies each entity
uniquely within the table
tables are partitioned behind the scenes
by Azure to support load balancing
across multiple storage nodes all the
other properties in the entity are
simple key value pairs the type of the
value is just one of the standard net
types string binary boolean date/time
and the like but again just to reiterate
there's no fixed schema built into the
table service itself a developer may
choose to enforce a schema on the client
side here we see that a table can have a
ragged shape with one row having a field
than other rows do not there's no strong
typing values in a column may have
different varying types wearing the
table service is per the ato net data
services specification in general you
should endeavor to always include the
partition key to limit the scope of your
query to partitions always served by a
single storage node previously I
mentioned that the partition key allows
a sure to scale queries to the table
service across multiple storage nodes
but it serves other purposes too since
entities with the same partition key are
stored together the cache performance of
doing table scans will be much better
within a single partition also if you
wish to group multiple write operations
into a single atomic transaction then
they must exist within the same table
partition naturally it follows that you
should not use distinct partition keys
for every single entity this slide
illustrates an automatic feature of
Azure table storage range partitions
will group entities that have
sequentially unique partition key values
to improve the performance of range
queries this prevents a range query from
needing to cross partition boundaries or
server boundaries which can negatively
impact performance the azure table
service uses partitions to automatically
load balanced reads and writes to your
data when there's a lot more load Azure
will move partitions to additional
servers and fan out your data as load
decreases as your will condense the data
back down into a fewer number of servers
you do not pay any extra for usage of
these servers it is automatic and the
scalable load balancing is built-in to
the pricing of Azure storage itself in
this video I'll use the ipython notebook
because this makes it easier for me to
show blocks of code and what's going on
if you want to follow along you can
place these code snippets into a plain
Python script you don't have to use the
ipython notebook in order to use the
azure Python SDK to start the notebook I
just type ipython notebook I'll create a
new notebook and we'll get started with
some imports these are several libraries
we'll need including the CSV module and
of course the azure dot storage as your
SDK module now what I've done is placed
my account name and my key directly into
a module in this directory called as
your secret this is to avoid having to
type them explicitly as strings here and
then needing to blur them out for the
rest of the video I'll also define our
container and with this information we
should be ready to create a handle to
the blob service now we can write a
little code that lists all of the blobs
inside to that particular container now
we can programmatically write a new file
create a temporary file here in our
current directory and we can upload it
to a sure
by going back to Azure storage Explorer
and hitting refresh we can see that
indeed sample 2 was uploaded and we can
view its contents we can also delete it
and once again we'll check in as your
storage Explorer that indeed has been
deleted now let's do something more
interesting we'll grab the contents of
the CSV file stored in the blob and
we'll create a new table out of all the
rows in that CSV file
we can check that there's a lot of data
there we even print out some of it
great now we'll create a new table
and now we'll ask the table service to
actually create the table now what we'll
do is write some code which will first
split the contents of the CSV file on
new lines and we'll just take the first
20 lines of that CSV file and using the
dict reader from the csv module we'll
convert every single line into a
dictionary of key value pairs then using
those keys and values we will create a
new as your entity object and then at
the end here we'll tell the table
service to insert that entity into our
new table we just created finally we'll
print the row to see what our successful
insertion looks like now we can see it
going and inserting every row one at a
time and now when it's complete we can
go over to as your storage Explorer and
click on the tables button hit query and
we'll see all of our rows inserted in
this video we looked in more detail at
the blob and table storage services of
azure these provide a very flexible data
model for many kinds of applications and
scalability and reliability is built
into their design as you design
next-generation data processing and
analysis codes it's worth thinking about
using Azure storage from the start so
that your code can naturally scale and
is naturally fault tolerant for more
technical information on these and many
code examples please visit MSDN and
windows azure comm thanks for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>