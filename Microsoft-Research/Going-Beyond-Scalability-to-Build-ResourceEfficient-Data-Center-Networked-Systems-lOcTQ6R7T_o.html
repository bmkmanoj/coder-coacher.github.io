<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Going Beyond Scalability to Build Resource-Efficient Data Center Networked Systems | Coder Coacher - Coaching Coders</title><meta content="Going Beyond Scalability to Build Resource-Efficient Data Center Networked Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Going Beyond Scalability to Build Resource-Efficient Data Center Networked Systems</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lOcTQ6R7T_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright thank you for coming we will get
started it's my pleasure to have George
Porter here George is from UC San Diego
he got his PhD back in 2005 2008 sorry
from from Berkeley and he spent the
majority of his time since then at UCSD
you know George from his data center
work he's been doing a bunch of work
with a bunch of fabulous students at UC
San Diego he's going to tell us about
that George also took over pretty much
all the days on a research from a mean
vedad when he left and went to Google
and George is also helping manage and
run the Center for network systems at
UCSD awesome thank you alright thanks
thanks shared and thank you all for the
opportunity to come and give you a talk
today it's a real pleasure to be here I
don't think I need to tell this audience
this but please ask questions during my
talk if you have any questions are
coming so all right so my name is George
Porter and I'm going to be talking today
about building resource efficient
data-intensive applications so it's
incredible how much of our lives has
moved online from information gathering
entertainment collaborative tools health
care medicine government effectively
everything we do in some way has has
moved online and each of these
applications is fundamentally driven by
data the quality of your user experience
using all of these sites depends on in
some way the quantity of data that each
of these applications can process over
and it's not just that these things are
driven by data you know it's sort of
like if you think of the way Amazon uses
data to build say product
recommendations Spotify uses data to
build custom radio stations being uses
data to build personalized search and it
isn't just that they're data-driven but
they're also data-driven on a per-user
basis so for example if we look at the
way Amazon works you know when you visit
the main sort of landing page on
amazon.com the page that gets generated
is customized to you and in fact each
time you access this page there's over
100 underlying applications that are
doing things like collaborative
filtering consulting with ad networks
your previous purchase histories
preferences etc and each of these
appliqué
nations is driven by data and so there's
an enormous amount of data processing at
i/o that goes on ahead of time behind
the scenes before you arrive at these
pages in order to generate all the data
that's needed to consult during that
request so that this content can be
customized to you and so all of these
different applications and data
processing requirements have driven the
need for very large data center
deployments so in order to scale up to
meet the needs of all of these different
users companies like Microsoft Google
Facebook and others have developed these
large data centers which are warehouse
scale building buildings housing sort of
tens to hundreds of thousands of servers
storage networking gear cooling power
etc and the result of these data centers
has been an incredible amount of
scalability and so for example you know
Google serves out a hundred billion
searches per month Facebook has a
billion active users amazon has actually
is closer to about 200 million active
users today and this incredible
scalability result has had to you know
has been arrived at in an incredibly
short amount of time so if you actually
last month was the 25th anniversary of
the web and about 20 years ago the first
sort of mainstream web browser was
released and back then really everything
was basically small-scale so Google's
first data center fit on a folding card
table and supposedly the Facebook you
know the first Facebook server was sort
of run out of a dorm room type
environment and so those numbers that I
just quoted to you you know 100 billion
searches a month and a billion active
users has had to been developed in about
15 years and 10 years respectfully and I
think even at established organization
as established companies the you're
going to see similar scaling results for
anything that's kind of user facing and
so these organizations have really had
to be driven by a relentless focus on
scalability and so in order to close the
gap between no users and effectively the
internet connected world's population in
order ten years everything that's been
developed has had to focus on
scalability so the data centers that I
described the applications that run in
those data centers the infrastructure
that underlie all of those applications
the storage infrastructure all of that
is really driven to be able to grow as
as possible and effectively grow at any
cost and this costs are incredible so I
don't have to tell you that there's
enormous capital expenses in terms of
building each of these data centers and
so the way you can think of that of
course is that every time you want to
roll out a new application or every time
you want to grow to a new set of users
you have to stamp out effectively one of
these billion-dollar buildings but
they're also incredibly expensive to
operate as well with sort of industry
estimates at tens of billions of
kilowatts hours a kind of industry wide
and yet underlying all of this
impressive scalability results lies an
enormous amount of inefficiency so again
industry estimates about six to twelve
percent of that power actually gets
translated into productive work and the
question becomes sort of why is that gap
exists like why is all of their this
inefficiency in terms of these data
intensive applications and one of the
main sources of inefficiency really
comes down to I oh it's really about
input-output you can think of this in
terms of aisle bottlenecks between
distributed applications and the
underlying data that lives on the
storage layer underneath them and
there's also enormous amounts of
bottlenecks in between nodes sort of in
a distributed cluster shuffling data
between each other and these bottlenecks
result in servers that end up waiting
for data so this is referred to as like
a pipeline bubble or a pipeline stall
where in OneNote is waiting for another
know to complete and before it can make
progress it has to wait till that data
arrives from that other node and so this
can cause these sort of cascading
performance failures we're in
large-scale systems end up spending a
lot of time waiting on data and this can
also kind of manifest in terms of
requiring a much larger compute in the
storage footprint than you would
otherwise need if you just looked at the
amount of processing needed to make the
data these applications work and so what
we really need to do is to focus on
recapturing I efficiency and this boils
down to kind of a very simple
application of on dolls law which is we
want to if we look at data-intensive
applications where Iowa's really the
bottleneck we need to eliminate any
unnecessary iOS that we can and for
those iOS that are necessary we want to
make sure that there is efficient as
possible so in the rest of the talk I'm
going to talk about this in effect in
two different domains which I'll get to
in a second
so stepping back for a second if we kind
of look at the last 25 years we've
really been focusing on this goal of
scale and achieving systems that are
able to scale and as we kind of pivot
and look towards the you know future
it's important that we develop systems
that are able to scale efficiently in
order to deal with you know growing user
populations and growing data set sizes
and so the work that i'm going to
ascribe falls into these two domains the
first is on Io efficient data processing
and I'm going to talk about some work
that my students myself and my
colleagues have worked on in terms of
building very large-scale efficient
sorting systems and using those to build
large-scale data processing systems and
the second domain is on the node to node
getting rid of node to node bottlenecks
to the system so focusing on Io
efficient data intensive networking and
we've been looking at data center
interconnect designs that rely on
circuit switching in addition to sort of
more traditional packet switching models
and combining those and we've been able
to show that this approach will allow
you to sort of cut the cost of your
network infrastructure by two point
eight X and the power x 6x and I'll go
into these in a little bit in a second
yes I always have a big problem in
so are their numbers is there a
quantitative way to show that I was
important yeah I was just this is the
big problem one of the big big problems
here physically you can imagine which I
pulled up nothing's go to sleep pretty
laughing oh it's good yeah yeah that's a
great question there are so when we sort
of so I guess you could look at this in
two different ways and one of them like
you said is to make systems of somehow
power proportional so that the amount of
power they draw matches the kind of
resource utilization that they're at and
there's definitely work in that and
that's an important line of work to do
but it seems like at least in today's
systems even if you either things aren't
very power power proportional so there's
a lot of overhead a lot of costs
associated with keeping all the machines
running and so you're really better off
trying to drive as much throughput
through your system as you can and sort
of maxed out all of your hardware that
that's a sort of a more efficient I
guess point in this space and I hope
that in the in kind of describing this
work I'm going to go through some
quantitative analysis of where some of
these bottlenecks are I think I'll
answer your question that there is in
fact you can you can actually see where
where those bottlenecks are great okay
so I want to talk first about IO
efficient data processing and I want to
define I would start out by kind of
defining what I mean when I say
data-intensive I've used that term a
couple times already and over the last
30 year 30 years or so the definition of
what makes something data-intensive has
changed quite a bit so in sort of the
mid-80s it might be able to say 100
megabytes as a data intensive job and
today it's maybe 100 terabytes or even a
petabyte and so over this 30 year time
span there's been effectively a million
fold increase in what we mean by data
intensive and so over that time period
the types of applications that have been
used to solve these jobs has changed
quite a bit and so today if you talk
about data intensive computing a lot of
times what you mean is for example
MapReduce this is a what what is a
representative example of a sort of data
intensive framework for doing processing
and MapReduce is actually not a new idea
if you're a list programmer you've been
using it for some time but if you're not
familiar with it I'll really briefly
describe it right now
so in a MapReduce program you're given
as input a set of key value pairs and
you start by applying a user supplies
map function to each of these pairs you
then group that the results of that
function application by key and you sort
each group and then finally you apply a
user-supplied reduce function to each of
these groups and so if we kind of zoom
in to what's going on in the
implementation of this programming model
we see that the application of the map
function and the application of the
reduce function which i'm going to call
map tasks and reduce tasks this is
what's called embarrassingly parallel
meaning that we can execute these
functions entirely node local without
any network communication and so what
we're really left with in terms of
building a MapReduce framework is
exactly this group by in this sort
operation and in a lot of ways this is
really the hard part about building
these large scale systems because it's
almost exactly opposite of
embarrassingly parallel generally
speaking data from each of these nodes
has to be shuffled conveyed and
delivered to each of these destinations
and the application of these functions
really is Bala neck based on all of
these iOS completing and so managing and
dealing with all of this i/o is an
enormous challenge and we're not the
first people to really identify that
it's a huge challenge actually in the
mid-80s the now late Jim Gray wanted to
focus people's attention on the
importance of the i/o subsystem when
building data processing systems so
looking beyond sort of you know how many
floating-point operations per second and
starting to look at a holistic view of
the system where you see kind of iOS as
a part of this and so the way that he
did this was actually really cool so he
proposed a contest a sorting contest in
which the idea was to see who could sort
100 megabytes of data the fastest and
this was great for two reasons one of
them is that you all we all learn about
sorting sort of freshman year of college
and so everyone involved in these
industry and academic efforts kind of
had a good sense of what it meant to
sort data but the second reason this was
really cool is that across a variety of
different data processing applications
we now have some representative
benchmark or stand-in for what the aisle
performances with the resource
efficiency of these applications is so
we can kind of do an apples-to-apples
comparison so obviously things have
changed quite a bit since the 80s and so
by the late 90s
to a terabyte terror sort record or
contest which was won by the berkeley
now project and then when we started a
project in 2009 the idea was to sort 100
terabytes of data and so this was held
by the Hadoop open-source MapReduce
project that was being hosted at Yahoo
wait so now we've got this really great
benchmark of I efficiency and I was
talking about how systems and practice
are not efficient and so now that we've
got this benchmark let's see how
deployed systems do in practice yeah
think about the shopping genie the
bottom it is true with the song but
under many other jobs with the map think
I fill it out of the data outside step
that is the bottom line yeah that's a
great question to the kind of
selectivity of that map operation it can
vary quite a bit obviously if you're
searching for for data and a very large
data set you're looking for kind of a
needle in a haystack the output of that
map will be a small set to set size I
guess the thing about sword is that the
selectivity of that is one to one so
every output record and so it's kind of
a worst case from the point of value of
i/o performance and so for a lot of jobs
that are low CPU to data item ratio it
looks a lot like sort and so that's why
we've been kind of focusing on is
because oftentimes what you're doing is
comparing items ranking items things
like that that don't require a lot of
CPU per item but you do end up having to
convey all this information somewhere
else quantification focus on this is one
to one business sale on
so that's a good point to uh I'm trying
to think I mean so there are you can
find evidence of this in various
published pieces of work and it varies
depending on the organization and I
think the audience in this room would
have a much better sense of what exactly
that CDF looks like that I would so I'd
love to chat with you about it to these
site you can talk about okay so great
now we've got this benchmark of what we
mean by resource efficiency and let's
see how well deployed systems do in
practice so in 2010 these two
researchers from HP Labs looked at the
results of that grace or contests and
they looked at all the winners and the
way they analyze that data was as
follows what they did was they took the
delivered performance of each of those
sorting systems and they compared it to
the inherent capabilities of the
underlying hardware platform and they
looked at that difference and the
results were surprising and incredibly
discouraging so on average ninety-four
percent of disk i/o is idle and about a
third of CPU capacity was idle and this
is among the winners so you know this is
this is definitely not good and if we
kind of look more specifically at this
2009 Yahoo result they were able to sort
100 terabytes of data with 30 452 nodes
in about three hours which is quite
impressive but if you actually kind of
look at what each node is contributing
to that overall result what you see is
that each of the disks in the system is
in some sense running at approximately
one percent efficiency and so we're in
the situation where we're able to
achieve these very impressive results
via scalability but we're running at
sort of low efficiency and remember
these are these data centers are
incredibly expensive to build and
operate and so the goal that we kind of
like to set out is to be able to achieve
the same data set size result in the
same amount of time but with effectively
two orders of magnitude fewer resources
every tool measure seems a little bit
strange to me the sounds like unless
you're evading your hardware or
specifically the logo you it seems like
you always have some resource that is
not a bottleneck and
they utilized so that's so in that sense
like what is measure really showing us
yeah so it's it's in some what we'd like
to get is a fully perfectly balanced
system right where all of our resources
are balanced with each other so that no
one resource is or I should say if we
were to reduce the amount of any one
resource the entire system should slow
down that's like that's some goal that's
implicit in the work that I'm describing
now as you point out there's a lot of
heterogeneous jobs and so in one set of
workloads or one set of jobs you may end
up with one resources the bottleneck and
in some other particular type of job you
might end up with a different resources
of all night I do think though that
focusing on storage is the bottleneck is
the tack that we've taken because a lot
of systems really our storage IO limited
and so you know using this as a way to
start solving some of the system's
problems of getting the storage io up is
is one of the goals that we've had I
think that can be good you mean that
they have major source of innovation
same sense of it is I King there either
I so yeah either they're idling or
they're fully being utilized and there's
not enough disks to actually keep the
workload that keep all the CPU is busy
or third they're fully being utilized
but there are extra iOS being issued
that aren't necessary that's another way
you can look at it so I hope that in
some way addresses your question another
guy could be that some sort of art no
unbox it storage or CPU since you say
you can see a standard server scheme
rates for yourself 100 terabytes or
excuse would you still have similar
problems because they essentially isn't
using well I'm totally give just gives
one person four hundred percent but as
long as I optimized for that metric
under some standards give them some
shoots out those levels so I think what
you're saying is sort of that you've you
sort of settled in a certain sense on a
binding of compute memory network and
storage and then you're sort of
replicating this unit to the data set
size that you need and I think that's
the way people build real systems right
you sort of provision a server model and
you kind of scale that out that's the
cluster that you build and what I would
say is that inherent in making that that
binding of compute storage networking
and memory your you already have an idea
of what that balances between CPU and IO
either I owe to the network I owe to the
storage so to a certain extent you've
already kind of have this sense that
there's some ratio and that's what year
that's why you build a platform in a
certain way and so when we started this
work and I don't have I didn't describe
it in the the slides here but we knew
that the types of jobs we were going to
be working on were low CPU to i/o and so
we wanted to use servers that has many
dis pot as possible in them and so at
the time we were able to get machines
that had 16 disks but if we had 25 disks
that would have you been even better
absolutely yeah mr. pollow in Revelation
you said that time your goal is to build
a system to the resource and utilization
essentially Palin stepdaughter instance
any one resource you reduce impacts the
system per pound now clear that's a good
code right because resources don't cost
equal amount of money so suppose this
guy was much cheaper than cpu I will be
happy to waste a large amount of disk
i/o as long as my CB utilization yeah I
wrote so you know for example penny sort
was essentially cleared towards doing
something attack yeah or might it so
what why not use that as a poor woman
who most amount of data at least
possible price we actually get to that
so I think I think I want to revisit us
because
one of the things we were focused on was
/ no deficiency and this sort contest
you mentioned penny sore there's also
like a jewel sort contest that we
entered as well and I guess what we
found is that the reason that we've been
focused so much on on disk i/o is that
otherwise you end up with all of the CPU
and memory that's sort of waiting on
basically on disks and you need a lot of
disk whenever you have data-intensive
applications just because of the
capacity issue and and so what we found
was that by kind of driving up the
efficiency of that resource we end up
getting energy efficiency I'll describe
that in just a minute I should stay low
gunfire dollars looking for one yeah and
so again when I talk about the
evaluation I'll mention this little this
sorting contest is not just a absolute
performance contest there are these
different categories and it gets to this
issue that so there is a work done per
watt and there's work done per dollar or
for penny and then they're simply who
can do work the fastest and those aren't
always they don't always lead you to the
same system design and so at a case in
point of this that's kind of interesting
is that for these eco sort for this
jewel sword contest there's kind of like
two solutions to this equation and we've
got one of the solutions and Dave
Anderson's group at CMU has the other
solution and so we've been focusing on
if we can build system that can just
handle raw throughput what we end up
with is even though our servers are 300
watts each and we've got 10 gig
networking and stuff you know you end up
with a very highly efficient system on
the other hand you can focus on Adams
and things like that and get a different
solution it just depends on on your ear
assumptions absolutely so I'll expose
that in just a second and we've been
talking about about this balance issue
and i just want to mention that right
now which is that to a certain extent
this project is is in the context of a
larger set of work on looking at trying
to come up with the right balance of
these different resources in terms of
addressing different types of jobs and
one aspect of balance has to do with the
data itself so in an ideal world we'd
like to divide our cluster up into these
groups and we would like to distribute
our data in a very uniform way across
each of these nodes and then we've got a
variety
processing elements on each nodes which
I'm going to represent with these
funnels that represent in some sense a
CPU or disk type resource and if we're
able to kind of very uniformly
distribute all this data and all these
resources then all of the data can be
processed uniformly and we don't end up
with a lot of pipeline stalls because
data gets generated as it's needed and
that's a very efficient way to design
systems of course the real world is not
as kind to us as we would like it to be
and data can be incredibly non-uniform
so if you look at for example census
data you know Seattle is going to have a
lot more entries in it than driftwood
Texas or something like that and so
these in this imbalance can end up
causing some of the notes to become
bottlenecks which causes this cascading
ripple effect but resources are also
highly heterogeneous as well so some
disks are faster than others but even if
you bought all exactly identical disks
all with the same part numbers you're
going to end up seeing this very wide
variance in delivered performance based
on just the fact you have so many of
these resources put into a into a single
cluster and one of the things is that
this imbalance one of the effects of
this imbalance is you end up with not
just an efficient I oh but actually
wasted iOS so the thing that's
interesting about sorting is that any
external there's this well-known lower
bound which is that any external sorting
algorithm requires that you read and
write each data item at least twice in
the worst case and what we say is that
any system that actually meets that
lower band has this to IO property and
that's one of the goals that we set for
ourselves when we started this work now
that imbalance that i just showed on the
previous slide can result in extra reads
and writes that aren't necessary and
this is due to what's called
intermediate data materialization
meaning that you don't have enough
memory to for example keep your entire
working set in DRAM and so you end up
issuing reads and writes to process that
data iteratively and that's what we
mentioned before about you can have your
disks running at full at a hundred
percent sort of load even though you end
up with extra iOS that are cutting that
effective load down to something like
one percent so it's not that in that
Yahoo cluster the disks were only being
operated at one percent of the time it's
just that one percent of their
performance got delivered into the
aggregate performance of the system and
just like the data can cause and balance
the imbalance disks and lead to this
exact same problem for the reason I
adjustment
okay so we'd like to restore balance and
we do that in two ways statically before
the job begins and then at runtime so
we're going to borrow techniques from
the database community to sample our
data to get a sense of where these
partition boundaries are going to be so
this is research this is things the
databases do all the time and that's how
we figure out what these partition
boundaries are but the key thing is is
that at runtime we still need to impose
balance because even if our data has
been statically allocated correctly do
these partitions because the on-disk
layout of the data can have non
uniformity in it we have to heal that at
runtime and that's what I'm going to
describe in this part of the talk right
now okay so we built us a TI of sorting
system called triton sort that we
presented in n SDI 2011 and it is
structured as follows so instead of many
fine grain tasks processing the data in
a divide-and-conquer approach we have
two phases of operation so in the first
phase the distribution phase we divide
our data up into these partitions based
on that on those samples and then we
read all of our data in in parallel and
we assign each data item to one of these
partitions and in phase one we send it
over the network to the note it belongs
to and we store it in one of these on
disk partitions so at the end of phase
one all the data is on the right node
and it's in the right partition but each
of these partitions is unsorted and so
in phase 2 in parallel across the
cluster we read in each of these
partitions sorted a memory and write it
back out again and we've also sized our
partitions so that we can ensure that
each of these are going to fit into
memory so to see that in action we start
by reading a buffer data offer input
disk we have a process that's assigning
it to these different partitions and
copying it into in memory buffers
designed for the different destination
nodes and then when these buffers get
full we have some code that sends them
over the network to the know they belong
to then on the receiving side as data
arrives we append it to a variety of
these on disk partitions and just to
give you a sense of the numbers that
involved here we've got eight output
disks on our machine and each disk has
about three or four hundred of these
partitions on you can think of it as
three or four hundred ondas files that
store the data in phase 2 we're going to
read one of these unsorted partitions
into memory sorted and right
so the bulk of that pipeline is the
details are in our paper and it's
actually pretty straightforward to
implement and the real complexity of
that system is exactly this partition
appending module because we have to
ensure that we're writing out data to
these disks in large enough batches that
the disks deliver good performance and
so I'll just described very briefly how
we do that now so this module is given
as input on the left a buffer of these
key value pairs and on the right we've
got a set of disks each of them is
holding a couple hundred of these
partitions and there's a thread before
each one of these is ready to write out
data to the disk so the first thing we
did was we implemented the kind of most
straightforward way of doing this which
is to sort of scan through this buffer
of key value pairs and rely on the
operating system to deliver and manage
the i/o for us so we just issued rights
or scatter gather rights or sort of
fancier rights and the result was that
the system performed we had low
performance and the reason for that was
really just due to the fact that there
wasn't enough buffering handled
automatically by the OS to ensure that
the rights getting delivered to these
disks were sufficiently large to run at
near their sequential speed so what we
did was we sort of scrounged up as much
of the memory as we possibly could about
eighty percent of the memory on each
node which was 20 gigabytes and we
managed all the buffering ourselves so
we divided this memory up across all of
our different partitions we copy data
into these partitions and when they get
full we write them out to disk but I
mentioned that there's this non
uniformity of the input data and so it
ends up happening is these partitions
are either really hot or very cold and
so taken as a whole our memory was not
particularly well utilized and so the
result of that meant that our rights
were not particularly very large so what
we ended up doing was building a load
balancer that ran at runtime in front of
our disks and it works as follows we
took that same 20 gigabytes of memory
and now we divided it up into two
million little ten kilo byte buffers
that we stick in a memory pool and so as
data starts arriving from the network we
basically are going to copy into these
little buffers and stick it into a data
structure here and the way this data
structure is organized is that we have a
row for each of our 25 hundred or so
partitions and each of these rows which
corresponds to one partition what we're
going to do is we you know grab a buffer
put there
data in there and then we add it to a
list or a chain of these buffers part
partition and the nice thing about this
data structure is that as partitions
popularity varies during a run even on
short timescales we can extend some of
these change to become longer and then
some of the less popular partitions have
shorter chains but none of our memories
actually being dedicated that isn't
actively being used in parallel with
this there is a process that's
constantly scanning this data structure
and it's looking for the longest length
chain which represents at that incident
time the largest right that we could
issue to the disk at a given time so
what we do is once we find this we pull
it out of this table we send it off to
this thread which is going to write it
off to the appropriate on this partition
and then it's going to take all of these
buffers add them back into the pool and
this is going to be the back pressure
mechanism that we use to push back
pressure back to the producing side of
this pipeline now this handles the
non-uniformity of the actual input data
but I mentioned resources can be
non-uniform as well and so imagine that
we have a couple discs that are slower
than other disks the way this same data
structure actually handles the problem
without requiring any modifications so
this process is constantly scanning for
these chains is only actually looking
for the subset of genes that could be
issued at that given time and so what
that means is if you have a slower disk
the chains behind it are going to build
up and you're going to end up issuing
larger rights to them which is going to
a little bit help mitigate this non
uniformity in some way ok so we've
looked a little bit about how we handle
IO in triton sort and I want to talk
about our evaluation so when we began
the project in terms of the hundred
terabyte grace sort the Hadoop MapReduce
project had been able to sort data point
578 terabytes per minute and then with
Triton sort in 2010 11 and 12 we were
able to sort at point seven to five
terabytes a minute using a cluster of 52
notes and it was just based on these
issues that I just talked about now as
is the case in any particular type of
contest eventually your record is taken
back again and at least in terms of 100
terabyte grace or Hadoop was able to run
at one point food for two terabytes per
minute on 2,200 nodes in 2013 with
more recent version have a dupe there's
some other categories I didn't describe
here like the indie benchmark which you
guys took back from us and so you know
we're working I guess to see if we can
to retake that but I mentioned that it's
not just raw performance that we were
really interested in the point of this
project was to focus on resource
efficiency and the community identified
that as a really important metric as
well and so in 2010 they added at Jule's
for category which exactly captures this
eco-efficiency and we were able to
capture in two thousand eleven and
twelve but also maintain it in 2013 and
the reason for that is because even
though we were beat out in terms of
absolute performance if you just look at
the quotient of the amount of work we
did times the number of nodes we're able
to push about two orders of magnitude
more throughput for through each of our
servers and so that's why we were able
to keep that kind of performance now I I
want to briefly mention fault tolerance
at this point because any system you
bill has to be fault tolerant I don't
have a lot of time to go into the
details here but what I would say is
that the fault tolerance approach that
you adapt really depends on the failure
assumptions that you have in the system
so if failures are really common you on
a pessimistic approach to fall times and
if failures are rare you want an
optimistic approach the Triton sort
pipeline i described require it relies
on very aggressive pipelining so you
know is we're not materializing data at
all whereas something like hadoop
requires materializing intermediate data
in a common case and so what i would say
is that you know we looked at some
publish results for example from Google
in 2010 and kind of what you see is it
at it sort of 10,000 node cluster size
you're seeing failures like every two
minutes or something like that and so
it's really important that you need to
you actually want task to be able to or
you want jobs to be able to survive
individual false and so materializing
job state is probably a really good idea
if you we actually talked to the people
at Cloudera and the average Hadoop
cluster sizes are order something like
30 to 200 notes and even adding an order
magnitude to these number of nodes you
end up with failure rates in the sort of
double-digit hours hundreds of hours
time frame and so here what we're going
to argue is that it's actually okay too
do job level fault tolerance where you
re execute jobs on failure as long as
the performance improvement you get by
running without fault tolerance is high
enough to to overcome the occasional
jabri execution so this is there's no
hard and fast rule here and i think
there's there's sort of a dividing line
depending on exactly what your failure
model is but this is something that
we've been looking at and the future
work that we've been actually Alex
Rasmussen which is the lead student on
the transfer project is we've been
looking at taking trace data and
selectively reacts accusing parts of the
pipeline that have failed or that parts
of the pipeline that depend on a failure
to mitigate the cost of rijeka cuting
this work maybe we can talk about that
offline okay so yeah gains in terms of I
think efficiency coming from resource
balancing versus for tolerance like
compared to competitors well it so if we
look at Hadoop for example there is one
extra materialization that happens after
the map task which we get rid of but
actually there are three different
places in the pipeline where
materialization happens and two of those
places are actually just due to date
askew so what I would say is that
effectively a third of the i/o we get
rid of is due to fault tolerance and to
third is due to resource and balance and
data set data set amounts why does it do
just do this I mean though there are the
practical reasons I they don't good is
there developers from jerks no no no no
wonder you so you should like all these
very nice results go and so you would
think the new guys just say we're just
gonna nom nom that'd be like oh yeah the
Hoodoo people are great I've worked so I
have a I have a patch that's been in
there since 2008 for this and it's part
of the problem is that but it's actually
inserting in a sense they are doing this
so there has been a huge move to these
in memory completely in-memory data
processing sort of applications and in
these cases you're getting rid of
effectively all the data in
materialization at the expense of more
cost but you know I think that
so that's kind of one extreme sort of
getting rid of all of the data
materialization I think though that if
you look at actually where Hadoop's
going with projects like tez which is
like this data flow thing you're you're
saying that they're giving users much
more control over what materializations
they do so right now you kind of fit
into the MapReduce model but if you're
doing something like an iterative job
there is now a lot of support for being
able to control exactly when those
materializations happen so I think you
know I think I think that's happening
after you take your own skew that you
build your on one job at a time right
and she really optimize the heck out of
this yeah but if you now have different
cluster with many jobs of different
sizes maybe not the water sort yeah how
would we just directly be applied you
lose some efficiency we have to tweak
thanks to make it really work yeah so
this is this is great because at a very
high level this first part of the talk
we are giving up statistical
multiplexing and what we're doing is
we're focusing on individual task
efficiency so rather than sort of taking
lots of different tasks that have
heterogeneous requirements putting them
on a system at a time and then coat
scheduling them we are dedicating
resources without using stat mugs and
one thing that I would say is that if
you've got a petabyte cluster and you
have a bunch of 10 terabyte jobs you
have a lot of opportunities so we're
doing stat mux but if your resource
constrained let's say you're a research
group let's say you're a startup you're
working in biology or something like
that you may want to solve petabytes
scale jobs but you only have the
resources to run petabyte scale clusters
and so the question of is stat MUX
better than not stat mugs is an
interesting question when you're not
resource constrained but if you are
resource constrained you can't even ask
that question and so I think that it's
good to focus on things like job and
cluster scheduling obviously if you are
doing stat mugs but it doesn't hurt to
also look at can you actually kind of
pull in as much efficiency as you can
out of individual tasks when you don't
have the opportunity to establish
there are some disabilities yeah I think
if you can get yeah so just to answer
this real quick if you assume that the
if you assume that the compute and the
storage are co-located with each other
you don't have a ton of choice in that
matter but if you separate them for
example like with the blizzard work from
an SDI last week where you actually get
to sort of logically separate storage
and compute you could imagine dedicating
a very tightly connected set of machines
to storage getting full bandwidth that
storage and then when that jobs done now
maybe an order of magnitude more
computers can access that same amount of
storage so you get light binding on that
I guess it depends so I don't want to
run too long so I'm going to move a
little bit forward sorting isn't all we
care about we obviously care about data
processing so we built this system
called famous and famous I'm sorry yeah
so we implemented all of these different
applications here and what I want to
show I don't have time to go into the
details but what you see on this graph
is the performance of our MapReduce
system where the y axis is throughput in
terms of megabytes per second per disk
in each of our phases the x axis is all
the different jobs that we've done and
different levels of skew and what you
see is for the vast majority of these
jobs we've pushed our storage
performance similar to our
record-setting sort performance now I
said almost all there is this cloudburst
example where in the first phase of
cloudburst it is i/o bound and so we see
that that performance improvement but
the second phase isn't diabound which
kind of exposes this point we talked
about at the beginning of the talk which
is that when you get rid of one bottom
like you can oftentimes push it
somewhere else and a place that you
typically push it as the network now for
us this wasn't a huge problem because
this go donated one of these big data
center switches to our group and we only
have fifty two nodes and we had enough
ports to give full bisection bandwidth
but if you've got you know 152,000 nodes
that's not such an easy problem and so
that kind of leads to the second part of
my talk which is focusing on the data
center interconnect so just like you
know applications have changed the
network has changed quite a bit as well
and we've seen this enormous growth in
terms of data rates and so the types of
networks that people have built to
address this growth in performance has
changed quite a bit my first exposure to
these kind of networks was in the 1994
when i worked at this
ISP in Houston and the way that a lot of
networks are built then and even today
is as these tree type structures and
you've got nodes along the bottom and
then you have layers of switching
getting increasingly powerful as you
move towards the root and if you imagine
let's say a hundred thousand Oh
datacenter at 10 gigabits a second
that's a pet a bit of aggregate
bandwidth demands now the real problem
is is that you simply can't from a
technology point of view by score
switches that are fast enough to
actually handle all of this bandwidth
and so you know so researchers have
actually looked back to the 1940s and
taken ideas from Bell Labs kind of in
the 40s and 50s and adapted them for
data center designs and so this is
what's called a folded clothes multi
rooted tree and it's to what some
version of this is deployed in many
types of data centers and this was
proposed to come two thousand eight and
the key thing here is that we don't have
these really powerful switches in the
middle of the network instead if we have
10 gigabit of second servers all of the
switches in our network or 10 gigabits a
second and we get all of that bandwidth
by relying on multipathing to deliver an
aggregate amount of bandwidth and so if
you have enough links in the network you
can load balance and distribute traffic
appropriately to get a high amount of
aggregate bandwidth and what we've done
is traded off impossible to buy switches
with a very challenging but solvable
with money problem of adding lots of
links into this network and so a 64,000
no data center has about 200,000 links
in it and these links are incredibly
expensive to kind of deal with them
installing them and managing them
they're also very expensive in terms of
cost and as we move from 10 to 40 to 100
gigabits a second of Ethernet they're
going to get disproportionately more
expensive and the real reason for that
is that we can't rely on the copper
cables that we know and love and we have
to move to fiber optics and the reason
for that is because of a property of
copper cables called the copper skin
effect which roughly speaking says that
the faster the data rate of a cable the
shorter it has to be so at a gigabit you
can buy spools of hundreds of meters
worth of Ethernet the second you go to
10 gigabits you're down to order 10
meters and at 100 gigabits you're
talking about a couple meters in length
and remember these are warehouse scale
buildings and so we have to overcome
this length limitation in some way
and so the way people do that is to rely
on optics which don't have this copper
skin effect and so you can send very
high bandwidth you can create very high
bandwidth links at very long lengths
this way now the problem with optics is
that you have to have some way to
convert between the electrical signals
that the switch understands and the
optical signals inside the fiber and so
you need a transceiver at either end of
this cable that has a laser and a photo
receiver in it which is used to make
this conversion and these transceivers
are sort of ballpark a hundred dollars
maybe 10 watts at 100 gigabits a second
and I know that several of you would
have much better precise information
about the pricing this is sort of based
on external information and papers and
stuff like that but the point is is that
they're not trivial in terms of cost and
you need two of these for each of these
say 200,000 cables so it adds up to a
lot of money and a lot of power to look
at the implications of that if we
imagine a 100 gigabit a second multi
rooted tree here and we look at the path
from a given source to a given
destination what we see is that packets
transiting this path are constantly
being converted to and from Optics each
at each of these switch hops at each
layer of switching from the leaf up to
the core and then from the core back to
the leaf and so the implication of this
is that for every device attached to the
network there's roughly speaking for 28
of these transceivers in the network
kind of conveying the traffic for that
device and so what a hundred thousand
nodes that's like a megawatt of power
and tens of millions of dollars or more
and if we step back from that for a
second I think it's worthwhile asking
why are we doing all of this packet
switching why are we doing that and what
I would say is that these folded clone
ette works the service model they
actually provide is that they allow you
to make a different and a unique
forwarding decision for each packet that
you send in the network but that service
model is I'm going to argue too strong
for many data centers and as a result
there's a gap between the service model
we're providing and the service model we
could potentially serve provide and this
gap is how we're going to get resource
efficiency and to say what I mean in
more specificity there's a lot of
locality and data centers and actually
Microsoft has been great about
publishing actual results from your
networks and this is a picture that's
reproduced from one of
papers and it's a little bit dated at
the moment but it does show kind of the
rack to rack traffic at an incident time
and although the details change over
time what you can see is that a bulk of
the traffic is going to a relatively
small number of output ports so there's
a certain amount of spatial locality in
these systems but if we kind of look
bottom up as well we see that there's a
lot of temporal locality as well and so
my student Rishi Kapoor published a
paper in conex last year where he looked
at a 10 gigabit server and he deployed a
variety of kind of representative
applications on top of it and he
measured the pack is leaving that server
at micro second time scales and what he
saw was that because of all the batching
that happens in applications in system
calls in the operating system kernel in
the Nick hardware all of that sort of
buffering and batching ends up
translating into tens to hundreds of
pack of packets that are correlated in
nature so you tend to see when servers
send large amounts of data from one
place to another they tend to do it in
these kind of correlated bursts and so
the key idea behind the second part of
the work is to use this temporal and
spatial locality to build cost effective
networks by adopting circuit switching
in addition to packet switching so if
you're not familiar with circuit
switching I'll give you a very brief
example this shows a one input port to
output port circuit switch and you can
think of this is just an empty box that
has some mirrors inside of it and light
enters the input port it reflects off
these mirrors and it leaves an output
port and if you want to make a circuit
switching decision there are tiny motors
underneath these mirrors that can move
them and this changes the angle of
reflection and causes the light to leave
out of a different port now this is
great because you don't need any
transceivers we're not doing this
conversion and it supports effectively
unlimited bandwidth meaning that as we
go from 10 to 40 to 100 gigabits a
second this technology doesn't have to
be changed but circuit switching is an
incredibly different service model than
packet switching and so this isn't just
a drop-in replacement for your packet
switches you really have to kind of
rethink the entire network stack and to
give you kind of an example of that I'm
going to talk about one aspect of the
service model that has changed which is
called the reconfiguration delay so the
reconfiguration delay Delta is the
amount of time it takes to change the
input to output mapping of that circuit
switch and it's roughly speaking the
time to move those little mirrors and
this determines how much locality you
need for these circuits which is to be
applicable in your network if Delta is
really large it means that it's
incredibly expensive to change the
circuit mapping there's a very high
overhead and so you only ever really
want to support very large highly stable
long loop connections that in the
networking world we call elephant flows
now on the other hand if Delta is really
small you can very rapidly reassign
circuits on short time scales and so you
can support very highly bursty
unpredictable traffic called mice flows
now I want to point out Delta's not
fundamental it's a technology dependent
parameter depends on how you build these
mirrors and and some other aspects of
the technology but it is a very
important parameter that determines this
mixture of circuits two packets yeah did
you also have count for the delay for
marrying the traffic before you not
narrow something and make the decision
to switch or chin yes Marvin times can
be much longer oh yes yes yes okay this
is great so this is talking about the
actual data plane now you're talking
about a control plane issue about how do
you figure out what signal system we
I'll describe that in a minute but we
started by an observe analyze act
approach that became too slow and so we
end up with a proactive approach that
I'll describe to the talking so yeah we
this is an exact problem we dealt with
you okay so we have the sense that a
majority of the traffic has locality but
not all of it and so the way you can
think about that pictorially is as
follows imagine that we have in a
network with n connected devices there's
n squared possible connections so we
could rank order all of those N squared
connections by the amount of traffic per
connection and because there's locality
the picture looks roughly speaking like
this where a bulk of the traffic is in a
relatively small number of these
connections and so this leads to what
we're saying is a hybrid design where
we're going to rely on both circuit
switching and packet switching so what
we like to do is take the head of this
distribution and send it over these
circuits witches and then this
relatively long tail that has a lot of
different connections but not a lot of
bandwidth we're going to send over a
less expensive lower speed packet switch
network and it's exactly this Delta
value that determines this mixture of
packets and circuits okay so we I told
you Delta's technology dependent and
when we started the project we had to
get some sense of what that value
was so we obtained an optical circuits
which that was developed in the late 90s
for the telecom industry and we
characterized it in our lab and what we
found was that the Delta value is about
30 milliseconds and what this means is
that you need to keep circuits up for
hundreds of milliseconds two seconds or
longer to amortize that overhead and so
it's really only stable for very it's
only appropriate for very highly stable
long live traffic and the place you see
long live traffic of the network is
generally speaking at the core we have a
lot of aggregation and so this led to
the development the Helios project which
was represented in sitcom 2010 and I
showed you that multi rooted tree before
imagine we're just focusing on the core
switching layer only and we're going to
get rid of most of the packet switches
in that switching layer and we're going
to replace them with a smaller number of
these circuits witches and then let's
abstract the rest of the network away
into these things we call pods which are
roughly speaking about a thousand
servers or so so the servers that links
the switches these are in these pods and
the idea behind the Helios project was
to support this type of an environment
and so what we do is we start by sending
traffic over our packet switches and
then there's a process that's looking
for these elephant flows and whenever it
finds one of these elephant flows it's
going to add updated flow rolls down
here to move it over to the circuit
switch so this is what you were talking
about about the time it takes to do that
because we've got 30 milliseconds so
it's like all the time in the world so
it's not a particularly big deal and so
we end up and there's details in the
paper about exactly how we do that but
finding those elephant flows moving them
over that over to here we can we can
achieve all of that in this kind of tens
of milliseconds time-bound yeah like
there's a tension between us who has a
big talk which is that yeah the first
time to talk is about efficiency which
by definition we excited you saw lindsay
blink yeah and it seems like it's
exactly the opposite of what you want
close their pelvis to select yeah yeah
so there's a commonality which is that
this is also getting rid of stat monks
if you want to think of it that way but
to get to your specific point I think
that you know one of the main things
here is that what we're doing here is
we're in a sense trying to build a
network that matches the average case
utilization even though that average
case is rapidly changing and the set of
nodes that need high bandwidth is also
rapidly changing today you're really
only option is to effectively provision
for the worst case and so you know in
this key in this way we're able to as
applications as their communication
patterns change we're able to migrate
things but even in the transfer case if
you think about the two phases in phase
one we were fully utilizing the network
but in face do we actually had no
network at all this model would allow us
to take that resource away and move it
to another instance that does neither
network so if we were to overlap phase
ones and twos in two different clusters
we could actually share the network
between the two so alright so the
results of the Helios project was that
we were able to get rid of one of these
transceivers in the core which doesn't
seem like a big deal but it's actually
represents a very large cost complexity
and power savings because when we looked
at this original network we were looking
at 10 gigabit networks and so all of
these pods we could entirely
interconnect them internally with
electrical cabling you only really need
it optics for these core switch layers
and that's where all the transceivers
were but as we want to start moving to
100 gigabits a second we're not going to
be able to make that assumption anymore
because we're going to have to start
putting optics inside of these pods just
because of the link limitation so we
need to start pushing circuit switching
closer to the host into these pods and
that led us to our second project which
is called more dia and I want to say
that the thing is is that if we were
able to aggregate over over a thousand
servers with this 3d MEMS technology
that was relatively slow if we want to
put circuit switching down to the host
we need a technology that's roughly
speaking a thousand times faster and so
we identified such a technology which is
a different kind of circuit switch
device called binary MEMS it's a little
bit different and what I will say is
that
the advantage of this binary men's
technology is it's very fast it's about
two microseconds so it's three orders of
magnitude faster but the downside about
it is that it's not scalable you can
only buy switches that are maybe 48
ports in size yeah citing are you making
a strong assumption work for
predictability because it two sides are
you so she said that if there's a
varsity parking brake so yeah constant
in 10 would you just keep ping pong in
because you saw of course you sent you
mislabeled a nice an elephant yeah
you're getting a circuit but now the
secular title yeah I have to go back so
yeah so implicit in this particular
design is the idea that what we need
because Delta so high we're only going
to consider traffic for this that's
stable for over a second so these little
births too fast because it's 30
milliseconds just to assign a circuit
and so for this the only kind of traffic
that we can actually support with this
is traffic that's going to be stable for
order a second or longer now with this
technology because we can reconfigure it
into microseconds we actually only need
traffic that's stable for about a
hundred microseconds to assign a circuit
to hear and so that's one of the major
issues here is that what we mean by
circuit traffic or locality traffic
depends on the Delta value and so now we
can actually support the burst of a
given server using circuits but in
getting to this point we can't do things
like measure assign flow rules it's
error because we only have a couple
microseconds to do that so we have to be
proactive and that's what this project
deals with yeah its progress until
you've got a beat on the counter so
close
a few my questioning that's not that's
gonna be encoded are almost wish
yourself ah so we don't read so instead
of estimating traffic based on looking
at packet counters and switches what we
actually do is we measure demand by
looking at hosts so we actually look at
the host what the buffer the send
buffers and the hosts are to see what
their demand is going to be in the
future and we can need to collect a
statistic from the post that could also
take it doesn't it so it takes order
microseconds tens of microseconds will
say to have the host send this data out
and I don't have time to get to it in
the talk and I actually don't have
slides to this but it turns out that in
this more dia design you can think of a
pair of tours I mean to towards
connected to each other so they only
actually have to exchange information on
a pairwise basis we don't have to
collect this globally do a decision and
send it back out again maybe we can talk
about that top like so so let's we
suggest being all the questions I have a
couple more minutes ago so and then we
can get to all these versions so in fact
there's details in the paper about how
we built more dia and I'm actually going
to skip over how we did it but the key
idea is that by these switches are able
to support multiple wavelengths of light
and by making a copy of the light by
tapping some of the light out of a fiber
we can actually replicate the signals
across multiple of these stations and
each of these switches can make
orthogonal switching decisions and this
is the key idea that we use to scale up
our design and by adding a variety of
these switches into one or more of these
ring networks you're able to support
order 600 tours or so with this design
ok now we built more dia over at UCSD
using these switches that we got from
this startup and we connected them to
our servers and we measured the
switching time of the composed system
and it is in fact this to micro second
results we're able to keep that fast
switching time even though we scaled up
to in this case 24 ports the key idea
here is that with 2 micro second switch
time we only need one hundred
microseconds of stability for something
to be circuit friendly and that means we
can actually support the traffic of a
single server and this led to our most
recent project which we just presented
last
wednesday at NS di over across the water
there which is exactly building a top of
Rex which that is a hybrid switch that
speaks to both circuits and packets at
the tour layer and if we this is the
premise of that project so it's very
simple if we've got a 10 gigabit packet
switched Network and we overlay a 100
gigabit circuit-switched Network into
our data center these effectively can be
put together to build a 100 gigabit
packet switch or common data center
workloads meaning that if there is
sufficient temporal and spatial locality
defined by a hundred microseconds worth
of bursts then you can deliver a service
model akin to this extremely expensive
network using too much less expensive
and lower costs network technologies and
we built reactor we using some this is
art we built an eight port reactor
prototype and we hooked it up to our
more dia Network and I just want show
you one of the graphs from that that
paper and then I'll sort of conclude so
the idea behind reactor is to give the
performance akin to a 100 gigabit packet
switch but using circuit switching and
so what we do is we deployed eight nodes
and we have seven of the nodes sending
data to the eighth node and this is the
view from the 8th note so what it's
seeing is that x axis is time in seconds
and the y axis is throughput and what
you're seeing is each of the incoming
flows is relatively stable very nicely
fair very uniform looking like kind of a
every smooth you know packet switch
network but in reality if we zoom into
this at micro second time scales what we
see is we're actually rapidly
multiplexing small bursts of data from
all of these different hosts and
delivering them to the end hosts and the
key idea is that we're able to rapidly
multiplex that link fast enough that the
transport protocol and the OS doesn't
realize that anything's going on and the
analogy here is to process scheduling or
if you can just schedule stuff fast
enough nobody notices that they don't
have access to that resource okay so the
key idea behind this line of work was
been to focus on the predominant sources
of cost and power in these networks
which is very surprisingly and sort of
counter intuitively cabling costs and
transceiver costs and so we built a
variety of projects that have dropped
number down to close to one transceiver
/ host at 100 gigabits a second and
what's nice is that as we go even beyond
100 gigabits a second this same approach
should be able to apply as well and I
just briefly want to mention that what
I've described thus far has been taking
what our existing building blocks and
prototyping them to build new types of
networks but we're also wanting to
complete that loop to build new building
blocks as well and so you know we
started with commercial technology we
built some prototype technology and now
we're interested in building novel
devices designed for data center
environments and so we're doing that
inside of a NSF there's an engineering
research center called cyan which is
about 12 institutions in about 30 p is
most of which are photonics and
physicists so they're photonics people
and physicist and the idea is that we're
taking all these organizations and
actually going back to a lot of the
building blocks we use we're designing
for the telecom industry so what we're
doing is we're kind of unwinding that
decision tree back to the assumptions
that were made in building optical
devices and instead of targeting them
towards the telecom world or we're now
doing is targeting them towards the data
center world which has a very different
set of assumptions and so there are
people in the center that are building
new devices and this is one example that
sort of has come together in the last
month and that I'll conclude so I
mentioned in the more dia design that we
have these binary MEMS switches in the
network that are making switching
decisions and they're basing those
decisions on the wavelengths of light
that is entering them now that's a kind
of an expensive design actually because
these switches are very expensive now
what you can do instead is there's
researchers in the center that have been
working on silicon photonics tunable
lasers which sounds very sci-fi but is
actually not a very it's not much more
expensive than building regular
transceivers today but the cool thing
about this is that by changing the
frequency at the source you actually
don't need those switches you can build
an entirely passive interconnect Network
and simply have the sources change the
frequencies that they transmit on and so
through the center we were able to take
that research and send it to a fab and
build it onto a chip and then partner
with this company in Berkeley to package
that in an sfp module that we can then
reinsert it into the more dia switch
and so that happened about three weeks
ago and so that's kind of what the
future work is for this project which is
to lower the cost of that of that
network okay so in summary you know it's
important to sort of pivot away from the
question of scaling to scaling in a
resource efficient way and I've talked a
little bit about I efficient data
processing and data intensive networking
and before I conclude I want to
acknowledge the incredible students that
I've had the opportunity to work with
that have been driving much of this
research and with that I'd like to thank
you for your time and open the Florida
questions yeah it's great that you got
this fishing delay so small but we've
also found that you're willing to deal
with instant traffic cops that you could
kind of get away with the need for a lot
of switching while carrying still an
orphan yeah you use your new tractor
yeah yeah so this yeah so this is like
things like oh SI and other projects
that rely on kind of overlay or
multi-hop it's another degree of freedom
so all of the designs that I've talked
about have been effectively either 0 hop
or one hop depending on how you look at
it and the second that you can start
forwarding traffic through
intermediaries maybe using things like
RDMA or something it gives you this
additional do your freedom where now you
have a scheduling decision which is do I
wait till I get a circuit assigned to me
or do I sort of send data to some
intermediary point which can then send
it on my behalf and it's it's actually
quite interesting because nothing that
we've talked about precludes that it's
just that that hasn't been something
that we've been focusing on but we're
told you that you a few microseconds
that doesn't make sense for did you see
further so this is this is an
interesting point because we were
sourcing with what's in some sense
fundamental here I think that there's an
interesting sweet spot at the kind of oh
of one micro second time scale because
it 10 gigabits of packets you know one
point two microseconds or so and in
order to use circuit switching well I
should say we've we've moved we haven't
looked at optical packet switching at
all we've really been focusing on
circuit switching because it's something
that's practical and it could be
deployed and so what you really need is
a burst size and if you're talking about
10 or 40 or even 100 gigabits a second
one microsecond gives you reasonable
births eyes that seemed to match well
with what servers are able to generate
if we were to push that switching speed
much lower we would end up kind of
hitting up to the packet boundary and
building effectively a packet switch
which isn't really what we want to do
and if we pushed it to be a higher value
we would end up needing so much
burstiness that it would be very
difficult to get servers to be able to
generate that so it happens to be a
particularly attractive spot at sort of
0 1 microsecond or so so we haven't
looked at trying to make switching
faster there are technologies that could
do that there's a SOA switches and stuff
that operate in nanoseconds but from our
point of view we lose our circuit
switching benefit from adopting those
people bidding just like I was like are
inspired optical switches right artic
doesn't need any switching because just
kind of at the front super sight they
just try things so terrific to me like
Wireless yeah well I would say that I
mean the work on say 60 gigahertz
Wireless in the data center in a sense
need switching because you're your there
physically moving things or you're
somehow choosing a different target to
transmit to I guess different they all
seem like an optical switch that instead
of like I think part of the part of the
thing it seems to me like the scheduling
control plane ideas under why it has
complexity yes and that complexity is
coming from the fact that you have this
switching schedule like no matter how
fast it still complex right that's right
um that doesn't show up like you know in
in cellular demeanor but not like you
know my phone can see
at any given time without actually
having by the global schedule across all
the transceivers well you do you have
channel mitigation so that's one thing
so imagine this network so with optics
it's like a wireless network where every
terminals hidden terminal so think of it
that way it's like if imagine a wireless
network where every node was hidden the
tunable laser idea that I just talked
about one of the things that's
interesting is that you end up in a
situation where to end however if to
let's say tours choose the same
frequency to transmit on that
interference will cause data loss and so
what we're going to do to solve that is
a very simple kind of brute force
approach which is to create a registry
service wherein you opportunistically
acquire a channel and then you register
with the service that you get the
channel and if two devices end up
conflicting on that one of them will win
and the others will stop sending and we
can bound the amount of time given the
amount of slick the slot time the
contention time can be small this is
just techniques from the wireless domain
and then we can code over that to make
use of so we don't lose data so this is
very much like a carrier sense approach
just in the optical domain my
understanding and I'm not a physicist or
an optics person really I'm a computer
scientist but my understanding from
topping to the Arctic's people because I
asked a lot about like they're cdma for
example can we do Oh FDMA or whatever
and my understanding is it is not every
promising approach but that's a little
bit out of my domain sweet dear what are
you saying couldn't you choose a
different matter that's what he's
talking minds so yes we do so yeah
that's what do we pick at 15 students
yes so our approach is basically that
you give it a list of in preference
order of the lambdas you want and it
tells you back you can have three and
you can apply for whatever but lamb does
that n square oh yeah yeah absolutely so
the number of land to see you get that
idea
okay current technology yeah current
technology the number of lambda 0 of 100
think of it that way not for his claim
additionally is this with you you know
if we want to support say 600 tours
let's say you need we need to add a
space switching into that as well that's
what we're going to do because we can't
plug we can't really fit more lambdas
into a fiber but you can have multiple
fibers effectively and what you can do
is choose which fiber you're going to
send data on and then which frequency
you're going to send within that fiber
and that's where i was mentioning that
point about how we don't need a global
scheduling decision because we do need a
global scheduler that decides what tours
are connected to each other tours but
once those two tours are connected to
each other the specific frequencies that
are being used can be negotiated on a
pairwise basis which is a much simpler
problem to deal with yeah leave your
lucky of those things too hyper that
you're welcome for him our newsletter
also a good comment about the upper
martin good at langson system
essentially scenes because a vibration
effects on o course it off yeah so this
is actually a great point there's um
there's two things I could say about
that one is that this stuff is super
reliable because it's built by the
telecom industry us are expensive so
they had these these goals of like 10 to
the minus eighteenth bit error rates the
reason that this stuff's expensive and I
mentioned before that we're back winding
back that decision tree to build devices
that are tailored to data centers which
is that we don't need to n to the
negative 18 bit error rate it we have
other ways of dealing with errors that
if they made the network substrate
significantly cheaper and more
integrated it might be a good trade-off
and so what I would say is the telecom
stuff we're using has been incredibly
reliable and then there was another
point that I was going to say about them
so yeah the technology we've used has
been fine so far but in terms of
handling failures in this model because
of this hundred of channels per fiber if
we move to a multi fiber or multi ring
type network the way you can think of
that is that I'm now spreading traffic
over multiple rings and so if one of
these rings were to fail in some way
that proportionally reduces my big
end with by n minus 1 and so there's
kind of a nice failure recovery model
there as well so you don't lose it's not
all or nothing you could imagine
degrading our service based on the
number of failures that you get so if a
laser fails or something like that you
might lose a fifth of your bandwidth or
something like that but you'd only
wonder percent of your balance awful oh
I don't have any concrete quantitative
data on failure rates between the two of
them but what I would say is that the
optical stuff especially this offer the
telecom industry is really reliable so
there are failures that occur of course
but it hasn't been a big prom for us now
one of the things that I will say is
that these tunable lasers the way they
actually tune is that they have very
small heaters next to the laser that
changed the temperature because the the
frequency that they transmit depends on
the temperature now the flip side is
that is in a data center if you have
changes in temperature if there's some
way to stabilize that temperature and so
one of the areas that's really that a
lot of the optics people are working on
is on devices don't require active
cooling and stabilization of temperature
but yeah we didn't run into a problem at
all so we have a sort of a data center
like the size of this room at UCSD and
it's got some chillers and stuff in it
and we have not experienced any failures
in three years but we're very small
scale so like in vibration plates that's
no nothing like that we also just as a
side point for the sorting record we had
a thousand spinning disks we never saw
any vibration effects it'll and the time
we use them you mentioned the suit
affected copper yeah lower frequencies
there's a fixed or making a set
replacing a single cable by a grade of
internet cable mmm i get to
ridiculousness my understanding is that
it does and again y'all are the experts
here but the labor involved in plugging
all these cables in is very non-trivial
and it's gotten to the point where
organizations like Google are building
these robots to build cable assemblies
so they can plug one layer of switching
into another and the second you say i
want to take 10 big fat cables and put a
500 of them together into a bundle as
big i think people don't like that
alright let's say let's thank our thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>