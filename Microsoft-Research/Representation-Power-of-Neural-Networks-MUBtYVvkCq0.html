<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Representation Power of Neural Networks | Coder Coacher - Coaching Coders</title><meta content="Representation Power of Neural Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Representation Power of Neural Networks</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MUBtYVvkCq0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good afternoon everyone so I'm very
happy to have Matt washed our gaskey
from University of Michigan so many of
us are interested in deep learning and
trying to understand what are its 30
Calendar paintings and Matt Roush will
try to tell us something new about those
hi thanks a lot and I'm very happy to be
here it was very nice to be invited by
Sebastian and I also say that I I like
this topic very very much and not not
just because it's become popular but
actually I think there's a lot of just
independently interesting things about
this problem and I think there's a lot
for basically everybody to contribute to
it just from from almost any standpoint
even just a purely mathematical one so
before I tell you what's in the talk and
any sort of summary most this talk will
be first principles so I'll just tell
you even even what a neural net is and
specifically the kinds of neural nets
we'll talk about today so a neural net
is just a way to write a function as a
graph so it's a computational graph that
works as follows so the graph has some
kind of multivariate input and then
there are nodes and they're
computational nodes the way they work is
they collect a vector from all of their
parents they wait for their vectors
there are parents to compute something
then they perform a linear combination
of what their parents did and then they
apply a fixed a fixed nonlinear function
Sigma so a class of neural nets the way
it's defined is we fix the network
layout you've noticed I left out some
edges so it's just some collection nodes
in some some collection of edges that's
fixed and the Sigma is fixed what we
vary are these linear combination
weights and one of the standard choices
for this nonlinear function Sigma is is
this funny function is identity on one
side and 0 on the other side of 0 so
this functions actually become very
popular lately it's kind of the most
popular one right now and when I started
thinking about this problem I thought
well you know I might as well just try
to get caught up and use this one
because it's popular in practice but
fascinatingly this was actually
beautiful
with mathematically so this this will be
the nicest one for us to work with today
and um if you follow any of this
literature there's all sorts of other
words people use to describe the kind of
fanciest most complicated versions of
neural nets that are popular and we
won't we won't be discussing those these
are words like pooling and convolution
we won't be discussing that today so
this is just a very simple version of a
neural network okay so this is what the
talk will cover the well will try to get
a handle on exactly what this class of
functions is because I've just I just
defined it symbolically but it's not at
all clear what what these actually look
like is I very very all these linear
combination weights so thus we wonder
stand what these actually look like what
this class of functions is so first
couple cover a classical result which is
that um they can can fit continuous
functions and this result has a very
strong limitation in the sense that it
does not tell us anything about what we
gain from having multiple layers of
these things so in practice and
especially lately people build these
very deep circuits but but these Cosco
results only say something about in fact
something with only two layers so so
because of that we'll talk about two
results that that do tell us a little
bit about the benefit of depth so one is
one of my personal favorite results in
in actually the entire machine learning
in statistical literature which is the
computation of VC dimension of these of
these functions and don't worry if you
don't know what that means we'll also
I'll also explain what VC dimension is
and then the second one is what I
collects potential separation it's
basically a case where if you allow
yourself to have multiple multiple
layers then you can get away with using
exponentially fewer nodes or
logarithmically as many nodes and so
this is in terms of in terms of hype
this is the this is the new result if
you want to call it that but on the
other hand it's actually as I'll say in
the closing remarks when I give a lot of
open problems the new result is actually
the I'll tell you guys it's actually the
wrong result it's not it's not nearly
what we wanted to prove so the problem
is still very open so so like I said
there's lots room for everybody to do
to be brought to all of these to all
these things okay so just to warm up and
start this whole set up very slowly when
I say that we're gonna fit a continuous
function I have to say what fit means so
one of the standard ways to say we fit a
function is in the LP sense so you're
familiar with it this is an LP norm and
it's just it's just basically an
integral and the point is that often we
would like this uniform sense which
means for every point in our domain we
are some epsilon away this one's a
little bit weaker we can average so that
we can give up entirely on some small
regions of points so this one is a this
one's easier to satisfy this one's
harder to satisfy oh and there was
little pictures that's kind of the
average sense and this is the uniform
sense and then in the later sections
we'll actually care about what I'll call
the classification sense of fit which
means there's a problem we want to
classify and we're gonna care about how
close we get to classify incorrectly on
this problem okay so continuing the
warm-up theme let's just cover a very
simple kind of the simplest possible
setting so what if we only have one
layer and of course we only have one
layer and I'm only doing a univariate
prediction so this is really just one
node it's known that with one node what
can you fit with that it's a simple
question and it has a simple answer if
this function Sigma is is monotone then
basically if it's monotone in this
linear combination I almost have an
indicator on a on a half space there's
some half space and I'm going to be
going up basically along with the normal
vector of that half space so another way
to say that is I clearly cannot fit
arbitrary continuous functions because
consider this one if I'm correct over
here I have to be small over here so
that means I'm large over here so if I'm
correct on this I'm wrong on either of
these two if I'm correct on these two
I'm wrong on this one so it's kind of a
kind of a direct argument tells us that
in either of the sets the fit we have to
make a pretty substantial error so this
was kind of a maybe as a stupid trivial
example but there are two reasons it was
valuable so one is we
prove to that one layer is not
sufficient and it's gonna end up that
this is actually tight so with two
layers this is all we're gonna need and
the second thing is that in in this
lower bound we were we had a function
that basically has one bump we have this
half space and a monotone function alone
so we're basically fitting something
with kind of one bump and it's gonna
happen and all the results today that if
we build a shallow network we basically
need to have the same order of nodes and
number of bumps in the function so so
this principle will drive all the lower
bounds today okay so to make they make
things a little bit more interesting let
me tell you about one of the kind of the
I considers to be the folklore proof of
how powerful a neural net is this isn't
quite as strong a result as the standard
one people throw around to say that
known as fitting continuous function
but I find personally find this result
to be extremely illustrative so let's
say you want to fit a continuous
function from 0 into the D so the
hypercube - to just 0 1 and in this
picture I've made it even easier I have
the red rate the thick red regions are
supposed to be where the function is 1
so it's 1 in those regions and it's 0
outside so if I wanted to fit this with
a neural net I claim that it's trivial
if I can fit a box with a neural net so
by bunk oh so in in a picture or yeah so
it's it's from the interval from its
from o 0 1 squared so it's from the
plain and then I've just drawn the level
curves so the two thick red so it's 1
inside yes sorry it's one insight here
one inside here 0s you have some
painting at the bar by the way this yeah
so yep - yeah I was just trying to
simplify the picture ok
near the boundary close from what does
irrelevant in a session yeah very
quickly ok
it's a pickling yeah you should call or
you could call it an ISIL line and I was
just you know at rest at scale one but
I'm okay yeah somehow drawing pictures
is there sorry but so if we had a neural
network that could that could l1 fit a
bus and in the lp sense fit a box then I
claimed this problem is trivial and the
reason is we just we just grid the space
and we just fit each one of those boxes
because then we can just add these all
up I can add things up I can take linear
combinations through a neural net so
I've reduced the problem of fitting this
function to just the task of fitting a
box and let me just say there's kind of
a common theme that I'll come up here
which is that I'm building basically a
gadget out of the neural network I'm
gonna have a little tiny neural network
that fits a box and I'm gonna sing look
at the span of those things so I just
like calling it a gadget okay and to to
to fit a box is also very easy for us so
just suppose that this nonlinear
function Sigma is very close to the
indicator function we're gonna see how
to fit it with just two D plus 1 to D
plus 1 nodes and the the hint as to why
it's 2 D plus 1 is because a box is an
intersection of 2 D half spaces so
because these neural net single nodes
behave roughly as an indicator on on a
on a half space got to D half spaces so
what I can do is I can just take one
dimension and I can put one of these I
can put one function with each each of
these normals or along each of these
normals so it's two in here and one one
in each of these I can do this for all
of them I end up with this and now I'm
in good shape because if I just apply
one more and I can threshold it at 2 D
minus 1/2 and it'll so only this region
will will be satisfied so basically it
just intersecting together 2d 2d have
spaces and then that's kind of what you
get after that so with this kind of
fuzzy reasoning we've
a continues function from your one to
d2r with 2.5 layers I say 2.5 because
I'm just using the linear combination
part of the neural network so with with
2 layers I fit a box and then the span
of these can fit any canoeist function
from 0 your 1 to D to R and then if I
apply another non-linearity then I'll be
at 3 layers but then I can't fit any um
but then I Raju's constrained to be 0 my
range is constrained to be 0 1 ok so
this is the this is kind of the folklore
result and so there are a couple there
are a couple of problems with it so so
one is that s was kind of pointed out
because everything is continuous we have
a lot of we have a lot of fudge factors
on these boundaries I can't exactly do
these these these hyper rectangles I
have a little bit of fudge on the ends
and so that's why I have to do an LP
type fit because I have to allow myself
to kind of make some errors in the
boundaries if I wanted to do a uniform
fit then in the supremum sense then I
then I wouldn't be able to use this
argument it wouldn't work
I also want to say how old this proof is
I consider this proof to be as ancient
as mathematics basically if you look at
Jordan content or or definition of
lebesgue integral or any of these things
you see these kinds of these kinds of
box arguments I'll also say that notice
the way the proof worked I claim that we
really didn't use anything about
composition of function or illness I
built up a basic class of functions and
I looked at its span and so in fact if
you look up how to prove for instance
that boosting is consistent data boost
other than whatever you call it then one
way to do it is to take decision trees
of a certain size you make them have two
denotes then they can fit boxes also and
so the same proof says that boosting can
fit arbitrary decision surfaces so again
this is only a vector space argument it
is not an argument using anything about
composition of functions we constructed
a basis class then we reasoned about its
span
okay and so we had a gap we had 2.5
Larry's the upper bound we didn't have
an a uniform fit and so we have a gap
between this and lower bound so we can
close this gap with the so this is the
result that everyone actually cites when
I say no one else can fit any function e
they cite this result by this guy george
de banco from 1989 and i don't i don't
know how it is for everybody else but
even though i see this sighted basically
infinitely it's like in every paper i've
actually never seen anyone discuss it
just just for reasons i I can't really
comprehend so just just a side comment
but I actually like this proof a lot
it's extremely clean and every bug that
you that you and every little kind of
nastiness and sloppiness and everything
I just said is gone from this proof so
and I will say that I'm I'm watching the
clock and so I'm gonna rush asker it
I'll just give the whole thing in detail
it's fine it's it's very clean
so this setup of the proof is very
similar to the last one I'm going to
build a gadget I'm gonna build some kind
of primitive object out of just
neural-net nodes and I'm gonna reason
about the span of this thing and before
I kind of used very hazy reasoning I
said fit things with boxes grid the
space Jordan content but we don't need
to do any of that because vector spaces
are such well understood objects that I
can just there's there's theorems that
can use I don't have to save you know
approximate things so the proof itself
is only using functional analysis that
might be why it's not discussed much
because the way it's stated is actually
maybe a little bit impenetrable but um
I'm actually gonna give a hilbert space
version of it and the hilbert space
version you can just read it knowing
what a vector it if you know what a
vector is and what faggus theorem is you
can just know understand the proof so
okay so here's the proof the first step
is you sit is you you could prove that a
single neural net node is what I call a
correlation gadget and let me I'm late
I left something out of this slide I
have to tell you what Sigma is all we
need for Sigma in this proof is that on
one side it limits to zero on one side
of limits to one the whole function is
bounded and measurable all you need it
can be zero you can do any stupid thing
you want and then it can go to one so
it's its approximation of the indicator
but it's a very weak approximate or the
indicator of the indicator so you have
to believe that this what I call the
correlation property so give me an F
that's non zero continuous F that's non
zero and F in it has been L two it has
to be an l2 function but give me an F
then there exist if it's non zero then
there exist choices of the linear
combination parameters so that this
integral is non zero so for any nonzero
function I can I can kind of kind of
detect some structure in it and
interesting enough the proof uses
basically the same box fitting argument
I gave earlier the effective way the
proof goes it says if the function is
continuous then I can kind of build up
boxes I can integrate the function using
boxes but those boxes have to have
nonzero met they have to have nonzero
measure otherwise the function is zero
so the proof of this actually embeds the
previous proof technically it has to use
Fourier analysis but it's the same it's
using the same thing under the hood this
is a lemma and then here's the in the
Hilbert case it's just a just almost a
direct argument so I have all of these
so these correlation guys these are just
all the functions here as I vary W 0 and
W so I look at the span of that ok
that's a subspace I'm an infinite
dimension so it's not might not be
closed so I take a closure of it l2 says
yeah this is one of the places where in
the full proof you have to work in a
uniform topology and this kind of I
guess so anyway so it's the is this
closure but now it's a closed subspace
it's a closed subspace and I'm at
Hilbert's spacing and talked about
things like perp vectors Oh Thackeray
and theorem all this kind of stuff so
the theorem will be that the closure of
the span of these things equals the
continuous functions by
definition of closure this means that
for any epsilon I have an element in
here that is that is a epsilon close so
the proof goes like this
so I say give me any continuous function
and I can project it onto my closed
subspace and then I can just look at
this this difference if I click the
difference of the two sorry thank you so
this implies that the contrast functions
on 0 under the T is a closed subspace of
no it's just it's not so what you can
see is the continuous functions is a
subset of disclosure but this cannot be
in equality no continuous function right
I'm talking about the out two continuous
functions so but the closure it's a net
apology of the inverses which is able to
closure some subspace yeah if that has
to be equal to the set about the
business function it's not edible KATUSA
is a statement this is the I'm giving
the weakened version of the proof which
is only for Hilbert spaces
I'm not giving the whole the whole
uniform version of the proof I don't
understand that statement that equality
there which is a closure that's fine
equals what you're right I should have I
should have justified the right-hand
side with only Hilbert spaces it's sort
of the full statement uses the the
uniform code here from closing in from
topology the best thing is the
continuous functions themselves are
inside the Hilbert space they're not the
helper space because it's not complete
yeah but but it is the inner product
space or you can do closure in there
yeah since since I see that yeah well
what I'll do is after I state because I
mean it's clear to me that you
understand this very well so after I
give to the Hilbert version I'll tell
you how to translate all the lines into
the functionality version this was
well let's believe you know I'm this
this is the this is the audience always
want to I mean this is this is great
because I have a background in his
functional analysis myself I have to ask
myself at some point why I so rarely see
details of this proof discussed and the
only thing I come up with is that is
that people get scared when they see the
phrases like hahn banach theorem and
they have to you know that this thing
for instance said this proof you know so
you said he used Fourier analysis but
but you have to take for a Fourier
transform of a measure not of not of a
function and so this already is
something that you know for instance
isn't well covered in my grad analysis
textbook so but you are right that I
that in this case I left out too much
detail ninety quality doesn't excite my
apologize for that
that aside let me just say how the rest
of the proof goes so so now let's let's
look at this thing so I know that I know
that F minus G is orthogonal it's in the
orthogonal complement to this subspace s
and that means that the inner product
between this and every element in s
itself also has to be zero but then I
can use the contrapositive this if that
zero that means that for all of these
that's zero which which means that there
is nothing outside of that means that my
perp is zero itself every perp element
values to zero so that that equality
holds so now that that's been said let
me explain just quickly for the experts
in the audience how to translate this
thing into into into what it actually is
supposed to be so I cannot you I don't
use the LTV space but I use this the
correct topology for continuous
functions is the set of continuous
functions by the way under the
restriction zr1 to the D to R it's that
that's important because it's important
that I'm talking about confuse functions
over a compact set so this together with
the uniform norm which is that soup norm
so that is a Banach space and the
important thing is that the dual of this
space is the set of radon measures on
this thing
signed read on sign measures and so then
this step so we cannot we cannot take
projections but I can use the Hahn
Banach theorem to get what's effectively
a Perfector and and so then I can still
use the same reasoning over here
so know that that was that was an
excellent point and um okay okay oh yeah
yeah nice do I feel very bad that all
right so this is this is the quick
summary so two layers is enough in a
very strong sense to sell infinity sense
and and also notice what we actually
constructed in this theorem these
correlation gadgets this is exactly what
you're doing in in boosting algorithms
for instance you basically find the weak
learner which is most correlate with a
thing this is an algorithmic proof you
know you greedily you would you wouldn't
you're constructing this fit you would
actually pick the most correlated thing
ever every iteration and so that this
has been done this is actually what I
would argue popularized to great extent
these greedy these greedy methods this
is big paper by Andrew Barron from 1993
and just a funny remark we can fake make
this algorithm deep in the following way
I can make I can take a deep network or
what it does that devotes part of every
layer to copying the input forward and
so I'm just and so then I can just every
time I would do my boosting type thing I
would just have the nodes just kind of
go like that and while this is kind of
stupid because that means I have you
know D nodes and every and every layer
doing the copying maybe there's some way
to compress it and okay the problem
though with this is that there were
there's not a good understanding of the
the number of nodes the number of layers
trade-off or of function composition in
general so we haven't it's nice that
this result is true but and you know I
really like the saveco proof but we have
not really gained that much about the
heart of the heart of the problem so the
next section like I said our results
very dear to my heart and because I've
slowed down a little bit I'll just kind
of state the highlights here
but but I will second that these are
absolutely beautiful results and I find
them personally very surprising so I'm
just going to assume everyone knows what
VC dimension is so the question is so
now that I gave you these networks and
so suppose that in those networks for
every one of those sigmoid functions is
is just the indicator zero one the
question is what is the VC dimension and
for me the fascinating thing is that
it's literally ignoring the log factor
it is just the number of parameters so
it's this first one and this to me is
interesting thing about because a simple
perceptron so only one node is also
theta of W now this one is actually fit
this one's W log W W is number of
parameters the network so all the edges
so nothing this this schema where I use
that function like just an indicator it
doesn't reflect the structure of the
non-linearity whatsoever
okay the proof is actually kind of a
straightforward induction now if I allow
these that I said these kind of popular
nonlinearities now the VC dimension
changes and not only does it become just
the number of parameters times number of
layers the theta was only closed by
Peter Bartlett and he hasn't even typed
it up yet if you look in his book
there's it's not completely nailed down
yet
so so this is already fascinating to me
that only increased is just
multiplicative by the number of layers
so yeah I personally find this quite
fascinating and I have to say not only
do I think the proof is beautiful but I
would argue that Bartlett himself loves
the proof because the proof is actually
the cover of the book I'm actually
serious I'm not making this up this this
might look like some kind of stain you
know some kind of radioactive staining
of a neuron with like axons or something
that's not it's actually not if you look
in the book in chapter 8 the figure
appears and he just kind of doodled on
it to make this I haven't I haven't
about this yet but but um literally the
proof of how this works is in there and
this is an amazing proof I mentioned yet
most number five metre square that you
ever see mine you just something that if
we do not have a bound on a number of
layers the only upper bound we know
right now is W squared yes but but for
these piecewise um blends being
piecewise polynomial this if the Sigma
is piecewise polynomial as we change
what Sigma is all sorts of terrible
things start happening yes yes I'll just
save one brief thing you might say ah
I'm sure regularity assumptions hold so
let's make Sigma concave convex goes to
0 goes to 1 and I can impose a
smoothness dot on it turns out I can
choose one of these to get VC dimension
infinite with only three nodes so this
is a very delicate business and how do
you actually prove these VC dimension
bounds
you have to use really high-powered
techniques so for those of you in the
audience that know stards theorem and
Nova's Uzair 'm soba zeus theorem talks
about counting intersections of
polynomials and high dimensions and it
starts to make sense why that would come
up so these proofs are amazing
so if um if you do have time after or
later this week you can ask me because I
I love all these results a lot ok so
what we know so far is that a flat
network can fit any continuous function
and we also know that the number of
functions we have in the classification
sense how many classifiers we get it
doesn't actually grow that fast with the
number of layers but what we don't know
is what these functions actually look
like so in an attempt to get a sense of
what the functions with many layers look
like and how different they are with
what you can get with a flat network I
asked and answered the following
question so the setup is as follows you
give me an integer k
so this is going to be a result that
holds for all positive integers K you
give me a K I can construct two K points
with the following property any flat
network with less than two K nodes in
the network will have error at least the
sixth and the result itself will
quantify what flat and all these things
mean there there are no it's actually
not even to use asymptotic notation it's
a very it's a very clean easy to prove
and easy to state result and it's not
just separation from flat and deep it'll
be for any arbitrary number a number of
layers and then the the punchline will
be that if you give me it ends up two K
layers I can get 0 error with just two K
parameters and something called a
recurrent net which is a fixed small
network so network of constant size and
then I take its output and plug it back
into itself and I do this K times it'll
also get 0 error and the reason why I
care about this audition
in addition is just understanding this
class of functions is because in a
statistical sense I know that this thing
has exponential the VC dimension of this
thing so maybe there's some hope for
learning these functions from data well
and the thing to contrast this against
is the switching lemma and related
circuit complexity results which get is
similar which get a similar trade-off so
yeah I'll talk about this more
maybe offline ok so let me tell you what
the class of Sigma's that I deal with is
actually yeah okay I'll just do this so
this is the class that has two nice
properties will slightly generalize that
constant and then identity function and
also I can use this class of functions
to induct the reason about what every
layer in every node in the entire
network is doing that's why this will be
a chameleon class to work with so I call
this function T T sawtooth but as T
pieces so I take the real line I
partition it into T intervals possibly
of course there are two of them have to
be infinite and it's a fine in each of
those pieces and I don't require the
function to be continuous so it can it
can have discontinuities so it can be
like a piece and then another piece that
doesn't connect in another piece so this
is T a fine and two examples are so this
kind of popular function is piecewise
offline with two pieces but then you can
come up with other examples so again
this is only a univariate example so
decision tree kind of business
meaningful as usual but this is a tree
with t minus 1 nodes will just have its
T sawtooth so this lower bound will also
apply to different algorithms so for
instance just lower bound also applies
to boosting but you might say it doesn't
matter X is univariate problem but it's
still there's all holes so reasoning
about these functions is very easy so
first if I have something that's s
sawtooth so its piecewise offline in s
pieces and I have another function which
is T sawtooth so its piecewise offline
and T pieces I claim that the summation
of these two is just s plus t minus 1
sawtooth and the proof is just to look
at every time the slope changes so every
one of the pieces over here and I notice
that in each one of these pieces they're
both they both have a fixed slope so I
can just count the number of time to
change pick two of them so that's
actually two - it's s plus t minus 2
changes they both agree on the first
interval so then S Plus t minus 1 on the
other hand if I compose the two together
it's it's
times T sawtooth and the way the way to
see what goes wrong is so I compose this
with this so this comes first so I take
the T sawtooth function a look at any
interval and any one of the T pieces
that define it well if I take that
interval and I map it through the
function I get another interval it's
it's a fine in that piece but that
interval I mapped you can't hit every
piece in this thing so for every
interval of the second func of the the
first of the function you apply the same
to sorry its composition should be
defined the other way for every for
every piece over here I get s pieces
over here so together I get s times T so
and if I apply an inductive argument to
it to a neural network this is actually
quite easy to see so what you do is you
look at any note any node in the network
and I take I take all the functions that
that plug into it so if I'm at some if
I'm at some layer if I'm at similarity a
right now all the all the notes plugging
into it are inductively going to be TM
to the J sawtooth I add together m of
them so I'm gonna be m TM to the J
sawtooth I apply my non linearity TM x
TM to the J sawtooth so the whole thing
is gonna be t TM J plus 1 so a tooth
that's the index I'd that's the so
that's the inductive step with the proof
so and so saying it again a network with
a T sawtooth non-linearity M nodes in
each layer and L layers is TM to the l
sawtooth and that the point of this is
that the number of bumps the number of
pieces in the function grows
exponentially in the number of layers
but only linearly in the you know you
know all the other parameters so this is
actually what's going to make the proof
go through we're building these bumps
much more quickly with with composition
than with them with addition
okay so this s it turns out is it's
basically gonna complete the proof of
the lower bound so I'll prove the next
slide a lemma the lemma just says that
if you give me any sequence of two K
points with labels alternating so the
prediction promise from the reals to
zero one you just give me any sequence
of any sequence of reals and I label
them zero one zero one zero one
alternate fast as possible and the claim
is that the claim is that any T prime
sawtooth so it's piecewise that fine in
at most T minus T prime pieces has to
have error at least this with oh so I
should say what it is two to the K minus
two T prime over three times two to the
K so yeah so I'll prove that on the next
slide and the reason this completes the
proof is because suppose that you have
use your your network structure
satisfies this inequality so if you just
plug this in you get that your error is
at least a sixth and to make sense of
what so this is where I'm gonna say the
concrete version of what the lower bound
actually is so to make sense of this
quantity let's say L equals two so so
two layers and let's say we're with this
oh do I put oh yeah sorry I wrote all
this out here so if I use this this
thing which was zero on one side and
identity on the other side which is to
sawtooth so T is 2 and let's say I used
two layers then if I have less than two
to the K over 2 2 nodes Hilaire air is
at least two six if I make it root K and
I have less than two to the root K then
uh then I once again have error six this
is pretty rough I will say actually this
this is this is very much improvable so
I guess a theory audience the rocker sir
video has this very nice improvement of
switching lemma from you said it's this
Fox right yeah so that one actually says
that if you just bump down the number of
layers by one you still get an
exponential gap
that isn't implied by this result so so
the kind of separation get from his
results over you know circuits and
whatever AC zero it's a stronger okay
okay so the way so here's how we prove
this lemma up here so I have my tea soft
tea sawtooth function I guess I said T
Prime and and I have 2 to the K
alternating points so because I'm
talking about classification and
classification I take everything that's
above a half and I make it one and
everything below make it zero so all
that matters is where I cross 0 baton
but where I crossed a half basically so
that means I get a function which is
piecewise constant in two T pieces
the reason it's two is because at the
discontinuities I can I can also cross
so that's why it's not T sawtooth 50 saw
at these two T to 2 T so the
discontinuities actually nuke the
constant and the bound is not tight with
constants in the continuous case okay so
now notice that if I just treat each of
these intervals as a bin then the number
of bins that get at most one point has
to be at most a number of bins so I have
at most two T bins with a single point
and that means that the number of bins
with at least two points it's n minus
two T it's the total number of points
that Landon bins with at least two
points is n minus two T and the reason
this completes the proof is because if
you're an interval that gets at least
two points they're an all training label
so you have to make error at least a
third in the limited s close to 1/2 but
it's at least a third so I just divide
this by 3 and it it completes the proof
sorry that was the end oh yeah
so yeah it's uh so yeah it's just a it's
just a counting argument that's all it
is
I thought the proof would be much more
complicated and so you notice that there
you know there are no you know there's
no there's no oh it's not like it holds
for certain can others it's it's just
that easy
now let me tell you really quick how the
upper bound works and this one's even
easier so the upper bound is I have to
find a function that's either in Keira
petitions of a constant size network as
the recurrent case or is that a kale air
network with K parameters that fits
these points exactly and it's really
easy so I take this function and I wrote
it out tediously up there just to
establish to you if you're doubting that
it is just three nodes and two layers
but this is the easier way to read the
functional form of it so just a little
pyramid and now the question is what
happens when you compose the pyramid
with itself
it's it's quite easy so it looks at
points that are less than a half
multiplies them by two points that are
bigger than a half
it reflects them so it's literally all
it does it's just it's and then by
induction so then so then I can just
take the set of points with alternating
labels to be the bottoms and the tops of
this function and that's really good um
one one one thing that I found kind of
fascinating because it wasn't by design
it was just kind of an accident so you
could argue I mean this isn't I don't
know how okay you can argue that this is
effectively an approximation of Fourier
basis you give me a K and I I construct
these kind of high-frequency piecewise
defined functions and there is a fairly
recent survey on neural nets where they
also just asserted that that a Fourier
basis is easy for a deep network also
you know the switching lemma that was
parity functions that's the Fourier
basis over the boolean domain so I don't
know if everyone's doing for a basis cuz
that's the first thing you think of but
interesting coincidence okay so we can
close from here so to summarize so first
we gave a couple classical results where
we can fit any Q's function with the
shallow Network then we pointed out that
that the VC dimension at least so number
of classifiers we can construct does not
grow too quickly so it's just linear in
the number of layers and and then we
gave this case where there do exist
functions where you have to blow up the
amount of parameters exponentially in
order in order to fit them with a
shallow thing and of course that doesn't
contradict the VC result in any sense
because it is just one class of
functions it's not describing you know
it's not saying that in every case we
can kind of reduce the complexity by by
a log we know logarithmic claim it's
kind of a nuance that's lost in the VC
characterization okay so I have a bunch
of kind of random remarks that are just
things I found very interesting so at
least for this construction if if I use
these indicator functions that the
result is false you don't increase the
complexity at all when you do
compositions I mentioned this to
sebastian earlier he point out
immediately that i'm using only the
univariate case and i can't actually
tell you that in Multipure case it is it
is slightly different but but already i
found it interesting that that um that
you have to use something that all the
roofs I know for these upper bounds not
just that pyramid function I showed you
but other proofs I know they need a
continuous function but like I said
earlier that class has infinite VC
dimension so you have to be very careful
which continuous functions you use oh
sorry yeah so this has oh I didn't
mention this this is used as a lemma in
the proof of the infinite VC dimension
for a very specific non-linearity
okay so another thing and this is a
result I'd really like actually think
I'm going to probably try to prove it
sometime this month is so we constructed
a single function that we know is very
expensive to so construct with something
that's shallow the question now is what
are some other ones and the result I
would like to approach is can we define
a notion of independence and rank so in
other words let's say I just have a grab
bag of functions that I can represent
efficiently with with a multiple neural
network and inefficiently with a shallow
Network is there some other function now
which I can't represent is let's say a
linear combination or just a composition
of these other ones now that I can throw
into this bag I can kind of keep
increasing the set of functions I can
characterize because we didn't had all
characterized all the functions that
have L lairs and M nodes in each layer
we didn't even remotely characterize
that function class but maybe there's
maybe we can find a couple more of these
functions maybe not just these piecewise
that fine before you transforms but
maybe there's a couple others that
together they actually do characterize
the entire clip that's what I mean by by
dense in quotes there is something I
really like to answer another thing is
is what actually is this function so a
lot of you know Leon Batu I
I gave actually this talk in Facebook
about a or sorry I gave a version of
these results some other stuff at
Facebook and Leon he said to me that the
thing but he was very fascinated by this
by this pyramid function so this pyramid
function is it's pretty funny so if you
take a symmetric function G and compose
and look at the compass in G composed
with with with the pyramid so what does
it do if G is from 0 1 2 0 1 if I go so
it pre composition so I go from 1 to 2
to the minus K I'm gonna replicate am i
replicate gene just a condensed version
of it and then from here to here so from
2 to the minus K 2 to 2 minus k plus 1
I'm gonna
I'm I'm gonna get the reverse of it
since I said symmetric I'm gonna
duplicate it so by this peak
compositions gonna repeat the function
two to the K times so it's like this
period just it's like a period operator
or a looping operator so alright so I
say that there's prior work if you know
about this coming off Arnold
representation result it uses
space-filling curves and fractals and
there are there are so this is a
business of paper there are people that
claim the results are relevant but I say
actually that it captures a lot of this
kind of interesting structure that's
also in this pyramid map and then so I
think in general there's a lot of room
to develop just nice theories about what
composition of functions does so some oh
and by the way I'm not making fun of
function houses because I actually spent
way too much of my PhD reading
functional analysis books and using it
but but now I realize wait like I really
need to know about more than just vector
spaces so but there are a lot of
interesting fields and maybe in during
the questions section which we'll get to
in just a moment people can just tell me
about other ones but I feel like we can
just keep doing so much more so of
course in in TCS there's circuit
complexity results so another family
results that I literally know nothing
about I just found out about from Lucas
Tom's blog is a suan in additive
combinatorics they look at they look at
the following problem you give me a
group and I look at a subset of it not a
subgroup but a subset and then if I look
at what just happens when I take the
group operation just apply it to itself
signe the group some sorry some subset
of the group not a subgroup so call it s
and I apply all the elements themselves
I get call it s squared s cube how does
what is the rate of growth of this thing
so me this is very very similar to this
how many functions I get us I add more
little errors problem so I think there
are lots of fields that are attacking
this problem of exactly what is this
function class I see yes you apply these
compositions I think there there could
be so that this for me is where this is
just a purely beautiful mathematical
question because I still don't
understand what these competitions are
and
so of course from our computational
perspective what we really want is some
structure that actually helps us design
algorithms some structure that algum can
pull out you know something maybe like
that correlation thing but a correlation
that works in a multi-layer fashion so
so for me that would be the the best
that'd be the best kind of structural
result okay we're done maybe just a
comment so the result that you told us
it's far from the separation entry
complexity right because yeah it's just
uniform not only University that's also
it's not the worst case in the worst
case it could still be that shallow and
deep as the same thing yeah there could
be certain functions that have that
property I just gave one function class
but as it went what survey do did is
that he showed that in the worst case
there is for
yes I well I have to trust you on that I
I thought he did not show that but I
just maybe don't know the result won't
of I thought he did something kind of
like this which normal where he gave a
specific function which is hard to
approximate with with one less layer but
maybe I'm wrong does anyone know so be
clear I thought the result was a
specific class he calls it the sips or
something functions where it exactly
exists in in a in a depth K circuit but
then if you go down to K minus one
circuits you can only get 1/2 minus
little o of 1 close without getting
without having an exponential blow-up in
the size of the circuit that was my
understanding of a circuit a result but
you know
yeah I feel bad saying X I'm not trying
to diminish the result I think it's not
actually give me go to your slates of
missing dimension you just told us there
were two results you said right yes go
ahead you can I just want to recall we
want to admire the book cover that's all
right
I didn't do this very well did I I spent
more time talking about the book cover
than about me so there was two Sigma's
you said dr. Holdren yes so educator
just oh oh yes yes yes
so just it just yes okay it was so good
it was this one this is the answer so
it's it's it's number of parameters
Sigma just indicator yeah the list and
this is if I use that one that is
popular now and the reason I picked this
function class was because for this we
do have a theta we do have a we do have
a tight upper and barrel where you can
ask about what other function classes I
just want to do so long as how these the
two numbers below the first like a
multiple-choice like okay this could be
the dancer or do you expect something
with them
oh wait so you're asking why I thought
these were relevant bullet points
including I could have tried to sneakily
construct this talk where I try to make
it sound like my result is actually
showing that there are tons of these
functions where it takes exponentially
as many nodes so if it was true that I
always could take any function as
exponential size with a shallow network
and compress it down to a linearly size
deep network that would imply that that
the VC dimension is exponential because
I'm getting all those functions so they
have to live somewhere so if my result
was not did that make sense X planar was
that
so what this you can interpret this
result is saying that my result which is
for a fixed class of functions there
actually aren't that many of them there
aren't that many of them then the second
bullet point is because if you look at
all the existing bounds a lot of the
upper bounds not for this function this
Sigma but for many others actually are
quadratic and I won't call out names
because maybe that's unfavorable of
conjecture is wrong but but people like
my rate you know way way up
they have conjectured to me that it that
it actually is super linear and for a
super multi linear so quadratic in some
cases which I have only the only
intuition I have that that might be true
is what those people told me so it might
actually be it might exploit Radek in
some real cases yeah yeah including ones
that people care about yeah it's not
just pathologies but they and I mean an
interesting case is if you saw my
reasoning was doing these little
piecewise thinks I'm out bins right
discretizing you know what what if I you
know what if that thing is is you know 1
over 1 plus e to the X which is which is
the sigmoid which is the common thing
people use for well 1 over 1 plus e to
the minus X that thing we can still
prove it theorem but right now the only
upper bound there is actually quadratic
or actually sorry that upper bound does
not even depend on the number of layers
so actually exhibiting a dependence of
number of layers is is often tricky
results in years ago I think I couldn't
be happy about that a result when years
ago saying that the new your networks
the size of the waves is more for me the
number and the size of the network
you mean the Magnum magnitude of the
waves yes so Kenny connect the dots how
does it connect the results
representatives me you mean how does it
connect bees each dimension results or
how does it connect back to my show that
the complexity of of the clasp
represented by your nets is dependent
more on the this is one claim in this
paper depends more on the size absolute
like l1 norm sorry like the l1 norm
yeah the l1 norms all the weights as
opposed to the number of words so how
does it kind of out you put it in
context here sure
so it's it's possible that what I'm
gonna say is wrong because I don't know
exactly what he meant by that but I'll
tell you what I do understand so it
depends on what exactly you're giving a
balance the VC dimension is about on the
classification error if we were for
instance caring about the logistic loss
or the exponential loss or hinge loss or
one of these for these we should use
something like a router more complexity
result if we use the Broderick
complexity on when we think about the
realm of complexity of this lost class
so I'm looking to run more complexity of
the loss composed with with a predictor
in that case then the norms of the
weights will come out naturally so if
you care about those kinds of problems
for instance you care about regression
then that quantity is the one that will
come out naturally the reason I do
consider to be apples in oranges is
because looms you really do care about
classification and these are theta bells
they're tight so not only that there
really are cases where you want
something out the other so one one thing
I was telling Sebastian about earlier
today is that this fun oh okay I won't
joke but though that composed pyramid
function and constructed it's got to to
the Cape it's got 2 to the K up and
downs in the interval that means it's
Lipschitz constant is 2 to the K so
there are some ways to analyze has cost
functions that would just blow up
of the complexity but if you care about
classification then then that's not the
right estimate so I would say that it's
problem dependent when you think about
the two different bounds and they are I
would say kind of apples and oranges I
don't find that answer entirely
satisfactory I just kind of summarized
for you the results about one of the
results about other but I think this is
a very interesting question actually
yeah I understand precisely what is the
norm you know that characterizes the
capacity of these neural networks this
is essentially open so what but Peter
Butler did I guess 15 years ago was
indeed I suppose the l1 norm over the
entire network was the l2 norm over the
entire lab but you could think of things
which are much finer because you know
the last layer and the first layers they
shouldn't be represented in the same way
in this know that still is showing that
if you have a boundary there's also
right I'm also so one you can express
the Rademacher so you can get the
dimension free rather macro complexity
down if you control the weight of the
entire neural network either in there
one dominant or not just like you know
just like one day oh this is different
for multi layers but you can expect
presumably to have a much bigger class
where you still have a dimension free
bound of the rather macro complexity but
where the norm is not going to be the
same in the last day or in suppose if I
can make one more comment here if you
look at the original Rademacher and
Gaussian complexity paper Bartlett
Mendelssohn there's a very nice piece
there's a very nice Rama complexity done
in there but I do not see discussed much
particularly never see discussed in some
reason Virata more complexity on a two
layer network so for a two layer network
so devout one of the big okay so I was
making it sound like VC versus
Rademacher it's a question about whether
you care about real valued objects or
0-1 objects basically but there's
another big deal which is of course
Ranma complexities distribution
pendant and so he has a bound in there
that actually depends on sparsity
structure a lot and so then he has an
really really really nice theorem in
there
for um basically the rod markup likes
you get is this s log n type of effect
that you always expect as far as things
if you know what I mean it sort of being
rude ended so it's an S login so this is
a number of non-zeros and so uh so there
is this other benefit abroad more
complex but it does allow us to have
these nice distribution dependent things
all right
thanks my fish</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>