<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Efficient Exact Gradient Update for training Deep Networks with Large Sparse Targets | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Efficient Exact Gradient Update for training Deep Networks with Large Sparse Targets - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Efficient Exact Gradient Update for training Deep Networks with Large Sparse Targets</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EGqFYZxVHjI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
Burnett's speaker is Pascal Vincent from
he flew away all the way from University
of Montreal here Pascal had many
contributions to deep learning is
probably mostly known for the noisy
kowtowing quarters which shed some light
on the importance of injecting noise
during training and today's going to
tell us how to compute efficient exact
radiance when the in training deep
networks when the target space is very
large thank you so this is joint work
with my students Alexander Edison and
variability so first for motivation
nowadays we tend to try and learn neural
networks that have very very large
output spaces and the first time I
actually counted this problem was in
back in two thousand and when I was
working with yoshua bengio on neural
language models that at the time exotic
idea and at the time we already had the
problem of having to predict a large
output vocabulary we were dealing with
like 30,000 words for recovery it was
already a computational problem then and
now it is still a reputational problem
because you want to predict the
properties of those very large vocab a
closer a large number of classes which
with the learner presentation of a
reasonable size gives you a very
expensive Oh of a big d tongue small D
computation for example at that output
layer so that's by no means restricted
to neural language models the problem
has no known also as extreme
classification and there's even a
workshop with nips about this and has a
lot of application it arises in many
cases it also arises in memory with
network extender like where you have
soft max over the size of the memory
which
we come quite large so test for the
motivation and they currently employed
approaches basically following two
categories either changing the
probability model like imposing a
hierarchical structure with your
hardcore soft max or sampling a small
subset of those many output dimensions
at each time which we see in in a nice
contrast of estimation or important
sampling based approaches but relative
to computing the full output both these
approaches are very good approximations
so what I'm going to talk about I'm
going to show that it's possible to do
the full exact gradient so no sampling
or nothing and in oh of small d square
so the dimension of the representation
not at the output rather than 0 of big d
smalley but with a qubit that's going to
be for restricted family of loss
functions okay so what we'll have we'll
have our inputs that are our input
vector to the network as well as our
target represented as sparse vectors now
sparse vectors they can live in a very
large space this Big D is going to be
big size of vocabulary but with a few
elements that are on so all we need to
do to represent them is to keep those
elements indexes and values and what
despise us is when we have to do a math
tricks multiplication with such a sparse
vector well it's simply going to be
weighted sum of a few columns actually K
columns of that matrix so we're going to
have an operation in oh of K d instead
of 0 of DD where K is very small whereas
Big D is very very big okay and
similarly when we do want to do a rank
one update of such a matrix with a
sparse vector well actually we're only
going to touch a small number of rows or
columns of the matrix
so again Oh of Katie instead of O of Big
D small D okay so here's the kinds of
architecture that are you're considering
we have some input X and some target why
both representative sparse vectors these
could be the vocabulary and bag of word
representation or similarly and D
dimensions that were considered where
they were considering here is a big d
that can be huge and a small K the
number of nonzero elements okay and then
we have a network with reasonably sized
intermediate layers of size small D and
finally an output layer that is the same
size as our sparse target vector that
the output typically will not be sparse
so finally these are compared with a
loss that's what's being optimized here
i'm considering squared loss I'll relax
that a little bit later but for now it's
going to be square loss okay so simply
comparing our output 0 to our target
vector Y now if we look at the forward
propagation what we have here is a first
weight matrix that's a big d time small
D matrix remember Big D's large so
that's a huge matrix and that's going to
take us to this layer but so normally
that would be a no of Big D small D
computation very big one but since our
input vector is smart spots this is of
KD so it's really cheap ok all the
others are pretty reasonable small d
square small d square and now we're
getting to our output output layer
matrix again it's a big d time small D
matrix we want to go from a small D
dimensional layer to a big D dimensional
output but here there's no reason for
this to be sparse and even if it was
this wouldn't help us much because
really the big dimension here is this
big d dimension so we're going to have
to take to multiply with all the
elements in that matrix so that's a
that's
Oh of Big D small D probably pretty
expensive operation in many cases I'm
showing this to contrast what's
happening here in the input layer let's
parse input versus the output with a
sparse target okay then this output is
compared to the target price target and
okay and this gives us the loss now
let's look at the back propagation
wonder back do a great buy propagation
so we compute you the residual there can
be of the big d it can be a little
cheaper bill and here comes the company
the expensive part the update to this
weight matrix is a rank one update but
it's a rank one update to the full Big D
time small D matrix right so it's also o
of Big D times Molly and computing the
gradient with respect to H is also a big
detail small the operation because
that's a matrix product whether for vac
full matrix and a non sparse vector
right but once we have this gradient
then all the rest okay so all this is
pretty ability expensive all the rest is
pretty cheap this is all simply of d
square same for the updates of these D
by D matrices and once we get that input
back to that input updating this big d
time small D matrix is cheap because
it's a rank one update with this sparse
input vector okay so contrary to what
happens here for updating the output
matrix updating here the input matrix is
very cheap but altogether this is a big
lead on small the operation with big d
huge okay actually i'm using a little
bit of the notation is three piglet I
small D which are these three operations
here so we're going to concentrate on
what's going on here at that output
later
alright so we can do much better than
that what I'm going to show is that it's
possible to compute the loss l the
gradient respect last hidden layer and
the exact same perform the exact same
gradient update to that full matrix big
device multi matrix all is possible in
oh of small d square ok without ever
computing the full output ok and how how
do we do that so there's two tricks the
first trick is that we're going to keep
an up-to-date small eat I small D matrix
Q ok just a find here and then we can
compute L cheaply so if we look at the
computer computation of the last
initially its wh- y so this is our
output ok this is a big detail small D
computation but if we we can rewrite it
this way and develop it and then we here
we see we have a w transpose W we can
replace that by our matrix Q which is a
d times D matrix multi-time small D so
it's a small d square operation and
similar here we have a w transpose Y but
why is a sparse vector so this is a
order of K small D operation ok and this
is cheap also so you see what we've done
here is we've replaced this very
expensive Big D time small the operation
by these operations that are all small d
square and we can compute the loss
without every computing the output
similarly for computing the gradient
with respect to H which we need to
further to the back propagation we we
can play the same trick so here we have
a w transpose W that appears that's
again q and again all this is all small
d square without the need of computing
the output wh alright that's a first
rate now if we look at our update of W
so the normal gradient update of W is
really a rank one of update of W which
is non sparse so we have this rank one
update here we have h vector which is
small
sighs and wh- y which is our residual
which is non sparse which is non sparse
okay and this big d size so this is
actually going to modify to touch all
big d x small the elements of w ok so
there's no choice it's all big details
moly ok so note that we can rewrite that
by developing this in this manner and by
developing it in this manner we see that
it can actually be composing two
sequential steps first doing this then
doing that ok we haven't changed
anything here and the complexity is
still owes big detail smoothly so how
are going to do to modify this w using
only small d square operations so the
trick is to replant represent this w
implicitly we're going to represent this
as the following factorization product
of the v matrix which is the same
dimensions and smaller small these times
multi u matrix and then instead of
updating w we're going to update it
implicitly by updating v and you and so
we take the two previous steps so this
first update to W is equivalent to
updating you in this manner and the
second update to W is it going to be
updating V in this manner so here we had
a full update of W that's a big detail
small D and here we have simply a rank
one update of view which is a small
detail small D matrix so that's cheap
here provided we keep an inverse of you
up to date and that's also cheap small
small d square and the big gain is
really here is because now here we had a
full update of V which was not sparse so
it touched all big detail small the
elements here we're going to have a
sparse update of V which is only going
to touch k rows of RV
here instead of touching all big d rose
so big ed candy can be 800,000 and K can
be two or three so note that this is not
the same as doing ordinary big back drop
on you and then V okay so it's very
different it does however do the exact
same update as the normal back rub a
blade of a single matrix W okay there's
some booking keeping operations we need
to keep up to date you and cue they're
also all d square so skipping that so
that gives us a full nine step algorithm
that's altogether of small d squared
okay so what are the benefits of this
approach well first there's a big
computational benefits that we expect we
have replaced basically an operation
that was three big detail small D
operations elementary operations by
roughly 12 small discorporation so that
sin expected speed-up of big d divided
by four small d so if we take sizes for
example a sparse target size of 800,000
with a reasonably sized dimension for
the present children 500 the speed up is
going to be four hundred and the big
gain here is in memory access is we're
not we're only accessing k small the
elements of V rather than the big d x
small the elements of w so this can be
straightforward extends the mini batch
case it doesn't change the complexity
also the approach that i showed four
squared error is applicable to the
spherical loss family which is basically
any loss that can be expressed by using
only the output elements that are
associated to non 0 target elements and
Q which is the sum of square of the
output elements okay
an S which is a sum of the elements
optionally so the linear output the
squared error that we've just looked at
as part of that family and unfortunately
regular soft lock softmax is not but
there are other normalized losses like
the linear spherical softmax where we
use a squaring instead of an exponential
so regarding the measured timing so we
did two implementations of this one for
when on CPUs and sorry one on CPUs and
one on GPUs so this is comparing the the
timings and bit with the unfucked ur
eyes so the normal standard back prop
and our factorize version so if you look
at the standard backdrop for many
bachelor of 128 examples and for sale so
this is this is a log log log scale okay
so it's it's almost nearly half a second
for a mini batches of size of 128 if we
use GPUs instead at some ten times
faster but what's remarkable here is if
you look at our our algorithm which are
in green you see that their timing for
that mini batch is independent of the
recovery slice that's what predicted so
so that's nice and here are the speed-up
curves so in red as well as our big d
divided by four small D which is our
theoretical speed up and the measured
speed up with our implementations on cpu
is in blue and on GPU is in the green so
it's pretty close
and if you look at the overall speed up
so that's comparing to a naive version
so the CPU unfucked ur eyes night
version if we look at the output layer
only if we use a GPUs instead it's like
six point eight men times faster if we
use our CPU factorized version it's 700
times faster than the CPU one factorized
and if we use the GPU factorized 3000
over 3,000 times faster than the cpu and
factorized and and that's for just for
the output layer computation so back
prop forward propagation back
propagation and update and it's similar
for the whole model speed up a little
less of course because there's the other
word other layers that are taking some
time okay but it's still a big speed up
all right note that all except oh we
also put here the air hog loose off max
just for reference all they're not
optimizing the same cost but except for
that hierarchical softmax all these
versions compute the exact same cost and
gradient update it's just different
algorithmic versions okay so to conclude
my talk I so showed an approach that
he's a huge speed up and that allows to
perform the exact gradient update and
networks with a very large sparse target
the time timing is independent of output
size and the gain is from fundamental
algorithmic complexity improvement and
not from hardware-specific optimization
the tricks so the main limitation of the
approach is that it's restricted to
losses in the spherical family and the
future we want to investigate the
spherical loss versus the softmax
compare those actually this has been
done some extent and so far it's not as
good as for language models and then
we're looking for compelling
applications they disapprove might be
better suited to sparse regression
targets actually
thank you very much so while we're
taking questions if you're a spotlight
presenter please approach and line up
here prepare for your talk and if you
have questions please approach door to
Mike's 1st question there um your trick
that you're using at the output layer is
in some ways very reminiscent of the
kernel trick that's used to transform
say svms from primal to duel
representation do you want to comment on
that because then you know in theory you
could do infinite dimensional d or is
that possible big right so I have given
it only little thought so far III notice
also that is indeed reminiscent of other
kernels but I don't think you could do a
infinite dimensional because the whole
trick is to actually compute this big
output matrix W explicitly and you don't
have that because what you have
basically when you when you when you
have a kernel is you have a you have a
kind of predetermined fixed expansion
right so so it's definitely similar I
don't think it could be applied for
infinite dimensional okay from what I
thought
so we're a bit short of time to list up
here thank you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>