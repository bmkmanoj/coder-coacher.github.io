<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>RDFox — A Modern Materialisation-Based RDF System | Coder Coacher - Coaching Coders</title><meta content="RDFox — A Modern Materialisation-Based RDF System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>RDFox — A Modern Materialisation-Based RDF System</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NAEFLsRN4Zw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let me just maybe briefly introduce
Boris actually I first met him while
visiting Oxford it was July or something
like this yeah assigned to me like he
does interesting things to say why don't
you come and give a talk and to my
understanding he does like theory some
very deep theoretical stuff as well he
does coding and very low level coding so
let's see i have no idea was going to be
she'll start or is this Academy okay so
hello everybody and welcome to my talk
thank you for giving me the opportunity
to tell you a little bit about my work
and my work is in the area of semantic
web ontology and rdf and there has been
a lot of talk about it so i won't give
you the standard you know introduction
of what semantic web is and rank i
should do this by presenting your two
applications real world applications
that have worked on in the past that use
these kinds of technologies and then
i'll introduce the actual technologies
later so in 2009 i did a three-month
project with the startup from barcelona
called experience on which got bought by
skyscanner and for which i developed a
bit of semantic technology know what
this company wanted to do they wanted to
improve search in tourism domain and in
particular so so far you have expedia
for example you have to know exactly
when you're where you're going you know
you have to na you have a very simple
form and have to fill that format and
what comes out comes out their idea was
to do something more advanced so for
example I mean okay because they
initially did it in Spanish so this is
in Spanish this means hotels close to
the Colosseum in Rome essentially now if
you did this if you try to solve this
query using google you would get all
sorts of random stuff because what as
you know Google would simply match what
keywords hotels Coliseum roms who might
actually get the Wikipedia web page
about the Colosseum and that kind of
stuff but this is clearly not what you
want you have to understand this this
query now there is clearly a natural
language layer here which I wasn't
involved in this is not my field but
once when you actually understand this
and actually they developed quite a nice
solution for this by the way are but
once when you understand this query what
then you need in order to actually give
the results you have to have a database
of facts you know colosseum rome where
is what you have to know that for
example you know not not just
geographical but you have to have
background knowledge about the whole
domain and for that they needed a way to
represent large amounts of heterogeneous
data I mean clearly you could stuff all
the central database and in the end
actually there is a Postgres database at
the back end but there is a little
antique layer in between which makes
these things a little bit simpler and
I'll show you a bit later how this is
done and another very important thing
for them was inference so for example if
you have a query for accommodation that
should include hotels bed and breakfasts
and all that kind of stuff which you can
automatically infer from from the data
that you've got in your database and
this is clearly not something that you
can do with just postgres another
application that so this was more a
research application which we did with
some Agnes Sampson from Korea but with
the research department so what they
wanted to do is they wanted to use
sensors on mobile devices like Wi-Fi GPS
you've got a lot of different sensors in
the phone to provide context services
context based services so what what is
the context the context is for example
at home in a shop with a friend that
kind of stuff and then you want to adapt
the behavior of the device depending on
the context so for example if you're
close to a friend who has a birthday
which is your context then there is an
action remind the owner of the phone to
congratulate birthday now clearly you
could do this in Java you could just
heart called all of these things you
know like if this then that but this is
very inflexible so the idea of sanction
was to use semantic web technologies to
describe what are those contexts and to
describe the adaptations declaratively
using technologies that are going to
introduce shortly and then they used an
inference engine so this was like a
small lane in memory
engine that ran that I actually
developed in the train inside their
phone that then could interpret this
data coming from the sensors and
actually decide what to do in a
particular situation so those were the
two motivating applications and now
we'll go into how do we go about solving
them so the first thing you need is a
data model in order to represent your
data about tourism or your data about
context and in particular people develop
this data model called resource
description framework this is quite
closely related to so this grew out of
the research on semi-structured data
bases from the 90s it is also related to
the research and graph databases so it's
kind of a graph like data model where we
have various nodes and this is like a
simple graph here where you have the
node representing some person called
Peter a course intro to AI Oxford
University I mean I think all of this is
fairly self-explanatory but then and
just to fix a little bit of terminology
so you have nodes which are sometimes
also called called resources then you
have these labeled arcs which are
sometimes called triples I'll use this
terminology throughout the talk
interchangeably and what is also quite
important is you have these special arcs
so for example rdf type is a built-in
arc which says that peter is an instance
of a class called person so unlike in a
database where you have your schema and
the data separate in rdf it's old you
know bungled into one big thing and then
you use these spatial ages to denote
that in for example have another special
edge here subclass off for example
saying that Professor is a surplus of
employee which is a subclass of person
and now these spatial ages have a
particular semantics associated with
them so what i can do is i can run
reasoning and then these red links are
the ones that you can infer from the
explicit data so you know this is the
very simple stuff so if peter is a is an
instance of class called professor and
professor is a subclass of employee then
peter is an
and then also you can infer you know
transitive closure of the class
hierarchy I mean those are very simple
examples but you can do this much more
complex I mean obviously I can't do this
all on the slides but for example in
this experience or scenario they used
quite a lot of this kind of inferences
to infer what is mirror to define
actually what is near to what and that
kind of stuff so this reason it is
something that actually differentiates
you know the semantic web technologies
it's a key aspect of semantic web
technologies compared to just playing
relational databases now in order to
actually capture so so we have this data
and we want to capture these rules of
inference formally for that we can use a
well-known formalism from databases
called data log actually it's not just a
face both databases in AI I mean this is
one of the most well-known formalisms
out there that's been used in various
contexts oh I'll just again just fix a
little bit of terminology I mean I won't
have a lot of definitions but I just
need some some terminology so we have
these constants which I don't go a lot
into that in rdf these resources or
constants all of those are synonymous
are identified using you arise and this
allows you to create these well-known
vocabulary so for example you could
create a Microsoft you know ontology
where you would then have you know a URI
representing the product class of
Microsoft I mean obviously this is
softened since that soft benefit of
semantic technologies in the sense that
there is nothing fundamental about this
just user you know Internet domain
prefix but it's quite useful in practice
then you have terms which you know you
can have these constants or variables
which are written like that although
sometimes I will omit this ? and i will
use XYZ for variables and then what you
can do is you can build atoms which are
of this form here so you have some any
relation and then you have some terms
here now this I hope won't confuse you
so when you talk about
Ripple's there is just one relation
called triple basically so we usually
avoid this R and instead of round
brackets we use angle brackets so this
this triple relation can be embedded
into the RDF can be embedded into the
relational model using a single ternary
relation and this is actually how you
can store rd of data in a database as
well but that's not the only way so what
people sometimes do is if you find this
s rd of type o so these kinds of triples
you store in a unary relation oh and
then you just store the subject in that
and then if B is different from rdf type
then you have a binary relation which
connects the subject and the object so
throughout this talk I will flip with in
between these various ways of looking at
RDF and I mean in the literature this is
a bit you know jumbled you have to low
exactly also where a particular paper is
coming from but ok however I can have
you know atoms of these four more atoms
of this form or atoms of these form it
doesn't really matter it's all kind of
the same thing that the underlying
technology is the same and then what
datalog is is it's a bunch of
implications of this form and this
essentially means whenever you can
instantiate the variables in this b12b n
you then derive whatever follows from
the head and then a data look program is
a finite set of data log rules and then
this is the main thing of the main point
of this paper so how they do reasoning
in detail look there are various
different possibilities one is to do
materialization which essentially means
before you actually go and do anything
with your database you run a
pre-processing step in which you apply
these rules until you reach a fixed
point so until you basically derive
everything that you can derive you store
all that in the database and then you
can just query the database as and when
queries arrive I mean obviously this you
know there is a you know cost increase
in memory the question is how long it
takes all
these things I'm going to talk about a
bit more later but the fact is that this
is not just academic actually this is
done for example an oracle system in a
quite a lot of commercial systems it
turns out that this memory blow up is
not that bad and also there are
techniques to actually not maybe
materialize everything but I won't go
into that in this particular talk yep do
questions and they might be interested
in the talk later on but one is all this
is sort of deterministic right where
their relationships are deterministic if
we have probabilistic sort of atoms you
might not know you might have a
probability that some like a hotel is a
luxury hotel yeah right so there is a
probability that the vote lizard
actually go there and there is also this
other issue of context according to this
particular website this hotel is
actually go 10 right in some other
website might be kinetic this other
website so how do I won't talk about
this in this stock but for example there
are probabilistic extensions of data log
you know how it exactly works out I'm
not actually an expert in probabilistic
data log but I just know that there are
so roughly what they're happens is you
basically make your rules in quote some
Bayesian network basically then your
facts are inputs to this Bayesian
network and then the evaluation of data
look gross corresponds to you know your
typical kind of you know inference
problem with Bayesian networks but I
mean you don't need non-deterministic
computation for that it's it's still you
know deterministic in a sense it's just
that you work with probabilities you
annotate each fact with the probability
what was the second thing contradictions
yes so what you can do is I mean it's
all a matter of encoding so for example
what you can do is instead of using a
ternary relation you can use a for every
relation and actually an rdf there is
this notion of named graphs which
basically assigns this fourth category
to the each triple which essentially can
be then used to encode any different
kind of context there has been also work
on you know
getting more structure into this context
information so you can have a lattice of
annotations and then you know there are
some rules on how you propagate these
annotations during inference all this is
done but it's not the topic of this talk
the topic of this talk is to actually
just do this materialization just for
your just just so that you see so what
you can do with data log is you can
encode these inference rules so for
example what I actually showed on that
slide before can be encoded like that
this is just one way of doing it in here
I'm using one fixed data log program I
could also extract the schema and then
create a program based on the schema but
it's all kind of the same thing I mean
it doesn't really the deepest oh that
are not necessarily important so what is
the goal okay all this is just you know
the background what is the goal of this
talk the goal is to develop techniques
for materialization of data log programs
and early of data now I'm going to
actually refine this a little bit
because we looked a bit into some of the
trains in database systems and there is
a big trade that trend towards first of
all main memory systems there were some
statements even that you know databases
will end up in main memory I mean just
recently we bought a 256 gig server for
less than 5,000 pounds so you know you
can store quite a lot in 256 gigabytes
so and and and because this is such an
intensive process main memory make stuff
make things much easier so I will focus
on main memory techniques which means
that I will for the most part assume
that random access is cheap I know this
is not the case caches and so on but you
know it's much better anyway than on
disk and then another thing is
materialization is computationally
intensive so I but you know already this
laptop here has four cores this server
that we bought has 16 physical cores and
32 virtual cores via hyper-threading so
I'm refining my goal here and saying you
know materialization but in main memory
multi-core systems and we've developed a
research prototype called RDF
so it's not already Fox but are they of
ox and you can download it and you can
play around with it okay so i will start
with describing this algorithm four
parallel materialization and before i do
that obviously i should tell you a
little bit about how this is done I mean
obviously we are not the first one who
came there and said oh let's paralyzed
this so there are generally two
different approaches the parallelization
one can be called intro query
parallelism which essentially means that
I mean all of these rule bodies
corresponds to queries which you have to
evaluate over to your data now if you
can show that two rules are independent
then you can run these queries in peril
but the problem is that in practice you
don't get that many independent queries
very often so your potential for
paralyzation is limited another
possibility is in track weary
parallelism which means that you
paralyze the evaluation of a single
query and there well at least two ways
to do that one is to partition rule
instantiations to threads so what people
sometimes do let's say you have some
variable X and you have n processors and
let's assume as we will see quite often
all these objects are recorded as
integers you know you use some
dictionary encoding so so this is
actually it makes sense so what you do
is you constrain rule bodies using an
atom of this form X mod M equals I and
then you know where this and then you
basically circuit on thread 0 is going
to be 0 on thread one is going to be 1
which essentially means that you now
evaluate your query but you basically
handle only certain rule instances on
different threads this the problem of
this kind of technique is that it is
susceptible to data skew because there
could be more of these X's which mod n
go to one rather than five for example
and then for example what you can do is
you can also paralyzed joint computation
there has been a lot of research in the
database community and how to do that
efficiently and in main memory but the
problem here is that well I don't
actually believe that this is as you
will see later this is that relevant
simply because you know you have this
data in memory
have it indexed quite a lot of you know
doing this main memory hash join is
about actually indexing the data whereas
you know this is not the most efficient
way of doing it if the data is already
in main memory and already index
basically so while it can be paralyzed
the thing you're still doing way too
much so what we want is actually a
technique that will distribute workload
two threads evenly and with minimum
overhead so that's my main goal and one
other thing which is quite interesting
here is that and that's actually
something that really hasn't been
considered in databases so far and that
is that you know during well first of
all to evaluate these queries I need
indexes and this is simple because you
know as you know like if I just run a
query over the whole database it's going
to take forever for example because
there can be many ways to infer a triple
they I need to do duplicate elimination
that's critical otherwise this algorithm
this bottom-up computation is not even
guaranteed to terminate so I can't sell
oil index it at the end actually I have
to do this as I go along basically you
know so now there has been a lot of work
on indexing rdf data but mostly this is
assumed that the data is static so you
can you know sort your data you can
apply very efficient solar joints you
know all this is great but the problem
is that here you're actually
interleaving query evaluation with
updates because you're evaluating this
query and then you're adding new data
all the time so these assumptions that
people actually used in databases are no
longer hold basically so what we need is
we need a data structure which supports
in boxes and efficient parallel updates
and that's a combination which i think
is somewhat unique in in this work yeah
in fact that yes they are coming from
the external or nobody infer that you
infer actually so I'll give you an
example just now so this so I've
actually outlined this idea so we have
this this problem first how to
distribute computation
course second how to encounter index
data and I'm going to now talk about
these two different aspects of the
problem so the first one is this new
algorithm which is actually quite simple
that we developed and I'm going to first
present it as if everything is running
on one thread and then I will show you
how we can actually efficiently
paralyzed that so let's assume that this
is my input database that is given and
this is the only rule that I need to
materialize and now I need to do this
materialization so this is essentially
what i will do i will first assume that
my data is ordered that's why i drew it
actually like this so there is an
implicit ordering from top to bottom so
you know we actually do store data in
arrays rather than sets and what we do
is we essentially extract an atom from
this from this table and we not try to
find you know we try to shove this item
basically into this rule match this one
body atom in all possible ways and when
I do this here so X goes to a y goes to
be and I get a sub-query lv ok now what
I do is I evaluate the sub-query in the
data and you might say okay airways here
so let me derive a of Y but this is
actually not good because then when I
reach a way then I will put this over
here and you know then I will match this
again with Rab before so there are two
ways to satisfy this rule once you you
start from are the other one is start
from a that's why when we do these
matches we do the matches we only with
respect to the facts that are above the
current fact and because this herve is
below the current fact we don't do
anything we simply skip over this
potential match the same thing you know
RAC the same thing happens current
sub-queries a of a RBC and so on and
then finally we get to a of a we now
stick this away here we get the
sub-query called are a
I and this Y can now be mapped to the
fact above and I derive new facts are a
B and AC this answers your question so
those are the facts that you actually
produce during during this computation
and you simply just continue I mean and
then you know when you get to the end
you're essentially done what this does
it essentially codes a well known
algorithm from database is called the
same in the eve technique which ensures
that you consider every rule instance
only once this is important because this
number of times you consider a
particular rule instance that's the you
know determining factor of performance
in this whole setting so okay this was
very simple actually really simple with
this provider that you map the sub
queries only two things which are above
your current fact yeah when completely
expensive right if you're if you have a
huge sort of table every time you come
to a new fact you have to explore well
for that you need indexes yeah order to
need indexes that's precisely why I need
indexing I'm not gonna explore the whole
table I'll get Scylla so okay how do we
paralyze this computation well the whole
point is that I can extract these facts
independently actually so i can let one
thread extract this fact one thread
extract this fact you know they can
consider these sub queries independently
because this part above the current fact
doesn't change you know the new facts
come always at the end and you've
actually decomposed this huge problem
into a large number of subproblems which
are fairly small evaluates this sub
query basically which you can very
efficiently distributes to different
course so there is no need for thread
synchronization only actually when they
reach the end of this table you know
then it could be that you know one trade
extracts one fact and there you know the
other thread has nothing to do
essentially but what we've noticed that
the threads are not until the last
final milliseconds essentially are
equally used basically simply because
there is no need for them to actually do
anything with each other great no okay
so suppose there are four threads yes
and there are four three properties for
rules aging and the tread finish and
basically I'd for more sort of property
and this guy needed yes only the end
because you see what usually happens is
that you I mean you have let's say you
know millions of facts right and you
start at the beginning so you know these
new facts are written at the end but
there is still this million facts in
between basically that the other threats
can actually process independently
that's why it so only at the end you
know when all threads are reading stuff
you know at the very end of this table
then you get there is a synchronization
step there but you know it happens only
at the very end and actually here is an
example where this algorithm completely
fails so you have again the same rule
but this encodes align basically so now
what you get is you know well you will
extract a away shove it in there you
will get a of B and down you actually
process that pact to get a of C and
there is nothing that the other threats
can do essentially but this is very
pathological example I mean I should
actually say this is a linear rule which
means so it can be paralyzed in theory
but we've looked a lot into trying to
make this theoretical result work in
practice but it's just actually one of
those things where I firmly believe it's
absolutely irrelevant for practice
because of the number of processors that
you need to make paralyzation payoff is
something like cubic basically so you
know although in there is there is a
disconnect there on this particular you
know example you will get this
pathological behavior but we actually
haven't seen that in practice ever and
there is very little need for threats to
synchronize here so this was the first
part yet go ahead you
evaluate its rule independently of the
other so one of the time more I mean can
you use thread to evaluate more than one
rules at once I could but actually so I
I mean this was the natural way of
paralyzing this in my opinion simply
because you have this one fact this
usually matches to a small subset of the
rules you know because you have this are
XA be basically you just find the rules
which contain are in this in this thing
and that gives you a small number of
rules which then you all evaluate in
this one thread we didn't actually see
the need to you know then paralyzed that
in particular because that would then
require some form of synchronization
between the threads but this was this
was good enough so to speak the number
of rules right we had actually
situations with like hundreds of
thousands of rules and it still works
really well simply because the I think
this is simply because the number of
facts is much larger than the number of
rules so because of that you know the
number of subproblems that you get is
again much much larger basically because
each of these facts correspond
corresponds to one sub problem so that's
why I think this is a better way to
paralyze it go ahead rules is you good
well that would be more difficult
obviously because I mean for example we
have to have rule index as well so to
find you know in this case is where we
have like this three hundred thousand
rules you know you don't want to scan
sequentially the rules so yeah that
would be major nine actually so rules
are assumed to be static although I will
get to one point where they do change a
bit but like that was not easy yep go
ahead so the way I was imagining this
it's basically materialization would
happen once right you have your facts
and you have your rules and then
material like this you would happen and
then you are you have got your thing now
you can do quiz right for so in the real
world basically a lot of facts might
keep on being out after that where there
is incremental updates with
developed a new algorithm for that
that's at the end of the talk and I
don't think I'm qualified managed to get
there but you know we did some work on
that as well however so so okay the next
thing is you know we now have this so as
everybody here noticed already we need
to have these facts facts indexed and
actually the main synchronization point
so I mean when we started this we were a
bit naive you know we thought like the
dis algorithm was the best thing since
sliced bread but actually not the
algorithm doesn't really tell you much
simply because the synchronization point
was the right thing into this table so
you have this like common data structure
that you need to keep updating all the
time and doing that is in my opinion
actually the great the biggest novelty
and this involves you know to an extent
just you know engineering the toes a
little bit of conceptual thinking of how
to organize this data structure that
keeps kind of being attacked by all
these threads at the same time so what
we roughly did so so what is the
requirement I mean we have these RDF
atoms and you know we have these three
terms t1 t2 t3 some of which can be
variables and now what happens is during
this query evaluation you get a term
which contains variable summer and you
want to return basically all the facts
that match this partial instantiated
atom so that's the same thing basically
so how do we do that well the obvious so
you have eight different patterns
obviously because nothing can be a
constant or variable you have eight
combinations one possibility is for
example you organize each triple into
three singly linked lists so for example
the SP league list collects triples with
the same subject and then within that
list the triples are grouped by
predicate why is this useful well you
know if you have a pointer to the head
of that list well you have all the
triples which match subject variable
variable and then because the triples
are grouped with this subject predicate
basically you can also use the same sub
list for you know if you have subject
predicate variable based
click and then you can do the same with
p 0 and 0 s and you've actually covered
all of these eight combinations now you
need also indexes into these entry
points so you need is index which will
given a subject give you the head of the
appropriate s list and then you have to
have an isp index which given a subject
and a predicate gives you the head of
the SP sub list within this big list
essentially and then you have also other
indices for these other two lists and
you'll finally have to have an eye SPO
which is essentially big index that
indexes everything for SP 0 and that's
actually very crucial because you have
to have this duplicate elimination you
have to eliminate these duplicates
eagerly now there is a small
optimization here binding patterns of
this form are rare so what we do is we
actually in the implementation use an
eye well SP 0 p and p list which
eliminates essentially one index and
then to answer patterns of this form we
use either is or i 0 depending on the
size of the index so just a small
optimization and this is what it roughly
looks like so you have one this big
table of in of triples this is a thing
so everybody does this in rdf so you use
a dictionary encoding you basically
replace all resources with integers for
that to use some kind of hash table
somewhere essentially we do this by
using small integers not some randomly
generated integers this allows us to
actually use these integers is array
indices and then you know while these
ripples arrived here in this table in
whatever order and you have the next
pointer to the senex triple in the same
group and you have your hash you have
your basically pointers to the start of
a particular list this guy here is an
array this guy here is a hash table this
guy is a hash table as well okay this
was all fine this also was there quite
at the beginning of our work but you
know it was still quite a paint to make
it work really well and why was that
simple because
in order to update all these lists it's
non-trivial II have all of these threads
you have to ensure some kind of
comparative have to ensure consistency
basically and you have to do it in such
a way that you know 16 threads actually
make progress that don't wait so we went
for like a round and runs and runs of
different possibilities and what we then
actually found out is that you know well
there is this idea of lock free
programming so you know okay I assume
everybody here is familiar with lock
based programming so what you do is you
have a shared resource and you simply
you know you simply have a look to you
lock this resource and one predicate
size access then when you're done you
unlock it everything's fine this ensures
linear is ability linear rise ability
which is this one abstract property that
you want your system to have makes
programming easy comparatively to
anything else but the problem is that
these kinds of systems are susceptible
to thread changeling so for example if
one lock if one thread acquires a log
and then dies fine okay then you can say
it was a bug but what if the thread gets
worked up because you know you hit
something and then you know like the
memory has to come from secondary
storage well then actually this all the
other threads are still being hindered
by this 13 essentially and this is a
problem and you have lots and lots of
rates basically it can lead to this
priority inversion which means that you
know for example if you have a higher
priority operation that still has to
wait for lower priority operation to
actually finish because of that people
have looked into lock free programming
now the definition of log 3 programming
is that at all times at least one thread
makes progress I mean so you can't say
okay there are no locks so it's lock
free I mean this is the actual
definition because obviously what is a
lock you'd have to define that somehow
formula these guys are usually
implemented using compare and set
instructions so what is compared set you
give it a location you
an expected value and you give it a new
value and it's like an instruction in
the process or every modern process or
has something like that what the
processor will do is we'll go to this
location read the value if that value is
equal to expected then it will write the
new value into that location and it will
return in all cases the old value
essentially and all of this is done in
Hardware lets say by locking the system
bus for example on x 84 86 just contrast
this there is also notion of weight free
programming this is not what we're doing
so wait free programming is essentially
when a thread is guaranteed to complete
every operation in a finite amount of
time I mean that's quite difficult to
ensure as you can imagine in this case
well thread code you know be delayed
forever because you know it does it
cause it does across it does it cuz
there is no guarantee that it will
eventually you know finish but you know
it's not really an issue and then the
other thing is that doing systems so
that they're fully lock-free that can be
actually quite expensive because in some
situations it just doesn't pay off so we
develop we invented this moniker mostly
lock-free which essentially means that
for the most part you know we are lock
free but in some cases we resort to
short leg locks so I'm going to now
demonstrate how this works and for
example one because we use this linked
lists I'm going to start with a simple
example so you have this linked list
here those are my triples that are
already there in this big table and now
you want to insert these new treats or
tooth rates want to insert in parallel
these new two triples in this list
between these two guys so how is that
done well both rates essentially say
okay you are my next guy both threads do
a cuss cuss into this location here they
say update so this thread on the Left
says update this to hear provided that
your previous value was this guy
and the same happens on the other thread
now hardware makes a decision and let's
say that the thread on the Left succeeds
and you know the thread of the left is
happy and goes on does whatever it wants
afterwards but the thread on the right
is not happy and the thread on the right
history I repeat the process now it
still wants to add stuff behind this guy
so what it does it now actually modifies
this here does the cause now there is no
other thread contending basically the
cut succeeds it's all fine it's a very
simple idea but the fact is that you
don't need any kind of locking it that
paint turns out to be much more
efficient in practice but leave notes
below it's not a lock no because you see
there is no locking here there is no
locking because at any point at least
one thread will succeed entering this
thing into the so for example let's
assume you go here and now this red dyes
okay fine it died this will ever get
inserted fine that's a different problem
but this red can continue so it's not a
spin lock does that because you see
neither thread ever did any action that
would prevent the other thread from
completing things spin lock would be if
for example you hate here some kind of
you know flag saying oh let me lock this
guy here so that i can write into this
location here that would be a spin lock
and then let's say that this thread
acquires the spin lock dies well you
know everybody else is stuffed basically
because they can't acquire the spin lock
until this thread resumes where is here
there is no such thing happening and
another thing is hash table so we have
these large hash tables they are
implemented actually we don't store keys
inside the hash table buckets but we
just store pointers to
the actual triples inside this hash
tables and let's now assume that both
threads we have two threads they both
want to add 136 triple to this hash
table okay they both hash you know this
triple get some index they both see oh
it's not you know what I mean we use
here open addressing essentially so they
both say okay this is not you know the
Triple so we need to scan this forward
now the problem is that here we do use a
very short-lived spinlock but I would
like to show you how we actually avoid
this in most cases so what now happens
is both rates have found a bucket and
both try to lock it you know and let's
say that the left freight succeeds the
life thread now basically adds this
triple writes down the pointer to the
triple and the right thread sees that oh
you know it's already there so I
eliminate duplicates now this was a spin
lock the reason why I had to do spin
lock is simple because i would need
multicast in this case so when i was at
this point so there were two threads now
poised to actually you know occupy this
one bucket here what I had to do is
ahead to allocate fresh memory to hold
this triple and then so this essentially
means I would now have to modify that
these three locations I would have to
modify this location all atomically now
there is this multipass which can be
simulated using single casts but in
practice I really don't believe it's
worth the bother ya go ahead as well
machine for example you can use a great
protection memory to do that yes but I
don't happen this works on I on int'l
yeah I know yeah I was like please just
give that yet but unfortunately it
doesn't work like that so there
was a small spinlock here not so bad
because I will show you that we actually
eliminate the speed lock in most cases
another thing that final case this also
is something that we did in the system
then paralyzed much better but then
another thing that now happens is you
have to take into account modern
architectures so when a thread when a
core rights to memory it does so via the
CPU cache now if two cores right to the
same location even if they use these you
know finds instructions costs and all
that you know you still actually have to
synchronize the cache of these caches
are independent you have to synchronize
them so what usually then happens is you
know the one thread actually has to
write down you know whatever it had in
its cache and then you know the other
thread can now write into that location
as well and then you know the other
thread can load it so there are these
cache coherency protocols I mean the
whole point is that at the hardware
level if rates actually keep writing to
the same location you will get a very
slow system which won't scale very well
and in this situation we have well at
least three such bottlenecks so one was
this pointer to the end of the triple
table right every time you add a triple
you increment that pointer every thread
rights to that single vocation which
essentially means that every thread
keeps kind of freshing other threads
cash another thing is pointer to the
next triple to process that's as well so
you know every time you increment that
pointer you are thrashing other threads
caches and then you know in this is I is
pio P&amp;amp;P you have these entries for sbo
P&amp;amp;P some o P combinations can be quite
common so for example something is of
type person that's going to have the
same entry like every every person is
going to go into the same entry and
every time when you derive you know x
type person you will be writing into
this place so all this actually made the
system still not be actually there where
we wanted it to be so okay to address
one what we simply do is we amortize
this so instead of extracting triples
one by one we extract the window I think
a thousand triples or something like
this so you essentially pay I mean you
still have to hit synchronization but
you amortize it by you know doing it in
bulk to read the this problem of writing
we solved in a similar way and what we
actually do is you know for every thread
we reserve some space in this big table
in advance again in blocks of thousand I
think and then let's say that both rates
now want to add 13 to what they can do
now is first of all because this is now
private space for both rates they
speculatively add this there okay in
both of them do this at the same time
and now they both you know compute the
syndics but you know now there is no
problem which outlined on the previous
slide simply because this is there I
know exactly what the address of that is
and I can just cus into this no spin
work needed anymore simply because this
memory allocation has been done in
advance okay what what happens with that
well you know I mean more triples will
be produced so this guy can actually use
this space for the next triple that it
needs to insert and in this way you make
triple insertion fully lock-free
essentially so there is just one other
place you know where we use logs but you
know so just very small corner case as
well and then the third thing was so if
you remember we had this problem common
groups always went to the same place in
memory well what we do is essentially
where we have this global is be index
but we also keep indices per thread
which keep addresses into which keep
their private insertion points so you
know there is this one list here which
goes from this triple to this to this
but every thread actually uses a
different triple in that list as its
insertion point so this essentially
means that actually this is even better
because because this insertion point is
now private to thread one you don't even
need to use these cost instructions
actually you can simply write it into
memory and rely on you know simply just
atomicity of memory operations of the
processor to actually do this so with
all of that we were able to achieve this
so there these are different you know
datasets I won't go into where we
extracted them they're fairly common
commonly found in the data in this
community this was a machine with 16
cores 16 physical physical cores 32
virtual cores and actually we got pretty
decent speed up so the speed up for
example for this claris data set which
was the best was on 16 cores it was 13
points something which is a thing fairly
decent I mean you will never have a
speed-up of exactly you know one but and
then even with multi with with this
virtual course you still got the
speed-up of roughly about 20 yeah the
y-axis is the speed up so this is
essentially how long it takes vs1 parade
yes so this was our kind of measure of
success in a sense we wanted to use this
course as much as we could and there are
some datasets for example this one here
it turns out there is not a lot of
reasoning there so you know this
actually works well when there is
actually something interesting to do
yeah go ahead oh those are just various
different versions of the data log
program so you know like it's a naming
scheme which we introduced in the paper
so I have no time to really go into that
it's not expansion happen oh well it
also depends in some cases you have
double but in some cases you have an
order of magnitude well okay to get this
order of magnitude expansion we actually
that's the Le it's the extended that's
what it stands for we developed a bunch
of rules which make reasoning much
harder
and then you know you get this like blow
up by factor of 10 or 20 even in some
cases this is simply because these rules
in code creation of large clicks we also
compare this with the state of the art
now this is a bit difficult because in
the state of the art we I mean there is
no other system of that form that you
know works in main memory targets
multi-threading so we couldn't just take
whatever so what we decided is we
decided to compare it with systems
running on a ram disk which is you know
not ideal but the best that we could do
essentially and implemented using
relational databases and there was this
owl in light although i think they
renamed it into graph gruffly be a
recently that's like company which
specializes in building these kinds of
systems and only on a single thread and
i don't have the time to really explain
this table in detail just you know the
thought that the headline message is
that it worked really well as you would
imagine otherwise it would with any
goals now okay because you know we had
quite a bit of discussion i'm going to
have to skip some of this presentation
so just may be very briefly what you can
do is in rdf you can say that to
resources are equal so for example you
can say us and USA is the same thing and
these kinds of things are quite hard you
can still encode them using rules but
what happens then is this encoding using
rules essentially copies so everywhere
where you see us you copy the triple and
sale but this also can be USA and this
is well you know this actually leads to
the derivation of quite a large number
of triples but the worst thing is
actually it leads to the derivation of
the number of derivations that you
derive a triple is even higher its cubic
in this case so there is this well-known
technique of so-called rewriting i won't
be able to run through the example
because i'm running late here just may
be very briefly the idea is essentially
that when you say you know america is
the same as you
say then you pick one of these two
constants and you replace all the other
occurrences of the other constant with
one now this has been used quite often
in practice in a lot of these systems
but actually we are I think the first to
actually make sure that all of this
works completely correctly in all the
corner cases and in particular what we
need to do is we need to update the
rules as well so in this this particular
example has been constructed so that you
don't actually get correct results so
you lose completeness essentially if you
don't update the rules and that adds a
layer of complexity and and so on but as
I said I have no time to now go into the
details here just and this is again an
evaluation that we did so this is you
know how long it takes if we do stuff
using axiomatization on various numbers
of threads and this is how long it takes
if we do it using rewriting is it as you
can see quite a lot of you know data
sets this rewriting speeds things up
quite significantly but also the
question was to ensure good speed up you
know so that we still effectively use
these threads and that was yet another
kind of important aspect of this work I
have no time to go into that and the
final thing which I'd like to just
briefly mention is as you've already
mentioned you do this materialization
it's all there in the database you run
your queries and then something changes
and then you know the idea of you know
yeah go ahead you see that the previous
implementations would get that assembly
wrong yeah precisely it's actually we'll
run it by them and it is really wrong
yeah so I mean you know it's a minor
commit the contribution is not major we
haven't invented that technique but you
know doing it correctly is still
involved basically and I mean we care
really about provable correctness so for
all of these algorithms we have actually
proof that the algorithm works correctly
in the face of you know paralyzation and
threading I mean then of course I make a
bug and all I can't reduce a bug into
the soft
so the proof is still you know just
there sitting on my desk but it's better
than nothing real so I so did this last
thing is basically as you've said you
know we have some data the data changes
what do we do now do we just you know
throw everything away and start from
scratch and wait for hours obviously the
answer is no and people have already
looked at that there is the so-called
delete read arrived algorithm which I
will briefly outline this is the only
algorithm to my knowledge we've actually
surveyed the literature quite
extensively the only algorithm which
works with recursive rules I mean there
are there solutions which are applicable
to non recursive rules but they don't
work correctly and our goal was again to
be you know bomb-proof at least on paper
exactly modular bugs in our software so
how does this D read algorithm work so
here is a very simple program and this
is my input database and now you know
what this essentially does it says a
implies C 0 B implies C 0 and then the
sea-ice imply each other in a cycle and
this is what materialization looks like
and now let's assume that I want to
delete a obey what period algorithm is
going to do it is going to first delete
all facts which have a derivation from a
of a and the way this is done this
algorithm modifies the rules in this way
so it says if Eric's gets deleted then
c0 gets deleted if B of X gets deleted
then c0 gets deleted and then for each
of these rules here you'll get again
this version of a rule which essentially
says well you know propagate these
deletions onward this is again the main
problem here is that you can have
multiple derivations like so for example
here we have a of X and B of X both
imply c0 okay and then one sweet but you
know this is now incorrect so at this
point I'm saying just be a vase you know
stays essentially what you now have to
do is you now have to read arrive those
facts which stay essentially and this is
again done not quite this is a very
concise way of summarizing what the
algorithm does there are more things
they're happening under the hood but I
mean I think this is enough to
understand the algorithm of the
conceptual level so you say okay for all
of these facts that were derived check
you know whether they're still with ever
deleted check whether they're still
derived and what you then actually do is
you now put all those back facts back in
okay there was an animation missing so I
would not have to remove all these lines
basically from c0 2 c'n you had a
question it was already there and it was
not derived so you have to keep track of
what was the right and what was
initially very right you have to keep
true you can delete only stuff which was
there in the beginning there is a whole
different area of research which tries
to delete delete derived facts but there
the semantics is not clear so what does
this mean so that's the the problems
there are not you know efficiency of how
we do this I mean they are but there is
yet another problem like what does it
all mean in the first place we actually
don't address that problem we address
the problem of just delete the explicit
facts this is a problem and actually the
problem is that in the semantic web of
facts actually tend to have quite a lot
of different derivations simple because
though of the way these ontology czar
constructed these rules are quite
redundant you know the number of
derivations is in the billions where is
the number of facts that you derive are
in the low millions for example so
because of that actually you really get
this like huge blow up in the number of
derivations and this algorithm tends to
be quite inefficient because you're
quite often trash you know sometimes
with so examples where you trashed half
of the database basically in order to
actually read arrive it later so we
clearly don't want that and we develop
the new algorithm which we call the
backward forward algorithm so again I
won't present everything but just you
know the rough idea is ok we again want
to delete this aoa so now just like in
the deodar Gordon
we essentially identify facts which are
derived from a of a but then we say okay
? is this fact still drivable well okay
we now look through the rules and say
okay this fact can be derived by this
rule so let's now go to go backwards and
check boa boa is still there i haven't
derived it so that guy stays but then c
of a stays as well and i'm done because
you know c of c 0 vase days i don't
actually go on to propagate stuff across
this big circular chain this gets much
more complicated because of a variety of
reasons which i don't have the time to
go into so there is this backward and
forward step but you'll have to read the
paper i hope it gets accepted it's
currently under submission go ahead 0
jigging if he is desirable I'm sorry the
beer is that very sharp checking that if
he of a is actually the rival um no
because you've deleted it explicitly
well yeah I guess I guess it would be
yes yes yes yes I should have maybe said
that check whether it is the rival but
there is no rule which derives it yeah
okay you're right yes there would be I
didn't think about the presentation and
the conclusion so okay so I mean this is
a main memory system and what we'd like
to do is obviously use the memory more
efficiently now for that we would need
to use some kind of data compression
this is a bit more difficult I mean we
had a very efficient data structure for
compressing the Saudi of data but the
problem there is that we used variable
encoding variable length encoding for
various parts of you know these tables
and and that is a problem simply because
you these right instructions are
guaranteed to be atomic only in most
processors only if you're aligned to
word boundary basically now this is
quite difficult than you know
to ensure atomicity even if your data is
not aligned so then we just basically
drop that and maybe we'll get to this at
some point as well another thing is like
software I talked only about reasoning
and this was our main focus so far but
this system also supports query and
there it could be better simply because
you know as you I'm sure although there
is this notion of query planning so when
you get a queer you want to find the
plan which is most efficient require
currently use a very simple simplistic
algorithm for that but we have some
ideas on how to improve this using some
theoretical results and bounded hyper
treatment and then another thing is like
so to do this planning you need what you
call joint cardinality estimation so you
have some order of your atoms and you
want to know roughly how many triples
will this produce now in databases
usually they do this you by sampling the
data using one-dimensional distributions
and then making what they call variable
in the argument independence assumption
which essentially means that if you have
an any relation then all of these n
columns are somehow statistically
independent from each other now in a
database this may not be such a big
problem because you have lots of
different columns lots of different
tables and you know but in here you have
just one table the triple table and
doing this variable independence
assumption it doesn't really work so
I've got some ideas on how to do this
using n-dimensional histograms but then
the problem is basically you have your
query estimation procedure now becomes
exponential in the number of variables
but I have got some idea again how to
apply some theoretical results to reduce
that so we'll see keep you know watch
this space thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>