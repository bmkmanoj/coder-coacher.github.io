<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - What Can't We Compute on Data Streams? | Coder Coacher - Coaching Coders</title><meta content="BDA - What Can't We Compute on Data Streams? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - What Can't We Compute on Data Streams?</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8PHqcQAjiNE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so i will begin with a few caveats
first is that this is a biased guided
tour through the basics by biased i mean
it reflects my personal biases so it's a
it's what i think is the best
introduction and so I apologize to
everybody in advance for a meeting your
favorite result I probably will I should
also say that this talk some some Ilan
told me to the well that the audience is
going to be you did tell me the audience
is going to be not really theory and so
I prepared something which is rather
general and trying to address a non
theory audience so the theorists and the
audience you might think that I'm saying
very simple things and yes I indeed I am
but this is only the basics so hopefully
I'll hint at some of the deeper things
okay so let's begin with some history
the history of lower bounds for data
streams begins actually with the very
first paper on data streams which is by
Monroe and Patterson so they were
looking at the median finding problem
actually selection more generally so you
have a stream of M values i shall use em
throughout the top for the length of the
stream and they resolve at least their
main result was that using p palces this
can be done in m to the 1 over P space
the tilde indicates that I am ignoring
certain log factors the theta indicates
that this is both the lower and an upper
bound so in fact they did prove a lower
bound right and you will find this
beautiful piece of growth in their paper
and I put it up there actually to
emphasize to you that I think writing a
good a good paper is the work of
literature and I think we use this these
days it's so nice to see pros like that
all right so there is however if you if
you look carefully through what they're
saying they're talking about an
adversary argument and these arguments
have a major weakness which is that
they're making an assumption the
assumption here is that the algorithm
only stoves values from the stream to
solve the problem right now in general
you could imagine doing some arithmetic
on the values as you see them
and why not maybe that helps so fast for
about 20 years this gets us to the
modern history of the problem and this
very seminal paper really by alone
mattias integrity probably made
datastream algorithms an important topic
of study in theoretical CS there they
did many things in here among other
things they introduced the frequency
moments problem which you hear a lot
about which Graham also mentioned in his
talk this morning the paper introduced
several important algorithmic techniques
these are still relevant I won't talk
about the algorithmic techniques but
most importantly from my point of view
they laid the foundations of data stream
lower bounds in particular by
recognizing the central role of
communication complexity and
communication complexity gets around
this issue that I brought up earlier
with adversarial type lower bounds that
they make it makes no assumption about
what exactly the data stream algorithm
stores or for that matter how it
computes for its stores yeah so most of
the stock is really going to be about
communication complexity and what role
it plays in data stream lower bounds I'm
going to talk about very simple
communication games and what they imply
and you'll see i hope that proving lower
bounds like this is actually fun it's
it's a nice game all right so why is
communication complexity important well
there's this obvious little reduction
which sales take any problem on data
streams you can take your stream and
split it up into chunks like that givi
chunk to a player and now you have a
multiplayer communication game because
you could have just two players the
chunks don't have to be equal in size
there are situations where its
advantages to make them unequal in size
and having done this a peep a streaming
algorithm naturally turns into a peer
ound communication protocol where around
it depends on how you define it but one
natural definition is when each player
gets to send one message that
constitutes one round the messages they
send each other are of course just the
internal state of the data stream
algorithm so any communication lower
bound translates into a stream lower
bound usually the communication problem
that results this way by cutting up the
stream isn't very nice as a problem it's
not so clean so you'll sometimes want to
take a special case which you can
interpret community
aurelian turn this into some clean
problem about zeros and ones all right
so without further ado what are some
communication problems that come up so I
came that there are three fundamental
communication problems that really every
theoretical computer scientist and
actually every computer scientist for
that matter ought to know okay and these
would be index equality and set disjoin
is and if for some reason you didn't
know about these then you will whether
by the time I'm done you all know what
it is that you absolutely should know
about these problems now actually there
are two more modern problems that I
think have also turned out to be
fundamental so one of these is the gap
Hamming distance problem which and the
other is the L infinity problem I will
define every one of these things ok so
let's start by talking about index
possibly the simplest communication
problem so here we have two inputs we
have two players Alice has a string of n
bits and Bob has an index between one
and n the they must talk to each other
and compute the index function which is
the KF bit of Alice's string ok the
parties may use randomization so the
answer only needs to be correct with
high probability so we think of two
measures of success for the problem or
rather to cost measures so one is a
straight up communication cost which is
the number of bits exchanged in a
worst-case run you want to minimize this
of course and the other is the error
probability which is the worst case over
inputs of the probability that you give
the wrong answer you want to minimize
that two or maybe make it less than some
threshold Epsilon ok so the astute among
you and of course people who see this
before will notice that if Allison poker
really to interact among themselves then
there's an easy solution where Bob sends
the index over to Ellis a fraud bob has
only log and bits of information with
him and alice has em so Alice has the
harder task if she has to communicate to
Bob so to make the problem interesting I
let's say require that communication
only go from Alice to Bob and in this
case you can basically show that there
is nothing non-trivial that Alice can do
in Alice can of course send her entire
input over to Bob at a cost of n bits
and that's pretty much the best you can
do
this is not a hard team to show as
communication complexity theorems go in
fact it's an exercise its to be precise
exercise for point 20 in the textbook of
kosher Levites and a son and it's also a
paper of oblige so a Bluffs paper
actually shows a little bit more than
what I have just stated you can enhance
the problem to this thing called
augmented index in the Augmented index
problem a little birdie tells of the
first k minus 1 bits of x note that he
wants the caped bit of x when sometimes
this information should be useless but
well it requires a proof and you can
prove this ok and i'm not going to go
into how exactly you prove this i want
to say why this is important for
streaming and so i came to this very
important this is this is a problem that
will handle a whole bunch of streaming
lower bounds right away so i'll give you
an example so let's return to the Monroe
Patterson problem which is finding the
median of the stream so you can do the
following reduction from index ok so
you're given an index instance X inkay
Alice constructs the following stream
which is n minus 1 copies of negative
infinity followed by the numbers 2 4 6 8
etc but with the lower order bit tweaked
according to the corresponding bit of X
right so I'll refer to this as embedding
a bit string in the stream you'll see
this idea come up again now if you look
at only alice's imposed in my transfer
function the million is exactly that
number 2 plus x1 right now when Bob
inserts an appropriate number of copies
of plus infinity to the strain then the
medial start shifting to the right right
in fact if he inserts the right number
of copies then the median is exactly the
number corresponding to the desired bit
and since I've encoded the bit in the
lower order bit of this number then all
I need to do is figure out of the median
is odd or not and I've solved index so
because I've solved index it means that
I must have communicated order n bits
therefore we get the theorem that median
finding requires order in space so you
might think that was to simply ever it
was but actually this type of idea and
increasing your sophisticated versions
of this idea applied to a few
number of problems in the air streams I
mean I can't begin to summarize them all
I've just put a few random examples down
here that the time I know about well so
I'm not about to go into these things
but you can sort of vaguely see what
these are right you can talk about more
than one pass this talked about only one
pass you can talk about random water
streams yeah by the way the references
that I put down out just in there is
some matching between the references and
the results and I'm not indicating
abilities like okay so that's the
beginning here's another application of
index it's kind of different from all
these sequence type problems so here's a
graph problem so suppose you wanted to
decide whether or not your input graph
stream describe the connected graph yeah
so here's something you can do so take
your index instance and encode it as a
graph problem as follows so the vertices
are fixed like this you have st and then
you have n vertices 1 up to n Ellis
throws in edges that correspond to the
ones in her bit string and Bob throws in
all possible edges on the right except
for the one that that's that corresponds
to his index ok so in this case except
for the edge from 5 30 includes all else
you will notice that the vertex 5 is
disconnected from the rest of the graph
and it is precisely because there is no
edge from 5 to t which is precisely
because the because alice has a 0 in the
fifth position alright so this fairly
easy reduction shows you that
connectivity requires omega n space and
at least you've used a single pass right
so actually this Omega n is a fairly
general phenomenon for a lot of graph
problems you can do a simple reductions
like this which show that almost all
interesting graph problems basically
require Omega n amount of space so you
can't really shoot for sub linear in n
amount of space though suppose you could
shoot for an amount of space that's like
n times some small function of them and
this is pretty much the realm of semi
streaming algorithms which is a big
topic of study I suppose the triangle
counting is one of those few exceptions
where you can sometimes do something
better in space sub linear in terms of
number of her
is all right let's move on to a
different communication problem this is
the Equality problem so the Equality
problem Alice and Bob both have n bit
strings and the question is whether the
strings are the same or not as simple as
that all right so here we have a login
bit communication protocol using very
simple fingerprinting idea I think to
this crowd I don't need to explain what
fingerprinting is so I'll leave it at
that in fact you can get it down to just
constant communication if you assume a
shared random source but if you did not
have randomness then this problem
provably needs Omega hand communication
right actually you can see I can remove
the Omega it can say that you need
exactly n bits of communication ok so
this shows us that randomization is
important to solve the Equality problem
and this in turn allows us to prove for
a number of data stream situations that
randomization is unavoidable ok so
here's one classic application and so
these are frequency moments again which
Graham defined for us this morning I
will redefine them J comes the number of
occurrences of the item J in the stream
and the sum of the cape hours of these
frequencies is the KSR frequency moment
denoted capital F to the K capital F sub
K alright an important special case is
f0 which is the number of distinct
elements in the stream of the
cardinality of your set so here's the
reduction from the Equality problem to
determining the quantity F 0 right this
is actually that same idea of embedding
the bit string into the stream in the
lower order bit except that instead of
producing a stream of integers I'm not
producing a stream of pair of integers
it does not really matter i can
re-encode these things those integers if
I wish but for the purposes of problems
like frequency moment I don't care that
the elements themselves are integers
since I'm only counting their
frequencies all right so it's written
symbolically bro you can see what it
does it's easy to check that the number
of elements in the combined stream is
basically n 14
each position plus the number of places
where the bit strings x and y differ
right so where X has a 1 and Y has a
zero I have two distinct elements in the
stream otherwise that I have only one
element okay so so delta x y denotes
this number this is called the hamming
distance between X and Y so this means
that if i solve f0 again I've basically
solved the Equality problem some
claiming here that if I have a good
approximation for f0 this also solves
the Equality problem there's a slight
problem with that not quite suppose x
and y differ in only one position then
the and i would pretty much have to get
a very good approximation for a 0 right
i would have to approximate f02 within
plus minus 1 which is the same as
determining it exactly fortunately there
is an easy trick to handle this so what
we really need is to make sure that
whenever x and y are not equal somehow
they're having distance magically is
rather high right and I'm sure you know
how to do that just apply an error
correcting code that ensures this
property okay so this shows that if you
want it to approximate say the number of
distinct elements or really any
frequency moment then you need to use
randomization alright so those are the
first two fundamental problems the third
is the set is Jonas problem now let me
let me point out that index captures a
certain type of hardness index captures
the hardness of doing things in one pass
equality captures the hardness of doing
things deterministically right now there
are times when you think your problem is
hard despite randomness and despite
using multiple passes what you do then
then you bring out the heavy hammer of
set is Jonas alright so this is the
saddest Jonas problem Alice and Bob have
n bit strings which you can view as
subsets of 1 up to n and the question is
are these sets disjoint so the destroy
dysfunction is say one if the sets
intersect and 0 otherwise I mean
historically this is how it's been
denoted it's a bit unfortunate it should
probably call nan disjoint this but hey
so what it is here's a different view of
that same problem you can write Alice's
input on top of Bob's and now you have a
2 by n matrix and the problem can be
thought of as distinguishing between the
following two cases
case 0 every column has weight at most
one you need to think that most a single
one in every column and in the other
case in case one one of the columns has
weight to indicating an intersection
between the steps otherwise the rest
have waited most one so if you think
about it this is actually a special case
of the disjoint is problem where I'm
promised a lot about the input yeah
nevertheless you can show that this
version of disturbance is also hard it
requires Omega n communication this is
even if you have randomness and no
matter how much interaction you use yeah
it's a classic result in communication
complexity but for the purposes of data
streams you need a further extension of
this which is if you extend this problem
from two players to t players in the
natural way just replace the number two
in the statement of the problem with T
then you get the tea party dishonest
problem whose communication complexity
grows like n over T okay this last
result actually is the is the end of a
pretty long series of papers which I've
just indicated out there now here's one
basic application of this joint is this
one's going to use only two parties okay
so we come back to the graph
connectedness problem for which we
showed a one-pass lower ground we can
show that a multipass lower bound also
holds okay so we just use a different
reduction we construct a different graph
so now Alice has a string and Bob has a
string to be thought of as sets if you
like so ms the introduces edges from s
to I whenever I is not intersect okay
and Bob introduces edges from j 2t
whenever J is not in his set the
observation is that this graph is
connected if and only if the sets are
disjoint okay because if there is an
element in common between x and y then
neither alice nor Bob would have
introduced the appropriate age for
example 5 is an intersection in the
given example and neither Alice nor Bob
introduces an edge to vertex five right
so it's disconnected ok that that's all
there is to it it really is that simple
now the classic application of disjoin s
is lower bounds for the estimation of
frequency movements and so first let me
say something about what happened
so it's known that 4k between 0 &amp;amp; 2 this
problem is the proximal in space it's
about 1 over epsilon squared times some
log factors 4k bigger or equal to 2 the
space starts growing kind of polynomial
e in the length of the string right so
there is a dichotomy in the algorithms
and this dichotomy is in fact real so
you can prove a lower bound that shows
that 4k at least two you do need space
into the 1-2 over K and this is using
using the tea party destroy nest result
that I just put on the previous slide
together with the reduction that's you
two alone Mattias legatee right so the
so they didn't get this result they got
a weaker result but after after the
optimum communication bound for this
Jonas we have the result that is shown
here okay so they just reduced from
disjoint nests with a large number of
players are not the innovativeness here
which is using a number of players that
actually depends on the input length
once I tell you this I you can go home
and do this reduction yourself it's very
easy so all you do is convert each set
into a stream and observe that if the
sets are disjoint like I said then every
frequency that most of one so there's a
frequency moment is going to be low or
else if the 0 in case one the sets
intersect and sorry it should say sets
intersect in case one in this case there
is an element with rather high frequency
which leads to a very high value of FK
so there's enough of a gap that
approximating FK will distinguish the
two cases okay everything I've said so
far by the way is pretty old hat to
somebody who works on data stream lower
bound and so on now let's get to
somewhat more modern problems so the gap
Hamming distance problem is the
following communication problem and I'm
saying that this is another fundamental
communication problem so here Alice and
Bob again have n bit strings and the I
guess you don't want to view them as
subsets of n all right so carry over
from the previous slide the the function
in this case is the gap Hamming distance
function of G HD it's defined as 0 if x
and y are closed and having distance and
one of their far and hamming distance
we're closed means
square root and below the expectation
and over two so if you think of x and y
is being uniform random strings the
expected distances are we talking about
one standard deviation below that versus
one standard deviation above that and
let's say we don't care if the distance
lies kind of in the middle right so this
is an easier problem than just deciding
one between above and below a sharp
threshold okay so so result of mine and
rigor is that this communication problem
needs omega n work randomized multipass
forever and this gives us another okay
so this gives us another lower bound for
this FK approximation among other things
so notice that the previous we talked
about the dependence of the space on on
em okay there's also this dependence on
epsilon right you see this one over
epsilon square in the upper bound and
actually you you probably encountered
one over epsilon squared in several data
stream are proposed including some
mentioned in talks this morning so the
serum that you can get is that this one
over epsilon square is inevitable and
the reason it comes up somehow is
because many of these data stream
problems incorporate a solution to gap
hemming so for instance here's how a
distinct elements incorporates a
solution you just use the exact same
reduction as before and observe the fact
that the number of elements is equal to
n plus the Hamming distance which we had
observed before and and and that's
that's basically it right and if you
want if you want one more piece of
information than the to the exponent in
1 over epsilon squared is directly
related to the square root n which witch
is which appears and kept having
distance okay all right final of the
basic problems is the L infinity problem
all right so this is the statement of
the problem so here Alice and Bob have
strings of length n a vector of length n
with entries in one up to em for some
large quantity M right so these are not
bit strings anymore this spring's of
integers say
between the rentals defined in the usual
way so L infinity is the max difference
between X I and X I minus y overall I
right now you need to distinguish
between the case when the L infinity
distance is at most of one meaning that
x and y are roughly equal on every
coordinate versus the L infinity
distance is M minus 1 which means there
exists a coordinate where the difference
is as high as n minus 1 as high as it
can be ok so so in a result in a paper
of barrio sefa tell they show that the
complexity of this problem scales as
linear in n it also does degrade with em
as you can imagine once em becomes very
large then the two cases 0ps on the one
case are kind of far apart further apart
so it should be easier to distinguish
between them alright so that's the right
behavior and over M Squared so this also
has many applications and a typical
application is lower bounds for
estimating norms for data streams with
deletions ok so data streams with
deletions if you don't like deletions
they come up naturally if you want to do
something like what is the difference
between yesterday's IP traffic in
today's I'd be traffic so let's say
you've collected something about ok so
you can imagine this as inserting all of
yesterday's IP traffic and then deleting
off trailerby traffic in there for
getting a different stream and any kind
of interesting thing you want to
estimate for that will at some level be
solving a version of the L infinity
problem ok so that's the basic biased
guided tour through the through
communication complexity problems I've
only mentioned the simple ones of course
as I said that's that's all I'm going to
mention now a word about how we actually
go about proving communication lower
bounds you see so so far all I've said
is you can reduce one of these
communication problems to your data
stream problem right actually there is a
whole book out there communication
complexity by crucial evidence on which
which is entirely about how you prove
lower bounds on communication I'm not
going to be able to summarize anything
meaningful in a single slide I should
mention though that the textbook already
is kind of outdated despite its
brilliance because there's a whole bunch
of new techniques
that haven't made it into the book which
is close to two decades old at this
point now one very common theme that
comes up in these new style lower bounds
is what's what's called direct sum so a
direct some result is something of the
following form so so you take a small
lower bound right by small lower bound I
mean something like you know Alice and
Bob xinput sort of some constant size
right if they're of constant size then
well you should have a constant lower
bound usually all you need to do to
prove this is that the problem isn't
completely trivial and cannot be solved
without communication all right so you
have a constant lower bound now scale up
this lower bound by saying something
along the lines of as the input size
grows by a factor of n then the
communication require also grows by a
factor close to n right if this happens
then you say that the problem
demonstrates direct some behavior ok I
should mention that this isn't a trivial
thing to prove it all far from it and
there are situations where it's not true
even so that's why it's hard but one way
in which results like this can be proven
to to not count the number of bits
communicated between Alice and Bob but
instead to look at the mutual
information between the input and the
messages so mutual information being
standard concept from information theory
and this paradigm is called the
information complexity paradigm when it
applies it's very easy to prove lower
bounds I will not say more about this
that sort of a talk on its own trying to
explain this paradigm but if any of you
is coming to stock in a week's time I'm
scheduled to give a talk on that there
so you can consider that part two of
this talk all right at any rate with
with some information theoretic ideas we
can actually handle all sides basic
communication problems and get all of
those communication results that I claim
ok I should mention though that it's not
quite so simple as saying that ok you
have you can view your problem as n fold
blow-up of a simpler problem and
therefore the results follow sometimes
we need to argue this look very
carefully and you need often some deep
mathematics in addition so just to give
you an example this is I think the
deepest mathematics that has come up so
far in any of these problems so it comes
up in the context of the gaff Hamming
distance problem and
requires for example this theorem which
you can take take home with a little
nugget so if you have a set with high
Gaussian volume in in RN case it's an
n-dimensional set with high Gaussian
volume not saying exactly what high is
I'm not quantifying things precisely but
if you do in such a set needs to be
thick along several directions where
thick means if you project along that
direction then you don't have a very
concentrated set intuitive may be but
there's all sorts of ifs and buts for
example I didn't say that that the
directions are actually orthogonal there
near orthogonal only and that sort of
makes all the difference it makes this
thing rather Harry to prove so this
really can be seen as a result in
functional analysis so if we had to dive
into this two together bound ok so these
are just the basics as I said already
these five communication problems if you
understand them well and master them you
can derive a whole bunch of results of
our data streams here's a sampling of
some other results that you can derive
very very nice paper of goo hye at all
that i should put put in a plug for so
they looked at information divergences
differences between probability
distributions and there's very different
various ways to measure difference
between property distributions so they
showed that with the two natural
exceptions of l1 and l2 it's impossible
to build small sketches for these so
basically giving you a complete theory
of what's doable and they use reductions
at the end at the end of the day they're
using reductions from dis joiners and
gap having if you wanted to see an
example of a very sophisticated
reduction in this space then i would
recommend the lower bound for graph
diameter estimation from Feigenbaum et
al this is also a reduction from index
but it's quite a challenge to find it I
mean I bet that if I told you reduce
from index and sent you home and you
haven't seen this paper before that you
couldn't come up with this quickly it's
a sophisticated reduction then sometimes
you need to generalize the problem in
some unexpected ways so there is some
recent work
I had with gram and Andrew on
probabilistic checking of data
structures and these reductions
basically from version of index that
involves multipla many players okay so i
won't say much more about how we can
make a problem with looks like index and
involves many players but you can and
that's what gives you the result let me
end by just putting up to my favorite
open problems in this space so for
estimating the length of the longest
increasing subsequence of a data stream
we know that the deterministic
complexity is order root them we don't
know anything about it randomized
complexity could be order roodemit could
be order log n right it's wide open and
i think this is a very interesting open
open question a different kind of hope
in question which is probably more
appealing to theorists is can we get
some very generally applicable direct
some results in communication complexity
i think this is a bit of a holy grail
now in communication complexity there's
a lot of people seeking it but if we got
there then i can imagine a unified
framework for several data stream lower
bounds more unified that there is right
now all right thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>