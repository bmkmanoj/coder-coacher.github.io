<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Direct Linear Structure-from-Motion | Coder Coacher - Coaching Coders</title><meta content="Direct Linear Structure-from-Motion - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Direct Linear Structure-from-Motion</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MaKOE25Xo_M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning thanks for coming it's my
pleasure to welcome in turn for the
second time so you visited us a few
years ago so paint graduated from the
Hong Kong University of Science and
Technology 2007 and from the down seven
to 14 he was with National University of
Singapore my alma mater so now he's with
simon fraser university so ping is very
well published in many areas ranging
from computational photography where
it's done video stabilization and so on
if your image matting and to low-level
vision physics-based vision photometric
stereo and so on and stereo and now
structure for function which is going to
be talking about today
thank you something and thank you all
for coming to my presentation my talk
the title is director linear camera
registration I change the title
yesterday evening I think this new title
reflect the scope of the talk better I'm
sorry just one second yeah for those are
watching live you can actually submit
life questions Florida thank you so this
talk is about structure from motion
which is a technique to recover three
distinct points and the camera poses in
3d space from just two dimensional
images this is a classical problem
computer vision and a larger well known
systems has been developed including the
famous for the tourism system in most of
the systems it consists of the following
steps the first step is a computation of
epipolar geometry basically the world
compute the relative post of two or
three cameras according to the future
correspondences in the images under the
many anagen algorithms such as six point
seven point eight point and five point
algorithms depending on the number of
feature points they are using and the
second step I chords camera registration
basically we need to upgrade their
relative pairwise motion into the
absolute global camera poses including
orientations and positions basically we
need to put our cameras in common global
coordinate system okay if the cameras
are not calibrated beforehand we still
need to do the auto calibration but I
skip that step because a lot of the
modern structure for motion systems are
focused on calibrated cameras so that
the last step is the bundle adjustment
which is a nonlinear optimization on to
fine-tune camera poses and the same
point coordinates to minimize the
reproduction arrow so if we look at this
three simple steps the first and the
third step are all very well studied
weasel you're arrogant theories and AG
reasons but actually the second step is
a open ad hoc and the heuristic in fact
it will refer to this classic textbook
of three division you can find that it
described the second step of camera
registration as a black art so if this
is a black heart how do exist insistence
solve this issue let's take this popular
incremental structure from motion
approach as an example it will first
begin with just the apparel images
starting from two images with the
computed epipolar geometry we can obtain
an initial reconstruction with just two
cameras and some same points and after
that we can add additional cameras one
by one but we cannot keep adding to the
Nazca camera because of error
accumulation so usually after adding a
few cameras we need to perform the
nonlinear boundary adjustment to
minimize the error illumination and then
after that we are going to go through
the same iteration of adding cameras and
the local bundle adjustment and here we
finish adding all cameras we're going to
do on the so called the global bundle
adjustment again to minimize the
reprojection error so that is the
typical pipeline of most of existing
structure from motion systems and as we
can tell della two major drawbacks in
this kind of conventional approach
firstly it caused the nonlinear bundle
adjustment a lot of times which is going
to be computationally expensive and in
fact we're going to see that usually
more than ninety percent of the
computation time is spent on this bundle
adjustment so it's inefficient and
secondary if we look at the problem from
the point of view of optimization this
incremental approach actually fix some
of the cameras before it's solving other
cameras so this kind of all symmetric
formulation
milliony leads to inferior results so
what we want to do is to solve our
cameras simultaneously to initialize the
bundle adjustment and if we can do so
the structure from motion pipeline is
going to be significantly simplified to
have just these three can you define the
steps and this approach is known as the
global structure from motion there has
been quite a number of very interesting
works for example these two methods they
can solve the camera orientations or
sometimes it's referred as the solution
of rotations of cameras they can solve
the rotations very nicely but the
solution to the camera centers or
translations sometimes get degenerated
this method of deriving elegant cause
the convex optimization by minimizing
the L infinity norm of the reprojection
arrow um it's it's a result is very
strong but we all know the L infinity
norm is sensitive to outliers so the
requires very careful outlier removal to
work well on real data and this work it
combines the discrete and continuous
optimization to solve the camera
registration problem but it is limited
to cameras roughly on the same plane
this more recent work derived a linear
solution for camera translations yet it
is degenerated at the collinear camera
motion although it sounds like a
collinear camera motion is just a
special case but it's actually a very
important special case if you think
about Street view images captured by a
car moving along a street those images
are roughly on an on a straight line so
canini emotion is an important special
case that we cannot ignore so we derive
a direct lineal method that has the
following on interesting properties
firstly the soil translation secondly it
does not degenerate at the canini
emotion
and the Saturday is Nina and the robust
so in the following I'm going to present
the detail our method so basically for
the global method the input are the
pairwise relative rotation and the
translation between camera pals we use
this rotation matrix r IJ and a
translation vector C IJ to indicate the
relative rotation and translation
between two cameras and what we want to
recover are the absolute orientation and
the position of our cameras yes with
pairwise relations rather than triplets
um good question um pairwise relations
are relatively are easy to compute
because it requires the weaker arm
matching between images for example you
will start with triplet that means you
have to have sufficient of view overlap
between all the story images we're going
to come to that later on in the talk
because you don't have you overlap the
instructions are disconnected anyway so
you sort of have to assume that you have
um not true because it could happen that
you have view overlap between the first
and second image and the third and the
second image but you might not have the
overlap between the first and the third
a images I mean the scales are
indeterminate unless there are some
points in common in which case you have
some pointing in common but the points
in common enough to determine the
relative motion between the first and
third a camera in many times we have
examples like that in they don't okay
okay at this moment let's assume we're
going to begin with a pairwise motion
all right so um actually the talk this
talk is going to focus on the
computation of camera centers we take
this existing method to compute the
camera orientations this message
basically take the lead Jibril averaging
approach basically the represent a
rotation matrix are by this Lee algebra
Omega which is a three by one vector it
has a multiplication of the rotation
angle with our n which is the unit the
unit the orientation of the rotation ax
ok so under this knee algebra
representation on the relative the
relative rotation constraint between two
cameras becomes a linear equation of the
Tunis algebra and so that they can
collect this kind of linear equations
and solve all our camera orientations in
a single linear equation okay so um
interesting properties that the camera
orientation can be solved without
knowing the camera center positions so
this is that you your approach yes n is
the orientation of the rotation ax theta
is the rotation angle yes so taking you
can't directly subtract two of these and
have a meaningful relationship right you
can take return ian divisions if you
want but you can't just subtract that
it's actually yes you are precisely
speaking on you need to take the skew
symmetric form of this vector Omega
right and that gives you a matrix and if
you take the exponential of that matrix
it would be the rotation matrix R yeah
there's a little more involved the math
but this is not our work and i'm not
going to match too much technical
details of it yeah but anyway let's
assume the camera orientations can be
solved and in the legs we are going to
just focus on the camera translations ok
let's first talk about the case of just
the three cameras if we have just those
three cameras we suppose two of them
their positions are known we can then
calculate the position of the third one
by simply intersecting these two nines
the relative translation orientations
but in real data these two lines there
are levuka planners so what do we need
to do is to take the mutual
perpendicular line segment a B and
define the middle point all right and
then we take the middle point as an
estimation of the third camera what is
really
interesting here is that we find the two
end points a B of this perpendicular
line segment they can actually calculate
can be calculated by these two linear
equations from the two camera centers
and this 2 matrix m1 m2 that can be pre
computed I'm going to explain the
computation in the next slide but at
this moment let's just assume they are
known then under this kind of two
equations will I be able to derive a
linear constraint between the three
camera positions and we can actually
obtain similar equations by computing
the perdition say of CI from the other
two cameras in that way we get three
linear vector equations and four three
vector unknowns so that we can solve arm
the camera positions of the three
cameras altogether consideration of
rejection a over here yeah we don't we
don't consider reprojection it's all
about the epipolar geometry here so in
the next let me explain how do we
calculate these two involved matrices m1
m2 so let's take the m1 as an example it
basically consists of a rotation and the
scanning to match on the rotate the
orientation and the length of this
relative motion to this one ok and for
the rotation component it can be
computed very easily for example if you
look at a problem in the local
coordinate system of this camera because
these two orientations are known so we
can determine rotation matrix to match
that you are in tations so the rotation
is easy the scanning component requires
us to determine the relative length of
these two bass lines and these can be
calculated according to the ratio of a
comer slim points depth we can
reconstruct this same point from this
pair of camera and the reconstructed
again
from another pair of camera and then the
ratio of these two depths will give us
the ratio of these two base line dance
so in that way we can compute the matrix
M 1 and what is really interesting is
that the whole computation of this
matrix m does not degenerate even the
three cameras alone exactly the same man
because both the rotation and the
scanning factor does not be generated
when cameras are known the same line so
this enables this method work with
canini emotion and we actually verify
that with experiment on in this chart
the horizontal axe is the smallest the
triangle angle formed by the three
cameras the vertical axe is the position
arrow of the camera centers so we find
that even when the three cameras are on
the same line this our method won't be
generate happens that there are some
configurations there are purifications
so what happens that would it lower you
some cameras a popular game camera and
rotating and move so what happens when I
have good question um I'm not too sure
about that having to think about this
question we use the projection which is
like well it might it might not work for
this message because it sounds like you
have two cameras at exactly the same
place yeah but rotated arm so we won't
be able to use that two cameras at the
same location to reconstruct anything
point so that for this method presenter
here I don't think it will work she
knows you have to check the such
generous is me check them out and then
yes yes we need we need to have some
outlier checkers so in the next I'm
going to generalize this message to the
case of multiple cameras if we have
multiple cameras we can give
find a graph structure well every camera
is a vertex and the two vertex are
collected if the relative motion between
the two cameras can be determined okay
we call it a match graph and we first
are defying a so-called attributed graph
which is a sub graph of the previous
graph and it is found by triangles glued
on a share the edge okay we're going to
just look at the biggest triplet graph
and then we can collect linear equations
from the triangles in this arm triplet
graph by stacking all the linear
equations together we can formulate the
camera translation estimation as a
linear equation ax equal to zero and we
can then solve all camera or additions
in a single equation so in this way we
can solve both the orientations and the
positions of our camera once this is
done we can then triangulate the
corresponding feature locations to
generate the same points so in this kind
of structure for motion approach we
actually separate the computation of
motion and structure we first compute a
motion and then we are solve the
structure this is kind of advantages
because the motion is usually a smaller
scale problem there are much less
parameters involved so in the next I'm
going to show you some of the results on
in this method we first evaluate the
accuracy of our method on this three
data set with known ground troops camera
motion and we compare it with some other
reason the method um for each data set
we highlight the result with smallest
arrow in red color this C is the
perdition arrow in millimeter of camera
centers are indicates the orientation
error in camera orientations in degree
so as you can tell you most of the time
our master will generate the smallest on
arrow in order data
and this is another example to verify
the computational efficiency of our
method compared with the visitor sfm
package so I firstly want to highlight
this row it shows the total computation
time exclude excluding the time spent on
feature matching and our AP Paula
geometry computation so um as you can
tell because our method is strictly
linear so when the data set becomes
larger the computational efficiency
becomes kalila so this rightmost on this
Trevi Fountain example our message
roughly 13 times faster than visit SF
aim and then we look at at the time
spend on bound adjustment we can tell
that for the arm incremental approach by
the way vs FM is an incremental
structure for motion system so it spend
most of the time on the boundary
adjustment all right and then we compare
the number of images reconstructed by
both method and we find is rather
similar but later we actually realized
this calculation here is biased because
our master can only reconstruct a
so-called a triplet graph ok so at that
time we only take the images in the
largest collected tribute at the graph
and throw those images to Bose system
and in that situation the reconstructed
similar number of images but then we
find is biased we actually should throw
or images to both distance and see what
will happen and in that situation
actually visualize of them is actually
better it will reconstruct over 500
images and our method only reconstructed
about 360 images so what is the problem
involved here this is actually we found
this because of the data Association we
can only reconstruct arm cameras in a
triplet graph so what if one edge is
missing what if we cannot determine the
relative motion between these two
cameras if that is the case our method
will break the data set into two and the
generator isolated 3d
instructions for each of them that is
the problem of the method i just
described and this is due to the scale
and b duty in 3d reconstruction
basically the structure from motion
agrasen won't know the absolute size of
a 3d structure for example if we have
three cameras they want to know if the
three camera form a small triangle on
the left or a bigger triangle on the
right so that means if the two triangles
are collected by a comma edge then we
can determine the relative scale of the
two reconstructions by the shared edge
but if the two triangles are only
collected by a vertex we won't be able
to determine the relative scale of that
two triangles that's why we can only
reconstruct a triplet a graph in the
message we just described earlier and
this happens in real data so for example
this is some Street view images and as I
explained earlier in this particular
case we can compute the relative motion
between these two images and these two
images but somehow the view overlap
between these two images is insufficient
to determine its relative motion and in
this situation if we throw these images
to the cistern I just described it's
going to break that data set into two
and get to reconstructions while the
ideally we hope to have this kind of
result okay and this also happens on our
internet images for example on those
other images collected from internet for
popular treason sides okay and armed to
give you a better idea about the data
distribution we visualize the
distribution here basically for the
popular trees inside the images are
distributed very unevenly there are some
popular viewpoints that has a lot of
images but there are very little image
in between so in a lot of time we
can determine the relative motion
between images at a similar view point
but we can we cannot determine relative
motion across viewpoints and when that
happens the previous method i just
described will generate a significant
distortion to the finer of
reconstruction so to address this issue
we might include simpoints into the
formulation for example we might replace
the third camera biasing point p and
they derive a similar linear constraint
about their positions well the slight
difference here is that this is a point
p is a point not a camera but very
interestingly we can still compute these
two matrix m 1 and 2 i'm going to
explain how to compute them in x night
at this moment it's a dune they can be
computed okay if we can compute them
well this will make it easier to form a
collective triplet graph because usually
a sim point is visible in many images so
this kind of edges between points and
cameras will help us to arm to collect
the cameras together basic idea was
drenched in the collection and then we
can just solve our points and cameras
together it seems like a simple solution
to the problem but o in this night I'm
first explained on how to compute that u
matrix in okay they can still be
computed if this is a sin point as I
mentioned that as a rotation component
the rotation can be computed as you you
for example we can look at that the
local coordinate system of a camera and
then because this relative this
orientation is known this orientation is
known so we can determine the rotation
and then what about the scale the scale
component can be decided by the midpoint
algorithm for triangulation basically on
for the scale factor we need to compute
the ratio of these names to this one and
there are another similar ratio
these two ratios can be solved by the
midpoint algorithm which basically
explore are the perpendicularity of a p
with respect to this X I and this
orientation XJ basically we can write an
algebraic equation okay for this to
perpendicularity and the e we nuke it at
the local coordinate assisting of a
camera and if we assume the baseline
ends here is unit of one then we can
simply write this as this linear
equation basically arm this SI is the
ratio between this distance and this one
so when the baseline lens is unit of one
s is simply that distance between a 2
c-i so the calculator of a becomes CI
plus SI x x i so that we get this kind
of need a constraint so we can solve the
two scales on from this media equation
so okay um other matrix can be computed
we can formulate everything together to
solve cameras and points together but
there are some drawbacks the first the
drawback is that the scale the problem
becomes bigger that are usually much
more thing points dying cameras okay the
problem becomes a little more
complicated but that is still okay the
real issue is that the formulation I
explained just now will introduce some
non-existent parameters for example you
we consider the future correspondence of
these two images if the feature
correspondence is just the wrong then
that does not exist such kind of point P
that can be projected to the to feature
locations essentially we introduce a
parameter P that should not existent
into the linear system and the
experimental we found this is very bad
it's much worse than there are new year
outlier equations we must get rid of
them so to address this issue we look at
chu camera pairs well as
engl same point is both visible and if
so we can derive a linear constraint
about that point syndication in both
camera pails and we can then further
eliminate the same point to obtain
linear constraints of the camera and
occasions in this way this is an idiot
constraint because the right-hand side
of both the equations here are linear
with respect to the camera locations so
essentially it gives us a linear
equation arm of the camera centers fuse
yes now he is seen from crc jzk and Cl
but you're not linking CITC que se jets
yes yes it's a um we don't we don't have
to have that link between them as known
as as known as we know this is the same
thing point on that is visible in all
these cameras right i mean initially in
wins describing the problem where
they're not lit triplet the triplets are
not there yes and then you say you know
as long as there's some point p that's
seen by parts and not triplets then it's
okay yes i guess the question is how do
you determine its assets info okay um
yeah two determines the same thing point
then you must have some linkages between
them but to derive this formulation we
don't require this kind of linkage yeah
does that answer your question
this is equation will be given from
difference p so assume you're gonna like
what piece yes right yes so some of the
move you want some of the money you are
right some of the some of them will be
wrong because it was a run future
correspondence this equation is going to
be wrong that's right yes but but the
last thing is that in the words the case
when this is a round match it only
introduces an outlier equation it does
not introduce this kind of non-existent
parameter an outlier equation is much
easier to deal with than a non-existent
parameter in a linear system okay so
essentially on this kind of formulation
now allow us to propagate information
along a feature track you are right
actually on we need some linkage between
these two cameras and these two cameras
so that we know this point P is the same
point so actually this there is a
feature tragic feature trajectory to
link them together and it's now the
formulation becomes that once you have a
feature trajectory then we can renate
the scale between the camera pairs along
the same feature trajectory we does not
rely on tripit of the graph anymore so
the formulation becomes that firstly we
are going to collect the feature tracks
unmount the images and then we can build
linear equations from the future tracks
a long feature track can generate many
equations and the last equation is not a
typo you see this C 3 appears twice on
basically for a trajectory of just three
cameras we can um basically use the
camera in the middle tries to get a
linear equation so this precisely
corresponds to the case when we can only
determine the relative motion between c2
and c3 and the c3 and c4 but cannot
determine the motion between c2 and c4
okay in that case we can stare up at any
linear equations and then we stack all
the linear equations together and we
solve them all together too
at all camera indications okay yes just
use the triplets if you have two cameras
you have three cameras and they see some
points in common yep that you can figure
out the relative scale on their
translations right um triplets are good
but the issue is that sometimes that
data Association isn't a strong enough
to determine the triple Utley the outer
two cameras may not have actually
bundled together or something you guys I
said it's not but as long as the two
pairwise reconstructions have some 3d
points in common then based on the scale
of those points you can determine the
relative scales between the two so can't
you just get the the MS that you need
now the reason even if it's missing
can't you get the scale just from the
points that are overlapping um I'm not
sure you we kind of think is your last
one the c2 c3 in other words white look
for quadruplets what you could always
just do trip because um because in that
away we we can only get the two
equations for the street on cameras we
want to be able to get a certain
equation and that that is insufficient
to solve the system yeah so in this way
we we actually can get them bet we can
get a stronger constrained like two
systems that you free constructed right
disconnected by a scale right so you're
right one equation so to might be enough
right I don't know that you need three
great um I think we need a three I can
go back to data so nice okay okay now
let's still resume that talk we can
discuss that in the afternoon okay so
okay anyway now we have a linear
equation like that ax equal to 0 and we
try to solve it for all camera center
positions and to be more robust we can
actually use the modern l1 optimization
technique we can solve this equation by
minimizing is l1 norm that would be more
rubbers that you various outliers like
outlier future correspondence outlier
essential matrix in the system so
essentially for the l1 optimized
asian on we need to minimize the l1 norm
of the residue vector this residue is
basically this a multiplied with ex and
the X is lying in this kind of space
Omega to avoid the trivial solution X
equal to 0 that is our formulation of
the l1 optimization to minimize this
problem we basically take the augmentin
a clown G function um this is very
similar to the usual acknowledged
multiplier basically this is the
Lagrange multiplier and this is a
quadratic attend to ensure the
convergence region is not real ok and
this message can be solved by this
so-called a tallit oscillating direction
direction method for multipliers
basically just it turret of lee solve or
they involve the parameters e x and the
lambda one by one in the iterative
fashion and it turns out that ye and the
nom de are very easy to solve but the
solution to x is a little bit hard
because we have this constraint on X so
to solve that we further arm massage
this ad mmm a little bit we linearize
the objective function by something
similar to china expansion to this kind
of way i'm going to skip most of the
math but in the end we can get a
closed-form update for the X so that we
can solve or they involve the parameters
one by one in closed form solution to
get all camera centers and as new year
when our cameras are solved that we then
triangulate the three distinct points
okay so I think it's time to show some
of the experimental results so again we
first evaluate on our results on this
data set with ground troops camera
motion and we compared with some reason
the method um we tested our message with
both l1 and l2 norm optimization we find
that for example in this to data set
postmessage give gives very similar
result but on this data set is actually
a more challenging case because of the
repetitive window patterns that are more
outliers there so in this case
the l1 method output outperforms the l2
master significantly and this nice shows
an evaluation on synthetic data with
weak data Association this is the
particular case of three cameras we
generate a threesome setting a camera
and three some 3d points in this space
and project them to our cameras to
compute the essential matrix and then to
solve the reconstruction problem we make
the essential matrix between two of the
cameras is very poor by limiting the
number of future correspondence to ten
and add significant a Gaussian noise to
the location of the feature on points in
that way we make the essential matrix
estimation between two of the cameras
very bad and there are then we plot the
perdition area of the cameras are with
respect to different amounts of our
Gaussian noise to the future locations
and we find our method is more robust to
our previous method and then we further
compare it with VCS FM on this data set
with relatively more images and
similarly we find that this director
linear approach is much faster than
vista fame on here we are maybe seven or
eight times faster because we are using
the l1 optimization so it is neither
snow or than the method i described the
earlier then here we show the number
reconstructed images at this time the
number of images reconstructed is closer
with the vision sfm but actually we
still have maybe 20 some images ness and
we actually checked what's happening
there we found in reconstructing more
images does not necessarily mean it's a
better result we found some of the
images that are included in the results
of this as a Fame by a round camera
registration because arm repetitive
feature
london sees the camera is just
reconstructed at a wrong location okay
and this is a validation basically we
check the consistency of our result and
that from with sfm we find that the
camera centers and camera orientations
that rather consistent to each other
okay and on this particular example
somehow because of the data association
between the images is weak the visuals
defame break it into six different 3d
reconstructions with these default
parameters okay so to summarize I've
talked about a direct lineal formulation
for camera registration basically the
problem becomes very simple just the
linear equation a equals ax equal to 0
this formulation is the rub us to
connealy emotion and the it's rubber
statue arm relatively weak data
Association and this simple formulation
open up new opportunities such as we can
use the modern l1 optimization to deal
with noisy data and I think there are
other opportunities for example we might
think about the drifting problem well
conventionally drifting problem can be
measured by for example the conveyance
matrix of the arm camera center
locations when the caverns is large we
know on probably that's drifting but
with this new formulation we can think
about drifting happens when this linear
equation is insufficient to determine X
uniquely and we have other techniques to
allowance this drifting such as for
example condition numbers against based
analysis and so on and we might also
apply these two studies the snom problem
we know in stem a critical open problem
is the so-called loop closure loop
closure usually consists of two steps
loop detection and the nupe optimization
basically the new closure problem is
that say
we have a camera that moves a big
distance move for a big circle and they
come back okay if the green curve shows
the true camera motion then what do you
get from the incremental system is you
you need this radical okay so we need to
somehow detect that there is a noob and
then optimize the radical somehow drag
this radical to the to bring the green
curve so on the Union it is involve some
nonlinear optimization and the arm is a
little bit complicated for real-time
performance but with our formulation
it's very simple on a loop closure it's
just the one more linear equation in the
linear system it won't cost too much
additional computation time thank you
for your attention
when you confront the proof that crap
then it's possible that these are that
the measurements are are being somehow
or counted depending on which lets you
choose because when you have a highly
against basically a lot of matches
between the cameras there are
essentially a lot of triplets and if you
count how many times each camera gets
included in a triplet right they vary
right right there may be like a central
camera which overlaps every right right
so is there some sort of over counting
we have a skin actually we there are
some details I didn't including the talk
basically we have a skin to select
sufficient that triangles to define the
constraints yeah basically we are we're
going to count how many times a camera
has been included in the linear equation
we need that number otherwise the
equations are going to be unbalanced
yeah yeah yes I'll you show a tech speed
up which is very impressive thank you
but I thought it might be even more cuz
you're not doing any nonlinear
authorization there's no rehabilitation
there's no fun lingers so we still do a
bundle adjustment um let me come back to
these um yeah so we only exclude
actually firstly we exclude the time for
feature matching and the arm hepatology
armature computation yeah the time we
have here is mostly building the linear
equations and actually we did some
theater rings of the feature tracks like
I mentioned we are for the triplet based
on messler we need to select the
triangles here we need to select a
feature tracks yeah we need to choose
the best of feature tracks possible
normalization um I think so the outlier
the future tracks in action can be
easily panel realized solving the salt
solution of the linear equation for data
pod I'm not sure yeah have a question
about the data Association so you said
so one of the things is one of the
process
some week that Association and but have
you conducted any experiments where the
data associations perfect because the
midpoint algorithm that you solve assume
this perpendicularly of strain in a and
B in perfect attraction that is zero you
will not be able to get your constraints
for the midpoint algorithm construct
those and matrices if the wreckage
engine is perfect is that true um
firstly um I think that it usually won't
happen even your data Association is
perfect your future location want to be
perfected that means this you know you
you need that two nines almost level
coplanar in 3d space and I think even
they are coplanar the midpoint a
greasing is still valid basically the
middle point algorithm is trying to
parameterizing the two nines and to try
to find a point on both of the lines and
collecting that two points requiring the
collection is perpendicular to both
lines that's the midpoint algorithm but
I forget to mention that we use the
middle point algorithm to compute the
scale when there's two cameras and one
thing point we actually cannot use it
for the case when all the word has it
our cameras because we use meter point
algorithm for data case then it will
become degenerated when the street
cameras are on the same line the middle
point algorithm is degenerative if the
triangle is online yeah but we can use
it for same point because you you need a
same point one the collinear with the
two cameras that can observe it
in the question well it's not let's
thank our speaker once more thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>