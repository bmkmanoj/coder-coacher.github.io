<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session 10 | Coder Coacher - Coaching Coders</title><meta content="Oral Session 10 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session 10</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yZQM2oSCoSE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thank you very much Mike and thanks to
the organizers for inviting me and it's
a real pleasure to be here I think it's
a actually a great opportunity to
connect back some of the things that are
going on in genomic sequencing you know
to the to the community here it's mike
says I was sort of somewhat involved in
machine learning community about 25
years ago and then shifted off as
sequencing the human genome started so
you know we're in a special time for
biology where over the last 20 years or
so we've moved from having extremely
sparse information genetic about the
genome sequences of organisms to to
nearly complete information and I didn't
find myself a pointer that I guess there
is one somewhere but in the 90s as you
can see over a period of just five years
there was a three orders of magnitude
you know increase in the in the size of
genomes that were sequenced and you know
that rate of increase of the ability to
obtain DNA sequence didn't stop then it
didn't stop the end of the human genome
project between 2000 and 2010 costs drop
by other five orders of magnitude much
faster than Moore's Law and that's
pretty daunting and it creates petabytes
of data a year now and it's seriously
demanding for computational you know for
the data analysis and interpretation so
you know initially we started by
thinking we'd get one sequence of each
species like the human genome sequence
but of course there isn't one human
there are there are billions of humans
and we all have different genome
sequences and a lot of what is
interesting is what is different between
them and how that relates to the
differences between between the
individuals so we can now study genetic
variation which has been an important
field for biology ER for the last
century but it's always been known in
the dark up until the last ten years or
so we can now do it directly in the
light we can we can sequence and see
what we're
working with so you know as you can see
in terms of costs the first human genome
costs about a billion dollars by 2009 we
sequenced 200 genome is about fifty
thousand dollars ago that that dropped
last year or by this year we're into the
tens of thousands of human genome
sequences cost of below five thousand
dollars it depends you know exactly what
deal you get from who's whoever and I'm
pretty confident that cost will continue
to drop their new companies new ideas
technological ideas for collecting data
will get human genome sequences below a
thousand dollars and there were millions
of human genomes for clinical usage for
research and for personal interest as
well so you know we need to somehow make
sense of all this information and in
doing that it's probably you know it's
not a surprise to you I think I mean I I
think we need to make use of the
structure in the data both for
representing the information effectively
and for inference on it and you know
it's not quite as scary as it might seem
because the more people you sequence the
more they look like each other we're all
related we're all related by evolution
and effectively at each place in the
genome is a tree that relates us and so
the common ancestor some time ago about
half a million or million years ago on
average it's quite high variance and
we're all descended all different
versions of the genome the several
thousand or the thousand or so there in
this audience derived from that and the
difference is that we observed the
caused by mutations that happen since
that common ancestor and so they're
shared in typically by some set of some
subset of the people and that sharing
means that there's a little frequency
the frequency that the variance has in
the population so most you can think of
genetic variation is composed of binary
events which were mutation events and
you those people who have the ancestral
form and those who have the derived form
okay so we get different events here at
different times which leads to different
different allele frequencies and you
know for those who are doing this of
standard Association genetic studies for
disease some mutations might cause
disease will have some other property
that is visible as its visible effect
and you can just count up you can take
people marked in D here who have the
disease and those who don't and you can
ask is there a statistical association
between those with disease and the
genetic variant and there's a huge power
to that because although normally
correlations or associations are sort of
give me a bit suspect you learn about
causality with genetics you essentially
know about causality you know that it's
not true that having the disease changes
your genetic state well you know cause
this mutation to happen 100,000 years
ago so you're pretty confident that
there's a causal direction from the
genotype 2 to the phenotype to the to
the disease so that would make
everything very easy we know how to do
lots of things on trees trees a nice
things that lots of algorithms on trees
unfortunately life isn't so simple the
tree here isn't just one tree the tree
relating as changes as you move along
the genome so at each position or locus
there's a tree but as you move along
there could be ancestral recombinations
which I'll describe what that is in a
moment which will change the tree and so
you get this complicated graph and one
way to think about it is you know we
think about a tree go back to a common
ancestor in time at one place but you
also have an inverse tree your pedigree
of all your your parents your ancestors
and those go those increase game
backwards in time so they and in some
sense the real structure is a is a sits
in between those two so let's consider a
simple case of four individuals and
these are three variable positions and
hear that a is a c and b c and d RT here
we've got a beard GS and CM das and here
a and C AAS and B and C and D are seized
so you know we could we could represent
them in binary we could show it a tree
that explains the first position does a
mutation that chain where a is the
ancestral 0 b c and d are all
one on that same tree we could put the
second mutation C and D carry the
mutation so we we place it there let
that point in the tree but there's no
way we can place a third mutation on
that tree to give rise the pattern that
we see for the third the third side and
in fact there's no tree on which we can
place three mutations and that's very
straight forward to show so such
patterns do exist though in real data
and what's going on is that the tree the
tree is different and so you know what
might have happened here is C is no
longer closely related to D but it's
actually most closely related to a so
then everything works out fine you can
place the mutation here and we
understand what's going on so we can and
what's happened is that at this point
somewhere between this position the left
the left the left side so that where
these two mutations are on the right
side the person C has inherits their DNA
from a different person in the past and
we know that you inherit your DNA from
your grandparents and you get a mixture
of your grandmother's and your
grandfather's DNA forming each of your
chromosomes so you get these events
which splice which cut at some point
switch over from grandmother to
grandfather and that's exactly what
we're seeing here and we can represent
that in a graph that looks has
bifurcations growth both going backwards
in time and forwards in time and this
bifurcation is due to a recombination
its are bringing together a pieces from
two different individuals in the past
into a modern thesis single piece of DNA
and in general you get this big
structure interesting structure accorded
it so that's the real structure there is
there is such a structure that relates
all the DNA of everybody in the room
unfortunately influence on these things
is hard so that's what we want that's
the right thing in some sense there's a
natural generative model for them it's
the coalescent with recombination King
when it's used the coalescent for those
who know about this in the
90 or so in the 70s 80s tavern griffiths
introduced looked at recombination and
there's been a lot of work and so on
they're good simulators there's also an
extremely good Markov approximation
which essentially runs left to right it
says you have a tree at one position and
you can sort of do operations where you
break a branch on the tree in the glue
it back somewhere else we can think
since it's a tree we can use a
horticultural analogy and talk about
pruning it and grafting it back which is
what you can do with real trees so so
you can have this Markov process where
the hidden state is a tree and you
observe mutations that you observe
sequences on the leaves unfortunately
they're infinitely many a RGS ancestral
recombination grass for any particular
data set if you have a set of data and
sampling histories is is an unsolved
problem actually there's mcmc strategies
they're extremely costly it's a very
hard space to search and also
conditional sampling is hard if you have
a set of data if you want to sample
choose another another individual or
place them into that structure that's a
non-trivial operation so that's the sort
of framework we're going to want to come
back to this in the future if that's
really what we're going to want to play
with there's one more thing that's
important which is it seeking the high
demands on accuracy so this is a section
of the genome it's about a thousand
bases long you can't see very well
there's a few letters doesn't matter
really these are actually craig Venter
and Jim Watson six sequences this is an
old slide there's one place where they
differ on this slide and that's typical
this one in a thousand places and they
both differ because they share a there's
a very much shared so if we want to have
it one percent sort of error rate and we
have a one in a thousand places overall
which different we need a 10 to the
minus five for base pair sequencing
error rate but if you want to get a
thousand people we need a 10 to the
minus eight for base pair sequence in
error rate raw sequencing accuracy is at
best x minus four so you know we're
gonna have to use statistical methods
and to integrate raw data whatever we do
to get it all
so let's just keep that in mind so I'm
going to tell you a little bit about the
thousand genomes project which is this
major international consortium that as
sequencing costs dropped look led the
way essentially in sequencing multiple
people to characterize genetic variation
and it to motivate when I neyo on to
later which is a new way to try and
carry out inference on a RGS in an
efficient fashion that hopefully might
might stimulate some ideas so the
thousand genomes consortium was put
together in 2007 with a number of
centers from across the world the Broad
Institute in Boston Baylor College of
Medicine wash you in in San Luis Sanger
Institute in UK and the BG I in China
there's the main centers but also some
of the manufacturers of instruments and
funders of course and we published pilot
data on 190 samples in 2010 we sequenced
a thousand samples in 2012 and
essentially complete now with completion
next year and the aim was defined
essentially all the common variation per
sample enough individuals from across
the world to find things that were
shared by at least one percent of people
try and be a bit more accurate and
deeper in the coding sequences to also
discover other types of genetic
variation as well as single point
mutations you can delete or rearrange
DNA and to identify a reference set of
human human sequences from the
individuals we sequence and it's been a
driver for the day to a lot of the
methodology that's used in sequencing
experiments now came out of this project
so we started with four populations who
had been studied previously in the
hapmap project see you are actually from
utah but they sort of northern european
ancestry so they are logically placed in
europe
a population from Africa from Nigeria
abadan and then ones from China and
Japan the phase one day that was
published about a year ago included more
populations from Asia Europe Africa and
populations to the Americas which are
interesting because in particular from
the Americas though this happens all
across the world of course you know the
ancestry isn't isn't a clean ancestry
there's a there's been a bring it
together of peoples from from different
continents and that leads to
complexities in the Arg in the genetic
structure in some sense deep deeper I
guess deeps cycles in the in the graph
from your point of view and you know the
complete project where sequencing is now
complete involves 2,500 people few over
that 2520 I think aiming to get 500 from
each major continental area split into
five groups from each continental area
so five basically 25 sets of 100
individuals more or less and we've got
some whole genome sequence from the mall
and also deep sequence from the coding
region the exome and of course we get a
wealth of data from this when we started
there were about 10 million genetic
variants known the first pilot found 15
million and it increased over 20 million
the total phase one was 38 found 38
million and the final is about 90
million product and this is a small
parlor genome and there's all sorts of
patterns of variation and these long
black lines there's a deletions you can
see that the African populations here in
brown in the middle or a little darker
there's a little more variation there
because there's more genetic variation
within Africa and there's all sorts of
other relationships that we you know
don't have time to go into unfortunately
there's one sort of biological point I
wanted to to discuss a bit I think might
be interesting and might be some ideas
here about improving this approach
before going back to you know how we're
going to handle this as we get more and
more day
so of course we're interested in
functional variants a lot of mutation is
neutral it can mutate any base in the
genome whether it's functional or not
but we're particularly interested in
mutations that might have led to a
change that has a function so here we're
looking at the distribution on the
x-axis is allele frequency so rare
things at the left and common things at
the right and what you see is that there
are more rare alleles that's a
well-known property of the coalescent
and what we expect in population
genetics but purple things are
synonymous mutations these are these are
mutations which are in coding sequence
but they don't actually change the
protein sequence there in the third base
position which is a most of them are a
random a position which which doesn't
actually affect the inner acid residue
that's translated and you can see that
they're actually asked some quite
they're quite there's a reasonable
number of common purple mutations some
of you the ones at the front can see
that if you go up this list the green
ones are nonsynonymous those change
protein sequence the blue ones are
splice variants those will miss create
incorrect splicing basically destroying
the protein function the red ones are
stop codons there's also lead to a stop
hard stopping translation so they also
tend to destroy the protein function and
the gold or yellow ones are things that
have been previously no annotated as
being functional human gene disease
mutations what you see is as you
increase in functionality or damage to
the protein there's more and more
tendency to have to suppress mutations
that are common and to have a relative
enrichment of rare ones now of course
what's going this is is not unexpected
what's going on is that damaging
mutations they may happen but they tend
to get selected against and so by
natural selection it's unlikely that
they'll drift the high frequencies
there's a basically a random walk a sort
of diffusion process happening to the
mutation frequency in it but there's a
pressure downwards for damaging
mutations so this isn't really an
enrichment of rare mutations it's a
suppression of common mutations for come
functional mutations and we can see that
in the bottom of the slide this is all
normalized by this anonymous rate so you
can see the synonymous rate is flat here
the greener there's a bit of a bias for
the nonsynonymous ones and progressively
more biases you go to the protein
loss-of-function mutations and the known
functional human mutations so this this
is a dynamic process though it's not
just that these were suppressed and
that's it there are always new mutations
happening and they're drifting in
frequency so to maintain to maintain
this type of curve here we need to
dynamically have mutations which are
damaging in this region which are being
which are being suppressed so because of
that we can actually estimate how many
on average damaging mutations each
person has so nobody in this room is
perfect you all have damaging mutations
you're carrying some of you I mean
there's enough people here and probably
there'll be some people who know they
have genetic diseases there's a few
percent of people you know where there's
a mutation which is has a very strong
effect your parents have it you know
that you have it there'll be some people
with with general diseases but many
others may be carriers for general use
ease they may have them in a
heterozygous state so it's it's there's
no visible phenotype but you might pass
it on or you have some weaker mutation
that's deleterious but it has a weak
effect so we can estimate that the load
per individual about 4 million genetic
variants separating each in pair of
genomes or probe individuals anyway and
we can ask you know we can estimate how
many deleterious functional mutations
that are per person in each cat in
different categories and that there's
approximately the order of a thousand
and if those a couple of hundred a few
hundred are probably in protein
sequences a few tens are probably lost a
function really killing the protein and
the rest though are probably in the non
in the non-coding sequence and we don't
know so much about that there's also
this interesting studies as an encode
project and so on so we're beginning to
work out where those are and what those
are so that's a kind of burden we all
carrying a burden of deleterious
mutations so how do we how do we
actually identify that's a sort of
generic
thing though you can statistically
estimate the load but you can't say
which ones are which of course we really
want to know which are which that's the
study of medical genetics or functional
genetics so here again we have a sort of
this is a standard plot that some of you
will have seen a little frequency on the
x-axis on a log scale so one percent is
in the middle common on the right-hand
end rare on the left-hand end a fraction
of a percent or below and then effect
size how strong is the mutation so
people started studying mutations that
have a strong effects like
hemochromatosis and so on and you can
find those and with the genome sequence
and tools now we can actually find the
gene and in the 90s you know hundreds of
genes actually a few thousand genes have
been found like that but they're all
very rare it turns out when we got to
start understanding human genetic
variation we could design experiments
that looked at common mutations the
first ones that we found and we can go
and use Association studies like I told
you and we thought we might find lots
more but actually almost everything we
find is very weak effect has no effect
however I just want to sort of say
Association studies have been important
and there's over a million people have
been studied a million people genotyped
23andme as a you know direct-to-consumer
company is genotype 400,000 people so
there's hundreds of thousands of people
and there are thousands of genetic
variants that have been identified as
being involved in disease and whether
whether or not they contribute heavily
to the population health burden they
connect biologically a gene with with a
function and that those are important
studies so to get back to our picture
there's nothing really in the top right
hand side there's still a serious
anywhere because you know that would
have been selected away there's not much
in the bottom left that we can get it I
mean maybe there is something there but
if it's very very rare and it has almost
no effect you know you're not going to
find enough of it to measure it but it's
a big hole in the middle you might ask
and that's what you know the field has
asked itself and we actually know
there's some things there we know of
genes where we've got to look because we
had candidate ideas and we
noun variants that had intermediate
effects I mean so on so so there's a
question now we can sequence can we
start to look directly for genetic
variants which are sort of 1% fraction
of a percent frequency some people call
them Goldilocks variants not too big and
not too small and really contribute
substantially on this axis so we wasn't
we were involved in such a study in the
UK with 10,000 sequences we sequence
4,000 whole genome 6,000 exomes we've
collected all the data basically it's
there we've got 50 million variants and
so on we started to look for functional
variants it was a little disappointing
that's and that's a message which other
people have been finding we have found
some additional new things there are a
few Goldilocks things and there are a
few things that were missed by the
previous studies because they actually
just weren't well represented or tagged
or acid before but if we go through this
the maths the numbers we have of our or
this this was on an earlier analysis on
twenty five hundred songs we done this
on all 3700 that number is just not
enough we know from the jiwa studies
that you need to start looking at tens
of thousands of people to see the
effects we're looking for so what's an
alternative strategy can we combine the
two datasets we have with getting this
data on thousands of people's genome
sequences the UK 10k is just one of a
set of projects across the world by the
way all with similar results can we
combine the jiwa studies with our
reference sequence kind of and so there
is a way to do that which is to impugn
full sequences into the G what
individuals it's basically missing data
problem you've got you've got some
picture of the genetic structure the
ability of your reference sequences you
want to build some model and you want to
get partial data from the G wos where
and what they do is they look at just a
million positions that are known to be
variable and type I say those so you're
not getting a complete sequence you've
got a partial sequence of these
individuals so here's an example here's
the set of reference sequences and a set
of reference of individuals with Benji
washed and
and we have you know we have hundreds of
thousands of these we have a few
thousand of these and what we do is we
look for matches we we look for in the
substring that's in the genotype to
individual we look for a match into at
the relevant positions to somebody this
this bit here matches that blue line
there so and then once we found a match
in principle we can copy down we can and
effectively what what we're actually
doing is using a hidden Markov model to
do this so there's a as an inference
framework for this and it says that
you're going to try and explain these
new variants here in terms of the mosaic
of pieces of the front from our
reference individuals and we can do that
in a probabilistic fashion we don't
actually know it maybe it matches
several of them and it maybe there's a
longer match to Psalm and a shorter
match to others we'd like longer matches
if we want to we have a process the
hidden state is which these guys you
copying from and you'll now a switch a
transition is switching between those
and those transitions in some sense
correspond to ancestral recombinations
in our RG model so this is a method that
state of the art it's used in these
programs impute and Mac it's quite heavy
it's an effectively approximation to
conditional sampling on the on the
ancestral recombination graph but
there's no tree here you know it's not
actually a very good approximation in
some sense but you know it works and
it's been very important in the field
here's an example of a place we're
looking at a what's called a Manhattan
plot this is the p-value log minus 10
log 10 p value on the axis of and so you
want you want to spike that would be a
very rare a low p value and the black is
real data and the gray is imputed in
we're getting a much stronger signal
from an imputed variant and it's likely
to be the functional variant even though
it wasn't typed and it's been also
important for integrating data across
projects and so on so that's the way
that things are done now what's the yeah
it's all sounds great wonderful problem
solved the trouble is that imputation
accuracy drops as allele frequency drops
just exactly in the region where most
interested around a percent if you take
the thousand genomes data you know it
falls off a cliff
so we can impute all the common stuff
and that's actually useful but it hasn't
solved our problem at least it didn't
haven't done up until recently now we
did notice as we sequence more
individuals that it got better you know
one God a progressive improvement from
say seventy to eighty two percent in
this category by going to more
individuals so the question is can we
can we you know as we could go to much
bigger sets how can we do better if we
do better either by improving our model
or went by improving the numbers we all
know that both things are good and we
probably ought to improve the speed as
well because the the methods are slow so
here's a pilot study that we've been in
the middle of undertaking now there's
the nine studies 13 and a half thousand
samples here's UK 10km thousand genomes
and a set of other projects in Holland
and Sardinia and Finland and so on and
and we've taken sites down 20 point zero
three percent one in 3,000 and if we
look at our imputation performance it
does it pushes us up so now at one
percent we've gone up to seventy eight
percent r squared correlation between
the truth and we can use holdout data
and establish a truth metric so so this
is looking promising and in fact
simulations suggest we should even do
better and up here in this eighteen
ninety percent level that's where we
want to be we really could start you're
not having to sequence look at the
Goldilocks region and imputing when on
the verge of that this coming year I
think and I wanted to say briefly but I
think i'm going to skip through this
rapidly imputation is also used actually
to integrate data because in some sense
it does have a working model of how all
the data you how to share data across
individuals and so what we can do is
instead of if you're a sequence one
person by themselves independent of all
other knowledge you have the sequence it
a very high redundancy so you can
statistically handle the uncertainty in
the all the sampling and and so on but
in order to find low frequency varies
you want that's very inefficient it's a
waste of time in a way to sequence at 30
or 50 x
and it turns out that you can share
across samples using imputation and that
from a population basis you're better
off this is a deep sequence of 50 30 X
on 50 samples and this is 16 X on 100
samples or a dex on 200 same amount of
sequencing you can push up your power to
detect things as you as you spread data
as long as you integrate it carefully
and that's the way we did things for the
thousand genomes project and we did meet
our goal of over ninety-five percent at
one percent frequency and we get a lot
of rare stuff as well so you know all
this more or less works and we can also
as well as actually finding variants we
can improve using imputation the
accuracy of calling the individual
sequences okay so how to speed it up so
hidden Markov model based methods like
computer computationally heavy their
quadratic in principle in the number of
reference sequences and there's some
things that they use to make them linear
but they're kind of heavy linear they
use MCMC sampling they come out of
statistical sort of framework and they
take thousands of CPU days for the sorts
of data set analyses that we do we have
big compute farms we have many thousand
machines and lots of you know it's
painful and they're not going to scale
to millions of genomes we want millions
of genomes it's only coming we have to
do this in 22 years or so so I want to
discuss with you a new data structure
thats related to other data structures
used in computational biology which we
I'm calling the positional
burrows-wheeler transform so let's
imagine you could sort all your
sequences let's say we're interested in
some position of course these sequences
are very long their millions of
positions long but let's the interested
in one position K let's imagine we've
sorted them all at that position of
course once you've sorted them then
finding matches is cheap because set
matches are adjacent in a sort order
it's like looking summed up in a phone
book and what we're showing here
actually is the the length of the match
of the preceding sequence and we've
sorted from this position going left
words and this is the next position
we're looking at
so this is analogous for those who know
about it to the burrows-wheeler
transform on strings on on on but it's
different because here we have a large
number of sequences all of which are
aligned so we have a kind of coordinate
system which is different from a
standard burrows-wheeler transform of a
of a string database okay so one thing
you notice is that there's some
clustering here because we've sorted
them and we know that things share share
sequences that means we look at the next
position there's a tendency these are
these are all sharing material Tennessee
for the next position to be shared as
well so there's this block of ones and
the block of zeros here and actually
this is a small data set when you get a
big data said it's extremely blocky I
forgot to mention so so what about this
sorting sorting seems nice but it's
expensive how do you sort it every
position well you can update your sort
it's kind of obvious really it's
computationally simple thing to do if
you sorted at some position and you want
to ask what's the sort of the next
position you just have to look at the
next character all the things that a 0
go to the top and all the things that go
one gives the bottom when you maintain
the sort order going backwards from that
because you're just adding 1 1 symbol on
so that's a very fast operation it's
done in a single pass down the sequence
so single pass to the entire data set
generates all the sort orders at all
positions so we can use this for
compression commercial is going to
become important later on compression is
always important it's good to have
compact data structures and it's also
indicative of prediction so we use those
the run-length encoding on these columns
we're storing the data basically column
by column in this each one well a
relative to its current sort these
run-length encoding of an extremely
naive sort I'm sure there's many i coded
it myself so we simulate 100,000
sequences with a standard coalescent
simulator there are 300,000 sites so
we've got you know 10 billion data
points or something 37 Giga bases
gigabytes of law output
so cheesy there's lots of ones and zeros
gzip compresses it 35 full to a gig you
know you might think that's quite good
but once you use this run-length
encoding it's a naive run-length
encoding it goes down to 7.7 megabytes
that's a forty eight hundred fold
compression so and actually that naive
yeah if i took the native order and run
length encoded it'll be two gigabytes so
that's worse than worse than gzip so
that's about it so that performance just
gets better the more sequences you get
we can we can subsample a hundred
thousand and go up here and the you know
the compression ratio increases from you
know six fold over gzip here at the
center 130 full this end so you know
that's what we want it's all behaving
nicely and this works with real data we
get a factor of six on the thousand
genomes said of 2,000 samples which is
more or less where we'd expect it to be
based on me on the simulation so what
about matches well maximal matches as we
said let's take a new sequence and oscar
matches arm next matches will be
adjacent in the in the in this in the in
this structure what we want to do is we
have this structure we want to place a
new sequence into this structure it's
going to basically follow along and
it'll map up and down and it'll sit next
to at each place it'll sit next to the
thing which it matches best currently in
the in the process and the up there's
also a simple linear update rule for
that it's the same rule actually as we
did for the standard sorting so we can
we can take two approaches we build an
index we can store all the relevant
information for the update it's quite a
big array it's probably you can be more
efficient but it's it's certainly it's
the size of the original data structure
or we can just match dynamically by
going through the structure of match a
set of things so here's the time to
match a thousand new sequences so if you
take a naive approach that will increase
linearly with the size of the reference
set the reference set here is going from
a thousand to 50,000 it took 50 seconds
on
my laptop to match to the chunk of
sequence from the simulation and it went
up to you know well 10,000 was 500
seconds and it's exactly linear as you
might expect in a naive implementation
so in the indexed implementation that's
constant especially taking a user second
independent of size and that's not
surprising it's it's it's like a hash
matching or a borrower's wheeler
matching and FM index matching it's like
a text matching type of algorithm but
we're doing it on these very long
strings into this data structure and
we're finding all the maximal substrate
matches for a thousand sequences in
about a second it increased at the end
because I started swapping I ran out of
memory for my data structure the batch
of process didn't have any you know just
will scale as far as you want but but
does increase at a slow rate but still
is 12 seconds versus 2500 for the naive
approach so this is a really fast
approach okay so what about inference I
mean this is all some sort of
algorithmic but this isn't in you know
inference machine learning meeting well
prediction is related to compression as
I said so let's think about this next
site position here let's call that Y
that you know with why I being the earth
element and we're gonna have di being
how much you share to the previous
string in the sort order and we can
consider a generative model for y given
D so that's quite a simple thing we've
just gotta you know one dimensional
vector of ones and zeros and we've got
some other information so a very simple
generative model which actually with
what they want the only one we've been
playing with so far is is a Markov model
we say the probability of Y I plus 1 is
conditional y 0 plus 1 is just
conditional on the previous element and
DIY AI and bi so so actually it's kind
of interesting to think about this you
could you can use this to generate
entire sets of sequences you could at
some points you can generate why and
then you after you've done that you can
update your data structure and then you
can
no in other ways so you'll generate a
fan your sequences this is entirely
different from a coalescent generator or
from a hidden Markov model generator
which would be some sort of random field
process for for iterating which our
people have been extremely slow to to
resample okay so you can we do anything
with this so it turns out that we
empirically sort of started to look at
what the probability of a switch is
we're good essentially what matters is
switching from ones to zeros so P that
why i plus 1 isn't equal to Y I given D
appears to be a pretty cleanly
exponential in indy as d gets large that
means that this gets smaller which is
good as you get longer matches then the
chance of switching drops it the chance
extending the match increases and
actually that's expected given D this is
if that were you'd expect because this
is sort of the derivative derivative of
the length distribution in some sense
it's the chance of extending a length so
and that would correspond to an extreme
value distribution on which is what
you'd expect for best matches so this
all fits so you know so we've
implemented an extremely simple
imputation process another point is you
can you can think of going from left and
right and combining data because this is
sort of bidirectional this structure so
you can impute from both directions and
I've run it on this data on the real
data set on thirteen thousand samples
there's the results that Jonathan got
with impute which is this heavily
invested software that took days to
phase and run on that data set this to
25 minutes on this laptop it's slightly
worse to my annoyance at the moment but
I think it's definitely in the same
space it's definitely better than the
original one it's a little bit it kind
of comes down down here just underneath
the yellow line and this you know
there's 26,000 haplotypes and half a
million sites involved in that
okay so now what could we do better well
I think there's quite a lot of tricks
and elements that are possible but one
interesting thing is to look at the
other models on this on this vector Y
and it very interesting natural one is
think about trees on it but because that
would correspond to a tree there's
nothing you know we haven't we sort of
got away from a tree we know that there
should be one can we have a tree to the
prior structure and the DS are
informative they might tell you when d
is largely might tell you that things
are closely related and when d is small
it might be that it's a deep deep split
so this is an open thing it's just an
idea I don't know about fast ways to
explore three priors I mean there's a
lot of work on that the data aren't
consistent with the tree in the form at
least I've drawn it you know so you have
to do something more sophisticated but
this is the right audience to think
about those things so I've got a few
minutes left five so I want to go back
relate this to some other work on ideas
getting back to raw data so far I've
been imagining i actually have in a
sequence or partial sequence for these
individuals sitting the reference i'm
imagining I have these lined up perfect
sequences but I to the raw data we have
a shotgun sequencing reads and these are
short what you with the way that we
sequence a genome is we don't start at
one end and go along we get 100 base
reads and we have to kind of work out
how they all fit together and onto the
reference the random fragments they can
be enriched if you want but and what we
do is we map them to a reference genome
we say this we think this read looks
like this piece of the reference genome
but their problems with that there can
be problems where the genome is
repetitive so the same sequence is found
in several places we're not quite sure
which one the weed goes to and they can
be problems because the read actually
doesn't come from the genome it comes
from a different version and that's what
we care about so we're actually
interested in the things that don't
match the reference so matching to the
reference is a slightly strange thing to
do and then we bet detect variants based
on multiple alignment of reeds in
there's methods for that so a lot of
modern methods they use the
burrows-wheeler transform and I've got a
couple of slides about this but I think
in the time I'm not going to go through
them you basically it's based on the
suffix array and you can update with a
formula that looks actually extremely
like the one not coincidentally that we
used for updating our sword order it's
the same it's essentially the same
formula and it's a constant time update
and you can update the match matching
will read and so bwa which is the I
think the most widely used program
developed by Hanley and when he was with
me and bowtie other programs will use
this but you get errors due to mapping
problems these errors picked up because
we found things present in Britain's but
not in Cu it should be the same and they
all in fact turn out to be around the
centromeres and Latino Mears ends the
hard bits of the genome that are
repetitive you get errors because of
alignment this is a region where there
would be lots of variants call these are
reads aligned to the reference but
actually what's happening is that
there's a duplication and this whole
said although all that so chatter of
apparent genetic variation is due to a
single clean insertion but the alignment
was problematical so a better way to
work is not really to use the reference
it just creates trouble is is to think
of your read set as representing your
individual you'd like to work with the
read set as a representation and think
about assembly you know work from in a
reference free fashion and there are
ways to do that you can start from some
read which is unique in a sample and you
can kind of build out from it and find
the alternate version that's in the
sample and here we've picked out a
variant that explains this bubble in the
assembly and the nice thing about that
is this is to yeast strains you get a
single base change looks like a sort of
divergence in the assembly like this but
also a big insertion which would be very
hard to map so you couldn't mapper read
on to that this is nothing there to map
to looks exactly the same in the
assembly version assembly version is
neutral to nature the variation and it's
attractive and if we do this it does
find things and cleanup problems that we
know that we have
in assembly so it turns out that some of
the state-of-the-art assembly methods
also use this burrows-wheeler
construction not apply to the reference
sequence not to map too but they take
all the raw data and they build build a
essentially an alignment abra a
borrower's wheeler transform of the reed
set and thats related closely to an
assembly graph and you can compute from
it in order n time in a single pass
effectively or the non transitive
overlap structure of all the reads so
you can get a minimal overlap structure
which gives you identifies all that all
the components of the assembly and you
can also derive all the components of
standard assembly graphs to growing
grass for length K so there's a paper by
Jared Simpson is now in Toronto about
that there's some open questions in that
field about whether the graph is the
same but the question which I kind of
think is important and interesting is
you know what are we going to do is
looking forwards as we get these large
amounts of genetic data we're going to
want to have a more sophisticated
reference to map our reads too we're
sort of going to want and can we merge
the the positional burrows-wheeler the
PB WT ideas I introduced earlier which
are about shared genetic structure of a
long distances so that was taking
advantage of shared of recent common
ancestry can we can we sort of combine
that with working risks with the primary
data rather than getting ourselves
confused by mapping to the wrong
reference so that's a challenge for this
field it's a very big data problem I
think it's a will you know ultimately a
clean way for us to work and it also
would mean that all the experimental
data sets I mean Terry mentioned RNA
seek data sets you would you would
interpret them in this context and and
my expectation is you need rather little
data per individual to get a very good
fit into the model and start making
inference about individuals so become
extremely efficient as well as we build
these structures so you'd have large
distributed data sets
the more people are more information the
less information you need per person you
know the better showing but we have to
be smart about it has to be fast and we
have to have algorithms scale and it
seems you know this is the sort of
genome-wide version of what I imagined
goes on with a page type search for
google and all these other web searching
systems and I think it's going to become
a big thing that it's a billion it's
already a multi-billion dollar
enterprise we're going to it's going to
be a very large-scale effort and I think
to get proper the moment I would say
there isn't real proper computational
engagement there's smart statistical
engagement and people you know managed
to put together what's needed to get at
things but there's a lot of scope for
using some of the technology that this
room you know has available to it and
the thought processes so is anyone
interested to help that's a question
collaborators postdocs your independent
or in collaboration would be fine of
course you know there are lots of other
big data machine learning problems in
genomics but I think this is a very core
thing you know the basic nature of the
genetic structure of mankind is is a
core thing that we should work on how to
represent and work with well so I'm
going to stop there these are people in
my group Jared and hang of left are
these people have done a lot of work on
these large datasets there's big
consortium volved this is 1000 genomes
was a long time we have more people than
the genomes but it's no longer true and
this is the UK technically groups and
thanks very much
alright thanks Richard for a wonderful
talk they're always time for some
questions Terry you mind microphone so
this is really exciting it's a tribute
to exponential growth of computing power
but let's ask what will happen as n goes
to infinity here in other words suppose
we achieve you know this compilation of
all human genomes hmm what can we use
that for and what do you think will
emerge from that era which is probably
not that far off well i have i have
another torque ashley so people think of
of genetic variance is happening kind of
hero there by random but you know with
seven billion people we actually know
that there's about a hundred copies of
every possible change at every base but
in the world as long as they weren't
lethal and you might say well long way
off sequencing everybody but actually if
you sequence a million people in a smart
where you can get towards saturation so
one thing we'll do is we'll there will
be people there are people with variants
in every in every gene and so from a
biology point of view we can exploit
that we can try and help you know find
out function of every base in the genome
that's we can from a clinical point of
view people who have severe effect
mutations we really want to match those
this happens already we want to find the
other people in the world who carry that
who put them in touch they can learn
from each other things that have worked
well eat therapies that have actually
worked we can transfer and even if we
they haven't prognosis is an important
thing so it's extremely important
diagnostically to identify functional
variants and to match and so having
structures that allow us to match
official effect efficiently it is very
important make sure
well positive selection has a you know
that affects the structure of the graph
as well and you can look for signs of it
and there is there examples of positive
selection in humans and yeah okay so
where are you straight in front of you
okay so using the current state of our
imputation algorithms could you estimate
roughly how many full sequences will
need in order to be able to do a good
job of entertaining everyone in the
world well it's all you know these
trade-offs as you get the best the best
data point on that is in it depends on
population structure in Iceland the
population of 300,000 they sequence
6,000 of them they've got genetic data
on 120 thousand or so and they can
impute those 120,000 extremely well but
because you can basically have a second
cousin who has been sequenced or third
cousin or so it depends you know this
gesture is a sliding scale essentially
we you can I I'm not going to in in
Europe and in America is the worst place
in the world sadly you have the most
kind of the biggest genetic pool that's
mixing most readily that you will get
the you know the order of a million
people is very strong for for anything
that's not recent
you have shown very impressively these
compression of these genomes did you
think about the best possible impression
so what's the minimum so like a lower
bound what you need in order to compress
these genomes so you have generate that
I don't know that's an open question 10
under if you take the standard model
lessons recombination I don't really
understand how that model the
coalescence recombination which is a
distribution now that correlates to my Y
distribution i I don't I don't have a
good model there's a very yeah that's
interesting okay thanks
alright the next talk is called b.i.g
and qu IC sparse inverse co variance
estimation for a million variables the
authors are excited Dylan profit Ravi
Kumar and pull drunk and she will be
giving the talk okay
hi everybody my name is Charles here and
today I'm going to present our new
algorithm big and quick which can solve
the estimator inverse covariance matrix
for one medium random variables and this
is a joint work with Matthias energy
fatigue and Russell so there are many
applications for the inverse coverings
estimation problem our first given.if
example in the FN iron print analysis so
in this example were given some FN I
images and are going to reveal the
functional connections between different
pixels in the brain image so we output a
graph like this and we can use the
inverse as the measuring technique to to
to this so first we consider in the
input each image each pixel is a random
variable so if we have P pixels in the
input image then we'll have pivotal
variables and we can estimate the
inverse coverage metrics which is P by P
sparse matrix and this matrix
corresponds to the adjacency matrix of
the output graphical model so there are
many other applications for inverse
currents estimation including gene
network discovery and many other
applications so now we formally define
our problem assuming we have P random
variables and those random variables
follow multivariate Gaussian
distribution so we observe n samples
from this distribution so we observed
why want one from this P dimensional
distribution and our goal is to recover
this distribution using a samples so we
are particularly interested in the
inverse covariance matrix because the
following reason so so for example if
the underlying graph is the chain graph
then the inverse coverage metrics will
be a tridiagonal matrix like this so we
can easily see that the nonzero elements
in this inverse current matrix
corresponds to the H in the underlying
graphical model
so we want to recover the inverse
covariance matrix using the given
samples and we are considering a
high-dimensional setting for a number of
random variables is much larger than
number of samples and since the nonzero
elements correspond to the edge in a
graphical model so we want to select
inverse covariance matrix which is very
sparse so people usually use this one
requires maximum likelihood estimator to
estimator inverse coverage metrics and
in this estimator we want to solve the
optimization problem like this when we
want to find a P by P matrix X and let
this X will minimize the negative log
likelihood in the same time we want X to
be sparse so we add one penalty into the
objective function so if we can solve
this problem we can estimate in verse
chorus matrix but this problem is very
hard to solve because the value in
research first we have one penalty here
which is non smooth and mobile number of
parameters in this optimization problem
skill kinetically with number of
variables so if so if we have p random
variables then this X matrix will be a P
by P matrix so we have P Square
parameters in this optimization problem
so there are many algorithms proposed to
solve this problem so five years ago
people can only solve problems with
about several hundreds of random
variables but in this two or three
recent 23 years people propose a lot of
new algorithms and now we can solve a
problem with p equal to 1000 or even p
equal to ten thousand however if we want
to even solve a larger problem we face a
memory problem so all the current
solvers required to store P by P matrix
solar need other p square memory so this
is why all the current stores cannot
handle problem with p larger and dirty
because it's we can now start P by P
matrix into memory but in many
application we want to solve this
problem for very large number of random
variables for example in the fMRI TI set
each input image have about two hundred
thousand pixels so so we want to solve a
problem with 200,000 random variables so
motivated by this application we propose
a new algorithm bigger and quick which
can solve a 1 million dimensional
problem in one day using a single
machine with 32 gigabytes memory so our
new equation big and quick is an
extension of the quick algorithm we
propose two years ago so in this talk
I'll first briefly talk about a quick
algorithm I'll show the difficulty to
scale to p equal to 1 million and i'll
show how to handle those difficulties so
the quick algorithm is a proximal newton
method where we first split the smooth
announced the most part in objective
function so we say 3 of X is the smooth
part and H of X is the nonce most part
which is the one penalty in our case so
we each each iteration we follow
contracted approximation for the smooth
part and we define the generalized
Newton direction to be the minimizer of
the converter approximation for smooth
part plus the nuns most part so in this
talk i use xt to denote the current
solution which is the pvp sparse matrix
and t is the newton direction
unfortunately we we cannot stop this sub
problem in the close phone so in a quick
algorithm we propose to use a coordinate
descent update to solve this problem so
in a corner design algorithm which time
we are only update one variable in the
matrix so if we only want to update tij
then we have closed form solution using
southridge holding operator in this
closed form solution the most difficult
turn to compute is list one WI transpose
d WJ and this w is the inverse of the
current solution so even if the current
solution is parsed the inverse of it
will be a P by P dense matrix so in a
quick algorithm we show that if we can
store these double matrix in the memory
the new hand tool update is update very
efficiently and this is the summary of
the quick hug arisen we first select a
subset which is the weakest the most
terrible will be zero and only a few
element will change from 0 to non zero
in this iteration and then we run call
the latest innovation to find a Newton
direction unleased subsets of free
variables LOL do a line search to find
the step size alpha which satisfies the
solid the new solution x plus alpha d
will be positive definite and will
satisfy us sufficient decrease condition
to check those two conditions we need to
do a karate federation on the P by P
matrix so now we analyze the time and
space complexity for this algorithm so
we have two parts in equality a descent
part we have to stall a w matrix so we
need order p square storage but it's
very efficient we only need other MP
computation / sweep m is the number of
nonzero in the current solution in a
line search part we need other piece
square storage MP order p cube
computation so now we want to handle a
problem with p about 1 million and a non
zero is the constant times p so we so we
cannot allow p square storage and we
cannot allow p cube computation so we
press deal with the Peace Corps storage
problem in the corner design update or
in the cornea is an update now we don't
have enough memory to store p by p w
metrics and we assume he only start em
current of w into memory when we want to
update the ith element we want to
compute the
eastern wa chance post dwt and this can
be due efficiently if WI and GG are in
the memory otherwise we have to
recompute wnw t by solving this linear
systems using conjugate gradient method
so this is an example if we can store
for currents of that blue into memory
and and and we have sought a blue 12 w
for in the memory and now queen elements
are presets which we want to do the call
the instant updates if we want to update
a 14 element and Natalie we found that
the wound up before already in the
memory so we can use so we call this
catch heat and we can directly compute
the update without recomputing wnw t so
this is very good and very efficient
however if we want to update a 69
element in the matrix we found taboo
sticks and w9 are not in the memory so
we have to recompute length using
conjugate gradient method we call it the
cache miss and is this very bad it's
very time consuming so ideally we want
to find a update sequence in the
coordinate descent method which can
minimize the number of cache miss
however this is a combinatorial
scheduling problem so is probably NP
hard so we propose the heuristic update
sequence which is updated burbles block
by block so our operation snacker block
coordinate descent algorithm each time
we update variables in roofing a block
so for example if in the ideal case that
the free set can be written as the rock
diagonal matrix then we can first
compute w12 w3 and update the first
block and then compute w42 w6 the update
a second block and so on and so forth so
in this draconian descent we only need
totally p Cullen computations each color
only need to be computed once
but in a real application is usually not
this ideal case so we have some elements
outside a diagonal blocks and our main
finding is that we can the extra current
computations can be characterized by the
notion of boundary nodes so I'm not sure
that a formal definition of boundary
nodes here but intuitively if we have
six nodes 1 2 3 and 4 5 6 and we found 2
is connected to four and five then we
say these three intermediate node other
boundary nodes so if we can so we can
write down the number of coloring
computations to be P plus the total
number of boundary nodes and we can
minimize tour number of boundary nodes
by using a graph clustering algorithm
and use this is very important in
practice because we can significantly
reduce the number of carbon computation
for example in the FN idea said when we
have points to 28 million random
variables if we use a random partition
and run call the block coordinate
descent then we need 1.6 million current
computations but if we do a graph
cluttering to meet match the boundary
nodes and then rank up black hole in the
Edison then we only need a point to 37
million counting computations and this
number is very close to P means we don't
have many additional current computation
to do so we can under a fixed size of
memory you can have similar time
complexity in the corner innocent update
and then in the line search procedure we
want to compute the log determinant of P
by P sparse matrix efficiently so here
we propose an algorithm to approximately
computer log determinant of the large
sparse matrix and our time complexity is
only other MP emmys number of nonzero
and we don't need any additional storage
so i will not show the detail here but
least this approximate computation for
log determinant is very interesting
and maybe apply to many other
applications so we also reduce the time
space complexity for online search and
this is the summary of our algorithm we
do also do a variable selection and we
can struggle partition by clustering we
run a cornea this black coordinate is an
algorithm to find a generous Newton
direction and we have the efficient 9
search procedure and we in the paper we
also have some theoretical analysis
because in this large-scale problem we
we have to compute the hash in using
iterative methods like conjugate
gradient method so we show that even if
we don't solve linear sister exactly we
can by carefully controlling the
researcher in the conjugate gradient
method we can like this we can achieve
the contracted convergence rate in
practice even if we don't exactly
computer Hessian matrix and this is the
scalability results so we increase the
size of pea size of number number of
random variables from 1000 to 1 million
we can see that the quick algorithm will
take more than three days if we have if
the P is equal to 30,000 here if we use
a big and quick algorithm we can solve
one hundred thousand dimensional problem
in 10 hours but if we will use a more
powerful machine with 32 cores then we
can stop a 1 million dimensional problem
in one day using just a single machine
and we also test our algorithm in the
medium size they are set for a p is
equal to 20,000 and we see the pic and
quick algorithm even if one core is the
faster than quick arisen so this gap is
because of mainly because of we have a
more efficient Loki terminal and
computation in a nicer step finally I'll
show the experiments on the FN I brain
analysis problem so in this problem we
have each image have 200,000 box
and we try our algorithms with different
regularization parameters to construct
the inverse covariance matrix with
different sparsity and we discussed with
the neuroscientist Russell who is also a
culture of this paper and we have the
following two findings first we find
that the in the inverse conference
matrix the boxes with the high degree
note the red and yellow notes are
generally found in the gray matter and
people usually think the gray matter is
more important in the brain in our brain
so it makes sense that they have higher
degree in the inverse covariance matrix
so we further to modularity clustering
using our in verse chorus metrics and we
can find some meaningful clusters called
spans to some brain modules for example
in this upper panel clusters this looks
like the same motor sensor network in
the brain analysis and lore and this
cluster looks like a combination of
three meaningful brain modules in neuro
scientist so so this is the first time
we can run the inverse currents as the
measuring for least large mean size f an
idea set so in the future we expect
there are many more interesting results
generated by our algorithm so in summary
we propose a big and quick algorithm
which can suffer 1 million dimensional
problem in one day and our innovation
tribute including the draconian descent
with clustering and efficient
computation for log determinant and
insect hedging conversation with
contrati convergence rate here are some
references for this talk thank you very
much and unhappy to take any question
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>