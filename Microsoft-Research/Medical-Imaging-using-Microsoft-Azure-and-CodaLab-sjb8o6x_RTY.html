<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Medical Imaging using Microsoft Azure and CodaLab | Coder Coacher - Coaching Coders</title><meta content="Medical Imaging using Microsoft Azure and CodaLab - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Medical Imaging using Microsoft Azure and CodaLab</b></h2><h5 class="post__date">2014-07-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sjb8o6x_RTY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to today's webcast medical
imaging and more I'd like to turn
today's event over to your presenter
Simon Simon you now have the floor all
right thank you very much and good
morning my name is Simon Mercer I'm the
director of health and well-being here
at Microsoft Research in Redmond just
outside Seattle so it's morning for me
and I'd like to tell you a little bit
about the different work that we've been
doing in house related to medical
imaging before I get to that let me tell
you a little bit about myself and and
what my role is inside Microsoft
Research I'm part of a group called the
outreach team and the goal of myself and
my colleagues is to demonstrate the
value that Microsoft Research can bring
when it comes to the application of
computer science in to different areas
of academic research medical imaging is
one of them but it's not the only area
that we work in we're quite broad
Microsoft Research itself consists of
about 1,200 people the vast majority of
them are computer science researchers
and they're based in labs around the
world we currently have around about 10
labs now of course as you might imagine
the focus of Microsoft Research is
computer science the types of things
that you might add that would be
applicable to Microsoft products and the
research that occurs in Microsoft
research finds its way into products
quite regularly but the thing about
researchers is that they're very
interested in applying their theoretical
computer science in different areas of
research and so that's really the
genesis of our interest in medical
because medical imaging is one field in
which it's possible to obtain large
quantities of complex data complex image
data etc which is very useful for us to
to build algorithms and to attempt to
analyze in different ways the
applications of doing so applied far
beyond the field of medical imaging and
relate to computer vision and the
analysis of images in general
nonetheless we do have researchers he
who was specifically interested in
medical imaging and I'll be talking a
little bit about their work and the
other work that we've been doing in
support of them over the last 18 months
or so so without further ado I'll move
on to my second slide here and I'll talk
to you a little bit about the context of
what we're doing and and the reasons why
medical imaging we believe that in order
to improve algorithms the larger the
data sets that you can train them on the
better your algorithm will be in fact
this was demonstrated to us a couple of
years ago when we were developing the
Kinect sensor which you'll be familiar
with from Xbox and so forth it can
recognize where you're standing and what
gestures you're performing and you can
use that to drive a game or some other
type of application when we were
developing it we created a large set of
body position images literally we took
people we stood up in front of a camera
we made them strike different poses and
then we used that set of images to train
an algorithm that would recognize body
positions more generally and when we've
done that we found actually we didn't
have a very good algorithm if you just
take a few tens of thousands of images
of people and train an algorithm on that
it turns out that's not really enough
data to get a very good algorithm for
body position recognition and so we
tried a different approach we created an
application which would automatically
generate large amounts of synthetic data
about body position and once we've got
truly millions of images we could train
our algorithm on that synthetic data and
result in the algorithms that underlie
Kinect today and what that told us is
simple the best data is more data and
the more data that you can train an
algorithm using the better the resulting
algorithm at whatever task it is that
you trained it for so broadly speaking
we use a little tagline here which
you'll see on the slide our goal is to
free the data and accelerate the
research the more data you can get it
the more easily you can get that data
the more data that then becomes
annotated and useful for training
algorithms the better the algorithms
whether applied in medical imaging or
elsewhere so the question that we asked
ourselves was what the best of
which to this would be and that we we
broke it into three phases you'll see
these indicated in the yellow bar across
the top how do you get the data
acquisition how you actually having got
the data how you make it useful by
annotating it and that's the annotation
phase and then how you share the data
with your colleagues and how you start
to incentivize incentivize people to
build better algorithms for medical
research and other purposes and so I'll
take those three different areas and
I'll tackle one by one the first one I'm
going to talk about is acquisition now I
don't have a terribly much to say about
that so I'll move on to to the rest of
the presentation quite quickly from here
the problem with data acquisition is
that it has to be well we're dealing
with when we talk about medical images
quite naturally we're dealing with
patient data and that data needs to be
anonymized and it needs to be received
from the hospitals which act as the
collection points of that data having
spoken to patients who have tumors and
and and other things from which medical
images are being collected is a general
rule are extremely keen to participate
in any type of research that would
improve the experiences of patients in a
similar position to them
in short the patient isn't the issue
patients are very happy to share their
data is the general rule hospitals quite
naturally tend to be more concerned with
protecting the the personally
identifiable information of patients at
cetera and as many of the people on this
call will know the way that their the
data can be obtained from hospitals is
by consulting through there an ethical
review board and of course the ethical
review board may make the data available
under certain circumstances perhaps not
publicly but only to a certain set of
individuals and in general it's always
useful to have that data anonymize if
not actually a requirement of the
ethical review board the problem with
all of this is this means that
extracting data from hospitals is a
question of dealing with it on a
hospital by an Hospital basis it doesn't
scale you have to approach the
hospital's you have to approach the
review boards and depending on
the nature of the data and the nature of
the use to which you putting the data
the hospital may apply different levels
of protection on that data this approach
doesn't scale now we still need that
data initially because we need to have
sets of real medical imaging data in
order to determine what we need to do
and in order to train our algorithms but
perhaps there's a longer-term future and
this is why I said I don't have terribly
much to say on this slide because I
don't have much to tell you right now
about that longer-term future but just
as with the Kinect
the best way for us to approach that was
to generate large amounts of synthetic
data you could imagine that if we were
capable of jannat generating synthetic
medical images of sufficient fidelity
that they could be used to train
algorithms which could then be applied
to real people's data then we'd solve
the data acquisition problem if we could
generate large quantities of synthetic
data than what we could easily do is if
you had a rare case that you wanted to
develop an algorithm to recognize
automatically you could generate a
million of those for our cases quite
easily when of course acquiring the same
data would be hard or impossible from
the general patient community so we are
moving in the direction of trying to
generate synthetic data the reason why I
tell you about the direction but I can't
tell you about the results yet is we've
only just started moving in that
direction
in collaboration with the French INRIA
Research Institute where we have some
joint postdocs
located who will be working on this
problem in the coming months and years
so basically our / our approach to
acquisition is solve it on for a few
hospitals in the in the near term so we
can gain data to train our algorithms
but in the longer term we see the future
as being the creation of automatic large
datasets where the ground truth data is
automatically recognized ie the labeling
has been done when the data was created
now that would be ideal but I do mention
labeling so I'll move on to the next
next piece which is annotation it's all
very well to have a large amount of
medical data available and of course
when this comes directly from patients
and has been anonymized then what you
really need to do is you need to then go
in and label that data so that you know
what it's showing
because currently state-of-the-art for
for the wrecking what a mated image
recognition on a computer is not
sufficiently good to automatically
recognize that it's looking say at a CT
scan and to kidney or or whatever it
might be on the screen and so what's
required for the annotations tab is you
need to get some rather expensive highly
trained radiologists put them in a room
show them your test data set and ask
them what it what it said and an
experienced radiologists will then go in
and annotate the data now the problem
with that is it's a slow process it's
extremely manual and of course you're
taking radiologists away from real
clinical work and quite possibly they
they will cost you a large amount of
money to do that anyway so the
annotation of large data sets is
problematic and that brings us on to a
tool called G Oz now as you'll see on
the slide I've got an indication of the
web pages there and the user forums that
you can that you can go to if you'd like
to download a copy of Geoff's play with
it yourself and ask questions on the
user forums of other people in that
position basically EOS is a
semi-automated
image annotation tool that we developed
in order to minimize the workload on
radiologists when they annotate datasets
because we need that ground truth data
and to train our algorithms we've run
some user workshops for Fujio so you'll
see a picture of one right there that we
ran in Cambridge last year because
although we're developing it as part of
Microsoft research we're doing so with
the input of the medical image analysis
community we had we brought in a range
of radiologists we asked them the sort
of features that they that they would
see as useful in this tool and we've
been expanding the tool in accordance
with their requests over the last year
or so it's available freely for download
but please it is only available for
research purposes please don't use it
for clinical or diagnostic purposes it's
not it wasn't built with that in mind so
what I'm going to do now is I'm going to
do
quick demo and I'm going to show you
what Gio's can do we believe we built a
tool which is quite into intuitive to
use and the radiologists who use it to
date have spoken about reducing their
workload by a very large amount by an
order of magnitude actually as what I
was told so let me just switch over to
my monitor and then I can drive the
application and show you it yeah ok you
should see a large grey screen with for
research use only not fusing diagnostic
procedures written in the middle this is
the opening page of the GOC application
and I will now load a medical image and
I'll show you how to annotate it here's
one that I prepared earlier and after a
second it will load a CT volume you
should see on your screen the interface
which is predominantly in black and grey
and you'll see three different
visualizations of the same CT volume a
large one in the in the Santa here and
two other views on the side hopefully
you should be able to see my mouse
moving indicating those views if you'd
like to switch between one view or the
other you simply click and you can move
it swap between the three different
views and through by scrolling you can
actually scroll through the CT volume I
know it updates slowly on this on this
call the scrolling is really rather
smooth ordinarily but I can see your
screens in the appears to be rather
jumpy there now if I'm interested in
this and I'd liked it well I suppose
actually just walk you quickly through a
few of the features in the interface
Geoff's is called Gio's because it uses
a geodesic image segmentation algorithm
that to actually recognize features in
medical images if you'd like to tune
that then you can go to the settings
over here opening that up you see a
range of different parameters that you
can set for the algorithm there I won't
mess with them right now what I can say
is you can save different combinations
of settings as presets and if I go down
to the other corner here you can see a
range of different presets which you
have the
the window display levels optimized for
different tissues I'm going to segment
some soft tissue here so I'm going to
select the liver settings even though
I'm actually going to segment a kidney
for you so that should that pretty
radically changes the view as you can
see that you can get some further
information about the exact signal as
you'll see in a histogram here and some
other features which are adjustable just
above it but in general this will do for
me now what I'm going to do is I'm going
to scroll through here until I see a
kidney come interview I happen to know
that the circular feature here is a
kidney and I'm going to segment that so
here's how I'm going to do it well first
of all I'm going to choose a
segmentation label and I will actually
change the default name from segment
label here to kidney okay so now I've
indicated that I will be segmenting a
kidney here I'll select this label which
I've already done I can change its color
but I'm going to leave it dark blue in
the hope that that displays well for you
now what I need to do is I need to
indicate which part of the this this
image is the kidney that I want to
segment and also I'm going to indicate
which parts are not the kidney that I
want to segment and by providing those
two different sets of information to the
algorithm the algorithm can work out all
of the different bits of the kidney and
that's that that's what I'm going to
show you now if you need to know how to
drive this application yourself what you
need you need to know the different
controls that you have you can simply
hold down the f1 button and the display
is replaced by a little crib sheet of
the different key bindings that you can
use I'm just going to use a couple of
these so I'll take that away I will show
you how to indicate an area and how to
indicate something that is not that area
so by taking this brush I'll simply hold
down the shift on the left mouse key and
I will mark the kidney here I'm just
basically swiping inside the kidney area
now what I'm going to do is I'm going to
swipe outside the kidney area I'm going
to say that's not kidney there well
that's not kidney they're still not
kidney over here okay done that's all I
needed to do now I'm going to hit the f5
button which activates the algorithm and
you'll see that what it's done is
all the bits of the kidney and it hasn't
led this area but it's that segmented
here bleed out into the area surrounding
the kidney but if I scroll up and down
through the CT volume you'll see
although it's got it right it starts to
lose the plot of it as we as we move
through the kidney you see at that point
it hasn't quite got it and got
everything and it gets worse and worse
and kind of Peters out as we get lower
down the segments so what I do is simply
repeat the process when i find a segment
that didn't quite make it here i just
swipe in here again and i'll give it
some more indicators of what no kidney
looks like and I'll hit the f5 button
again and what it'll do is it will do
the segmentation on that slice and all
the other slices that it can find but
again it starts losing the plot a bit of
further down now what I can do actually
is I don't have to scroll through and
find the bits that it doesn't find I can
simply get it to suggest where it feels
a little weak on on kidney segmentation
so I clicked on the suggest button at
the top there as you can see it didn't
quite get certain areas here so I'll
just keep on woops segmenting like this
and indicating areas that aren't kidney
I don't have to keep on doing all of
this in every slice but this will help
me work out the remainder of the kidney
nearly there scroll through to a bit
that it really didn't go down here and
I'll indicate this and then I'll go a
little bit further and I wonder Kate
with fat okay that should be enough and
I pushed the f5 button once more it
works it out well nearly there it looks
like it's it's decided it
this isn't kidney so let me just fix
that part thinks a little bit more we're
nearly there
okay I'll go right up to the to the top
end of the kidney here indicate a little
bit more emphatically though that's
kidney and this is not kidney and the f5
button for what I hope will be the last
time yeah there we go okay
now as you can see I've segmented the
kidney and I
did so in a few strokes quite typically
there are applications out there where a
radiologist has to click around the
entire margins on every CT scan slides -
to do the same work
now if you'll see ok I've managed to
segment all of the pieces there but if I
switch to one of the other views you can
see that also it will show you
segmentation in all three different
visualizations here so you can see that
I covered all of the area having done
the segmentation I can click on
segmentation info over here open up a
panel which has a little report on it
ordinarily well in some versions you
will see a three-dimensional
representation here is showing the
actual volume rendering of the segmented
area but if you're interested in simply
a volumetric measurement you can see
that that's that's present present on
the report here and the histogram here
shows signal intensity throughout the
different slices you can add some notes
you can save and print this report for
your records so that's a quick rundown
through through GRS as I say the purpose
of Geoff's really is to automate or to
remove as much of the labor as possible
from the image segmentation process the
reason why we care about this is part of
our workflow here is that the larger
amount of ground truth labeled data that
we can generate the batterer job we can
use training medical image analysis
algorithms but nonetheless this tool is
useful more broadly than simply
Microsoft Research and so we've made it
available under the non-commercial
license that I spoke about before and
either you can do a web search for
Geoff's GE OS or you can look at the the
web links on the on my slide deck or
simply go to the medical imaging page of
Microsoft Research and it will find
about everything I'm talking about here
is linked from those pages anyway I'm
going to go back to my slides now so if
you'll indulge me for just one second
here we are okay right hopefully you can
see my slides again and I'm back on the
gr slide and I yes I think I said
everything I need to say about that
moving on okay so let's imagine that
you've you've gone on this journey with
me and you've gone down these different
steps of the medical imaging pipeline we
have somewhat solved the issue of
obtaining data from hospitals although
we have to still approach them on a
case-by-case basis one day we hope to
have synthetic data which we'll get
around that issue and get around the
issues of patient confidentiality now
that we've got a larger volume of
medical imaging data available it's just
raw data it still isn't useful for
training algorithms and we need
therefore to actually have some means of
annotating larger volumes of data than
were available before and I've shown you
Geoff's that enables that now this this
slide I'm talking about the third part
of the process and the cloud-based part
and also the largest part of what I'm
going to talk about and that relates to
a platform that we're building at the
moment and it's called coda lab coda lab
is intended well we include it here
under the collaboration area the third
part of what I've been talking about in
the medical imaging pipeline but it's
much more than just collaboration I just
had to use one word to describe this
piece but I found it not possible to
encapsulate all the different aspects of
coda lab in a single word so
collaboration is one piece but there's
more
what is coda lab coat a lab is a
playground coda lab is intended to be a
place where you can actually improve
algorithms easily and interactively and
in collaboration with other people in
your community who would like to see
improved algorithms now I attend a lot
of medical imaging workshops and one of
the major ones that occurs each year is
called Makai mi CCA I now Makai is held
in September each year and in
conjunction with a Mekhi there tend to
be a lot of medical imaging competitions
a competition in this context is is
quite simple if I'm a medical imaging
researcher and
want to figure out whether I've got my
algorithm tuned and optimized in such a
way that makes it perhaps of the best in
the field asked you know image
segmentation or labeling or or
registration or any one of these areas
that medical imaging people care about
then what I might want to do is I might
want to take a common data set I might
want to put it in a common place and I
might then want to invite all of the
other groups out there all of the other
medical imaging researchers working on
it on problems similar to my own I might
want to invite them to try their
algorithm against the same data set that
I'm that I put up there so that we can
actually produce a benchmark a common
set of results showing the relative
strength of performance a lot of
different algorithms on this particular
task in other words a competition now
code Allah supports competitions it also
incidentally supports many more things
and most of them I'm not going to go
into today you're welcome to browse
around the running instance of coda lab
which you'll see is deployed at wWOZ lab
org and you're also welcome to
participate in the open source project
that's associated with coda lab well
I'll tell you briefly a little bit about
what those other areas do but I won't be
demoing them this is all about
worksheets and experiments let's imagine
a world in which we are running a lot of
competitions on the Codel a platform and
as I'll show you in a minute we are
running a lot of competitions or some
competitions now we've got more in the
pipeline and I'd like to invite all of
you on the call today to explore it and
if you have a use for this please come
to code labs org and try it for yourself
and try to set up a competition I'll
show you how in a minute but once you
actually have these competitions running
and you have a lot of algorithms that
people are contributing and validating
essentially through the competition and
saying my algorithm performs this well
in this particular challenge then we
essentially aren't just growing a pool
of shared data sets and a pool of
competitions we're also growing a pool
of user-contributed algorithms now there
are plenty of other examples in the
community far more broadly than just
medical imaging actually about how
people have run competitions one that
was quite famous
those help a little while ago was
actually run by the streaming video
company Netflix and Netflix have a
recommender system so if you if you come
along and and you're a user of Netflix
and you're interested in say you know
horror movies then Netflix after seeing
a few of your selections in the horror
movie genre will start suggesting to you
other horror movies that you might like
to watch and so you know and of course
there are many other things if you're a
a customer on amazon.com you'll see you
know people who like the things that
you're looking at also liked these other
things now of course all of this is
driven under the under the surface by
recommender algorithms and there is of
course a tremendous requirement to
improve these recommender algorithms to
provide users of the services like
Amazon and Netflix with better
experience
so Netflix went out there and they set
up this thing called the Netflix
recommender challenge and they offered a
very substantial amount of money I
believe as a million dollars and to
anyone who could improve the existing
Netflix recommendation algorithm by 10%
now it turns out that it's relatively
easy to come up with a solution that's a
few percent better than than the Netflix
algorithm they using currently but of
course a few percent doesn't cut it you
need to get over ten percent or you
don't win the prize and many people
tried and fundamentally everybody failed
to win the Netflix recommend a challenge
using their different approaches turns
out you can get about 8% but after that
people start coming up empty it's just
difficult to improve beyond about eight
percent but nonetheless somebody did win
the record Netflix recommended
Challenger and the way they won it was
that the two front running algorithms in
combination produced more than 10% of an
improvement in isolation neither did but
in combination they managed to achieve
the goal and what that tells me is that
if you're running a competition and you
have the front-running algorithm in fact
the best approach may not be your
algorithm at all it might be your
algorithm in combination with some of
the other frontrunners and in fact that
is borne out in medical imaging
challenges as it was with the Netflix
recommended challenge now that's a very
long-winded way of me telling you what
the other piece aside from recommended
aside from challenges it is in collab
it's a process of taking algorithms and
stringing them together and creating
your own workflows or taking a data set
of your own that wasn't actually used in
any of the challenges and subject to the
licensing constraints on the individual
algorithms that you'll find in in coda
lab actually taking those algorithms and
producing pipelines so that you can
analyze your data sets with these
algorithms not just the data sets that
they were used for in the competition
and of course with each algorithm that
you're using you'll be able to tell
exactly how good it is because it's been
benchmarked against its peers in a
standard competition which you'll be
able to see and browse the results of so
that's kind of the end-to-end vision for
coda lab if you like but we're not there
yet we have all the medical imaging
stuff for competition set up and I'll be
showing you that in a second but we
don't yet have a fully working piece
that does the experimentation side
you're welcome to go to the tool and
browse and see how far we've got but
it's not in final form it's not even
close to it yet but you'll see it
develop you might well ask yourself you
know isn't this a bit peculiar mean
Microsoft does office in Word and
SharePoint and PowerPoint all of the
other things that we do but you know we
know we never invite people to come and
see early prototypes and see how they
work well in this particular case you
have to bear in mind that I work for
Microsoft Research I'm not in the
product side and Microsoft Research is
essentially a much more open
organization and we do a lot of our
development in the open in this
particular case as you'll have seen from
the slide this is not a project that is
solely Microsoft's responsibilities
Microsoft in fact doesn't quote-unquote
own the project ourselves what we did
was we developed it using an open-source
foundation similar to the Apache
foundation called outer curve and we
transferred ownership down to curve and
we made out a curve well we can make out
a curve erratically have agreed to
release a coda lab as Apache 2 license
to open-source
in short you can go to github you can
look at the coder lab project on github
and you can see every line of code that
we've written you can see our approaches
and because it's written in Python if or
you're an academic you may well be more
familiar with that
many of the more microsoft standard
languages and that's good for us because
we'd like to you to invite or we'd like
to invite you not only to use code
allowed but to contribute to is an open
source project as with any other
competition platform it's not perfectly
suited to any competition or any use you
might have for it hopefully you will be
able to use the standard set of features
but we're actually providing you with
the opportunity to build on the features
that you want and then have those
incorporated into the core code and made
available to everyone else and he wants
to use coder lab for their work so over
time this will become a snowball more
features will be added over time it'll
become applicable to more competitions
and it'll be less work for people to run
a competition of their own because
others who've gone before them have
contributed extra features ok so that's
basically it and what I'm going to do is
that are now going to just take over the
screen again and walk you through what
coder lab does so one second okay you
should be seeing my monitor very nice
Pacific Northwest scene two on being
today and I'm going to go over here and
I'm just going to open up a web browser
and I'm going to show you a few
different pages now actually before I
move on to coda lab I forgot to mention
this when I got to it so I'll mention it
now there is of course a user guide
included with Geass if you download the
Geo's installer it will install a user
guide on your machine as well double
clicking on the user guide opens it in a
web page and you can actually be walked
through each of the steps of running
Geass if you're interested in doing so
you'll also find a number of videos on
the web and how-to guides and so forth
just search for gos and you'll get there
okay so back to coda lab Here I am at
WWE lab org this is a public site so you
can be welcome to go there sign up and
play around even establish your own
competitions if you wish to do so now as
you can see it's divided into two pieces
we've got worksheets and competitions
the worksheets refers to all that
experimentation stuff I mentioned
earlier the things that we're not really
fully developed yet but you're welcome
to to collaborate with us on its
development I won't bother going into
that piece what I'll do instead is I'll
go into the competitions piece which you
can access either by clicking on the
competitions label that you see I'm
pointing out now or going up to the menu
bar at the top and looking at
competitions there I'll click on this
label and what it does is it takes me to
a list of running competitions as I say
Kota lamb is already underway it's
already a project that's being that's in
use by a number of groups not just
medical imaging people by the way and
you can browse a set of competitions for
medical imaging and other things here
what I'm going to do is I'm going to
take a look at brats 2012 this is the
multimodal brain tumor segmentation
challenge but that's been run sponsored
by the National Cancer Institute for at
least the last three years and they'll
be running it again in 2014
very shortly and you'll be able to come
back here and see it once we've got it
set up which probably will be by July so
I'll click on the multimodal
brain tumor segmentation challenge from
2012 and I'll just walk you through the
different pieces of the competition here
I'm not signed in right now
so I can't get into this competition and
I'll talk a little bit more about that
in a minute but I can show you what the
public can see ok so it takes me first
to this page where I'm invited to learn
the details of the competition I will
indicate a couple of features to you
here you'll see this grey bar across the
top showing the different phases of the
competition a competition can have any
number of faces in this particular case
there was an early training phase in
which data from a previous year's
competition was was released to the
participants just really so they could
get used to training algorithms on a set
of data the second phase which began in
August on August the first in 2012
released a new set of of data which what
they did is they released both of the
actual medical images and the ground
truth segmentations of that data so
people could train their algorithm and
actually check his accuracy versus the
ground truth which they could see then
when the challenge ran which is the
third phase here then a second set of
data was released and that said were the
medical images themselves anonymized of
course but
ground truth data was concealed and then
of course people ran their algorithms
against that against those medical
images and attempted to predict the
location of different features and then
after that the there was an automated
evaluation process which occurred which
figured out how close each one of the
participants was to the ground truth
information and ranked them and of
course that those were the results of
the challenge but I'm getting ahead of
myself
this is just the the different the
different phases of the challenge and
we've you ran a challenge yourself you
could customize and have any number of
phases in any order and with any
duration that you chose to do including
open-ended phases a face doesn't have to
add okay but I'm just learning the
details at the moment so I want to
figure out if I want to participate in
this particular challenge so by going to
learn the details tab I see three
different sections down here overview
evaluation in terms and conditions again
this is customizable if you run a
competition of your own you can add
additional phases and their different
additional pages if you like but this is
just to enable me to figure out if I
want to compete so I can read this
boilerplate here and I can figure out
whether it's what I'm interested in okay
now the next thing I might want to know
is okay this is my area let's see if I
can if I'm interested in the evaluation
criteria now what this does is this goes
into some technical depth about the
different types of computationally
mediated evaluation that your results
will be subjected to and there are a set
of standard measurements and you'll see
some one listed here sensitively
specificity house-door dyes jacquard etc
and don't worry if those particular
terms don't mean anything to you they
are technical terms of specific metrics
that determine the accuracy of someone's
prediction versus the ground truth and
so this is just really announcing that
those are the different criteria that
will be used to judge your entry should
you choose to participate now if you are
going to choose to participate you'll
need to know the terms and conditions
under which the data is released and
this this is listed here and you see
different types of requirements that
have been set by the competition owner
saying you know the data I have here
this is often the case with met
data has been released under certain
conditions and you must agree to those
conditions before you're allowed to see
the data and that that's essentially
entirely definable by the owner of the
competition okay let's say I've read
these terms and conditions and now I'm
comfortable I want to participate this
is my area I'm interested in the ways in
which this challenge would be evaluated
and I am compliant with the terms and
conditions so I'll go to participate to
get started I need to be logged in okay
fine
well let's log in and then what I'll do
actually is I'll go to the main page
here and I'll just show you how you can
sign up obviously here but if you
already have a login as I do I'll sign
in using the the menu bar here now I'll
just quickly enter my user name and
password and I will sign in okay take a
second to sign me in okay so this is
what you see when you sign in you're
taken to a space on coda lab called my
code I'm now my coda lab at the moment
only really deals with competitions and
what it does is because you could be a
participant in certain competitions or
you can be an owner of a competition and
actually set it up configure it and run
it yourself what we've done is we've
created this little space so you can see
the competitions you chose to join and
the competitions that you're running
we're having to go through the whole big
public list now I'm not running any
competitions right now I could create
one and I'll show you how in a minute
but I'm in these two competitions oh
look I'm already in the Mekhi challenge
so I can click on here and I can go to
that makai challenge and I can see a lot
more about that challenge now if I go to
the participate app I've been accepted
for participation so I can either go go
to the data which in this particular
case lives in the virtual skeleton
database in Switzerland clicking on that
link would take me to the VSD and enable
me to access that data incidentally you
don't have to go to an external data
source you can store the data locally on
Azure either in your own account and
link to link it to Cola and that's
useful if you have large amounts of data
or it's only been released to you or you
you're concerned about adding extra
layers of security there all of those
are possibilities also if you have truly
huge amounts of data of course data
storage costs on area
and so those costs would be billable
separately from the main code of
incidence and typically they can be
picked up by the competition organizer
okay so I can get the data in this
location in in this particular case it
just links out to somewhere else and
once I've analyzed that data I can
submit my results so I'll go here okay
now actually I've been I've been
participating in this challenge for a
little while now so I've actually
submitted several sets of results rather
dull I'm afraid because I've called them
all test submission Bernama lists you
can cite say see that I've made three
different submissions to this to this
challenge I've made three different
attempts essentially to win the
challenge let me take a look at one of
these okay so I can click on the little
plus sign here now our I should say just
before I get into this if I had a fifth
set that I wanted to submit I simply
click the button here
it would pop up an upload dialog and I
could upload my results of the cloud and
have them automatically evaluated I'm
not going to do that now because the
evaluation script takes a couple of
minutes to run and I'd rather just just
keep showing you rather than watch what
your an application run for a while so
what I'll do is I'll show you the
results of that run I'll click on a plus
here okay having clicked on the plus
sign next to that particular line you'll
see I've opened it up and it shows me a
range of different things about that
submission okay so I'm I can download
the submission so I can have it back so
I can see the results that I uploaded
earlier if I wanted to do that now I
uploaded this one actually I did in
January so I could actually retrieve my
results from out here now and take
another look at them if I don't have
them around I can also look at the
standard output in other words I can see
what happened when I ran my results and
it actually records a little real-time
log here of the results in how they were
process it's not very interesting really
the the point of this is it helps
troubleshoot if anything went wrong on
the cloud while you were running it it
unbundled the the data that you provided
it
compares it with the with the ground
truth data it invokes a custom program
for evaluation which I'll talk about
later and then it generates the results
and spits them out in a comma-separated
format in this particular case you know
in other cases it spits them in
different ways but that just happens to
be what this challenge needed okay so
let me just go back here to to Cola
okay now let's say that I'm interested
okay so I know that my my submission ran
now I want to see the results of my
submission well I can take a look at
just about everything there was that
happened on the cloud with my results I
have just popped up a window here where
I can look at the the fine detail if you
like now I'll go in here and I'll look
at the scores that are generated on the
patient data set and if I double click
on it here it'll start up Excel because
it's a CSV format file but of course
being CSV it's also available in many
other formats and this is the the
details of my competition entry I won't
go into all of these because obviously
there are other complex really I'm
showing you this to say that we're not
concealing anything under the hood you
can take a look at the code that
generated these results you can take a
look at these results themselves which
are the raw data as a result of running
they're running that evaluation
algorithm and you can also see the
summarized data which is your
competition entry where it's how well
you actually did because this
competition actually as a summarization
process and reduces this much larger
data set into your only your competition
results and I'll show you those right
now okay but you can get behind the
scenes to see absolutely everything that
you might be interested in about how
your competition was run okay so I'll
show you what happened to these results
I could submit them to the leaderboard
here in which case a check mark would
would appear in this column here now as
you can see I already submitted the
fourth set here a check mark appears
here so let's just look at those so I
can click on the confident well actually
I go to the results tab here I should
say and what that will do is it shows
those results in context as you can see
these were the
your participants in the in the brats
2012 challenge and as you can see they
did really rather well and all of the
different metrics that we that we scored
here and then down here paroled s Jameis
and me I was the worst of the bunch with
my predictions but as you can see that
for the different areas of the tumor the
complete tumor the core area the
enhancing region the dice scores are
presented here and ranked for the
individual participants I can click on
different columns of sort in different
orders or I can download as a CSV again
which opens up in Excel if you wanted to
you can download all of this data and
play with it yourself so we're trying
not to be restrictive just because
you're using this platform doesn't mean
you get tied to this platform okay and
that's really how you go about a
competition at least how have you go
about participating in a competition but
there are a couple of others other
things I should mention in terms of how
you set up a competition if you go to my
coder lab as I mentioned earlier you can
go to competitions I'm running and you
can create a competition yourself if you
do so it'll put you into an HTML based
competition editor and walk you through
the different process the different
processes required to define the phases
of your competition to define devise the
I'm sorry - to determine the leaderboard
for each phase of the competition which
are optional where you can have
leaderboards tell how you're doing at
each individual phase and define the
terms and conditions etc but if you
don't want to go through the HTML editor
we've made it easy for you there's other
ways of doing it so I'll just minimise
coda lab there for a second and I'll
show you something else here this and
let me see if I can find the other piece
of what I wanted to show you I seem to
have closed it down I can't find it
quickly so I won't waste time on that
essentially what I'm saying here is that
you can you can I see you know I think
it's the HTML editor to find your
competition which is fine or if you'd
rather not go through them you know more
about how to use this system you can use
this format instead this is yeah more
which tells me yet another
markup language it's not a creation of
Microsoft it's quite common in the
academic community it's essentially a
dialect of XML which enables you to
specify in this particular case a set of
commands which will define your
competition fully now this yeah more
file as you can see this actually
relates to a machine learning challenge
that that's being set up for use of
makai the medical imaging competition or
the at medical imaging meeting that i
mentioned earlier and in this particular
cases of machine learning challenge and
what it does is it defines a single
phase and it defines all of the
different columns that that should go
onto the leaderboard the order that they
should take whether any of them should
be grouped together etc in short this
one simple file enables you to set up an
entire competition but there are certain
exceptions to that or rather simple rule
you still have to define exactly how
your competition is evaluated if you're
using standard medical imaging
terminology you're attempting to do
segmentation which is a very standard
medical imaging activity which you'll
find the vast majority of competitions
run amock I have some segmentation
element associated with them then you
can also go to our site and download an
evaluation script which for which we're
very grateful to to our collaborators in
the technical university of vienna and a
Hanbury and his feelers ease in
particular and they created a large and
complex medical imaging evaluation
script which means that all of the
standard metrics are already encoded all
you have to do is use their script if
however you have a range of requirements
for evaluation which are unique to your
competition then inevitably as the
competition only you'll have to write
that script we can help you to do so and
there's ample documentation on the site
once you've written that script you
simply hook it in under the the line
scoring program in this yanil file you
upload the yama and you upload the
scoring program and the data and so
forth and it'll automatically make a
competition for you looking just like
the competition we walk through a few
moments ago and of course we do have
user support people here who will help
you do it if you get stuck now going
back now to the website here we are so
that would happen if you wanted to do an
upload or use the HTML editor to create
a competition and that's pretty much it
for for the general part of collab I've
shown you all the way through
competitions and I've shown you actually
if you go to the site you'll see there's
a range of medical competitions already
in progress and there's a pipeline of
others that will be coming online so
watch this space for further ones in the
future the last part I want to talk
about here with relation to coder lab is
how you might contribute features so if
your program are out there if you're
thinking of running competition yourself
but what you've seen would almost do but
not quite fit what what what you need to
do and you'd have to add some some
features to make it usable you could do
that if you go down at the bottom here
you'll see a github is labeled here a
github is an open source repository for
those who don't know it if I click on
there it takes us to our github page
I'll just maximize this so you can see
it more clearly this is our page on the
github site and I can go to our project
by viewing the project on github up here
and now you can see more details about
collab how you get started
we've got coda lab wiki here so you can
look at your frequently asked questions
or answers to your frequently asked
questions you can also see how you get
started the fact that we support UNIX
based competitions actually the Linux
based competitions as well as for
Windows and the Mac so if you're not a
strong Windows user and you're you find
your user base prefers Linux that's not
a problem that supports Linux
competitions alongside Windows based
ones and there's tons in terms of
information here about a lot of features
that I haven't had time to talk about
today relating for example to to the way
you might want to parallelize your
workflow when you're on the cloud to get
evaluation done more quickly but there's
a range of different things and you'll
find them in here if you're interested
there's much much more documentation
I've only scratched the surface here ok
now I'm going to go back now to my slide
deck
just one second while I do that okay so
you should see my co2 lab slide back
again so I'll just skip on from that I
spent a long time talking about how
competitions have run so I won't run
through this except to say that we're
implementing a series of shared access
signatures for those of you who are
familiar with adhere to secure the data
at the moment because you can link out
to other other data sources you can
still use this with secure data you just
need to protect it at the source and but
when you upload data to our system we'll
have a shared access scheme which will
enable you to restrict data only to
those who should see it in a very secure
way and in the future we'll be
introducing a federated access system to
extend that security model far beyond
collab to secure data and third-party
accounts and beyond so I won't go into
this in details there's more details on
the web if you're interested in the
technical side to it I also won't talk
about exactly how a participant
participates because I've shown you that
so I'll just move on to this my final
slide this has been a presentation as
part of the Microsoft Azure for research
program which hopes to show you the
benefits of using the cloud for academic
research
thank you very much for your time if
there are any questions please feel free
I am I am them over or failing that of
course you can also follow up in email
I'll just move on one more slide to show
you this if you do want further
information on the medical imaging work
go to research Microsoft comm slash Med
imaging and with that link you'll go to
you'll be taken to a page which will
show you far more than just what I've
shown you today but will enable you to
find download links for Geoff's
references to code I'll have etc so
thank you very much for your time I'll
turn you back to the moderator now okay
Simon I did have just a few questions if
you guys want look at those in the I am
real quickly
okay I hadn't seen those car my mistake
you know I just popped them in there
okay okay so two questions here the
first one is does curdle have support
other clouds with regard to application
runs in data storage no it doesn't it's
an answer or as your only solution we do
make it possible for people to
contribute to the development of coda
lab by using Python etc but currently
it's all hardwired into as you are under
the hood there will be nothing in
principle to stop someone adapting the
code to use for example Amazon but it to
the extent that coda lab uses the
features of as here in particular you'd
have to find the equivalents on Amazon
although I'm sure they do exist the
second point is how application runs
underneath coda lab build that is a good
question
and at the moment although I do have a
good answer for you it's not a complete
answer as I say coda lab is a work in
progress at the moment
essentially the way coda lab works is
there is a default Azure account which
is the default location for upload of
everything that goes into coda lab that
account is owned by whoever owns the
codelab
instance now because it's an open source
project there's nothing to stop you from
downloading and installing your own coda
lab instance in fact we'll help you do
it that's not our first choice to be
quite frank because the whole point of
coda lab is community if you're putting
stuff up there to share it with your
colleagues surely it's better to go to
an instance that's already running in a
common location so that all of your
colleagues and their colleagues are
using the same instance and can
collaborate with you but anyway
sometimes you might want to run your own
and so we do make that possible so at
the moment the costs of the runs are
devolved to the owner of the coda lab
instance the owner of the coda lab
instance right now is Microsoft and will
shortly become professor Percy Liang who
is a machine learning researcher at
Stanford University and over time he
will assume a responsibility for
for these matters and will become one of
the faces of the community that we're
building
we have other academics who were also
collaborating with who may also choose
to run their own code alive instances in
the longer run now that's the case at
the moment so everything gets billed
directly to the application owner or
rather I should say the incidence owner
what we will be doing in the future is
we'll be adding a billing system in
there which will enable at the behest or
at the direction of the instance owner
they will enable the costs to be
devolved to the competition owner so
that will mean essentially if you have a
need to run a competition and if you
want to if you have the resources to do
so you may also have to cover the
billing to run that competition now that
the extent of that billing will entirely
depend on the size of the competition
that you run now what will be happening
with with most competitions is by
default they will just be uploaded into
the default account and costs will be
covered by the coder lab owner but the
reality is if you've got truly huge
competitions with truly large data
storage and and software running needs
then the competition the instance owner
may well ask you to to take
responsibility for the billing piece
that you incur ok I hope that lets cover
that all right Thank You Simon
so everyone we hope that you have found
today's information helpful if you
enjoyed today's webcast or have feedback
and how we can provide you with a better
event please let us know by completing
our survey you should see the link and a
pop-up box on your screen at this time
like to extend a big thank you to our
presenter Simon Mercer this concludes
today's webcast you may now disconnect
from this call
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>