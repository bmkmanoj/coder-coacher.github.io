<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Verified Concurrent Programmes: Laws of Programming with Concurrency | Coder Coacher - Coaching Coders</title><meta content="Verified Concurrent Programmes: Laws of Programming with Concurrency - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Verified Concurrent Programmes: Laws of Programming with Concurrency</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G83nBjXBQCo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
laws of programming I did published an
article in 1987 in the communications of
the ACM called laws of programming and
it built only with with sequential
programming 1987 I should have known
better front please I have to apologize
for a weak voice and I think it's how
you fever could you come down to the
front please if you can't hear this
please come down to the front would you
like to come down to the front
thank you then I can then I can speak
more quietly but just recently that's
probably about five or six years ago I
discovered how to construct a law for
concurrency and investigating it over
the year as I found it is a remarkably
powerful law and those of you haven't
heard of it before might know it under
the name of the exchange at all which is
used in category theory so summary what
are the laws what do they mean are they
useful are they true and are they
beautiful well I'll leave you to answer
the last question but I hope to show you
needs to give you some evidence the
answers to these questions should be
well the laws are going to be algebraic
laws of the usual kind very similar to
those you find in high school algebra
relating to numbers and the rhythmic
they will contain three variables which
were assumed to be universally
quantified and the like to give you some
intuition as to what those variables
stand for the variables P Q R will stand
for specifications designs or programs
descriptions of the behavior of a
program that we may have written already
or may yet to be executed in the future
or maybe it's QT now they may the
descriptions may be used to describe the
desired behavior of a program or the a
planned behavior as a program that's in
the case of a design or the actual
behavior of a program of a particular
program which has been written and is
executing past present or future a
single bad behavior will be recorded as
a set of actions which take place in or
near a computer that is executing the
program now the algebra will I will deal
initially with three operators then
which stands for sequential composition
concurrent or with which stands for
concurrent composition and the third one
is a constant skip which does nothing it
has no events in its execution for
example and P semicolon Q describes the
behavior which might result if you
executed all the events in in the
execution of P and followed these are
all events in the execution of Q that I
think should be one way of executing P
semicolon Q if it's written in a program
and P with Q describes the concurrent
execution of P and Q it contains all
events in P all events in
and it starts when but but both
execution starts at the same time and
end at the same time
so quite a familiar constructed it was
introduced by Dijkstra there will be
five axioms to start with a simple
axioms both operators are associative
both of them have the same unit skin and
concurrency is commutative
I hope your programming intuition will
persuade you that those axioms are true
or if not true then it would be an
awfully good idea if they were true
because that's what axioms are good at
they were good at specifying what you
want of your language without telling
you how to implement it or how to
implement programs in the language or or
how to prove them in any way they really
are quite one might almost say intuitive
since we've written these axioms down we
can note something about their syntactic
structure if you take any of those
axioms and you reverse the operands of
all the semicolons in it you will get
another axiom in that same structure
know of another axiom in the same group
or possibly even the same axiom so this
means that if you're using only these
axioms in a proof and you perform the
reversal on every line of that proof all
the axioms will remain axioms and all
the deductions will remain valid when
you substitute equals for equals in an
algebraic proof
so more formally we can we can use a
meta theorem known in the computer
science community as theorems for free
which says that when a Trenor theorem is
translated by reversing the operands of
all the semicolons or of some of the
concurrent of operators the result is
always a is also a theorem this is a
sort of time reversal symmetry which our
axiom so far obey similar to the time
reversal symmetry x' that you find for
many but not all the laws of physics
concept of ordering or refinement the
I'll use just an ordinary implication
arrow to stand for a refinement which
means that every execution described by
the left operand of the implication is
also described by the right-hand side
there's very similar implication we're
all models of the left-hand side we'll
need models of the right-hand side in
other words if P is more predictable and
more controllable than Q if P is a
program and Q is a specification then we
can use refinement for just correctness
of the program in accordance with the
specification if both of them are
designs it means that the design on the
Left adds more decisions is more
constraining on the behavior of the
design than the design on Q so we can
use this refinement relation in the
standard way
justify a chain of steps in the
development of a program now axiom
refinement is a partial order this is
really essential for the stepwise
refinement method to develop programs we
can noted a duality principle again with
these axiom swapping the operands of
refinement translates each of the axioms
into itself and this justifies a duality
rule by order of us all if P refines Q
is a theorem on the axioms would that be
true so is Q ROP that's the possible but
just rather quickly in case it's wrong
it's not really essential the concept of
monotonicity it should be very well
known to all mathematicians any body
would claim to be an engineer and not a
mathematician great in that case you may
not have heard of modernity but you use
it every day when you designed a product
a product out of components and somebody
comes up with a better version of one of
the components that you've used you
replace the original by the better one
and as an engineer would you expect your
whole assembly to be better or not I
said better and by better I meant better
in all respects last longer it's cheaper
it's faster it uses less power
everything it's better because that's
what we've got to mean by refinement we
want to forget all those respects in
which something might be better and just
use the bitterness of it it's a
justified replacement
of a an existing component by a better
one I would claim that if if you found
as indeed engineers vary quite
frequently do find installing a new
component does not make it better that
means that the original design was based
on a theory a theory of physics whatever
electronics which was incorrect they
should not based on some principle which
does not preserve Manas Amissah T so the
monotonicity has a very important
engineering implication as well of
course as being the fundamental concept
in mathematics yes that's why I can say
the only question being why do you make
the thing in the middle axiom since it
will probably be a theorem later
everything I've told you so far is an
axiom all my hand-waving is just an
explanation of why you should want it to
be true
I will the question that you have asked
is equivalent to the question is it true
and in order to show that it's true we
have to construct a model and prove the
axiom so it's in like in other
mathematics
one man's axiom is another man's theorem
and that's very nice relationship so
montanus t-this is a better theorem of
course allows the replacement of a term
you know in an expression by a better
one in all contexts
and that's just a statement of this fact
it's very very commonly used in
mathematics and even in algebra so let
me introduce now the one size it might
contain the mistake but I think you will
understand the exchange axiom this is
the new one which I saw you scribbling
around exploring various things you
suddenly write something down Wow it
seems to be true and then later on one
discovers it it's actually useful as
well the exchange axiom is an axiom
about concurrent it it's slightly
difficult to make it intuitive and the
reason why we want it I'll come on to
the reason why we want it to be true
when I described how useful it is but
intuitively you could say that the left
hand side is a more interleaved version
of the right hand side it makes a
decision as to how to schedule B
behavior if you look at the right hand
side it has two threads it may the left
hand side makes a decision on how those
two threads are to be scheduled it says
that the two displayed semicolons
in on the right hand side are to occur
together in other words all the events
in P and Q are to happen before any of
the events in P dash and Q dash so we've
got a semicolon on the left hand side it
separates the events from P - P - 2 Q
tokuda from Q - Q -
the two semicolons have been aligned
with each other in time to ensure that
only a subset of the interleavings
will be true it's slightly confusing
because it may be that the left-hand
side is actually undefined and it has no
executions but that that that will be
the secret of the validity of this law
in circumstances where we have the
possibility of deadlock if that's going
a little bit deeply for this stage we
need to have a look at the model to
explore how how the left hand side can
be undefined whereas the right hand side
is defined if there are inputs and
outputs between P and Q then the left
hand side may be undefined in cases
where the right hand side is defined
applications well I would like to claim
that the rules are useful there are at
least as useful as the laws of Hall
logic for the proof of correctness of
programs extended by separation logic to
the proof of concurrent programs every
proof as a correctness of program is an
application of the laws that I have
shown you so far
all these shall I say almost that
there's many applications of these laws
and the way I can prove that is to prove
that the laws the laws of Hall logic
actually follow from the simple
axiomatic foundation
I also recently discovered that the same
is true of the laws of milner logic of
the operational semantics which uses
milna triples instead of or triples to
describe how programs should be executed
so again although not all process
algebras are defined by Milner's
principles the we can derive from from
the laws a version of an operational
semantics from all of our language which
looks are useful in defining particular
executions of the of the program's
so the operational semantics tends to be
used by by implementers or anybody who's
looking for a particular execution of
the program whereas the proof rules are
used by people who want to make
generalizations about all programs
having a certain property so in order to
prove the laws of whole logic I've got
to define the primitive judgment of
whole logic which I do in this simple
way the triple is defined just as a
simple comparison refinement between a
sequential composition and between and
an arbitrary program so if P describes
what has happened so far and Q is then
executed to completion then the overall
behind behavior of P and Q together will
be a refinement of our now in
traditional logic P and P and R are
confined to be assertions what I will
have to somehow persuade you to accept
may be temporarily
is the idea that it doesn't matter
replacing the PNR biometry descriptions
of the behavior of a program for example
if you take a relational view about a
program and P could describe the
behavior of the program from the moment
that it started an R could describe the
program up to an arbitrary that fixed
moment in the future or indeed for
infinity or infinity backwards so the
whole laws are really special cases of
the general laws which are derived from
this definition I know that maybe people
will find it difficult to swallow the
idea that one can reason it triples
about programs specifications and
designs a lot of people have been kind
enough to make a mistake so remember the
definition look at the theorem of the
sorry I should say the axiom or axiom of
sequential composition which I've
written just below it I claim that I can
prove the validity of that law from the
axioms that I've given you so far
there isn't much room on the rest of
this slide but I'll try to show you oh
yeah
first of all I'll expand the definitions
and then play using the monotonicity of
semicolon I can replace that s by P
semicolon q and because of the
sensitivity I have now proved the
conclusion that took me 30 seconds to
prove have you ever seen a semantic
proof that took 30 seconds inductions no
nothing so what about the Routh rule of
consequence I don't think the proof is
much longer uses the same two properties
so I'll leave that as an exercise you
can take a minute and a half I'm
concurrent separation logic this is the
modular rule for proving concurrent
programs
it's allows if you have to prove the
correctness of a particular kind of of
the conclusion containing a parallel
composition you can split that proof
into two parts if you can split the two
assertions into two parts and then prove
the two parts separately
now this is actually equivalent to the
exchange law so if you don't like the
exchange law but you do like separation
logic you're being inconsistent they
really are inter derive a ball let's see
what we'll do is first he approves the
modularity rule implies the exchange law
we look at the two assumptions of the
modularity rule and turn them into
theorems by making the right-hand side
of the refinement the same as the
left-hand side now making the same
replacement in the conclusion of the
modularity law sorry we we will get that
implication there which is just a
statement of the exchange at all okay
that's nice so we now want to show the
exchange Lauren implies modularity so we
we the modularity law has two
assumptions so we can assume that the
antecedents are modularity door and
monotonicity of width gives us the
implication the refinement shown on the
next slide are the exchange law has a
right-hand side which is I've cut it out
correctly the same as the left-hand side
of this and the left-hand side would
look like this and by transitivity we
have the modularity rule
good I hope you're thinking a little bit
faster than I am because I'm not quite
following the proof as I'm saying it but
I know that Victor has it's very
comforting so here we have some other
important rules of separation imagine
these follow from from the young frame
theorem by just replacing one of the
operands by skip and vikas semicolon and
a parallel have the same unit these laws
are just a simplification of the
modularity rule and it's interesting
that we can find a whole rule which
couldn't have been expressed if F and P
were assertions which has the same shape
as the frame law the framing program F
has to be the first operand of the
semicolon okay
so I've left out all the basic
statements they need to be added they
are going to be expressed as axioms in
the same way as in the same way as in
whole logic so the rule of assignment
carries straight over into a law of
programming as was described in the
article in the communications of the ACM
the definition of the Milner triple is
really the same as that if the whole
triple with just the two operands
reversed the Q and the P and it's but
it's red it's interpreted in a different
way it says that are may be executed by
first executing Q the
q is written in the middle of the line
with P as a continuation to be executed
when Q is finished so again it's a
refinement maybe other ways of executing
are than by executing Q first you might
execute Q dashed first and have a
continuation P dashed and they both be
valid executions of the same of the same
R now let's prove one of the basic laws
of CCS that our Q then P may first do Q
and then P it is a tautology and if you
expand the definition you'll see that it
is a tautology now that's the rule by
which ccs introduces the meaning of the
prefixing action in the standard ccs
definition due to Milner the Q has to be
an atomic action and therefore Q has to
be a single input or single I put and
that is the form in which Milner quotes
the law but I'm using what are called a
big step semantics in which I can trace
allow our Q to be again any program so
these restrictions do tend to stick in
the throat of people who are familiar
with the various kinds of logic and
that's fair enough
because in application if you are
applying if you're applying our theory
to real programs you will in practice
you something much more like whole logic
because it's specially designed for
proving
Magnus and similarly if you're using
your logic to guide an operational
semantics or an implementation of the
language you will use an operational
semantics a presentation of the same
laws but for Theoretical purposes we
want to bring these two studies together
and show that they are consistent
now of course acres of poeple print has
been expended on proving the consistency
of operational and axiomatic semantics
and that's if you're lucky most papers
will present only one of these two my
suggestion is that if you start with the
axioms and make the intellectual
generalization of not to distinguish
assertions from program not to
distinguish basic actions from other
actions then all of these things follow
from the same laws and if the laws are
true I don't even have to say that every
model of program execution and behavior
which satisfies the laws is also going
to satisfy the operational semantics and
the verification semantics period I
proved it to you a shame I've been
nursing
our composition little-bitty you just
said they're wrong concurrently but you
didn't specify what kind of interference
is the model that's part of the model I
think depends on parallel composition
being in a particular way and that being
the same as separation logic says about
the true fighter I and my colleagues two
of them here Bernhard have shown that
there are enormous numbers of
interpretations of these two semantics
which make him work so if if it doesn't
work for yours try a bit harder thank
you you haven't shown it seems like a
conditional statement and I believe
that's because that would be the model
no I can show the axioms for it they are
in fact shown in the in the
communications of the ACM article I also
have a choice to union okay because my
next question was gonna be and you have
something like that conditional on one
branch puzzle so the answer is yes yes
you can s you can nest semicolon the
currency at will if you really thing
that's spooky introduced in a cm
statement yes so I think your the way
that you've stated it it's sort of
misleading you're being a little unfair
right I mean it make it sound that it's
if what's in work you know your logic is
simply a special case of this but of
course it's not because whore actually
has incomparable axioms right they're
models of horror logic which are not
models of yes in fact you can't derive
laws from whole logic or from
operational semantics but if you do the
derivation in the other direction which
has been done by Stephan who's already
as also here you get a very large number
of laws it turns out that for a whole
logic it's not necessary for you can
have a weak associativity law in which
the which is just a refinement that the
I think a pardon difference arises when
we introduce choice which I haven't done
in in in the case of whole logic
semicolon distributes backwards or
forwards through through but not I mean
through the lies that you have the fact
that they already in those that the
frame works right
and those imply that things are
sufficient signal for the original or
logic which means that it could prove
some additional Hort triples that
wouldn't be true in logic right so yes
that's right the summary so I think as
long as the accidents are consistent
really we talked about all models
those axioms then what I'm saying is
really very strong it doesn't say that
but it isn't in equivalence the the the
to calculate are definitely weaker and
the laws so here's a nice correspondence
I've proved for you the validity of
sequential composition in the or logic
and the Norfolk sequential composition
in Milner logic is written just below it
of course Milner didn't like sequential
composition because you can define it in
terms of of concurrent composition and
but if you do want to put sequential
composition into a precious algebra this
would seem to be a very reasonable big
step law to do it by if you look
carefully you'll see that one of them is
just the time reversal of the other so
all those years when I thought that
robin Milnes semantics was a was a rival
of mine I was completely mistaken we
were arguing about which end of which
end of the egg to open first this time
go forwards or backwards and the laws
are as I say insensitive to that
distinction concurrency MCCS the
modularity rule exchange law is also
insensitive to time reversal and
therefore we can derive the concurrency
law for CCS which is just that
where P and P - have to be restricted to
particular basic actions only if P and P
- are synchronized it's just a time
reversal of the separation logic rule
frame rule for milner logic is exactly
what he quotes as a rule for concurrency
in CCS
if given thread can perform an action it
can perform the same action in any
concurrent context and the second rule
is the dual of the frame rule for
sequential composition and that is the
second thing you have to introduce into
into a process calculus in order to
defines the execution of semicolon
uniquely see the pnp parallel P primed
you know we're always required day they
were opposite actions like a and a bar
yes so it couldn't be applied for
arbitrary P and P Prime
that's right in essence it was a partial
operation definitely but here parallel
is a total operation I'm using a big
step semantics so it's sort of not quite
so
considered models which are much better
at providing counter examples and
algebra in fact in algebra you can't you
can't find counter examples at all so I
think models and algebra have a very
important complementary role in fact an
extension of builders and the claim is
that it's perhaps not oh yes
the claim isn't everything let's say
about Milner I wish I could have checked
with him so it has to be a version of
because when you do small step and have
the basic actions then for example it's
kind not wanting not having the
sequential composition make sense you
know it's because you've gone to the big
step that these generalizations are
coming up yes particularly if you look
at my formulation of the sequential laws
in the Milner notation you will see that
when they even even when those are
restricted to just atomic actions on the
arrows that they are complete enough
yeah
so this is another time reversal sort of
thing refinement reversal the sort of
insight that on any step of execution a
computer can reduce the range of
non-determinism of what it still has to
do whereas in the case of a proof on any
step of proof the conclusion can
increase the range of non-determinism of
the antecedents it's just another
duality but when you consider the rule
of consequence we get a similar rule for
the process algebra approach which uses
the fact that execution steps may refine
the program that is being executed as a
result of internal actions and this law
games I think would not be acceptable to
Milner but it is to me I'm afraid let's
have a look so summarizing what's before
both the horrid milna rules the rules of
the calculus are derived from the same
algebra program algebra is simpler than
each of the calculus as less operands
less clutter and it's stronger than both
of them combined they're more useful for
example because we can we can use them
very directly an optimization
and in transformation of expressions
which if we have a rule based approach
to reasoning gets a little bit more
cumbersome so at least in this sense the
the an operational interpretation and
operational semantics and a maxim attic
semantics gained a measure of
compatibility consistency from those
being derived by the same laws it's not
that there aren't models of the weaker
laws of course there are models of the
weakened laws would differentiate them
if that's what you want to do I want to
unify them so this is this is the way to
do it so I can two models what are the
behavior I'm going to take a very
general definition in fact I've already
given it it's a set of events occurring
in and around a computer that or a
computer system that is executing a
program so the immediate immediate
interactions with the real-world
environment of the computer system are
also included as part of the behavior
really essential in order to model
interactive systems so let a movent be
the set of all occurrences of all such
events that ever were or ever could be
that seems a little bit why don't I go I
construct them out of something why
bother mathematics allows the existence
of sets and if I want to base my whole
algebra and IO model on the existence I
can parameterize my model by the set of
events I can decide later whether the
events have a unique time of occurrence
whether they have any
position of occurrence whether they
represent an assignment or an output or
a fetch all of these things can be
decided data all I need initially is a
big a big enough set of events which act
as a sort of carry us out of the algebra
and on which I can build
parameterize the syntax in my language
and the semantics of the basic commands
of the Lachman language so remember they
are occurrence is two events two
occurrences are always different so I'm
now going to introduce introduce I say
it's already appeared in many of the
talks in this workshop the concept of
causal connection between events
certainly used quite a lot in in the
lectures on weak memory it's also used
at a higher level in in computer science
when you deal with message sequence
charts or boolean waveform diagrams
while you draw you draw an engineer's
draw an arrow between two events I a
rising edge or a deforming edge if they
are intended to be causally connected
and here are some examples in ordinary
software the end output on a reliable
tracker channel has the end input on the
same channel dependent upon it
yes release and the elf the choir the
end assignment and the read a read of
the value assigned by the end assignment
and in strong memory the read of the
enth value assigned by the end
assignment happens before the n plus
first assignment in a week week memory
that link has been broken and you can
read things that were written sorry they
they could be broken now I think in
future week memories that will be broken
because future memories will be too slow
just too slow to make it possible to
even give a guarantees that current week
memories give so let's take the
reflexive transitive closure of a
dependency relation which is the sort of
direct or indirect necessary condition
relation for some event to happen and we
take its converse in usual way and
examples are that every allocation of
resource must reflects transitively
precede every use of it and every
disposal of resource must follow every
use of it in fact if you really are
defining the language by describing what
you what the really essential condition
on allocation and deletion is those two
statements define allocation
release they don't tell you how to do a
memory allocator or whatever memory
allocator you do should satisfy that
property no we can interpret these
symbols to explore the expressive power
of our model we can say that if he comes
before F and F happens before he since
happens before is reflexive it's not
inconsistent
it's just that they must be the same
atomic action
conversely if neither of these two
dependencies occur then the two events
are independent of each other and they
can occur actually concurrently or one
of them can be occur before the other or
vice versa or they may overlap in time
the model hasn't said what the
interpretation of it is but we can see
how expressive the model is in saying
the sort of things we want to say about
events even if they're not atomic action
or parts of the same atomic action
maybe this is only an example of the
kind of intuition that you might have I
think you may have some better
intuitions actually so let's talk about
possible traces or sets of events P Q
and R and I can define P times Q as the
Cartesian product of P and Q and the
usual way and prove a little theorem
that our Cartesian product distributes
through Union introduced Union yet and
now I want to define sequential
composition this is just one of the ways
of defining sequential composition I'm
working on general recipes for the
defining other versions of sequential
composition if that's what you want so
let's introduce a relation arbitrary
relation which I call Zack sequence a
relation between events and I'm going to
define my sequential composition
operator in this slightly peculiar way
it's just the union of the events that
occur in B with the events that occur in
in Q
so do you want me to finish there's
nothing else scheduled Thanks
right I'm defining a relation between
traces which states either P is
undefined this is the Scott define meant
ordering between traces this one is the
Scott define meant ordering between
traces because I'm going to want the
semicolon to be undefined in cases where
this is false so this says if you take
an arbitrary event from P in an
arbitrary event from Q they will also be
in the sequencing relation so we okay
let's go on we want to prove the
associative law from just the definition
we've left the actual definition of seq
as a parameter of the model you can get
different versions of semicolon by
filling in that parameter in different
ways so here is a proof that no matter
what the relation you feel for seq you
will define an associative operator so
the LHS is defined if all the operands
on the left hand side are defined
so SEC must hold between the events of P
and Q between thee and it must hold
between all the events of P and R and
all the events of Q and R that's
expressed by the fact that we can
distribute the x through the or and when
we do that we can prove that the right
hand side is defined as well I don't
think it's worth spending any more time
on that but that but that's the length
of the proof now some particular
examples that might be interesting for
sequence P semicolon Q could be the
negation of the converse of the
dependency relation it says that nothing
no action of the first operand depends
on any action of the second operand well
if it did then you wouldn't be able to
complete the first operand before you
started the second operand because
execution of the first operand
completion the first operand depends on
something that is only available to you
after you started the second operand so
this seems a very weak definition of
sequential composition and for that
reason it's very nice for example it
does describe the way that sequential
composition is implemented on this
machine after optimization
and here's an example let's represent
events by just natural numbers we have a
sequential composition between two
disjoint sets
none of which only has actions that
occur later than all the actions of the
other so the sequential composition is
just the direct union and here here we
get an undefined one because we have the
same number on both sides and the knot
here
rules that out because the equal or less
than is reflexive so let's define a
version of concurrent composition it's
done by just taking power as the union
of the sequence operator whatever it may
be and it's converse so far it's just
just the reflexive closure of Seck no we
define people are like you in the same
way by requiring everything to be in the
parallel relation in order for it to be
defined it's easy to see that because
power is itself are commutative the
sequence will also be commutative and
the proof that it's associative is the
same as the proof that sequencing was
associative and this exchange satisfies
the exchange law with semicolon here are
some examples and the last one is
perhaps interesting it really shows you
that interleaving is one of the ways
doing a parallel composition that is a
proof of the exchange law what we're in
what I'm interested in is working out
theories that are going to be useful in
the design of tools for software
engineering
I don't expect real programmers ever to
understand a theory as complicated as
this although perhaps they might learn
it in their secondary schools one day
but tool builders really must agree on
the semantics the theory of the
programming programs which their tools
are operating on I'm going to
differentiate three kinds of tools a
verification tools by which I will
include program analysis tools
implementation tools like compilers and
runtimes and testing tools and this is
something which hitherto I have grossly
neglected although that I believe that
the testing tools are the things that
will be the first to apply formal
methods in general programming practice
because testing is what programmers
actually do already and if you could
make a 10% increase in the efficiency of
testing by ordinary programmers you
would please them and their managers and
their employers quite a lot so testing
tools verification tools really require
a deductive sort of logic and the
compilation tools and implementation
tools really rely on an operational
interpretation of programs and testing
tools depend on the models the
denotational semantics
the original Scot semantics which tells
you what the program's actually mean and
in my view I brought to you what they
mean is that is the set of executions of
that program because there's the
executions that you're testing tools are
actually going to have to show to you
and explain to you the proofs I've shown
just simply show how you prove what
sense those three types of tools can
ensure their mutual consistency through
the intermediary of the laws the testing
tools describing how programs are
actually modeling how programs are
actually executed from which you can
prove the laws and then from the laws
you can prove the validity of the two
two kinds of other semantics that's the
big picture it was a summary of what I
was hoping to say the laws are useful
they tend to lead to short short formula
short theorems and short proofs they
prove the consistency of proof rules or
operational rules if that's what you
want and say establish well sorry
consistency with the implementations I
haven't shown you very much but give
them just a start of how you would
construct a model showing that the look
and proving that the laws are true of
the model and I'm coming to believe that
the laws are true not just of software
but of hardware sorry the laws are valid
in models of but it that way
hardware software and indeed the real
world
even in the real world an event cannot
happen if it depends on an event that
has not yet happened violation of that
principle would be shall we say run
counter to thousands of years of
scientific research might get you a pub
it might get your results published in
the news of the world exist but it's not
as far as we know good science and but
also beautiful well as I'll leave that
to your judgment not everybody agrees
with me and the first person I'd like to
quote it doesn't agree with me was Isaac
Newton Isaac Newton worked I've
beautiful laws for differential calculus
go to called the differential calculus
in these days he called it fluxions he
wrote in a letter to Richard Gregory are
specious algebra I it looks convincing
but it's invalid that it's the
infinitesimal calculus is fit enough to
find out it has a heuristic value
suggesting what theorems the might be
but it's entirely unfit to consign to
writing you can't put it in your
publications and and committed to
posterity
I think posterity has taken a very
different view Bertrand Russell in his
introduction to mathematical philosophy
says the method of postulation has many
advantages they are the same as the
advantages of theft over honest toil he
put this remark rather carefully when he
moved from a rather simple branch of the
foundations of mathematics to one that
really was quite a toil
going from into just a fractions
got read line its 18th century German
philosopher had an answer one word
answer to both Newton and Russell
calculators let us calculate because
algebra is the tool of calculation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>