<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards Zero Latency Photonic Switching | Coder Coacher - Coaching Coders</title><meta content="Towards Zero Latency Photonic Switching - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards Zero Latency Photonic Switching</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C8PQsjK-BKA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's my pleasure to have a few words
here so you graduated new CL 2008 then
they move to Cambridge where it was a
research fellow at computer lab and now
he's back to UCL as a as a lecture so he
has been working a lot in optical
connections and today is going to talk
out about his recent work on photonic
interconnect and the optical switching
ok thanks a lot ok thank you yeah it's
very really great to be here so yes
palisade er I'm i confer saw an optical
engineering background I spent quite a
bit of time in industry designing
products for for long-haul fiber
communications and I did a PhD also
looking at a long-haul fiber
communication so this is trying to send
the maximum number of bits per second
along you know of the maximum distance
so under the Pacific or sort of
situations like that so after my well
Jackie during my PhD I came here to
Cambridge to work for the for the Intel
Research Center which was here that here
at that time sadly no longer here and I
got interested in using optics for for
computing for optical interconnect and
so after after my PhD I went to the
computer lab where I wanted to learn
more about computer architecture so it
occurred to me that there's lot of
people looking at optical networking as
I saw a black box not considering some
applications or what was going to
actually attach to the end of the nodes
of this network and there are people
doing some work on networking on variety
of levels of computing maybe taking
making assumptions about optics but they
didn't so understand the optics or the
lowest level so I wanted to to work in
that interface between the two and
that's what I've been trying to do over
the last few years so as Palo sale yeah
I spent three years at the computer lab
working with
people like Andrew more in the networks
group john crofts group and also with
with simon more in the computer
architecture group and then i moved back
to to UCL because i wanted to have
experimental facilities in in in optical
communications as well so i now work for
the optical networks group in a UCL but
five academics here most of our work is
still on on the long-haul optics so
maximizing bit rates using advanced
modulation formats and digital signal
processing we've also got some network
some large-scale networking work like
cognitive optical networks
reconfigurable networks for for access
and an long haul and what I brought back
broke to the team is is the is it is the
interconnect work but we've got a very
well equipped lab for optical
communications 40 gig aboard that's
probably an underestimate now and very
large transmission distances or
trans-pacific transmission distances and
although I didn't put it up there we've
also got a lot of expertise in FPGA work
both for DSP and for controlling the
network control okay so starting point
there's a lot of people looking at data
center power consumption these are vast
data centers I almost require their own
powerful powered generation facilities
they that they've grown grown so so
large now but looking at the detail
according to google only five percent of
that that power is consumed by the by
the networking devices and so there's a
lot of people looking at optical
networks for data centers and justifying
its in terms of energy but we're talking
about only five five percent there
and if we look at this this study and
the bottom right here which shows this
of energy profile as we access a cloud
device from from a mobile phone we can
see here I hope you can read that that
the that the big switch is the course
which is that the switch is at the top
end of the data center hierarchy are
actually very efficient and even the top
of racks which the energy and efficient
points are the the user access end the
actual mobile phone and access and down
at the server and rach scale and that's
where I've focused my attention on this
at the server and rach level because
these google this google figure of 5%
doesn't include the networking elements
which are actually inside the server
itself and now increasingly actually on
the server chip itself including i
firmly believe will have the photonic
components actually on the server chip
as well and of course a big issue down
at the server level is not not just the
total amount of power you're consuming
but also where it's consumed this is the
power density so we've reached a
situation now where we can put several
billion transistors on a chip but if we
try and switch them all at once it'll
it'll overheat and so this issue of dark
silicon is becoming very important yep
what is the percentage or facilities
power consumption that is sort of
networking whatever now working right I
was just just about to come on today
actually so yeah so if we look at a high
performance server this is a spark spark
t3 which is you've got quite a lot of
literature written about it which is why
I've some use it as an example here this
is a 16-core core device it's got a very
large crossbar on
chip to connect the the cause with the
the l2 caches 512 gigabytes per second
consumes actually quite a surprisingly
small amount of the power but we've also
got 1.3 terabits per second for for
memory both both links to two drams and
and the and to provide chip-to-chip
coherence because this is designed to
work as a force socket server with a
shared memory across all four chips and
we've also got pci and two integrated
ethernet ports on chip as well and this
is this off chip interconnect you know
well over terabit per second which is
consuming twenty percent of the of the
power consumption in this in this
particular chip and i would think that's
probably typical so of course with with
devices like the high-speed serial ports
and the and the ethernet going on chip i
think it's inevitable at some stage we
will also have the photonic components
going onto the chip as well particularly
given all the interest in in photonic
interconnect so i don't know what's a
level of of knowledge people have of
these photonic developments but just to
give you a few very brief examples here
in in silicon now of course silicon uses
the all the investment that's gone into
CMOS which is why there's so much
excitement about it we can build what
wave guides at the at the top there but
very very tiny wave guides which which
can have very small bend radius radii so
if you if you bend an optical fiber
you'll get start to get losses
eventually break of course in silicon
because of the refractive index of
silicon we can get very small radii
enhance build very small features still
very much larger than the c MOS
transistor but small compared to any
other putana components that are
available these ring resonators down on
the bottom left of attracted a lot of
attention because they can modulate
filter ends and switch some temperature
issues with them but you know research
is looking into those we can also build
photodiodes by integrating germanium
into the into the silicon the difficult
thing is integrating lasers but people
have worked out ways of integrating 35
materials into silicon so you know we
can put lasers on on a silicon chip if
we want or we can keep the lasers off
chip and just pump in like a photonic
power supply and we can also build
waveguides into into pinter printed
circuit boards as well that's another
activity cambridge and we can also
switch at fairly high rate so one
nanosecond switching time again throw
them the CMOS but still very much faster
than other switching technologies so the
only widely used commercial optical
switching technology at the moment is I
things like liquid crystal piezoelectric
control or or MEMS MEMS devices which
switch very slowly so these are ideal
for circuit switching for reconfiguring
a system and leaving it in a state for a
long period but if we want to do
anything on a shorter timescale packet
switching or setting up very short scale
circuits for the shared memory we need a
much faster switch and there are a
number of candidates for this which have
been researched we've got semiconductor
optical amplifiers on the left here
which if we switch the amplifier on we
will pass the light if we switch it off
it absorbs the light so we can build a
tool to port switch by by splitting the
light and switching one or other of the
of the amplifiers on or both of them if
we want
broadcast magazine der interferometers
our own another option very very very
fast they've got some crosstalk issues
and also the ring resonator that I just
mentioned can be used for switching and
this is this is little difficult to to
understand basically if the if the ring
is in resonance that means it's it's a
whole number of wavelengths light is
taken from the horizontal waveguide
there on to the vertical waveguide and
we can change the resonance by changing
the refractive index of the of the ring
I won't go that into that too too much I
can explain it more if afterwards if P
if people are interested so we have lots
of exciting developments going on in in
photonic integration and silicon
photonics and there's a couple of
concepts we've been looking at in our
work at UCL one is fo for shared memory
I got interested in shared memory when I
was working with the computer
architecture group but a Cambridge so if
we have this or multi multi socket
server rather than having this this to
layer interconnect of an on-chip
crossbar or mesh network and an off-chip
a chip two chip network which is is done
in electronics because there's a
fundamental difference between the
off-chip wave transmission lines and the
on-chip wires we can build a single
Network which could interconnect all the
cause through some sort of optical
optical switch and give us effectively
singlehop connection between any cause
the challenge there of course is how do
you set up those connections and that's
been the subject of our of our research
how come on stats in a minute
the other thing we've been looking at on
the on the rack scale rather than the
single server scale is to to replace the
top of racks which you know sort of data
this sort of traditional leaf and spine
data center hierarchy within with an
optical switch and have devices that the
transmitters and receivers integrated
onto the server chip so we have no other
devices in the path again we got direct
server to server connections within Iraq
to hob connections between any server in
the in the data center and this is only
one possible topology you could use of
course but it's it's kind of an obvious
way of inserting an optical switch into
this arrangement and it has a number of
advantages we can get very high
bandwidth to this helps to overcome
things I in catched issues we don't have
the same pin and front panel bandwidth
issues we'd have with electronics and
the arbitration problem again comes down
to can we set up the connections fast
enough is relatively intractable if
we're just talking about providing
enough connections to connect the
servers and sum up links to the spine
switches 64 ports optical switches I
actually 128 port switches have now been
been been demonstrated in an integrated
chip so those are the two concepts I'm
going to talk about the shared memory
network for a single chip or multiple
chips and this optical optical top of
rec
so as I've just mentioned that the
challenge here is can we given that in
shared memory we're dealing with very
short packets that the challenge becomes
can we set up these connections fast
enough so we have so lots of nodes
connected to an optical switch this is a
very sort of traditional switching
arrangement and we don't do anything too
clever the latency is to determine light
like this we need to send some sort of
requests to the the switch controller
the sums of arbitration process has to
take place and then we give the
permission to send of course this
process goes on within an ethernet
switch as well but that's all contained
within a single device if we want to use
the tonics effectively and put this our
signals into the photonic domain on the
server chip this this is going to become
a sort of distributed problem where the
the switch and the nodes are separated
at least by a few meters we've then
obviously there's some time of flight
for the signals to to reach the the
receiver and then of course the
serialization latency can be very low
here because we can we can generate very
large bandwidth optically
synchronization latency means we need to
recover the clock we need to have us the
same clock between transmitter and
receiver we're looking at that problem
but I won't talk about it too much today
so what I'm really going to focus on is
is this process of setting up the
optical paths on on the switch and just
before I start on our work i'll just
give you a brief overview of some of the
ways that other people have looked at
this this problem so on the top left
here but speculative transmission in
which we we send a request to the
switch controller and very shortly
afterwards without repeating for reply
we send a data so if the if the arbiter
can find a path through the switch then
the data goes straight through to the
receiver with very low latency if it
can't find a path that's the data is
either dropped or or reroute it in some
way people have looked at optical
optical methods of arbitration tends to
involve very sore complex optical
schemes HP did some work in this and
then a lot of groups simply try and
avoid the arbitration problem altogether
by by using a different top optical
topology so on the top right here you
can see this scheme proposed by Oracle
they've decided that optical switching
is just too too far away or too
difficult to to do and so so their
approach is simply to use the the
bandwidth you can get in optics we've
got space and wavelength dimensions to
play with and create an all to all
network so we connect every every node
with a with it with its own individual
connection defined by other space and
wavelength and of course this means you
need n squared transmitters and
receivers an alternative which is
slightly less complex is this single
writer multiple reader scheme the first
time I came across this was the Firefly
project Northwestern University but this
has become very popular as a topology
for the networks on chip and sometimes
for the largest scale optical networks
as well here we have n waveguides
optical waveguides on each waveguide one
node is the tramp is the transmitting
node all the others are listeners and
they and they they listen for
communications that are that are
intended for for that node so so again
we need a lot more receivers in this
case than we would in a
in a conventional switch so these are
sore to ways the altar all network and
the single writer multiple reader two
ways of avoiding the arbitration problem
by by increasing the complexity of the
optical network so by comparison what
we're in our shared memory work we we've
done some work on on Alan arbiters
actually you know we can we can do it
arbitration even for quite a large
number of ports 64 ports in a few
nanoseconds we know the latency is very
critical for these shared memory systems
but also at this level on the level of a
single server with a shared memory the
Communications is also very predictable
so our approach is to keep the central
switch keep the keep the arbiter but try
and use it as little as possible as much
as possible try and predict the
communication that's going to take take
place in this in in this shared memory
system so i'll just briefly describe
well the traffic looks like on a on a
shared memory memory system i don't know
how familiar people people are with this
so here we've got a number of processes
on the on this interconnection network
they've each got a private l1 some
proportion of a shared l2 and and the
directory associated with with that
block of memory so they control a block
of memory and they own and they have the
directory co-located for that block of
memory so if core core 0 here wants to
access a memory control by core three
because it's going to work yeah it first
needs to send a request message and this
is quite
all message just 8 bytes that goes to
the directory CPU three that realizes
that that data is also it all already is
it's been given exclusive access to this
cpu n so it has to send a message to CPU
and again just a short control message
CPU n invalidates that that data in its
in its cache and then the data the
present state of the data is sent back
to the requesting node that's inserted
into the l1 cache and then an
acknowledgement message goes back to the
node that has the directory so you can
see most of the most of the
communication here is in terms of very
short messages and optics which is a
naturally a source circuit switching
technology might not be the ideal
communication medium here but also the
communication between there's only a
certain number of transactions we can
carry out on this I mean the simplest
ones are load requests and write
requests in which there are there are no
other sharers and you can see here that
these these just can be met by having a
bi-directional circuit so when the
initial request is made here in the load
request case we we know straight away
that there's going to be very in near
future a data response message or in the
right request we also have this on block
message a kind of acknowledgement
message and so if we set up a
bi-directional circuit here we can avoid
the arbitration process for for for the
subsequent messages in the in this in
this transaction this is very quite a
simple idea but it turns out to be very
powerful in a bit later on I'll show you
some of our work looking at predicting
more complex
transactions and also predicting the
first request message as well but this
just applying this simple method of
setting up bi-directional circuits when
when initial request is made actually
turns out to be to be very powerful on
its own so we've run the parsec
benchmark in jam five and looked at the
traffic patterns within there we can
save about sixty percent of the
arbitration latency on average across
the benchmark irrespective of how large
the network is so if the time of flight
if we've got a network on chip the time
of flight between switch and a node is
just going to be one or two clock cycles
if we've got this multi socket server it
could be up to ten clock cycles but we
have the same average saving and what
that means is that the load on these
networks is actually pretty low so we
don't get a lot of contention because
you would expect as you if you set up
these bi-directional circuits obviously
the longer the longer the time of flight
the the longer the circuits have to be
but this linear relationship basically
shows that the load on this network is
low enough that we we these longer
circuits don't interfere with other
traffic so that's this so graph is in
terms of the the arbitration overhead in
other words that the number of clock
cycles it takes to set up the circuit it
also be useful to to compare compare
this this scheme applied to the the
basic so optical switch with with with
these schemes that don't which avoid
arbitration by such as the single writer
multiple reader scheme so in our assumed
case we've got this
this broadband switch we're going to
send 660 gigabits per second per port 16
wavelengths at 10 gigabit per second
fairly conservative really but we found
out we don't need more bandwidth from
that actually in this case we've shown
that through the gym fives simulations
and in this case we need 16 transmitters
for each port read the switch and the
arbiter by comparison in the single
right of multiple reader case we've got
we need a lot more transmitters so we
yes we could put 16 wavelengths on to
every on to every note 616 wavelengths
of bandwidth on to every node but this
would mean we'd have a extremely large
number of transmitters and receivers so
in practice people are looking at these
single right of multiple reader networks
are assuming that you would only have a
small number of of wavelengths so the
serialization latency is a lot lower in
this case boots on the on the other hand
you get you get reduced or eliminated
arbitration latency on there on the
right-hand case yeah yeah that said lead
to more efficient broadcast because some
cash coins protocols like Intel's use
broadcast yeah that's right do it does
depend on that so we're using a
directory protocol with which yeah it
doesn't it doesn't it doesn't use use
broadcast but I know yeah broadcast
would be a very useful thing it really
depends on how your you're doing the the
receiver here actually as to whether you
can broadcast I mean I've betrayed the
receivers as being ring resonators here
so with ring resonators I actually what
you need to do in this case which I
didn't mention is basically telepacific
receiver to expect a communication you
need
reservations scheme actually because the
ring resonators can can either be on in
which case they take all the optical
power into their you know into their
node and don't leave any for anybody
else or they're off in which case the
power continues along the ring that they
can't be half half 0 because they
because their resonant devices they
can't be half on or you know extract ten
percent of the power so so actually you
have to tell say no to to expect a
communication from from no node 0 if you
could rearrange this and use splitters
to effectively send power to all you
know to broadcast in which case you yeah
you would be able to do a broadcast but
then of course you'd have to you'd have
to actually transmit n times the the
optical power as well in that case so
yeah either either way you do it you're
kind of limited by this of power and
complexity yeah just dessert to answer
the call yeah great
yeah so so this this shows the
performance in terms of the average
memory access time for for the crossbar
switch is the first to set of bars there
for the the the central switch for each
of the PASOK benchmarks and these are
for the single writer multiple readers
with 1124 wavelengths and we've
referenced everything to the single
writer multiple reader with with one
wavelength and you see I mean the
variation there's a lot of variation in
terms of between the benchmarks in terms
of speed up in the memory access time
but the key thing to note here is that
if we look at where we're arbitrating /
memory transactional memory sequence we
get about fifteen percent speed up and
in order to get the same the same
performance from the single writer
multiple reader you need for wavelengths
/ purport however if we look at the the
power consumption because of the
additional receivers and transmitters we
need that that network with the 44
wavelengths actually consumes
considerably more power so we're getting
the same performance with with with less
power less complexity oops something or
we can get we can get better performance
with with a you know a small decrease in
power compared to the one wavelength
case okay so so I'll just describe some
of the other work which is is currently
on ongoing obviously this bi-directional
circuit idea it's actually quite quite
quite simple we can go much further than
this
so one of the things we're working on
with Columbia University is trying to
predict that first request message so
before it's actually issued by the by
the processor so if we look at the the
results for the bi-directional case here
we've be plotted for each of the PASOK
benchmarks the the proportion of
circuits which require no arbitration
step if we just leave the circuits open
so when a memory transaction is
completed if we if we leave the circuits
open we're basically look we're trying
to exploit the locality by basically
predicting that the next transaction
will be to the same to the same node and
that turns out to be very successful
particularly depending on the benchmark
that is in the case of VIPs seventy
percent of the memory transactions
require no prediction no arbitration
step if you leave the circuits open you
can also see again as I said before how
low the contention is on these these
shared memory networks lesson two
percent of of transactions actually
experience contention but the question
is can we make a better prediction okay
we get a really good results here in
terms of VIPs but if we are running
swaptions only we're only right with
this prediction that the next next call
will be the same one as last time we're
only right three-point-five percent of
the time so with Columbia we're looking
at we're looking at similar techniques
that are used in pre fetches actually to
to try and make a better prediction so
when a memory tracks out transaction is
finished do we leave the circuit open or
is it better to based on knowledge of
the algorithm and patterns that have
gone in the gone before is it better to
to change to a different circuit at that
stage in anticipation that a new
transaction will will use a different
circuit
so that's one of the things we're we're
working on another thing which we're
would just or writing a paper on at the
moment actually is predicting more
complex transactions because I've just
described this algorithm which uses just
two core transactions but of course if
we need to invalidate memory memory
blocks because they're being shared we
have more complex memory transactions
which involve three or more or more
cause and for this we've looked at a
method of misprediction so so thi lay in
this case in the top left hand corner
which is a store requests with with an
invalid eight core one looking based on
previous experience can can predict with
quite high level of accuracy actually
over eighty percent in most benchmarks
can predict that well there will be a
mess in the l2 and can and can therefore
set up some of these other circuits in
advance which gives you a further
decrease in the in the arbitration
latency okay so those are things we're
working on in the the shared memory
shared memory case
so one thing I'm very interested in here
is having a having a unified network for
on-chip and off trip I think that will
give us a big advantage over the over
the sort to stage networks you see in
that in that spark server and many other
servers but also looking at whether we
can make efficient larger shared memory
systems using these ideas of course you
can't get rid of the the extra time of
flight if you keep adding nodes to the
shared memory network over over multiple
chips but we can at least minimize the
arbitration latency sufficiently that we
just the the latency will be dominated
by it by the time of flight latency of
course driven by the large bandwidth we
can reduce serialization latency as much
as possible as well ok I'm going to go
on now to the other scenario which we've
looked at which is the the top of rack
switch so in the shared memory Network
I've just been describing because we've
shown a way of reducing the need for
arbitration as much as possible but
there are always be situations in which
we still need to carry out that
arbitration process and also if we look
at networks on largest levels like the
ethernet network which is going to
interconnect a data center the traffic
is going to be maybe much less
predictable I mean maybe there are ways
of predicting it I but it's going to be
less predictable in the shared memory
case so we need ways of being able to
carry out arbitration as rapidly as as
possible and the other thing we were
concerned about here was actually the as
I mentioned at the start the power
density issue on the server on the
server chip itself because of course as
we put more of more functionality onto
the onto the server chip such as the
network
net network interfaces is already going
on chip as we start for the photonics on
chip of course all those functions add
to the the power density problem of the
server chip so what we try to do here
was build a top of rack switch which has
both low latency and also has a very
simple server-side interface to get the
minimum power consumption to try and
push the complexity into the the top of
rack switch so we started off with a
virtual channel switch as a baseline so
in this case we have week you are our
packets in the in the transmitter
bye-bye output port / out by output port
destination we send requests to the the
top of racks which when we want to send
switch arbitration is carried out and
then the grant or permission to send is
it sent back to the server because this
is route this relatively slow process
that we want we want to avoid we also
found in our work we've characterized a
lot of these these control plane
elements that we that they're required
for this we've done a six in thesis on
on these these components like the
arbiter or allocator that the
controller's you need on the server side
and we found that a really large part of
the power is actually in these in these
five phones in the buffering that we
need here in fact where you can see that
from the from the lists or power is the
power chart at the bottom left here so
so this shows at loads from one percent
to sixty percent the sort distribution
of power that's consumed on the process
that you
so we're not looking at total network
power now we're looking at specifically
at the power consumed on the on the
processor chip itself and the the
adapter the sort of turquoise bar there
is it significant and also it can't be
power gated so it's the same whatever
the network load we can't do a great
deal about the third ease because if
we're going to operate at high bit rates
you know to use the advantages of
optical communication that's that's kind
of inevitable source of power
consumption although it is actually you
can gate that so so that that will be an
energy of energy proportional source
another thing which isn't energy
proportion is the actual optical power
so as we as we pump optical power into
the chip that power is going to be
absorbed on the chip and contribute to
the thermal problems and that also is a
significant proportion and then we have
the transmitters very very small and
also able to power gate them and the
receivers which shown here not power
gated that there are ways of power
gating the receivers it tends to involve
increased latency though so we've
characterized these components using a
six synthesis we're trying to get these
these control planes up and running on
fpgas at the moment and yeah so we
should be able to do a lot better than
the virtual channel switch so the scheme
that we came up with we published at ofc
this year basically has a very simple
server-side interface and contains
electronic buffers in the in the in the
top of rack switch so we're going to use
electronic buffering because it's much
more much more dense than than any sub
optical memory that's that exists we're
also going to use wavelength stripe
as I described before in order to
maximize the bandwidth and give us low
serialization latency so the scheme
works like this we have this simplified
what we call sendin forget interface so
as soon as data comes into the FIFO we
we send that straight away we don't well
sorry we send a request to the allocator
it's like like a speculative scheme and
then we send the data we don't wait for
until we know that there's a there's a
switch path available so we send out the
whole package on this wavelength stripe
format slightly behind the the request
to the on the control plane yeah so it's
speculative transmission is attempted of
course if the if there's a switch path
available this the data goes straight
through to its its intended receiver if
it's if it fails the arbitration process
it's routed to these electronic buffers
at the at the top of rack switch so this
avoids the complexity of buffering in
the in the transmitter and pushes that
complexity into the into the top of rack
switch and another advantage of this is
when we when we retransmit those failed
speculative packets they are they are
stored close to the switch we also give
priority to the to the buffered packets
over new packets in the allocator which
maintains the ordering of packets if
that's important you know in the
application also we found when we were
characterizing these control planes if
you need reordering buffers that that is
a very significant power source far more
than the transmitter buffering because
they have to be in source special access
arrangements of the memories
so just to summarize the latency here
you can see the baseline vc switch on
the on the left here with its of control
delays due to the request allocation and
grant process so i put allocation on
sometimes here when I when I say
allocation I'm it's the same as
arbitration so it's so I've used the two
terms interchangeably once the the grant
is sent then data can be sent through
the switch so in the sender forget
interface we send the request shortly
followed by the data if that's
successful we get much lower lower
latency if it's unsuccessful we have to
go through now they are the arbitration
process again and the data is Center to
at a later time so that's one side of it
that's the low latency side of the of
the scheme but the other side is
reducing the well the sender forget also
interface also gives us lower power
consumption because of the simplified
interface on the server chip but also we
want to reduce the the optical power and
for that we need to look at the actual
optical switch itself so I've shown you
this before the three types of fast
optical switches we have in the SOA the
amplifier case we can overcome losses
using this by not just giving us a
transparent path but actually providing
amplification in those SOA elements but
it also produces noise as well
inevitably which reduce affects your
ability to to cascade the cascade the
switches both the mac sender and the
ring resonator switch introduce some
losses into the into the into the switch
path which if you try and cascade a
large number of elements
mean that you you have to end up having
to transmit a much larger power so here
if we if we have an off-chip laser going
through the first server chip where the
transmitter is going from the switch
chip and then on to a receiver the
receiving chip you can see if we have
one of the passive switches the Mac
sender or the ring resonator say with 10
DB loss that's quite low actually for
the most switching you know some
large-scale switches we end up having to
transmit a much higher power than if we
use a switch with gain the semiconductor
optical amplifier switches and all that
additional power is going to be it's
going to be absorbed onto the chip and
contribute to the thermal thermal
problem but we needed a switching
architecture which which gave us yeah
gave us the low loss but also it gave us
good sort of crosstalk suppression and
was scalable to a large number reports
and so we've been working with the
people it's in the engineering
department at Cambridge they've been
they've developed this switch which
combines Mac sender elements with with
the semiconductor optical amplifier
switch and it works by by creating an
array of these four port switching
switching blocks showing the top there
in to us or multiple stage switching
fabric and they've demonstrated that
this can can scale to 128 ports and this
is with integrated switches so what
they've built at the moment is actually
an eight port switch but then they've
done experiments to sort cascade these
elements to show that it will it will
scale to 128 ports before noise degrades
the signal
so for specifically for for this
buffered switch architecture we we
looked at two two schemes one uses two
of the needs of n by n switches with
input and output switches it's more
flexible but uses more of the four port
switching blocks then the other case we
looked at not quite so flexible in the
way that it deals with the requests but
it's simpler in terms of its of a number
of optical umbrella and elements
involved and just using the looking at
the the speculative transmission scheme
that we use so this is without taking
into account any sort contention you can
see here we get much lower latency on
the using the speculative scheme our
proposed scheme that's the the rent of
the blue and the and the green lines
then with the VC the VC switch here and
again I've plotted this over over
several meters to show we could use a
scheme for it for the shared memory
communication as well where we'd be
working down its or tens of centimeters
or over the top of rack which your scale
to a few meters we take contention into
account we get some quite interesting
results here from our from out of my
simulations so we if we use a uniform
traffic as you guys you'd probably
expect that the VC switch does give you
a higher saturation load this is using a
single stage of arbitration at the
expense of extra latency you could you
could push the other throughput of the
VC switch even even higher but of course
you get higher latency at low loads
because of because it's gotta wait for
the grant to come back but this
particularly in the in the case one
switch the more complex switch it's
getting pretty close to the throughput
of the of the VC switch with
with a single round of arbitration but
the interesting thing we found was when
we use different traffic patterns such
as streaming traffic between two ports
with random traffic on the other ports
or or in cast traffic which we were
particularly interested to look at
because this is a big issue for a in
data centers and a higher bandwidth
optical switch should be able to give
you advantages there in these two cases
the stream and the in cast actually the
the the speculative top of racks which
actually gives you better performance
across all all loads compared to the
virtual channel switch and this is due
to the effect of keeping your failed
speculative packets near to the near to
the switch itself so in terms of the
power consumption we we get a
considerable reduction in the power
consumption on the server chip itself by
using this combination of the simple
interface and the and the max under SOA
switch which is the case shown on the on
the right there you can see at thirty
percent load we've reduced the optical
power and the network interface power to
almost negligible levels really so the
bulk of the power is the is the receiver
and the sir DS which are yeah so
difficult to to power gate but difficult
to to eliminate from the scheme and you
can you can see in terms of if we
separate this into dynamic energy and
static power it's it's the it's the
static power the non power Gaeta bull
sources that we're really making a big
reduction in in terms of total network
power actually it's interesting that yes
in if we use a
ring resonator switch or something like
that we we get very poor energy energy
proportionality very very similar power
across the whole across the whole range
of loads and this is because we we need
a high high optical power which is a in
order to overcome the losses of the
switch using a semiconductor optical
amplifier switch we actually get very
good natural energy proportionality
because the SOA is one of the highest
power components and in the in the
system and when we when we don't need
the switch path on we get very extremely
low power that negligible power really
so you can see there is a small total
energy penalty for using this this
buffered the recirculation top-of-rack
switch particularly at I high loads but
we get yeah there's a small penalty at
high loads for using the recirculation
switch is because more SOA elements need
to be switched on in total compared to
the the basic bc switch but we so it's a
question of where we actually dissipate
the power do we dissipate the power on
the processor chip in the vc case which
leads to it gives you this this power
density problem or do we dissipate it
you know elsewhere in the top of rack
switch which is it was one of the aims
of the recirculation case forgot to use
those okay so I've produced I've sort of
presented to two sets of work here one
is one was on the the shared memory case
looking at eliminating or reducing the
need for arbitration by using prediction
and then these this optical top of rack
switch which uses a speculative
technique to reduce
reduced latency and this this magazine
der SOA switch in order to reduce the
power consumption on the on the
processor chip itself and at that point
I'll well I'll stop thank you for your
attention okay not weekend thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>