<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prediction Engines | Coder Coacher - Coaching Coders</title><meta content="Prediction Engines - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prediction Engines</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EwrB3-_eEuI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
welcome to the session on prediction
engines it's my pleasure and honor to
host this session with three awesome
speakers Robin Hanson Nicholas Lambert
and San made oz and I'm hosting the
session on behalf of several Microsoft
researchers who work on this area of
prediction engines Miro sebastian david
Rothschild and Jen wortman Vaughan I
just want to set the stage and set some
context what do we mean by a prediction
engine okay let's break down this phrase
into its parts a prediction first of all
prediction at least for the for the
purposes of this session for the next
three talks by prediction I don't mean a
categorical statement Hillary Clinton
will win the next election that my god
talking head or pundit might may say it
what I mean is basically probability a
statement a statement of uncertainty
twenty-seven percent chance that Hillary
Clinton will win the next election a
something after all that reflects the
true state of the world right no one
knows for sure whether she will win or
not we all we know is there's there's
some chance but along with a statement
like that isn't is a little bit of
difficulty in evaluating it so any
statement like that on its own you can't
say whether it's right or wrong right so
even if you say there's a seventy
percent chance Romney will win in North
Dakota say in the in the GOP primary
that particular prediction may be wrong
but you really need to evaluate those
predictions in groups so among these 10
predictions that we made before Super
Tuesday for all the Super Tuesday States
for the GOP primary among these ten
states with a total total percent chance
of eight hundred seventy five percent
chance so we expected to get about nine
out of ten right and that that's about
the ratio that we that we achieved so
what about the engine part prediction
engine the engine is the technology side
of things that the reason why things are
different today than they were
10 20 years ago it's the computational
power the new online world is making
these things easier so the technology to
collect predictions from expert experts
from a crowd the technology to formulate
predictions from historical data using
machine learning using a statistical
modeling the technology that's
transforming the way we do polling so
for example doing a standard random
sample still quite expensive but you
know doing it a self-selected but by a
sample has become virtually free in the
world of Microsoft Facebook Google etc
like our one of the researchers David
Rothchild where thinks and works a lot
on this topic taking things like
prediction markets which I'll talk more
about in a minute and improving the way
that we can improving the way that we
run these things and like a lot of these
things the devil is in the details the
user interface matters a lot the again
technology is changing things we no
longer are restricted to calling people
and asking a single multiple choice
question we can make a graphical
interface we can make an interactive we
can make it entertaining make it
engaging and we can get a lot more
information from people with platforms
like like Xbox for example and we can
think about the incentives both the sort
of gaming incentives and also the
monetary incentives and once we have
these predictions we have to think about
how to combine them either through
averaging or through machine learning
through online learning we can think
about how to handle combinatorial
predictions or related predictions so
for example or prediction about if
someone predicts that Clinton will win
both Ohio and Florida that says
something about her probability to win
just Florida by itself and once we have
this sort of combinatorial explosion of
predictions we need good ways to browse
and search through predictions we need
ways to explain predictions to the
average average consumer and help
visualize those predictions and all of
this stuff is adapting as
we speak and we at Microsoft and among
the speakers are engaged constant
experimentation and tinkering and one of
the things we do a lot is think of a new
idea actually build that thing open it
up to you new users see what happens and
repeat so ideally the ideal prediction
engine would predict you know it would
be general purpose it could be used to
predict you know nearly anything even
things we don't know much about it would
be accurate it would be real time it
would not only have the capability to
predict nearly anything but be able to
do that affordably and kind of a sigh a
side goal is to get past the current
state of affairs which rewards people
who are the most persuasive people who
have the who are saying things with the
most conviction or people with the most
power or even people with the the most
computational power the fastest
computers get away from that and give
credit where credit is due to so the
extent that the engine is rewarding or
giving reputation and credit it should
be it should reward actual information
and not these other factors so one
prediction mechanism that's at least two
of the three speakers will talk about
and if not the if not ideal at least you
know it does have a lot of those
properties is a prediction market so
I'll define what that is so the speakers
don't have to so this is for example
take a take a random variable like will
Clinton win the next election turn it
into a financial instrument that pays
one dollar if that happens and nothing
otherwise so in 2016 after the election
that contract will be worth either
exactly one or exactly zero but today we
don't know how much that's worth it's
worth basically it's worth the expected
value or how much whatever you think the
probability of that event is and
different people will disagree on the
probability of that event some people
may buy some people may sell there could
be millions of dollars traded on that
security but eventually we'll reach some
sort of equilibrium price at which point
no one's willing to move the price
further and we can think of that as an
estimate a consensus estimate of the
problem
that event and you can trade that online
on places like betting exchanges like
betfair and it may it runs exactly like
a continuous double auction sort of like
it just like the stock market works
there's a bid Q there's an ask you and
it may look hard to interpret but it's
written in terms of European odds but
you can convert it back into
probabilities and what this is saying is
basically according to this market that
she has between 27 and 30 percent chance
of winning and that so this type of
technology can allow us to predict let
you know prick many things even things
we don't know much about like for
example Eurovision and it one place to
that's to go that has like an aggregator
is David Rothchild blog predict wise it
collects together lots of these
predictions from all around the web from
different markets from different sources
and puts them in a format that that's
easy to digest and this style of market
has a good track record of empirical
accuracy so this is just one example but
there's a lot of examples this is over
several decades predictions about
football games and it's the Vegas odds
it's it's the place where people are
putting real money down on the line
which is getting the the most accurate
predictions this type of technology is
also real time so here's an example
where we were writing about the
prediction markets during the the GOP
primary and as a result I was watching a
lot more debate Republican debates than
I then I recommend anyone to watch but
one of those debates if you remember
there's one debate in particular with
Rick Perry he already had a few week
debate performances but there's one
particular debate where he he couldn't
remember the third of the three
government agencies that he was going to
cut and he he just couldn't remember he
finally just said oops I can't remember
and almost within hours
probability of winning went from pretty
small to to basically negligible went
from eight down to 24 another example of
real time after the so right before
Super Tuesday the markets were giving
Romney at eighty percent chance to win
Ohio but actually all the polls were
favoring Santorum in Ohio so what was
happening well Ron Romney had just one
in Michigan the day before and the
market knows that all these states are
highly correlated so if Romney wins
Michigan he's likely to win Ohio and the
market can react almost immediately the
polls take a few days to catch up again
polls versus markets if you remember the
primary season if you were just watching
the polls every lots of different
candidates had their time ahead Rick
Perry Herman Cain Gingrich Santorum and
they kept swapping with Romney even at
one point believe it or not Donald Trump
was at the head of the Republican polls
so if you're just watching the polls and
this is the story the media wants to to
to highlight is that well Romney might
lose all these other people are ahead at
various points but actually if you're
looking at the market it's a much more
boring story throughout the primary
really Romney was favored almost the
whole time with the exception of at the
beginning with Rick Perry coming in and
then a couple days at the end of 2011
where Gingrich and Romney were close to
tide so you can get lots of predictions
like this on places like betfair and
other place is pretty amazing you can go
get predictions about all types of
political outcomes and other outcomes
but sort of by definition there's only a
polynomial number of those predictions
right because the way they all run is
every single prediction is it is its own
continuous double auction with its own
database its own order flow what we
tried to what we worked on building is
what you might call a combinatorial
prediction market or getting more
towards this predict almost anything
concept where you can take you take an
exponential explosion so if you
all 50 states as binary variables now
you can predict any boolean function of
those 50 bear variables like Romney will
win Iowa but Obama will still win the
election or the same party will win both
Michigan and Ohio or Obama who win
between six and eight states that begin
with the letter M or they'll be some you
know after the election there'll be some
path of blue that goes that connects all
the way from Canada to Mexico so you can
make all these kinds of predictions and
from that you can get interesting
correlations that the help answer
questions like how pivotal was Ohio
really everyone sang Ohio's pivotal
pivotal and everyone is saying all the
money's flowing into Ohio but we could
actually show that well actually it's
true if Obama wins any of Ohio Florida
or Virginia he had a very very high
likelihood of going on to win the
election whereas Romney really needed to
win all three or we we also had some
things which correlate like if the jobs
report in October we could correlate
between what the jobs report in October
looked like and Romney's chances so if
the jobs report is looks bad Romney
might win and actually moving forward
and maybe Robin will talk about this
we'd like to get beyond the horse race
of it like who will win and why and more
towards like what is the outcome of the
election so if Romney wins what will
happen to unemployment what will happen
to war casualties what will happen to
tax rates what will happen to health
care etc and that's presumably the
things we actually care about the not
not the actual individual in the White
House but the but the but the policies
that he or she puts forth so we built
that combinatorial prediction market in
theory it allows you to create billions
and billions of predictions or actually
in this case 18 quadrillion for you know
quote-unquote for free but in practice
there's a lot of questions about whether
like people can actually anything
interesting can come out of people
playing in such a vast vast space so it
was
fully working beta and field test that
we ran with just a few hundred users are
about 700 users and you can read about
those details in the recent paper that
we had in the ACM electronic commerce
conference and this is kind of what it
looked like you could make those kinds
of combinational predictions like the
same party will win both Ohio and
Pennsylvania and in and the amazing
thing to one to me is you know these few
hundred traders are doing something
quite interesting you know just for just
for fun not for real money they are able
to do all you put on a question like
jobs numbers do something intelligent
and so for example because of the
combinatorial language we don't just get
the point estimate of the number of jobs
created in September we can get an
entire distribution and you can see the
blue line is is how we initialized it
and our traders really did you know they
sort of narrowed the prediction and
moved it to the left and they ended up
peaking right around the actual number
so I will go on with the now like I said
I'm honored to host this session we have
some amazing speakers I'll introduce
them one at a time as we go and these
three speakers will cover if not a
representative sample of topics and
prediction engines a pretty wide
spectrum of different topics in
prediction engines including getting
information from an expert getting
information from crowds including
prediction markets including scoring
rules including experimentation
including combinatorial prediction
markets so I'm excited to go forward and
I'll introduce the first speaker the
first speaker and what we'll do is we'll
try to have each speaker about 15
minutes we'll have we'll try to have
time for questions after each speaker
and then we'll try to save some time at
the end for questions for all the
speakers so Sammy if you can come up the
first speaker is San made us he's a
fessor at Washington University in st.
Louis in the computer science department
he does work in machine learning and
computational social science and he won
an NSF Career Award and his degrees are
from MIT and Harvard so thank you Sam
may
ok
and okay okay great so thanks very much
for the introduction Dave it's it's an
honor for me to be here so I'm going to
be talking today so actually I'm not
quite yet at washu in st. Louis I will
be there in a month or two officially
I'm still technically a professor at
Virginia Tech and when we talk about
work i did before this at rpi as well
with several my students and colleagues
and I'm grateful that dave has set all
of this up so well for me and it's a
pleasure to be giving this talk in front
of many of the people whose work really
excited me about this area so just
continuing on with this idea of water
prediction markets good for I'd like to
talk about them for a little bit and
draw some analogies to financial markets
and how we think about information
flowing in these markets so I want to
talk about two different ends of the
spectrum one of these is the information
aggregation end of the spectrum and this
is really to my mind one of the big
benefits off prediction markets is
suppose you have suppose you're
interested in getting a quantitative
estimate of the answer to a question so
suppose you're interested in the
probability that the global average
temperature 10 years from now will be
more than point to Celsius higher than
it is today okay so you might have a lot
of different people who have you know
different opinions about this so you
might have people who are domain experts
you might have people who are policy
experts in this case you know you might
have al gore who has strong beliefs
about this issue you might have people
who are conspiracy theorists of a
certain kind or you might just have the
average person sitting at home who's
like I have no idea why are you asking
you this question right what you want to
do is you want to come up with a number
that you can meaningfully use as an
estimate of the probability that this
event will actually occur so one of the
things that prediction markets are
really good for is essentially
incentivizing people to put their money
where their mouth is so that people can
come in and affect the price and get it
to move to actually give you something
that you can use if you're going to be
making decisions based on an actual
probability that this event will occur
so this is the
sort of information aggregation end of
the spectrum and it's something that you
know financial markets for example are
known to be very good at in terms of
valuing you know what what is a company
worth right in the same way prediction
markets can be really good at figuring
out what is the probability that an
event will occur there's another side of
this which I consider sort of the
opposite end of the spectrum almost but
something else that prediction markets
are also very good at is this issue of
information dissemination so in this
case you know you have all of these
people they have all this evidence
that's available to them and they have
you know their good faith opinions based
on this evidence but it's possible that
they just don't have all the information
that they need in order to come up with
with a good estimate right so it could
be that all of these people have these
have their own opinions about what the
temperature is going to be but dr. evil
is sitting over there being like haha hi
have this evil plan to focus lasers on
the earth and heat it up right so what
happens now well it's possible that dr.
evil has a henchman who wants to profit
off of this information this henchman
knows that you know the earth is going
to get much much warmer because dr. evil
is going to focus lasers on the earth so
the prediction market incentivizes this
person dr. Evil's and dr. Evil's
henchmen to come in and basically trade
based on the information available to
him and drive the price towards sort of
a correct estimate so does the
information aggregation end of the
spectrum does the information
dissemination end of the spectrum
they're actually sort of you know along
the same scale these aren't completely
separate things but but these are two
things that I want you to keep in mind
as we go forward okay so Davis already
set this up pretty well but you know
there's plenty of evidence now that
prediction markets do a really good job
at several of these information
aggregation types of tasks so you know
the classic example that they're well
known to do well at is in election
markets so the Iowa electronic markets
which operate in the US and in trade
which is now unfortunately shut down
betfair which they have talked about
have all done a very good job of
predicting a big election outcomes the
the Microsoft Project on the
combinatorial market for the twenty
twelve elections did an impressive job
of producing various forecasts about
different types of combinatorial
outcomes in that space we know that
prediction markets have been used and
done a relatively good job at sales
forecasts and so on in the corporate
setting and then they've been lab
experiments that actually try to
demonstrate the information aggregation
properties some of which Robin has been
involved in and you know it's it's
pretty clear that you know most of our
intuitions from financial markets carry
over pretty well in this information
aggregation setting there's been
somewhat less work at this dissemination
end of the spectrum as I like to think
of it but there are a couple of examples
where it's worked pretty well so I think
one of the famous examples was that
Microsoft was running internal
prediction markets and the moment that
these markets went live it became clear
that there was a product that was
scheduled to be released that many
people thought would be released on time
there was actually not going to be
released on time so that was an example
where the existence of the prediction
market incentivize people who had the
information to come and reveal it and
therefore better information became
public than would have been public
otherwise another example that I thought
was really neat was the gates hillman
prediction market which was run by a
Bothan and thomas and home at CMU where
they were basically running a market on
when will the new computer science
building open and and they and there
were various interesting findings from
that including things like there was one
person who was a really active
participant in this market who became
good friends with the building
supervisor over there and had the
building supervisor cell phone number
and used to get you know phone calls
from the building supervisor whenever
there was any big information happening
over there and then trade based on that
information in the market another
example that I'm going to talk about is
some recent work that we've been doing
that will be presented at triple-a I
actually tomorrow on instructor rating
markets where we've been running
prediction markets in which we get
students who are taking classes to
actually predict what evaluation will
their instructor receive as a way of
both providing dynamic feedback to
instructors on the progress of their
classes and what's going well and what's
not going well
well as to actually perform a field
experiment in markets where manipulation
is possible because people are actually
affecting the very outcomes that they're
trading on because people who are
trading are also the people who are
raiding their professors so what I want
to talk about briefly is today in the
next 10 minutes or so is two different
big research questions that I see as you
know very important questions for the
future of this field the first of these
is what's the right way to design these
markets so I'll talk mostly about
markets that are either binary that is
0-1 or continuous outcome markets where
as Dave mentioned you know that the
standard approach is to use continuous
double auctions but there are many other
options for how to design these markets
and I'm going to talk a little bit about
the option of including a market maker
who is a liquidity provider to
incentivize people to come and trade in
these markets and talk about sort of you
know what the possibilities are and I'll
talk a little bit about comparing two
different market makers the logarithmic
markets scoring rule which is due to
Robin and has been greatly popularized
in the AI community by the work of
diphenyl can dealing Chen and others but
so I'm going to talk a little bit about
sort of what are the different
trade-offs that you make when you
structure these markets in different
ways there's also i should point out a
whole host of interesting questions that
i won't touch on which is what's the
right way to design combinatorial or
interval markets and I think you know
others may speak about this as well but
I think that one of the really cool
areas in which computer science can be
very helpful is in thinking about the
interface between the front end in the
back end that is what's the user
experience like what is the set of
levers available to them to actually
convey their information to the market
and how does that interact with the
pricing engine at the back end the
second big set of questions I think for
prediction markets research is the
question of whether manipulation is a
problem or not so there have been lab
experiments that suggest that you know
price manipulation as such need not be a
problem but you know there was this
somewhat famous failure of the policy
analysis markets which somewhat
unfortunately got branded
betting on terrorism in the public media
but let's let's pause it for a moment
that people don't want you know markets
in which you can go out there and you
can bet on okay will there be a
terrorist event in Seattle or in New
York City you know during the next week
or so let's just you know assume that
that's not possible that's not something
that the public will accept there may be
moral issues and so on the same time
it's very clear that election markets
for example have been very successful
and there's no issue with that right but
these things fall along a spectrum right
there somewhere in between there where
there are options of sort of how
manipulable these markets are how much
insider information there is in these
markets and one thing that I think is
going to be really important to
determine in the future is where does
the line lie what are the kinds of
markets that we can successfully run and
what are the limits of manipulable Atene
these kinds of situations and when
should be concerned about it and when
should we not be concerned about it so
let me just dive into a couple of things
so so the first question that I'm going
to talk about is market making so the
standard design for most prediction
markets is to run these continuous
double auctions which have typically
been used in financial markets as well
the problem with that is that these
markets can often end up being thin
markets that is not a lot of people
trading in them and the famous saying is
you know liquidity baguettes liquidity
when people come in and they trade in
markets they create liquidity which
encourages other people to come and
trade then therefore creating more
liquidity right if you don't have people
coming in and trading then there may be
people with information out there that
they want to share but they're not
willing to come in and trade in that
market because of the various risks that
they would have to hold in order to do
so so one common solution to this is to
use a market maker who is an agent who
is specifically designated to provide
liquidity in these markets and the
logarithmic market scoring rule has
emerged as pretty much the de facto
standard for market making in prediction
markets it's got a lot of nice
properties for example it's got abounded
it's got a lost bound that says that you
know it can't lose more than a certain
amount of money it's nicely extensible
to combinatorial markets and so on
however if we're actually going to
eventually
be running real money markets and you
know these things scale up then there
are some problems associated with this
as well which is that you know in
general if it's incorporating
information it will typically make
losses and it can be sensitive to a
particular parameter and choosing how to
set that parameter can be difficult
there have been some extensions of this
work but one thing that we've been
working on is to take a different
approach to market making and we've
designed a bayesian market maker that
learns from the information content of
trades the advantage of our market
makers that it's not necessarily
loss-making the disadvantage is that
potentially you know there's no loss
bound so it's potentially capable of
making a lot of loss however it can also
provide pretty intuitive market
properties that are the kinds of
properties that we want for example
having higher spreads during times of
uncertainty and lower spreads during
times of more stability in the market
the question becomes how do we actually
compare these market makers so there's
many different ways of doing this and
what I really advocate isn't all of the
above approach which is that I think
that you know we should take every
option available to us so this includes
doing things like lab experiments which
can be very well controlled and
therefore you know exactly what you're
getting but at the same time they can be
difficult to scale because they're quite
expensive both in terms of the time to
run them the availability of human
subjects and so on and so forth another
great option is to run field experiments
or to talk about experiences with
deployments as folks at Microsoft and
other places have been doing and as I
will hopefully talk up for a couple of
minutes about in terms of our instructor
rating markets which is to actually say
okay we're going to run these markets in
the wild maybe get a few hundred people
to participate these are typically play
money markets but they provide a
different kind of experience in which
people have more time to interact with
the market and to learn about it and
therefore we can learn new things and
the third kind of way of comparing them
is to really run experiments in which
you have a bunch of trading agents who
are participating or run tournaments so
I know that many economists get scared
when you start talking about agent-based
modeling but a couple of things to keep
in mind over here one is that you can
run a lot more tests in this manner and
while it's important to be careful about
how you interpret the results this can
be really great
for debugging because you can end up
seeing you know many different
properties of the market by varying the
compositions of different types of
traders in the population just to give
you a quick example so we've we've run
all of these kinds of experiments but so
we run some trading agent experiments
where where we gave agents access to
slowly improving information on the true
probability of an event through
basically repeated coin flips and we
allowed them to use different strategies
in this so we had traders that were just
trade based on these fundamentals we had
traders who were so-called learn
rational expectations or learning
traders who would use both their own
information and the market price and we
employed various agents that we're using
different trading strategies from the
technical trading literature for example
momentum traders and then we varied the
proportions of this in the population as
we tried to compare our market maker the
Bayesian market maker with the
logarithmic market scoring roll across a
broad range of settings and what we've
got over here so I'm just going to
briefly go through this the what we were
interested in comparing is what is the
average profit that the market maker
makes or the loss that it takes what's
the spread which is a measure of the
liquidity that the market maker is
providing to the market and what is the
root mean squared deviation that is a
measure of the accuracy of how good is
the price in comparing with the
fundamental value and in these
experiments we can see that across a
pretty broad range of settings for
approximately the same amount of
liquidity being provided the Bayesian
market maker has the advantage over LMS
are of potentially being able to provide
a more price accuracy without needing as
much subsidy in order to run the market
the caveat to this of course is that we
don't have a lost bound so it's possible
in catastrophic cases for the Bayesian
market maker to actually lose more money
than then the LMS ours lost bound would
allow it to but basically the point over
here is that you can run a lot of
different specifications you can learn a
lot about the behaviors of these kinds
of market structures by using trading
agent experiments as one off a set of
options including lab experiments and
field experiments with deployment now
I'm very quickly going to go through my
instructor rating markets experiment
where analogous to
action markets what we wanted to do in
this case was run markets where
instructors were teaching the class
could receive dynamic feedback on the
progress of their class by allowing
students who were taking that class to
actually trade on what evaluations the
instructors would receive so what we did
was we ran markets for 10 different
classes at rpi each course had a
security that liquidates between 0 and
100 with all orders going through a
market making algorithm so now students
at rpi could trade in any of these
markets but there were only allowed to
rate instructors for the classes that
they were taking we ran 52 week rating
periods and in each rating period
everyone's account started with an
initial allotment of fake money or
shares at the end of the two weeks
students in the class rated their
instructor and each market would
liquidate based on the average rating
received by the instructor which is a
number between 0 and 100 and we gave
people prizes that were essentially
lottery based but based on sort of how
much money they had one in the
prediction markets and the prices were
like ipod nanos and Apple TVs so the big
question that we were thinking about is
are these markets going to be
manipulated well the short answer is we
found very little evidence of any
manipulation in this case so for one is
is there price manipulation and
basically the answer is no prices
incorporate new information and we found
in all our tests that the market prices
were reflective of the actual ratings
that that the instructors would receive
at the end of that two-week period who
was providing this information well we
ran a test to try and figure out if it
was students were in the class that they
were taking who were providing the new
information or not I won't go into
details of the test but basically we
just looked at trades that occurred
between the last liquidation price for
that instructor and the next liquidation
price and tried to see if the trade
moved the price in the correct direction
or not and we found that people who were
in the class would trade more often than
not in the correct direction therefore
moving the price towards the future
liquidation and people who were not in
the class would trade towards the past
liquidation value so with students who
are in the class for providing the
information the final question becomes
okay but what if the whole thing is a
big fake right this is a fake world
we're creating everyone could just agree
hey we're going to rate the instructor
high and bet all our money on it and
then we're all going to make a lot of
money turns out that this did not happen
because the correlation between the
ratings that instructors received in
this case we're very high with the
official Institute evaluations which
were completely unrelated to our markets
that we did at the end of the semester
so just in closing the takeaways they
want to take that I want you to go home
with from this talk are there's a ton of
exciting applications of prediction
markets ranging from things that we care
about is professors like instructor
ratings to important policy questions
exciting questions about combinatorial
outcomes but in order to actually get
this to work there's some really
interesting research questions to dive
into we have to design these markets
right to get them to work the devil is
in the details so there's questions
about should we set these markets up
with a market maker interesting
questions at the interface of market
design and user interface design and we
also need to think very carefully about
manipulation we shouldn't throw the baby
out with the bathwater just because
manipulation is possible doesn't mean
that the market won't be useful and that
it won't convey to us information that
we can use to make better decisions in
the future no thank you for your
attention
and maybe one question and then and
Robin can can come up one or two quick
quick questions any questions make
English
so so the question was have we looked at
at gamification and essentially using
game theory in integrated with these
markets in some ways so yeah I mean
we've thought about a lot of these
issues as it goes along so one issue
that often arises especially in the
context of the instructor rating markets
is how did the particular incentives
that we set up interact with the
outcomes that we see and we're using
these rank based incentives it's kind of
like the NBA Draft Lottery that is you
know how much you get affects the
probability that you're going to win one
of the prizes that obviously can lead to
different behaviors than just running a
market where you get proportional
outcomes we thought it made sense for
this case and I don't think it affected
our results very much but I think that
that's something that's very important
to think about any time that you're
designing one of these markets that is
how does the particular structure of
incentives you've set up interact with
the behavior that people are exhibiting
in that market thank you
uh so the next speaker is Robin Hanson
he's a economist and professor at George
Mason University in many ways he's the
sort of father of prediction markets
that were pioneers of prediction markets
he's a principal architect of the sum of
the first internal internal corporate
prediction markets and some of the first
web markets and also has done a lot of
innovations on combinatorial and
conditional prediction markets and also
manipulation in prediction markets so
thanks Robin thank you some of the work
I'm presenting is joint with these
colleagues of mine on the dagger project
which I'll tell you about in a moment
these are some screenshots from the
dagger project so on the left hand it's
it's not that pretty but what we could
do on the left hand side we have a long
list of topics in world affairs if you
pick one of those topics like number 359
it'll tell you about that this is a
question on back in 2012 whether Israel
would invade the Gaza Strip at the
moment at that time there was a
20-percent consensus in the system that
was the best estimate of the system of
the chance of that event happening and
you as a user of the system were tasked
with rousing these estimates and finding
one you thought was wrong and if you
think it's wrong changing it to
something you think is better so you
might pick this and edit it to be 16 and
then when you're and what it would do
then it would tell you that if this
event turns out to be true you will gain
so many points and if it turns out to be
false you lose points and in general you
are betting that the estimates you're
providing are an improvement in
information and so you end up with a
relative stake and in the upper right
hand corner you see your expected score
on average what your score will be in
your task is to try to increase your
score and to therefore contribute more
information to the system so you approve
the edit and after your edit the system
shows to everyone else the edit you just
made that's the current best estimate
that they don't like it they can change
it again so this is an edit based
interface a way to do prediction markets
where each person there's always a
consensus on everything you go in and
find the things you think is wrong and
you change them this system was special
and different because this is a
combinatorial prediction market as Dave
was talking about so in this case you
can take any
these other questions on the right hand
side you can pick one and you can assume
it so you can assume that at least one
Hamas rocket will blow up in the
Jerusalem City Limits between a certain
dates and you might think that event
would be predictive of the events in
question here on the right hand side and
so you could go make an assumption and
now if you edited that number on the
right hand side you're editing a
conditional estimate this is the
conditional probability of that event
given the assumption you make and in
this system we could make several
assumptions on the left hand side so the
interface we're trying to offer is you
can go in and edit a joint distribution
over the set of possibilities and at any
one time there's a consensus and you can
change it you have the incentive to
change the consensus toward the truth so
that you will an average make profits
that way so this is the dagger system
that we worked on together now the
dagger system was on Foreign Affair
events it's over now it ran for almost
two years we had roughly a hundred
claims at a time that were up there that
you could edit and we had thousands of
users but typically only 300 per month
actually doing something and we paid our
users we've had a pun fund of money we
paid them for their participation we
wanted to be able to pay them for
performance that is the ones who did
better get paid more we were not allowed
to do that because apparently because
we're developing something to be using
the government later they never pay
people for performance there later so we
can't pay people for performance here
that was the argument so we are now
moving from to a new system that we're
beginning and we haven't yet fielded but
to go live in a few months and this is
going to be called cycas and this will
be on science and technology questions
so we want to solicit people's
participation to help us develop good
questions about the progress in science
and technology and we are tasked with
increasing our are our numbers by a
factor of 10 in terms of having ten
times as many claims live at any one
time ten times as many users active
doing things and no we can't pay people
anything now because apparently this
gamification meme has gotten around and
there's no need every to pay anybody for
doing useful things because you can just
make things fun and then they'll do all
your things for free
Thanks so I'm going to tell you now
about some of the technology behind
dagger and some of the then some of the
technology we're going to develop force
I cast there's lots of wonderful things
you can do with prediction markets I'm
in general very excited about the
general potential although it gets
frustrating because it's hard to get
people to actually use them but here's
at least a place where we're doing it
but this is a computer science audience
so here I'm going to tell you about
computer science like tech so some
people already discussed the logarithmic
marking scoring rule so the idea is that
ordinary markets like the stock market
have a performance problem when you have
lots of people trading on a small number
of claims they work great but if you try
to get lots of estimates out of a few
people they work lousy because you end
up with this thin market problem there's
an old tech called the scoring world
proper scoring rule men you may have
heard of its a way to just ask a person
lots of questions and you can do that
and it works fine but then when you have
lots of people answering the same
question there's always been a problem
how to combine that this market scoring
will is something that does a nice
interpolation it becomes the scoring
rule or an ordinary market depending on
the circumstances and it is basically a
proper scoring rule that's shared so you
can think of a line of people using a
proper scoring rule and the rule of that
each person gets to use the proper
scoring will if they just pay off the
last guy in line so they get the
difference that they have the same
standard incentives but that difference
means that it ends up being equivalent
to a market maker in a market a shared
property scoring role is equivalent to a
market maker and there's a direct
relationship there and it turns out also
that there's a nice uniqueness property
about proper about this sort of
mechanism in terms of combinatorics if
you make a conditional edit the
probability of a given B that edit
doesn't change the probability of B it
only changes the conditional probability
only if you have this particular
logarithmic market scoring will form the
general task though as a computer
scientist here is there's going to be
this probability distribution somebody
you have to let people edit the
probability distribution you have to be
to represent and store that but in
addition you have to have a data
structure that represents their assets
what how much they will get or lose in
any particular state there's a simple
formula that represents the relationship
between the Edit they make
and their assets and it's very nice and
elegant thing to have also you tend to
want to have a produce an expected
scorer averaging over all the states
what's their average performance you
know sometimes want to do that
conditional as I'll say the problem is
the generic way to do this if we've got
way too many states is just not feasible
so how do we handle a combinatorial
space of assets and states so I want to
frame this as a set of problems and
solutions the generic idea is that
simple prediction markets work fine but
they only give you a small number of
topics what we want to be able to do is
elaborate this full combinatorial space
but how do we represent that how do we
have a limited representation of the
combinatorial space there's a well-known
technology of Bayesian or Markov
networks we're going to apply there's
very standard ways to create bayesian
Markov networks with limited
representation to express a joint
distribution over a large space but a
problem is those algorithms and methods
were developed for probabilities and
they didn't even think about assets how
to store and represent a combinatorial
representation of assets and how to
update that in the response to these
sorts of changes so we're going to find
a way to use a variation on bayesian
networks to also store and update the
assets so that we can have a full a full
combinatorial prediction market based on
bayesian networks there are many ways to
do comm unit or exile ike david's
approach in many ways the emphasis here
is we stayed for a while at least on
bayesian networks that we can compute
exactly so there are never any errors
other than roundoff errors and so we
never have to worry about being a money
pump where users can anticipate our
errors and then take us to the cleaners
because we aren't we're making mistakes
and calculating the prices here we never
have to make mistakes and calculating
the prices so within that realm how far
could we get so here's the problem of
what we're trying to do and the abstract
users browse a set of estimates they
make assumptions if they like and they
see various conditional estimates and at
some point they find a particular one
they want to focus on and so at that
point the system has to be able to
compute conditional estimates so that's
the first item there the probability of
a target variable that they're focused
on conditional on the assumptions
they've made and we have to be able to
compute that next they're thinking about
editing this number and one it's good to
give them some context of their current
asset
in relation to the thing they're
thinking of editing ie are they on net
long or short do they at the moment
stand to gain or lose depending on how
this resolves that will inform their
choice if they're already really long
maybe they don't mind this much going
short because they're backing off on
previous trades they want to go long
more they're sort of going moral in here
maybe that's more risky in addition what
we'd like to be able to do is tell you
this is the current number given your
current resources this is as far as you
could move it we could give you the
limits or we could have you try
something and tell you sorry no that
doesn't work so the user then commits
they say I make this at it in the range
that we have to update our
representation of the probability
distribution to reflect that we have to
update a representation of their assets
to reflect that so that we're ready to
start over again and periodically users
like to see how they're doing in terms
of a expected score of their overall
portfolio and so we want to be able to
calculate that as well so these are our
generic combinatorial edit based system
meets this is what we need to compute
want to highlight one issue which is one
simple way of doing this is with respect
to each edit it's easy to figure out
what a pile of assets looks like that
that edit creates and you could just
typically create take cash make little
piles of assets and leave them with a
big pile of stuff the problem is they
quickly run out of cash and then they
don't we don't necessarily know how to
take this pile of things they bought and
use it to support new traits what we
want is an integrated data structure
that makes it easy to calculate for any
given new trade how to use all the
things they bought before in order to
support the new trait so this is an
illustration here you could have made a
bet on be given a one be given a 2 etc
and on the right hand side we show you
that you would could have taken X
dollars of cash and created the
following assets with that and if you
just had a simple system that did it
that way you'd end up with this pile of
stuff on the right but you wouldn't
necessarily know how to take the pile of
stuff on the right and use it to support
new traits particularly might not notice
that the trades on the right hand side
the far right hand column that just adds
up the cash and a smart system would
know that you had lots of cash to
support new trains in a dumb system
wouldn't realize that so that's part of
the computational challenge here is to
have a system that knows how to figure
out given the trades you made before
what that represents in order to support
new trades so as I said we're using
bayesian
of networks which should be familiar to
lots of you a key working assumption of
bayesian mark of networks is that you
have a product representation of the
probability distribution the probability
of any one state is a product over
what's called clicks and separators
which are these sets clumps of variables
and it turns out with the function that
the logarithm of markets scoring will we
can make a simple transformation of the
assets a logarithm logarithmic
transformations exponential ones so that
the assets also satisfy product rule so
that means we can use the same
mechanisms that that use the product
rule to represent an update
probabilities to represent an update the
assets and that's our key trick so that
we can then use exact updating of
probabilities exact calculation the
minimum probabilities we translate that
to exact calculation of assets and
minimum assets to support trades and
also a simple variation allows us to do
expected assets that also tends up to be
as we do a sum over each of these clicks
and we get accept expected assets so we
did a test before we went live showing
some scaling of these variables and in
fact as usual with a Bayesian network
things are exponential in the tree with
but otherwise they're nice and linear in
the number of variables that we have in
the system so in this test we went up to
a thousand variables in our running
system we typically had a hundred and we
typically had say a tree with the five
or so so it was actually quite feasible
so basically I've showed you that with
these this basic set of approaches we
were able to achieve all these
computational purposes and tada but of
course it's Tara with respect to a set
of constraining assumptions and now
we're asked to go beyond that so again
we're asked to have a lot more claims so
that's more challenging computationally
we're asked to allow a much bigger tree
with not to constrain it to be a tree
ouch we decided that because it's really
hard to actually do the expected value
calculation will back off and show
people min asset calculations for their
longer in chart which is often what they
mainly care about and that's a lot
easier to compute q but another
constraining assumption we had here is
you typically you're only able to make
local edits in the tree that is
all the variables that you were assuming
and having as your target had all be
within the same click because that
didn't violate the conditional
independence assumptions of the network
structure itself so if you want to allow
people to pick any variable anywhere and
then edit any other variable anywhere
else then the question is are they
making a direct connection are they
trying to change the network structure
or are they accepting the rest of the
network structure and trying to make an
indirect set of things within the
network to make that connection we're
also going to try to be more expressive
about the kinds of values we can do so
well because we're challenged to come up
with hundreds of variable thousands of
variables I had this clever idea I say
okay let's say we're betting on who's
going to win the nobel prize in physics
we can think of that as a big
abstraction category hierarchy with all
these leaves maybe thousands or tens of
thousands of leaves about where the
topic will end up being a winner and
isn't that like tens of thousands of
questions so therefore we plan to have
abstraction hierarchies like this that
we can import and bet on major
innovation major events the end of the
year best innovation of the year Nobel
Prize things like that and be able to
let people that on where in the
hierarchy this will end up so they can
start say at the top and just say in
which category they think it is or they
can dive down if they have stronger
opinions and edit lower levels of the
tree and talk about this particular
research project versus that one so
that's a way in which we can have
hundreds of variable thousands of
variables and ask questions in essence
with a relatively small structuring cost
another thing we plan to do is that with
this logarithmic barking scoring oral
mechanism it's act there's actually a
simple transformation that lets us
present to the user piecewise linear
asset functions that they can edit and
or piecewise exponential probability
distributions and so in fact we just you
know the behind the scenes there's just
a small number this is as if it were a
small number of discrete values but we
can show it to the user as these
piecewise linear functions so now you
can say geez I want to bet I don't want
to be so negative on 2030 maybe I'll
move that point part of the curve up
answers the line and we'll move to
another piecewise linear asset number or
you could pick one of those points in
the piecewise exponential curve and move
it up or down and you would be at
in essence editing that and we know
exactly how to do that in order to
support it and it's not very hard
computationally so of course one of the
problems is betting on whether something
will happen in 2032 2032 activity for
our users but the what will happen in
the Nobel Prize or the New York Times
best of the year is a lot more engaging
people because it'll happen sooner so
will in the market try to have both
kinds of things long-term things that
are more fundamental interest and
shorter term things for activity and
interest and to meet our number of
questions mark I think that's what I'll
end with now and open it for questions
if your hand is up I can't see it
questions for Robin that's right hi Ravi
ponyo Microsoft Research inside a
question to what extent did you get a
sense of how people understood the
implications of making conditional
probability assumptions like how can how
consistent was that or were they because
there are a lot of known cognitive
biases around that kind of thing we
didn't get very much conditional editing
so the data on that as far as we had
some live experiments where there was
more of that in general users of these
systems can be best modeled as
relatively noisy they it's the sum total
of a lot of their actions together that
produces something reliable and
impressive but any you shouldn't trust
anyone at it too much so I most people
self-selected out they they didn't make
conditional edits so that gives me more
confidence than those who did they
actually you know chose to do it but and
you know the conditional adits did in
fact add information on average so the
markets were more informed because of
the conditional edits
of arguing all my business we manually
created the network structure and we
weren't too happy with having to do that
but at least we could allow people to
make combinatorial edits we would like
in the next system to allow users to
make more suggestions about the network
structure but we have a very basic
trade-off in that we need to have a
structure because that's the only way we
can manage our computation and it has to
be limited and so we can't allow
everybody to edit everything so we have
to have some caution or reluctance and
in general if we add some kinds of
complexity somewhere we'll have to take
it away somewhere else and so we have to
decide what our rule for taking
complexity away in order to keep a bound
on that so we have some elegant
theoretical ideas which would work great
if people were all abstract theorists
but we have much more concerns about how
to do this with real people and their
typical noisiness so we expect we will
have some combination of letting them
make an edit and then saying you know
that doesn't represent our current
network structure do you really mean to
wanting suggest changing the network
structure or you were you just trying to
say that the net effect of everything is
this number and try to try to push
people the status to be more forceful
TIFF saying no I really mean your
structure is wrong any more questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>