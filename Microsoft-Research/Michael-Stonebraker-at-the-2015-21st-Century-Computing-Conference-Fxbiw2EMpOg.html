<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Michael Stonebraker at the 2015 21st Century Computing Conference | Coder Coacher - Coaching Coders</title><meta content="Michael Stonebraker at the 2015 21st Century Computing Conference - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Michael Stonebraker at the 2015 21st Century Computing Conference</b></h2><h5 class="post__date">2015-11-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Fxbiw2EMpOg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome professor michael Stone greater
I always knew that Microsoft was a
powerful company but until this morning
I didn't realize just how powerful they
were they ordered up a view of the
mountains air quality that is fabulous
and blue sky so I'm now a Microsoft
believer now when they asked me to give
this talk they said you have to wear a
suit and I said come on no one in our
field ever wears a suit and they said
Chinese tradition where a suit so I
immediately sent email to Leslie Lamport
who will be speaking next and I said why
don't we have a revolt both of us won't
wear a suit and Leslie said
unfortunately i work for microsoft i'm
wearing a suit and so today I'm going to
talk about Big Data now some marketing
person somewhere invented this term a
few years ago and I'm delighted to
report that I now know what I've been
doing for 40 years I've been working on
Big Data forever and so I will talk
about what it means to me and what I
think is important so to think about Big
Data a lot of people say it means three
things that each begin with the letter V
that you've got a big data problem if
you've got too much of it so you got a
volume problem or you got a big data
problem if it's coming at you too fast
and your software
keep up or you've got a big data problem
if it's coming at you from too many
places and you've got a data integration
nightmare and the issues that you deal
with in these three different v's are
completely different so I'll talk about
each one of them and in the volume space
if you want to do what I call stupid
analytics if you just want to do sequel
analytics that's one class of problems
and the second one is if you want to do
the stuff Peter Lee was talking about
which is much more complicated analytics
so it'll be four different things we're
actually going to deal with the first
one is pretty simple if you want to do
sequel analytics there are I know of 20
or 30 production implementations on
hundreds of nodes with petabytes of data
and working day in day out so the data
warehouse products you know work well
several of them and so this is this
market is well addressed by commercial
software right now for example vertica
which is you know one of the warehouse
companies their largest their largest
user is a company called zinga zinga
built the game farmville which I can
gladly report to you that I have never
played so Zingo records every click of
every user everywhere in the world on
every one of their games in real time
and use it for business intelligence
purposes which is to figure out how to
get users all over the world to buy more
virtual goods anyway this is in my
opinion a solved problem which is you by
and large users are happy
even when you've got very large amounts
of data but the one thing I want to
remind you is there's been a dramatic
sea change in the last decade so around
10 years ago if you went around and you
talked to the companies selling data
warehouse products they were all selling
what are called row stores which is the
next thing in storage was the next
attribute in the same record they would
lay out data record by record by record
on the disk sequel server did this
everybody else in the data warehouse
market did the same thing that was the
year I started a company called vertica
and vertica looked at things totally
differently gotta rotate your thinking
90 degrees they stored data on disk
column by column by column so the next
thing in storage is not the next
attribute in the same record it's the
same attribute in the next record this
turns out to be wildly faster than the
roast or approach and vertica was
completely disruptive in this market it
was a factor of fifty or a hundred
faster than the roast or products so it
was disruptive it was done through a
startup I think that's the other perhaps
more common way of doing disrupt
disruption in the market is to do a
start-up and then threaten the major
vendors and so in the last ten years the
entire market has switched to column
stores so Microsoft PDW which is their
data warehouse product is a column store
ten years later so why is all this
happening you know columnstore is just
wildly faster than a row store for some
very good technical reasons that I don't
have time to go into the successful
vendors are all
to get there and essentially everyone
except Oracle has gotten to a multi-node
columnstore that is blindingly fast so
performance of data warehouse products
has increased by a factor of 50 in the
last decade because of this disruptive
change in the way to build data
warehouse systems however from my point
of view this is yesterday's news as
Peter was saying the things people want
to do now machine learning machine
translation data clustering predictive
models all of that stuff that's the next
big thing so this is the world of the
quants to use a Wall Street term I refer
to them as rocket scientists because
what they talk about is the stuff Peter
was talking about but the thing to a
database person is that if you burrow
down into their algorithms and take a
look at what they're actually doing most
of it is specified as linear algebra on
arrays it is not specified as sequel on
tables that has nothing to do with this
world and if you further burrow down
into the algorithms most of them end up
being iterations with an inner loop
that's a few linear algebra operation
such as matrix multiply singular value
decomposition that kind of stuff well
the convince you of this I wanted to go
through a very simple example and so
we'll use the stock market which chinese
people are you know wildly enthusiastic
about your market goes up and down so
let's just consider the closing price on
all trading days for the last five years
of two stocks say a and B think of them
as Huawei and Alibaba if you want so
if you're an electronic trading and what
you want to do is say are these two
stocks correlated are the two x series
correlated or not if they are then if
one of them goes up you want to buy the
other and so forth are you happy okay so
the simplest thing you could do is say
let's let's compute the covariance
between these two time series so unless
I copy down how to do it from my
statistics book wrong it's at the bottom
of the slide in red so that's what you
want to compute so that's not very hard
you can do it on your cell phone but now
let's suppose you want to do this for
all stocks on the new york stock
exchange there's about four thousand of
them so five years of data is about a
thousand trading days so you've got this
red matrix which is you've got a single
stock and you've got a thousand closing
prices and then you've got four thousand
stocks so what I want to do is do do
this covariance calculation for all
pairs of stocks okay what is that well
if you give me a little latitude and say
I can not worried about the constants
and subtract off the means this is that
red matrix-matrix multiply times its
transpose so that's the inner loop of
what people want to do so most of Peters
algorithms boil down to please do
computations like this at very large
scale you know so peter is an ideal
consumer of big data the minute you can
solve his problem he says I want more
training data more the minute you can
solve this for a quant on Wall Street he
says now I want to do it for every stock
on the planet not just for the ones
the new york stock exchange so this is
what you want to do and notice that this
is not in sequel and not even defined on
tables so in my opinion what's the job
description of a data scientist so the
data scientist has to clean and curate
his data that's the majority of his time
right now and I'll talk about that more
when we talk about the variety problem
so until he gets tired he does some data
management and then he does some of this
quant stuff and you simply iterate doing
this so that's what a data scientist
does and so what's going to happen is
that currently you have business
analysts running sequel on data
warehouses this is all going to move to
data scientists running this kind of
stuff on their data so how how are we
going to accommodate this sort of user
requirement so what you've got to do is
this sort of array analytics there's a
lot of array calculation going on now
you might say but some of my problems
are defined on graphs such as social
networking kinds of problems to me
graphs are just sparse arrays it's more
of the same kind of stuff and then
you've got to do data management
iterated with this sort of stuff and
make it scale because the minute you
make Peter happy he's going to define a
bigger problem and tell you to do that
one so make it scale too many cores many
nodes out of memory data you know make
it make it get really big so that's the
problem so how to do that well if you do
this right now chances are you're a fan
of some statistics package something
like our or SAS or MATLAB so you can say
well I love our well our has essentially
no day
management capabilities it's a file
based system it allows you to compute
lots of array analytics but it doesn't
scale and it doesn't really do data
management so yeah that solves some of
the problem well maybe we should just
run oracle oracle doesn't scale very
well either but put that one aside so
how do you do matrix multiply in a
relational database system well the
answer is not very well now you can
write and say it's a great exercise some
of you who are who think you're pretty
smart try writing matrix multiply in
sequel well it's doable it requires a
simulation of what of what arrays are
going to look like in terms of tables
but it's really slow and and it's really
difficult to do singular value
decomposition and sequel so it's equal
only is kind of problematic so you say
well most systems these days support
user-defined functions and so let's just
code the matrix operations as user
defined functions that's starting to
happen and so support for you
user-defined functions either written
and are or written in C++ is starting to
happen but it's awkward and challenging
so this is not such a great idea so what
people do right now is they tear their
hair out they the data sits in some
relational database system they run the
data management over there then they
copy the stuff they're interested in
over to our or MATLAB and run the
statistics over there this is a terrible
solution you got to learn two different
totally different systems and you got to
copy the world back and forth all that
does is give a lot of money to your
network vendor
and are still doesn't scale so the
research challenge is to do better this
is clearly this is what people do day in
day out right now this is the world of
data scientists out in the real world so
do better than this ok so my favorite
solution is to say this is an array
problem this is not a table problem why
don't we have an array database system
so think outside the box get rid of
tables and replace them by arrays so I
wrote a system a few years ago called
side EB it supports sequel on on arrays
has built-in analytics blindingly fast
on matrix multiply like everything else
these days it's open source Linux so
check out a possible disruptive solution
you can go to site eb org see what you
think one of the reasons why arrays are
such a great idea is that if you
simulate an array in terms of a table
well you have an array which is say has
dimensions I and J and then has a value
so if you want to have a if you want to
have a four by four matrix then it's got
the thing on the left is the table
simulation notice that the dimensions
are explicitly there and there's a lot
of data on the right is the array
representation notice that you can
compress off the dimensions completely
and lots of times you want to subset
this array pick out a geographic subset
so all it can be nicely co located on
the right but on the left it's a lot
more difficult to do that so it an array
database system has some inherent
advantages and my feeling is that
long-term this is the kind of technology
that's going to
win now I would be remiss if I didn't
tell you how horrible the dupe was so
let's look at Hadoop circa 2012 so
Hadoop was really a three-level stack
there was HDFS which is the file system
at the bottom there was a thing called
MapReduce in the middle which was
written by Google and an open-source
version was written by Yahoo and then
there were various high level systems at
the top hive pig mahout that kind of
stuff so this was the stack that was
being sold by the Hadoop vendors circa
three years ago and what at what they
found out was that when you tried to get
real world users to use this MapReduce
is not an interesting distributed
computing platform for lots of very good
technical reasons so it failed doing
distributed computing MapReduce is also
not an interesting data management
platform putting sequel on top of
MapReduce results in horrible
performance so MapReduce wasn't
interesting for data management wasn't
interesting in for distributed computing
basically MapReduce failed completely so
who abandoned MapReduce well Google who
wrote MapReduce they wrote it as a
application specific solution for their
web crawl database so they abandoned
MapReduce in about 2011 so the
purpose-built application for which
MapReduce was designed google jettison
that five years ago and they've got
Dremel f1 and a variety of other systems
for other stuff MapReduce is not on
their roadmap as something they're
interested in going forward
Cloudera has which is a major Hadoop
vendor has basically abandoned MapReduce
also so in Palo which is their new
sequel DBMS not built on MapReduce
Hortonworks and Facebook have similar
projects so what's the future of the
Hadoop stack well it's a file system at
the bottom but HDFS has horrible
performance problems and so they are
probably going to get fixed sequel is
going to be at the top Impala kinds of
systems you know the data warehouse
style database systems are all going to
run on top of HDFS so the data warehouse
market in the Hadoop market are going to
converge completely it's the same
problem do business analysis so the
latest marketing speak from the Hadoop
vendors is called data lakes stay tuned
we'll talk about that little bit but in
the the thing you should think about is
that the data warehouse market and the
Hadoop market are basically the same
market so now what about spark so spark
I think is really interesting spark came
into existence as a way to do
distributed computing way faster than
MapReduce now notice that nobody wants
MapReduce so the original design of
spark was to fix a problem that nobody
wanted solved so fairly quickly the
commercial company data bricks that
selling spark figured out that people
want sequel seventy percent of the
accesses to spark right now are in
sparks equal spark is a sequel market
unfortunately spark is not a sequel
engine at all it has no transactions no
persistence no index
is now this stuff is all going to have
to get fixed so presumably spark sequel
will start to look a lot like Impala a
lot lot like commercial data warehouse
implementations and they will presumably
be competing in the data warehouse
market against Impala and all the
current data warehouse vendors and may
the best system win thirty percent of
the spark market is distributed
computing mostly coming from Scala and
it's way more functional than MapReduce
however these things are dd's which is
the format of spark data is basically
that's not what anybody wants quickly
data bricks is trying to morph that to
something called data frames which is an
our kind of style of data structures so
there's going to be a lot of evolution
in the spark world hold on to your seat
belt this is the next shiny object but
what is going to look like next year who
knows so spark could be interesting
almost certainly it's going to compete
in the data warehouse market okay so
with that I think in the business
business intelligence stupid analytics
market there's a lot of high there's a
lot of systems that work very well and
presumably spark and Hadoop are going to
join them with yet more products but the
big action is how to do scalable
analytics in the middle of data
management and if you're going to work
on something that's a great open problem
to figure out how to solve that let's
quickly move to high velocity so the
thing that's really nice is that the
commercial market keeps changing the
rule
the Internet of Things is in the process
of sensor tagging everything of
significance whether whether it's a
gizmo that you wear on your wrist that's
going to keep your vital signs whether
it's marathon runners doing races we are
tagging lots of wild wild birds and
animals and they are all reporting their
state and and or other data in real time
this is creating a deluge we're all
using smartphones as a mobile platform
that sends velocity through the roof so
this stuff is breaking everybody's
infrastructure with a constant need
please go faster so there's two
different kinds of problems that people
have when they say go faster the first
one is what I call big pattern little
state so if you want to do electronic
trading mostly what those guys on Wall
Street want to do is safe look for a
pattern in this sea of trading data so
find me a strawberry followed within a
hundred milliseconds by a banana so this
is the world of complex event processing
CEP and there are a bunch of commercial
systems that pretty well address this
market so this is not a transactional
database market this is simply process
the firehose looking for patterns what
I'm much more interested in is the other
version of big velocity which is what I
call big state and little pattern so
there's an electronic trading company
which accounts for ten percent of the
volume on the New York Stock Exchange
right now they have electronic trading
desks
in New York in London and Tokyo probably
here in Beijing and so these electronic
trading desks are trading securities in
real time and so the CEO of this company
wants to assemble his worldwide global
position for or against you know any
stock on the planet so that if he has
too big a position that he can say my
risk is too high he can you know pull
the panic button and say I'll lower my
risk so alert me if my exposure is
greater than some amount now notice that
this is not looking for patterns in the
fire hose this is reliably recording the
effect of every trade everywhere in the
world and this this is assemble in a
database what my current global position
is now if you ever lose a message your
database is worthless so you can't lose
anything so never lose anything and
record my state reliably at very very
high speed so this looks like a very
high performance transaction processing
problem and this is I'm something I'm
very interested in so if you want to
solve this problem you can run any one
of the commercial relational database
systems that I affectionately call old
sequel so you can run microsoft sequel
server it's one of the old sequel
elephants you can run dos anos equal
system there are 75 or 100 vendors who
will sell you what they called a no
sequel system and they advocate giving
up both sequel and acid and then a third
class of solutions is what I call new
sequel which is to retain sequel retain
ass
said but get rid of the legacy
implementations from the old sequel
vendors in favor of something much light
weight and much faster so I'll talk
about each of these for a minute the
current elephants are slow they are
simply way slow if you want to run a
million transactions a second and you
know don't bother trying to do it on any
of the legacy implementations they their
sequel is fine but their implementation
of transactions is just way too slow I
on some others wrote a paper called
through the oltp looking glass in 2007
explains exactly why old sequel is never
going to work at high performance in
this sort of market so go check out the
paper for a lot more details so why not
go to go to a no sequel system now I
have two things to say about no sequel
the first one is if you advocate giving
up sequel then you're advocating using a
low level notation which is what sequel
gets compiled into never bet against the
compiler I have a lot of gray hair I can
remember when when the gravy the grey
haired people of my earlier days would
say you must code in IBM assembler
because there's no way to go fast unless
you can control all the registers on the
machine we all know today how ridiculous
that is you don't want to bet against
the compiler high level languages are
good low-level notations are bad 40
years of data based research has
convinced everybody of this and the no
sequel guys haven't quite figured it out
yet give up acid the traditional
implementation of transactions from the
legacy vendors in
is too slow absolutely true one option
is to get rid of them and I'm not a big
fan of this the reason is if you have to
move a hundred dollars from over here to
over there you've got to have
transactions to do that and there's some
very good examples in the Bitcoin world
of people who've lost a lot of money
because of attacks on a no sequel no
sequel system so if you need
transactions and you don't have them
then you open yourself up to be
vulnerable to some very bad errors and
or you have to tear your hair out in
user code so my prediction is that
sequel and no sequel systems will in
fact merge no sequel meant no sequel in
2012 so then the marketing's position
from the no sequel vendors moved to
saying well not only sequel we have an
alternative sequel you need sequel for
some things but we have a different
solution in my opinion these days no
sequel means not yet sequel because the
no sequel vendors have figured out that
high level languages are good and are
proceeding to write higher-level
languages on top of their systems Mongo
and Cassandra especially have
implemented high level languages that
look exactly like sequel so this world
is no sequel is moving to sequel and the
sequel guys are adding JSON to their
engines which is one of the big features
of the no sequel vendors the flexibility
of JSON data types so I think these two
markets will simply you know they will
move toward each other
and no sequel engines will no longer be
called no sequel they will simply be
implementations of sequel with different
characteristics than than other vendors
I'm a huge fan of new sequel which is to
say transactions are fundamentally
useful to almost everybody and the
problem is the legacy vendors have bad
implementations of this stuff which are
just too slow but that doesn't say you
can't do something a lot faster so vote
DB which is a commercialization of an
MIT system called H store is about a
hundred times faster than the legacy
guys on TPC see it does that with a very
lightweight transaction system careful
attention to threading it's a main
memory open source sequel engine
blindingly fast heckett on which is a
Microsoft database system has just come
out as part of sequel server 14 is
another implementation of new sequel so
it's possible to go fast just not not
with the legacy systems that are 30 or
40 years old at this point okay so big
velocity what it really is is either
it's a stream processing problem or it's
a high-performance transaction problem
and as a high-performance transaction
problem they're starting to be very high
speed implementations but I think the
key thing key takeaway is figuring out
how to make transactions go very very
fast is there are lots of ideas in this
space and lots of room for future
improvement okay I have nine minutes
remaining I will talk about variety
typical enterprise somebody like Federal
Express
has 5,000 operational data systems now
fedex has a data warehouse only a few of
the other of these systems get their
data gets loaded into the data warehouse
so the typical data warehouse is
assembling data from 10 or 20 systems
what about the other 40 980 a big
enterprise like Verizon
telecommunications company has 10,000
operational systems big enterprises have
lots of operational systems in large
part because they divide into
independent business units so that they
can get things done and independent
business units create independent silos
of data so there are lots of operational
systems inside big enterprises and then
of course this isn't to mention
spreadsheets and web pages access
databases and then everybody wants to
get weather data off of the web and data
from a variety of public data sources so
there's a need for scalability in
integrating a large number of
operational data systems so the
traditional wisdom on how to do this is
called the extract transform and load
market so ETL systems if you've heard of
datastage if you've heard of informatica
those are some of the major vendors who
do this so here's what you do you
construct a global schema in advance you
send your smartest wizard to figure out
what that global schema is going to be
once you have that then for every local
data source that you want to move into
your this global system you send a
programmer out to interview the owner of
the
data system figure out what's in the
data source map it to the global schema
and write a script to do this
transformation and figure out how to
clean and and remove the duplicates from
the data source and this works for maybe
20 data sources because it's it requires
a global schema up front which is very
hard to do for a larger number of data
sources and it requires a train
programmer to deal with each source
individually requires too much human
involvement now the current etl guys are
now selling something called master data
management MDM is simply the new
buzzword for the for how to do this
stuff now this is not going to scale has
no chance of ever scaling and you've got
all kinds of enterprises who are saying
I don't have 20 data sources that I want
to integrate I have 1000 or I have 3000
so for example groupon which is the
daily-deal company that does coupons
they are building a global database of
small businesses which are the people
they are selling to they're integrating
10,000 data sources to do that so they
want to do this problem at scale 10,000
and etl has no chance of ever getting
there so here's a big user need for
scalability which is not being addressed
by the traditional vendors okay so this
is a this is I think an absolutely key
area where new ideas need to come into
existence to be researched vetted and
figure out what ends up working so I'm
involved in a startup called data tamer
that I'll tell you about
slide in a moment and it may be a good
approach to doing this at scale their
collection of startups trifecta which is
a startup from Joey hellerstein at
Berkeley and a bunch of others who are
like this they are focused on the
individual data scientist his curation
problem and so it's very focused on
non-programmers on visualization and on
the needs of an individual data
scientist but anyway I think this is an
area where new ideas are really needed
so what does data tamer do so data
curation to me is you've got to ingest
the data from wherever it is you've then
got to transform it to some different
notation say euros to dollars or you
want to dollars or whatever you've got
to clean it fundamentally any data
source has between ten and twenty
percent of its data is just plain wrong
so minus ninety nine often means no you
know people misspell names people
misspell companies and so forth then
you've got to take multiple data sources
and manage to map the attributes
together so you have a data set of
employees I have a data set of employees
you call its salary I call it wages you
gotta line that stuff up and then if
there are duplicates you've got to do
entity consolidation so data curation is
all of this stuff and historically this
has been incredibly expensive this is
the 800-pound gorilla in the corner
which is before you can do any data
science you've got to do all of this
stuff to make the data line up and be
semantically correct
that is fundamentally expensive using
traditional techniques and the
traditional techniques just don't scale
so this is a huge problem so what is
what does data tamer do we take a wee
borrows you know a play from Peter Lee's
playbook the answer is use machine
learning and statistics to figure out
everything you can automatically so pick
all the low-hanging fruit automatically
and then when you have to ask a human
you've got to ask a domain expert and
you can't ask a programmer the ETL
programmer has no clue what you know
what the answers to these kind of
questions are so for example you know
tamer has a customer which is Novartis
which is a large drug company they're
using this for genetics data and so I
see e 50 and I see you 50 are to
genetics terms are they the same thing
or are they different in other words is
one of misspelling of the other or are
they different genetic terms programmer
has no clue you've got to ask a
geneticist a domain expert so tamer has
an expert crowdsourcing system so when a
human needs to answer a question say to
build up training data you've got to ask
a domain expert you cannot ask a
programmer so this has a significant
return on investment it's just way
cheaper than trying to use traditional
techniques so this is one possible way
to deal with to deal with the big
variety problem but my advice to you is
this is incredibly important if you want
to work on this problem how or go find a
real world user go to alibaba go to
huawei go go to some real world user and
figure out what his
data curation problem is and solve that
problem because in this world a lot of
people come up with algorithms and then
that say I wonder if these algorithms
actually work on real-world data you've
got to do it the other way around find
real world data and then fix that ok so
I'm almost out of time but I said I'd
return to data Lakes the current
thinking about from the Hadoop vendors
is put all of your data into a data lake
based on HDFS and life will be great so
dump all your raw data into HDFS which
is what a data lake really now notice
that this is just the ingest piece of
data curation all the rest of it is the
hard part and it's still what you've got
left to go so if you take the advice of
the Hadoop vendors and dump all of your
raw data in HDFS you don't create a data
lake you create a data swamp and
curation is what you then need to do to
create a lake that you can use so data
lakes are simply a marketing buzz word
for in the in just piece of data
curation so in summary if you want to
deal with big data if you want to do
sequel analytics get a column store if
you want to do smart analytics well the
jury is kind of out I'm a big fan of
array stores but there may well be other
solutions if you have a velocity problem
you need a streaming product or you need
a high-performance oltp system depending
on what kind of stuff you have no sequel
guys currently are have a great out of
box experience they're easy to use
they're going to move to sequel but you
a lot of that stuff in your
organization's right now legacy vendors
implementations aren't going to go away
they're completely obsolete they're
going to get replaced by all of this
other stuff but in the meantime you're
going to have to deal with it and then
you're going to have one or more
curation systems to try and solve the
scalability in the variety space so
you're your stack inside a company is
going to have a lot of stuff in it and
my single takeaway is use the right tool
for the right job thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>