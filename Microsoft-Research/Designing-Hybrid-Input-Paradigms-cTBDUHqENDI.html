<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Designing Hybrid Input Paradigms | Coder Coacher - Coaching Coders</title><meta content="Designing Hybrid Input Paradigms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Designing Hybrid Input Paradigms</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cTBDUHqENDI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
my name is Abby sullen I work in a group
called computer mediated living which is
a group that specializes in
human-computer interaction which is
essentially about understanding the
relationship between people and
technology and trying to invent new
experiences for people on the back of
that so sometimes we draw on
technological innovation sometimes it's
on we draw on user understanding and
other times we draw a design so I'm
hoping that through the course of this
talk I can demonstrate through the
research that I'll be describing how all
of those three disciplines come into
play so I'm going to be talking about
new ways of exploring computer input
paradigms and there's no doubt that new
kinds of ways of interacting with
computers can be revolutionary it can
revolutionize not just the way we
interact with devices but the kinds of
devices that we build and indeed the
kinds of tasks that we carry out in our
everyday lives yet outside of the
research world it's interesting that
it's really only in the past few years
at mass-market devices have made the big
league be on keyboard and mouse so with
the introduction of the iphone we now
started to see multi-touch come into a
lot of different devices and hot on the
heels of this we are seeing now the
transforming impact of in air gesture
and Microsoft's Kinect to gaming
technology was instrumental here and and
these changes in turn are creating a
kind of new enthusiasm for alternative
ways of interacting with the computers
so we see not just the impact of gesture
and multi-touch but also renewed
enthusiasm ow things like voice I gaze
and even brain computer interaction for
example much of this is down to the
maturing of the technology itself but
also the breakthroughs are undoubtedly
about design as well but the point here
I want to make is that I think we're
perched on the edge of many more radical
changes as a result of new input
paradigms which is one of the reasons
why I'm excited about this area of work
so in this talk I want to explore the
design problem really of
you open up the input space to new
possibilities and I'm going to start by
briefly describing our approach to this
and also discussing some of the
arguments and visions behind why we'd
want to do this in the first place and
then I'm going to consider how we
effectively design these systems by
describing two lines of research which
we've been attempting to do to do this
in the first line of research i'm going
to be talking about some work we've done
in surgical settings i'm trying to add
both gesture and speech to surgical
interaction and then i'm going to talk
about how we have been playing around
with adding gesture to desktop
interaction around a desktop pc and i'm
going to be describing these these
adventures that we've been embarked on
some of which have involved testing in
the wild which which is in itself a
challenging thing which I'll also talk
about and then I'm going to conclude
talking about some of the challenges and
the lessons that we've learned trying to
do this right so in the title i use the
word hybrid and I'd and I use that word
deliberately because it says something
about the approach that we're taking so
I'm using the word hybrid by analogy to
a hybrid car where you have a new form
of power sitting alongside the existing
engine within the context of a highly
evolved in complex system so my concern
here is to see whether we can add new
input capabilities which complement
would already exist and work alongside
ingrained and already successful
interaction techniques so that's what i
mean by hybrid and this is different
from a lot of the research that's gone
on around alternative input modalities
so a lot of that work looks to replace
one input channel with another and that
makes a lot of sense for example you're
talking about people with disabilities
so you might use voice to do text input
or you might use i gaze to take over the
cursor control and it also makes a lot
of sense if you're looking at certain
situations where for example your hands
are busy so if you're driving hands-free
input makes it makes a lot of sense but
we're asking some different questions
here actually we're asking whether we
can enrich new kinds of input without
without
create entirely new systems and we're
also asking whether we can add new
capabilities to existing systems without
getting rid of the good bits without
getting rid of the bits that are kind of
highly evolved and useful so why might
we want to take this approach well one
reason is clearly about offering people
choice right so you you when you offer
people up choice they can choose how
they interact depending on their
circumstances and this is recognizing
that different input modalities have
different affordances right so if i want
to write a paper i'm going to sit down
at my keyboard and do that because
keyboard is great for high input high
velocity and if i want to sit back on
the couch and look through the web i
might grab a touch tablet do that
instead it's much much more suited for
that kind of activity so we can think
much more deeply about how we offer up
choice and systems and this isn't just
about input obviously so if you look at
Microsoft Surface one of the things
that's happening there's is to try and
play around with the physical form
factor of that device to allow you have
detachable touchscreen and so on but
other arguments really hark back to much
older visions that have been around a
long time so one of the arguments has
been that you know we have these rich
capacities for action and yet we don't
you utilize the bandwidth of human
beings right so we force everybody to
interact through these little keyboard
and mouse input devices and we're not
using everything else so it's a kind of
a bandwidth argument and multi-touch is
starting to draw on that to let us use
two hands multiple fingers and so on and
another argument goes back actually this
time way back so I'm talking about early
80s when people like dick volt started
to build systems like put that there at
MIT and his argument was that you can
why don't we draw in the way that people
naturally interact with each other so
looking at interaction more as a kind of
dialogue I'll just play a little snippet
of it it's very dated it's kind of
interesting
I'm waiting for you have to go ahead
create for it oil tanker where they're
blue cruises where yo mama's make a
yellow sailboat where know that okay so
you see what's happening there that
they're trying to make make the dialogue
take advantage of the fact that people
point and speak a lot and so it's trying
to build much more intelligence into the
interface and the argument is that this
is something we do intuitively so why
don't we build systems that respond in
this way but there's some big issues
here which is how do you add new input
capabilities without adding complexity
and extra workload for the user that
turns out to be a really big challenge
so I'm not going to talk about two
programs of research to give you sort of
examples of how we're working through
these these issues and I'm going to talk
about first the surgery work right so
surgery is an interesting domain within
which to work because if you think about
why we might want additional forms of
input there are certain situations where
it really makes sense to try and replace
mouse and keyboard interaction with
other kinds of input so certain surgical
kinds of settings like interventional
radiology like vascular surgery
neurosurgery are more and more highly
dependent on images rich data to look
inside the body so that you don't have
to open the body right up so with more
minimally invasive surgery there's more
dependence on on data to see inside now
the problem is of course that if you
want to look click through those images
if you want to manipulate them rotate
them and so on this is usually done
through mouse and keyboard and surgeons
are scrubbed up they can't touch any of
that stuff so they have to ask somebody
to help them with
and that turns out to be very cumbersome
and very effortful and and so we're
really interested in how people in these
situations get around those those
problems so what you see and sometimes
people do things in a very unorthodox
way on the left hand side you see a
radiologist who's essentially what he's
done is stuck stuck his hand inside his
gown which is not sterile on the inside
but sterile on the outside and he's
using its mouse right the infection
control people really found on this by
the way but they all do it or more
commonly what you see like in vascular
surgery neurosurgery is that you they'll
ask an assistant to flip to the next
image no twist a little bit right go go
up go down no that's the wrong image go
to the next image that's sort of thing
it can be very cumbersome so so to see
how we might address this we spent we
spent a couple years actually following
a whole cycle of development that where
we aim to put the clinicians at the very
center of this design process to see how
we can design effective systems for them
and so the way we typically do our work
is we would start with some some field
work actually trying to figure out how
it is that the people are doing the work
what you know what are the complexities
of that work how do they do it in teams
and so on and to look for opportunities
where we might be able to put touchless
systems into these situations and it
turns out that as we were doing this
work in all these different settings it
was some the vascular surgeons who were
the keenest to get on and build
something and this was a team of
surgeons down and now guises at Thomas's
Hospital in London and so with them we
started to sit around the table talk
about what they might need then we built
a series of prototypes which we test it
first in the lab here brought the team
up here to look at then we took it down
to their operating peers we put them in
their operating theorists without the
patients there to try out how it worked
in that setting and lighting conditions
of things are all very different down
there and then we started to do use it
real real cases so it's being used in
several well lots of new cup
cases since last year and so let me just
I'll show you a video it's the best way
to describe in detail what we did
minimally invasive vascular surgery
surgeons are reliable yeah currently ago
they were scrubbed they don't wanna meet
or serve dinner to manipulate the data
for them maybe night for me to the
technology presents some real
opportunities to allow the touchless
interaction through close collaboration
with vascular surgeons that guys in st.
Thomas's hospitals we have developed
assistant for manipulating 3d images the
system supports the surgeons to pan zoom
rotate lock mark move fade the 3d
overlay to view the underlying 2d
fluoroscopic image
he can't play small
we have used voice control for the
execution of discrete commands and
gesture for the commands that lie upon a
continuum and lend themselves to a
physical manipulation metaphor seamless
transitions between gestural commands
allow the surgeon to have his attention
on the images during manipulation one of
the interaction design challenges we
faced was the need for a gesture set to
be performed at the bedside in close
proximity to other personnel therefore
gestures needed to be confined to the
area in front of the physicians torso as
the space above below or to the sides of
that area may not be sterile we achieve
this through a clutch mechanism whereby
the hands can be brought back towards
the body to be repositioned without
moving the image this connect system is
allowing the jury to get control of
energy no subject goes the operation to
interact with it as I without
interrupting the firm the operation
without me having to engage my brain and
try and communicate to third party since
May 2012 the system has been in use in
patient cases in the vascular surgery
theatre at st. Thomas's Hospital it so
that gives you good sense of the kinds
of things that we did now as you might
imagine and some things we mentioned
here a lot of design challenges to
building these kinds of systems so we're
taking connect which don't forget is
designed for a whole body gaming right
and we're taking in as a situation where
you can only see people's torsos people
are tightly packed together the sterile
area is here so the gesture set had to
be much more kind of contained which you
know which put a lot of constraints on
how we can actually do things another
issue is that we had this idea of modes
which are things briefly mentioned in
the video so what you don't want is any
gestures to start making the data fly
around and of course when teams are
talking to each other they gesturing so
we had a control mode where gestures had
an impact on the system and move stuff
around and we had a pointing mode which
was a default mode where whatever
gestures took place in that zone didn't
have an impact on the data so having
said all that the process that we went
through was fairly successful so we
found that
the surgeons ability to control these
images had it had an effect in many
different ways so for example one thing
we found was that surgeons well so the
rotate command was one handed we found
that they could hold the catheter inside
the patient's body and do some of this
rotation which is what they need to do a
lot in the course of an operation other
times you'd see the surgeon standing
there fading the 3d rendering on top of
the x-ray in and out to get a real sense
of where instruments were in the body so
it long times a reflection where he
would just stand there and do that and
at other times he would talk to his his
colleague right so another consultant
come in about a case and they'd spend
time just fiddling back and forth to see
they get a sense of what was going on
now all of these things would not have
been possible in that way had they been
having Hassim a colleague the whole time
move it to the left move to the right
now now fiddle it you know so it's just
hard to express these things and they
really appreciated the the ability to be
more hands-on without being hands-on but
we also found that over the course of
deploying these in real cases we had to
revisit a lot of our initial design
assumptions and I've just I don't really
have time to go into a detail on that
but let me mention too that I think are
quite important here so the first thing
is that when we first design a system we
required the surgeons to wave as you do
in a Kinect game to get in and out of a
mode and the surgeons didn't like that I
said it was way too inefficient is there
another way we can do this and
originally we haven't thought about
putting voice in because we didn't want
to over complicate things so this is the
thing about when you add new modalities
you have to worry about the complexity
of the interface they said no no we want
voice we want voice so we put that in we
brought in all kinds of different
consultants with all kinds of different
accents and it worked really well so we
then started putting voice in for more
of the discrete command so every time
they had to lock the screen or switch
modes anything discrete voice worked a
real treat and that was something that
we hadn't expected but anytime they had
to do something speacial that's when
gestures came in and that's when they
worked really really well so
that was one of the learnings hope that
we made that sometimes adding new modes
of input doesn't necessarily make things
more complex sometimes it makes it
easier if you choose the right modality
for the job at hand a second thing that
we learned was about the importance of
feedback so we worked hard on providing
feedback in the system big icons that
use excuse they get locked icon at the
bottom of the screen just to tell the
user is what state the system was in but
it turned out that this little feedback
window in the bottom left-hand corner of
the screen this is in a large version
was actually the most important bit of
feedback and what this is showing is how
the Kinect is parsing the people in the
field of view so what connect us is it
segments out different skeletons it
tracks the bodies of the different
people in the field of view and it
imposes a skeleton model on top of that
and what you find is that when people
are closely packed in the field of view
sometimes the system makes mistakes so
in this case here what you see is the
surgeon that connect actually thinks the
arm of the surgeon is the arm of the
person behind the surgeon so it's kind
of skill of skeletons going to bleeding
into the other person and what we found
is that this was really useful for them
to figure out why the system sometimes
was messing up and also meant that they
can kind of duck out of the way so you
saw the team kind of ducking out of way
of the zone of interaction so that was
really really important so since that
time we've been building and testing a
different system this time for use in
neurosurgery Adam Brooks hospital here
in Cambridge and I'm not going to take
too much about this system at the moment
mainly due to time but also because this
is an ongoing project at the moment
except to say that many of the issues
are very similar in that the heavily
dependent on image data in this case
they're more hindered because the images
are stuck to the wall they're not even
next to the patient operating table so
they have to get up and go over and look
at them and the nature of the work is
but other than that the nature's are
it's very very different so usually
they're sitting down and the surgeons
looking through an endoscope which is a
camera inserted into the body working on
problem annual aneurysms operating on
the brain in the spinal cord so it's
very delicate work
long periods of time where you're
actually sitting and looking through in
this way so some of the design
requirements for this obviously have
been quite different so originally the
surgeon we were working with saw the
vascular system he said I really like
the idea of that gesture but he said
I've got to use it in one handed because
i'm sitting down and i can only turn my
body and do this and we said okay fine
we started building it all around one
handed gesture and then we started to
deploy it and in the second case that he
had things started to go wrong is a
highly stressful kind of surgery and he
started shouting at the machine he
wanted it to do things without him
having to gesture and afterwards he said
you know be really nice if I could just
shout for it to rotate or shout for it
to clip and it would start doing that
for me and so we're looking at more and
more giving more automatic control to
the system when he wants it and and so
the requirements as I say are quite
different so let me just wrap up this
little piece of the top and and just say
what I think the main learnings are
about designing hybrid input in surgical
settings so the first thing is it's kind
of tricky is kind of tricky to do it
right and even the best guesses can turn
out to be wrong when you put these
things into practice but the top
takeaways I would say is that adding new
modalities such as speech and gesture
can make interactions simpler and more
efficient if you do it right another is
that whenever you add a new form of
input you have to provide effective
feedback for telling users what the
interaction zone is what things are
possible and what the system has
interpreted about what's going on in
front of that system and finally as a
neurosurgery shows that there might be a
balance that needs to be struck between
automatic control and user control and
that these issues are something that
designers and developers need to think
about pretty deeply okay so let me just
move on quickly to the second line of
work here which is moving now from the
world of surgery to the world of desktop
work and this is related to a demo we've
got outside which I'll point
to afterwards but here what we're
interested in doing is adding new input
capabilities to mouse and keyboard
interaction so this is called the two
foot pc project and you might ask well
why so we actually started with a
technology-focused question here was
which was that if you could put a depth
camera on top of a computer let's say
your desktop computer and you could open
up the space of interaction around
screen keyboard and mouse what kinds of
things would that enable people to do
okay so it's kind of like opening up a
bubble of interaction in the proximate
space around keyboard and mouse and we
didn't want to just open this up for the
sake of seeing what the technology could
do what we wanted to do was to take a
very principled approach to this and so
one of the things we had at our disposal
was a whole bunch of work that some of
us and our larger community have been
working on for years which is looking at
how people do document work how do they
use digital documents and physical
documents what do they do when they're
doing their work at their computer so we
really wanted to draw on that and put
together a set of design principles
which would guide us to design gestures
in the space which we hoped would be
useful for people and not just for the
sake of doing it so the design
principles we started with were these
and the first one is what I've already
talked about which is in line with the
idea of hybrid input so here what we
want to do is to very much to complement
keyboard and mouse use and not replace
it you may have seen some demos from
other companies people doing input in
the gesture space and you'll notice a
lot of it is trying to replace mouse and
keyboard use we're not doing that okay
we're doing something quite different
here so what we're trying to do is come
up with a gesture set that allows people
to interweave mouse and keyboard use
with the gestures so always coming back
to mouse and keyboard use and then
interjecting the gestures in between and
hopefully we can do this in a way that
doesn't add more complexity but gives
more users more power and flexibility
that's the goal okay and that's that's
the first big principle another big
concern was that the gestures would be
economic and what this says is you don't
want gestures where you holding your
hands out here for a long time right you
don't want Tom
Cruise in Minority Report doing this
because it's tiring well maybe not for
Tom Cruise actually but for normal
people it would be so so we want to make
them ergonomic and we also wanted to
leverage the power of gestures so I
talked earlier about choosing the right
modality for the right task right so
what touch what keyboard and mouse is
really good at is precise things right
precision selecting and pointing so why
would you want to do that in the air air
air gestures are not good at precise
pointing and selecting you've got to
hold your hand it for a long time and
there it's just not ideal but what
gestures are good at is very expressive
things you can do things casually you
can do them expressively and that's what
gestures good at so let's let's play to
the strengths of the input modality and
another design goal really related to
this is to try and exploit the power of
two hands so when you have gestures
obviously you've got two hands can we
bring in gestures so that for example
you can leave one hand on the mouse as
you do something with your other hand
and finally we want to adjust your set
and this is a you know very common thing
for making things usable you want the
gesture set to be easily easy to learn
and easy to remember over over the long
term so that's what we started with and
then we went on to do something which
our designer on our team did which is to
put together a series of animated
sketches of interaction so what I'm
going to show you here is an animation
it's not real but this is often how he
will sketch out a design experience so
that we can get a sense of it by just
looking at what it might look like so
I'm going to start with this one in
which we're playing around with this
issue of trying to unload what often
happens on the mouse hand onto the
second hand to try and make navigation
easier and he's done it here in Excel so
this is smoke and mirrors this isn't
real so what we're thinking about here
is an interaction zone that might be
next to your keyboard and this is where
you use your non-dominant hand your
non-dominant hand is very good at doing
coarse-grained
navigation you know big large movements
while your left your right hand so your
dominant hand does the pointing and
selecting with the mouse so here he's
showing moving around spreadsheet and
using his right hand to point select and
anybody's got RS I might appreciate this
that you unload some of the work onto it
onto your second hand another thing we
played around with was the idea of quick
formatting commands so here he's doing
just a quick freeze of the left-hand
pane in micronesia get you know so it's
playing around the idea that you might
have just quick gestures to do some of
the things that you might want to do
frequently the second animation we're
going to show now the idea in now in
Microsoft Word of thinking of the hands
as kind of proxies for objects or
documents ok so imagine these things are
do interesting things for you as for
example bookmarking so in this example
we have a document he's holding his hand
up navigating somewhere else and when he
lets go it springs back to where he was
so it's the idea of looking at what
people would do with physical documents
here he's taking a quick glance at an
outline view of this document you know
so just tipping your hand that might
make you allow you to look at something
else as you're typing and put it away
again and note that's a nice spring back
action it's a temporary thing or bring
in a different document from the side
right this is just playing around with
concepts here so you might look at it
put it away again and it's all one
smooth movement there he's pinning it so
he's bringing it in pinning it and then
I'm pinning it putting it away okay so
these are just they're not precise
actions are very flexible they're very
casual here he's tiling two things side
by side okay it's a quick window layout
is another thing that gestures might do
and then untying them and here is going
to resize flick of the hand
so you know again none of these things
are precise things none of them require
you to hold your hand up for a long
period of time or at least if they do
we'll get rid of them okay so that those
were the aspirations those are the
things we're trying to build actually
building these systems is difficult if a
difficult challenge we've got two of our
crack developer team here in the room
today who be trying to build these the
system and they're going to show you the
demo of it later and I'm going to show
you a video now for what we actually got
which is isn't perfect but we're getting
there and the system that i'm going to
show you now doesn't do the on desk
navigation stuff yet we're still working
on that it's it concentrates more on the
in-air stuff so let me show you when we
are insistent free and just for the
attraction to the desktop require an all
you get up camera mounted above the PC
the system you taste variety in air
gestures within the proximal space
around keyboard and screen the aim is to
explore how such gestural interaction
might enhance rather than replace mouse
and keyboard use recognizing that
different input modalities have
different kinds of affordances our main
gesture set is designed to be low
physical effort so it avoids sustained
in air gestures and is designed to be
interwoven with mouse and keyboard use
it is also designed to be easy to learn
by use the keyboard as a frame of
reference for inner gestures happen in
the space above the keyboard the peak
gesture allows a temporary glance at
another application without shifting
focus from the main task
switching the active window is a
two-handed action if the user wants to
keep the selected application open they
pin it with the other hand helping them
to switch tasks to the new window a
pinch gesture invokes search note how
the hands can move between typing and
gesturing seamlessly other in air
gestures allow quick ways of doing
window layout for example a two-handed
gesture provides a quick way of laying
out two windows side by side repeating
the gesture resets the state some of the
in air gestures use the metaphor of
physically grabbing the active window
then windows can be docked left or right
it can also be minimized and maximized
finally some gestures allow quick
navigation such as forward and back
there is also gesture to show the start
screen and then go back to the desktop
our system is general in its approach to
sensing hand gestures in touch the
Kinect camera captures raw depth data of
interactions occurring on the desk an
adaptive foreground segmentation allows
us to isolate the hands we then use a
random forest based classifier to
identify salient features of the hand as
well as detect the hand pose each pixel
in the foreground image is classified as
either fingertips shown in pink the
wrist shown in gray or the overall hand
state which transitions between
different colors as the user performs
different gestures the different hand
poses that we can currently classify as
shown in sequence would synthetically
generate a training data shown in set
this allows us to classify a variety of
hand States without requiring full hand
tracking to determine a specific hand
post from this label data we first
compute a histogram for each hand across
all hand States and select the state
with the highest frequency using this
approach we can create a gesture
recognizer easily by just observing the
presence of a specific hand state for a
certain number of frames more complex
gestures can also be developed by
observing transitions between multiple
hand States we deal with clutter such as
books and documents on everyday desktops
by adapting our background segmentation
model over time and integrating new data
that remains static only this avoids
integrating hands that are interacting
and moving with the scene into our
background model as shown
okay it's a bit of a technology there
too at the end so it's a difficult
challenging system to build for many
different reasons but we got to the
point where we tested it in the lab
iteratively and then we decided we're
going to test it in real life and by
that I mean in at people's desks for an
extended period of time in this lab to
do real work which is a challenge right
and we had six people using it over the
summer for a couple of weeks and what
we're really interested here was to find
out what kind of gestures were they
finding valuable and for what reasons
did they ever take the place of doing
particular kinds of shortcuts because
some of some of the things could be done
with shortcut keys and whether they
could interweave the gestures with their
mouse and keyboard use which after all
is one of the goals that we had another
goal was to understand the challenge of
using this in a real environment so
we're the gestures tiring over long
periods of time something we were hoping
to avoid with a robust and reliable or
do they become just annoying and
distracting so these are things that we
really came to them to find out also
whether they were easy to learn and
remember over two weeks because it's
fine to do these things in a short-term
test in the lab but if you ask people to
do them for a lot longer than the
learning is more kind of realistically
assess so the findings we got some kind
of early results from the field trial
and it turns out that testing this in a
real person's desk for doing real work
throws up all kinds of challenges that
we just never see in a lab setting which
is tends to be nice and clean and and
very controlled so for example one thing
is we found that people bring very large
objects like documents coffee cups all
kinds of stuff across the keyboard in in
the field of view and this has the
potential to trigger things that you
don't want to happen so Pete and Chris
you're building the system would come in
they'd find a way around it they'd
reject the code so that it would deal
with these kinds of things another thing
we found was people had lots of
idiosyncratic gestures so we had one guy
who would always sit and think like this
and I
time you did this he close an
application which was not so and and
then in other people would kind of sit
like like I do this I but I'm thinking I
sit like that and I'm triggering search
you know so there are things you're not
going to get that in lab situation
you're only going to get it when you
look at what people really do and these
are very ingrained habits to you can't
ask somebody to start doing this if
that's what they do all the time so
again we had to we had to change the
code design around this and and
sometimes think about changing the
gesture self as well so one of the
ironies then is that when you try and
design a gesture set that interweaves
with real interaction you're more likely
to kind of come up with a gesture set
that's more easily confused with what
people do so that's a real design
challenge to try and get around that
secondly so some of the gestures
actually did prove to be really really
valuable for people they love the peak
one because that's after all something
that you can't easily do with a bunch of
shortcut keys or Mouse and cute mouse
interaction and that was nice for doing
things like peeking at an IM window if
you're working on something they also
really like the window manipulation
docking especially people who didn't do
that with shortcut keys and and just
moving from from clearing the desktop
and moving into windows 8 and so on
those those were really popular as well
and so you might ask well if you can do
these to the shortcut keys why would you
bother it turns out that having the
choice was something people really liked
and so there was some times when just
depending what tasks they were doing it
was actually easier to do the gesture
and then other times they were going to
kind of lean back mode so they might be
not kind of up close it over the
keyboard and like looking at the web but
they might be leaning back and they just
wanted to do forward in back gestures in
a kind of more relaxed mode so yes they
do compete with our engrade habits and
sometimes those things are more
efficient but just having the choice
here and allowing people to do new
things with something they really liked
and then finally while the gesture set
was initially easy to learn we didn't
learn enough from surgery work to put
enough feedback in the system so what we
found was
that over time people needed more
feedback just to know how the system was
seeing their gestures and what it
thought was going on so we added this
little feedback bar on the top right
which would just highlight whatever
gesture the system thought was being
triggered and that helped people to
diagnose when things didn't go as I
expected but it also kind of reinforced
their learning / times that was a really
important thing to do so so all in all
it seems like we've had some glimmers of
success for trying to extend the input
capabilities around a system that lets
you know it that some people have been
spent many many years in graining their
their existing work practices around
keyboard and mouse so actually it's a
very high bar to come in and try and
change it or to interweave and try
incorporate new things into those work
practices but we still have a lot to
learn here we have a lot to learn about
how users do these activities how we can
design them for them and what the
technical issues are but luckily when
you have a multidisciplinary team that's
that's one way in which you can start to
to attack attack these challenges so I'm
going to finish up now and I just want
to finish up by summarizing with six
challenges which I believe are the kind
of main challenges for designing these
these kind of hybrid systems so the
first challenge is to find combinations
of input that are stronger and simpler
when they're together rather than more
complex and more cumbersome and I hope
I've shown in this talk that you can do
that not just by looking at kind of the
user research in the area but from
design exercises from firsthand
observation of what people do and then
from evaluation of systems in real world
situations a second is that part and
parcel this is understanding and
leveraging the power of different input
modes so for example why use in air
gestures for fine-grained pointing when
we have other input devices that do that
better and why use gesture for mode
switching where
simple voice command will do that a
better job for you so it's perhaps too
obvious to say but different input modes
have different affordances and they're
not simple substitutes one for the other
thirdly we have to recognize that
achieving this also means achieving a
balance sometimes between user control
and system control and this is something
that i showed the surgery work well
there's some situations in which user
control is crucial like being able to
fiddle around with that 3d model while
you're talking to somebody and other
times automatic control is it's much
more desirable forth everyday skills and
habits can be exploited in the design of
hybrid input systems like designing
gestures based on physical manipulation
skills like we had in the in the surgery
case for example but at the same time
existing habits will shape how your
system is used and it may give you
problems that you didn't anticipate by
competing with ingrained habits and
competing with the way that people have
their workspaces laid out as we showed
in the 2-foot pc system but again i
think you're never going to know this
until you deploy a system in real
working practice and fifth that adding
new in capabilities to an existing
system will will never be totally
seamless or instantly natural okay so
users need need the interface to help
them understand whether they be whether
they're being sensed in the first place
what the zone of interaction is what
actions are possible and what what the
system thinks is going on users need all
that feedback and good feedbacks crucial
to designing these well and number six
is testing in the wild so I'd argue that
meeting any of the above five talent
challenges means you have to deploy a
system for real and that that's perhaps
the biggest challenge of all because you
need something fairly robust and
reliable to do that you need an
excellent technical team and people who
understand users and designers so we
have a long way to go in terms of
creating new forms of input and changing
current practice really sets a high bar
for success but I hope to have convinced
you in this talk that
it is possible to create these new new
input paradigms and when we do we have
the potential to offer users a lot more
power and flexibility in the ways that
they engage for their digital world so
I'll leave it there thank you any
questions at all this point hands going
up shooting up oh good question we did
an interesting internship we have to
really excellent in turns over the
summer looking at eye tracking and
looking at where it makes sense to put
gaze into systems choose that there's a
whole lot of literature you probably
know this on I gaze and where it's where
it makes sense of where it doesn't make
sense so for example I guess doesn't
make sense for moving pointers around
but it does make sense in setting the
context for other kinds of things so
we've been playing around with a
technique called the magic technique
which has been out there for a number of
years which moves your cursor to the
kind of vicinity of where you want to
interact and then your mouse does read
this a fine grain work we've also been
playing around with the use of I gaze
plus gesture and I gaze plus voice and
written a paper on that which I'm not
sure well we'll find out in November
whether it's gonna be paid but we again
have been testing these techniques with
people at the desks and one of the
things you find is that unlike the lab
situation calibration is a huge issue
right because people are up and down
from the desks all the time and if you
haven't got the calibration right
everything else doesn't work properly
and there's a bunch of other issues too
so I think it's a really interesting
area but again we need we need to attack
this in a very principled way and not
just randomly because we do it because
we can do it now be have you been back
to any of the two pc anything
or any of the gestures that they learned
during that well weeks no um because
most of them are interns and they've
gone but we could however one couple
people here still actually a couple of
them did say that when the system was
shut down they tried using the either
kind of naturally try using these things
and then realized oh so I switched off
so that tells us that some of them did
become sort of chemical incorporated
into this which is really interesting
but as for remembering the whole set no
people didn't remember the whole set
because the ones i didn't use just
dropped out scuzzy other challenges that
some of the gestures are culturally rude
yeah certain countries
internationalizing the solution they may
presenter challenge yes that's right
there's a very question yet regarding
those dentures how to design a virtual
keyboard on the monitor so you can just
simply remote control like point into a
a canal just completely remove the
keyboard no because that would go
completely against the design principles
that's about precision and we don't want
to do precision in the air right so
we've got a keyboard if you don't need
one in the air and the physical keyboard
is much better so he was there a
measurable improvement in the outcome
for patients undergoing vascular surgery
it's really hard to measure things like
that so they didn't die no it's not the
more secure say it is and the NHS ask
these kinds of questions and and it's
you know it's it's a serious question in
the sense that it to put these new
technologies in often what people want
is numbers but it's what would you
measure I mean would you measure the
time of the operation because it might
be that actually giving surgeons this
ability makes them spend longer but they
might have have a deeper understanding
what's going on you know so you so just
getting quantification is
is difficult and it's something we
thought about and we sort of given up on
how to really do that what we have is
testimonials from the doctors who are
using it and the other thing that's
interesting here is that this team is
the team that gets the really difficult
cases because they put that 3d rendering
system together with the showing the
aorta and that's it that's really
crucial for doing difficult cases where
you can't actually figure out where you
didn't give you a lot of background on
this but usually when vascular cases
come in they don't have that 3d model
they just have the the 2ds fluoroscopic
image but these guys have the 3d as well
and that helps them do things that other
teams wouldn't touch so they have the
difficult cases anyway and some of them
are hugely long so again it's hard to
come by okay hand at the backwards to
make something it was the 24 pc have you
got any thoughts of how you apply it to
say the mobile laptop you so maybe
perching up and working anywhere yeah so
um what you'll see at the demo room is a
big rig stuck over the the pc and it
looks really cumbersome and clunking you
think well i never want that on my my
desk but actually there's no reason why
we can't do a much more lightweight
portable version of this that might even
be embedded in your laptop so there are
mobile ways to do this but of course
we're not there yet we're thinking we're
still refining the gesture set and we're
not doing we're not the point where
we're doing that but but there's no
reason why these things couldn't be made
more mobile so we are thinking about
that
one is the other time we've interaction
will go beyond 12 son pc user time to
upload something to a crowd or to the
number pc mobile be very action to this
rubbish and the second the second one is
on learning strategist and do we blow
all over this just at once or to do the
users of the point just one little bit
at a time of them and teaching them a
goal so first question have transferring
things across piece here to up to club
we don't have gestures for that at the
moment we do have a gesture which I did
show for flicking a window from one
monitor to the other which is palm down
now one of the reason we didn't show
that it's sometimes a problem because
when you reach for your mouse you fling
a window I mean it's really hard to
design these gesture set so that they
they interweave and they don't trigger
when you don't want to so we haven't no
we haven't done gestures for
transferring a cross compute because of
course now we would look at going to the
cloud and I'm not sure you needed
something physical for that but I think
it's something to think about learning
well we had so what we did when we train
people we just had a visual cheat sheet
with all the gestures and what they did
and we ran them through all of them and
it I suppose we could have introduced
them more stepwise well we just didn't
do that we do we fit okay here's the set
and we'd come around every day and see
what they were using what they weren't
using and ask my and so we were we were
there pestering them all the time we
didn't do a kind of controlled
assessment of the learning okay one more
anybody one more question um I think
yourself down the front here next
Saturday
in the alleged to think about different
aspects of resilience in the application
because I'm thinking about inconsistency
of response resistance in response they
need an application failure because the
desktop scenario may be a ridiculous
cause a rubicon theatre scenario
beginning to the boundary of critical
systems yeah I mean we obviously worried
about that too but we made sure that in
the surgery system there was always a
backup so if if the gesture if the
Kinect camera failed or the system
dropped out they just go back to the way
they normally did it so we had a guy
sitting behind who had who could take
control of the images if if it needed to
be so there was never a problem with
that in fact we were really worried
about ethical approval and getting into
hospital and things but because it was
seen as not on the central path but is a
kind of nice to have add on it wasn't a
problem and they let us in which was
great scary but great okay Paul get a
call at time that thanks oh my god
thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>