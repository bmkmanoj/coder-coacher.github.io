<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Algorithms and Perception for Interactive Free-Viewpoint Image-Based Navigation | Coder Coacher - Coaching Coders</title><meta content="Algorithms and Perception for Interactive Free-Viewpoint Image-Based Navigation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Algorithms and Perception for Interactive Free-Viewpoint Image-Based Navigation</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xtY5L3yNPlc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi it's my pleasure to invite graph
terrassa for talk here graph is
finishing his PhD under this provision
of George rheticus at inria on area of
image based rendering thanks a lot for
the introduction and a very good
afternoon to everybody I will be talking
about my work on image based rendering
so let's start from the basics the basic
pipeline in all of graphics is to first
do scene modeling you create a model of
the scene then you would need to do some
editing of the materials brdf design the
illumination of the scene and that's
when you get the final result but all of
these steps required intervention of
trained artists depending upon how good
and image you you want to get and that
brings image wasting in image based
rendering into the picture because
anybody can take images and if we could
render the scene just from images that
would be really cool so the problem with
image based rendering is that you can
you can create novel views from input
images only if you reconstruct the whole
scene exactly from your input images
that turns out to be a very hard problem
and is active research in in reconstruct
with geometry the materials and the
Lighting's of scene but you can only do
approximate novel views so the challenge
in in ibro dimitris rendering is
basically how do we create an
approximate novel view and what is an
approximate novel view so I'll try to
answer all of these questions in in my
thesis my work is mostly focused on the
first part which is how to create
approximate novel views and I was lucky
to be part of the projects in the team
which which were on perceptual analysis
of image based rendering so let me start
by explaining the problem statement in
more detail we are trying to do image
based rendering of urban scenarios so we
are looking at at data sets at at images
which have a lot of architecture trees
cause geometry we are trying to use
handheld
the image is captured with handle
cameras only we don't want to depend
upon upon laser scans if we had this
data it would be great but we don't want
to rely on on on dense data we would
like to do this in as few images as
possible the example you see here is an
example of a dense capture lots of
images captured of the scene we would
like to do this in sparser and sparser
captures because that allows us to very
large scale image based rendering zuv of
really huge data sets and lastly we will
try to approach the problem of a free
viewpoint navigation most existing work
approaches the problem of view
interpolation where you jump from one
input view to the other input views but
but doing free viewpoint navigation is a
much harder problem and we would like to
start working in this direction so brief
overview of image based rendering it all
started with light fields and image
based modeling way back in 96 there's
been a lot of work as i said on Bo
interpolation this field has spawned a
number of other applications like up
sampling of videos and camera
stabilization and as I said I'll all of
my work is focused on ibr off of urban
scenes and we have a lot of commercial
products in this field already and all
of these products use a very basic form
of image based rendering which you just
have one plane you you take you jump
from image 12 image to and just do very
basic blending klett approach this
problem and try to as well as possible a
standard approach is to reconstruct the
geometry of the scene and then any pixel
in the target view can be recreated by
back projecting onto the geometry and
reproductive into the input views this
gives us and and the earliest work was
indeed based on on this approach and
this this actually works really well if
your geometry is good as it turns out
the geometry of of of urban scenes and
and and for that matter any data set
with enough enough complexity is not
completely accurate and that's why most
of recent work has focused on
alleviating the artifacts we get because
of slightly inaccurate geometry for
example in this work optical flow is
used to fix coasting artifacts one
you've you
you've rejected your input images onto a
proxy if there is an alignment issue
optical flow can be used to align these
images back and this works very well as
long as your alignment problem is off by
a few pixels more recently this is the
works which was partly done it at MSR
ambient point clouds are used for
unreconstructed areas of the image the
assumption is that the main object of
interest is very well reconstructed as
you can see here and the background in
the sky can fade in and fade out so we
try to identify the problems in
reconstruction in multi view stereo we
know that these these approaches can
give you sparse a noisy data if you
don't have enough texture in your input
images if the texture is is too random
or to stochastic if the geometry is too
complex and all of these issues are
aggravated if your input image is a wide
base line if the spacing between input
images is a lot and the overlap is quite
little so far from from point of view
fibr this is an input image and a depth
map which which we got from existing
multi-view serio techniques the problem
really is that it can be quite dense in
some regions for example the trees is
almost not there very few depth samples
and occlusion boundaries are simply not
there in in in the depth maps so with
this we now know that the geometry
estimation does not give us dense data
everywhere and for such areas we will
try to use image based approximation
will try to use image warping and the
two main issues are both occlusion
boundaries and depth samples which which
which which we are missing so why image
warping image warping because it has
been shown to work well for a small
number of points if you have a few
pixels which can be reproductive
reliably into the novel view you can
work your whole image with this in a
least square sense so all of this was
inspired by the camera stabilization
paper of louetta and we we improve that
to use a shape preserving verb as I'll
explain afterwards and it's a more
intuitive way of controlling
how the input image is converted to a
novel view you can use depth to do that
but the effect of depth is more indirect
you don't know exact you can understand
the artifact that you will get because
of noisy depth but if you design your
algorithm on the basis of image words
this is a more intuitive way of
controlling how the how the image will
be will be reached projected into the
target view so I'll allow go first over
my first project more briefly this this
project was a dgs are a couple of years
back and this this really served as a
proof of concept that image warping
indeed is useful so we start with a
bunch of input images we reconstruct the
depth maps for each of the input images
by using standard off-the-shelf
multi-view story approaches and we mark
all the occlusion boundaries
approximately by and we ask the user to
mark all the occlusion boundaries and
I'll explain why we do this afterwards
so with this data we have depth samples
and I said we're going to use these
depth samples as constraints for image
warping each pixel shown here in gray
can be reproductive into the not into
the target view but all the white areas
cannot be reproductive because they have
no depth at this moment if you overlay a
big vault mesh on top of this a grid and
use each of the each of the gray pixels
to reproject the whole mesh then you can
probably handle the empty regions and
that's what the shape preserving work is
all about so we have the reprojection
constraint which comes from depth
samples so we overlay a big sort of a
quad mesh on top of the whole input
image a 2d mesh some pixels as shown
here have depth we can reap roject these
pixels into any target view and we can
work the whole mesh along with it to
work the mesh we need to hold the whole
mesh together by some kind of 2d
constraint that we use as a shape
preserving constraint which which
enforces that each triangle of the word
mesh is only allowed to undergo a
translation on the image plane a
rotation or an isotropic scale the end
effect is that each triangle does not
undergo arbitrary I find deformations
that preserves the local shape of the
word mesh but this as you would you
would notice is a smooth image board and
this will be a problem at occlusion
boundaries because the background will
move at a different speed compared to
the foreground and this indeed is the
problems you can see in this not very
well done illustration if you have some
points in the foreground in green
someone the background in red and you
move the novel camera in one direction
the points will try to fold over each
other which is the case of occlusion
when the foreground occlude is the
background and the same thing happens if
you move the camera in the other
direction in this case the word mesh
will stretch because the background will
move in a different direction than the
foreground and this will lead to
artifacts as shown here this is an input
image moving the camera in opposite
directions leads will lead to these kind
of ugly distortions so to handle this we
embed a discontinuity in the warp mesh
this is done in the form of an elastic
band so as I said we marked all the
occlusion boundaries proximately by hand
so we have these very nice edges of
around each each foreground object we
insert a narrow band of triangles around
these around the foreground objects and
inside these triangles we do not apply
the shape reserving constraint this
makes sure that this area is allowed to
deform arbitrarily but the rest of the
mesh does have the shape preserving
constraint and that stays the way it
should the end effect is the part above
this elastic band shown here does not
affect the part below and these two part
can can move in opposite directions or
they can even fold over and and this
constraint was the main contribution of
the paper this this this allow this
showed us that you can even use smooth
image works for doing a discontinuous
sort of effect at the same time you can
maintain the shape of the foreground
object imagine an object which has a
right angle is horizontal edge and a
vertical edge and you want to make sure
that this right angles stays the same
while you're working your image of
normal view to constrain that we
constrain the angle between any two
neighbouring edges of
of the foreground object with these with
these four constraints we get the full
image water energy function and we use
and we solve it in least square sense
and that the the effect of all of this
is the earlier distortions which you saw
are now contained inside the elastic
band which we marked earlier so that so
the red region shown here was in the
beginning just one pixel wide but
warping the image to different places
allows all the distortion to be absorbed
inside that narrow band and we know
where the distortion is so we can use
other images to feel content into these
areas so now we'll go over the rendering
pipeline I hope you can see the input
cameras okay yes so the input cameras
are all shown in gray the target view
for each target view we pick a few input
images bought them and blend them I
won't go into the details of blending so
here are some results so on that side
you see results of of earlier approaches
as instructed luma graph with the best
quality model that we could obtain and
you would notice that we get ghosting
artifacts there because the model was
not perfect and here we don't have the
same ghosting artifacts the results
aren't perfect but we managed to improve
significantly in on a corrosion
boundaries in this case you will see the
whole tree moves at least as one block
and it's not broken up completely all
all the all the normal camera paths here
are not quite view interpolation but
almost very close they are not very free
viewpoint yet and the reason they're not
we could not do completely free
viewpoint rendering was because we used
a global image war and this led to
distortions when you move too far in or
too far out so I'll show some
comparisons of of these distortions
afterwards the major limitations were
first it was a global image war because
and this led to distortions shall
explain later and second was that we had
to ask the user to mark all the edges
manually and this just this led to a
problem I'm sorry
yes man you also have to somehow specify
the severs the retreat was in front of
the building or is that automatic no
that's all that's automatic because you
do have a reconstruction so you can use
that to infer the ordering of things so
and as as as you can see this would be a
painful process if you have to do the
same thing for every single image of
every single data set makes this
approach not very practical and being a
global image board the it was real time
but the system was quite big if you're
trying to solve a bot much for the whole
system it became quite big and more so
because we had to embed these these
elastic bands so it was a conformal
deloney triangulation and that made it
even harder it was it it had it it a
numerical issues and other problems and
lastly as I said we had distortions so
to address these issues we did the paper
which I will present at siggraph in in a
few days mainly the main targets were to
do something completely automatic not
depend upon manual intervention be real
time take it easy so for this we will
again use 2d image based approximations
to get occlusion boundaries we've used
image / segmentation so you divide the
image into lots and lots of super pixels
and these super pixels are they always
capture all the collusion boundaries
they have lots of other edges but we can
handle that because we're using a shape
reserving board explain that in a bit to
handle areas with no depth we add some
fake depth some approximate depth in
these empty areas it's important to note
that the depth field ad is is not photo
consistent it's only useful for doing
these approximate image warps you cannot
use this depth to to help you in surface
reconstruction third step is is largely
the same as image war but this time it
will be a local war applied on each of a
pixel individually and this this this
reduces the size of the warp and makes
it much more easier to to actually solve
it becomes much much more lightweight
last step is adaptive blending where
we'll try to alleviate artifacts which
which arise because of ghosting and
popping
so the pre process is largely the same
we again use multi hbu steady to to
reconstruct a depth map for each input
image the second step has changed now
instead of instead of applying instead
of asking the user to mark the edges
manually we use image / segmentation and
from this point onwards the rest of the
algorithm will operate on super pixels
as as the basic building block so the
next step is to add depth into empty
regions so here is a depth map and a
super pixel segmentation if you overlay
these two things I hope you can see it
over there that you have lots of areas
which have lots of depth that's all
grant that's all fine but the lots of
super pixels which have no depth which
appear in white and we cannot represent
them in any way in in the novel view if
we bought each with pixel individually
so to handle such cases we can find
depth from other super pixels which have
the same visual content and are
spatially close so let's try to mark all
of these these problematic areas in
green and look at any one of them for
example so we have these two two ideas
to find approximate depth spatial
proximity and same visual content first
let's try to find for pixel which have
the same visual content and these could
be anywhere and actually if he ranks
were pixels on by the order of of
similarity individual content we found
that the rank one two three four five
were always in sort of very different
parts of the image so you can completely
rely on this we had to incorporate six
spatial proximity problem with this is
that if you put spatial proximity across
the image plane and RGB difference in 21
weighted distance metric you have to
adjust the weights every time for each
input image and for each data set so to
solve this we use a super pixel graph we
have super pixels like this we consider
each of ixil as a node in the graph and
the edge between two super pixels is the
distance between their histograms their
RGB histograms and what this basically
means is that an edge between 200 pixels
on the wall will be quite small because
they have the same visual content and
two subjects on the tree will also be
low but any
thing between the wall and tree will be
quite high and histograms give you a
nice way of evaluating this metric
because the shapes and size of zoo
pixels are quite arbitrary but their
content is supposed to be very
homogeneous so with this you you now
have a graph of for your whole image and
you're trying to find the best neighbor
of the targets per pixel shown in red
over here and you just want to refine
you just want to choose a few of these
yellow guys where to you just want to
find where to choose depth from let's
zoom into this area this and try to find
any path from one of the red guys to one
of the yellow guys but could be from
anywhere in the image and you can
imagine the shortest path will be
something that jumps over super pixels
which always have the same visual
content if you stayed within the tree
always though that path cost will be
small if if you ever jumped outside the
tree you will immediately incur a big
edge weight over there so we so we
retain this so we rank all of these
yellow guys by their shortest path cost
and we can find shortest path for all of
these yellow guys and we retain those
which have which have the really
smallest costs and these have the same
visual content because that's what we
started with and they are also closed in
the image they are also more or less on
the same scene object now it's more
important to understand that the reason
we are choosing these three is not
because there's clothes in Euclidean
sense they are closed on this graph this
algorithm will converge to Euclidean
sense if you had depth in enabling super
pixels but if it if you did not it could
go and find depth from a relatively far
off place without trying to find their
for the Red guy from the wall which is
much closer actually so at this point we
now have a super pixel where we wanted
to add some depth we found some
candidates for it which have some depth
samples and we can interpolate these
steps samples to add one pixel
with depth into the target super pixel
so and we can so i won't go into the
details of this interpolation and we can
add a few depth samples you could go on
and add depth to every single pixel of
the super pixel but that's not required
because we're about to use a shape
preserving war when i'll explain why we
need that we started with something like
this with sparse depth and we have added
some these samples artificially now we
have something in every super pixel and
they're good to go to the next step
which is image warping so if now that
you have depth samples in every super
pixel you could just triangulate these
depth samples and reproject each of the
vertices to get a target view this will
lead to problems because we added these
approximate samples and even if even if
you look at the original samples they
can be a bit of noise a bit of a bit of
inaccuracies so the effect of of this
tiny bit of noise can be seen on on the
balcony part which was well
reconstructed but kept samples do carry
a little bit of numerical errors and in
the top part you'll see problems because
we added approximate depth same on the
tree and on the bush the result of our
shape preserving work is that it does
not leave as many cracks simply because
we regularize the effect of of depth
samples we not it's it's ali square
smoothing and as i said before the shape
preserving work gives it a
straightforward intuitive way of
controlling the effect of these steps
samples so let's jump into the warp
itself it's yet as i said these are the
areas where we actually added depth
samples in the previous step and that's
where you can see the best effect of the
shape reserving warp so the vault is
exactly the same the same reprojection
constraints but within each super pixel
we projected into the target view and
you can work the super pixel like that
the shape is immigrants it is also the
same we try to make sure that each wat
mesh each face of the wat mesh does not
deform too much and with this we can now
choose for input images to blend to work
for each target view which you can see
on one side and the final blended result
on the other side and and let's now just
take a brief
look at the adaptive blending step the
basic waits for blending images for
image may straining come from
unstructured looming graph a very old
approach but still holds holds good
these weights are based on orientations
of cameras choosing the closest cameras
the problem with these weights is that
they blend too aggressively and in such
situations you always get ghosting
artifacts if you blend too much and this
is something that we found in another
work which I'll talk about in a little
bit so to handle this to handle this
notion of adaptive blending which means
we want to blend only when absolutely
necessary we use this idea of super
pixel neighborhood across images so
let's assume this two images zoomed in
area of two images and two super pixels
just mount artificially over there these
two super pixels will be considered
neighbors if the depth samples are one
reproject into the other this means that
there is some side of correspondence
between these two super pixels they
belong to the same part of the same
scene object if that is the case it's
probably okay to blend them if at any
pixel target view you're trying to blend
two super pixels which are not neighbors
and hence do not belong to the same
region of the same object of the sea
nada to beg your pardon it's not a good
idea to blend them because you will end
up with ghosting artifacts in such cases
we favor one candidate quite heavily we
increase its its blending Wade
artificially and that that reduces the
amount of ghosting quite a lot and
lastly for any small cracks left we just
use basic hole filling to get the final
result now I'll just show some free
viewpoint results in these cases you can
see that we're actually moving the novel
camera shown in red quite far from the
input images and in this case actually
really far from the input images so this
is taken from across the road from in
between on the sidewalk and the images
which are warped are shown in blue and
you can see that we do we do get
parallax effects even though we move
quite far from
from from the input cameras and that's
that's the main goal of this project we
experimented with our approach on a
number of input scenes we can find more
details in the videos and paper and I
would would request you do that so we're
not compared to existing approaches this
is combined with floating textures which
used optical flow to alleviate ghosting
artifacts and you can see in this case
it was not designed for cases where the
artifacts are really big so it does not
converge as well as it it it should have
and in this one we compared with ambient
point clouds which uses this hazy point
cloud for unreconstructed areas again
this was not designed for cases where
your unreconstructed areas are actually
on the main scene object and then it
looks like an artifact and it's also
important to note that I'm in point
clause with a view interpolation only
technique it doesn't handle freeview
navigation lastly we compared to our own
approach the one I explained before and
I said we get distortions when we move
very far away from the input cameras you
can see in these cases and it's also
more to note that the new approach is
completely automatic were the only one
required a lot of manual intervention
aside from being a bit slower and and
heavier for the machine so the major
limitation of our approach is that we do
not handle sharp depth gradients within
super pixels we assume that each super
pixel is does not have a big gradient if
you're looking at any surface of the
scene at a grazing angle then any super
pixels on that surface will have a sharp
gradient and if it had to work this it
will warp more in one direction than the
other direction and the isotropic scale
assumption of the shape preserving vault
breaks down and that's when we get
artifacts you would like to solve this
issue by by redesigning a shape shape
preserving word itself and that's not
complete income inconceivable so with
this I am done with one section which
was the main work of of 18 feet with me
into projects of the PhD I'll jump into
two other projects where I was co-author
so these were to understand
sexual issues an image based rendering
the first paper was to understand the
age-old question of blending versus
popping so you'll notice that in both of
these papers we use very simple ibr
scenario we use simple scenes which have
just a facade and a balcony in front the
proxy for the scene was just one big
plane as as it is in applications like
Street View and lastly the ibr approach
used is probably the oldest one ever
just apply texture map and the reason we
use is very simplified setting goes to
make sure that we have complete control
on the studies they have no other
variables obviously this has to be
improved and generalized to to arbitrary
arbitrary scenes but we had to start
somewhere so in this paper will try to
understand this issue of blending versus
popping so so that's basically what
street view does you have panoramas at a
discreet places and novel views are
constructed by by this by this sort of
by choosing contributions from the two
images the scene geometry could be
anything but the proxy is always a flat
plane so in this in in this study we
showed people image based rendering
results and a reference video along the
same path and we were and they were
asked to rate the level of artifacts in
these videos so for example in this case
there is no blending so you'll notice
that the edge of of the balcony really
pops and the popping is over big
distance because the sparse capture
there are few input images so it jumps
over larger distances the second one the
popping is smaller but more frequent
because you have more input images some
other case in terms of blending we again
have a sparse capture where you can
notice lots of hazy effects especially
on the the banners over there and lastly
on this door and part of the window if
you can see over there has this big
ghost which is released in two very
different places simply because the
sparse capture and the dense
capture would bring the ghosts really
close and merge them as you can see in
this case so we showed all of these one
by one to the user with along with the
reference view and ask them to rate all
of this and we got a strong guideline
that people like seeing a sharp image
which pops a little bit compared to a
ghosted image which is just always has
low frequency which is missing
high-frequency details for example in
this case you notice it's using image 1
image 1 and then it pops to image to at
some point there you go and then uses
image to people like this more than
something like this which is always a
bit hazy so you don't see the big
artifact the big jump but it's always a
little bit blurry in the next experiment
we tried to understand what could be a
compromise between blending and popping
so we use what is known as crossfade
where you use image 1 up to a certain
point then you fade between image 1 and
image 2 then you use image 2 so this
turns out to be a good compromise
between and the other cases popping and
blending our extreme forms of crossfade
a pop is crossfade of length zero blend
is crossfade of length complete path so
it so we again showed all of these with
the reference views and we got a strong
guideline that cross fades for long
durations are less acceptable than cross
phase for shorter duration again for the
same reason that people like looking at
a crisp image with a small sort of
change and then again a crisp image but
this also shows that people are not good
at noticing a perspective distortion in
this case you can see it's distorting it
changes and it's distorted again but
people don't seem to mind that or they
don't notice that and that's the reason
we did the next project which was to
understand this issue of perspective
distortion so now there's no blending no
popping just a single input image via
the single proxy just just and we
project the symbol on to the proxy
viewed from different places and we
should see perspective distortions this
paper will be at siggraph this year
again a few days so for this for for
example in this case
that does not look like a right angle
this is this is a single image mapped
onto a flat plane and the other one
looks slightly more acceptable so what's
the limit at what point can we say that
the user will object to it that at what
point the user will say yeah that does
not look like a right angle so the two
experiments were to understand this this
idea of distortions so we showed in
experiment one we showed a images all
these still images of facades with
balconies and we ask the user what do
you think that angle is and the way they
answered this was using a hinge device
it took us a long time to figure out how
to ask this question because if you put
sliders people will make mistakes all
the time so this turned out to be a nice
way people would see it and actually
replicate the angle in front of them so
we recorded all of this data in
experiment 2 we asked with a very simple
question does that look like a right
angle to you and people were asked to
rate this from one to five and both of
these experiments should give us data
that that matches and that's exactly
what because I won't go into how we deal
with this all of this data but the end
end result of all this was that we now
have a model of of predicting the level
of distortions for any normal view
position for example in this case on the
far right we you will see a top view of
the scene with the capture camera and
the proxy has used the two dots over
there are they are these two edges of of
the exclusion and you will notice now
that the user is trying to escape it
plays ok so now the user is trying to
sketch ni be rpath over there and red
means bad distortions and blue means
acceptable so this and and we can decide
the rating of this path using the data
we already had so this is one the first
ways of predicting how bad the
distortion will be without actually
running the application itself so I'll
just let it go over again so in the
beginning you see it's quite bad and
then it gets acceptable it's a very
Wiggly part and then it gets to be 0
a so what's the use of this we can use
this this prior knowledge to restrict
where does the user to restrict the
navigation zone of users for example in
this case we can only allow user to to
actually translate in except to say so
you can see like a blink effect where we
actually stop the navigation at that
point we don't allow the user to go any
further this ends we can do the same
thing for turning this allows the
application to restrict navigation into
acceptable zone this is something that
we do in games a lot we restrict the
movement of people within a fair zone so
the conclusions of all of this work is
first we should focus on PV poor
navigation vfb we have been doing
viewing w for a long time and that's
something we can handle quite well and
now it's about time that we jump to the
next level second important idea that we
want to push forward is we are trying to
do image based rendering which is
approximate by definition so we can make
approximations / / reconstruction over
data we get from reconstruction to do
something more than that thirdly because
we are using some approximate data above
depth we should be using some form of
regularization in the hip in the form of
image warps or something else rather
than just directly projecting all of
this data into noble views lastly I be
our choices should be based on
perceptual analysis and not what the
first author thinks is good looking so
this this work is actually being used in
an immersive space with stereo in a
European project this is actually work
in progress we are porting everything to
the cave in the future this will be the
starting application used for a European
project which will work on on on using
ibr for backdrops of games so there's
all of this code is already made the
games company in Italy for trying to
work their way out of it open questions
in image based rendering how do we use
machine learning how do we exploit all
of this information we get from machine
learning there's a lot of work on on
on semantic labeling of images and we
simply ignoring all of this can be used
this data to improve results secondly
they have all this nice ideas for will
be attending which I which have been
around for quite a while all the view
interpolation work by zip neck the work
on reflections last year and and asked
of where we focus on wide base line
stuff is there an approach which can
bring all of these nice ideas together
I'm not talking about a switch case
statement where you do this when in this
kitchen in this case and do something
else in some other case but is there a
nice way to combine the advantage of all
of these approaches thirdly I BR is has
been studied in so many different
contexts it's hard to compare one
context to the other context light feels
for example is one end where you have
hundreds of images for one object very
structured capture you can do amazing
things with this street level ibr is two
images for a whole building it is not
much you can do with it is there a way
to go smoothly from one and of the
horizon to the other end and how would
artistic effects we're trying to make
things look good we can use artistic
effects to hide artifacts and and give
nice or immersive experiences and and
some work is already there in the form
of ambient point clouds which use this
nice artistic effects 22 hayes out the
whole background and every time we write
an image based rendering paper we always
say it's real time it's all very good
but the real question to ask is because
this actually work on a mobile device if
the answer is no we have room for
improving the efficiency of these
approaches make them very lightweight
that's that's the only place where this
can be useful for and lastly we can use
image based rendering if all of this
works we can then use it as a nice tool
for virtual reality or four games
variable free where you have to make the
backdrops in very quick time after this
arms all of this stuff I've been doing
right now I'm working on a project
called halide which is a programming
language which isolates scheduling from
algorithm in image processing pipelines
so you can optimize both of these things
in different ways the code does not get
entwined you you don't have to make your
algorithm into
for loops and vectorization code so it's
clean it's it's supposed to make it is
it's being designed to make life easier
for for people who write code so this is
in in collaboration with the group at
MIT in terms of future research I would
like to exploit multi-view imagery in
more detail people do take lots and lots
of pictures but right now we have giving
them tools to edit only one picture at a
time which means that if he added only
one picture then you only want to keep
one of them of all the pictures you took
at the same time we are saying let's to
image based rendering which is a nice
innovative way of visualizing lots of
pictures at the same time so we have to
give them tools to edit whole data sets
at the same time not just one photograph
and the edits we are talking about can
be appearance manipulation so these are
some pictures I took and if i want to
change how the statue looks i would like
to do the edits in one image and get it
propagated to all the other images
automatically this sounds bit easy for
static scenes but once your scenes
become more dynamic if if if the scene
geometry changes if i have a friend in
in the picture and i want to change my
edits on my friend it becomes a harder
problem interesting nonetheless at the
same time we can try to change the scene
itself for example this is where we
empty the metro station in paris to take
pictures in that station and the problem
was that they didn't like the
advertisements they wanted
advertisements of 1950s so how do we
remove advertisements in one image and
get that to propagate overall 50 images
that we took over there and this has to
I mean respect all the parallax effects
and everything so that's a challenging
problem and what happens if I took very
few pictures and I don't have enough
data to propagate it from one to the
other can I use pictures which other
people took to do the same thing that's
the next level of follow this research
and finally can we think of these very
large photo collections as unstructured
light fields light field is very
structured capture a very small object
hundreds of images and then you
do amazing things off after capture can
we develop similar functionality for
this very large group of images can we
think of them as unstructured light
fields lastly I would like to keep
learning I would like to explore machine
learning in which in more detail work on
image propagation and manipulation this
is something which I've been wanting to
do in my PhD itself but never had enough
time to actually work on this properly
and I like being a part of groups which
to which will lots of other things a bit
of work in systems and stereo is
something which I would love to do and
and my last project is sort of an
example where I am working with a
compilers group to learn more and more
things with this I would like to thank
you all for your attention that would be
happy to answer any questions
so this yes yes yes yes I mean this is
this is completely real-time the pre
processing stages of course the like the
whole reconstruction pipeline that is
all offline but but the actual rendering
runs at 50 frames per second without any
problems because the least-square
systems are now very small easy to solve
we can use parallel course into all
kinds of stuff it's all real time but as
I said the overhead is still there he
still have to store like 15 images to do
rendering of a data set so how do you
optimize this how do you optimize amount
of story how how do you optimize amount
of memories one of the GPUs and other
things that's that's where I mean by
efficiency do you nice work another
analysis of how far can I go and distort
images for me yes why not let me try to
doing the opposite and trying to guide
people when they capture the images
where they should yes this is actually
something which which we have thought
about but we never got caught very
strong idea but it's actually a work way
so there's a paper by MIT where where
they're trying to capture a light field
and the system guides the person where
to move the camera around and that's
very cool but we would like to X ideally
you should be able to extend this to a
multi-view an environment where you
we're actually in this work we do get
some guidelines of how many images to
capture it doesn't tell us where to
capture but it does to like if you see
the heat map over there if you have one
image that gives you this blue zone if
you have another image next to it that
will give you another blue zone and then
depending on how many images you want to
use you can make the whole thing
completely blue if you use in lots and
lots of images so that gives us a
guideline on how on the number of images
to use this is assuming that you're
doing a street capture which is a where
there's a car and there's not much you
can do you can't go forward or backward
but for handled images it would be good
to to explore this field
and another question when you moved from
using a single global war for the entire
thing to using local warps for each of
the super pixels doesn't that mean that
the square pixels being warped
independently can introduce lots of
cracks yes it does and that's exactly
exactly etc exactly you do get this this
so so we expected this problem to show
up thankfully so as a consequence of
some accident and some design this
doesn't happen first of all because
we're using a shape reserving warp the
super pixels itself do not deform that
badly the third the way super pixel a
deforms is quite similar to the way BD
forms and if they were neighboring in
image at rest this sort of maintain
their shape in the next image yes yes
yes but you get to pictures from other
images which which don't do this which
will introduce which will which can be
used to to do some hole filling in the
cracks left in between that's exactly
why you use multiple images to fill
these small areas and give the parallax
effect as you go along it's interesting
that you don't notice a lot of either
popping or posting in those cracks I
guess because of what you said that the
work is well-behaved enough but right
it's a very enough it's consistent
enough one day brings new mrs. our Civil
War yes just what you'd expect if
they're similar
yes if if you're deaf was accurate then
it it will always be really pristine but
that's what the the word was supposed to
alleviate if you don't have exactly the
same depth and and some of it comes from
the blending heuristics because in some
cases as I said if we do notice
situations where you're not supposed to
blend these two guys so which one should
you a favor and that's when we say okay
choose the background a parallax error
in the background isn't is less
noticeable than apparel accident in the
foreground and that makes quite a lot of
difference a very simple idea but it it
it improves your experience by a fair
margin there is so much yeah right thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>