<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - Harri Valpola | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - Harri Valpola - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - Harri Valpola</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZlyqNiPFu2s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so to introduce Harry's work you know
some of you I guess the the the very few
of you that were around in the very
early days of the resurgence of deep
learning back in the ancient history ie
2007 or something like that well know
that in the very early days those of us
are around we had a very strong emphasis
on unsupervised learning a few of us
think yo sure right also Hong lack
marker really over their PhD thesis
still a lot of work on learning from
unlabeled data and then the whole
community shifted because it worked so
well to supervised learning and recently
I think Hari developed a algorithm
pretty complicated algorithm I should
say it's a bit of a mystery to some of
us why work so well that that is able to
do remarkably good unsupervised learning
so excited to have him tell us about
ladder networks which feels like a
exciting direction for doing much more
with unsupervised learning
well labeled a Thursday so Hari thank
you Andrew so I'm Harvey ball Paulo the
CEO and founder of the curious AI
company and I'm going to tell you about
the latter network which we've developed
here the authors of the paper and we are
all based in Helsinki most of us are now
with the curious AI company curiously in
the paper we presented state-of-the-art
results in several categories with fully
connected and convolutional models with
all labels and were very few labels and
this was possible because we have
finally got supervised and unsupervised
learning to work together properly and
I'd like to highlight these results from
permutation invariant amnesty with all
labels the state-of-the-art results have
come from fully supervised methods for
the past several years now we are
finally able to improve those results
with unsupervised learning and quite
significantly with 100 labels the
improvement is particularly dramatic you
know our result is better than what deep
learning got nine years ago with six
hundred times more labels
here's the fully connected network we
use for permutation in their indemnity
this animation shows standard supervised
learning with backpropagation so you you
get an idea of how forward computation
training target error and back
propagation are visualized and as you
all know this training results in a
hierarchy of increasingly abstract
features in such training humans are
typically the ultimate source of of the
abstractions but an interesting question
is could we find them in an unsupervised
manner note that lowest layers have
details which are needed for local
reconstruction but higher layer
abstractions are needed for bridging the
gap over long distances in space and
time and between modalities now imagine
you are a feature detector in the middle
of the network and you get all kinds of
input patches in your receptive field in
the unstable again unsupervised case how
do you know which of all the features
are relevant in other words where do you
get your training targets from the
answer we and many others have proposes
that the targets can be derived from the
context as expectations in other words
the context can provide virtual labels
for the feature detectors inside the
network this type of learning is
well-known to happen in hierarchical
latent variable models the only problem
is nobody has so far managed to make
inference and learning efficient and the
model flexible all at the same time so
our solution is to directly learn the
whole inference process rather than
derive it from a statistical model this
solution is compatible with basically
any deep learning architecture for
instance we have used special
normalization and and so on we simply
add a decoder with an auxilary denoting
tasks the solution is efficient and
scales just as fully supervised Network
and in this talk I'm going to explain
how the ingredients of the ladder
network are combined together if I will
have time left I will also tell you
about some cool new results so let me
start with denoting source separation
let's take the feed-forward Network we
saw earlier zoom into one of the hidden
units and study its activation histogram
in a typical well-trained Network these
histograms are typically non-gaussian so
let's then visualize the Joint
Distribution of two of the inputs to
this unit of course there are many other
units but here are two every dot is in
this cutter plot this one observation on
any layer there are typically very many
units with all kinds of information so
if we take a random projection we are
likely to end up with the rather
Gaussian distribution in other words a
good feature detector will likely have a
lower entropy than a reference Gaussian
with the same variance this difference
is known as negentropy and maximizing it
is a well-known criterion in independent
component analysis sparse coding and so
on so
mmm sorry
so we can train a better feature by
encouraging the activation distribution
of the unit to be more non-gaussian here
you see how the projection vector turns
as a result of such training and what's
more we can back propagate this training
signal to lower levels of the network
which will be encouraged to transform
the input distribution so that the
detector will be even cleaner this type
of learning is pretty much what happens
in hierarchical latent variable models
so now the question is where do those
red arrows come from a well-known
solution is the e/m algorithm where we
alternate between e and M stem and the
AE step delivers the target for learning
as an expectation and is where it works
as follows we start with activations
which are assumed to contain noise from
noisy observations we then postulate a
parametric model for the observation
distribution and train its parameters
this model can be hierarchical so the
model can represent very complex
distributions then we take the model of
the distribution and evaluate the
posterior expectation analytically or
approximately the result is a training
target which points to a higher
probability it's important to note here
that this vector field is just an
alternative way to represent the
underlying data distribution me and my
colleagues have been developing a
technique called denoting saw separation
and it's an efficient extension of many
saw separation techniques and built upon
the long research tradition in
unsupervised learning in Helsinki the
key idea is to skip the probabilistic
modeling part and represent explicitly
the e step which can be seen as a
procedure to denote observations it's
often very easy and intuitive to define
such a procedure but now we will study
another cool technique which is able to
directly learn this procedure in turn
the nursing outside order denoising
auto-encoders were developed here in
Montreal in your show Benji's group they
are clever extension of auto-encoders
and their task is not only to
reconstruct but also to denote and
interestingly this encourages the
denoising auto-encoder to implicitly
model the complete joint distribution of
the data so let's see how it works
basic idea is to corrupt the input with
noise this will move the data points in
random directions
when we look at the whole data
distribution we can see that these
random movements create a diffusion flow
which points in the direction exactly
opposite to the arrows we need it for
the e step so now you can take an
outside order and train it to D noise
here you see how we present noisy inputs
and clean targets to the network this
will directly learn the procedure for
computing the e step with enough
capacity the network will be able to
represent very complexity steps and
therefore implicitly capture very
complex distributions at least in theory
there is one big problem with this setup
still remains and I'm going to explain
how we solve it the problem is this
since the goal of the denoising
auto-encoder is also to reconstruct it
will have to hold on to all the details
it's not possible to reconstruct from
the abstract features alone this is not
the type of Reps in abstract abstraction
hierarchy we are the supervised learning
task would expect from the encoder
supervised learning would like to
discard the details while the auto
encoder tries to preserve them the
solution I found for this is to imitate
the computational structure of inference
by message passing in hierarchical
Edinburgh all models the encoder
corresponds to the forward path which
starts with the data and where we
essentially represent the likelihood of
latent variables given the data and the
decoder corresponds to the backward pass
which computes the final posterior
probabilities crucially these paths have
to interact in much the same way as when
we combine likelihood and prior to form
the posterior another way to say this is
that shortcuts from the encoder leak the
details to the Ingo encoder decoder
allowing high layers of the encoder to
discard them still I find that it's more
instructive to see this as an inference
procedure rather than reconstruction a
related idea by Marco aurélio runs out
o here and others focused on the
invertibility of max pooling they
allowed the pooling indices to leak to
the decoder but now we leak much more
and we get away with that since the auto
encoder is not just trying to copy but
also 2d noise
this will encourage the network to
implement complete Bayesian inference
rather than just copy the noisy
observations so we have used unit wise
shortcut connections and that
corresponds to assuming conditionally
independent latent variables any
dependencies can and will still be
modeled by the higher layers of the
network so this does not restrict the
ability to represent complex nonlinear
and non Gaussian dependencies and this
is important for modeling complex
real-world distributions so putting it
all together
by asking this expressing network 2d
noise we have turned it into a powerful
distribution estimator so now we are
ready to connect the dots we start by
looking started by looking at how to
train a single single hidden unit but
let's now zoom out to consider training
a whole layer of units since large
networks represent information with the
distributed population of units we take
an encoder which presumably implements
likelihood computations one way or
another
then we add a denoising auto-encoder on
top of it for computing the to learn the
procedure from computing the e step for
the whole layer by simply minimizing the
squared error between the output and the
likelihood computations and the Dinos
expectations provided by the implicit
prior model we trained both parts of the
model at the same time the lower part
learns lang denoting saw separation and
the upper part as a denoising
auto-encoder in order to guarantee
efficient learning in very deep models
or with long time delays we want to
apply this type of learning on many or
all of the layers of the encoder in
addition to standard supervised learning
on top of the encoder so if this encoder
had for instance 20 layers this would
correspond to 21 learning problems
however since our denoising auto-encoder
is compatible with abstraction
hierarchies we can combine all these
networks into a single network which
looks like a large denoising
auto-encoder
with lateral shortcut connections and
noises added on each layer of the
encoder
but since we still need the clean
targets to train the decoder part we
therefore we evaluate the encoder also
without noise so ladies and gentlemen
meet the latter Network it's really
simple but surprisingly powerful on
every layer the clean encoder provides
the target for the decoder and vice
versa we also have the supervised target
on top of the noisy encoder and the
corruption noise regular rise is
supervised learning in much the same way
as dropout for the price of three
encoder passes we are therefore able to
implement learning on every layer of the
network there's a training cost right
next to every parameter of the network I
have some time so I can explain this
extra right I pick just one one thing
which I think is important and that's
semi-supervised learning of sequential
data we have some other results too but
no time this is a real-world video clip
from our collaborators at Nvidia
Helsinki they have one of the best car
detectors but you can still see that
they soon see how the detection is
flicker in and out of existence so here
frame by frame this clicker means that
temporal context could be used for self
labeling so in order to make efficient
use of temporal context which you can
find for instance in these kinds of
videos we expand the latter Network in
time dimension we turned the ladder into
a recurrent version by applying it on
every time instance and connecting the
encoder forward in time and the decoder
backwards in time admittedly it looks a
bit messy and more like a scaffold than
a ladder but note that we had a very
good reason for implement in temporal
connections this way because we are
imitating the computational structure of
message passing in states based models
so the encoder basically implements
state estimation analogous to Kalman
filtering or the forward path of Viterbi
algorithm if you work with hidden markov
models and the decoder implements the
backward pass which is analogous to
Kalman smoothing
we're backward passive Viterbi algorithm
so you see how this idea of message
passing is general and makes it very
easy to extend the larger Network so now
consider a simple illustrative example
where we design a Markov chain out of
Emily's digits the hidden state
corresponds to the digits class and
every state emits one of the digits in
the dataset all the information about
the state is contained in the digit
class state transitions are
probabilistic with incremental one being
the most probable so zero one two three
four five and so on but influence of two
or three are possible too as a side note
suppose you would like to learn this
internal representation with a deep
recurrent neural network that tries to
learn to predict the next frame using
the normal means great error loss the
trouble is only the digit class is
relevant for prediction and therefore
the optimal prediction just the class
mean even if the state transition is
deterministic a probabilistic increment
means that the optimal prediction is a
mixture of class means and that trouble
here is that while it is in principle
possible to create the joint
distribution of the pixels of next frame
it doesn't help much at the level of
individual pixels to prove the point we
will completely remove the whole
subspace spanned by class means making
it impossible to predict anything at
pixel level with mean squared error loss
even in principle yet a completely
unsupervised recurrent ladder network
learns a very good internal
representation in a fully unsupervised
manner we don't have to tell it that
there are ten classes it will
automatically find a distribute
representation which turns out to form
ten clusters the distributed coding even
captures similarities between similar
looking classes and classes that are
close by in the sequence much like in
word embeddings finally let me point out
that in order to scales unsupervised
learning to really large problems I
believe we have to pay attention to
relevance it's well known that in humans
attention plays an important role in
learning luckily the latter network is
fully compatible with such attentional
modulation as we have shown already in
2007 in a cortical model whose
simplification the current ladder
network is
so to summarize we solved unsupervised
learning by letting go of our in inner
statistician and letting the latter
network learn his own internal
representations of distributions and how
to do inference on them the result is
compatible simple efficient and
available for you to experiment with in
github thank you
um to be rough questions for Ari
maybe let me ask personally I think you
presented a few on semi-supervised
learning results show um where he had
labeled em in stitches and unlabeled
Amnesty Jets right um
many years ago Hall act and some his
collaborators proposed a related
framework called self taught learning it
was a very simple insight that you can
use unlabeled data from a totally random
distribution not just on label and
assistive images but maybe random images
downloaded off the internet and the
insight and self-taught learning was
that you have essentially unlimited
access to that data
if you don't constraint so have it come
from your test distribution have you
done any experiments along that lines
things we haven't but I admit it would
be very interesting and we could try to
just fit in with lots and lots of data
and then try to see what kind of
representations it's learn on the other
hand I believe that in order to get
really really good semi-supervised
learning working we have to do them
together so it's not enough to just
first two unsupervised pre-training it's
important that supervised learning can
give an idea of what's the rough
solution and unsupervised learning can
then find you in this representation so
it's actually turning upside down what's
being done before so instead of
unsupervised pre training and supervises
fine-tuning is it's like the other way
around
well not supervised pre-training might
not be a great idea in some cases but
anyway i think that it's very important
to how both of these tasks at the same
time
then again unsupervised free free
training does help no question about
them any other questions right cool okay
so my question is if you didn't have the
supervised information on the top of the
labels and you only had access to
unsupervised data what prevents the
higher layers from not learning anything
oh they do learn something but I mean
that if you have really large scale
problems like image in it how do you
know that the highest layers will will
be relevant for your task which there
are so many different things to learn if
you think of natural video for instance
and if you're only interested in cars
and things related to traffic you should
definitely put that knowledge in but
yeah in principle you can just learn all
kinds of things so I think what happens
is that the lower layers want to
minimize that annoying error of the
upper layers and in order to do that
they're trying to provide something that
their upper layers can continue is
right sewed up
all right oh great with that let's let's
let's thank Hari and I'm definitely
gonna go to your github repository
you
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>