<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>RealTime Collaborative Analysis with (Almost) Pure SQL: A Case Study in Biogeochemical Oceanography | Coder Coacher - Coaching Coders</title><meta content="RealTime Collaborative Analysis with (Almost) Pure SQL: A Case Study in Biogeochemical Oceanography - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>RealTime Collaborative Analysis with (Almost) Pure SQL: A Case Study in Biogeochemical Oceanography</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4wOiJHH01gI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so thanks for coming I'm Ralph Watling
in microsoft research connections and
i'm here to introduce Dan halperin who
has formerly been here as an intern in
the NRG group with victor ball doing
wireless networking in data centers and
is now at University of Washington East
Science Institute think I'm getting any
that wrong and then talk about sharing
sequel with oceanographers thank you all
right hi everyone thanks for coming I am
here to tell you today about an
experiment we did where we tried to
figure out whether our query is a
service system which is designed for
scientists would actually help real-time
science integration meeting and this is
joint work with all these people you see
below and in fact here we are and the
the key point of takeaway on this slide
is that and this project the top three
of us here are computer scientists
working at u-dub in the computer science
department and the East Science
Institute and the bottom three here
oceanographers from u-dub and from Woods
Hole Oceanographic Institute this was
actually a joint collaboration both with
these people on this paper but also with
a wide collection of oceanographers as
i'll tell you more about later it's also
actually a pretty short talk so feel
free to throwing questions at any time
so one of the beliefs in the scientific
computing community is the databases are
really good fit for science and one of
the reasons is that you know there's the
there's a real flexibility and power to
the relational model so the standard
language of databases right you know
joining data together filtering it and
other times data processing very well
fit into this obstruction and
declarative languages are great because
they're fairly straightforward you know
almost English like languages in which
you express what you want without
worrying so much about how you get it on
the other hand it turns out that people
really aren't using databases that much
in science now there are a few there are
a few existing successes and there's
some pretty dramatic ones like the Sloan
Digital Sky Survey which was the project
that came out of Microsoft Research Jim
Gray things like that but they're really
high touch so if you need someone like
Jim Gray plus a few other people to take
a few years
their time to build something you're
really doing it wrong right that's not
going to scale so there's a lot of
people working on a gun sort of both
existing solutions and new solutions
that are tailored for sort of database
technology for scientists but if we
really want a claim we're making impact
we need some more field studies of
artwork right we can't just throw them
over the wall write a paper and say look
we have this cool new thing so this is
one of the challenges we're tackling
with our existing work doing queries a
service for scientists that's when I
tell you about in this talk so imagine i
said to you a bunch of scientists are
going to come together at a data
synthesis workshop and what they're
going to want to do is they're going to
want to do real-time collaborative
analytics so they're going to be coming
together from different labs bringing
each one bringing their own data say I
process some data about how much iron
there is in the water you analyzed what
nutrients there were and we want to see
if we can learn things by putting our
heads together and so what they're going
to be doing is they're going to be in
this you know they're in this room for a
short period of time they're going to be
generating questions they're going to be
integrating their separate data sets
together and they will be trying to
analyze and visualize the results on all
this on the fly so the question we asked
is could we actually help scientists use
that facetime in a more efficient way so
the way these meetings might work
generally is scientists would say okay
here's my excel spreadsheet there's your
Excel spreadsheet let's see if we can
sort of copy and paste things in the
right way get the columns lined up and
then use them some graphs maybe some of
them would right sort of on-demand
Python or our scripts that would do a
little bit more complex integration but
our answer is that we think that sequel
for science is a good solution here so
want to talk about this you know how
would we use sequel for collaborative
data integration well on the one hand
it's a great fit for databases right I
mean sort of the language of databases
is data integration analytics right
things you know express things as joins
and projections and aggregates and
ad-hoc innovative queries are really
easy to write when you know just by
modifying sequel here and there so for
instance you can refer to call us by
their names instead of say in Python
where you might have to like do some
wyandotte split and then index on the
column and things that really make you
unhappy at the same time for scientific
data this is a terrible idea because
most science most data is generated in
lab notebooks right which means it
doesn't have this sort of well
engineered schema if you take a database
class where you go work at an impression
basis that's you know database is 101
the first thing you do is you work out
the schema for your system what keys you
need what indexes you need so on and so
forth and scientific data just does not
have that property so there's a tons of
different formats of data and there's
absolutely no schema so this is not a
talk about sequel share but I'm tell you
a bit about the service we built called
sequel share and how it works and so
what you do is scientists for the most
part upload their data as is so we have
a sequel as your database running here
at Microsoft we have also you know some
some Amazon front ends for rest
interfaces in an azure cloud deployment
that helps sort of mediate like a rest
server and things like that for the
database and the way you get your data
into the system is you just take your
you know your CSV or your Excel
spreadsheet and you essentially drag it
onto the website and the website will go
okay this thing looks like it has rows
and columns I'm going to guess what the
column separators are I'm going to if
you have them in the first row extract
the call names if not i'll call them
column 1 2 3 and i'm going to sort of
automatically infer what the types are
you know are these numbers defaults of
strings if we don't know what's going on
and the key is that all you have to do
is drag your your csv in right you don't
have to care about the schema you don't
have to do any ddl saying like you know
here is the table we're going to create
once you get these data sets in you can
now create queries in sequel share by
writing sequel views so you're in a web
browser you're online you can say you
know select this column out of that data
set or filter rows where the salinity is
not is not negative you know which means
essentially not a number
I mean essentially anything rows and
columns and we have custom importers for
lots of other formats it turns out
though that things like even blast
databases are rows and columns if you
want them to be now this also means are
you know fasta file so I rows and
columns if you want them to be your GF f
files sorry I'm pretty sure we have a
netcdf importer although I haven't
worked with that specifically yeah but
so and one of the goals here right is
that you're expressing computation here
only as view so you're writing declared
of sequel statement and you're writing
later later data processing is done by
writing a new view on top of the old
view so you get views on views on views
all the way up to the top and you get
sort of derived results and then once
once you've sort of processed your data
in some way that you like well then you
can export the data so you can do things
like you can just share it publicly on
the web so our service will just say
here's the data set you know um you can
share it with your collaborators so that
picture if there is a front swab
Rabelais our collaborator at u-dub and
then also there's you know there's a
REST API for doing mashups on the web
and so in this experiment our hypothesis
was that sequel as a service would
enable better real-time collaborative
data analysis in particular because when
someone said oh I want to join this data
in that data or oh can you filter out
the outliers well those things are easy
to express in sequel not only that but
we thought that the service part would
be very important because you know the
web the web pneus of this and the fact
that you can access it from any browser
and that you don't need to install on
the infrastructure to get this up and
going is a key enabler that
interactivity and so what we did is a
field test with oceanographers and so
you know obviously the primary goal here
is to make this data integration meeting
go really well but the secondary goal is
to learn you know what can we take away
from how you should be doing data
enabled collaborative analysis in
general so how you know can we create a
replica procedure for did it enabled
meeting so here's the setup we did in
May 2012 there was a research cruise in
Northeast Pacific Ocean so that ship
there is the RV Thompson Richards
festival Thompson and 40 scientists from
33 labs at 20 institutions spread across
the u.s. all inside the US I think in
two different oceanographic disciplines
came together to take their data on this
boat and so there are a whole bunch of
different types of ocean
ographers in here they self divided into
two camps the one group people called
molecular ecologist that do things with
DNA and protein sequences and the you
know the men all kinds of genomic like
things and also geochemists who are
studying you know the chemistry and
physics of the ocean so how much iron is
there how much salinity is there and
things like that and after the cruise
everyone took their own data home to
their own lab and they process the
samples and did some preliminary
analysis so they could say you know okay
here's the csv file saying when i was at
you know at this station at this depth
this is how much iron i saw things like
that or at this station at this depth
these are these are all the protein
sequences i saw in various abundances
and then in fact we 2013 so earlier this
year we all got together at u-dub friday
harbor lab to try and synthesize this
data so to put their heads together and
see what they could learn and two of us
to you know computer scientists that
we're just getting familiar with sequel
and sequel share attended as what we
called sequel stenographers so our goal
was to sort of listen the conversation
whenever some would say well you know i
wonder if this data and that data
related we would go find the datasets
we'd write sequel queries to integrate
them and then we use our you know rest
interface and other tools we've bitten
built to actually go visualize that data
or maybe even export it for
domain-specific analysis and the goal
that was really to see you know is seek
will share a good fit for this type of
thing yeah we were doing what was your
typical wait
between somebody raising a question if
you started starting to touch the keys
and cancer yeah so we'll get there um it
was it's pretty high it's pretty high
variance so depending on how familiar we
were with the data and how much the
question was like something we'd already
answered it was either really easy a
really hard and I'll explain more about
that later in the talk uh so in general
what i'mma tell you the rest of this
talk is sort of the three phases we went
through so obviously like you know for
me I'm a computer scientist I know
nothing about Oceanography at all and
I'm handed a pile of spreadsheets and
told next monday we're going to be
asking you questions about this stuff so
what do we do before the workshop to
prepare for this meeting how did we go
at the workshop and then from this whole
process what did we learn so of course
um this tests before the workshop I
think are pretty straightforward at
least at a high level what do you do
will you collect all the data from the
scientists and you load it into sequel
chair and that's its own kind of fun
process of you know getting different
CSVs and figuring out how to load them
figuring out what little things you need
to debug in them the other thing we did
ahead of time is collect sort of what
you call the 20 the 20 questions here so
you asked scientists ahead of time what
are questions you think you'd want to
ask at the meeting just to give it to
get a representative sample of the kinds
of things they want to ask and then we
spent the weekend taking those questions
and trying to figure out okay how can I
turn you know some science jargon here
into something I understand how can how
can I take the things I've extracted and
figure out which data sets that date is
actually embedded in because the data
sets all come in with you know science
provided names sometimes little
annotation things like that and then how
can i connect these days hits together
and how can I answer that question so
the data collection was an oceanographic
research cruise along what's called line
P this is sort of a standard transect
that the oceanographic graphic data is
collected on so there's tons of the data
there and you can see there are eight
stations p 1 through P eight and we stop
to all them except for p 7 and they
collected three types of data on this
cruise so there's underway data which is
data you take continuously well ship is
in motion so it's usually surface data
because you can't lower instruments
while you're while you're in motion
really and not in control the depth at
least so things like you know at the
surface how much chlorophyll is there
what's the temperature what's the
salinity how does the atmosphere look
like there's
lot of sample data so one of the biggest
kind of both data collections is that
when the ship is on station save stops
at p1 it's going to lower a bunch of
instruments and say you're going to have
bottles that snap closed at certain
depths you're going to lower to capture
water for later analysis you're going to
have pumps that you lower to a certain
depth that have filters to try and
collect microorganisms of a certain size
that you're then going to going to you
know count and sequence later and so
that's the sample data and then there's
the cache data which is continuous
measurements of the water column so as
you're lowering these instruments to
snap shut samples of specific depths
you're also going to say you know keep a
trace the temperature salinity
connectivity all the way down and so you
sort of had these three different types
of data and for this cruise where this
data integration meeting was an explicit
goal we had we had a little bit more
control over the data schema and this is
why sort of the you know the question of
our is it always CSVs well for this
meeting we we had a bit of an advantage
here and so for oceanographic data in
general latitude longitude depth and
time form a natural key right it's
centrally like where you are and when
you are and so you know the location
either the station is sort of map to
latitude longitude and if you're at the
surface you know your depth is zero but
for the most part it's it's very simple
and Francois our collaborator who had
some experience with equal share develop
this template on his own we didn't have
input into this and sent this out saying
you know please please just put your
data in and so you can see on the Left
we have these key columns and there are
some redundancies in them and there are
some sort of surrogate keys that map to
the whole hookey and then you know the
idea was the scientist would go in and
insert their own data on the right now
this is some organization but what it
doesn't solve is any of the integration
and cleaning problems so for instance
under way data here's what it might look
like so you have columns like datetime
latitude longitude temperature you know
air temperature conductivity salinity so
you can see you know lots of variables
at along these samples and then we also
have some external data sets so for
instance this one here is a bathymetry
data set which means just what's the
depth of the ocean floor at each point
so this came off some website and we
have some data sets about things like
protein sequences so you know we found a
protein it has this function you know to
two out of five you know so still like
forty percent of the annotation seems
related to this function and forty
sent to that other function things like
that so looking at what kinds of
proteins we see there's a lot of things
like instrument calibrations so these
kind of single row tables that say oh
well you know when I calibrated my you
know mash tun measurement these are the
this is this the fit I found it so
that's sort of relevant later so in some
sense what we're trying to do is you're
trying to capture a lot of the data you
would see in your lab notebook and your
sensors and your spreadsheets and you
know put it into the database and so you
might ask you know sort of like how
these data sets look well we ended up
with 47 of them and these two graphs
here show the amount of data in terms of
the number of columns the data points
and the number of sorry the number of
rows data points the number of columns
sort of features measured and so you can
see on the left side which you should
notice a log scale almost all the data
sets have fewer than a thousand rows so
not that many data points per table the
biggest have 10 million rows and these
are things like protein function
databases or protein sequence data in
terms of the number of columns the
number of variables you measure most the
data sets have sort of between 10 to 20
variables or fewer and the most data set
has 69 columns and so here they're
measuring all kinds of variables and
then some of these columns also include
things like error estimates but one of
the key takeaways here is that for
experiments like this you know when you
hear big data you think of the 3 v's
variety volume and velocity right and
here it's not really high velocity it's
not really high volume it's mostly high
variety just tons of different kinds of
data you're being put together so you
know you get the data and then you've
got to get the questions too and so some
of the questions we got ahead of time
where things like what is the
relationship between virus concentration
and temperature or when iron
concentration is below some amount some
concentration here how many iron related
proteins are detected in that squee
based on protein annotations so we spent
the weekend trying to manually translate
these into sequel identifying data sets
we needed the last thing we built ahead
of the workshop is a bunch of tools for
online visualization and export so in
particular sage is just a Python sort of
plotting library so we set up a little
rest connector for for python that would
connect a sequel share and let you say
give me this table giving these two
columns and throw it on the screen it's
very simple but it was actually pretty
powerful because once we set up we want
sweet
rated some data we could just go plug
the table and name and over here and say
oh here's the graph of slowing diverse
connectivity so you can see this you
know there's no design on this right
it's just sort of like the ugliest
possible simplest graph you could see
but you get a really nice plot that is
useful for sort of answering question on
line now the other thing we did is build
an adapter for ocean data viewer odv and
this is a domain-specific tool that
oceanographers use all the time so you
load your you're essentially your
spreadsheet in and then it starts
letting you make plots like this so on
the right here you see a map it's a
depth chart so on the x-axis you have
the latitude so as we're going west on
the cruise on the y-axis you have the
depth of the instrument each yellow dot
there is a sample point so it's one of
those this is um ccd data so it's you
know it's along the water column
continuously sampled and I believe what
we're plotting is fluorescence and so
OTV automatically sort of interpolates
these colors saying like here are
interesting fluorescent patterns in the
top hundred fifty meters of the water
column sorry the vertical yellow
datasets to your peas your station
you're stalking yeah exactly so that's p
1 through P eight with p7 missing um I
think at least maybe there's one station
off to the right that's not there but
exactly and one of the cool things about
this even right away we started seeing
is that in odv actually format
conversion is pretty much just a query
so ocean dataview takes tabular data but
then usually when you import the data
what you have to do is you have to sort
of say manually like this column is
latitude this column is longitude this
column is temperature because whatever
name you used in your data set is
actually you know it's not our Matthew
detective so it turns out you can build
some simple tools if you have this web
service that say go get me the column
names let's introspect on them let's see
oh this one looks like its latitude and
let's permute the columns and remade
them to the thing that anybody will
recognize and so of course you know you
can automatically sort of permuting
rename things handle bad values and get
some file that will graciously load in
20 DB so we answered those questions we
plotted some things what do we do at the
workshop so we'd get some question like
this and we'd say okay you know we want
to visualize the ratio of iron to
nitrate concentration so it's this ratio
of fe and 0 3 and so we'd say okay well
how are we going to go find the data at
all so this is you know the screen that
shows you in sequel share this is all
your data sets and you can see that they
have
names they have text descriptions and
they have tags and sort of how well
these are populated depends on who's
using it and what they've done we're
working on automated systems to do
better so when the first ways you might
find data is you might do a keyword
search say you know show me the data
sets that have iron is a keyword you
might also look at these text
descriptions in English and you might
look the data set names themselves and
so once you find the right data then you
join the data sets together and so this
here is actually sort of the
straightforward sequel code to join the
two tables on station and dip so we have
one table here called nitrate isotope
data another table here called iron
concentration join with iron related
proteins and we're writing sequel here
that says you know give me these columns
out join the tables and you know make
sure the stations match and they are the
same depth and we didn't get an na for
the nitrate data right so you get some
some sort of data like this and you can
preview the results and by getting this
live preview off in the sequel there and
getting this live preview you can of
course iterate this right you can
actually debug your queries and see what
happens to do this in real time and so
once we did that as I mentioned we would
then connect this up to our sage
visualization engine and so you can see
here on the right this is the Python
code and for the most part all it's
doing is loading our little sequel share
connector and then saying here's the
table name and columns and let's go plot
it and what was cool about this is you
know since we had this little Python
script here we can sort of in a very
online real-time manner we can say oh
you know another column has error bars
let's just plot error bars on the sides
of them and so here this is you know so
total amount of iron with error bars
again step at the different stations and
then of course in odv you can make these
visualizations like this just sort of
very naturally using these domain
specific tools and this is one of those
graphs that was one of the major results
from this from this conference and so
from this meeting and so what's
happening here is that we're plotting
the the amount of zinc in the water
against the amount of cobalt and you
know you see this nice sort of
anti-correlated trend in verse linear
trend here um but then these points down
here at the bottom left are really
interesting so what the people who know
the metals dataset is well you know
these organisms in their internal
processes they really prefer to use zinc
however if there's not very much think
if we're
you know zinc starvation then they might
actually substitute cobalt in for zinc
and so maybe that's what's happening
down here and these points where when
you see when zinc is low cobalt goes
really low maybe that's what's happening
and we need to go investigate this
further in particular we need to go look
at the protein sequence data at those
points which unfortunately was not ready
at the time of the meeting so in general
the process of answering these questions
however you have to do a bunch of
debugging and repair tasks so when you
find two tables that have the data you
want will you go wait a minute they
don't join why is that maybe it's
because the units are wrong right one
person use degrees in one format one
person use degrees in another other
times you know you have to fix a typo
like there was one person who you know
the station convention is p1 through PA
tande someone just wrote one through
eight it's like oh I have to insert the
string p there well that's pretty easy
other times you know people often put
sort of data in the file names to end of
a file called station one in a file call
station two in a file called station
three well intake in sequel right it's
really easy to you know select star from
table comma value as field so you know
select everything from file one and then
add a column called station as p1 and
then Union those together and so now you
can actually you know sort of trivially
clean up and integrate data and add data
back in some times tables have the wrong
format so you know it's in when you're
writing a lab notebook right you often
some sort of unfit the data so you'll
have you know station here and then
you'll have variable one value variable
to value variable three value but these
are really the same variables right
maybe actually station names in the
column saying what you really want is a
table that says you know station depth
value station depth value instead of
station depth depth depth depth value
value value value because you want to
ask queries about that and so you know
in sequel you can easily pivot non pivot
tables other times they would join two
data sets together and then go you know
why is there an exponential increase in
the number of points what happened why
did everything get squared and you go oh
well obviously when this scientist did
their data they had replicates so they
had three different measurements at the
same you know physical point and when
you did your data you had replicates and
so now when you join them together
you're going to have nine values then go
oh yeah well I want my replicas I don't
want there's just throw an average in
right and so it's really easy to once
you see the joint recognize these
problems and then adapt the query to fix
this another case is you get simple
things like when you're visualize
wait a minute the maps in the long part
of the world oh this person swap lat and
longitude but the big takeaway here
right all of these are sequel statements
this is like very simple yeah I dream
potentially automatically detecting this
kind of situation like wait IDs like
suggests fixes here's the SQL you need
to paste in to pin you need to make the
stables lighten up person yeah so the
questions about sort of automatically
detecting and recovering from these
problems and this is absolutely a goal
right so one of the things we learned
one of the biggest things we learned in
this whole experience is just sort of
the variety in the number of ways things
can go wrong and one of the really cool
things about scientific data is that for
the most part it's read-only so you know
sequel server actually right is designed
you know whatever power Airlines they're
going to be buying tickets transactions
a really important online transaction
processing is sort of the keyword you
might hear a lot and scientific data
doesn't have that property so we're
thinking a lot about like you know like
given the workload from this experiment
if they go do another cruise and throw
all the data in could we sort of
automatically just go and join all the
things and draw all the same graphs but
of course you know they're going to have
different formats right they're going to
have pivoted data they're going to have
added columns they're going to have
change units because they're using a
different instrument they're gonna be a
different station names and so sort of
there's a lot of interesting work about
how could we recognize situations and
automatically figure it out um and so
you know that leads me writing sort of
the last part my talk which is what have
we learned from doing this whole
experience so a bunch of things went
right so what you know one of them was
we were we were really able to answer
most queries and when we weren't it was
because sort of the question was crazy
and I'll explain that in a minute or
because the data just wasn't there yet
you know we sort of the the protein
sequence date is pretty hard to actually
get going from what I understand on top
of that you know the queries really were
fast enough with no tunings as I
mentioned early the day sets were
actually pretty small and variety was
the challenge and so for instance right
ninety-five percent of the of the sequel
queries we issued finish within one
second and they all finished within 10
seconds so we weren't waiting around the
database itself that much and finally
you know the the conclusion from the
scientist is that this was really super
useful so the interactive hypothesis
generation and testing like you know hey
if we plot viruses versus bacteria will
we see a correlation and oh I had this
cool way of measuring certain
populations can we see if those are a
huge fraction of the viruses we say well
no not really so we could do this very
interactively we could also do
interactive quality control the data
right so that sort of online inspecting
the VIS or the the previewing results
let you right away realize where things
are going wrong and we could also do
cross validation across disciplines
which is one of the key goals of this
meeting and of course the other thing we
found here is that this service oriented
nature this was really invaluable so all
those second step tools right the
visualization the analysis and even sort
of the online querying in the web
browser they all rely on the fact that
we have this rest interface up and you
know we were able to spin up those tools
like the ODB exporter took me like 12
hours right and it's because you know
rest is such a nicely defined protocol
and it's they're good libraries forward
and you know it's just easy to do those
kinds of things at the same time a lot
of things went wrong so for instance
number one there weren't enough of a
sequel author of people so you know we
just we just had way two little people
to get everything we wanted to get done
and the other thing was as Rob hinted at
earlier there was a pretty high variance
and how long it took us to answer a
question so once we've gotten bootstrap
kind of after the after that weekend of
prep and in the first you know half hour
of the meeting and frequent took us less
than a minute to answer a question and
part of that was just due to the crappy
connection of the internet things like
that but then there were also a lot of
longer cycles so 5 15 minutes for either
like new query types or bugs in the
tools so for instance one time someone
said hey you know I really want to see
the correlation between these two
variables and I'm stick to going okay
well I can definitely do correlation in
sequel right I know it can happen but
you have to give me total break to
actually work at that idiot I'm doing
too much other stuff right now and it
turned out later that person actually
just wanted to pull out them so you know
that person I was able to answer quickly
but you know for those new query types
it's like working at the idioms on
demand is something we're working on it
so it would help to have a library of
these idioms which is something else
we're developing there are also some
tool bugs so it turns out that most
browsers and web sort of client
libraries limit urls to to hunt to two
thousand characters and so the interface
we were using you know if you're getting
query answer you want to use a get
request your data is in the URL and so
if you
automatically generating sequel queries
over you know 100 columns it's pretty
easy to hit that 2,000 character URL
limit and then and then you cry and have
to figure out a real-time workaround um
and the biggest thing here is you know
we uncovered a new research agenda so in
general right and this has been hit out
on some past slides right scientists
should really be authoring queries right
we shouldn't have to go to these
meetings and so some things this has led
us to is a bunch of education and
training resources so we have a github
project open on sequel share and we have
a bunch of labs but mostly the Roberts
lab at u-dub who actually are using
people share on a daily basis and
whenever they have a question you know
hey I'm getting this weird error they
actually create a new issue on github
like it then gets mailed to me and I go
in there and I go oh you know here's
sort of what's going on try and explain
what's going on and then we take all
those issues and we've turned them back
into wiki pages where you know they had
built up a library of you know here's
the sequel view that turns a GFF file
into a FASTA file and I may be getting
those two things wrong but roughly it's
like you know permute the columns to add
one because 10 next one once one in next
or so they have a whole bunch of common
domain specific task for their genomics
work that the wiki is getting coffee
with idioms and they're getting really
good at copying and pasting and adapting
the queries and and we're trying to
convey the debugging but we have a lot
of other ideas about about how to do
better education also things like sort
of you know GUI like authoring tools
where you know you say I want to join
this data set in that data set and
instead of giving you a sequel prompts
we sort of help you do it with some more
visualized way and then getting to where
Adrian mentioned right using sort of a
query autocomplete so we have some
preliminary studies that say imagine
we've seen scientists query of this
corpus for a lot right and now you start
writing a query select star from table
join table and it says okay well here's
you know 20 where clauses that are that
seem to be relevant waited in some
interesting way we found that those
sometimes helping you know getting this
work kind of more robust and also just
just pushing more on it basically
something we're really interested in one
of the other things we learned as a
consequence is that the visualization
was really invaluable so there's just
nothing that compares right you know
looking at the table can gives you some
intuition but nothing appears to like
plotting it and playing with it and so
out of that we worked with some students
at u-dub and we built this thing
called sequel sure graphs and since I
have a little time I thought maybe I
would show this to you so here you have
you know you log in and you just get
this sort of like list of data sets and
we're working on better ways to you know
to filter this and show customized views
that you see here you know sequel share
join test it was authored by Bill and
you know some of those English
description of it so for instance just
Weka columns and then once you pick one
of these data sets so here this is the
periodic table you actually have a live
sequel prompt that you can edit here and
then it will go to sequel share fetch
that you don't issue that query fetch
the results it'll so it is a sequel
share will tell you oh this is the
string call mrs. an incall this is a
float column this is a date column and
so some of the string calls we don't
know how to handle but then down here
you know select your x-axis and so
here's the you know the atomic mass the
group the period the melting temperature
so on and so forth and so maybe you know
you plot the atomic mass against the
Kelvin boiling and then you hit generate
and you get this plot down here and so
now we have all these you know various
elements using their atomic number
against their Kelvin boiling point right
so this is the kind of thing where you
know we have now a much better way to do
visual ation given a query than we did
with that ugly sort of Python page
interface that has become really
valuable and so then once we had this
running some of our scientist started
saying well you know where we have this
intrument we've built an we're going to
put on the ship and we're just going to
let it go and we won't be able to
monitor it from shore so where is the
ship is the instrument working what is
it seeing those so that we can sort of
interactively help redirect the ship in
real time so we might say hey you just
pass them interesting feature go back
and take some samples which is something
oceanographer want to do for a long time
right now they just sort of go to this
stage for the most part they just go to
the stations and sample there and so we
had this other dashboard right here
right here that is live but I tweaked it
before this to make it a little more
efficient and so what this is is right
now there's a 30 day cruise going that
left you dub on august seven and so if i
zoom out you'll see that they left you
dub like a seventh and they're going to
Hawaii and so they went out this is that
same line P here all these waypoints
they went out line p and they turn south
and then they're going to go to Hawaii
where the boats actually from the elem
wanna and so this stuff down here is
live live display of how much water
pressure there is through their underway
flow cytometry some calibration metrics
you know how many events there are per
second how many particles they're seeing
in the water how many of what the ratio
of particles total they see to the
particles they recognize some
calibration data and then over there on
the right you can see things like okay
where is the ship right now relative to
a waypoint how fast is the ship moving
so you can see now the ship is stopped
it's on station and down here actually
this is the phytoplankton concentration
so they have a classifier that's running
on this data and so you can see okay you
know the green population which i guess
is Pico is currently the most prolific
and then but you can see right you know
so there was there was a nighttime and
so maybe the Publishing's change over
time and you know there's some refresh
issues here so you can actually sort of
interact with this and you can see the
science output here and so that like
Francois is was on shore has actually
been sending ginger an email saying you
know hey look population just changed
you know where are you what's going on
are you seeing the interregnum
management's maybe you should stop and
sample if you have extra time so that
that's something that you know that
really came out of this and then of
course the other thing we're getting at
is like I mentioned earlier you know
developing these idioms for awkward
sequel so one of the one of the very
common things you want to do in data
cleaning is rename a column right in
sequel the only way to rename a column
right now so you can't do reg ex liver
column names right it's really good to
do row stuff but you can't do column
wise sort of automated stuff so if so if
you want to um you know rename in a
column you have this you've to write the
statement select this column as its new
name comma every other column
individually from the database and so we
had some programmatic tools that would
help you auto generate these kinds of
awkward constructs but you know actually
making you know sort of developing a new
front-end language where you can do some
you know stick in idioms for placements
like you know it would be easy to have a
rename function took a table name and a
column name things like that that where
it was sort of automatically get
strength substituted on the back end for
the for the query you actually wanted to
issue and then of course the biggest
thing we need is more controlled
experiments so we did this and we said
hey it worked
but what we really want is some you know
sort of real like HDI experience where
we get people in a room and we say you
know can you do things you couldn't do
before are you more efficient this way
so on and so forth so this concludes my
talk I guess the big takeaway here is
that our experiment right how does equal
shares how does equal as a service help
real-time data integration that was a
success so there were 40 scientists from
30 labs bringing in 20 students bringing
the data together and it actually worked
really well and we found that both of
these components were important so the
sequel was important because it
expresses all the data management
integration cleaning sort of
understanding adapting cast we needed
and the service was important because it
was the key enabler of the interactivity
so the web browser lets you
automatically set of sort of you know
visualize and debug things and then
there were the individualized in tabular
way and then all the mashups were really
important for getting the scientists to
download data into our dental data in
the matlab download data into odv and
also sort of just live plot things out
there and that because of this we could
do sort of rapid-fire hypothesis
generation and testing at least visually
quality control cross-validation and
then of course in the talk I didn't
cover nearly nearly a fraction of what
we actually learned but you know we
really did uncover a lot of new research
challenges for the sequel for science
agenda so this concludes my talk thanks
for coming take any questions you have
oh yeah so I was just thinking that so
bugs exists but it exists and suppose a
son now you're thinking about gradually
taking a computer scientist I'm just
letting the scientist Angela so what
happens when scientists think they are
seeing something interesting whereas
it's actually do a bug in the code um so
they're a couple things so one is you
know this is a big problem it's already
happening in science today right except
except that in science today it's sort
of manual mucking around with Excel
spreadsheets and potentially Python
scripts where you have sort of this
hodgepodge of of data and computation
and it's not necessarily clear
provenance and things like that so one
of the cool things about sequel share
and perhaps this didn't come across as
clear as it could have in the talk is
that literally when everything is a view
you have really crisply documented
Providence of all of the data so what I
mean by that is this is this file here
SDS tab this is the raw data and then up
here I have this so you can see right
this is essentially select star from the
raw file table SCS tab and this is
because whatever input data you give us
we sort of wrap it in this in this
actual table and then even this this
this view here sts is a view over the
raw data but then later I have this this
is the this is the view of the event
rate which is one of the plots on that
display and so you can see here right
it's okay give me the 500 most recent
points from give me the distinct date
and event rate divided by a thousand as
event thousand events per second from
this SDS view table and so I'm give the
top 500 points ordering them by date
time descending and then that and that's
what we actually go plot in that web
page and so if you wanna update the web
page you update this view but again
right so when I say hey you know that
this is a pointer to that data set we
used you can actually then go and look
at well what what's in the SDS view so I
think if I I do this so now this is the
SDS view and you can see this is mostly
type conversion right cast the time is a
datetime get the difference from 1970
till now is the time stamp and then here
we're actually converting latitude and
longitude from sort of base 60 units to
base 100 units I forget what the
technical terms are but we want decimal
degrees and that's not what they gave us
and so you can see right here this is
the conversion for latitude and
longitude and then mostly sort of some
type some type matching down below so I
mean the cool thing is that yes there
are bugs but we're completely storing
the provenance and if you go update the
view your say oh I found a bug I'm going
to change it the next time someone pulls
the data because the data sort of
recomputed automatically whenever you
pull it by nature of viewer into
database processing everyone gets the
precious copy of the data all the time
we also have coming we have do I support
right so you can actually like assign a
DOI so a digital object identifier to
one of your data sets you can cite it in
your paper and then you can click that
link go here and see the full province
for the computation so we're not
necessarily helping fix bugs now but we
are at least sort of providing better
provenance we think going off on their
own and they say I think I get the hang
of this then they have to sort of to
send risk if they format something they
get a chart back and they say oh here's
some interesting new science if they
publish that their reputation is on the
line and it's based on them not having a
bug in or single
that make sense well sure but this is no
different than and to know how to use
their tools oh no I'm not saying it's a
bad thing I think is interesting it's
like you know scientist asked to take
office yeah way of thinking about yeah I
mean you know honestly right getting the
monitor the scientist is something that
is interesting and challenging all
around so just I mean all these things
come come with trade-offs right like
Reinhart and Rogoff you know well if
they didn't make their Excel
spreadsheets public then they wouldn't
have been caught at the same time i
think is fairly good for the world that
they were and that other people are and
i think that the the flip side of this
is perhaps we can get people to be a
little more rigorous in their process I
don't know I mean I I now sign all my
reviews when I review papers right I all
my code for my thesis is public on
github and and it's weird right it's
really weird I mean even posting papers
but when they're accepted but not you
know when they're published which you
can do if you treat the copyright policy
like you want to is something that
people have issues with but i think you
know reproducibly is becoming
increasingly important as we're more and
more data driven and less sort of
process driven and so that's the way we
have to go I have a question if I think
there was another question back there
speed online thanks okay so my question
was mainly that
is really interesting that you're
offering this is a service and you know
uploading all the stars a cloud but some
of these tests especially we talk about
scientists that could be in remote areas
so what kind of experience do you have I
mean what connection did you use and how
do the upload data how to do I mean some
of this is in real time so what
challenges did you face in terms of you
know displaying this information in real
time and not having a huge dilemma for
you because you said there were
connection problems not surprised at all
yeah um what can I say so that this the
service first local thing is just a huge
trade off in general so one of the good
things about this is so we got the data
in the service we were at u-dub right so
that's sort of fast connection it's over
to Microsoft so it's pretty pretty fast
no matter what you do and that was great
in general these types of problems right
when you use a hosted database seis
equalizer or any other you know hosted
service the computation now is super
fast because it's in there so in one
sense the remoteness of it is a big boon
because all the queries run fast they
can store all the data and you only need
to actually bring back the data you want
to visualize or the data you need to
analyze so right these previews are all
pretty fixed size we want to give you
the top hundred rose you know you'll
like to see the sequel things like that
um at the same time it's absolutely the
case that when you actually want to
download a data set for sort of local
visualization you know you have to pull
the data down and that can be a huge
bottleneck our creation was wasn't
terrible but it wasn't great it was sort
of like maybe a good hotel Wi-Fi um uh
yeah from friday harbor lub part was
just the number people in the room and
we had skype going all the time in the
background things like that I mean I
think we're fundamentally at at at a
disconnect there right because if
everyone's that you know I could have
imagined we would have deployed sequel
share on a server in the lab which is
doable I mean you know you you can you
can point the back end at a sort of a
local sequel server instead of a sequel
Azure you can you can spin up you can
run all the same code on the local linux
box for the rest in it for the amazon
part in the local windows box for the
microsoft part pretty easily so we could
have done it all locally for us the
service is better because the sign
just don't have the IT budget or the IT
expertise in a lot of cases to be
hosting those things so my question was
you have 40 people for you scientists
there at the end of that meeting is
there a roadmap or plan for them to
wander back off to wherever their lab is
and continue to ask the system these
kinds of questions so that they're not
dependent upon a stenographer they can
if they choose to making this kind of
related they can figure out how to be
experienced those columns um so we tend
to have better sets with scientists that
are currently at u-dub so the u-dub
people the u-dub people are doing this
right we are sort of getting mean the
answer is most of them haven't right
most of them haven't haven't used the
system much since the meeting the ones
that have did things like they use the
system only to integrate data sets and
then they pull it down and do the rest
of it and ODB locally and sometimes they
even ask you know they say hey can you
add in this data set or can you edit the
view to update to the new version of my
data set and so they're really not using
this longer requiring until I happen
sweet
if that's an interesting question two is
how do you write and that's I mean so
sort of you know this whole like let's
use github issues to do training and
let's have the wiki that's all sort of
posted this meeting by a long shot so
we're really sort of ramping up on how
to do both sort of automated stuff but
also better training and better user
experience and better you know authoring
without the command prompt necessarily
without the sequel prom necessarily but
that's all future work we did not have
great success with it on this project
yet</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>