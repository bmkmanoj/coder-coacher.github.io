<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Body Part Recognition and the Development of Kinect | Coder Coacher - Coaching Coders</title><meta content="Body Part Recognition and the Development of Kinect - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Body Part Recognition and the Development of Kinect</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QPYf6pXe_4Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay otherwise I think we're ready to go
second machine learning talk this
afternoon it's a pleasure to welcome
Jamie shorten actually Jamie is another
of our former PhD scholars when when did
you start Jamie because we just heard
from John that he was the number one
five years before the official launch of
the program to thought okay so you were
just before the official launch of the
program and now Jamie owes a reit senior
research and the machine learning group
Jamie's been been quite influential on
the connect work and he's also for this
work just received a prestigious award
by the I Triple E pattern analysis
machine intelligence committee called
young researcher award so
congratulations on those in the next
hour you'll take us to to the technology
that makes it enact a connect to take
great thanks Carla and good afternoon
everyone
great to see so many faces here and it
was great to talk to some of you at the
poster session earlier okay so I'm going
to be talking a little bit about human
skeletal tracking and in particular
telling our story from you know the MS
or Cambridge perspective of the
development of Kinect so at this point
Kinect does not I think need too much
introduction it's essentially a camera
that plugs into your your Xbox or your
your PC and combines a number of
technologies particularly on the Xbox
side where we take the camera itself is
able to sense sense jet depth rather
than just a standard cameras RGB we
built some software that does real-time
human skeletal tracking and follows your
your body motions in real-time and it's
got face and voice recognition and the
combination of these new technologies
when it launched really opened up some
very new applications especially in
gaming but more broadly as we'll see to
so so our involvement here was primarily
on on the the second of those those
technologies this this human skeletal
tracking or human motion tracking the
idea in as it started was that you'd be
able to stand in your living room in
front of the the camera move your body
that the camera is able to see what
you're doing understand what you're
doing track your motion and that motion
can then be translated into the game to
allow you to to play games without
having to hold a controller or to wear a
suit or anything like that and so today
this is sort of where we're going we're
going to start by having a quick
introduction to visual recognition
visual object recognition then we'll
we'll talk about project natal which was
the codename for for Kinect and how that
how we got involved how the project
developed and we'll sort of then sort of
do a little bit of a deep dive on body
part recognition which is the algorithm
we came up with to to enable this to be
robust and to work for everyone out of
the box and then at the end we'll we'll
spend a little bit of time just looking
at some some follow-on pieces of work
that have come out of this okay so let's
dive in with visual recognition visual
object recognition we can all look at
this car and you know it's obvious it's
a it's a car it's obvious it's a red car
in a field and if you look closely it's
a Ferrari so you know this is trivial
for us but of course to a computer this
is just a grid of of numbers of RGB
values and and so we're going to have to
somehow interpret this this grid of
numbers to be able to to infer
high-level things about the image that
we that we want that might be useful now
this is made challenging due to a number
of problems first of all you can view
objects from all sorts of different
angles you can have very different
lighting at night or silhouette I mean
is these were going to be extreme
examples but they'll illustrate the
points I think scale objects look very
different close up to far away
pose of the object again some quite
extreme examples there occlusions and
the environment so all of these things
to some extent you're going to have to
think about when you're you're dealing
with visual object recognition and
clearly trying to sort of code all of
the possibilities of all of these things
by hand is going to be rather
impractical and so in the last 10 years
plus there's been obviously a big trend
towards using machine learning to do
this kind of thing and you know I've got
this cartoon in the middle here but what
does machine learning do really
fundamentally you take some training
data perhaps we're going to try and
disambiguate images that contain seats
from non seats we're going to program in
some features or have them learned as
well and then we're going to let this
thing turn away and do its due it's
number crunching and it will come out
with some encapsulated learned models
some representation of the the training
data and the patterns in there such that
then you can pass in some test images
say an image with a seat in it and it
will say seat or an image with a car in
it and say not a seat so this is a very
very toy trivial example but is is the
general paradigm that we're going to be
using here now you know as Scarlett
mentioned I was one of the lucky enough
to be one of the PhD scholars in the
very early days before the program had
even really officially started and you
know I was looking at this stuff in my
PhD and I didn't really think that there
was any particularly interesting
applications thereof but you know it
seemed fun it seemed like a good good
idea at the time so you know the first
thing we looked at was could we find
horses in images by looking at their
sort of outline contours and fragments
of those contours and stitching those
together
and then we moved a year or two later
into trying to do this task of semantic
segmentation where you try and label
every pixel in an image with a number of
different object categories and then
sort of turn that into a coherent
segmentation where we also have the
labels
and then we even took that a bit further
and made this you know fast this was you
know back in 2008 running at about ten
frames a second on a laptop so you know
as I say at the time we thought you know
great this is fun but okay what's the
best application we can think of maybe
image search but nothing particularly
practical but it turns out it will have
some application as you'll see as we go
on so um you know I joined MSR here
full-time as a postdoc in 2008 about you
know June or so and then sort of a
couple of months later you can see I got
this email out of the blue from a guy
called Mark who worked in Xbox and he
had seen some of my PhD research and
wanted to and discuss an important
scenario with me as you can see this is
our Thursday evening at 8 o'clock I
think I just got home I sort of picked
up my email and you know reply to him
said yeah sure sure let's let's talk how
about how would next Tuesday do that
seems like the first available date on
my calendar
and he wrote back and that evening a bit
later and said how about now so
thankfully I said said yes okay final
we'll take this see where it goes and he
described what was at the time known as
project natal this this idea that we
would have a would we'd be trying to
track the human body in front of a
camera in the living room now this
sounded little far-fetched to us at the
time and I'll explain why so the task is
really what's called human pose
estimation and the idea is you're going
to take some input here a cartoon image
and you're going to try and find the
positions of the various joints in the
body and you're going to join those up
into a skeleton here and really you want
to do this in 3d because that will give
you a lot more information so what does
that mean you're gonna have the X Y Zed
coordinates for each of the joints or
equivalently sort of the the angles
between the different joints in the body
and if you could do this you could get
something that looked a bit like this so
you take a video sequence on the left
and you'd get some kind of 3d skeleton
representation over time on the right
and the idea of being that it would be
much easier to do all sorts of gesture
recognition or you know physically-based
manipulation using the signal on the
right which is a very low dimensional
abstracted representation of what you're
doing with your body rather than trying
to interpret at every frame the the
image that you're seeing but this as I
say was something we thought was a bit
far-fetched because it's quite hard and
it's hard for multiple reasons not just
the reasons that I showed you for the
general class of object recognition you
know viewpoints lighting scale etc but
also because humans have an incredible
variety in the in the poses they can get
their bodies into not just that but you
know people come in different shapes and
sizes
people wear clothing have hairstyle
carry things and you know with a real
sensor you might have problems of
cropping of the the field of view sense
or not seeing the whole person at once
and then you've got issues like well the
person may be wearing the same colored
clothing as the background there's a
focus on hiding in the bushes there and
you might have you know clutter you
might have lots of people crowding
together now again these are kind of
extreme examples but to some extent
we're going to have to solve all of
these issues too to be able to ship
something that works for everyone in
their living rooms okay so so fine how
did how do people approach this in the
past well one approach is what's called
motion capture this is what Hollywood
uses to make animated movies well well
what happens is you you were you Don a
suit the suit has all these these white
markers on it and you've got a very
carefully calibrated rig of cameras set
up you know maybe 10 20 cameras set up
around the room and they're tracking
these white blobs and then based on
those are reconstructing your mode
and this works beautifully it's just
very expensive and you can possibly set
this up in everyone's living room so
computer vision has been trying to go
after this task for a long time as well
and here's sort of three axes upon which
different approaches have varied in how
they have how they go about solving this
and we'll just talk briefly through each
of these so the first one is is whether
you you're looking at a single image a
single monocular image or whether you're
using stereo or 3d information somehow
the second one is whether you're
tracking motion over time or whether
you're just predicting one frame at a
time and the third is whether you're
trying to infer the pose of the whole
body or whether you're trying to break
up the body in two parts and somehow
stitch those together so we'll come back
to these as we go so mark you know when
I was chatting with him he also
mentioned some of the requirements that
we had to fulfill for this so he had to
work for any human pose had to work for
any body shape and size we weren't
allowed to calibrate on the user or
instrument the user and for some
definition of never-fail it must never
fail so that sound it's hard and then he
said oh and by the way it has to run on
the xbox360 and we're only allowed to
use 10% of its compute resources because
the games have to run so that sounded
even harder so okay at this point we
were like okay no hope no hope oh and
there's one one more thing this was mid
2008 he said we've got about a year and
a half to get this thing completely done
out the door from nothing through
research through product ization so yeah
this this sounded like an impossible
vision at the time but a couple of
things did give us some hope so the
first thing was the the depth camera so
this is the first image that Mark sent
me of the you know the signal that you
get off this new this new at the time
new depth sensing camera and it was
quite I thought it was quite remarkable
I mean you can see there's detail on the
face up there you don't have the problem
of sort of the hand in front of the body
sort of not you can tell that this hand
is separate from the the rest of the
body and there's all sorts of nice
properties in here so how how does this
camera work well his his the original
connect the new connectors a little bit
different but you know somewhat similar
anyway so this camera has these three
windows on the front on the Left we've
got an infrared emitter that's shining
out some infrared light you've got a
standard RGB camera in the middle which
is really only used for face recognition
and for video conferencing and then on
the the right there you've got the
infrared camera that's picking up the
light reflecting in the scene from this
emitter and these two the infrared
emitter camera pair form the the depth
camera and this is based on a principle
called structured light so the idea
really is that the the infrared emitter
that I showed you on the left is shining
out a sort of pseudo random pattern of
dots into the world which you can think
of as a sort of a barcode and according
to where the object is in the scene
whether it's if it was at depth D 1 here
you'd see the barcode would hit the
surface there be reflected here and land
at this position here on on the imaging
plane in the infrared camera if instead
the light had continued to add object
further away at this position here it
would get reflected and appear at a
slightly different position translated
horizontally in the image on the imaging
plane and by knowing that we're
expecting a particular pattern somewhere
along that line and searching for it you
can essentially infer the depth
information and just to sort of
visualize that I'll play a video in the
top right there as the person as the
sheet of paper here has moved backwards
and forwards you'll see this pattern of
dots is translating left and right
okay so that's structured light and what
happens when you do this oops let's just
go back for a second
so this is gonna play here we go okay so
the image that you'll get looks
something like this and because it's a
depth camera you have all the
calibration details you can actually
take the the depth values and the XY
coordinates and that will get that's
enough to give you the 3d coordinates
and so you can then sort of create
virtual views from the side or from the
top so for the particular task of human
pose estimation that we were going after
turns out the depth has some rather nice
properties it's going to work in
low-light so if people want to play
games in their living room and they turn
the lights off it will be no problem as
I've already suggested this sort of the
the there's this easy segmentation of
the foreground from the background you
also know this the scale of the objects
that you're seeing or at least you know
exactly how far they are metrically away
from the camera and you don't have to
worry about similar you know about
dealing with differences in color and
texture of clothing of skin of hair it's
all sort of this uniform gray that that
just measures depth so that's that's
really nice and so of course people have
had been looking at using depth signals
often from stereo cameras but sort of
starting to use depth cameras by the
time we were starting to look at this
but none of them had managed to take
anything out of the lab they'd been
focusing very much on you know getting a
few toys sequences working but not
really going after the the thing that
anyone can just jump in and use and
that's because of these these quite
difficult remaining challenges so pose
variation size and shape variation image
cropping and clothing etc so I said that
the depth camera is the first thing that
made us think okay maybe we can we can
solve this and the other thing that made
us think that this is this might be
right for you know
this might be ready for the world was
was this video that they sent us so this
is an early prototype system that the
Xbox team had developed you'll see there
there's the sort of points point cloud
in the background of dots of a person
that walks up to this skeleton and when
they get into the right position and
they're standing like this
it sort of locks on and they can dance
around and follow the person and we saw
this when we were blown away this was
amazing we said okay so you've solved it
already why are you talking to us at all
I mean job done and it turned out that
the the reason they were talking to us
is that this wasn't particularly robust
so what do I mean by that
so as I say this is a tracking solution
so the idea is that you know where you
are approximately at time T and you
observe a new depth frame at time T plus
one and then you try and make some
prediction based on where you were about
where you will be and when this works
you know this works beautifully as we as
we saw it gives you a nice smooth result
it's relatively efficient however it's
got two little issues one is that you
have to initialize it somehow so that's
why the person walked up like that and
had to sort of walk into the exactly the
right position and then it sort of
locked on the other problem is that if
you move unpredictably too quickly or or
suddenly then it can lose track it you
know it will sometimes not be able to
predict what your motion in from the
future and then it will be too far away
excuse me from a from a good solution to
be able to sort of refine the estimate
to the correct answer and so any
solution that's purely based on tracking
like this is is liable to a catastrophic
failure and so it became clear that our
mission effectively was was the
following we were going to try and take
the algorithm and I just showed you and
augment it with a sort of initialization
ability so that we didn't have to stand
up like that and also to detect and
recover from from failures
okay so how white one go about this well
the first thing we thought of sorry
click-click-click first thing we thought
of was was this the idea that you would
match the whole pose of the person at
once so you know if you saw the full
image of the person then you just sort
of run through a flipbook essentially of
images of people in different poses and
find the closest the nearest neighbor
and you could do that reasonably
efficiently using some kind of hierarchy
and you know there was prior art that
showed that you could do a reasonable
job with that but the problem with this
approach is that it's it's exponential
the number of poses you have to
represent is exponential in the number
of joints you have because for every
pose I can get my right arm and I can
jointly get my left arm into the same
number of poses and so it's very hard to
scale this up which is kind of why it's
a little bit jerky there on the right
and we sort of actually tried out this
and implemented a prototype similar
system and you see it works you know
quite well at the moment but then as
soon because we haven't trained it on on
this as soon as he brings up the other
arm it's not able to to track that and
so we need to essentially square our
training data just for that other arm
and then if we throw in the legs etc and
then all the crops we we quickly realize
that this was not going to be the thing
that would that filled the need we had
and so the alternative to matching the
whole body is to try and match parts of
the body and so we quickly realized that
this was going to be a good thing to do
and then just the question was how how
can we do this how can we do it
efficiently and how do we get round all
of those sort of nuisance factors of
pose and and shape etc that we've seen
well the solution that we hit on is the
following it's called body part
recognition the idea is that you're
going to
go through every pixel in your depth
image and try and classify that pixel
into one of a number of different parts
of the body so that they say the right
hand might be this orange color and that
as you move your hand no matter what
position you put your hand in it will
always stay that orange color and no
matter what your body shape is as well
so it's going to be a local estimate
we're going to basically base that
prediction on that pixel in the depth
image and some local region around the
image such that we can essentially D
couple all the joints in the body and we
don't have this exponential blow-up in
the the number of poses and this leads
to reduced training data requirements
and reduced training time we're going to
take a frame-by-frame approach so we
were not going to use any temporal
information at all because of the
problems of robustness that we've
discussed already and it had to be very
fast or you know we have this very very
tight compute budget so we had to fit
into that somehow and the the particular
approach that I'll describe in a second
was based on extremely simple depth
image comparison pixel comparison
features integrated into a decision
forest classifier that could be
implemented in parallel on the GPU and
so the the final algorithm that we ended
up with the final pipeline for skeletal
tracking on on Kinect looks a bit like
this so we first capture the depth image
off the camera and remove the background
which I'm not going to go into detail on
here then we're going to infer these
body part labels at every pixel in the
image and then we can cluster those
across different pixels to produce
hypotheses about where the different
joints are these crosses here and
finally we can take those hypotheses
about the joint positions in 3d and fit
those together into a skeleton and again
I'm not going to talk about that today
but we'll talk about these two bits in
some detail ok so let's start by looking
at this
so as I mentioned we're going to try and
classify each pixel into one of a number
of different body parts and we're going
to do that based on some window of the
depth image around that pixel so for
example the pics up here on the hand
we're going to extract some window here
and base all our features on that now we
don't actually have to exactly like
really extract that feature you can just
sort of this is a conceptual way of
thinking about it you can just leave the
original image as as is and just allow
your features to access a small portion
of the image now it turns out that this
is basically exactly the same thing as
what I would doing in my PhD so you know
despite the fact not knowing any
particularly sensible applications at
the time turns out that it does have
that an application and it's and it's
per body part recognition and connect so
you never know quite when you know
something that you're you're doing blue
skies research could turn out to be kind
of useful so and then then the other
challenge was how to deal with these
these these problems and we were going
to take the approach of just training it
with tons and tons of data such that the
machine learning would deal with all of
these nuisances all of these in in
variances that we needed to encode and
and just hope for the best you know
throw it all in let it turn away hope we
had the capacity in our learners to be
able to to deal with that so to do that
we needed some training data and so
here's an example of some of the data
that we captured to start with this is
some real data from a Kinect and someone
has sort of painstakingly hand labeled
all of these using some paint interface
now clearly this is not going to scale
particularly well and we'll see how we
come come we'll come back to how we get
around that in a second but you can see
that what we're just going to try and do
is we're going to provide this is the
input this is the desired output and our
machine learning algorithm will try and
learn the mapping from pixels here to
pixels here
but to do that it's going to need some
features some some features and how do
we look how could we extract useful
information from the depth image now
again because we're on a very tight
budget compute budget we're going to
keep these very very simple and
essentially they're they're you know
incredibly simple essentially at this
pixel X what we're going to do is we're
going to compare the the depth value at
that pixel with the depth value at some
pixel has been offset by some Delta now
why might this be a good thing to do
well in this particular pixel position X
you'll get some large you know the
magnitude of the difference of those two
pixels in depth will be quite large
because ones on the object ones in the
background which we're saying is going
to be far away if you move to a
different position if you were trying to
classify this pixel down here you'll get
a similar large difference however if
you were to move over to a point on the
other side of the body the difference in
depth between these two pixels is going
to be much much smaller and so you can
well imagine that this particular sort
of Delta offset might be a useful
feature to help disambiguate the left
side of the body from the right side of
the body by similar reasoning you can
see that a feature like this that looks
upwards might help discriminate these
two points near the top of the body from
a point from points lower down on the
body like that one and we can go one
step further by saying that because
we've got depth information we know and
we know how perspective works in in
images as you get further away these
these Delta's should get smaller so we
can just scale those offsets by the the
depth of the pixel that you care about
such that if you were twice as far away
and the image was half the size these
offsets would also be half the size so
this gives us depth invariance
essentially for free but individually
these features are going to be you know
almost useless they're just marginally
above above useless but that it turns
out is enough
and it's enough because we can combine
them in in at least the way we did it
was a decision tree classifier so how
does this work well you pass in you know
this image window centered at pixel X
into the root root node of this tree we
evaluate one of those features perhaps
the one that looks left and that gives
us some some depth difference and we
compare that to some learned threshold
and according to that comparison we're
going to branch left or right let's say
we branched left we're then going to
look up a different feature perhaps the
one that looked upwards compared to a
different threshold and again we're
branch left or right for the sake of
simplicity let's imagine we had reached
a leaf node here and at the leaf node in
the tree we're going to store some
distribution just a normalized histogram
of all of the labels from the training
set that reached that node and you can
imagine how the rest of the tree pans
out but notice that there's this sort of
conditional computation going on so the
feature that you evaluate at the second
layer here is dependent on what happened
at the first layer and as you get deeper
and deeper and deeper what this means is
you get you know a lot of
representational power how do you train
these well relatively simple you take
all of the pixels in all of your
training images and you create you start
with a sort of a root node in your tree
and you can build a histogram of all the
body part labels in the ground truth and
at the root node you'd expect it to be
sort of roughly uniform because all of
these parts are roughly the same size
and then what you're going to do is
hypothesize a large number of candidate
features and thresholds and test each of
them to see how it splits the data so
for each of those features feature
threshold pairs it will induce some
partition on your your input data into a
left set and a right set and again you
can compute those empirical
distributions of the training labels and
then you can just compute the the
entropy or the information gain that you
that that particular candidate gave you
and that will
and then what you do is you just take
the one that gives you the the largest
information gaining reduces maximally
reduces the entropy the uncertainty and
if you keep doing this and you keep
growing deeper and deeper trees
obviously much deeper than at what I
just showed you your hope is that you
can drive the entropy at the leaves to
zero which means you're completely
certain that this if you reach this leaf
a test time that that this class is the
one you should be predicting obviously
this is a very very short introduction
to this if you are interested in this
kind of stuff in more detail we have
written a book which has all sorts of
tutorial in it comes with some software
and stuff like that so you know excuse
that a quick plug but and I hope you
find it useful okay so we did that and
we did we trained it up on a small set
of data and then if we this is sort of
one of the first videos we had of the
system running live and it kind of works
you can see that these parts are
following me around as I'm moving around
in the office there's some problems
there with the background subtraction at
the time but on the whole you know
modulo a lot of noise and incorrect
classifications it sort of got the core
structure at least quite quite nicely so
that was our sort of proof of concept
and and from there we were we we thought
okay well hopefully all we need to do
now is just push on this turn the handle
get more data generate you know train
these things up to deeper depths more
complicated structures and so that's
what we did so how did we deal with this
data problem because obviously painting
all of those images by hand is is
extremely painful and not you you're
never going to be able to scale up and
so we turn to computer graphics and
thankfully depth images are relatively
easy to synthetically generate using
just standard graphics pipelines much
easier it turns out than RGB where you'd
have to send simulate all the clothing
and you know texture and hair and
etc so we went to a mocap studio
recorded a lot of mocap Distilled that
down to what we thought was a useful set
of poses that span the space of motion
that we would be interested in we could
then retarget that to different body
characters to different shapes and sizes
and that allow allowed us to sort of
quickly scale up without having to
record lots and lots and lots more data
for each different body shape and then
we could just render us basically as
much training data as as we wanted with
both the depth image and the body part
labels and so we could generate
essentially an infinite training set
here's just a video of what that looks
like in practice this is a deep zoom
image where we've we've just splattered
down a million of these training images
into one canvas and you can sort of see
the the variety that's in there and this
the scale of this thing as well and I
wonder if you can guess what it's going
to spell out when you zoom out so that's
what a million images looks like okay so
we have now our training data we have
our features we have our learning
algorithm let's see what happens so we
train this up and I'm going to show you
now an example of the the importance of
training very deep trees so there's a
couple of text input images on the left
here in the middle we've got the ground
truth body part labels that we want the
system to predict and on the right I'm
going to show you an animation of what
happens as you go deeper and deeper in
in the tree so if you go to do depth 2
that means you've done exactly one text
at every pixel in your image what it's
decided to learn we didn't tell it this
it decided to learn that one of these
features that looks upwards and looks
quite a long distance is is the right
thing to do so it's able to it's decided
that segmenting the top half of the body
from the bottom half of the body is a
good thing now because of the the way
that the features work you've got this
sort of go
the silhouette of the the head there but
that will get corrected later on as
you'll see on this sort of somewhat
harder image down here it's also doing a
reasonable job although it's you know
the position of the the lower half is
somewhat different again this is
something that it's going to learn to
create correct as it goes so if we go
one layer deeper this means that every
pixel has had two tests although are
different two tests according to what
what happened at the first at the root
node it's now decided that it wants to
split the left half of the body from the
right half of the body similarly down
here it's it's doing a reasonable job
there so if we keep training deeper and
deeper and deeper by the time we get to
about depth ten we're starting to get a
lot of structure up here that's matching
this the ground truth reasonably well
and you're doing okay down here on this
harder example but it's got its a it's
got a big mistake here where it thinks
the back of the person is is the head
and that's because you know it's decided
that with only ten tests the best thing
it can do is the most sort of the way to
maximize accuracy on your training set
is to is to make that mistake but if we
keep training deeper and deeper and
deeper by the time we get to about 17 or
18 we're now correcting that mistake so
the back is now correct and the heads
coming out in roughly the correct place
but also this easier image here is
giving a much more confident and correct
answer and I can sort of visualize this
on a graph as well what we were just
looking at is sort of the results of
this red curve as we increase the number
the depth in the trees the accuracy
keeps going up now for comparison I've
got that was on on the large the full
training set if we train with a much
smaller set fifteen thousand images only
you can see what's an instance of
overfitting here so there's so many
parameters in your tree by the depth
eighteen that it's it's sort of being a
yeah and there's so little data with
only fifteen thousand images that it's
not being able to fit very good models
there and it started the test accurate
ease therefore starting to go down even
though the
training accuracy would have continued
to go up so how so you know in terms of
the final system that we ended up with
we had to scale it up to these 31
different body parts we trained a forest
of three different trees to depth 20
obviously that's a lot of nodes we
trained off about a million images with
about 2000 pixels per image and about
10,000 of those feature feature
threshold tests features that we that we
looked at earlier now this sounds
expensive it was it took about a week on
a fairly large cluster at the time
although you could do it a lot quicker
now of course but the very nice property
was the test time was really efficient
so each pixel you only had to evaluate
20 times 3 feature evaluations per pixel
and each of those could be evaluated in
parallel on a Jeep on the GPU that we
had in the x-box and so that made you
know all this training worth it you
could do all the hard work offline and
then you'd have a lightweight system
that you could deploy to your users and
it would run very quickly and this is
what it kind of looked like when we had
finished with it so on the Left you're
seeing a sequence of depth images coming
off the camera and on the right the the
body parts which are now you know
following the body a lot better than
they were with the smaller training set
with more complex motion different you
know a different person in this case and
note that there's no tracking or
smoothing here this is all per frame
independent and so we don't have this
issue of robustness to worry about so
that was the the second stage in this
pipeline that's just very very briefly
look at the the third stage here
essentially what we do is you you take
the predictions from here these are
probabilities and you take the depth
image here and you use the depth image
to work out this sort of 3d point
of of pixels that correspond to this and
you take the probabilities from here and
you just sort of throw those out into
into a sort of a 3d probability cloud
and then you just run some clustering we
used a mean shift clustering algorithm
but I'm sure you could use other
clustering algorithms and that gave you
these these hypotheses about the body
joints it might give you zero one two or
more hypotheses about each joint but
let's have a look at what this might
look like in a video so this is the same
video on the top row and on the bottom
row you're now seeing the front side and
top views of the same sequence where the
gray points of the the depth the depth
pixels that have been the 3d point cloud
of these depth pixels and the colored
squares are the predictions these
hypotheses about the the joint positions
and again these are just done with
without any any tracking just frame by
frame and so you can well imagine that
if you start to take those and stitch
those together then you can do this
thing relatively easily there's actually
a lot more for that that I'm not going
to talk about today but you know that
this is the step that incorporates that
the temporal information because you
don't want to throw out temporal
information it is useful but you have to
use it in the right way and you have to
have a constant source source of
reinitialize ation to make sure you're
robust so that the fit skeleton stage it
exploits the temporal information also
exploits kinematic constraints so that
you know the the distance between the
wrist and the elbow is roughly the same
as from one arm to the other and roughly
the same as the elbow to the shoulder
and so that so that was that
and you know after a lot of hard work
and sleepless nights we we've got this
thing out the door it's launched in it
went is it is it 2010 I think November
2010 and you know did remarkably well
certainly exceeded all of our
expectations there was a fun launch
party in LA with a animatronic elephant
and all sorts of things I did go yes yes
it was it was good I didn't get to ride
the elephant a bit and of course there
were some really fun games and that was
what it was all about but very quickly
after launch it became apparent that
there was a lot of enthusiasm for using
Kinect for other things and you know
I'll just play a little video it's a
little bit of a cheesy marketing video
so please excuse that but it's quite
it's quite nicely illustrates the point
at least
we thought this will be fun to play and
it was something amazing the world
starting to imagine things we had
unexpected things
hopeful things
beautiful things
he's fired things
which is why he can hope the world keeps
asking us what we will do with next
just as excited to ask the world the
same thing okay so I don't think I would
advise any of you to try and build a
bomb disposal unit using Kinect yet
however there were some some real things
and real projects that we did in there
the medical example with the doctor in
the operating theater sort of
controlling the the display using
gestures so that they don't have to
touch a keyboard or Mouse and the world
wide telescope example where you could
you know zoom through space by gestures
those were both real real things and I'm
sure there are you know other other
things that physiotherapy is clearly
something that people are very
interested in using this technology for
but you know more generally you know
Kinect is opening up a lot of avenues
for new research and products that
wouldn't have been possible without the
abstraction that the depth camera gives
you all that the the skeleton tracking
gives you and it's you know it is very
exciting to see what's what's going on
with that so I'll just end with a couple
of quick examples of of some of the
stuff we did afterwards and this isn't
the very latest stuff but it's I think
it's quite interesting so the first
example here is is Kinect fusion so the
idea in this work would is that you you
know you pick up the Kinect camera and
you you sort of move it around the room
and start scanning in your world and the
Kinect fusion algorithm is essentially
first of all working out where your
camera has moved in 3d relative to the
previous frame and building up that
camera trajectory over time but also in
the background building up a detailed 3d
reconstruction of the scene and
averaging out a lot of the noise that
you see in the input image on the top
left here so you get these really nice
smooth reconstructions on the bottom
right and you know this this work is all
relatively small-scale scenes and static
scenes and since then we've been
building on that
doing much larger scenes doing whole
bookstores and dealing with motion and
deforming objects as they as they move
so you don't have to have a static scene
another thing that we looked at was
basically taking the same machine
learning approach this this pixel wise
classification and going off to a
slightly different task which is open
and closed group hand detection because
the original system that we launched
just had the hand position it couldn't
tell you what you were doing with your
hands and so you'll see here as Jem
who's a postdoc with us moves his hands
around and opens them and closes them
when they close they turn red because
we've trained this off of data of
people's hands open and closed and when
they're open they go blue and so you can
now you've got a sort of a grasp signal
you've got an engaged signal which we
didn't have before and that opens up a
lot of fun applications so here's a
example with a map and you can you know
essentially simulate the pinch the the
pinch zoom effect on on on the honor
surface on a on a touch device with a in
air no touch kind of pinch zoom thing
and we'll have this running I think on
Thursday for the demo session and you
can have a play around with it and we
have done you know subsequent to this
we've been doing a lot of work on on
fully articulated hand tracking which is
an even more exciting and challenging
topic happy to chat with you about that
if you're interested okay so I will wrap
up I think you know one of the key
messages is you know go do amazing stuff
and even if you don't see an immediate
use for it you never know what's going
to happen down the line machine learning
clearly is is you know is really hitting
some home runs now it's able to do a lot
of fantastic things not just with this
but in you know image classification and
all sorts of other applications and you
know using machine learning for 4x is a
fun
thing to do and can can have great
success and of course connect the camera
itself and the skeletal tracking
technology is allowing us to do a lot of
fun new applications and taking them out
of the lab in many cases as well so I
will say thank you very much
well if kynect can also recognize
fingers how could it let you play the
virtual violin in the ad so could you
build a virtual violin yeah in theory if
yeah if you could track the fingers very
accurately you could imagine building
some kind of air violin or air guitar of
course there's there's a the subtle
issue of haptic feedback and you know
one of the reasons that people can play
violin well is that they have when they
push down on the string they feel it and
when they sort of bow they can feel the
the pressure on that and and so dealing
with that issue to actually really
generate what feels like a useful rich
air violin would would be very hard I
think however there may be other
instruments that you could imagine new
instruments that have never existed you
know like the theremin but you know new
versions thereof that might feel more
natural and might be quite interesting
so that's a really interesting question
and how does the resolution of the old
Kinect compare with the new Kinect the
thing about the old Kinect is even
though so let me just tell you the
numbers on paper I mean the old Kinect
is 640 480 notionally and the new one is
512 424 I think so it sounds like it's
smaller however the the resolution of
the old Kinect is not actually you know
not every pixel is independent and
that's because of that pattern of dots
so what happens is the pattern of dots
is shined out into the world and you
have to aggregate you have to match a
pattern which means you have to match
over some small region of the world
which means that the effective number of
pixels you have is much smaller than you
would think from the the number on paper
so the new characters considerably you
know in practice it's considerably more
detailed that the edges are much better
and also the Z resolution is also much
higher that's a good question I don't
know the precise details of how its
manufactured but essentially you want
something that is pretty random because
you want it to be non repeating you
could you could design it once I think I
think it's something like injecting
bubbles into some bit of plastics it's
something like that and you do that sort
of randomly and then you have to
calibrate each individual camera but I
don't know the precise details on how
that's manufactured
some kind of symmetry about the object
the way you did this right so you
wouldn't give me able to track like
anymore this by this machine so there's
nothing you don't have to have anything
symmetrical here the fact that the human
body is symmetrical is actually a
nuisance factor because you then got a
you know this thing looks very much like
this thing and you've got to work out
which side is which and actually that's
one of the hardest things is working out
is the left hand the right there and the
right which which is left and which is
right could you use it to track
something more amorphous um I don't see
why not if you could get some training
data of to represent that I expect this
would work
you know reasonably well out of the box
you said that you in the mocap sessions
you had about a hundred thousand poses
and before you said that it was a very
painstaking process to label such poses
I take it you didn't did you manually
label the hundred thousand poses like
sorry sorry if that was confusing okay
so the the painstaking bit is taking the
let me just bring up the relevant slide
because this all the painstaking bit is
doing the manual painting of the
training images so where is it gone this
one the painstaking bit is taking that
and creating that by hand that would be
very painful
the the cheap the relatively much much
cheaper way of doing things was the idea
of gone going to the motion capture
studio here we go and recording mocap
which is just the the joint positions
and the joint angles and using that to
generate using computer graphics that
both the depth image and these these
things using sort of texture maps
essentially on the on the body so you
don't you know we didn't have to paint
these manually we could just basically
get some a couple of people to put on
one of these full body suits and move
around for a few hours
record that and then we could generate
as much
as we wanted from that very very deep
freeze how did you avoid overfitting
so through lots of data so I mean the
more data you have the the less likely
you are to overfit clearly decision
trees are potentially very prone to
overfitting and purely through through
lots of data here it's how we avoided it
you know there are some heuristics you
can use like not splitting nodes that
don't have enough training samples and
things like that we didn't bother with
that because we had all of this this
data and we had enough you know we
engineered enough we we actually spent
about six months building the
distributed training algorithm to be
able to scale it up to that that links
to this sort of scale</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>