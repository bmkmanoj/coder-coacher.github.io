<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Algorithms for Near-Separable Nonnegative Matrix Factorization | Coder Coacher - Coaching Coders</title><meta content="Algorithms for Near-Separable Nonnegative Matrix Factorization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Algorithms for Near-Separable Nonnegative Matrix Factorization</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O8YDAMjIzYs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay thanks for coming we're welcoming
Abhishek Kumar today from University of
Maryland College Park where he's working
with held MA on a number of interesting
topics and today he'll tell us about the
work he did at IBM TJ Watson with the
class and money on matrix factorization
which we all like and use Thanks so I'll
talk about algorithms for inseparable
non-negative matrix factorization and
here is the outline of the talk so I'll
start with introducing the problem of
nmf and the separable T reduction and
then I talk about algorithms for near
separable nmf in the next part I will
talk about extensions of these
algorithms to other loss functions like
l1 loss or Bragman divergence and in the
end I'll talk about some other work that
I have done so let me start with the
introducing the problem of nmf for this
problem setting is that we had given a
non-negative matrix X here and the goal
is to factor it into two matrices W and
H so that the product W times H is close
to X in some distance measure and again
W and H are also non-negative so all the
entries of these matrices are non R
greater than 0 or or 0 the small R here
is the inner dimension of the
factorization it's always usually much
less than both m and n so non-negative
rank is is the smallest inner dimension
for which we have an exact factorization
so there is no error in the
factorization here so the smallest R for
which we have no error in the
factorization that's called non-negative
rank and important point about non
negative rank is that it's always
greater than linear rank and always less
than or equal to both m and n now
another point about nmf is that it can
be non unique so even if we ignore the
permit the permutations and scalings of
columns of W and rows of H and F can
still be non uux there can be multiple
non-negative matrix factorization for a
given matrix
now let's understand the motivation
behind nmf is a small example here so
I've shown the images from a database
called Schwimmer and it had it has
images of a creature that is swimming
and it has four legs so four legs can be
in four different orientations so there
are total four to the word for 256
images and each image is of size 32 plus
32
now let's say we vectorize all these
images and put them along the rows of a
matrix X so X is now 256 times 1 0 to 4
and we do a low rank factorization on on
X so we factor it as W times H now in
this model every row of X can be seen as
a linear combinations of rows of H and
the combination coefficients are given
by the corresponding row of W the rows
of H are called basis if they are
linearly independent and if they are not
linearly independent we call them topics
for dictionary now suppose we have
non-negative W here so what does that
mean that means that we are combining
the rows of H additively so we are
adding the topics to generate every data
point so it gives a very clear in to
clear nice interpretation to the model
on the other hand if we don't have non
negativity on W and H in that case we
can get a low rank factorization using
SVD as well which is optimal in terms of
minimizing the Frobenius norm between X
and W times H so let's see what basis we
get using SVD here so this is the basis
we get using SVD and again each basis
image is a row of matrix H which I have
reshaped to this display as an image so
here you can see that topics are not
very interpretable now as soon as we put
non negativity constraints on on W and H
we get interpretability in topics so you
can see that every image in the database
here can be generated by adding all
these topics and we also get sparsity in
the representation because it's no much
your goal was to reproduce the matrix
right some would like to produce dolls
ya Yin is this to minimize the Frobenius
norm between X and WH so we also get
sparsity in the representation because
because of non negativity so in this
example you can see that we can generate
any image in the database by using at
most 5 topics from from this set of 17
topics so we get as parts W and which is
again useful in some applications now
nmf has various applications and one of
them is topic modeling in topic modeling
we are given a corpus of documents and
the goal is to learn prominent topics
from this corpus
now let's say we represent our corpus as
a matrix X here where each row is a
document so X is document by word matrix
and we factor it as W times H so W is
document by topic and H is topping by
word
so here every document is generated by
leaked by irritably combining the topics
in the rows of matrix H so here I've
shown some topics that are recovered
using n MF so again each topic is a row
of H and you can see the first topic is
corresponding to some Olympic Games
second topic corresponds to space
mission and third corresponds to stock
market I guess and so on so n MF does
give us reasonable topics yes you
probably answer this later
so given H which you talks and like some
new document it wasn't in the document
it'd be nice maybe to say hey what
topics is this new document you know I
think that means it's sort of like
computing a little bit of W for this new
executive but it's not part of the
overall process because the topics are
fixed let's say yeah yep so what is you
gonna talk about that computation
because that computation is that
necessarily this what if if the new you
know that the new document belongs to
these topics then then it's yeah yeah I
mean if there is a no novel topic there
then that's the problem otherwise that's
it's from the same yeah if it is from
the same decision then you can you can
basically solve a problem which
minimizes the distance between X and W
times H where you know H and you know
the vector X and you solve for W so you
solve a non negative least square
problem with unknown W you know in a
weird way it's kind of but you can spend
all all the time of the world fighting
some stalkers but then maybe I'm gonna
have a twenty billion documents yeah so
I don't want to want to spend all that
much time for document figuring out the
topics yeah I mean in my experience it's
definitely faster than later and
additionally allocation if you if you
have n MF and that's fast faster than
Lda but yes again I mean you have to
paralyze and do all sorts of
optimizations to make it work so other
applications of n MF are in
hyperspectral and mixing blind source
separation and also in in a few
unsupervised feature extraction problems
in computer vision so it was shown in
2009 that yes
some intuition so why why is nmf give
you that more sort of topic oriented
inseparable thing than SVD the yeah
because you just not been able to go
negative forces it somehow like what's
the intuition intuition here is that
when you have non negativity you are
only allowed to add the topics right so
every topic in some sense is forced to
pick certain parts of the images from
the database yeah and SVD you only also
have negativity so you can basically
trade off now because you are forced to
pick certain parts of the images so
you'll pick those parts which are
appearing in most of the images so
that's why you get the topics here given
for a given number for a given R will
the SVD you will give you a better a bit
to your ax will give you this more
interpretive will say yes yes so is it
so so what is so there might be multiple
ways to do that right you could do
Edwyn regularization you could do this
this kind of positiveness constraints so
what is the drive there to use this
strategy or goes to expand and one of
projection onto some with Elven you will
not get non negativity you can get like
small you can get several zeros but you
will have also negative elements so I
guess this type of interpretability you
will not get even if you use elven
regularization on wnh
doing for example SVD with everyone
relies on and still be a 3d with Alvin
yeah let's say you decide do this and I
Coutu and then one regularizer because
I'm not satisfied with no sloppy right
I think SVD with Alvin will be
non-convex so so again I mean this so
this video also non-convex but you can
get a global solution because it's
second value problem but if you put l1
norm penalty in the objective then I
guess you will reach some local minima
so first problem is that it will not be
a global Optima and second is I don't
think you will get interpretability like
you get with with nmf yeah
so in 2009 it was shown that and the
problem of nmf is np-hard
the problem of exact nmf so it also
means that the approximate Animas
problem is also np-hard
now I'll mention a few problems that are
solvable in polynomial time so the first
problem that can be solved in polynomial
time is if we treat the inner dimension
of the factorization is constant in that
case the complexity is polynomial in in
n and n but still it is exponentially
not so so it so it's not very practical
yes our R is definitely an input but the
complexity here is is exponential in R
so it's it's exponential in the size of
your data so it's np-hard but if you
treat our as a constant then it's
polynomial in N and M so complexity is
like n times M to the power R square 2
to the power R so it's actually double
exponential in yeah
so second second instance when the
problem is solvable in polynomial time
is when rank of your matrix X is 1 or
you are seeking in a factorization of
inner dimension 1 so in that case as if
you do SVD on a data you directly get
nmf because top singular vectors of a
non-negative matrix there are they are
non negative third cases when your rank
of data is is 2 so in that case also it
can be shown by some geometric arguments
that nmf can be solved in polynomial
time now because nmf is np-hard the most
common approaches to solve this problem
are based on local search so these
approaches start by randomly
initializing matrices W and H and then
they fix one block of variables and
optimize father's and this is repeated
for a number of cycles until the
approach converges to some stationary
point but the problem is that these
approaches are not guaranteed to
converge to a global Optima so so there
is a danger with the local search
methods that can converge to different
stationary points depending on the
initial initialization so separately
assumption was proposed to make the
otherwise Animas problem practical so
the assumption basically says that you
have an identity matrix that is hidden
somewhere in your write vector matrix H
so in other words some columns of H are
coming from the matrix identity so if
you look at the matrix inside the
bracket here the first our columns are
identity and the rest of the columns are
given by H Prime and the columns of
identity are hidden by this permutation
matrix P so and we don't know where they
are hidden so in other words the
assumption is also equivalent to saying
that we have some columns of matrix X
that make up matrix W so the columns
that are given by the index at a those
are the columns that constitute W
exactly
now those columns of X that appear in W
we call them anchor columns and here I
have donated them by the by the index at
a here yes
people use this notation for conjoining
matrices sometimes it just makes sense
for me to draw a picture or you're like
okay this is see this little block of
matrix that has the identity in that
block over there that's not been done
these are zeros or some can you do that
picture for us are you allowed to do
that picture yeah they picked it is
unfortunately not not there there's oh
okay
so the picture looks like this so you
have this big matrix X sure
and then here you have some columns that
are coming from identity so maybe this
column is from identity and somewhere
else you have some other column but you
don't know where these columns are
appearing so yeah and that's what the
columns are can be anything they can be
any non-negative values so that so when
you multiply W with the column of
identity that column transferred
directly to its yeah so wherever you
have this one appearing so the second
column of W will be transferred to that
position you are saying that in every
topic there is at least one word that is
not appearing in other topics so there
is at least one yes yes yes yeah they
are called anchor words and it's kind of
reasonable to assume in topic modeling
because if you take like two topics like
sports or politics you can assume that
there is one word in sport that is not a
very in politics and and so on it's
stronger so you're saying that H prime
is orthogonal to those as well H prime
is it's not also H prime can be anything
so there can be anything non-negative
this way yeah that's a good
so okay so the columns where we have
identity they are called anchor columns
and you can see that rest of the columns
are just kunuk combinations of these
anchor columns because H is non-negative
so all the columns of X they are
generated by non-negative combinations
of these anchor columns now the problem
of nmf is is reduced to finding the
extreme raise of the data cone here and
the reason is that if you have these
extreme rays they are generated by the
anchor columns of X and once you have
the anchor columns you can get directly
matrix W and then you solve for H so
here is a geometric picture of the
problem so the points here are the
columns of X the matrix X and the red
points are the anchor columns and black
points are rest of the columns so in
this picture you can see that all the
points all the columns of X they are
inside the conical hull of the red
points the anchors there's one condition
of the anchors which is I'm just gonna
say to er there's some words that are
only in one topic but it's actually
relying on another property of this
which is in fact you get the whole
identity matrix so that for every term
that term must occur at only one topic
there must be some topic for which and
only curse doesn't curl
oh no I'm wrong Sam because you're
saying it's in the linear combination
yes
zen-chan is that dynamic topping you
have at least one word that is not
presenting others other words are just a
combination of the anchor words the
words that are shared they can be
represented in the space of the anchor
words is that uh-huh yeah that's a
little one way to say in a weird way
these words are sort of like like if you
just need these magic words then all the
other words are just points in the space
that is that's where I'm determined by
the basis of this think of words yeah
okay now suppose we we cut this cone by
a positive hyperplane so the hyperplane
is given by P transpose X equal to C and
P and C both are positive and then we
scale all these points so that they lie
on this hyperplane so the problem now
reduces to finding the vertices of the
convex hull of the scaled points because
the points that were anchored columns
earlier they are now vertices of this
convex hull so we can also solve this
enemies problem when we scale the data
and then find the vertices of this
convex hull of the scale points now the
question is is it a reasonable
assumption or not and in topic modeling
it is original option as we as we
discussed that there is at least one
word in in every topic that is not
present in others in hyperspectral
unmixing also it's it's a reasonable
assumption as it has been pointed out by
some recent papers and in general image
processing the answer might vary from
application to application in some of
them it might be reasonable and in the
end I'll show one application where it
does give good results now so far we saw
a pure separable problem in the noisy
problem we yes
so so for a real data set yes there can
be many vertices but you want to pick
the vertices which will give you the
least factorization error so because you
are a signal rank of a factorization so
you want to pick only small odd number
of vertices so you want to pick those
vertices that will give you less
activation error
spacely's hospital
so if you have n points at most you can
have n vertices I mean depending on the
dimensionality of the space that will
vary but it turns out that if you have
to select our word so in every iteration
you can select one word one word one
vertex of the convex hull and if you
have to pick all the vertices that
problem can be solved in polynomial time
but again you have to pick only you have
to be selective and picking the vertices
so yes so again I'm missing some
intuition because I feel like we we sort
of added something very tiny and got a
huge too much game for it so I mean all
we need is that there's at least for
each topic there's one dot at least one
document that has one word that was the
only time to that word up here like this
seemed like a really trivial thing to
add to your set and all of a sudden it
becomes not is that right that's all you
need like in markers it's almost like a
key but you also have work to appear
once no so there is no notion of notion
of document in the matrix H so H is a
chest topic by words so you assume that
the topics that you are recovering those
topics have this property
to another huge weight distribution has
to be like that
so I mean so the anchor vert can appear
in in multiple documents needs to occur
in every document that has that but this
way at least mountebank of where's base
of it let's dip here so if there can be
multiple anchor words for a topic right
fluttered don't think they're gonna be
more than one incredibie but we've
assumed that there is at least that's
where that identity matrix thing comes
from it any matrix is you know it has
one one in every column we wonder who
have every row but like an extra staff
but isn't it true that other words that
are going across multiple topics they
have to occur with at least one of the
anchor words for each topic for them to
be discovered is that so the words that
are not exclusive than more than a non
anchor for them to correctly get
identified as long as having some weight
to the topic they have to co-occur with
at least one of some of the basis words
something converts yeah Danglars is that
true so if I had only one document huh
so given all the documents on a topic if
only one of them had my anchor word that
won't work it needs to and that's not
gonna happen to have every other word
about topic all
everything
so actually so if you see if you take a
particular column of this matrix H right
that column generates the corresponding
words in the in the matrix X right and
that column is responsible for combining
the columns of W right so in in your
document it is possible that the anchor
word is not present that's possible but
so you have you have these columns of W
that are getting combined right and the
combination coefficients are yes I think
I think you are right so if the topic is
present in a document that anchor word
has to be has to be there I think you
can sort of look at that diagram just
mmm
I mean what the promise of using mmm is
that it sort of sounds like it's just
numbers but it's not right I mean ham is
documents and it's Word documents and
that and it's words and so then you can
now live above your own age so W has
documents vertically and topics
horizontally right and then H has words
by topic right and so now you look at it
and it basically any document that has
topic five or something like that passed
out of that or it has to have that work
that's the definition of it yeah that's
what that enterprise yeah yeah do I -
their topic and yes it's like if you
only knew what that word is then you
could just read off the answer
that's what makes it sort of extreme
within within the approximation all
right I think what's hard is that you
give this simple condition on the
solution but it's hard to figure how
this place of document matrices which
like have this property right yeah well
I get some day does it have this anchor
property yeah I mean so there is there
is no test but it has to come from the
domain knowledge that you have so for
the topic modeling I mean it's a safe
assumption to have that if your topics
are discriminatory then you can say that
you can assume that there will be at
least one word that discriminates it
from other topics but if you have very
close by topics like in sports if you
have like baseball in in West Coast and
baseball in East Coast then maybe
they're not very discriminative so in
that case maybe all the words are common
between these two topics if I give it X
matrix which might verify that yes so
it's global solution under that
structure which some some noise probably
yeah and there you can you can still
tell me something about so if you yeah I
mean if you so in text data you always
have some noise I mean this assumption
is not not exactly satisfied so you will
always have some noise and in that case
also it turns out that you get I mean
reasonable results with with this
assumption so again in the noisy problem
we have this separable structure and
then we add some noise to it and again
the goal here is to recover the anchored
columns so next I'll talk about the
algorithms for near separable nmf
so let me start with reviewing some
recent activity that's been happening in
this area so SR and co-authors they came
up with the with this approach where
they minimize the Frobenius norm of X
minus X transpose H X times H and with a
sparse penalty on H so they take the
Infinity norm of each row of H and then
they take l1 norm of these in infinity
norms so this non penalty induces many
exactly zero rows and the rows of H that
are nonzero
that end up selecting the corresponding
columns of X and they become our anchor
columns so it's in some sense it's a
sparse regression problem with a row
sparsity on H and now if we have a pure
separable problem in that case it can be
shown that this kind of optimization
problem can direct can recover all the
anchor columns so we recover all the
anchor columns of X by solving this if
there are no repeated columns in X now
there are some drawbacks of this method
so one is that if there are repeated
columns or similar columns then this
method will fail so it will not recover
all the anchor columns another drawback
is that it's not very scalable so we
have to reduce the dimensionality of
data and subsample the points to make it
scalable yeah
Rose firstly from the from the first
I was not with these column starting
explore sense oh boy
so so the rows no you want rows
partially on edge because rows part city
will give you some nonzero rows in edge
right and those nonzero rows will end up
selecting the corresponding column from
X so you can see that here we are
multiplying X with H right so the rows
of H that that are nonzero they will
select the column of H column of X
corresponding column of X so let's say
so let's say there are some rows so at
you the full square matrix now and there
are some rows that are non zero right
and here is X so these rows will select
the corresponding columns and they are
now your anchor columns so again H is so
there is a subset of rows that are
nonzero and these are your topics
all these topics in exactly
it helped that the nutria sluicin
accountants have many
so each of these topics yes
so yeah I mean you assume that there is
some discrimination in the topics
depending on I mean there is at least
one word that discriminates
so recently Aurora and coders they also
came up with the approach where they
view this problem as minimizing the the
Frobenius norm again but they scale all
these points and they solve the convex
hull problem so they recover the
vertices of this convex hull and they
and that gives them the matrix W but
this approach is not very scalable more
recently with our friend co-authors they
proposed a linear programming based
method where the minimize the Frobenius
norm the norm of X minus x times H under
some constraints on H and this again is
guaranteed to recover all the inter
columns if the problem is pure separable
and the optimize is using parallel STD
so the approach is very scalable and the
noise performance is also shown to be
better than the other a paper
D'Alessandro versus they also came up
with the approach where they view this
problem as recovering the vertices of
the convex hull and they use a property
that has strongly convex function over a
polytope maximizes on a vertex so they
so they evaluate this function on all
the data points and they pull up the
vertices and they incremental e select
the vertices so in each iteration they
select one vertex and the approach is
very fast the only lighting one drawback
of this approach is that it it requires
that the set of anchors are linearly
independent so it assumes that the W
matrix here is full rank if it is not
full length then it will not recover all
the anchor columns so when we started
working on this problem we did some
preliminary experiments and we made some
observations so one observation was that
real text data is very noisy so if we
take this DVD data set it's a classic
topic modeling data and it has 30 topics
so if I take a NMS with 30 inner
dimension factorization and in that case
the ratio of noise to the ratio of data
is about 90% so 90% of your data is not
explained by the separable structure so
it's very important to choose a write
noise model
further when we are in high dimensions
it can happen that all your columns are
anchor columns so in that case it's the
performance depends on which anchored
columns we choose because there are
multiple options and we are only allowed
to select only our number of columns so
we have to be selective in choosing the
right anchor columns this is a topic
detection and tracking data so it was I
think released in early 2000 also and it
has 30 topics and and I don't have other
topics exactly but the document about
documents are around nineteen thousand
yeah
topics are fairly distinct very distinct
words
teamwork every size is 20 25,000 is the
so is the index set of the anchor column
so the size of so the number of anchor
columns actually is 30 so there are 30
topics smaller yeah intelligent this is
my assumption right because the TE
t-shirt may not have that problem yes I
mean it's a assumption that we have in
topic modeling so we are going to test
it on this data with you if you were to
run this on CD GME buying or you will
get a different ratio or every wrong
that's too if you increase R then you're
then more and more data will be
explained by this structure yeah but I
mean in the data it's given that there
are 30 topics so they say that 30 topics
are there
so there is one more thing about the
previous methods and they were by Alvin
normalizing the columns of X so they go
after the convex hull problem and this
is problematic in text data because in
text we are more used to using the
tf-idf representation of documents and
we Elven normalize the rows of matrix X
and if on top of it we l1 normalize the
columns then that part of the tf-idf
structure and in our experience that
adversely affects the performance under
noise so these observations they
motivate us to look for something that
directly solves a conic hull problem and
there is no need to normalize the
columns of X so again the approaches is
conical method to recover the extreme
waves of the data cone and it is
inspired by characterization of extreme
rays and extreme points from Clarkson
and Dula paper and I think this should
be 1999 so the approach recovers the
extreme rays incrementally one in one
extreme ray in each iteration and here
I've shown a current cone after three
iterations so we have added three
extreme rays to the cone and let's say
we want to expand this cone by a fourth
extreme rate so to expand this cone we
project all the external points to the
cone and find normals through the phases
so you can see the green points are
external to the current cone the blue
points are internal to the current cone
and red points are the extreme rays so
we project all the green points to the
current cone and that give us the
normals to the phases of the current
cone then we take a residual so we pick
a face and then rotate it outside the
cone until it hits a last point on that
side and that last point is our extreme
rate so in this case we add this point
as our anchor column and this is a new
extreme rate and we expand the cone so
here is algorithmic sketch of the method
so we start with empty anchor set and we
have a matrix R
is initialized to X and the first step
is a selection step where we select the
anchor column so this is done by
evaluating this criteria here so we
evaluate our transpose times X I divided
by Q norm of X I and X I is the ith
column of X and then we evaluate this
vector using the selection operator and
the selection operator takes a vector
and gives out a scalar so we do this for
all the columns of X and the column
which gives us the maximum value that is
our anchor column and we add it to our
set of anchors so the next step is after
we have selected this anchor column and
we have expanded the cone in the next
step we want to project all the external
points to the current cone and this is
done by solving this non negative least
squares problem so we minimize the
Frobenius norm of X minus set of anchors
times H with a non negativity constraint
on H and then we compute the residuals
and every column of the residual is
normal to some face of the cone and now
we get the result matrix we go back
again to the selection step pull out a
new anchor expand the cone and then so
on so this is repeated until we have
selected the desired number of anchor
columns
now this is really a family of
algorithms because depending on the
selection operator here and the Kunar we
can have different variants of the
algorithm so one variant is when the
selection operator is it takes a vector
and the output is a jth element of that
vector and suppose Q is equal to one so
in the denominator we have one norm of G
of X I so this variant can be shown to
solve the pure separable problem exactly
so it recovers all the extreme rays of
this data cone and of course the
performance under noise will depend on
which element of V is chosen by the
selection operator but if it's pure
separable problem then you choose any L
any element of V and that will solve the
problem
second variant is that selection
operator looks like this so it takes a
vector V and takes all the non-negative
elements and computes their l 2-norm
and q is equal to 2 here so we take 2
norm of X L now this variant is very
similar to our total matching pursuit
style of algorithms in signal processing
just that it is a non-negative variant
of OMP algorithms and intuitively it
picks a column that minimizes the
Frobenius norm on the current residual
matrix but this variant does not solve
the pure separable problem correct
accurately so it leaves out one or two
anchors but this variant actually
performs very well in the noise so the
noise performance is pretty good
objective which say oh yeah we are
minimizing the normal and
so minimize the urbanist norm of X minus
W times H so that's why we have this
Frobenius norm production here
oops your vanity guarantee because you
mentioned first if if the problem is
solvable I'm going to find it we'll have
no more fat equals zero now for you I
buried them and I get from normal and
what do I know is it good yeah
so turn the noisy problem actually we
have shown empirically that it's better
than the previous methods but on the
formal guarantees we don't have it yet
so we are working on that so if it's a
noisy problem then you want to show that
if your noise is bounded then your
output of the algorithm is also bounded
in some sense so one advantage of the
method is that model selection is very
easy because we are incremental e adding
these topics so in every iteration we
add one topic so all the previous
solutions they are contained in the
current solution so we can stop whenever
we have some external criteria that is
met by the algorithm and we don't need
to rerun it every time for different
values of R and we have a scalable
implementation which exploits the row
sparsity of matrix H and both selection
and projection step here they are very
easily parallelizable and as we will see
later the it compares hillbilly with the
with previous methods on the same
problem so let's go back again to this
swimmer data so I have shown here some
topics that are recovered by local
search methods so on the left side you
can see it's a it's a bad run for local
search because it will converge to a bad
local optima and one of the topics here
is very heavily corrupted on the right I
have shown a good run where it converts
to a reasonably good local Optima so
this is a danger with local search
methods that depending on the
initialization you can converge to a bed
local Optima yeah so
do I care yeah yeah like what but I
would assume that that would you know
whatever let's call that bag 1b you know
I assume it's never used because if you
actually use that topic generating data
you get a piece something that doesn't
look like any example right yeah but
again it has missed one topic because
the topic which should have been here
that is not true
no but if you try to reconstruct your
dataset with these topics you will get
some vectorization error so it's not
exact or even a few more choppers if you
throw in more topics then probably these
topics also will be corrupted because
you want to because you have more topics
then some of the structure that is
present in the current topics that will
move to the next topic
this seems incredibly sensitive to
something that you couldn't yes so so in
this I mean in local search methods you
have to know the number of topics
beforehand that you want to recover so
that is like so that's why I mean you
have to run for a different number of
topics way worse that like hey just look
all that much worse than be if you told
me that if I didn't know the right
number of topics they'd all be screwed
up that's C and I'm like of course I
don't know the number of yes so I mean
so that's why people run these methods
for different values of smaller and then
they basically choose using real
problems don't have well
yeah that's true I mean so so I mean any
topic modeling methods even the anchor
assumption assumes that real theta it
does have that somehow if you looked
carefully enough the data we tell you
yeah so with anchor assumption the odd
method the advantage is that it
incrementally pulls our tankers so I
mean even if you don't know the topics
beforehand you can start running the
algorithm and stop whenever you feel
that the next topic that you're getting
is not does not make sense or something
so here I've shown topics by separable
methods so the other topics recovered by
bit arts method and bit off method has
has two step size parameters so primal
step size and dual step size and
depending on how you set these
parameters the convergence might might
vary so you can see there are some
shadows in the topics and this topic is
not is not very clean in D'Alessandro
verses so again the assumption here is
that the WC matrix should be full rank
so in this data set it turns out that
this assumption does not hold and that
is the reason it stops after recovering
14 topics whereas there are 17 topics in
the in the data now the proposed method
it recovers all the topics and the
topics also look reasonably clean and
there is a clear-cut structure in the
topics now let's add some noise to this
data and see what topics we get using
different methods so on the top left
I've shown topics by local search so
here there is some noise that is cracked
in in the topics and but rest of them
are very clean in bit off there are some
topics that are reasonably well and in
others there is no clear-cut structures
in dallisa novices it turns out that
almost all the topics are kind of blurry
and there is no structure in the topics
in the proposed method there are some
topics that are not very clean like this
one and this one
but rest of them are are kind of
reasonable and there is no noise that is
corrected in the topics so here is the
experiment that tests this method on
recovery of anchors so the generative
model is that we generate X by W times H
plus some noise and W is uniformly
generated between zero and one H has a
separable structure so the first 20
columns of H are an identity and the
rest of the columns are sampled from a
durational a distribution so they lie on
the simplex
Simar is a synthetic greater yeah but it
satisfies the
Yeah right but but the data has an exact
factorization and if you look at the
right sector' matrix you will have the
columns of identity there so so
basically this data satisfies the sub
rebuilding option
so so yeah in this one we have a pure
separable structure and then we add some
noise and the noise is is the Gaussian
noise with zero mean and Delta standard
deviation so here is the result so on
the x axis we have the noise level which
is the standard deviation of this
Gaussian distribution Delta and on the y
axis I have the fraction of anchors that
are correctly recovered by these methods
so the black curve here is the proposed
x-ray method the red line is the hot
topics which is the method of bit off
and their name is at as hot topics and
the blue line here is the is by deletion
process so in this type of noise it
turns out that x-ray is better in
recovering the anchor columns exactly
under noise and of course as we keep on
increasing the noise the performance
drops now in this experiment we want to
evaluate the selected features of
selected words on on a prediction task
so again every row of X is a is a data
point is a document and we are selecting
columns that means we are selecting
words from this corpus and then we train
a multi-class SVM using these selected
words and we want to evaluate the
prediction accuracy using these anchor
words so we use two data sets T DT and
Reuters T DT has 30 classes 30 topics
and reuters has 10 topics now on the
x-axis I have the number of selected
words anchor columns on the y axis I
have the SVM accuracy so you can see
that as we keep increasing the number of
birds the accuracy increases uniformly
but there is a huge gap between the
proposed x-ray method and bit off
Angeles so the the magenta line is the
x-ray greedy variant and the black line
is the probably correct variant of x-ray
so both these variants are better than
than the previous methods
is the local search just the greedy
local search on this chart local search
is is not there but it lies somewhere so
it's better than the separable methods
it lies somewhere here somewhere here
and the reason is that because it's not
restricted to selecting only the columns
because it allows mixing of these
columns right so you get better
prediction accuracy so you have a
classification experiment yeah so you
use the lower so we use we use matrix W
to train to train our classifier is the
number of columns in W yeah but is it
that you shrink a bigger R or is it that
you solve the problem for given orange
so so on the x-axis I have the number of
selected features the number of columns
in W and as you keep increasing the
columns your accuracy keeps going up
because you had more and more features
to a classifier and you want to see how
good are these features on a prediction
task so you want to evaluate the quality
of these selected features yeah that's
the dotted black line if you use all the
words enjoyed SVD
so I mean be evaluated non-convex nmf
methods and they are little bit better
than I I think yeah you are right in
that sense because pulling out these
anchored columns takes time and I think
linear SVM will be faster if you use
just all the words but this experiment
was just to evaluate the quality of
words that are selected by these
different methods and test their
prediction performance data for
the factorization and then also we split
data so the number of documents we split
in 20 and 80 20 percent is for training
and 80 percent is for testing and H
packets or it's on the whole you know so
it's totally unsupervised so it's on the
whole data point so basically like most
papers which deal with those dimension
reduction they show that at one point if
the training set is small enough the
representation you get by doing the
decomposition is better did you look at
this like so so I I can so you say I
have 20 percent training now as I go to
four percent training is it better than
the original features on my paper try to
sell it in this way is the right way to
sell it but yeah that make sense
my supervisor I rule to make up for the
lack of training I think that makes
sense but I think in this section we
didn't build it right I think I
increased from 20 started from 20 but
and then increase it further and I
observed that it's not better than if
you use all the words but maybe if we
have like 10% of 5% training data then I
guess these methods will outperform
so this experiment evaluates the the
selected features on clustering tasks so
I mean I'll not go into more detail here
but the you can see the magenta curve in
the black curve of the proposed method
so they are better than beta orphaned
Angeles on both these data sets here are
some large scale experiments that we did
so we evaluate it on three data sets are
CB 1 ppl 2 and IBM filter data and the
statistics of the data sets are here and
we compare with hot topics which is the
method by bit off and co-authors so this
is also very scalable method and it's
optimized using STD and on you can see
that on ppl 2 we are about three times
faster than hot topics and on Twitter
data which is very very sparse we are
around 60 times faster than hot topics
so we have a we have a scalable
algorithm that's that can be practical
so we didn't look at the factorization
error here yeah so it's just running
time for recovering 100 topics the
problem is that if you compare the error
it's not Apple to Apple comparison
because in hot topics you optimize the
different norm on the on the error and
in X let's say position and measure the
previous norm of my yeah I mean I'm sure
that if you look at the norm of through
biggest norm of noise then x-ray will be
better because hot topics you optimize
the some different norm so
so next I'll move on to some extensions
of these algorithm to other loss
functions like l1 loss and Bragman
divergence so let me start with
introducing the problem of low-rank
approximation so in loading
approximation we are given the matrix X
and we want to approximate it with a low
rank matrix L and so we in the objective
we have a rank penalty here or we have a
rank constraint and this problem is non
convex but we can get a global solution
using SVD in polynomial time now we can
imagine that low robust counter part of
this problem where we assume that the
noise matrix is sparse so we replace the
flow being is known by by l1 norm and we
retain the rank constraint as it is so
this problem is also non convex and it
is also believed to be np-hard in so
both these earlier problems that I
discussed we can imagine a non negative
rank version of these problems so a non
negative low-rank approximation we are
given a non negative matrix X and the
goal is to approximate it with a non
negative matrix L with this constraint
that non negative rank of L should be
small and this problem is exactly the
Animas problem which is np-hard now in
the robust version of this problem we
assume that the noise is sparse so we
replace Frobenius norm by this l1 norm
and we have the non negative rank
constraint and this problem is also
np-hard now both these approximations
the low-rank approximation and low non
negative rank approximation they can be
used to do foreground/background
separation so background is something
which is slowly varying across data
points and foreground is something which
is varying relatively faster but it has
low energy so it can be used to do
foreground background separation in
video as well as in text but because
because the previous previous versions
of the problems are NP hard the question
is how to make them tractable so here is
one popular approach to make robust
low-rank approximation tractable so they
replaced the rank function by this new
clear norm penalty and
then this problem becomes convex and
this is well study well studied in the
literature and then the name of robust
pca and the in the robust load non-
ranked approximation we propose to use
separate bility assumption to make it
tractable because the this problem
earlier which we saw this was np-hard so
we proposed to use separate bility to
make it tractable so the problem is that
we want to minimize the l1 norm of X
minus W times H with a separable
structure on H and the question that we
ask is how does it compare with convex
if I'd robust pca on common applications
now before going into this i will talk
about the algorithm that we have so so
again this is a conical algorithm that
recovers the extreme rays of the conical
hull and this time we do it with the
element residuals so we project all the
external points to the current cone and
the projections are in terms of l1 norm
and once we have these projections we
pull out a new extreme day using these
elements visuals and expand our cone so
here is a sketch of the algorithm we
start with empty anchor set and the
matrix D is initialized to X so the
first step is the selection step where
we evaluate this criteria here and that
gives us the end in the next anchor
column so di is the ith column of D XJ
is the 8th column of X and P is any
positive vector so whichever column of X
gives us the maximum value of this
criteria that is selected as anchor
column and once we have expended the
current cone we take our external points
and then project onto the current cone
and this is done by solving a non
negative least absolute division problem
so we minimize the Elven norm of X minus
set of anchors times H with a non
negativity constraint on H and the
matrix D is just a matrix of sub
gradient of this loss function so if the
residual residual entry is 0 we have
dij zero if the residual is non G non
zero then we have the sign of the
residual as SD IJ and once we have the
matrix D we go back to the selection
step pull out a new n correct and expand
the cone and this is done until we have
the desired number of anchors now this
procedure it can be shown that it
probably solves the pure separable
problem but again in the in the noisy
version it will depend on on which I we
end up choosing here now this method can
also be extended to handle more general
loss functions inside bergman divergence
so I want you to to focus on the on the
last bullet here so we want to minimize
the divergence between X and W times H
and we have a separable constraint on on
H and so this problem we have a
selection criteria that recovers the
anchor columns using Bragman divergence
but I'll not go into more detail here
because I don't think we have we have
enough time so this is again to test the
method on on recovery of anchors so
generative model is X equal to W times h
plus noise and W is again uniform
between 0 &amp;amp; 1 h is separable and the
noise matrix is now Laplace so it's
sampled from a Laplace distribution with
0 mean and Delta a standard deviation
and once we have the noise we make all
the negative entry is equal to 0 so
there are approximately 50% entries in
in the noise that are 0 and the recovery
performance is here so on the x-axis I
have the standard deviation of the noise
which is Delta on Y axis are the
fraction of anchors that are correctly
recovered so the black curve is a
proposed robust x-ray method and it's
better than the previous methods and
this experiment is just as a sanity
check to make sure that the method is
performing as expected so it forms well
under under the sparse noise setting
now let's go back to the foreground
background separation problem in in
video so again here our data matrix is
is such that we have each row a video
frame and we want to decompose it into
two matrices a low non-negative rank
matrix and a sparse matrix so low
non-negative rank matrix will model the
background and sparse matrix will model
the foreground and we want to see how
separable nm of methods perform under
under this setting so here I've shown
some images on on this data so you can
see that the separable method with the
with the inner dimension of to it
recovers the background and foreground
reasonably well is that you cannot take
the word intensity when you have those
like components which like for texture I
guess it's more three two and four so
you're asking in terms of separable
methods or in general animals and why
would I use a like they have pixel
intensities and I want to have like kind
of decomposition and ammonia no too
yes so I mean the most popular methods
for this problem are as you said based
on robust pca where you allow negative
entries our motivation was to check
whether non-negative matrix
factorization does perform well for this
problem or not and again the I think the
intuition is that so your background is
that is a low non-negative rank matrix
and I guess you want to represent every
pixel that is present in the in the in
the video frame as a non-negative
combination of certain elements
so again like text data this is also
non-negative non-native data so I mean
the motivation because of the topic and
then maybe you wants to subtract it if
another topic is presently they are say
topic is something hiding your lamp or
something right they cannot do that
right then and for text data it's kind
of intuitive that you don't see you
don't have you don't put the top against
the need to eat words for images it's it
seems I mean you can also look at it in
terms of energy emetic sense so if you
take all the pixels right and you
represent it in the inner space so all
the data is contained in the
non-negative
and all you are doing it to to basically
bound this data in a cone basically and
I mean that can always be done and now
you're asking whether there exists a
cone with very small number of extreme
rays that can that can basically model
the background where there's several
light sources and the West versus is
thrown on or off well it makes sense to
have specific just top so X
corresponding to the lighting being on
or off in certain ways which would just
eliminate certain fraction of pixels but
I mean if some pixels are off for
example you can put a low of it on the
on the off light source factor and they
will be no I mean they can be any real
number bigger than zero
that seems kind of like for like if you
give me like scans in stuff I'll to tell
you by if you give me like natural
images I can see both effect in term of
explaining the image like with a few
source of Violation I can see that you
have subtracting it effect in the same
topic as additive effect better you have
something moving in front of the light
so it will it would it will basically
this thing is left or right right so
this would use exactly three we need
like four topics there instead of two
because yeah I mean that's always there
so you you may end up increasing your
number of topics if you have this
non-negative constraint but again this
on this date actually this result was
obtained with the factorization of inner
dimension two so it's a very low rank
factorization yeah I have it in the in
the next slide so this is a quantitative
comparison between robust pca and and
separable nmf so again robust piece here
we don't have any non negativity
constraints on the on the low rank part
so we use two data sets here bootstrap
and Airport Hall and for these two video
data sites we have that correct ground
truth so we know which pixels are
foregone so we have them labeled and
here I've shown the ROC curves for these
two data sets so on the x axis I have
the false positives on y axis I have the
true positives so in this in these two
data sets you can see that the robust
x-ray is as good as robust pca is
performing as good as robust pca and the
advantage that we get here is of speed
because in robust pca
you have to solve in each iteration you
have to solve SVD problem so this in
some sense limits the the speed
like Angela it works quite like the
first thing I will try you that you'll
give you a bunch of images right we do
SVD and subscribe the main components at
the foreground yes yeah yeah so so
robust pca just a convex relaxation of
the SVD problem because I mean SVD you
have to know what what rank you want and
here you have so what I'm saying is that
you say you're saying two things there's
a row rank representation which explains
the background but and there's the
sparse foreground right yeah let's have
remove this possible problem but you
human you brought the and I've heard the
ROC curve so the problem becomes the I
suppose useful maybe you get most of so
I mean so in SPD the assumption is that
you have Gaussian noise yeah you know I
I'm not free using the audio section I
could use this over yeah
but again I mean I think in this sort of
setting where foreground background
separation I think SVD is not going to
perform well so you need like you need
to model a sparsity of foreground
now this is my last application here so
so far we saw that we had data points
along the rows of this matrix and we
were selecting columns so this was
equivalent of selecting some some
features from the from the matrix and
now let's say we we transpose this
matrix X so in this case we will select
some samples from the data which are
representative or exemplars of your
whole data sets so this setting can be
used in video summarization or in text
corpus summarization in terms of a few
few documents or a few number of frames
and we compare in this setting we
compare method with the with this method
which was proposed in 2012 and they do
it by solving this problem so they
minimize the Frobenius norm of X minus x
times C and there were rows parts
penalty on C here and these are the
results so we use Reuters and DBC
which are text datasets and the setting
here is that we select some samples and
then we train a SVM with those selected
samples and the hope is that if the
selected samples are diverse enough in
that case we can get a good prediction
error with with with a small number of
samples so on the x axis I have the
number of selected samples and on the y
axis is the prediction accuracy so as we
keep increasing the number of samples we
get a jump in the prediction accuracy
and the red line and the black line are
the x-ray method and the magenta line is
the method by al-hawa so on Reuters
I think robust x-ray is better than both
these methods and on and on BBC it turns
out that frumious non-persian of x-ray
is better than and then the robust
version and and along fer
so in the end I will just summarize what
we discussed so we saw the septa build
assumption and it makes the Animus
problem tractable and it turns out to be
a reasonable assumption in topic
modeling and hyperspectral and mixing we
also saw a separable scalable family of
algorithms for near separable nmf and
the algorithms are based on recovering
the extreme rate of the conical hull and
they directly attacked the conical
health problem and not it is not
required to scale the columns of X and
the solution is built up incremental e
so we select one extremely in each
iteration so it's very easy to do
cross-validation we also outperform the
previous methods in terms of performance
and and speed and the algorithms can be
extended to other loss functions like
element loss and fragmentary genes now
in the future we want to do a formal
noise analysis of the of the method and
we also want to work on the streaming
version of the algorithms where either
rows or columns of X are coming in a
streaming fashion in the end I will
acknowledge my collaborators here so
because in the money and provision
combos from IBM TJ Watson and I'll just
very briefly mention some other work
that I have done so I worked on
multi-touch learning modeling the
grouping structure in multicast learning
I have also worked on multiple output
regression where we model the
conditional mean and conditional in
conditional inverse Kovan structure on
the on the parameters and i've also
worked on a little bit on transfer
learning in this line of work I have
worked on multiple kernel learning so we
are given a set of kernel so you want to
combine those to learn a final good
kernel here the setting is that we are
given multiple similarity graphs and we
want to learn spectral embedding
combining these similarity graphs so
both these approaches they are based on
either core training or
realization and this problem setting is
that we have a database in which we have
data points from one modality and the
query is coming from a different
modality and the goal is to give in a
query we want to recover the
corresponding element from the database
so one example is like image retrieval
from text query and and vice versa
and I guess that's it from me and thank
you very much for your patience thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>