<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Barrelfish: A Study In Distributed Operating Systems On Multicore Architectures Part - 1 | Coder Coacher - Coaching Coders</title><meta content="Barrelfish: A Study In Distributed Operating Systems On Multicore Architectures Part - 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Barrelfish: A Study In Distributed Operating Systems On Multicore Architectures Part - 1</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gnd9LPWv1U8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay hello again everybody in this talk
I'm going to be talking about barrel
fish which is a research operating
system that I was involved in for a
couple of years so again I'm not going
to be telling you about any particular
kind of concepts that you might want to
take away but I'm going to be describing
this research operating system and how
we dealt with the challenges of modern
multi-core architectures okay so the
basic problem
the starting point for this work was
that the operating systems that we use
today actually not very well-suited to
what we anticipate for hardware in the
near future and in the long term as well
the operate the way operating systems
are structured means that they don't
scale well as the number of cores
increases they don't handle the
increasing diversity of hardware you saw
yesterday how Hardware was very diverse
in the past a lot of people anticipate
that it will become increasingly diverse
in the in the future as well
and the other problem with operating
systems today is that as Hardware
evolves because of their structure
because they're they're very large
complex pieces of software it's actually
very hard for the operating system
developers to keep up as the hardware
changes so you know optimizations that
are tuned for one type of hardware I
have to be retuned for a different type
of hardware and so on and that can be a
very complex procedure
so I mentioned this yesterday but the
the basic premise that I'm working from
is that computer hardware is looking
more and more like a network by that I
mean there's a high communication delay
between cause the cause can come and go
with power management etc and the cause
are not necessarily homogeneous say they
can be of different types within the
same system and as a consequence we has
in the the members of the bearish team
believe that the operating system should
be structured in a very similar way to a
distributed system so in order to
articulate this idea about structuring
an operating system as a distributed
system we invented a reference model
which we called the multi kernel model
so it's basically a writing down of the
way in which we think operating systems
will need to be structured for future
hardware and it's based on three design
principles so the first one is explicit
message passing so the basic assumption
is that all communication will be by
message passing rather than by shared
memory a hardware neutral structure so
we're advocating that you don't want to
be designing your operating system to be
tuned for the hardware that you
anticipate it will be run on and
replication of common States so instead
of sharing common State as the default
we're saying you should replicate it so
barrel fish is an implementation of the
multi kernel model okay we invented the
multi cam model and we built barrel fish
and in fact we did both of them at the
same time but we believe that the the
abstractions that we're talking about
the multi-channel model can also be
implemented by I mean there are
different ways to implement them and
that will become clear
okay so the outline of this talk I'm
going to start by looking at multi-core
Hardware today and the diversity of it
in particular yeah
mm-hmm
so similar to clustered objects that I
mentioned yesterday it's really going to
depend on the nature of that state and
how it's going to be used so state that
partitions well and where mote you know
most of the accesses will be local
obviously you want to replicate so that
you you have fast access to the parts of
that state that are relevant to the
local core in other cases you almost
certainly want to have some kind of
server process and be sending it
messages that's a implementation
decision and in fact that's part of the
reason why barrel fish is not the only
way to implement this model but I will
go through these design principles in
more details so hopefully that will make
that clearer yep
I will show you examples actually
okay so I'm going to briefly talk about
multi-core Hardware and then I'm going
to talk about how the characteristics of
this hardware challenges the way that
current operating systems are structured
and then I'm going to go on and describe
the multi kernel model in some more
detail followed by the barrel fish
operating system itself okay so what
we've had up until now is Moore's law
being a very reliable predictor of
increases in computing speed moore's law
states that the number of transistors on
a chip will double roughly every two
years at pretty much a constant cost and
this is basically because of advances in
semiconductor technology leading to them
being able to increase transistor
density and in fact it's been pretty
reliable since 1965 which is quite
amazing so this graphic is which I think
I took from the Intel website is just
showing the year of introduction on the
X and number of transistors on the y
axis and highlighting some particular
chips there so a lot of people have
observed that there's some problems with
Moore's law continuing the way it has
done since 1965 so the improvements that
we see are driven specifically by
increases in transistor density
increases in clock frequency by power
savings per transistor and various
techniques to exploit instruction level
parallelism all of these things are
running into problems and basically it's
it's like a big red flag saying well
Moore's law isn't going to continue to
hold
first of all the powerwall this is a
kind of cute picture showing somebody
frying an egg on an AMD Athlon okay so
what's this really talking about well
one of the things that drives Moore's
law is that we can increase clock
frequency however clock rate and power
are correlated so the power dissipation
depends on the clock rate and on the
voltage increasing the clock frequency
means more power is dissipated so more
cooling has to be required
correspondingly decreases in the voltage
we can compensate for that by reducing
the power consumption but then you get
more static power leakage so either way
we're running into the limit of being
able to cool commodity micro processors
next problem is known as the memory wall
and this is talking about the growing
disparity of speed between the CPU and
accessing memory especially memory
outside the the CPU chip so because of
these increases in disparity memory is
basically becoming the bottleneck so in
1985 with a 1 megahertz CPU clock it
only took 500 nanoseconds to access
memory memory speeds have grown about 10
percent a year and CPU about 55 percent
so obviously there there's a problem and
it's interesting to contrast this with
the first system I talked about
yesterday the CDOT MMP where the memory
speeds were hugely greater than the CPU
and they had a difference
and then there's the ILP walls so
instruction level parallelism is is
basically a bunch of techniques to
reorder and pipeline instructions which
basically makes things go faster however
it does require some more requires more
complicated hardware so it increases the
complexity there and also increases the
power consumption and so we're back to
the same problem and essentially the
performance improvements that we can get
from ILP have stopped in have stopped
increasing so this this phrase power
wall plus ILP wall plus memory wall
equals brick wall I think that comes
from a report by Berkley from Berkley by
Dave Patterson and other people and so
to summarize the we we can't make
processes faster performance is
dominated by the time to access memory
and while accessing that memory the
stalling of ILP improvements means that
the the processors can't keep busy while
waiting so as a consequence we're moving
to multi-core the improvements in speed
that everybody expects are coming from
having multiple cores so multi-core
processors it's basically the future of
computing or even the present
I don't think Microsoft has shipped a
uniprocessor version of the operating
system since 1996 I believe so Microsoft
obviously decided a while ago that there
that was that was done most multi-core
chips so far shared memory
multiprocessors SMP machines
or um a machines they have a single
memory shared by all the processes and
communication essentially happens
through shared memory and as I talked
about yesterday the the hardware will
provide the cache coherence so let's
look at how trends with these types of
machines are affecting the resources
that are available to the operating
system so here there's there's a typical
configuration of an SMP machine we've
got two calls sharing a level two cache
and there are four cores on this chip
and they go off chip to main memory if
we double the number of cores on the
chip we've got double the number of
cores sharing the cache so this leads to
some problems more calls does bring you
more cycles so you can get more done
faster however often it's not
necessarily proportionately more cache
and almost certainly not more off ship
bandwidth or total main memory capacity
on the network side again we have in
this picture it's showing more of the
interconnect between the chips and main
memory and the i/o system so again we
introduce more cores and if they're
faster essentially what happens is more
memory bandwidth is used as a
consequence and so buses have to be
replaced with point-to-point networks or
crossbars and we see the the Numa memory
configuration as well - to compensate
for the contention that's around with
the memory access and basically this
means that the interconnect apologies
are getting more complicated and
the latencies to access memory
increasingly important and cache misses
are more and more expensive as the
number of processors grows or the memory
demands from each processor grow like
any centralized resource in a system it
the interconnect does become a bandwidth
and so as a consequence of of more and
faster processors the memory bandwidth
also needs to increase ok so what about
the the new Merkin figuration so here
I'm showing multiple CPUs where each of
them has some local ram and this reduces
the memory bandwidth contention
basically because the memory is
physically distributed however as as I
said many times yesterday the time to
access memory depends on the location of
the word in memory and and so this is
where we often see message passing
multiple address spaces and processes
exchanging messages to communicate and
this can potentially scale a lot more
than an SMP machine to hundreds or
thousands of cores even ok so I forgot I
had the animation right adding CPUs
brings more well memory caches and
there's you you definitely get better
locality so the the CPU the processor
only needs to go to memory for in the in
the case of actual shared data which is
good so the the implications of these
trends in Hardware for software the
basically that the the performance that
we get for free from Moore's Law is
leveling off so this this graphic is
showing
sequential performance on the y-axis and
a notional year on the x-axis so
historically we we've just been getting
these gains as I said before by the the
increases in clock frequency and and so
on
now that's leveling off we're getting
more cause and so there's there's
definitely a perception there that
there's this a region of lost
performance you know the software has to
figure out how to make use of those
additional calls to to give us back the
the slope that we're accustomed to I
mean you can't deny that software is
becoming more and more resource hungry I
mean you know there's fancy graphics
rendering we're crunching larger and
larger amounts of data more complicated
functionality our expectations of
software are always getting higher we
never want to go back to you know
software from the 1980s and so we have
to keep that performance trend heading
upwards or else it will feel like
computing is going backwards and I don't
think that will be acceptable to a lot
of people ok so that's a my my very
brief overview of where we stand with
multi-core hardware at the moment and
there's a lot of interesting things
written about this and if if that piques
your interest these are a good place to
start
okay so getting into the specifics what
are the challenges for operating systems
given this environment this context of
of multi-core hardware this is a picture
of how operating systems today are
structured the monolithic operating
system like like Linux like Windows so
basically there's an assumption that all
the cores are homogeneous main memory
holds the global data structures and so
there has to be synchronization to
access global data but it's a very kind
of simple structure and Windows is an
example okay it's simple from the
outside but actually when you dive in
there's there's quite a lot of
complexity within the operating system
so in Windows this is a kind of picture
of how Windows modules are structured so
we have something in in kernel space we
have something called the executive and
this is what manages the processes and
the threads memory the system
configuration power management and and
all that stuff the kernel itself is very
small very minimal and that's that does
what we think of as the core operating
system functionality the the thread
scheduling and synchronization handling
interrupts dispatching exceptions you
could think of the executive a bit like
that the policy for the operating system
whereas the colonel is mechanism so the
executive decides what's how things have
done and the kernel is you know holds
the the mechanisms that are used to do
that and then there's something called
the hardware abstraction layer which is
the low-level interface to the hardware
and that's what makes Windows portable
so the challenges for operating systems
like Windows I'm going to talk about
four things in particular here AM Dells
law the fact that the communication
communication between processes and
between memory and processes is that the
costs are increasing the costs of
sharing data in this environment and the
diversity of hardware and the message
that I'm trying to get across here is
that current operating systems were not
designed to scale and were not designed
to effectively make use of the the
parallelism available to them okay so um
Dells law if sorting takes 70 percent of
the execution time of a sequential
program and you replace that sorting
algorithm with something that scales
perfectly then on a machine with n cos
how many calls do you need to get a
speed-up 4 times the original well this
is the the speed-up on the left and the
number of calls on the x-axis the top
line is showing the the speed-up that we
want our 4x speed up but the actual is
somewhat underneath there and if there
even is you get up to 16 calls you're
not even at a 3x speed up why is that
this is the basis of Amdahl's law so
here F is the fraction of the program's
execution time that is infinitely
parallelizable so that's like the the
sorting algorithm and the example two
slides ago we have to assume that the
remaining fraction of the program is
totally sequential so it has to execute
sequentially and then the maximum
speed-up on C processes or cos is
defined by Andals equation so
what's going on well looking again at
our 4x speed up we look to the right the
limit as the number of cores or the
number of processes approaches infinity
is actually only ever going to be 3.33
given that our the parallelizable
section of this program is only 70%
we've got 30% sequential and that's
going to stop us ever achieving the
speed up that we want him if that
parallelizable fraction is really small
then according to Andals law the limit
is only one point 1 1 times if it's 98%
even it's still not great so here we're
going for a 50x speed up even at a
hundred and 26 cause it's only we're
only at 36 X so what are the
implications any serialization will
limit scaling even things like messages
that are being exchanged serialized in
flight a lot of people have talked about
whether this means there are practical
limits to the number of parallel
processes there's a lot of very
interesting questions around when the
costs of executing a parallel program
the overheads of parallelizing outweigh
the benefits but that's another whole
separate area that i'm not going to go
into any way the the corollary a
corollary of Amdahl's law is you really
want to make the common case fast in
your program the opportunity for
improvement is affected by how much time
the event that you're improving consumes
so you know if the if F is small then
the optimizations you make are going to
have a very small effect an example of a
Modell's law in in the real world is RSS
in Windows so before 2007 the
the windows networking stack really
failed very poorly and the reasons for
this were basically no parallelism the
the packet processing had to be done on
one CPU at a time and there was no way
to balance the overall load of the
system and cache locality was very poor
because CPU where the packets were
process wasn't necessarily where they
were consequently so subsequently acted
on so that the solution increase the
parallelism as per M Dells law so RSS
stands for receive side scaling and
basically it takes TCP connections
applies a hash function and uses that
hash function to decide which CPU should
process the packets it has some clever
stuff to preserve in order packet
delivery and load of other things but it
does require hardware support so this is
the kind of this is an example of an
operating system feature where the
operating system designers did not
anticipate the multi-core hardware and
so were bitten by Andals law and had to
go in and do a pretty substantial
redesign or introduce a pretty complex
feature that involved hardware support
in order to compensate
okay the next challenge for software
that arises from multi-core hardware is
basically just the cost of communication
we saw a lot of this yesterday and it's
still the case today so in this example
there's a diagram of a machine chip
called AMD Barcelona which has eight
sockets and four calls on each socket
and it has this kind of weird
interconnects topology so it really is
the case that the the the first socket
can send messages to its neighbors below
and beside and so on except for the last
ones which also have this diagonal
opportunity and I do not know why that
is so looking at the memory access
latency here for a quarter access it's
local level one cache is two
microseconds level to 15 on a different
core the cache and on different core 130
and level 375 however going to level 3
caches on different sockets is varies
between 190 and 369 down there in a
table sorry those are cycles not
microseconds if we look at the those
costs normalized to the cost of
accessing a local level one cache when
the call has to go to a cache that's 2
hops away it can be up to a hundred and
thirty times as expensive as going to
the local one and this might not seem
like very much but these the fact that
these differences arise can actually
have the significant effects in software
because there's there's a lot of layers
and there's a lot of complexity but when
it comes down to it
those differences do matter okay so the
the third multi-core challenge is the
cost of sharing so I reviewed hardware
cache coherence yesterday so I won't go
through this is exactly the same slide
what are the implications of having
hardware cache coherence well basically
when you squint at it accessing shared
memory is essentially sending messages
the cache coherency protocol is causing
cache lines to be moved along the
interconnect between cores and any kind
of sharing right sharing of a single
cache line means that those cache lines
are going to be moving around the
machine even when the data isn't truly
shared so false sharing which is what's
happening when the data is not really
shared but it's co-located on the same
cache line I have a picture here showing
what happens
so if core a is updating variable X in
its local cache x and y are both
co-located on the same cache line okay
so Corre updates X call B has a copy of
X in its cache so as you may remember
from the hardware the state diagram of
the cache coherency protocol updating
the cache line will cause it the copies
in other caches to be invalidated so out
that goes now if Corby wants to come and
read Y which is not a variable that it's
sharing with X but it happens to be on
the same cache line it has to to to
issue a fetch X will give it the copy of
the cache line and so on this all just
you know continue
so the
the two calls will be bouncing the cash
line around even though all they're
doing in in as far as the software is
concerned it's reading two variables
that are independent reading and writing
independent variables so this is an
important design feature for software
people especially in an operating system
the the software developers have to be
aware of what data might be shared
inadvertently and make sure that they
deal with it
and they need to understand how their
their caches are working how their cache
coherence protocol works so that the
costs of this obviously it's generating
a lot of unnecessary traffic on the
interconnect as I just said the
designers need to lay out data
structures really carefully if there is
false sharing then essentially that
those two operations the reading the
write on the variables X Y in the
previous example they're essentially
being serialized and fine-tuning code
like this makes it quite complex and and
requires significant engineering effort
it's actually very it can be very
difficult to find these problems it's
quite can't the effects can be quite
subtle and even knowing where to look it
can be quite difficult to find
especially in a substantial piece of
software and of course fine tuning to
avoid something to avoid false sharing
reduces portability to other types of
hardware ok so the next multi-core
challenge is contention on global
operating system resources so I want to
show you a short video this is just a
three minute video and it's an interview
with a guy called mark russinovich so
he's a technical fellow at Microsoft he
currently works in Windows as year that
at the time this interview was made he
was work
in the windows kernel team so he was the
founder of a company called sysinternals
which reversed it reverse engineered
Windows in a very impressive way and
developed a whole suite of tools for
analyzing performance for monitoring
what's going on inside Windows which
were free still are free to download and
incredibly useful everyone in Microsoft
used them so then Microsoft bought the
company and Mark started working for
Microsoft so here he's talking about
some of the internal improvements to
Windows 7 so this I think this video is
a couple of years old and I've just cut
out for the the three minutes I think
irrelevant he's going to be talking
about getting rid of a global data
structure called the dispatcher lock in
Windows 7 and how difficult it was so
I'll play that now sound big in the
sense that it has been something that as
systems have gotten bigger and more
we've got more cores what hearing has
head as a problem more and more and it
was a daunting engineering exercise to
figure out how to address this problem
that is something called the dispatcher
lock okay so in the scheduler leading up
to win 7 when ever a thread waits on an
object or wait wakes up on an object
whenever it makes a transition like that
from running to waiting in the scheduler
it there's an acquisition of a global
lock called the dispatcher lock and so
if you've got on a multiprocessor
workload but you've got lots of threads
running on the system on different CPUs
and they're waking up and sleeping and
communicating with each other
the dispatcher lock becomes beef hot
what we call the hottest lock in the
system caches that lots of threads lots
of cpus end up spinning waiting for that
lock and you
not to be able to do anything useful of
all those threads are spinning and you
know a two-way or four-way system weave
in an eight-way or 16 way that's not a
big deal that as you get into larger and
larger systems it gets into a bigger and
bigger problem and you start looking at
sequel workloads which is what we see
today on larger systems it ends up being
a significant issue for limiting the
scalability of those systems and one of
the things that we're doing is looking
for - we're seeing of course Moore's Law
not work heat and power and density not
all to support their kinds of increases
in clock speed that we've been seeing
and now we're moving the multi-core
systems and there's people that say
we're going to have our airplane waste
systems on our desktop in a few years
any case it's a big problem on servers
as they get bigger it's also potentially
a problem the Futurama client systems
and a guy named Arun Kishan who's a
kernel developer here kind of a
superstar took it as a kind of a side
project to see if he could break it and
Dave Cutler the original architect of
the scheduler spent a lot of time
himself working on trying to get rid of
that dispatcher lot and kind of he's
moved on to the rent dog team which is
something else so I've been disclosed at
the PD scene so arena took over the
scheduler and decided to tackle it
himself and his it's one of those cases
I think we're somebody come in with a
fresh look from the outside you're not
involved with the original developer the
code can see the problem in a slightly
different way and figure out I went
through it
you did that standing and so that's kind
of an unexpected bonus it we know we
couldn't go on planning to get rid of
that because it was such an intricate
problem okay so um I believe that this
business of getting rid of the
dispatcher lock actually involved
touching 58 separate source code files
inside Windows so it was quite a big
deal okay so the the final multi-core
challenge for software is the diversity
of hardware so we're looking at an
increase in heterogeneity of cause we're
going to see cause with different
performance characteristics different
instruction set variants different
architectures specialized functionality
and in fact we're already seeing this to
an extent and we we really can't tune OS
design to any one machine architecture
anymore and and of course these changes
in hardware typically happening on a
much faster scale than system software
and so the engineering effort to you
know fix the various bottlenecks that
arise as the hardware changes are
becoming really huge so some examples
the AMD Barcelona that I mentioned
earlier this is a quad core there's a
shared level three cache between the
four cores and the other two level two
and level three l2 and l3 our victim
caches which is a different way of
arranging how data moves between the
three levels of the cache hierarchy some
Niagra to this has eight ultrasparc
calls on the same die and each core has
eight concurrent Hardware threads and
there's there's an interesting paper
which we reference in now our paper on
barrel fish about
designing reader/writer lock that was
optimized specifically for this
particular architecture and you know
something that's going to work here with
this particular cache hierarchy isn't
going to perform particularly well on
another machine so here there's a shared
four megabyte layered level two cache
divided into three banks and that's
accessed by an eight port crossbar
switch and then there's the the Cell
processor so here there's a
general-purpose call the power processor
element and eight specialized
coprocessors for data intensive stuff
for multimedia processing vector
processing cryptography and so on and
this I saw some stuff on the internet
that this is has a reputation for being
extremely difficult to program recent
research system is something called the
single chip cloud computer so here
there's 48 cause they're all on the same
chip and there's an on-chip Network and
there's hardware support and this for
message passing and no hardware cache
coherence so that's an interesting
machine so in conclusion these
multi-core challenges parallelism we've
got problems with Andals law we've got
the the costs of sharing we've got the
the bottlenecks that are introduced by
global resources as the number of cores
scales and then we've got these really
high costs of communicating going to
main memory or communicating between
cores and added to that this diversity
in Hardware this wide variety of systems
that's starting to appear the fact that
they're evolving all the time and that
we're seeing heterogeneity within within
one system and so to reiterate we we
believe that
operating systems okay they're
performing all right in this environment
at the moment because of tweaks like
what what you saw mark russinovich
talking about there but they're really
not designed they weren't designed from
the outset to to deal with this
environment and there's some some
further reading the this technical
report is is the report I referred to
earlier with Dave Patterson's quote
about the the brick wall it's a really
interesting read actually if you're
interested in parallel computing I I
recommend it okay so I think I've got
about fifteen more minutes before lunch
so I'm gonna dive into the multi kernel
model but and then continue after lunch
with the rest of it I won't go through
all of it in 15 minutes okay so I showed
you this slide before basically the
multi kernel model allows us it's
structured in order to allow us to apply
insights from distributed systems to the
problems that we're facing of of scaling
of diversity of adaptivity in operating
systems on future hardware so what are
these properties of a distributed system
that we think the the operating system
should be cognizant of
well obviously parallelism
message-passing state replication and
being able to use all of these features
in order to get good scalability
potentially to get fault tolerance
although that's not something we've
considered yet in barrel fish to get
very good locality so everything that's
local is very fast and to make sure that
where there is sharing as you'll see
sharing is actually inevitable you know
at some level in a tightly coupled
multiprocessor there is going to be some
share
so I'm I'm cheating a bit by telling you
it's purely message-passing but I will
explain where we do have sharing we
should be it should be explicit and
that's something you see in a
distributed system as well so here's a
diagram of the multi kernel model of
this this notional reference model so at
the lowest level we've basically got a
bunch of heterogeneous cause there's no
shared memory between the cores at least
that's the idea they don't share
anything in memory running in the
software we have a small amount of
architecture specific code which is it's
necessary however the bulk of the
operating system functionality is in
this general-purpose OS node which has a
state replica for operating system data
structures and the only way that these
OS nodes communicate is by exchanging
messages asynchronously or non-blocking
so an example of where we are actually
going to be using shared memory in this
even in this reference model let alone
in the implementation is that we need
some sharing in order to implement these
messaging channels so the state replicas
on the OS nodes are going to need to
coordinate a be coordinated in order to
make sure that state is consistent and
so here we the multi kernel model says
you use some agreement algorithm to to
do so and then above that we have a
bunch of applications which may be local
to a core or may span multiple calls and
by the way the multi kernel model is not
saying anything about how applications
share memory so applications are
perfectly free to share memory if they
wish to do so there's we're completely
agnostic about that we're just talking
here about the operating system
so the the multi-channel model is built
on three design principles and I'm going
to go through them one at a time so the
first one is explicit message passing
okay so fundamentally I mean we had
talked about this quite a bit
certainly yesterday in a bit today
message passing decouples the structure
of the system from the way in which
calls communicate with each other so in
contrast to shared memory where it's all
tied up the you know the the way that a
data structure is arranged for example
is is very intrinsic to the way in which
calls
share that data structure with message
passing we have the communication
patterns very explicit and that gives us
opportunities for optimizations and also
for debugging and for changing the way
that these communication patterns take
place there are also opportunities for
things like pipelining and batching with
message passing and we believe that this
is a better match for future hardware
because it means that we can have
heterogeneous cause we don't need to
rely on cache coherency or memory
coherency and it also allows split phase
operations so asynchronous operations
message passing a message can be sent
and while waiting for a reply the call
can be doing some useful work so a lot
of these things were advocating here are
yes yes
yes yes well so the multi kernel model
doesn't say anything about whether we
provide that or not but in terms of
implementation barrel fish certainly
provides that and I would imagine
imagine anybody you know most
applications are written that way it
would be kind of crazy not to yeah so a
lot of these advantages we can get from
message passing sort of standard things
people design point solutions for
example data structures that can be
updated using only one or two cache
misses on particular architectures but
it that obviously has a bunch of the the
problems that I mentioned earlier and of
course the great advantage of messaging
is that the system is very modular and
components only communicate through
well-defined interfaces shared memory
versus message passing so there's a very
famous paper from 1978 by Lauer and
Needham and they actually argue that
although message passing and shared
memory are joules and we saw an example
this with Mark yesterday in fact the
choice of one model over another depends
upon the machine architecture and that's
a very insightful comment given the way
architecture is evolving today so we
have seen that shared memory has been
favored up until now but when we look at
it there are certain trade-offs that
depend upon the the size of the data
being shared and how much contention
there is so I'm going to show you an
experiment that we did for barrel fish
basically to see whether there was ever
it was ever the case that message
passing could be cheaper than shared
memory on a tightly coupled
multiprocessor on a multi-core machine
so what we did is we measured the the
latency per operation of updating a
shared data structure an array without
locking we we didn't do any locking so
calls were just overwriting
the values that other course had written
and the hardware was this 16 core AMD
Opteron so the hard way I'm telling you
the hard way because I suppose it's kind
of good form because the hardware
matters as you will see though the way
the caches are organized and shared
affects the answer okay so for this
experiment the first part the costs of
shared memory so here the the threads
are pinned to each core directly and
each thread is updating the same small
set of memory locations the same array
without locking and so what's going on
is that the underlying cache coherence
protocol is moving that array between
the caches as necessary so consequences
of this are that while that cache line
is being fetched or invalidated
it'll be invalidated if it's in the
local cache and some other cache
modifies it the processor is stalled the
speed of this operation is limited is
also limited by how long it takes to do
these round trips for these cache lines
to bounce around on the interconnect and
so what we see is that the performance
depends upon how big this array is how
many cache lines it it takes and also
the number of cores that are involved so
here are the results of this experiment
so on the x-axis I've got the number of
cores going from 2 up to 16 and on the
y-axis the the cycles per update
operation and you can see that when
there's only one cache line performance
really
doesn't change much it scales pretty
well with a number of calls however when
the number of cache lines goes up to
eight then we're actually seeing
increasingly long latencies and this is
purely the effects of the cache
coherence protocol moving those those
cache lines around so this is
effectively exposing the scalability of
the cache coherence protocol and of
course while the reason that it takes
that the cycles per operation increases
is because the calls are stalled while
waiting for the cache line to get back
into their cache so in contrast we look
at message passing so here we're running
on on one of the calls a single server
thread and that server thread is solely
responsible for updating the array
there's the same array that we had
before and on the other calls other
threads are just sending our pcs sending
requests to the server saying do my
update the the in the implementation of
this experiment the client threads are
actually issuing a particular type of
RPC a lightweight RPC and it's one of
these mechanisms that's tuned to the
hardware in this particular case so we
assumed that we make sure the procedure
call fits in a single cache line so this
you know message passing is actually
under the covers using the cache
coherence protocol itself which is it's
a fairly subtle point and I don't think
it affects the results but just for
completeness so you understand how we
constructed this experiment so in here
we we had the clients block while they
were waiting for a response from the
server and so here's a visualization of
it basically the delays here happened
because there
at the server queue up and it takes time
for each request to be serviced so the
results here I'm just showing two lines
those two lines really close together
one is for updating an array of we that
covers one cache line and the other is
eight cache lines and they're so close
together that I've emitted the other
sizes and so basically as you would
expect the size of the data being
updated doesn't actually have any impact
on the scalability here so the the
increases that we're seeing are caused
by that queuing delay at the server okay
so let's look at both shared memory and
message passing together and see what
happens so the top line the red one is
the the shared memory with eight cache
lines which as we saw before scales
poorly initially here so the message
passing is more expensive than all of
the shared memory versions apart from
the eight cache line version however at
when we have more than four cores and
more than four cache lines the lines
cross and we actually get message
passing being faster the line at the
very bottom is the cost per update
operation by the server in the message
passing experiments so basically what's
happening there is that the server is
getting really good cache efficiency so
it can perform twice as many updates per
second as all the sixteen shared memory
threads combined so this this experiment
yeah so there's more remember that I
said these the the message passing
version was the client threads were
sending blocking requests if those
we're non-blocking if they were
asynchronous then essentially the
difference there between the time it
takes for the server to do the update
and what we're reporting for the client
there is would be spare cycles and in
theory at least although this is quite
hard to implement to get you know to do
this effectively that those client
threads could be doing other work or
those client calls could be doing other
work while waiting for the response from
the server I should say because Miguel
has taken me to task over this
experiment before that it's this is a
very contrived experiment this doesn't
necessarily tell you anything except
that given this hardware you can
contrive to to make shared memory be
more expensive than message passing but
I believe it does highlight the various
that the relevant elements that you know
the the considerations you you need to
make to compare the costs of of the two
approaches and if if our theories about
what's going to happen with hardware in
the future right and in particular if we
scaled so many calls that we don't have
Hardware cache coherence then we really
are moving into a message passing regime
whether we like it or not so I think
that's a good place to stop the lunch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>