<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR-INRIA Workshop On Computer Vision and Machine Learning | Coder Coacher - Coaching Coders</title><meta content="MSR-INRIA Workshop On Computer Vision and Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR-INRIA Workshop On Computer Vision and Machine Learning</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hgU9TxcxfJQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hello thank you very much so Sebastian
ago 15 minutes right 15 plus 5 so I'm
going to try to squeeze a four hour
tutorial into 15 minutes which is going
to be really hard but they are actually
rather than doing that I'm going to be
focusing on one or two main topics so in
this is quite large new piece of work
some of you have already you know seen
this before but hopefully i will show
you something different this time it's
about forests you know they become very
very popular in machine learning in
computer vision and also lately in
machine in medical image analysis and so
what a lot of us not just me but a lot
of us have been trying to do is you
build a unified model they tries to
explain how far is work and you know
across the many different papers out
there so that we unify all the work but
also you know once you've done that we
try to push forward and trying to figure
out how else your forest can be used and
so hopefully today in these few mins
i'll try to give you a flavor of this
like in the machine on ebay look at the
SC boot can effectively already
explained it works and wait for as long
yes but mostly for classification ok
ninety-five percent of the work out
there is on classification for us what
we call decision forest is a slightly
more general term which allows us to do
all these other things as well so like I
said ninety-five percent of sargon yeah
especially the last three things so what
you find in the literature is you're
mostly classification forest you find
very very very little work on regression
forest and the work you see you find
there is I think never probabilistic in
integration forest if you find that
paper which starts regression forest in
a probabilistic way which doesn't come
from this lot please
point me to it but then as you will see
soon you know we try to push the model
to be able to do other things as well so
effectively we try to bridge the gap
between supervised and unsupervised
machine learning and also as you will
see between disparity of models and
generative models as well so hopefully
I'll be able to show you some of this or
I'm going to skip a lot of this stuff so
classification forests we have talked
about it in the past it has been used in
connect and in many other places as well
regression forest as you saw earlier
from you know Jamie's talk has also been
used in something which has the
potential to do connects or the things
as well and I'm not going to talk too
much about it in this talk what I will
be focusing more is on density forests
and so figuring out how we can use
forest you know for constructing compact
representation of a probability density
function learning this probability
density function from unlabeled you know
samples from a labeled data but other
things that we do described in our work
which by the way is available right now
I say foundations and trends journal
paper is also how this work can also be
used for money for learning and how it
relates to more conventional money for
learning techniques and finally we also
show how you can you know bridge the gap
really put together you know supervised
and unsupervised learning making use of
the fact that we can discover densities
of data in their higher dimensional
feature space in conjunction with you
know a few label points which may be
expensive to gate and do semi-supervised
learning so I'm going to skip a lot of
stuff and I'm gonna concentrate mostly
like I said on density forest so let me
get there just the sake
Oh get there so it's here so it this
talk is not much about application is
more about the sort of efforts have been
doing towards your modeling you know
forest and unifying in all the work has
been done out there but the density
forest is slightly new I haven't seen
that done before especially in this kind
of way and it's all very experimental so
first of all it is about generative
models we were going to construct a a
compact model from where i will show you
we will be able to generate random
sample samples according to a learned
distribution so i realized that in this
audience you know all of you are really
cleaning movies so while you were
talking this morning i had to you know
put in a movie this is one of my
favorite ones is 12 monkeys and the
reason why you put it in is because
sorry let me the reason why I put it in
is because it's the best explanation of
genetic models i've come across so far
so let's see this is a sci-fi movie and
what brad pitt will be explaining to you
in a minute is how you can use genetic
models to predict what people are going
to do in the future okay
I'll be just another kidnapping feature
and Geoffrey shrink are you take that
extra this is your leader a certifiable
lunatic before the former psychologists
all these plans work I know what Flacco
irresponsible schemes and now god knows
what you've written out there on that
wall tears less I try just write a walls
think I told her about the army of the
12 monkeys hmm and possible you know
what I do you know why because you
pathetically ineffectual and
pusillanimous pretend friend to animals
I'll tell you why because I had anything
to do with their six years ago there was
no such thing I hadn't even thought they
now come she knows what's going on
here's my theory on man when I was
institutionalized my brains saw stably
the guys and mental health I was
interrogated I was x-rayed I was
examined thoroughly then they took
everything about me good engine computer
well then created this model of my mind
yes he's back model they've managed to
generate every thought I could possibly
have in the next say ten years which
they then filtered through a probability
matrix of some kind everything I was
gonna do in that period some you say so
that was it so I'm gonna explain now to
brad pitt how these probability matrix
of some kind works right so let's
imagine we have you know a set of
unlabeled points here on the left you
know it these are represented in a very
high dimensional feature space so i want
to do is we're going to build a set of
you know trees they are going to be
trained randomly which what they're
going to do if Italy is anyway they're
going to cluster this pace in ways which
makes sense and we will define what this
you know make sense is you will see
where it soon that you know the
algorithm for constructing these trees
it has got very direct links to
expectation maximization for instance
with one of the main differences that
you know this is your hierarchical so
just like an expectation maximization
you have this iterative process where
you try to fit for instance some
Gaussian represent a
to the data the same thing we're trying
to do here and you know we stick for
simplicity with gaussian representations
but you know the way in which the data
is split is your hierarchical and you
know each of the internal nodes
represents a binary split of a subset of
this data by in order to make this work
we need to define an energy function a
cost function which for us is always a
source of information game so this is
sort of you know the glue of this model
the thing that you know allows you know
this model to work so many across so
many different settings and tasks as
well because what's interesting is that
as you know we know ready information
gain and the related entropy function
can be defined both for discrete labels
or continuous levels can be described
defined both in the supervised cases
where we have your label data attached
to our training set and unsupervised
cases as well so in this case we were
talking about no supervision just you
know the feature representation itself
and so how you know does this
information gain work so for instance if
again as the working assumption we
assume to work with gaussians for each
of these partitions then we can use the
standard definition of a continuous your
entropy defined for Gaussian
distributions which is defined in here
and is nothing else than you know very
intuitively you know a function of the
size the volume if you want of these
blobs so splitting these two this
ensemble of data into two blobs is
convenient or not you know depending on
whether they we gain anything from you
know the sum of the final volumes
compared to the original volume itself
and so actually yet here we go here we
have an example of you know what this
information game tries to capture so
imagine day we have this data set you
know before the node and then we are
trying to split it so before the node in
the set has you know we have a certain
Gaussian which is this blue one and it
an entropy associated to that Gaussian
and if we decide to split the data with
a horizontal split which intuitively
clearly is not a very good choice we'll
end up having is you know to child nodes
the green and the red ones and the two
gaussians associated to those are very
highly overlapping this is an indication
visual indication that the split is not
very good the information gained in this
case you know contact ibly is about one
and if instead we use a more reasonable
splits like the vertical one we obtain
two gaussians which are much better
separated and this is reflected into a
much higher information game so this is
just to refresh your memories really on
how information gain works for unlabeled
data in a parameter is setting like this
so we can do this you know for many
different trees and so what happens is
that your when we should original do we
choose those features which produce the
highest information gain so at each of
these split nodes will have certain what
we call weak learners or split functions
that could be axis aligned or not and
eventually at the leaves you know a
certain number of training points will
arrive at those least leaves a certain
number of clusters and each of these
clusters is represented with a Gaussian
again we stick to Gotham's but when we
train a forest of trees you know each of
the split nodes will be trained with
some level of randomness as is standard
practice when you're dealing with random
forests and other alternative is to use
the bugging technique which we can use
in here as well there are least two ways
in which we can inject randomness and so
now that we have many different
clustering trees effectively we're at
each leaf nodes you have a different
Gaussian representation what is the
final density function for the whole
forest it is given by this it is just a
linear combination of Gaussian so that's
why this model is sorry it is this one
it's a linear combination of individual
tree gaussians and so you can see that
you know this is you know in the way in
which it looks you know this is very
similar
to a gaussian mixture model but you know
again there are differences not only in
the way the training is done but also in
the fact that each of these gaussians is
actually domain bounded so you know it
lives only within a certain well-defined
domain in the future space and you know
this will become clear a little bit
later on of course when talking about a
generative models there is always the
difficulty of you know figuring out what
the partition function is to sort of
sorts to make sure that you know it the
probability is correctly normalized and
it adds to one and this can be worked
out you know it's not too difficult to
achieve at least in certain situations
you know the situation being that you
have your axis aligned you know split
criteria in the split nodes if you have
more complicated you're weak learners
then it's a little bit more difficult to
achieve but this is for instance the
output of a single tree applied to a
square 2d domain so did you see the the
gaussians which are bounded by this you
know sort of quadrilaterals and you know
this z partition function make sure that
you know the volume is obtained by this
entire surface you know Sam's the one so
I'm moving it quickly because I would
like to show you some examples so this
is for instance very simple right sorry
this is a very simple toy example we
have in a 2d feature space we have you
know two blobs and we train different
trees this is this different forest
story this is what happens when we train
a forest with depth to very simple
stumps depp three that five and we use
axis aligned you know split criteria so
in this case of course you know we get
nothing else but you know sum of two
gaussians in this case we get you know
extra splitting and hear much more
splitting so as always you know when
looking at for aside now we're looking
at many other machine learning
techniques we need to be careful with
the possibility of overfitting so we
need to be a little bit smarter and this
is what happens during testing so now
what I'm doing is I've trained
three forests from the few training that
I have and now I'm trying to compute the
probability density function POV for
each of the points not just the training
ones ball so everything else in this
white square and so you see what the
difference is in terms you know when you
go from shallower trees to deeper trees
in this case we achieve a very clear
overfitting on the left portion here you
see is not playing correctly the left
portion you see the output of an
individual tree on the right portion
instead you see the output of the entire
forest so you see this averaging effect
being achieved over you know the output
of individual trees in telling you it's
not true that they don't over fit isn't
it medicine yes sir game yes I don't
know what they say in the book it's not
true day you can know / fate with
forests you can / fit food for is very
badly indeed in fact there are as we
make it very clear in our book which
should be coming out in less than a year
or so there are two main parameters
which control the behavior of the forest
one is the number of trees you have in
the forest and the other one is the
depth of the trees now what is true is
there you know with the number of trees
in the forest you cannot over fit if you
have a million trees or 10 million trees
it doesn't matter the more you have the
better of course there are computational
implications but with the depth of the
tree you know you have the usual
overfitting sort of your sweet spot so
you need to be very very careful in
connect for instance in at least in the
first installation Jamie used a trees of
depth 20 and because the data is so
complex you know we're having reached
overfitting yet but perhaps if we use
depth 30 or 40 which would be
computationally intractable I suppose
then we would see a degradation as well
you do the same things um something over
and again I DNA yeah luckily the
bootstrapping thing less the key to
nothing yeah I mean the fact that you
have randomness definitely alleviates as
you can see here it alleviates the
problem right and therefore how you
incorporate randomness how much you
sample in walk in what way your sample
will have different effects on on the
generalization re on the overfitting
problem absolutely and we do investigate
this you know at length in our own book
so I welcome you to have a look at the
book and tell me what you think about
that we do do an investigation that that
the amount of randomness and the model
of the randomness have got a huge effect
and i would i would say that perhaps the
amount of randomness that we call role
in the book is probably the third most
important parameter in forest two
minutes okay what should i do them i'll
show you some results like spirals we
love spirals so in the previous case
where the data was so simple we were
very easily achieving overfitting this
case which is a little bit more
complicated if you use your very small
stumps you know we achieve underfitting
and trees of degree 6 of depth 6 seem to
be working a lot better here but i
wanted to so we have comparisons
quantitative and qualitative with
respect to p.m. we respect to both
randomized ems your multiple games you
know GM mems and all sorts of things but
what's cool i think is that you know all
of this theory corresponds to very
simple algorithms for actually something
for you know this is a true generative
model so from certain you know data
points that you have seen earlier we
have learned you know some compact
representation of this PDF and now using
the tree structure alone by traversing
the trees in randomly in a guide that
random way let's say
you came sample points which is you know
what has been shown in these animations
so we don't sample you know from these
images we sample from the tree using the
structure of the tree and this can be
used for instance to solve this sort of
problems where you have ambiguity this
is you know if X X is my input variable
and Y is what I want to obtain as an
output my dependent variable this is a
mathematical relation not a function
because there is ambiguity here
ambiguity occurs all the time in
computer vision so if as an input I have
eggs I would like to estimate why I want
to make sure that I captured the nature
of the ambiguity in the data now if i
use a regression forest because the
regression forest can cope most
naturally with functions rather than
mathematical relations we cannot do that
but if instead go through this density
generative model we can do this so you
know if I had you know given an X a fix
X that it was here we will be able to we
would be able to obtain the output
conditional distribution which will look
like this for different values of x 0
you obtain a proper multimodal
distribution for the condition now this
is all rushed because I don't have time
but like I said there is a lot of
material you have all the slides already
available from the computer vision web
pages you have taken a core part you
have the foundation entrance paper and
now we are writing a book with Springer
where there will be a lot of invited
chapters by Pascal fois Roberto and many
other top researchers who have
experience with forests and in the book
we try to show how these different
peoples have effectively used this model
though they probably didn't know they
were using this model for many different
applications both in computer vision any
medical image analysis I think you'll be
a really nice piece of work and I
welcome you all to give me in comments
yes toby toby is writing the code for
the books and it will be called made
available he's promised me giving that
attribution we are asking silly
questions can you explain why it would
be not correct to soap or from the
density image why you have to solve over
the tree because it's a much more
compact representation now when you are
in two dimension in this toy example
yeah you can sample from that you know
discrete quantized distribution is very
easy to do so but here we try to be a
little bit more general and so when you
have a real scenario you have much
higher dimensionality and you cannot do
that right so you cannot do this so
something from the tree by traversing
the tree from the root to the leaves
which is an algorithm I didn't explain
why it's very efficient is the best way
to them yes when you optimize range yes
it could be interpreted as you know
minimizing a global cost function which
is you know related to this information
gain correct but actually the way in
which the optimization works is greedy
right so you make local choices and
that's the one of the many things there
is no fantastic about trees so in theory
you could think of redoing all of this
but in a more basin way where rather
than deciding in a hard way whether to
send the data point to the left or to
the right child you send it to both with
different weights so it's a typical in
your bayesian way of doing things but in
practice you know you would find it a
lot more difficult you know
computationally to optimize things to
train things and to test them as well
but some people like John Wayne here are
thinking about how to do that in a sort
of compromised way to achieve some of
the flavors of your Basin trees but
maintain some of the
speed of this sort of techniques thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>