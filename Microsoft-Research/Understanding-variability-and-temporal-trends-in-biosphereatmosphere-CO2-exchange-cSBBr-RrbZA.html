<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding variability and temporal trends in biosphere-atmosphere CO2 exchange | Coder Coacher - Coaching Coders</title><meta content="Understanding variability and temporal trends in biosphere-atmosphere CO2 exchange - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding variability and temporal trends in biosphere-atmosphere CO2 exchange</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cSBBr-RrbZA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's my pleasure to introduce and
welcome to the lab Trevor Keenan who's
come all the way over from Harvard
University in the Richardson lab and and
without further ado I'll let you get
started Trevor welcome thank you hi
thanks for coming everybody hopefully
it'll be worth your while and I am a
postdoc in the Richardson lab in harvard
university I've been there about two
years I'm working on model data fusion
and quantifying uncertainty in the
terrestrial carbon cycle and today I'm
going to be talking about this am hope
you enjoy it so basically to put this in
perspective and it's kind of focused
around climate change and I just like to
give it a little bit of the history of
what we consider time a change and
because we've got a lot of skeptics out
there and people who don't take it into
the context of history and it goes way
back to this French mathematician where
you might have heard of hair furrier who
actually realized that the earth should
be a lot colder than it is back in 1824
and that it was the existence of an
atmosphere that made that the surface of
the earth was actually a lot warmer than
it should be then 18-49 Tyndall
discovered that methane could keep em
heat in the atmosphere they thought that
it was actually retaining heat and not
reflecting it and then in nineteen
ninety they discovered that this
carbonic acid as they were calling it
that was also keeping the atmosphere
quite warm and they projected if it were
to change in the future temperatures of
the earth would change little did they
know that we were actually going to try
that out and so in 1958 Dave Keeling he
was the first to am succeeded in
measuring the concentration of carbon in
the atmosphere and he set up his
research station in man Aloha and it
didn't take him long to of em to realize
that this was going up and I was going
up at quite a fast pace after ten years
he could already see a trend and he had
no idea but it was going up a lot more
so this is what is happening um and what
that is what that implies is that
yes temperatures are expected to change
following the laws of physics we don't
know how much because of any feedbacks
within the earth system but we can
already observe that temperatures have
changed in the recent past this is a
graph of global temperatures and you can
see the trend line from the beginning of
our mid 1800s to now and as you move
that trend line further into the more
recent past it gets more steep so
temperatures are changing their changing
faster as Pam time goes on and it's kind
of worrying and they're projected to
change even more so this is recent
changes in temperature lobley separated
by regions and in the orange you see
model projections of how we expect these
temperatures to change this is from the
IPCC report it's you can argue how much
how realistic these projected
temperature changes are but if they fall
anywhere within that range it's
something very very serious and
something we need to address and so
that's what my work kind of is motivated
by and of course it's a huge system it's
very complex you can't study at all so I
tend to focus on the terrestrial carbon
cycle which is the biggest player in
cycling of this carbon dioxide through
the Earth's system and this is a graph
of em the terrestrial are at a global
carbon cycle if you notice these two
numbers are the biggest ones on it and
they're the amount of carbon that's
taken out of the atmosphere by
vegetation and put back in so true for
the synthesis we take carbon dioxide out
of the atmosphere and respiration
processes but carbon dioxide back into
it and the balance between these two
very big numbers is quite a small number
but it's the biggest player in the
cycling of carbon dioxide truly Earth
system so that what that basically boils
down to is changes in either of these
two numbers and custer very big can have
a really big impact on the net amount of
carbon taken out of the atmosphere by
the biosphere so to illustrate how big
this impact is this is a graph of global
carbon dioxide atmospheric carbon
dioxide concentrations and by latitude
and time time is the x-axis so you see
from
every year and the northern latitudes
you get this dip and this is from
phonology or from the greening of the
biosphere during summer in the Northern
Hemisphere most planter for the majority
of and vegetation is located in Lord and
atmosphere in the northern hemisphere so
you see basically a summer comes the
systems green up a lot of carbon dioxide
is taken out of the atmosphere and then
in winter a lot of it comes back into
respiration and this process dictates
the interannual variability of the
growth rate of atmospheric co2 and
ultimately the long term trend in the
growth of em aspheric co2 so basically
here i'm just showing that thrust field
plants have a huge impact on atmospheric
carbon they're cycling it in and out so
it's very important to understand how
they're doing that and how that those
functions might change in the future how
do you do that basically comes down to
the schematic which is the focus of much
of my work so here you see
photosynthesis takes carbon out of the
atmosphere that carbon is later
allocated to different carbon pools so
it's put into growth into stem or
expired for maintenance of that and of
that would split into the roots because
plants need roots to acquire nutrients
and water and then it ultimately ends up
in the soil where again is respired and
all these happen at different rates
they're dependent on different climatic
drivers and the focus of my work is to
understand how this dependent what is
this dependence on different scales so
we want to look at the response of
carbon cycle to climatic drivers this is
the most basic we need to understand
this if you want to understand how it
will change in the future we want to
make models of this so that we can
actually predict what's going to be
happening in the future with some kind
of certainty to enable governments and
policymakers to generally manage our
existence in the Earth's system and we
want to use all the data available to
inform that understanding so we have
measurements that we go and make we've
got models that we build off hypothesis
we need to merge them all together to
inform our understanding so the talk
structure basically is going to be what
this data is what they
we got how much of it is there what the
model approaches we use our and the
general philosophy of my work and how
you merge models with data and some
specific questions and how we've been
I'm applying this philosophy with the
models and data to direct questions so
the data and most of it is based on this
network of eddy covariance towers now
this is an eddy covariance tower here
they're basically instrumentation at the
top which are mounted above these towers
the towers are designed to be above the
canopy of dressed really cool systems
and they're measuring the amount of
carbon going into and out of the system
they're measuring this at a very high
temporal frequency along with the amount
of water that's going in different
energy balance components and the
meteorology at the site so there you can
link a couple of the fluxes of carbon
that you're seeing with the driving
meteorology and you can assess how that
changes for different ecosystem types
along with that we've got different we
call them ancillary measurements so
they're people who've gone out and
measured different aspects of the
ecosystems they get that data together
with this making synthesis which informs
us in how the ecosystem is working and
how it's cycling carbon into and out of
the atmosphere so they started back in
the eddy covariance towers of course
ecological research has been going for
decades but the eddy covariance
methodology started at Harvard about
1992 and it looks a little bit like this
and basically here you have time from
1992 to not quite the present these are
what are measured so in that ecosystem
exchanges the flux of carbon or how much
carbon is going into or out of the
ecosystem at any particular time and
expressed in micro moles per meter
square per second but these are hourly
fluxes and here you see in the winter
there's not a whole lot going on in the
summer the forest greens up it's a
deciduous forests that leaves come out
it's that's being active a lot of
carbons drawn into the system and then
in the winter when the leaves come off a
lot of respiration is happening so that
carbon a lot of it comes back out so on
average positive numbers here mean that
there's carbon being put into the
atmosphere by the ecosystem
and negative numbers mean that has been
taken out of the atmosphere by the
ecosystem and we're interested in the
balance between the two and how that
balance shifts with different climatic
drivers so that's started in 1992 and
since then its proliferated all over the
world so we have these sites this is the
Harvard side and since then primarily in
the US and the UK and Europe and sites
began to be set up in the mid-90s and
now in from early 2000s we have sites
spray springing up everywhere so we've
got a tremendous amount of data to deal
with and I'm very high temporal
frequency on many many different
ecosystem types and this is sort of the
wave of the future and there are a lot
more there's a lot of interest in in
building these networks and so all of
those sites were set up by individual
researchers mostly professors at
universities who had an interest in am
in making these measurements usually was
a band of different researchers so one
researcher would be interested in the
eddy covariance tower not to be more
interested in soil processes they get
together and some kind of loose
collaboration would emerge and from that
recently they've set up and networks
data acquisition networks such as neon
where they plan there's funding from an
NSF I believe in the National Science
Foundation in America for 30 years of a
coordinated network of data acquisition
so you're not talking anymore about
different and researchers taking on
different aspects this is all going to
be done by basically technicians who are
going to be making the measurements and
making those measurements available to
the public so it's a huge wealth of
information it's hard to sell from this
graph actually sorry about the quality
but here you have an eddy covariance
tower and now mounted on a crane so I
can get more special information inside
the forest you've got a huge amount of
instrumentation measuring every
different aspect of the forest that is
deemed interesting for the carbon cycle
so this is a ton of data to deal with
we're also coupling that with and
flyovers so high resolution kind of
lidar and measurements of the canopy
coupled with satellite remote sensing
and
all of this they have focused on
something like a 62 different sites in
America there's a similar em network
being set up in europe and australia and
and this is really how what we're going
to be looking at the in the future is
networks like this which are fully
integrated i have a ton of data on
different aspects of the ecosystem and
it's going to be real challenge to
figure out how how do we deal with this
data so one way we deal with the data is
by building models and using those
models in collaboration usually a loose
collaboration with the data and there
are two different modeling approaches
you can think of so one is
hypothesis-driven which is kind of
traditional global dynamic vegetation
model or something like that where you
build in something you've observed in an
ecosystem and you parameterize an
equation off it you build that into the
model you cope with another equation
that describes another aspect and all of
these are built on your hypotheses the
other is a daily driven model such as
artificial neural networks which make no
assumptions about the structure of what
you're observing it's a data mining
approach so you don't make any kind of
hypotheses and and you don't build any
structural equations into the model and
both of these are complementary I use
them together and they can work out very
well yep describe later so this is the
data based approach where you've got a
lot of different drivers the ones that
were interested in our life temperature
environmental drivers mostly and and
then some characteristics of the of the
ecosystem itself and you give it all the
data you throw it all out of it makes
these nonlinear connections and predicts
what you're observing and then can
hopefully predict where you've got gaps
and your observations so this can be
really useful because you don't have to
make any assumptions the other
approaches the assumption based approach
where you have structural connections
between different aspects of the system
so here you've got climate variables and
the soil characteristics some kind of
ecophysiology interacting with forest
structure and looked at like this it
seems very simple but of course
everybody is measuring something
different everybody is obsessed with
a different process so you can really
build in a huge amount of complexity
behind this and you get all these
interacting para servirle parameterize
from different measurements de sperrit
in space and time and potentially not
even related and you get something quite
similar to a big monster with a lot of
parameters and there can be very hard to
work with but it can be very informative
because it's built on hypothesis that
can be tested and theoretically and
should be able to predict outside of the
space in which it strains or the
measurements that are used to
parameterize it given that it's built on
processes so those are the two modeling
approaches that I use and this kind of
the state of the art where we're at with
these modeling approaches as i said the
process based model is more adequate for
predicting outside the space of which is
being trained so they're more usually
used to predict the future and so here
we see the state of the art of the
biggest or most commonly used models of
a carbon cycling terrestrial carbon
cycling and ocean carbon cycling and you
see it's hard to tell from this graph
but they really don't agree on how much
the land is taking out of the atmosphere
in the present day and that range is
quite large and when you run them into
the future they give you very very
different predictions of what's going to
happen to the amount of carbon that the
land is taking out of the atmosphere
this makes it very very difficult to
formulate any structural policy that
will help us deal with the problem of
climate change because if your land is
going to be putting into the atmosphere
this much carbon sorry taking out that's
the end of climate change we don't need
to worry yeah that's it done you know we
can all burn as much fossil fuels as we
want that's fantastic if it's doing this
that's a negative feedback that climate
change so it's going to be even worse
than we predict I mean that's a huge
range and and it's quite a big problem
they do should something similar you
really don't have a very good idea of em
how much the ocean is taking out of the
atmosphere and so I'm going to talk a
bit about about this issue and but first
the philosophy of the approach and there
are kind of two big velocities out there
I guess are two big approaches that you
could take to science there's the
kind of more theoretical approach for
you make simple systems and to get more
tractable answers to your problems and
you can idealize and and avoid some of
the complexity that's out there or you
could but you can't really predict very
much sorry I would say or you can take a
policy kind of driven approach where we
put everything in and we do big scale
predictions of how things are going to
be in the future with a lot of
confidence so that policy can be made on
these projections and these are two very
different approaches but they can
benefit from each other and and they can
actually be quite complimentary if you
merge it two together and that's kind of
what i try to do and so talk a bit about
the work that i have been doing recently
and i've been building this model
basically and it's called forest biomass
assimilation allocation and respiration
are affectionately known as fubar and
it's a parsimonious model of the forest
system so it's designed to not have too
many parameters it's a based on
hypothesis of course so different
process interactions between the
different compartments of the ecosystem
so you have photosynthesis and coupled
to conductance and that tells you how
much carbon is going into the forest
ecosystem that's later allocated to
different carbon compartments which
respire at different rates and which
leads to the net uptake or how much
carbon the forest systems predicted to
take out of the atmosphere or put back
in the idea is to make a fairly simple
model that can be informed by the data
of course if you've got a complex model
leave many many parameters and it's very
difficult to get a handle on what those
parameters are so this although will be
a fairly simple model and was designed
to be fairly very accurate taking into
account the processes that could be
shown to be important by the data not
just throwing in everything in the
kitchen sink and what I've been doing is
embedding that in I kind of what you
could think of a model uncertainty
estimate it's a bit more intelligent
than just a simple model sensitivity
whereas your and optimizing the model
using all the data available and then
you're kind of quantifying the
uncertainty around the model
predictions based on the optimum that
you can get and you can think of it as
evolution in time so well the
traditional approach to modeling is you
give it a the model of set of parameters
and then you make your predictions here
you don't you search for the best set of
parameters available to evolving the
parameter set and each and each
evolution you test the model against the
observations and as the parameters that
evolves the fit between the model and
the observations gets better until you
get to the best or evolve to the best
parameter set and that can be really
informative it looks a bit like this
when you're walking around parameter
space so you start off here you're
looking to get here and so it seems
fairly simple you just wander around in
parameter space always going towards the
better fit and eventually get down here
and you get to the optimum and that's
all well and good when you're dealing
with few parameters and few observations
but often it can be a bit more tricky
and most often when you're dealing with
a model and such as we're using your
observations or systems such as we're
treating it looks a lot like this which
you can't really see unfortunately but
there's a very complex terrain there a
lot of places where you can think you
found the minimum weight it's not
actually a low global minimum and so you
have to use more advanced techniques to
finding that minimum and exploring
around it to quantify the uncertainty in
the model so what it means is that we
can explore the model we can explore the
parameters that would give you an
equivalent fit through observation so we
can use the data to tell the model what
it should be doing and to estimate the
parameters of the model from the data
and you can effectively merge the model
with data as opposed to the traditional
approach where you go and measure one
thing here you measure one thing there
you get some parameters from these
measurements and you put them into the
model this uses the measurements
directly in a synchronous fashion all
together to estimate what the model
should be and tell you and if your
models too complex or not complex enough
and you can also incorporate measurement
there's of course every measurement you
make is subject to some sort of
uncertainty we don't know what the real
value is ever and and we can
most importantly give you confidence
bounds on your model projections and so
some specific questions as i mentioned
i'm working at Harvard where we have
this really long term data set which is
really quite cool because it can give
you some interesting temporal
information on how systems change over
time and so for example here on the
right is this net ecosystem exchange
that I mentioned before it's the amount
of carbon being taken out of the
atmosphere by the forest or put back in
and I can't see if you can really see
this very well but basically this is the
zero line above that the system is
putting carbon into the atmosphere below
it's taking it out these are annual free
em numbers for how much on an annual
basis the system's taking out of the
atmosphere and you can see that since
1992 you've got this long-term trend of
am quite radical increase in the amount
of carbon that the forest is taking out
of the atmosphere the bull in
perspective if this was happening
happening globally in all the forests if
they all doubled their uptake over to 15
years then that would also be the end of
climate change we wouldn't have to worry
very much because the forest would be
taking care of it so it's very important
to try to figure out why this is
happening I mean what's happening in the
forest that's driving this change and so
this was the first question that we
applied our approach to we want to look
at this long-term trend we want to do it
by fusing the model with all of the data
so as we don't have to make any
assumptions about what our model
parameter should be and to see while
we're at it if we can use that m
structure to give better put and
projections of the future expected
future of the carbon cycling of this
system and on the climate change so I
mentioned some can all the data
available that we're going to use
there's a ton of data however forest
people if you measuring I once they
practically everything cuz i wish it was
practically everything because there's
stuff that they haven't been measuring
but we've got really a lot of
information so we've got this at 20-year
record of net ecosystem exchange we've
got soil respiration chambers where
people go out and put chambers on the
soil all over the place and measure how
much carbon is coming out of their soil
under different environmental conditions
we've got measurements of leaf area so
people go out and measure how much leaf
there is actually on the forest can't be
so that's important to know how much
carbon they can take out of the
atmosphere how much leaves are falling
off every year and when
fall off a carbon in roots and cabin in
woods or how the carbon in biomass is
distributed in the forest and the timing
of a phonology so when the leaves come
out and when they fall off and how
that's coupled to climate because it
changes independence of climate every
year and and soil at different soil
characteristics and turnover times of
soil carbon so basically the idea is
that you've these all represent
different things that are in the model
right so you can kind of couple the
model with all this information to try
and get it to predict all these things
at the same time you can do that by
training it on some of these things are
all of them together then testing it on
all of them together but in periods that
you haven't trained it to see if the
model is actually doing capturing the
functional response for the ecosystem to
climate and that's what we did and
basically what you see here is this net
ecosystem exchange here X in grams
carbon per meter squared per day so
that's the black dots that you see and
so you see again a winter there's not a
lot happening and summer you get this
huge uptake as the ecosystem activates
and in winter you get more carbon
respired because there's no footage
since this happening and the blue is the
model predictions and if you look at it
like this it looks pretty good you see
you great the models doing a very good
job we can all go home and and it's
doing a good job for daily net ecosystem
exchange but it doesn't get the trend at
all it can't reproduce this trend that
we observed and it doesn't get
interannual variability so I mentioned
earlier this flying carpet diagram of
how carbon comes the atmospheric carbon
is drawn down and and put back up into
the atmosphere it can predict the
magnitude of this variability which is
also kind of worrying and so it doesn't
get the trend it's kind of a problem and
if we want to make any sort of
reasonable predictions of the future if
it can't even predict the temporal trend
now and today in your life worrying why
why poison not get this is it model
error have we built in the wrong
hypothesis into the model there are
structural connections within the model
system incorrectly primer parameterised
and is it a lack of sensitivity changing
climate climate is changing so maybe the
model is not sensitive enough to this
long-term change in
climate or is a data error so you left
with all these questions and you it's
very tricky to know you know to have a
good answer for these but one method you
can use is a data mining tool such as
the artificial neural network I
mentioned earlier where you can don't
have to make any hypothetical
assumptions you can train it to the data
and if it does better job than the model
then you know that your model is wrong
and that's what we did so here earlier I
presented 18 years of the Harvard forest
eddy covariance flux data and this is
the same 18 years let into tree
different periods for the first six
years the second six years in the third
six years if you remember from the early
graph I should have it on here sorry
about that but the trend that am in the
long term eddy covariance record was
only really apparent in the last six
years so you had quite a stable period
for the first 12 and then with a slow
decline but then a sudden decline in the
last six years so what you see here each
circle is six years looped over itself
starting in January going around for the
six years for each month these are daily
residuals between the model predictions
and the observations so you see here the
zero line in black throughout the year
the residuals show no systematic bias
which means that the annual totals
should be fairly correct if you look
here this is where it was trained in the
middle six years the same thing of
course it was strengthened easier so you
kind of expect it to be doing a good job
it wasn't transit easier so the fact
that it's doing a good job is reassuring
you know that it's getting something
right at least but when you move to the
last six years the six years to show
this pronounced a uptake increase in
uptake you see that there's the
systematic and bias in predicting summer
fluxes so this tells us two things that
tells us that it's not the data that's
the problem not necessarily if the
instrumentation for example was giving a
systematic error that was increasing
over time which is what I kind of taught
her to be then you see this through the
whole year but actually the bias artist
increased uptake is only happening
during summer so the canopy is doing
something which is something quite
remarkable which is doubling its uptake
during the summer for no reason and
known to the model and so then you say
okay well maybe the model is just a poor
model maybe there's something that's
missing that it's not getting so we
trained the neural network to the same
climate data and it does a very good job
I'm actually the model at fubar mud
London your electric do an equivalently
good job which is very reassuring for
the model and and then when you test it
in the first period it does an excellent
job as well and in the last period it
also shows the systematic bias in summer
uptake so you can use this combined
these two approaches this to validate
each other in a way and say that well
what we're observing is actually real
and we don't know why or what it is
that's why i said earlier i wished it
measured everything because we don't
have all the measurements necessary to
be able to answer that question so i
mentioned earlier that we were also
going to see if the data that am i have
the merging the data with the model
would be useful for informing model
projections of the future and said
something the models are designed to do
and we do that by quantifying the
uncertainty in the parameters so when
you optimize the model you also quantify
how uncertain the parameters are and
here you see the 40-odd parameters that
are included in the model the red dots
are the optimum parameter values but the
black area around them is where those
parameters could be to give you an
equivalent fit to the data so some of
them they're quite large some parameters
are not well informed by the data and/or
and they don't give a big em if they
don't lead to a big difference in the
model projections so it might also be
that the model is not particularly
sensitive to these parameters and some
of them are very very tight very well
informed by the data I'm very obviously
very important to the model so the idea
is to take these parameters so the
parameters can be anywhere in here and
use those all those parameters to
project the future to kind of quantify
and this is a quantification of the
uncertainty in your model now and if you
use that uncertainty project the future
you can quantify the uncertainty in the
model projections and um I'm not sure
about your respective fields but this is
something that is very very rarely
rarely done in our field unfortunately
and that's why we have all these
disperse projections of the future with
no quantification of uncertainty around
those projections and this is what it
looks like so here you've got a 100
years of muddled net ecosystem exchange
all right amount of carbon taken out are
put back into the atmosphere by the
forest and expressed on an annual basis
so but yeah this this is photosynthesis
so these are the components of this
number for the senses how much cabins
been taken out by photosynthesis of
course and then autotrophic and
heterotrophic respiration are how much
is respired back by the either living
components of the system or the the
actual and respiration of stored carbon
in the soil and the different shades
represent different ways of informing
the model with the data so you notice
that the dark shade here and uncertainty
really gets a lot bigger as you get
further into the future and by by 2050
you don't know if this forest is going
to be taking out carbon or putting
carbon back in and this is what happens
if you only use and the eddy covariance
data to train the model so if you just
use that to tell you what what your
model and parameters should be you can
get really good fits to the present day
data by the model but once you move into
the future you can get these very
divergent model projections which are
highly sensitive to the initial
parameters but if you use all the data
so if you use data from different
sources you can get very tightly
constrained projections of the future
which are not sensitive to changing in
the changes in the climate so you can be
very confident about your projections in
the future or at least about the
parameter based on certainty in those
projections and you can also tell where
that uncertainty is coming from so
obviously here from autotrophic
respiration including more data reduced
the uncertainty in the model projections
same with heterotrophic grip and
respiration and this led to a large
reduction the uncertainty in the model
unfortunately the model doesn't get the
trend in the current day right so we
can't really be confident about future
projections so this is a gross
overestimation of confidence in future
projections
because yes sorry yes yeah no sorry the
lycra is using all all data available
yeah the black is just using the eddy
covariance data which is in my field
traditionally taught to be the most I'm
data rich data because our information
rich data because it's very high
frequency and high temporal frequency
got hourly measurements for 20 years the
middle gray is using a combination of
eddy covariance data and some of the
other data I'll talk about that in a
minute and so this inspired us to China
think about well what date is useful I
mean we've got measurements of
everything are they all equally useful
and are some of them even necessary do
we need to spend thousands and thousands
of dollars measuring this one thing or
is there information in some of the
other data that is included in this and
that tells us about this measurement
already automatically so we did is we
set up this experiment drapes calling
rate my data and where we train the
model iteratively against each data
stream and on each iteration you'd
select the best the data stream that
gives you the best improvement in model
performance and so at the end of this
what you're going to see in the middle I
want to explain it before it showed up
there but you've the last iteration is
using all data together and the first
iteration is just using the best data
stream or their day extreme they can see
the the most information kind of looks
like this and hmm can see if you can see
that probably but here you've got the
first at each stage it selects the best
data stream or the stream that gives you
the best reduction and model projection
uncertainty and that is our DS that are
highlighted here and on the right here
you see the posterior distribution of
model error we call it so it's how
certain you can be about the model
projections so you hear when you're
using all data your uncertainty is
around 1 which means that the model
agrees with beta uncertainty and but as
you add more data to the system as
you're moving down they have the data
provides a huge amount of information
initially but after a while adding more
data doesn't give you a better model it
doesn't tell you anything more about the
system and so for example the timing of
leaf out so budburst when the leaves
come out and starts the growing season
and first it doesn't actually really
tell you anything if you go out and make
these measurements because these
measurements that information is already
an embedded in the fluxes so in the
fluxes is when the news came out you
start seeing a lot of activity so if
you're training the model to the fluxes
you can already automatically know when
the leaves are coming out and there are
a lot of things like that such as a
let's see soil turnover is one of the
highest ranked and in the presence of
soil turnover having soil carbon
estimates of the stock of soil carbon
and soil respiration and it's not very
informative because you already know
what to turn over time out of the soil
is so you can infer the stocks and the
respiration and these next two graphs I
don't know if you've worked with magic
eye at all or played around with it it
really helps if you have but if you blur
your vision a little bit you start to
see some patterns and this is what
happens to parameter uncertainty as we
add more data to the system and you'll
see that here the parameters are all
highly uncertain they're just using one
data stream doesn't really inform you of
what the parameters should be the
parameters could be anything give you an
equivalent fit to that data stream as
you add more data to the system
parameter and certainly reduces but
about here parameter uncertainty doesn't
really get any smaller and so adding all
this data going out and measuring all
this stuff is not necessary for our
question of carbon cycling and the same
for making projections of the future
this is net ecosystem exchange projected
100 years into the future here we see
from the previous graph this explosion
and uncertainty in the future as you get
busted at 2050 and how much that is
reduced by the addition of each
additional data stream and you see that
relatively few data streams really
constrain their future projections of
carbon cycling so if you're interested
in projecting carbon cycling really
don't need to measure everything under
the Sun you just need to measure these
six or seven key measure
that will our information rich and
really tell you a lot about how the
system is working with regards to carbon
cycling so the conclusion i think i just
went through them but um yeah our
projections of the future obviously an
underestimate of the real uncertainty
because we can't get this short-term
change we still don't understand why and
there's a lot of work being put into
that now because it's a it's a very big
question why is the forest changing
subjects so much and why can we not
predict it when we have theoretically
all the data we need and so long term
trends are very sensitive to initial
conditions if you use all the data to
inform the model you get very good
projections if you don't if you just use
the data which is net ecosystem exchange
which is the most traditionally used in
my field and you get this high
sensitivity to initial conditions which
could in some way explain those
divergent projections of carbon
terrestrial carbon uptake that I showed
earlier and we can use model uncertainty
analysis I'm calling your model data
fusion basically integrating models with
data to really help reduce this
uncertainty and this I really believe is
fundamental to advancing our field and
we don't need all day they're not all d
theta is equal someday who's more useful
than others so I'm going to talk quickly
about interannual variability because I
mentioned it earlier and I mentioned
highlighted is something else that the
models don't really get and so what we
did to have a look at this as we took 11
sites with the longest and measurements
records in the world and so that gives
19 once I cures this is really good
because you can really get a handle on
what that interannual variability is
with these long-term sites we use 13
models and tree remote sensing products
to evaluate whether modules we're doing
a good job or not em and we're looking
at variability and lo and behold the
models did a terrible job and this is
the chi-squared evaluation of the models
against the data basically it's on a log
scale so you can't even see one but
anything above one it's far outside
measurement error and basically what i
want to say here is that the models
didn't get this interannual variability
but we couldn't really tell why because
you've got a bunch of models that are
all parameterised differently and
they're not parameterize using the data
directly so you've got this huge
introduc
nevera from and parameterization which
made it very difficult if all these
models are very diverse characteristics
and we couldn't tell what characteristic
was particularly better or worse which
is really frustrating and we did em by
looking at in a context of variability
we realized that canopy phonology and
snow pack dynamics were systematically
among all models poorly simulated but
those weren't necessarily the reasons
for all this bad model performance so to
address this we em we decided decompose
the time series so you can look at a
time series in modes of variability at
different scales so up here you have the
original net ecosystem exchange time
series measurements from the eddy
covariance and on a daily basis you can
decompose that into variability at
different modes of a variability so
you've got em down here the interannual
variability that I mentioned earlier you
can also get seasonal variability
monthly weekly daily and you can do this
for models and they feel like so the
contrast have variability for models how
models are capturing variability at
different modes of variability and if
that explains the poor performance for
annual variability so we did this for
the optimized model we also did it for
the artificial neural networks and we
tested I'm going to present the results
against the monthly anomalies as a kind
of a case study and this is what it
looks like basically and this is the
correlation of the monthly variability
in the model with the monthly observed
variability this is the correlation of
the residual between models and
observations with different climate
drivers so here you've got humidity air
temp air temperature and radiation so
what you can see briefly from this is
that models tend to do particularly well
at different sites and the models that
don't do well at these sites have strong
and covariance structures with different
climatic drivers it's thousands that you
can improve these models by building in
these are improving the representation
of the impact of humidity here in this
case I'm in the models but more
interestingly is the neural networks as
data mining tools
they're kind of can be perceived to be
the best you can do so they're kind of
you can consider them a quantification
of the actual coupling of the observers
to the meteorological drivers and
actually some sites aren't coupled at
all to the meteorology interesting and
and some of the sites are very coupled
and the models that tended to do well
did better at the sites that were sure
the strongest coupling City meteorology
which was a reassuring and especially
the fubar model which was trained at
each of these sites low-tech was also
trained at each of these sites and the
other models were not so you can show
you that low tech and fubar relatively
simple models all of these other models
are very some of them are really really
complex models and so they have a lot of
process representation and this shows
that with a very simple model you can
actually capture the majority of the
dynamics of the system and I should
mention that some of these sites that
are not coupled to the atmosphere it's
not because of measurement error it's
just because the ecosystem below them is
very heterogeneous so depending on where
the wind is coming from you get fluxes
coming from a different system and so
for example this Park Falls is a tower
that's really huge 100 meters high so
the footprint of that's how I we call it
or what it's seeing is very diverse but
I just want to use that as an it
illustration of how you can go into a
lot of detail with these fluxes you can
apply a lot of different techniques such
as neural networks or and variance
decomposition and they can tell you a
lot about how to models do and how and
what's necessary to model these systems
so we're applying the same approaches to
a lot of different and kind of questions
I spoke about this focus because that's
what I has been my main work for the
last two years obviously I've been doing
a lot of other stuff I'm very interested
in assessing the impact of phonology and
model errors how accurately do we need
to model phenology how errors in
phenology feed forward into errors and
the carbon balance and when the model
carbon bonds and different allocation
model structures obviously if different
residence times of carbon planet enters
an ecosystem it can stay there for a
long time and then be released or it can
be cycled too quickly and this is very
important for the react
tivity of that cabin pool to changes in
future climate and a really big question
I'm interested in is how complicated do
models need to be can we build simple
models that are just as good and by
removing parameter error and volatile
organic compounds are emitted by most
plants and they react with atmospheric
chemistry to reduce impacts on ozone
concentrations and they're also very
interesting to model and patterns in G
biogeography I guess I shouldn't put all
that up there because it's just
confusing but it's stuff that I'm really
excited about and so future directions
basically I think this says it all we've
got these research stations that are
continuously monitoring and have been
doing so for more than a decade a lot of
them and it's a huge amount of
information it's a lot of data and we
really haven't figured out what's the
best way to deal with it and it's all up
on the web actually a Microsoft and
supported website where you can download
the data and but then developing
techniques to really interpret the data
and merger with models it's very
challenging especially for people and
from an academic institutions who are
naturally always computer savvy or have
the resources at hand or the technical
advice next door and and that's being
merged in systematic ways with different
remote sensing techniques and different
instrumentation yeah and the future
problem for our research is going to be
to figure out how to use this data how
to merge it all together models to give
a better informed projections of the
Earth system with the end goal being
that policymakers can and will actually
do something with this information
because we can give them a more
confident em information what the future
is expected to be like
that's what the future is expected to be
like right now and we really need to do
something about that I mean that's very
low information content if you want to
make any decisions thank you so your
stories basically want to improve
predictions about the future and so
you're advocating not finding the
optimal parameterization of your model
but sometimes when you really want you
don't be care about your model grounders
and I would really care about is your
predictions so given that Y is optimized
model crackers the right approach rather
than doing something
right so am optimizing the model is
important to be able to quantify the
uncertainty in the model so the optimum
I'm not really interested in the optimum
parameter set and exactly but the space
around that set how close are far away
from that set you can be and still given
equivalent and model performance I'd say
that the objective is not necessarily to
give constrained projections of the
future to be able to better inform
because as I showed there are lots of
things that we can predict like the
current trends but an optimized model
which has a quantification a constrained
quantification of uncertainty is easier
to falsify you can it's a lot easier to
highlight where it's going wrong because
you're very confident about what it's
doing
uh-huh so I well every data source has
its own error structure and for example
the eddy covariance measurement says
it's a pretty well understood and double
exponential distribution of error around
the proportional to the size of the flux
and we can incorporate that into the SD
optimization using the cost function
which is the quantification of the
difference between the model and the
observations so basically everything is
made relevant to the observation error
so if your model predictions fall within
the observation error for a particular
observation then it's considered to be
doing a good job basically then
non-mathematical speak but and we do
that differently for all the
observations so each different
observation has its own independent
quantification of the error associated
with it yep
see milligrams quite common 71 dataset
and then I have different effects
yeah yeah definitely yeah yeah I'm I was
talking with Dave Campbell who's the guy
leading me on and explaining the work he
hasn't seen the results yet but he
thought it was very exciting at least
and so yeah I think they'll be very
receptive I mean if you can there's so
much more weaker we were really
constrained for that experiment with
computing time is basically it takes a
lot of computational reiteration of
running the model with different data
combinations and so if you can combine
that with a more factorial approach and
also the cost of acquisition of the data
it would be a lot really informative to
tell you you know okay this this is not
worth that much but it doesn't cost that
much so does it tell us something
proportionately useful but yeah I don't
have the resources to do that right now
cited
so when I started the project the idea
was to use an existing model and but I
looked at all the Munson they're all
subject to the pH D syndrome I colors
where people doing the PhD they get very
focused on a particular aspect and they
develop it and develop it to you know
for years and so you get this as you
term it unbalanced model for you get a
huge amount of complexity in one part
and then everything else is simple and
so I decide it's a balanced model where
each component has a similar level of
complexity which I found incredibly
difficult to find out there and other
models yeah so I decided that you just
build one
i I don't know I don't I think it's very
hard to take something out of a model
once you put it in and and there's it's
been very difficult to quantify what to
take out so I presented the model study
where we had 15 different models some of
them had like 15 soil layers some of
them had one and some of them had
multiple canopy layers or just one big
leaf and we couldn't tell which is
better they were all poor just the
output yeah done in that framework just
the episode each modeling teams mitted
their model runs and some of them were
obviously like having been there one
model I won't name it but it killed all
the trees at one Forest a temperate
forest from water stress in three years
and that's what we submitted ahem so
these are all like parameter problems
that people just set parameter wrong and
then the model output is doesn't make
any sense for obvious reasons but it's
very hard so that's an extreme case but
all of them are subject to this
parameter uncertainty where people have
picked the parameter and so you change
that primary you get different metal
output so it's very difficult to tease
apart model differences apart from
primary differences
right well it gives them out a lot more
wiggle room so it becomes a bit more
difficult to constrain your model but of
course if you've no uncertainty you're
assuming your measurements are right M
so you get an over constraint model to
that particular measurement yeah yeah
you get more am certain predictions from
your model of course if you assume the
muddle the datas is rice than the bottle
is more confident that it's right I
haven't done it for projections of the
future and yeah I could you would just
get a model that is overconfident yeah
you're right yeah yeah yeah I don't know
haven't done the experiment but
right and so this there's not a whole
lot of it being done my field is people
using a regression tree analysis for
upscaling and so a lot of its focused on
upscaling these point observations from
different towers in space and your
networks seem to be the most otoo ative
way of doing it I'm not an expert by any
means on data mining and so it was more
directly understandable that I could do
but I'm sure it or much I mean when your
own networks is a lot of things you
could do that would be much cooler so
they they're just obviously the
instantaneous response of the system to
a driver whereas real systems will have
lags at different time scales which
neural networks at this most simple
neural network can't get I know there
are more complex ones they that are
focused on that but yeah sure
I want about software aspects of these
models and a particular about 60 or 70
different models they don't forget
indeed
yes right yeah yeah i think the you
could there that would be fantastic i
mean having all these models merged in a
similar framework would solve a lot of
these problems but the main problem is
the input and output formats for the
models so how to model understands data
and how it reproduces or outputs data or
its results having that all in a
streamlined that would involve a lot of
code editing and things like that which
becomes a very much more involved
project but yeah yeah you probably would
yeah because they're all driven with
like standard drivers and but how they
expect to get those drivers is different
between different models mostly like C
code for Tron yeah
yeah but mmm yeah that would be
fantastic to have them all in the common
kind of system but I think it might be a
lot of work yeah yeah
yeah yeah there are a lot of differences
but the system is the same that yeah so
you could you could put them together I
think yeah I hope you can I have to yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>