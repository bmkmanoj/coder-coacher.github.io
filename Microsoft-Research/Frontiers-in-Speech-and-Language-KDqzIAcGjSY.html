<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Frontiers in Speech and Language | Coder Coacher - Coaching Coders</title><meta content="Frontiers in Speech and Language - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Frontiers in Speech and Language</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KDqzIAcGjSY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
well thank you very much for that kind
introduction and also thank you very
much for inviting me to give a talk here
today today I'm going to be talking
about some work that we've been doing on
applications of sub modularity in speech
recognition and natural language
processing this is primarily going to be
an empirical talk it's going to talk
about practical uses of submodular
functions what I'm not going to talk
about our some of the algorithmic
characteristics of the procedures that
are used to actually do the optimization
necessary for some modular functions is
use so as an outline first just in case
there is some uncertainty about the
definition of sub modulator will briefly
review that and at least get an
intuition as to why they're useful as a
model for a variety of problems in
speech and language and then we'll talk
about a number of different applications
that we've done in using them for things
like document summarization and also for
data summarization when you have large
data sets how do you summarize them with
minimal distortion in a variety of
different settings including both speech
recognition statistical machine
translation and also although this is
not a computer vision session I'll give
one additional application in image
collection summarization so this is a
you know each one of these topics i
should say at the outset you know could
easily occupy half an hour talk so I'm
unfortunately inevitably going to have
to skip a lot of the details of these of
these presentations in fact I've already
shortened it quite a bit from what I had
originally wanted to present but if
there any questions at any time or at
the end please feel to ask me and I can
provide you with whatever details you're
interested in knowing and then lastly in
terms of the introduction I'd like to
acknowledge a number of colleagues and
students I've worked with on this work
okay
so first of all what is sub modularity
and why do I think of it as a
generalized form of Independence or
complexity measure we can just look at
the definition of sub modularity so so
modularity is a function on on on
subsets of some finite set so V is a
visa finite set and 2 to the visas all
subsets of a set and a submodular
function is defined on any subset and
provides a value for any subset and the
function is so modular if it's the case
that for all possible subsets it
satisfies this particular inequality
which you know some of you might look
like a sort of a discrete form of
concavity perhaps an equivalent
definition that's perhaps more intuitive
to some of you is the notion of
diminishing returns and that basically
says that if you consider the sub-module
function as evaluation function that
basically anything that you don't have
becomes less valuable as the context in
which you consider it grows and so the
context in which you consider it is
either a or b so b is the larger context
and a is the smaller context and v is
something that you don't have and this
is the incremental or the marginal game
of V in the context of a and that's
larger in the smaller context than V is
in the larger context so b is the larger
context so that's the definition of some
modularity so this is the notion of
diminishing returns at them the more you
have anything you don't have becomes
less valuable when the sub-module
function is sort of a value-based you
know measures value so here's an example
of some other functions that there we go
there's the example so a very simple
example is balls and earn so imagine
that you have large number of balls and
you measure the value of the balls based
on the diversity of colors so in this
particular case on this left earn the
value is 2 because you have two
different colors and it doesn't matter
how many balls you have what matters
what what is important is how many
colors you have and in this particular
case on the left when you add this blue
ball you increase the value by one and
if you add more green or more red balls
the increase the increasing value
doesn't decrease I mean doesn't increase
it can only decrease in case if you if
you ever happen to add a blue ball then
then adding a blue ball to an urn with
blue balls already in it makes that
additional blue ball have value zero so
that's the notion of diminishing returns
and that's therefore submodular function
this is a classic submodular function
that's useful for such context as
defining what some modularity is now
discrete optimization why do we care
about submodular functions one is
because that they're useful models for a
variety of different domains and they've
been useful for in the in historically
in operations research and commercial
optimization and economics in game
theory and in pure mathematics and more
recently they've become of interest in
machine learning maybe in the past
decade and and even more recently
they've started to become of interest in
the some of the applications of machine
learning like speech and language and so
why do we care about so much of
functions not only are they potentially
a good model for a variety of domains
but also because they're amenable to
fast optimization in many cases or fast
approximate optimization with guarantees
so here we've given a set of finite set
of objects V and 2 to the N RS are
they're all all possible subsets of that
finite set and so a is a particular
subset and we have some function which
judges the quantity quality or value or
cost of each possible subsets so f of a
some real number and so two possible
optimization problems we might be
interested in are these in the
unconstrained form of optimization it's
basically just finding the minimum
subset the vet subset that that has
minimum value or finding the sub that
that has maximum value and if F is an
iff is arbitrary the function arbitrary
then these optimization problems are
hopelessly intractable and in fact in
approximate but it turns out that when F
is sub modular that equation 3 can be
done in polynomial time and in fact
oftentimes for certain sub-module
function can be done in very very low
order polynomial time and then equation
4 can be done approximately well
constant factor approximation also in
low order polynomial time so some
modularity basically renders these two
optimization problems not only possible
but also feasible and practical for many
problems so this is potentially useful
now another form of optimization is when
you have add constraints discrete
constraints
so here basically let's say we're only
interested in a feasible set of possible
subsets so script s is the feasible set
and what are some possible fusible sets
well they might be of the form of
cardinality constraints so we're only
interested in sets of a given size or
maybe we're only interested in sets of a
given cost where W is some cost vector
which measures the cost of each
individual element in the set or the
spirit the set s could correspond to
various combinatorial objects because
let's say V is this is a set of edges us
in some graph then the feasible set
could correspond to trees or matchings
or paths or vertex covers and cuts or
any combinatorial object which you might
imagine being of use or even more
interestingly perhaps s might actually
be level sets of some other submodular
function or some other function like for
example s might be the feasible sets
might be those s such that G where G you
some other function G evaluate our nests
is no greater than alpha and so we have
these different optimization problems
these are constrained optimization
problems and again without making any
additional assumptions these these
optimizations are hopelessly intractable
and hopelessly interproximal but when F
and G are so modular they're often
solvable and again lower polynomial time
often times with reasonable
approximation guarantees and so a lot of
the work that we've been doing in the
past few years has been on algorithms
for doing this in particular for example
when gr when when this feasible set does
correspond to level sets of some
addition some separates of modular
function so that was some work we did
last year but I'm not going to talk any
at all about that I'm just instead going
to talk about the usages of equations
three and four and five and six for
speech and language processing so some
modularity is useful as a model of a
physical process that doesn't mean that
some modularity necessarily truly exists
but it it is a useful model and the
meaning of the function of the
sub-module function depends on whether
or not maybe you're interested in
maximizing or minimizing it for
maximizing so much a function typically
what one uses it so much we function for
is for models of diversity
our coverage or span or information and
then for minimization oftentimes one
uses sub modularity as models of costs
that have a cooperative nature where if
you choose one item than the cost of
another item lessons or complexity
measures or roughness measures or
irregularity measures and some modular
is useful for either of these contexts
and so in speech in text and NLP
basically uses so much of function
depending on whether or not you're
interested in say minimizing cost or
maximizing diversity or sometimes both
so a function that perhaps everybody is
familiar with is the entropy function
and the entropy function is a classic
example of a submodular function and the
reason why I think this is an
interesting one is because Shannon in
his famous 1948 paper sort of defined a
set of properties of what one wants to
have for an information function and
then came up with the entropy function
which is defined based on a joint
probability distribution over a large
set of random variables and here's
entropy function is seen as subsets of
random variables it's not it's not like
a parametrized by the distribution but
given a fixed distribution entropy seen
as choosing subsets of random variables
is a submodular function and that is as
you all know a classic instance of an
information function from the Shannon
perspective but there are many many
other sub-module functions that are
functions that are sub-module in fact so
modularity is a is a vast class of
functions they live in a cone with 2 to
the n degrees of freedom and including
things like set covers and graph cuts
and bipartite neighborhood functions and
sons the closed end or concave mixtures
and so on and so forth so there's a very
large set of sub-module functions but in
each case what it turns out to be
interesting is that they can all be seen
as essentially forms of Shannon
information functions without
necessarily them having the difficulties
associated with the entropy function
because even for example querying an
entropy function itself can can just
itself take exponential time because you
need to do a probabilistic inference
problem to get an entropic query and you
could essentially make additional
structural assumptions about the
probability distribution
to make the entropy query tractable but
that doing that sort of presupposes the
very structure that you exactly want to
be able to discern based on using the
entropy itself so that's a chicken and
egg problem which we can't solve but
what we can do is instead use other
forms of so modular some modularity as
information functions and we avoid this
exponential query cost problem because
not all not all so much of functions in
fact most some logic functions don't
have this exponential query cost problem
like graph cut for example so that
basically means that any submodular
function defines some sort of abstract
and some sense combinatorial notion of
of independence and so based on the
Salar function you can define
independence abstract independence based
on addict tivity or conditional
independence based on another form of
additivity or strict dependence based on
there being a strict inequality in this
equation or notions of sort of contr
tutorial conditional mutual information
and other notions of information amongst
sets of sets in equation 11 so then the
question is in all of these applications
which I'm about to talk about how do you
actually find a sub major function
they're generally two approaches one
which is based on sort of heuristics
where you design a so much function by
hand based on perhaps knowledge of the
domain that you're working in and you
acknowledge that it's a surrogate
function that you're optimizing a
surrogate but it often hopefully if
you've designed it well will work for a
given domain but a more recent and
promising approach in the machine
learning community is to actually learn
the sub-module function in some way or
another or at least learn within
subclasses of submerged your funds so
based on data learn the function
automatically so I'll present results in
both of these cases so the first result
which at this point is now a couple of
years old it's about two years old now
is in the context of extractive document
summarization and here so so first of
all why is some modularity useful model
for extractive document summarization
well in document summarization the
problem is basically about a large
collection of documents and you want to
choose a subset of the set of sentences
that somehow represents the doc the set
of documents that you start with and so
here the idea is that on the Left we're
considering we have one possible summary
which is green
we're considering adding the blue
sentence and we're wondering how
valuable is that blue sentence in the
context of this green summary versus the
context on the right where we've got a
larger summary so we use there's more
green and we're considering adding the
blue on the right hand side and chances
are that if you're considering adding
the blue sentence to a larger summary
what that blue sentence provides in
terms of its information content is is
more likely to be redundant in the
larger context in a smaller context and
that's exactly the property of sub
modularity so with that realization we
thought well maybe some modularity would
be a useful model for documents or
active document summarization so then we
looked in at the literature and in fact
we found that unbeknownst to the authors
that were actually doing this work so
modularity had been implicitly so is
that five minutes in or five minutes to
the end five minutes to the end okay so
this is really a short talk so anyway I
I will skip all of this stuff we found
that some modularity had been used
implicitly including for example the
metric that people have used to measure
the performance of a given summary the
the Rouge measure which had been
developed by by linguists as a way of
measuring automatically without
additional human intervention how good a
given summary is that turns out to be a
sahaja function so we then proceeded to
define a class of so much of functions
these are just different so much of
functions that we used including various
mixtures and we also learnt mixtures of
sub modular functions in a max margin
framework which the t-cells which I
can't go over it because when have time
and we evaluated it on a number of
different standard document
summarization corpora provided by NIST
these are the document understanding
conference corpora duck corpora and
there's four of these corpora and we
used Rouge which is that measure that I
had mentioned before as an evaluation
strategy and we found that basically by
using smarter and smarter strategies for
constructing submerged or functions the
approach worked better and better this
was the best system in in 2004 and this
was this was our work progressively over
the
and and as far as I know this these
results on the right from 2012 in each
one of these corporis still stand as the
best results ever reported on all of
these corpora so this is for deco for
dark 05 dec 06 inductor seven so this
sort of suggests that that some
modularity is quite natural for
extractive document summarization and
perhaps one should when pursuing
document doc you're active document
summarization see it as a sub modular
design problem so the next thing I want
to talk about is data summarization so
here basically we've got this you know
people have talked a lot about the big
data problem or maybe the big data
blessing and the old adage goes
something like there's no data like more
data and I think I'd like to sort of
turn that expression around and say in
fact it's not that there's no data like
there's no data like more data but
rather the issue is more data is like no
data and the reason more data is like no
data is when you start looking at these
these training curves basically as the
amount of training data in a machine
learning system increases you start
getting a diminishing returns so this is
some slides that I stole from Andrews
machine learning course from 2011 but it
turned out it was very easy to find
these kinds of slides because if you
look around on the web there are lots of
people who have shown that as you add
more data you start getting the
saturation effect where at some point or
another more data doesn't help any
longer and so this seems to suggest that
there may be a sub modularity of some
form going on in the training data issue
so this is exactly the diminishing
returns problem the more you have the
less valuable is anything you don't have
and for very very large complex machine
learning systems like deep models that
everybody's been talking about recently
or support vector machines basically
this is bad because I'm going to skip
this slide it's bad because you're
spending a lot of time training on data
redundantly you're wasting your time on
data that is already being expressed by
other data and perhaps the the goal is
to choose a subset of that data and then
with that with minimal loss so this is
really the key question
can statistical predictions be made cut
made more cost effective with smaller
data sets now cost effective doesn't
mean necessarily that accuracy doesn't
go down it means that you have a much
smaller data set and the accuracy goes
down minimally and so we did this in the
context of speech summarization and so
here basically the problem is in the
context of speech recognition how do you
choose a subset of the training set in
an intelligent way and train on that and
I think I've already said this but let
me just give you a couple of facts about
this so we tried this on some fairly
standardized speech recognition corporis
switchboard and switch with cellular and
Fisher data which is fairly large
vocabulary fluent spontaneous speech
recognition corpus and we found that
this to be somewhat true in that data
but my guess is that when you look at
for example the corporate world who have
training data sets that are much much
larger than what exists in the academic
world that this redundancy problem would
be would be even more cute and very
briefly here are some results so what
what this is showing is the top table
shows a Gaussian mixture of a speech
recognition system and the bottom table
shows a deep neural network-based speech
recognition system the the right hand
column shows the word error rate using
one hundred percent of the data in both
cases you can see that the deep system
does better than the gaussian mixture
system and this is showing various
different subsets chosen either by
random subsets and plus standard
deviations there's a there's an existing
approach in the speech recognition
literature called the histogram entropy
approach which is non so modular
approach which gets these results and
then there's various different sub
module approaches here's one of them and
you can say that in indicates the sub
modular selection does better than any
of these other approaches in choosing
the best data set and I thought what was
interesting about this is that if you
compare this number 29.3 to 31.0 it's
basically saying that a deep neural
network system with only ten percent of
the data does better than a gaussian
mixture system with one hundred percent
of the data so from the perspective of
doing rapid development of deep neural
network system models maybe you could
choose this this
ten percent and get maybe a factor of
ten speed up in trying how many layers
you want or how wide it is or what type
of nonlinear you want to use all other
kinds of things the design decisions
that one needs to do when designing a
complex system you could iterate over
that ten percent and more rapidly get a
good solution then one would get if one
were to use one hundred percent of the
data so we also looked at this from the
context of machine translation and again
very similar to document summarization
we found that many people in the past
have actually inadvertently used sub
modularity and all of these references
including some very recent ones and some
quite elaborate and intricate methods
actually turn out to be sub module
optimization problems and so what
including the classic one by more on
Lewis which actually turns out to be a
modular optimization problem so it's in
some sense weaker than so modularity and
so what we did is we approach to
director using sub modular functions and
the class of some major functions which
I don't have time to go over is
basically a sum of weighted concave
functions over modular functions and
this is this is a class of functions
that I think is particularly well-suited
to natural language processing to the
national image processing community
because there's been so much work on
feature engineering this community so
here basically the idea is that you have
a sum of terms over the set of features
of a given object and you have weights
of features and then concave functions
over modular functions and this
basically says this sort of ways the the
set for this particular feature so the
results are based on are quite nice I
think comparing again random with
standard deviations or the cross entropy
approach which is what the method and
most people use and these are various
different sub modular systems showing
that they do far better than then the
other base line approaches so I'm going
to skip the image summarization because
I think I'm running out of time but I
just wanted to say that we've also
applied this to image summarization
problems and and also developed a new
strategy for learning sub modular
functions and the approaches that
approach works very well for image
summarization as well
so there's also two other pieces of work
that I think are worth mentioning so we
approached the alignment problem in
machine translation as a sub macho
optimization problem which generalizes
bipartite matching and that also
improved results and then and then
lastly a class-based in factored
language model construction based on
word clustering using so modular balance
the modular cuts and that was also
something that achieved good language
model perplexity results from that was
some fairly old older work but there are
a lot of applications that I sort of see
that some modularity can potentially be
useful for national in processing
applications into the future and hear
what I presented was a few of them and
also as an advertisement there has
recently been a couple of tutorials on
some modularity both of which are
available as videos so if you're
interested in some of them are
theoretical aspects of sub modularity
there's this nips tutorial which
occurred last December and 2003 and then
there's a there's a machine learning
summer school tutorial that occurred a
couple of months ago which talks more
about some of the applications of sub
modularity machine learning so that's
the end of the talk thank you very much
no data I think the similarly would
argue that the reason why yeah so I mean
what the question was that one should
also include the complexity of the model
in looking at those kinds of plots and
it may very well be the case that what's
being saturated is not the information
that exists in the data but what is
saturated is the capabilities of the
model and I believe I think that's right
I believe that is true and in fact we've
done some experiments precisely on that
where we've sort of done the reverse
we've limited the models complexity
specifically so we could understand the
saturation effect I do think these
things are intertwined however that
doesn't mean that there isn't a
redundancy problem in the data no matter
what the inherent complexity of the
model is what capacity of the model is
there's also going to be redundant in
the data these things doing are mixed
together but there's going to be there
is a separation there is a redundancy in
the data and if we can actually choose
the non redundant data set we're not
wasting our time on the redundant part
so so I think in the interest of time
we're going to have to hold further
questions for for when the session is
over I think Jeff will be around at
lunch and we can all ask him questions
then let's thank the speaker again
our next speaker is Chris Manning chris
is a professor of computer science and
linguistics at Stanford University he's
a fellow of the ACM the triple AI and
the ACL he's literally written the book
on natural language processing and
information retrieval I think many of
you probably use his text books in your
courses he's done seminal work in
statistical parsing computational
semantics computational pragmatics and
hierarchical deep learning and he's
going to talk about these subjects today
thank you I'm good morning artificial
intelligence requires that we can create
computers that have the knowledge that
human beings have and the primary place
that computers can get that knowledge is
from texts so for humans that at some
point humans were able to evolve
language and that gave us a crucial
advantage of being able to communicate
among each other and that led to the
general ascendancy of human beings but
even after that it was really a very
short period ago that human beings
developed written languages and that
provided the ability to communicate
ideas across great distances and across
times and there was really that
development that maintained an extremely
short period of time just several
thousand years we could move from Bronze
Age technology to the kind of
smartphones and tablets that we have
today and so the question now is how can
we get computers to be able to read and
understand all this knowledge the
initial attempts to do that were two for
human beings to attempt to write down
grammars and lexicons the human
languages in esoteric formalisms and to
be used those to be able to build path
structures for sentences this never
worked very well human languages are too
vast they're too ambiguous and their
interpretation is to contact
the pendant and so that led into the
idea of maybe we could use this vast
amount of text in a second way we could
also use the vast amount of text as a
way to learn languages and sometimes
that's just using the vast amount of
text by itself but sometimes in fact
quite often it means that we are using
the knowledge of human beings to
annotate the text in various ways so
here this piece of text has been
annotated with part of speech tags for
each word and so this led into the
development in the late 80s and 1990s
and 2000's of statistical probabilistic
approaches to machine out to natural
language understanding and these were
incredibly successful so by putting
probabilities over things like how
likely words are to follow each other or
how likely words are to get different
parts of speech in a certain context
this led to all of the good tools that
we have today speech recognition
context-sensitive spelling correction
question answering machine translation
so that was a great advance but
nevertheless that's a big limitation to
what was done here that although we had
continuous real numbers representing the
probabilities of different events we
were still building these models over
exactly the same symbolic
representations that were used in the
rule-based systems for natural language
understanding so why was that a problem
if you just look at a symbol like Motel
it doesn't really look problematic but
if you think about that in machine
learning or statistical terms what we
end up doing is turning this into a huge
feature vector where we have a position
for every different symbol and so
there's one position where it's the one
for motel and these vectors grow very
large by the time you go to web scale
corpora they might be vectors that are
millions long but it's not really the
largeness by itself that's the big
problem the big problem is that these
vectors give us no representation of
meaning relationships between words
so if we have representations for motel
and hotel they just have a 1 in some
different position and they have no
inherent similarity their similarity is
zero so that's been a big problem for
progress in combating the sparsity of
language and so a lot of work in the
last decade has then focused on how can
we get meaning representations of words
that show meaning similarities and the
primary method by which that has been
done is to use word distributions so
here we have the word linguistics
showing some contexts in which it's
appear and we use those contexts to
induce a dense vector based
representation of the word and these
models are very successful so that if we
take these kinds of models and then look
at the position of different words in a
vector space representation they do a
very good job at capturing similarity
relationships between words there are
many methods that have been used to do
this but I just want to spend a couple
of minutes telling you about a new way
that we've developed in our lab
primarily by one of my postdocs Jeffrey
Pennington and the idea of this method
is to develop a new method that uses
global statistics over a large corpus of
text to introduce induce word
representations and the idea is that if
we look at a word and the probability of
other words appearing in its context
over ice we might get large
probabilities for both solid and water
whereas for steam we might get large
probabilities for gas and water so the
difference between large and small by
itself doesn't indicate the difference
between ice and steam but if we take the
ratio between the two of them we then
get a dimension of meaning that we have
a large small contrast between gas and
solar that represents a meaning
difference between ice and steam whereas
for either a meaning component that they
share like water or for some other
random word that approximately cancels
out to one and this actually
work so if we plug in the counts from
Wikipedia you can see that this roughly
gets the kind of effect that we want to
get out of that okay so that's basically
the model which we're calling the glove
model so we want to learn a model so the
dot product between words is
approximately equal to the log
probability of one word appearing the
context of the other and then if we do
that precisely vector differences
correspond to this ratio of
probabilities and so essentially we're
training the model to try and get these
things equal with one extracts aspect in
it here which we're scaling by the
probabilities of the co-occurrence how
well does that work well now I'm here
are the words that are closest to the
word frog in this space and the first
and the second one look really good
about at that point my knowledge of
biology fails but it turns out that all
those words I'd never heard of and also
frogs and concluding this beautiful
australian tree frog and the top left
okay so there are other methods of doing
this one of the recent methods that's
attracted a lot of attention is the work
of Google on the Sebo model and a lot of
these methods have been evaluated using
a word analogy task those developed a
Microsoft last year so the Sebo method
if you use large dimensionalities and
large corpora does a fairly good job at
working outward analogy tasks if you
make the dimensionality bigger and the
corpus bigger it starts to do an even
better job at working out these
similarities but we've actually with
this glove method it's getting
significantly better results again than
the word tyvek model what can you do
with these kind of word representations
many things they're very good for lots
and lots of natural language tasks but I
thought I'd just mention one of them so
this is some recent work from Jacob
Devlin at BBN in making use of these
distributed word representations and
machine translation and so the idea in
machine translation is we want to be
starting to build up the translation of
a text and we want to do that in a way
that
sensitive to the preceding words we've
generated in the translation and depends
on the source language translation so
we'd like to work out the probability of
generating the next word of the
translation based on preceding words in
the translation we're building up and a
big window of source context the problem
is that this is basically impossible to
do making use of categorical symbolic
representations because you're just
killed by the sparsity that if you're
trying to represent these or as these
one hot vectors you can't possibly
condition onset that much text even if
you've building models over billions of
words of text but if you used
distributed representations because they
give you a wonderful ability to share
meaning components than you can and so
here are some results from this model so
BBN has a very strong machine
translation system in the 2012 NIST open
empty evaluations they were in first
place with a blue score 49.5 for Arabic
since then they've improved it a little
bit to 49.8 but adding in this kind of
bilingual neural network language model
gives them a very large significant
increase in the quality of the machine
translation system with just this one
change indeed if instead you consider
having more of a baseline empty system
of the kind that we talked about in
class or something and the only thing
that you add to it is this bilingual new
language model it gives you an increase
that takes you up base essentially to
the level of all the other things that
have been done over the decades to make
machine translation systems better so
those are very significant advances but
we don't only want to understand the
meaning of words we want to go on
understand the meaning of sentences and
texts and one of the ways that we
understand the meaning of sentences is
by putting over them this kind of
dependency grammar pars where when we
have some verb we're looking at as
arguments that subject and its object
argument or for the object it's got an
adjective modifier and this is kind of
representations being increasingly you
for a lot of semantic representation in
natural language processing and I just
want to put in a little advertisement
for a moment of course we don't only
want to do this for English and
something I've actually been very
involved in in the last year's trying to
develop common dependency
representations that build on our
Stanford dependencies representation
across different languages and if we can
do it with the same dependency
conventions and the same label set that
allows cross-language comparison and
doing tasks like translation much more
easily than when we have incomparable
resources I'm very interested in
developing that effort but if we go back
to this picture here we have words for
which I just proposed a distributed
representation but we also have part of
speech tags and these grammatical
relations and if we do nothing else
they're still symbolic categorical
representations so an interesting
question is well can we extend things
further because we also have
similarities and parts of speech between
singular nouns and proper nouns or
common nouns and proper nouns we also
have similarities between some of the
relations so we could hope to embed all
of those in distributed representations
and so this is a dependency parcel
that's been developed recently by Don
Chichen in my group and that's precisely
doing this so at any point in the past
we've got a partial representation the
sentence that's been built we're
considering how to add extra words and
we're projecting all of that into a
distributed representation of all of
words part of speech and grammatical
relations and then putting that through
a deep neural network with a novel
activation function and to predict the
next step in the parsing how well does
that work so in dependency parsing there
are two primary methods of being used
they've been transitioned based shift
reduced style dependency parses which
are by far the most used actually
because they're great property is that
they're really fast so if you want to be
doing web scale parsing that's just what
you want to use there were then graph
based passes which are many minimal
spanning tree style algorithms which
have tended to work a little bit better
but have tended to work about two orders
of magnitude more slowly so the
wonderful thing with this parser is you
can have a transition based parser that
works basically as well as the
best-known graph-based parser but it
works actually not only with the speed
of a transition based parser it actually
works faster than most transition based
passes and the reason for that is in the
categorical transition based parsers it
turns out that really the feature
computation dominates the computation
time and therefore you can do that more
quickly with the dense distributed
representations of a deep learning
dependency partha okay so that gives us
some structures for the first sentences
but we want to go on beyond that and do
more for representing and working with
the meaning of sentences and there are
several ways that you're going to go
about doing that and there are different
ways that we've worked with one way that
you might want to ground the meaning of
sentences is to make you some multi
modal forms of learning and so represent
relate sentences to pictures that show
the same events and an appealing way of
doing that is to take pictures and embed
them in a distributed representation and
then what you want to do is have
sentences with the same that describe
the picture being placed close by in the
same represent same vector space
representation and so this is something
that Richard so shown colleagues
including me have been working on
recently so the basic idea is this so
for the picture we run it through the
kind of convolutional deep learning
analysis that's become very common in
recent image processing work in which
i'm not going to ascribe any of the
details of right now for the sentence we
build a dependency pars using the kind
of methods just described and then over
this dependency parse tree we build up
recursively a compositional meaning
representation of the sentence
and then what we'd like is for those two
things to be equal and then in the
typical fashion of neural network models
when they're not equal we're then back
propagating the errors the differences
between the two into the two halves of
the model and iterating over that until
they become more similar so in just a
fraction more detail for working out the
language representation the dependencies
give a tree structure and so what we're
doing is working out compositionally
meanings for sentences so when we have
at night we're starting off with word
representations and then we're building
representations for phrases and we're
doing that by taking each word vector or
previous lower child vector multiply it
by matrix that's encodes the grammatical
relation and then superimposing those
vectors on top of each other okay how
well does that work we be able to test
this with a small data set put together
by Julia hockemeyer and colleagues which
is a thousand images with five
descriptions of each like this there's
only really small data set I imagine you
could get considerably better methods
with larger data sets but this is what
we've done at the moment but you know if
we show a picture and ask for a
description these are the kind of
results we get some right some wrong but
I think this is already interesting and
showing the power of dependency
representations so early we'd worked
with some other kinds of constituency
pars representations it turns out on
this task perhaps because of the limited
amount of data they actually perform
worse than using a bag of words
representation that doesn't model any of
the sentence structure of the sentence
but because dependency trees give a very
direct representation of sentence
structure that model performs nicely
better than baseline bag of words or
canonical correlation analysis models
now this theme of wanting to exploit
recursive structure in neural network
models is something that's been a main
focus of my group I should mention that
it's still an issue that is being
debated and
searched in different ways so just
recently at asml 2014 people at Google
produced a model which it does provide a
representation of sentences or
paragraphs of text but it's a completely
flat representation that doesn't have
any compositional structure the works
out sentence meanings but nevertheless
they were able to get some very good
results from that which beat out some of
the results we had previously published
on sentiment analysis but you know
nevertheless fundamentally I don't
believe this is right that sentences
have compositional structure where parts
go in to build the meanings of bigger
parts and eventually good models will
want to model that and you can see that
in many places here's another piece of
work that came from ACL 2014 from AR and
colleagues and so they're looking at the
task of political ideology detection and
precisely in a different language domain
they're making the exact same point
about wanting to find and exploit
compositional structure of language so
their point is here that well if you
have the phrase death tax that
represents a Republican ideology or its
adverse effects on small business
Republican ideology but then when you
take another piece of text like they
dubbed it though well that's actually
neutral in terms of ideology because
that could go either way but once you
then build the bigger constituent of
they dubbed it the death tax well then
using dubbing and then a piece of
Republican ideology is then giving you a
larger constituent that has progressive
ideology and so we want to represent
exactly that kind of structure and that
isn't something that only happens with
language I think this exactly the same
thing will come to be seen to be vital
in doing image understanding as well
because as soon as you have a complex
scene well it's got the same kind of
compositional structure of parts that go
together in an understanding of the
scene and when you want to
move deeper semantic tasks or another
task that we've worked on is relation
extraction where you're taking pieces of
text and a working getting out of them
particular logical relations so here we
have a relation of a message being about
a particular topic and well precisely
how we wanting to do this is exploit the
compositional structure of the sentence
meaning in our neural network to get out
these relations okay so I hope I've
given you some idea of the general
approach here and its importance the
what we want to do is to have computers
with understanding and the only feasible
way of getting that understanding is to
exploit the knowledge that already
exists now in the modern world that's
maybe more likely to be Wikipedia pages
than actually going to the Trinity
College Library but it's the same
language information and what I've
wanted to argue is that for getting out
that language information a central tool
we want is being able to easily get out
meanings and some of the tools for
getting up meanings that we have is
representing meaning similarities and
distributed representations using other
representations like dependency
representations that most manifestly
represent meanings and so together tools
such as these and others our leading to
a new resurgence and come in
computational semantics and so there's
now a lot of exciting work in doing
natural language understanding happening
in the NLP community and I think that
will lead to new abilities for us to
have knowledge that we can use to give
us artificial intelligence thank you
thank you for that inspirational talk I
think while nima is setting up we can
maybe take one question mari press the
button
I think the answer is that the
implications are good but that's
prospective rather than real so I mean
there are two advantages that having
distributed representations can give you
for parsing one is that it's very easy
to learn word representations in an
unsupervised manner and so that gives a
very good angle for having a system that
knows a lot more about word meanings and
meaning relationships over a vast
vocabulary of words even though our
supervised posit raining data is small
and only knows about a small vocabulary
of words the other big way in which
distributed representations has been
helping and I think this is you know
most of the gains that for example at
BBN machine translation system as
getting is precisely distributed
representations because they allow
sharing of meaning components a really
good way to combat sparsity and so for
the sort of more detailed probabilities
that you're getting is how likely one
word is to modify another even if the
word is seen distributed representations
can help but you know haven't actually
shown that and you're of course totally
right that a lot of our NOP tools work
great on well edited conventional texts
and work poorly as soon as you're going
into informal texts of various kinds
like discussion forums and Twitter posts
and things like that and there is a lot
of work to be done to do that kind of
domain adaptation but I very much do
think that distributed representations a
useful tool in doing that so in the
interest of time will have to hold
further questions for for later let's
thank the speaker again
the next speaker is Don you dong is
going to tell us about the application
of of some of these methods neural
networks in particular in speech
recognition dong is a principal
researcher in the speech and dialogue
research group here at Microsoft and
he's pioneered the use of deep
context-dependent neural networks in
speech recognition in the last few years
this is really literally revolutionized
the field and shortly after his initial
work IBM Google Amazon and many academic
institutions have all of that adopted
this basic technology since then dong
has shown that neural networks actually
learned a language independent
representation of speech and shown how
to recognize multiple speakers talking
at once and so today he's going to tell
us something about his journey doing
this research in what he's found thank
you Jeff so over the last several years
we have seen a great implementing
accuracies in speech recognition systems
mostly due to the application of deep
neural network to the superior
conditioning systems so in 2009
University of Toronto published a paper
in nips workshop the issues that if the
use deep neural network you can improve
the phoneme recognition on a teammate
recognition task to 22.4 phoneme error
rate this result is a better than the
coffee machine model using maximum
likelihood training criterion but worse
than the same model train using sequence
descriptive training approach
however this is something shoot
promising results because in the past
and the neural network hybrid system
only can beat the GMM systems tuned also
using the monofin and now it can beat
systems to in using the tri phone system
treating of the maximum likelihood
training stretch so in these
architectures is still used the same
components used in the early 1990s the
model mana foam states using frame
descriptive training often called a
cross entropy tuning criterion and mfcc
features so in 2010 microsoft
researchers started to work to see
whether this game can be carried over to
larger vocab a speech recognition and
the first thing detroit is the same
thing as before so basically we applied
this mon phone architecture directory to
the larger vocab read speech recognition
and it turns out that we observed a
similar performance it's better than the
maximum likelihood change GMM system but
it was then the system train using
sequence discriminate so the gain comes
when when we applied the context
dependent phoneme units as the training
correct train the network article notes
in the DNN and suddenly the eluate was
reduced significantly as you can see you
can see that it's reduced to thirty
percent on this small task this work
didn't generate a lot of impact as a
time because the task we used is a voice
search internal task and the tree
service is only 24 hours so people have
argued that
whether the same thing can be extended
to larger data sets or other tasks so
one year later we published another
paper on this and this time we scaled up
on these findings and apply this to the
switchboard benchmark system and this
time this this task has over 300 hours
of training data and surprisingly
actually we saw bigger gains on this
task than the smaller task we tried
before and you can see that by applying
this DN technique we can actually reduce
the error by almost one-third and
compared to the GMM system trini using
the sequence described between approach
also at that time this dln system is
still using the frame cross-entropy
training criteria so since then when you
work has been done to further improve
the deep learning systems for special
recognition and I will go through some
of those improvements I think are most
relevant to the task so the first thing
we want to improve easy decoding speed
because otherwise you cannot use this in
the real-time systems and Google
properly published a paper in late 2011
and assume that by just use engineering
optimizations you can reduce the long
time by 20 times so basically reduced
from 3.89 real time factors to only 0.2
long so it's this means that the barrier
to adopt this technique in the real-time
speech recognition system is come and
the second thing we want to improve is
the training speed
many groups have spent lots of effort on
that in Microsoft we are proposed to use
a pipeline training algorithm where we
can paralyze across four GPUs with about
3.3 times speedup and Google are
proposed to use a SGD training algorithm
across different CPUs and also get a
great polarization but more importantly
later on IBM in the Microsoft was
proposed to the technique to use
low-rank approximation so illusionary
you have a very large with matrix
between two layers and you can use
single evaluated composition and convert
that into two smaller matrices and
without loose accuracy this can cut the
post model size and the trees and
antechinus speed of sorry decoding time
by to third okay so this is a very
significant pixel to further improve the
accuracy different groups have tried to
apply the sequence descriptive training
technique that has been shown to be
successful in gmm so actually in the
early 2009 IBM already published paper
to shoot that sequence discriminant can
be done on the new network in America
model hybrid system unfortunately at
that time they are not using deep
learning techniques they have a normal
conventional Cheryl model and they're
not human modeling lots of serums at a
time so the performance of the system is
worse than the GMM system but the tissue
that the sequence discriminating can
significantly reduce the error rate
compared to the sea crossings between
and almost at the same time microsoft
also try to apply these techniques but
we only tried on the timid and so we
don't didn't see any
significant okay the breakthrough came
in 2012 when I p.m. successfully applied
the just technically proposed previously
to the CD TN hmm and it issues that by
just to this they can further reduce the
air it by about seventeen percent
directive Lee and this result of 13
points trees is very significant because
in the past even if you use multi pass
the cooling all the normalization
techniques a temptation techniques using
gmm you can only achieve 14.5 water or
it of this task but now by just using a
single pass and DN system and you can
already beat it so from that time arm I
think almost all the companies switch to
the DN system in 2013 many groups
actually can successfully implement this
sequence discrete between in technique
actually there are lots of tricks to
make it to work including how you
generate lattice how to commence the
ladies to avoid things like a silence
frame long way and how to confuse all of
it and how to select the screening
criterion for your task so lots of work
has been also devoted to trying to
understand why TN system actually works
and in 2011 Microsoft researchers have
tried to see whether the feature
engineering techniques worked before
40mm also works for TN and
we found out that those techniques helps
much more on the shell model stay on the
different models and we consider that
this is because in the DN we jointly
learn the visual representation and the
log linear model and so lots of the
feature engineering techniques we used
before may be reduced or removed so in
2012 Bush University of Toronto and
Microsoft partially results on using log
filter Bank features instead of mfcc as
the feature to the TN and the posts you
deserve better results on teammate and a
larger vocab a speech recognition and
from that time on I think almost all the
groups don't no longer use them FCC the
features the typically just use log
filter bank and one criticism to the new
network systems is that it's difficult
to adapt asian and the deputation
quality is bad and in 2013 we actually
sure that at least for some tasks
adaptation and can be very effective for
example in this case we propose a clear
divergence regularize approach to the
adaptation and we achieve three to
twenty percent dirty world it's on this
short message dictation task depends on
how many appearances you have for the
adaptation lots of other works has also
been assumed that you can use it
structure like this basically this
structure is the same for noise over
twenty speaker code and speaker we're
training basically you have a separate
segment is input which models they speak
or noise or something like that and
adjust that you tune this or directed
why from your observations and then
use that to offset the bias of your new
Nintendo work and you can achieve great
adaptation ability for example in IBM
system we can reduce the arrow lead from
14.1 and to 12.4 by just using this
speaker we're training and other people
have also been working to see you as a
convolutional network can help speech
recognition and this is mainly because
in the speech recognition you still have
some structures along the frequency axis
and for some tasks we do see some
improvements for example in the
switchboard on the voice search we see
some improvements using the CNN but on
some tasks we don't see any game but by
combining CA and a DN IBM to that they
can achieve on the switchboard ask ten
point four percent whatever it which i
think is the best published result of
this task right now and about thirty
percent better than the best possible
result you can achieve using GMM system
in the reason is there are other
advancements along the way for example
many groups have applied the multitask
and the transfer learning techniques to
the system to and apply that to
multilingual salud resource language yes
and multimodal Yassa and you've
University of Toronto and google also
tried to use long short term memory
recurrent network to the ESR and the
lumo said is that Google can achieve
additional ten percent world elite
reduction by using STM we don't know the
exact detail yet because they were
published the paper in September this
year
and at Microsoft we have been studying
working on single channel mixed speech
ASR and we can beat the past best result
by a big margin as well on this task so
moving forward we consider that the
closed lock single focus ASR is largely
solved problem in most commercial
systems nowadays you can achieve more
than ninety percent accuracy and some
companies even consider their to do
their goal is to get ninety percent
sentence accuracy okay so the park we
still have areas we need to work for
example the five film microphone lakini
she is still very bad and when we
recognize on the very noisy condition
it's also very bad we have many speakers
talk at the same time the promise is a
very bad but some of those tasks may be
easily solved if you have an opportunity
that for example if for the far field
conditioning probably you can get
significant improvement once you collect
enough data for that but for some tasks
we need some new models too so we are
working to see whether we can build a
system that can continuously predict
adapt and learn and this requires some
new framework and we are building a new
abstract model we call computer network
its generalization of the dnc n st n and
many other that architectures like that
and we believe that by applying these
kind of models to the special akin task
we might be able to advance the state of
that further that's all thanks
we can take a question at yes chin you
know you're talking about getting more
data in order to to train more
sophisticated models in the case of far
fear assimilation how do you get more
data are you going to collect just real
realistic data or simulation data I
think in the early earlier development
stage you can use synthesize the data
but eventually you still want to have
the looting are collected and I was
wondering have you considered for
example the ability to just represent
the FFT itself with a deep neural
network the f of t is you know can be
represented with a butterfly network
which requires login stages and maybe
one would need like login layers to
represent it in which case maybe going
drastically deep or devastatingly deep
would essentially allow one to not even
need male frequency or mel scale filter
Bank transformation first it seems like
the male scaling should be relatively
easy could could probably be done with
one layer but it's the FFT really which
might be the challenge I was wondering
if you consider doing that yeah question
so I p.m. already heard a paper
published issue that you can learn the
male frequency matrix directly and we at
the microsoft actually yasha &amp;amp; mike and
i had a inter-party of this summer we
are working to see whether we can learn
this FFT staff directory
well it's the steely the early stage so
we don't know yet okay I think we should
wrap it up here let's thank all the
speakers again and thank you all for
coming i hope you have a sense now of
what we sort of think is the frontiers
of speech and language</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>