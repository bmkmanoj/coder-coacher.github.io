<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hedging Against Uncertainty via Multiple Diverse Predictions | Coder Coacher - Coaching Coders</title><meta content="Hedging Against Uncertainty via Multiple Diverse Predictions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hedging Against Uncertainty via Multiple Diverse Predictions</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4WHabOgWFfk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay I guess we can start so it's my
great pleasure to introduce through
patra to you so he did his PhD at CMU in
2010 was it yeah but also worked at
Cornell and after his PhD actually in
2010 he also was at my career search
doing a great internship with you yes
and from 2010 on he was first a TTI
Chicago as a research assistant
professor and then in 2012 moved to
Blacksburg to Virginia Tech where he's
now professor so turn it over to you
thanks Sebastian thank you all for
continuing to endure a long day of talks
you guys are in for the long haul the
marathon version of talks so although
this one will be simple it'll be it's
not a very mathy talk but I haven't
really prepared a long talk it's maybe
like 40 minutes or so I've kept enough
time for questions so if anything is
unclear please feel free to ask and
we'll stop so the main sort of premise
of this talk I'm teaching undergrads
this semester some of my main premise of
this talk is to take motivation from
undergrads which is when you're
uncleared of what the right answer is
which is fairly common when you're an
undergrad you should be making multiple
guesses and so I was unclear of what my
audience distribution was going to be so
I came prepared with multiple talks so
I'm going to in the first half or like
you know first 30 minutes of for 35
minutes out gonna talk about this
hedging against uncertainty with
multiple predictions and in the last
5-10 minutes i'll talk about this one
project that were working on that we're
calling cloud CV which is providing
vision algorithms as a service so let me
let me get started so two to motivate
some of the problems that we're working
on you know we're all the where computer
vision heart is a hard problem but it's
really an ambiguous problem as well and
there's ambiguity at multiple levels and
I've shown this example a few times so
if you haven't seen this example before
anyone want to take a guess as to what
that is what for cheese who sure I mean
it could be anything that's the whole
point
so these are really nicely crafted
examples so that's now that you see the
image in the full context it's it's
still low resolution is still blurry but
you can somehow tell that you know that
looks like a person that looks like a
table and therefore maybe that's a plate
these these examples were constructed
extremely well in the sense that there
are patches in these multiple these
images that are pixel for pixel
identical its exact same patch and so
any feature or any information that you
would extract from them would be
identical across images yet you're
supposed to interpret this patch as a
plate this patch as a shoe that patches
may be a pedestrian and that patch is
the cell phone based on the context
around them and it's not just this sort
of tells you about the ambiguity that
happens in vision problems it's not just
vision here's a here's a phrase that
that we may have heard this is really
famous phrase from Groucho Marx while
hunting in Africa I shot an elephant in
my pajamas how an elephant got into my
pajamas I'll never know now let's be
computer scientists and destroy the joke
by overanalyzing it why is the joke
funny the jokes funny because I claim
when you first read it in your head you
solve a natural language processing
problem you've solve a parse tree and
you come up with that first parse tree
and it says ok in my pajamas is actually
a preposition it modifies the verb
shooting or and but when you read the
second part you realize oh no no in my
pajamas is actually modifying the noun
with the elephant and that's that's it's
those are two completely plausible
interpretations of that sentence and you
and we sort of switch back and forth
between the based on the context that
appears outside that sentence and in
substance this is all sort of motivated
I really like this this quote from from
Daphne's book which is uncertainty in AI
essentially arises out of limitations
and our ability to observe the world at
any given point of time any image any
video any sentence is a extremely thin
slice of reality and you're supposed to
make sense of that it arises because of
our limit limitations in our ability to
model it we don't know we don't have
good models for sentences for images for
videos for objects
and possibly even because of innate non
determinism and in some sense if we
think about what are the problems why do
vision systems are why do learning-based
vision systems fail they we they feel
because of few particular reasons so and
usually our model classes just wrong
here's here's a picture that I like to
use it's from a couple of years ago and
I use it to motivate that all of us if
you look at the models that we use it
would assume that Sebastian would what
walk like that forever his limbs never
including himself the model is a
tree-structured model it assumes that if
someone tells you where the head of a
person is and the toss of a person is
the right half becomes to conditionally
independent of the less left half and
that would be fine if all of us walked
like that but we don't and it's the
restriction of the model class to make
life easier for downstream influence
algorithms but that that then implies
that any predictions that you make from
this algorithms are going to be limited
we have limitations in the in
computational limitations we can't
actually make optimal inferences from
models that are more complicated and so
we end up making approximate inferences
there as well and I think the most the
biggest problem here is the inherent
ambiguity in certain tasks for we've
seen these sort of psycho physics
experiments where for the same stimulus
subjects perceive two perfectly
plausible interpretations of the same
stimulus so that dancer can be seen can
be understood as rotating left or
rotating life right depending on
different people don't see differently
you can make it go back and forth if you
hide the top half of the body you can do
it people develop a good skill trying to
do that and that picture can be of a key
of a young woman looking away from the
camera or it could be an older looking
older woman looking down and I suppose
you see one based on the age group that
you line and this happens yes
isn't it normally the case that there
isn't a large-scale ambiguity yeah so
these are extreme these are extreme
examples to illustrate my point but I do
think this is a problem anytime you put
a user in the loop where for the same
input people will expect two different
outputs so here's a here's an image
where two different users might say that
no I was actually trying to cut out this
object and the other person might say no
I was trying to cut out all instances of
an object so this does happen that you
have cases where there are just multiple
plausible answers there that may be
because people expect different answers
or it may be because this the input is
just noisy or the model is just
incorrect so my I'm not going to solve
all of those problems and they will
continue beyond me but my claim here is
is simple that when faced with these
problems one of the things that you
cannot do is you cannot have models that
make a single prediction that's really
not an accurate representation of what
that model believes it's it's it's it's
uncertainty mismanagement essentially
and what you'd like to do is have a
better representation of uncertainty and
convey that forward as an algorithm
today we are essentially stuck between
two extremes algorithms today do one of
two things they can take an image they
can they can test it against it and
either at one end give you their most
likely belief here's what I think the
person is in this image and here's where
I think their body parts are or at the
other end they can give you marginal
probabilities for all the constituent
parts of the model the marginal the
marginal probability is here for example
are indicating here's where the strength
of the blue here is indicating here's
what I think the head is the problem
with marginal probabilities if you
notice here there are three perfectly
valid placements of where the leg can be
that's why there's a blue here and
there's a blue here as well so even
though your model started out knowing
that people have two legs and two hands
when you look at the marginal
probabilities you lose all structure and
you you forget that you are you output
something that is implausible under the
model so in some sense you have these
two extremes where at one and you have
no your full structure and no
uncertainty or output one you're not
putting one answer with 0 with 0
uncertainty and at the other end you you
you're preserving the uncertainty which
you have no structure and what I'd like
to do is is play around this spectrum
and build algorithms that preserve in
some sense a sense of uncertainty and
also maintain structured outputs so give
you something that tells you what the
structure of the output is so here's
here's an example of the kinds of
algorithms that I'm going to build we're
going to work with algorithms I take an
image and output to you a set of laws
abou interpretations of what could be
going on in the image each one each one
of those solutions is a structured
output so in the first case if you
forced my algorithm give you one
answered it would say here's a person
and I don't know about anything else so
that's what black is indicating and
that's partially correct but incomplete
if you allow my algorithm to make a
second guess it would say this is a
person and this is a person as well
that's partially correct and partially
incorrect because that's not a person
the third answered labels if there is a
dog and the fourth one actually gets it
right with a horse and I would argue
that such a set is far more useful than
a single answer like that or marginal
probabilities at each pixel's which
wouldn't tell you what the model really
believes in in a structured cells and
later on in the second half of the talk
i'm going to tell you what to do with
such a set you can sort of build ranking
algorithms that sort of reading such
lists of outputs so as I said this is
not a very mathy talk but I want to
recently down my notation so I can
ground the examples that i'm going to
give and it's i'm sure many of you are
familiar with these things but this is
mostly so that we're all at the same
plane so i'm going to work with as an
example work with conditional our new
fields so there's an image there's a
bunch of variables it let's work with it
for now as an image labeling problem
even though that's not what we can be
restricted to we can do this for other
things there are variables y1 through
y/n there may be n pixels we'd like to
build a scoring function that says how
good is any particular labeling or any
segmentation of this image will do that
with a node potential that tells me ok
that patch or that pixel likes to be
class 3 because that number is high and
it doesn't like any of the other classes
and for for an edge I can sort of have a
matrix that says how well these two
patches
I reward you if they take the same label
these are just examples they don't have
to be pots model they don't have to be
this form but it's just a good working
example to work with we can have higher
order potentials as well the idea here
is that as soon as you define this
you're implicitly working with a Gibbs
distribution with a full distribution
over all possible segmentations or
labelings of this image that's that's
implicitly defined as soon as you write
down your factors and what people
usually do is work with map infants
algorithms that are going to find you
the most likely or the highest-scoring
interpretation of that image or solution
under this search space and you hope
that that works out that's good enough
we're not going to work with a single
best algorithm what we're going to do is
we're going to think of this as a search
problem and the problem is this green
box indicates the set of all possible
structured outputs so every dot here is
a structured output that says here's a
full segmentation and that maybe this
person and this dot here is a
segmentation that contains a person and
another region is a person and this
entire space is all possible labelings
of these pixels and what I'm going to
set up a problem here is a subset
selection problem so I'm going to
actually reason about all possible
subsets of this exponentially size set
so to me each of those dot is an item
and I have to build a subset of an
arbitrary size of these items and they
should I want to convey this is
difficult problem the order if there are
K labels at each pixel can take and
there are n pixels that original problem
the size of this set was already k to
the end that was exponentially large and
I'm reasoning about everyone to keep it
in a subset or not so that's 2 to the K
to the end that's a double a
quote-unquote double the exponential
space that I'm dealing with so this is a
difficult problem most algorithms just
search for a single best solution here
and that's NP hard as well so in the
most general case this problem is
obviously hopeless we cannot we cannot
search for efficiently for a subset
interestingly there so we've been
working on this line of work on trying
to find such a subset it started a
couple years ago and then
we have a nips workshop paper and
something that still is that we're still
working on and we can actually show that
under certain conditions you can
formulate this problem as a submodular
greedy submodular maximization problem
and that allows you to give near-optimal
greedy algorithms and by near optimal I
mean that will have a greedy algorithm
that has a constant factor approximation
guarantee of about sixty three percent
of the optimal and it's actually known
that you can't do better than that under
certain certain conditions so I let that
sink in that's a strong statement to
make and with strong statement come
strong qualifications why should that be
possible it's it's possible under
certain conditions if you if you can if
you can find the map or the highest
scoring one efficiently or provably
exactly are provably approximately then
we can sort of propagate those
guarantees forward but if the one best
search is intractable then this is
hopeless as well it so the general case
there are no guarantees but under
certain conditions we can actually give
you strong guarantees so i won't i won't
walk you through all the math of it but
i'll give you some intuition of how we
can do this and the way we actually set
this up is is essentially a packing
problem if you will so we view every
solution and radius under some diversity
function some distance function around
each solution and we think of the size
of the union of the ham union of these
of these radius balls around the
solutions that you have picked and
that's a diversity encouraging function
because if you end up picking the same
thing over and over again or if you pick
things that are very close to each other
then this size of the union will be
small if you pick things that are far
apart then the size of the unions will
be large so if I pick these four
solutions there they have very little
intersection and so the size of the
unions is large and what I'm going to
search for is this subset problem so
find me a subset that is high scoring
plus lambda which is a trade-off
parameter times the diversity which is
the size of this this coverage term so
it's a score plus diversity optimization
problem and I because I won't go to
although it is but because I
can formulate this as a submodular
function as a modular maximization
problem the way to do this is with a
greedy algorithm so you find Oh
interestingly like the score of a set is
just some of the scores of things inside
it the way to do this is with a greedy
algorithm where at any given point of
time you have chosen some set of
solutions and you find the next one
which is high scoring and diverse with
respect to the things that you have
already chosen so if you've chosen these
two I will revise you for picking this
because it's far away from those and
this may be a good solution on its own
so find good things that are different
from the things that you've already
chosen it's a very natural notion
computationally you still need certain
something else what what is needed here
is this space is exponentially large so
I can't even check the improvement that
an item will give me so I can't write a
for loop on the space of items but we
can actually show that the score is
coming from that CRF so there were no
potentials and edge potentials we can
actually show that under certain
conditions if you squint a little tilt
your head jump on temple it'll read the
paper that we've written you can
actually see that this diversity is
basically a higher-order factor you add
a factor to your factor graph which is
sort of pushing you away from the
previous solutions that you've chosen so
it sort of becomes interestingly just
another map infants call with a perturb
factor so there's there's a new factor
that there's so this is showing you a
summation on all previous solutions and
it just sort of pushes you away from all
previous solutions so if you've done a
lot of work in building up map inference
algorithms that other people have you
can just reuse them there's no need to
sort of abandon all algorithms here and
this week finding em solutions
essentially means running map inference
m times with adding a new factor into
your factor graph and what does this
factor look like so it depends on what
kind of diversity you want so if you
want something like show me new label so
if your diversity ideas label occurrence
so I've seen a dog in this image now I
will reward you if you use a label which
is not dog so find me something else
that I haven't seen before then
would become a factor which in the
literature is called label costs and
there has been work on that already if
you use a diversity which is called
label transition so I've seen the dog
cat boundary before or I've seen a dog
chair boundary for find me some other
boundary it'll become a different factor
which is for cooperative cuts which
stephanie worked on and push main theory
worked on as well if you want something
that is just hammering diversity just go
away in Hamming sense and you don't care
you know which label you flip just how
many you flip and that becomes something
called cardinality potential that Danny
in the audience has worked on and if you
do a little hack that we we actually
worked on it it's not all the guarantees
go away for this algorithm that we
worked on but then in that case if
you're willing to trade off guarantees
for performance you can actually show
that this is just a node potential
perturbation so you it doesn't have any
guarantees anymore but you can just your
higher order factor can actually be
absorbed into the node potential so you
can use your original map inference
algorithm as well the interesting thing
here was what we found so obviously
we're finding these because I knew of
the algorithms that I could use for
which higher order potentials have been
developed but the interesting thing was
with all of those had already been
developed I didn't have to build any new
algorithms I just had to use existing
stuff and we could define diversity
functions I would be able to use them so
what do these things look like so what
do these diverse solutions look like so
here's here's an experiment that we did
on on binary segmentation or interactive
segmentation where you have an image and
you have a user in the loop that
scribbles on an imogen says this green
stuff I'm interested in that's an object
this blue stuff I am NOT interested in
that's background please cut out the
foreground for me so this is a fairly
standard problem we set up a binary pair
wise conditional random field look at
the colors of each pixel's encourage
smoothness that's a fairly standard
model here's the one best of the map
solution from that it's a reasonable
solution in this case you cut out that
object that you interested in here the
contrast for that arm was low so you
missed out part of the arm if you ask
for strictly the second best without any
emphasis on diverse
you just find me something else that is
high scoring there is not this you would
get this other solution there's there's
not much difference there is difference
though there's this one dot that flips
and that's perfectly reasonable this is
what you would expect to happen you
would like to learn scoring functions
that are robust that have a little slope
and so you're just one pixel down the
slope and here's what you would get if
you ask for a diverse second best
solution you would get something in this
case another instance of the object so
this is far away from this one in
Hamming sense and in this case thing lon
long structure has been completed here
and I would argue those two are this and
this are much more useful for an
interactive setting where if you show
that to a user they can just pick one of
the solutions and not have to interact
with the system over and over again
trying to fix the mistakes made by the
first one so in this case this is just
given best so node potential
perturbation so you are adding to your
you're adding a cost to the note here if
it's a if the scoring function then you
are subtracting away from each node
potential of constant number lambda so
you'd get less of a reward for picking
the same solution as the first one so
it's it's just if you give me exactly
the same answer I will penalize you for
that and so it tries to be so as many
pixels as it flips away from this
solution it gets a reward lambda times a
reward for that and you run map again
yes
oh sorry I also asking second bastard
ever oh okay sorry the middle column is
an algorithm called em best map you what
you do is you find this was not the
firmware and Glover's an algorithm this
is from yeah you vice in 03 what they do
is you find this is finding mid
marginals and that's what push me pick
the variable with the lowest with margin
which have a similar minimizer and just
flip it and that works yeah it is
guarantee it guarantee is exact yeah
that's I think some of those exponents
are also push meets min marginal paper
and yes the first problem how do you set
lambda for the third column it's a
tunable parameter so what we do in most
of these experiments is you have a data
set and you have a validation data set
so you're sort of picking the lambda
that is giving you a set of solutions on
validation that is that lead to a set of
solution that have high Oracle accuracy
which means that the you producer set
one of which is really one of those
items the set is highly accurate size of
that sense right yes so you pick a site
you pick an M that you want and will fit
a parameter that works for that M so you
should think of it as lambda sub M so
for any M equals ten will find you
something that will give you good den
solutions and this is not that
validation set this is what I'm using
that lambda on a new data set on the
test
okay so so we hopefully that should that
lease tells you that you know we can we
can do this we have algorithms that can
release produce diverse plausible
solutions like this what would I we
initially worked in this problem and I
was really interested in what next what
comes after that what do we do with
algorithms like these this is really
hard to do because if you if you want to
build an autonomous driving system and
it has a pedestrian detection algorithm
running on it that tells you that there
could be ten pedestrians in the scene
that's not how you can do driving right
I have to I have to go over one of them
otherwise I won't be able to drive so I
need to know which one is the right
answer and in some sense what do I do
with the set of solutions are in
algorithm returns so that's sort of like
I'd like to focus on so there's a few
different things that you can do and
I've roughly sorted them in the order of
increasing side information
quote-unquote side information what else
do I have access to I like the I like
the first option because that involves
doing nothing it just means make it
somebody else's problem pass it down
stream or in a fancy way call it a user
in the loop setting where you just pass
the solutions to a user the second one
is is interesting what you could do is
if you have a temporal dimension to your
problem in this case we did this for
pose estimation and videos the idea is
that you would like to you'd like to
know where people's head torso right arm
left arm is in every frame and here is
for example i'm showing you i'm showing
you i'm flipping through 10 plausible
pose hypotheses on every frame it's just
10 that it's cycling through over and
over again what you notice is in a lot
of images this one this one this one the
legs are fairly stable the model kind of
is confident about where the legs are
the arms are kind of flailing flying
around the head plays around here sort
of the arm you have no idea because the
contrast is low here for example there
sometimes double counting happening both
legs are actually being placed on the
same leg but one of the solution fixes
that and I would argue that these are
this is a useful set of solutions rather
than reasoning about all of them if you
had access to video you could just set
up a trellis problem what you could do
is on
each frame you could extract 10 of these
10 of the eastern of these on every
frame independently completely
independent you haven't done any any
temporal reasoning and then you can just
ask for the shortest path on this
trellis which is finding you a set of
plausible poses that are close to each
other in time and that leads to this
result we're on the Left I am showing
you the highest-scoring pose hypothesis
in every frame independently on the
right I'm showing you this is something
that's keeping track of the top 30
powers hypotheses and then finding a
smooth transition what you notice is on
the left it jumps around sometimes it
sticks to a person in the background
sometimes it gets confused because of
cluttered and on the right it does a
much better job of tracking that person
because it keeps around a set of things
where the person could be in it there's
a smooth path there so make sense okay
but here's something else that we can do
if you're so in this case your
additional information was time i'm
going to i'm going to show you something
that we present at cvpr earlier this
year which does beijing risk
minimization or at least tries to mimic
it in some sense and this is when your
additional information is coming in a
sense of the loss function that you're
going to use so what if you really
wanted to solve this problem and all you
had access to was one frame you don't
get a video you don't get time you just
have this image and 10 options and you
have to pick one the reasonable thing to
do or one of the things you can do is
just pick the one with the highest score
but the way these solutions were
generated was in a greedy order so you'd
end up picking the highest score which
was Bria first one every time and that's
what map was doing in the first place so
you'd have to use some other information
to pick something that your model
doesn't believe in the most but makes
sense why would that be the case here's
here's my slide on statistics 101 this
is sort of the what you're supposed to
do so in some sense there is there is a
loss function that at hand that that's
how you measure quality or accuracy
right it could be hamming it could be
Pascal accuracy it could be intersection
of a union whatever you'd like at some
task loss that you'd like to work with
and you you're actually fitting
not that scoring function is actually
fitting a distribution right you have
access to this probability of Y given X
for each image what you should be doing
is something which is writing down an
expected loss so if I make this
prediction under the entire distribution
what is the average loss that i will pay
over all possible other things that this
could be and the optimal predictor in
some sense the min base risk predictor
is one that picks a predictor that
minimizes is expected loss what map does
is it picks the one with the highest
score which is based optimal under a 0 1
loss function so if you have a loss
function where as soon as you predict
something other than ground truth you
get 0 your loss function is one and only
0 when you pick the same thing that when
you should be picking the high-scoring
but in most realistic applications we
don't have a 0 1 loss function if you
get half the body right if you get half
the segmentation right you get some
rewards it's not the case that if you're
off by a pixel your revolts go to 0 so
why don't we do this we could there are
only two problems that summation is
intractable and that minimization is
intractable that summation is over all
possible posed hypotheses all possible
segmentations in that minimization is
over all possible hypotheses and all
possible segmentations so here's a
here's a first attempt of this this is
the this is what you would do if you had
access to this it's the simplest thing
that you could do restrict the summation
to those 10 or 20 different things and
restrict the maximized minimization to
goes 10 or 20 there's different things
and what this really corresponds to is
taking the distribution that your access
to and assuming that it's just composed
of a bunch of Delta functions at the
special solutions that you've chosen and
under this distribution then obviously i
can compute any expectation i want and i
can compute any minimization i want it's
going to be only one of those 50 things
and implementing this is is really
trivial it's it's a few lines of code
all you have to do is if you have access
to these four solutions you just create
an M by M matrix where m is the number
of solutions and the entries of those
matrixes pretend one of them is the
ground truth and compute the accuracy or
the loss of the other one just pretend
if this was true what would you pay if
the other one
is your prediction you build a matrix
like that and you build a vector of
exponentiated scores you don't even have
to normalize you don't need the
partition function if you just because
all you're interested in the aren't max
will augment and this matrix vector
product is doing this expected loss for
you and you can just pick the one that
is the lowest entry in that resulting
vector and you can see in some sense
that if this matrix on the left was a 0
1 matrix if it only had ones on the
diagonals then you would just pick the
highest-scoring thing because you just
returned to you the scores of the
underneath and that's that's when you
would return back map but we should not
be doing that because that's not the
loss function a time and this simple
thing works it's it's really simple to
implement but it actually does really
well so here's on the x-axis the number
of guesses that you make you in every
image that's the number of solutions it
goes from 1 to 46 on the y-axis is
accuracy and what I'm showing you here
is an Oracle accuracy so for each for
each image if I if someone were to
magically tell you what's the best
solution in the set you've chosen what
what would be the accuracy would achieve
so this is an upper bound you can't do
better than this what's surprising here
there is already so good like it there's
a lot of potential here you look at the
number at one and that's sixty-five
percent you look at the number at 40 and
that's eighty-five percent so if you
look at a model and somebody reported in
the paper or that I have a model it gets
sixty-five percent accuracy you'd be
like yeah okay it does it's doing
something but it's narrowing down its
beliefs to a whole to just a set of 40
things and it's really accurate one of
those what he thinks is pretty accurate
so this is a much more sensible model
than that number 65 would lead you to
believe right so there's a lot of
potential here and what if now we do
that min basis thing I talked about so
this is the this is the state of the art
that picks the highest scoring thing
that gets 65 and this is that min base
risking that I talked about and
literally implementing this we didn't
change the model we downloaded the model
made available by yang and raman on we
looked at their code in their code they
were producing the highest scoring one
all we have to do is perturb the map a
call map again get ten solutions get 40
solutions
find the pairwise accuracies between
those solutions andrey rank them and
pick the highest scoring one and this
already does seven percent better and
that's that's what we believe as the
it's a state-of-the-art yes I've been
asked that question before it's a subtle
thing what's happening is that this loss
function is a corpus loss it's an
average precision of key precision of
key points so it actually you don't
return just a solution you need to
return a solution in a score so I can
sort it over the data set and the two
things are returning the same solution
but different scores for it and that's
why those sorting is different that's
why the number is different but it's
just an artifact of this experiment like
they should write because you get a
score it's not probability style and
basically presumably somebody has to do
that mapping between this Lizzie
otherwise everything would be right so
one interesting thing would that we
found in our experiments that if you
even throw out the score so they say you
can you can convert scores two
probabilities by doing exponentiated
scores by temperature and that's the
temperature parameter that you can play
with we try this one thing where you say
temperature to infinity which means
throw out the scores just pretend all of
these 10 things that you found are
equally likely even that works quite
well a function of the lambda parameters
are investigating have moved sensitivity
from two parameters to one parameter I
count that us again but that one
kilometer is extremely sensitive so you
need to have a good set of solutions and
so here's here's the argument that i
always make in this talk doing 1d grid
search is not hard it is really easy to
do 1d grid search and you don't need a
lot of data to fit one parameter which
may have 10 or 20 or 30 different values
yeah you have to do it with belief as
machine learning people we cannot fit
one parameter I'm not giving a talk when
I say they're 100 million parameters and
I'm learning that or 20 images yeah
lovely is probably quite different right
it's just and
it's probably much tighter boo you know
like the rate the effective rate under
is kind of like an effective radius
beyond what you want solutions yeah so
what I'm thinking is if the lambdas
tight if the radius is tight you're
going to get sort of a particle
approximation to the limo to the mode oh
I see someone has to be large so if
lambda is large then you're going to see
things far away because it's trying to
push you with the increased strength if
lambda is small you get things in a
small way one thing I refined is because
we have this implicit em how many
solutions are you going to produce you
have to fix em and then I can tweak a
lambda because if you need to just
produce ten things that are really
diverse then I choose a bigger lambda
because it's a greedy algorithm if you
need only if you need 50 things then I
work with a smaller lambda because each
greedy step is pushing me away from
everything before and so that's that
changes how strong I need to push
because I'm going to end up pushing
forty nine times before i get to the
50th one so it's a smaller it also feels
like 10,000 will be worth checking em
equals n thousand if even a thousand is
small compared to the just linear right
yeah it's linear in the am we haven't
gone to 10,000 we've usually observed
plots like these where things flatten
out and that's usually in the few dozens
lynch 10 20 30 40 50 we haven't gone
much higher than that if you do the if
if you do one group date way basically
with the with the temperature thing and
say ok forget about the score yeah then
you want to return also need a lot m to
be a big thing then you're essentially
doing uniform sampling over the whole
space in getting the mean the middle of
the of the space right then we'll get
the constant solution every time missing
a friend San thousand doesn't feel at
the whole space with us credited so to
begin the phone coverage of the whole
all spaced if your lamb baked enough yes
machine learning question so did you
mean we may be more interested in the
math under a motor than its height
um we're working on it but you know well
I guess that wasn't it seems like you
perhaps you could estimate using similar
techniques like if you set your radius
small when you can you can see if I'm
still in the same mode that kind of
thing I wondered we've tried things like
that where you can you can add a
clamping factor which is sort of killing
everything instead it's the other way
around so now you should restrict
everything to be within a handing ball
and then you sample within the dynamic
ball or you produce diverse things
within the having ball or whatever and
you can try to estimate the mass under a
Hamming ball around a solution it's one
of those things where your goal is
really not to estimate the mass under
this distribution or more your goal is
to estimate empirical risk is not
empirical risk with base risk and that
has a loss factor and a probability
factor and we're working on those things
and ultimately that needs to lead to a
better predictor and yes so those are
all things that we've thought about and
and it it should help in certain cases
we've seen improvements but we're
working on them I'm sure you have
thought about it and I'll show you this
question is a million times a for the
decision problem I mean for displaying
anything to the user I completely get
the diversity but for the decision
problem shouldn't just be sampling from
the distribution be there right baseline
I just of course these are not
calibrated pieces Ramadan's detector
right yeah there are other post
estimation pipelines which do provide
calibrated beliefs so in samples for
posterior samples so just grab a hundred
samples from a posterior yes all the
same problem should be should be the
right places so we definitely compare
the sampling in the original diversity
paper I'm wondering if we compared to
sampling so here's how we compared to
put urban map which is approximating iid
sampling right and it put urban map 100
solutions do bet and doing approximate
basis risk under those but open map
solutions does beat map it does but diff
ambassadors better than that those
solutions are better and I think if you
keep increasing the number of solutions
then maybe sampling will beat out but if
you are only going to restrict it to 10
20 30 or a small
then you rather want to spread those
around in pee diverse because you'd like
to present alternatives and if you have
a small mode in your sampler is not
getting out of the first one then you're
really not seeing another solution
because ultimately you're also going to
predict one of these samples right so
something in that set has to be a
interesting alternative to them to the
largest mode if you will sort of a
related question but not directly into
this talk but so structured DPP's right
water presumably like Ben Tasker when
you're looking at the structure DPP's it
with a spoon like he must have sort of
considered the same motivations to
evaluate their effective with
effectiveness in generating the samples
so structured DPP's so he's referring to
a problem stick model where they
actually directly models the space of
subsets to the best of my knowledge
structure DPP's are restricted to stre
structured models because anything
higher you can't do then in a second
order message passing it doesn't work
for anything that is loopy one of the I
think I think it's interesting if that
could be generalized that would that
would be very interesting one of the key
benefits of of this approach that and
one of the reasons why we use it a lot
is that it reuses existing machinery and
so you can get multiple solutions from
without actually developing new
infrasound sebastian's answering
Sebastian's question okaying that either
you can do uniform sampling right or you
can or you can do important sampling all
you can do sampling from a VP our
innovation and it would be nice to see
if it has anybody sort of compared the
different sampling strategies for this
kind of problem for estimation right the
model is already tree structures we can
do exact iid samples match oh yeah so we
sent into a lower approximation there's
that destroyed ok so so this is
something simple here helps it works
better than map and I think one of the
reasons it works better than map
is we are exploiting information about
the loss function you're not doing 01
which is what map would be optimal for
okay so here's here's one other thing
that you can do what if you in one case
we said your additional information was
time in another case your additional
information was the loss function what
if you really are willing to now design
a pipeline system where the second stage
is free to extract more information more
features be a full-fledged machine
learning system in its own and you can
do that you can you can think of this as
a ranking problem so we did this for the
past all segmentation data set the task
a semantic segmentation for each pixel
labeled it as one of the 20 Pascal
categories dog person cat in whatever we
can actually produce multiple of these
at training time you know which one is
the most accurate because you have
access to ground truth so you can pick
that and you can train something which
essentially looks like a ranking
function so you extract features that
are functions of the segmentation and of
the image and this time you don't have
to restrict yourself to something simple
you can extract arbitrary boundary
constraint closeness constraints you can
extract higher order potentials you can
do you can extract deep features on all
of this and that's fine because you just
need to evaluate these features on these
10 or 20 things you don't need to be
able to optimize over all possible space
of solutions you multiply that by a
parameter that's your new score and
you'd like to learn this so that the
score is higher than everything else the
score of the best thing in the satisfied
then everything else and you can then
treat this as a regular structure
prediction problem where you're
minimizing some regularizer and forcing
it greater than the slide this is fairly
standard and this works well so what
we've done is on the x-axis again is the
number of solutions that we're producing
on each image on the y-axis is pass
called accuracy here is the
state-of-the-art roughly two years ago
when the challenge stop this was roughly
the winning entry in 2012 and so that's
our starting point that's what we're
producing multiple solutions from here
is the Oracle accuracy again there is
the accuracy of the best thing if
someone magically told us what's
interesting here is that you know it
goes from roughly 45 to roughly 60 with
just 10 there's the this time we're not
even going as I is 50 or 100 these just
10 solutions and you
do really well if you could do read it
if you could read ank them so there's a
there's a large potential here on this
is that basis gain thing base risk thing
that I talked about it doesn't work too
terribly well doesn't hurt you get zero
point nine and one percent better if you
add more features and solve the ranking
thing that I talked about you do a
little bit better it's about three
percent better than map so you can
actually do much better here you're
still far away there's a gap but you can
you release made some progress oh dear
this here at cvpr everybody was doing
deep learning so you got to throw in
deep features in there yes they helped
even for this reranking problem so you
can actually do a little bit better
there and we're working on something
that is a segment is it is a direct
segmentation model is it is a deep model
that directly outputs a segmentation of
course segmentation map and we're using
it to read rank those existing solutions
again so even though we're working on
something orthogonal we're using that to
rank these existing solutions so there's
no new information used in generating
solutions it's only in reranking them
and that does even well so we're sort of
roughly fifty percent through in that
gap if to give you a sense of what these
numbers are and how much that gain is in
the two years since we started here's at
next week ec cv here's the current state
of art on this data set so we're still
we're still better we're slightly better
than that interestingly we're starting
from this point right so I'm always
excited when I see something better
comes out because for me what that means
is all my curves will now start from
here because I can always take that
model and produce multiple solutions but
even starting from a weaker model we can
do reranking and do better than than the
current state of the art so this this
works you can throw any new idea any
feature you have either in the original
model then you have to worry about
inference or in the lead anchor and then
you don't have to worry about influence
so quite liberating in that sense okay
so interesting else that we're working
on but as quite unfinished so far here's
here's an idea that that I really like
when you have an image you're not just
interested in semantic segmentation you
not just omit interest in pose
estimation you're interested in all of
them you'd like to take an image and you
like to understand it where are the
people what is the 3d geometry
what are the objects in this image what
is the scene reconstruction if you have
to build one ginormous graphical model
for all of computer vision there is just
no hope there's no structure there is
there's no like 13 model isn't going to
cut it but we can treat all of these as
different modules and we can produce
multiple plausible hypotheses from each
one of them and now it's a much easier
tractable reasoning problem because all
you're doing is okay i have m solutions
from this module m solutions for the
from the 3d estimation m solutions from
the from the segmentation engine and i
can ask what are the most consistent
couple in here this one agrees with this
one there is a 3d structure here then I
see a person on there there's a there's
a person labeled here and this is agrees
with this thing the mistakes are not
going to be correlated but hopefully the
right things are going to be correlated
yes not one big model why is this not
one big model your hypothesis you go is
pointed out is a mixture point method
sure you're not abstraction sense where
there's an input and there's an output
yes this is one big model but I'm not
right if you think about doing belief
propagation Europe you're doing some
kind of schedules on some kind of model
with now rather than passing point mass
messages around your passing makes your
appointments messages around sure so
doing it is make sure point mass
messages good enough a vision do we need
to go further would it make sense to
actually write this down as a model it
make sense to take some little things
that yeah merging and split them and do
this kind of thing on smaller so that's
a large is upset but here's here's the
way to think about it right each one of
these is a research problem in its own
it has its own different kinds of models
that people come up with any time
something changes this is extremely
modular I can always incorporate new
things produce multiple solutions and it
serve separates out my sort of
consistency problem from my reasoning
problem with like each module and
higher-order reasoning are sort of
somewhat separated out if I integrate
all of that there's a variable that
reasons about a pixel and there's
another variable that's reason about
limbs of people it's just much harder to
do joint reasoning over all of that and
and make updates to that as things
improve it I agree that semantically yes
this whole thing is one big model and
you can view it as passing messages on
this Delta functions okay and so so I
would argue that this is not just a
two-pole reasoning problem it sir it's
an expanded ranking problem you're not
ranking an individual module your
ranking over the space of these tuples
and here's an interesting example I'll
show you only qualitative example in
this case which is on the pass called
segmentation data set you have an image
and you have a sentence from someone
describing that image n HP laptops it's
on the desk next to a dell monitor and
it suffers from the classical ambiguity
you don't know if the if the desk is
next to the dell monitor or the HP
laptop is next to the dell monitor and
if you actually get multiple parse trees
you'll see that ambiguity so next it
could be modifying the extra deck desk
or next be modifying sits and that's
exposing that ambiguity that you can't
tell from the sentence but if you have
you don't even need multiple here but
yeah that's this example where your
semantics segmentation engine knows it's
pretty confident in all those solutions
that these two monitors are close to
each other not the table and you can you
can sort of match those and rewrite
those by looking at those two modules
together and you didn't have to build a
model that reasoned about sentence and
images simultaneously in the fourth
stage okay so i'll i'll stop I is this
part of my talk I just have like five
minutes after this to show a demo it
doesn't contain any math but if to
summarize this this part of the part of
my talk I i we all know that our models
are not accurate all models are wrong we
don't even if we could write the perfect
model for a problem it would be
intractable to a reason anything about
it but some way some beliefs that are
models ever useful and what we'd like to
do is be able to extract out those
reasonable beliefs and do some
interesting second order processing /
over those solutions or beliefs so and
this is one way of doing it so let me
actually quick
I tell you about the other thing in just
five or ten minutes what this project
that were working on which we call in
cloud CV the idea is that the the
history of computer vision is really the
history of data sets every time a new
data set comes in it makes life harder
initially I remember being at grad
school when Pascal was released it was a
row six it was the second year the
challenge was running and I remember I
was a master soon some of the PhD
students ahead of me would look at those
images there's no way object recognition
can work on this this is not how data
sets are collected this is just wrong
but in the years since then we've made
in we've made significant gains we've
made object recognition work on natural
images like that it had you know twenty
two thousand images twenty object
categories and then image that came
along scrap 22,000 it's 1.4 million
images and one hundred and a thousand
object categories at the classification
level and 400,000 with 200 object
categories but in some sense we have
this this this new data center is also
driving the presence of new models so
people are able to report these numbers
that you know my model is 54 million
parameters and it's trained on 1.4
million images and you're seeing crazier
and crazier statistics reported in
images where in papers were you know
this I trained my model on two thousand
images with 32,000 cores two thousand
machines 30,000 close for one week or I
trained my model with 1.7 billion
parameters with 12,000 course how many
of us can compete with that I know
Microsoft can but if you're if you're a
poor young academic like me you don't
have the kind of resources to be able to
yeah I'll get 32,000 course I just can't
do that so in some sense we're reaching
that point where the size of our data
sets is an enabler and an isolator we're
reaching that point where if all of us
have to keep solving the same problems
just to be relevant just to start
working on a problem it's going to it's
going to be significantly harder for for
people to gain ground so you have to if
you do basic tasks like build and
maintain a cluster you have to sort of
scale vision algorithms and identify
sort of where are the parallelism what
is there
design primitives that you need to use
and you need to understand sort of
distributed computing so it's two of
those things are not relevant if you
want to work on object recognition it
shouldn't be which shouldn't be that we
have to be experts in distributed
computing computer vision and owned a
cluster just to be able to train an
object recognition system but that's
reality that's what at least students
today are much much better at than we
saw 10 or 15 or 20 years ago so we're
working on this system that we're
calling cloud CV it's an ambitious name
with a fairly restrictive goal right now
what we're trying to do is create at
least object recognition API is
available on the web so anybody can
upload images which are sent at the back
end we have some support from Microsoft
in the form of their 0 0 cloud computing
offering and Amazon Web Services and we
do some distributed processing built on
graph lab which is project that started
at CMU is now at u-dub and a start-up as
well I can show you so just statically
speaking we share a lot of data so on
all of the image net data set about 1.4
million images we actually extracted
sort of 16 industry-standard if you will
features on all of those images and we
share that on our website and that's
just a static download that's roughly
400 gigs of data and it's we computed
it's about 1.5 years of CPU time that
others don't have to compute that people
have told me that they are using that
Phi it's much more useful if somebody
has just done that and made it available
what we are also doing is building sort
of conf net things and making available
on the website so let me show you
so our website is called cloud CV we've
posted a web-based demo here OOP waited
my image go it's huge so this is a 0
comma it's a picture of a previous
mentor of mine I do internships with
really cool people is let's that's Rahul
and he went even scuba diving and what I
can I can do is drop that in here this
is sent to send to our servers and it
actually reports back this is the
standard imagenet 1000 way
classification auto-tagging problem but
it's a scuba diver there's I don't know
where the electric ray is coming from
but there's a tiger shark and a great
white shark that's neither of those
rebel doesn't go swimming near sharks
that may actually attack humans that's a
reef shark but it's difficult to tell
you don't have to use this so one of the
things that I realize that we realize
there were a whole bunch of people were
computing decaf features for everything
even batch matching if you will so what
we we release something called a decaf
surfer so if you don't have GPUs or if
you don't want to install cafe just
download your images in here you can
even put your data set in dropbox point
us to a folder so i have an apps folder
here this is only five we don't we don't
restrict too many on the web it just
goes to our machines and we with the
standard imagenet confident we extract
the CAF and it's available as matt files
you can instead of having this on web
you can actually just save it your
Dropbox folder as well you don't have to
do everything on the on the web so if
you're a little Savior you can do this
in Python so here's the here's the total
Python code that you would have to write
there's we have an API where you can
just sort of include cloud CVS and as an
object there's a config file where you
just point us to a there's a JSON
conflict
well you just point us to what you want
to run and what folder your images are
living in so for example that config
file in this case the I can't find it
but there's a contract file that
contains a list of just folders and you
just say call to be dog start and those
things are sent to our servers this is a
restful api which means you don't have
to have Python on you don't have to have
your laptop one you just issue a command
close your laptop go somewhere else
those images are download and results
are placed in your dropbox folder so you
can be using resources or cloud
computing resources that you don't have
to be constantly connected to and you
can get results back that way so we're
sort of our target audience i think here
is grad students if you if you if it's a
weekend results i do one monday this
might be a reasonable resource for you
to use okay so i'll stop that thank you
maybe one or two short questions how
much does it cost how much does it cost
it's all right but also is a user so
Microsoft pays for this right now so
it's free so as long as you will
continue to pay for this it will
continue to be free so I should ask you
how much does it cost don't use episode
that services okay if there are no more
questions danica let's improve again
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>