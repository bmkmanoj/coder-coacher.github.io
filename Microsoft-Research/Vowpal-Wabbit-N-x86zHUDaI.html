<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Vowpal Wabbit | Coder Coacher - Coaching Coders</title><meta content="Vowpal Wabbit - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Vowpal Wabbit</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N-x86zHUDaI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so this is a Bobbitt Beauvoir but is an
open-source project OSHA's on github
which you can download here if you're
interested ok so I think I want to try
to get across what exactly VW is as well
as possible I think the best description
is it's kind of a research delivery
vehicle so there's various research that
we do in machine learning and putting
that into VW has been one of my goals
because I think if you just guys what I
ended up frustrated as a theorist at
times because I would develop algorithms
I thought were great and and maybe I was
right or maybe I was wrong but nobody
was trying them and by actually putting
these algorithms into code I can put
them into state with other people can
use them and then I can discover if I
was right or wrong or or maybe something
in between ok so that there's various
things that kind of make VW different
and I wanted to just give you a sense of
what those are so one of them is that
it's online by default almost all
machine learning systems are used batch
oriented solutions so you give it a big
data set and it turns for awhile and
then gives you a solution VW is actually
online that means it's very interactive
and that makes a huge difference in
terms of your productivity when you're
trying to solve a problem when I first
encounter a machine learning problem
let's say besides the data set is a
gigabyte or less I think typically by
the time most systems have loaded the
data I've already done my first
experiment and within an hour I've found
all the parameters there's actually
quite a number which I can I can just
kind of hand tune very easily to get a
very good solution like the 95% solution
that makes it very productive another
thing is hashing so BW is designed to
take his input much more raw information
than is typical of many open source
packages so you can input raw text right
so this is this is a valid input example
says that you'd have a label one
with with these features okay so there's
a combination of several things that
this implies one of them is you don't
have to pre-process data as much as you
would for some other systems the other
thing is often because we're using
hashing it turns out that for some
datasets you can actually compress your
model state a great deal and via the
hashing mechanism and that can be super
powerful when you start getting to very
large model states okay so another trick
is is all reduce so about three years
ago actually Alec and I and Miro and
Olivier released a paper on terascale
learning and as far as I know actually
that is the the datasets that are
discussed in that paper and then solved
are the largest digits that have been
solved by a public algorithm I'm kind of
surprised I would expect that after
three years somebody would come along
with something better but but this is
this is a very effective approach to
learning across a cluster of different
machines okay so another everyone is
reductions so this is actually one of my
starting points for frustration because
I had this idea that maybe I could take
all the complex problems and people
actually encounter in the real world and
turn them into simple problems and then
solve the simple problems and then and
then make the simple problems it makes
solution to the same problems be a
solution to the complex problem right
it's a very natural thing that happens
to computer science all the time but and
it was kind of often done just kind of
like one against all was a very common
reduction people just did all the time
but it wasn't formalized or studied so
inside a VW we've been creating a
reduction stack which allows me to be to
solve a lot of different problems this
is very important if you're trying to
solve your problem because often I think
you don't have a simple regression
problem or you don't have a simple
classification problem you have some
sort of much more complex thing and
figure out how to map that into
what into the the system is done for you
by the reductions to a large extent okay
and then another thing is this is
interactive learning right so people
don't just do machine learning for
platonic study purposes they typically
do machine learning because they want to
take whatever the machine learner
predicts and use that to change
something about the world right but when
you go and you go and change something
about the world suddenly the the types
of guarantees that we have from learning
algorithms don't really carry over very
well all right so if you are using
machine learning to choose which news
stories display or using machine
learning to to predict which treatment a
doctor should use or if you're using
machine learning for all these different
purposes you should start thinking about
the difference between causation and
correlation typical learning regression
classification gets it correlation but
you really want to get a causation when
you get it trying to get a causation you
really need to be thinking about things
like exploration and how you use
exploration information effectively to
get a good solution
the VW has algorithms for this unlike
almost everything else and then how
we'll be talking in a couple minutes
about learning to search and there's
several several others which are nbw
with it I don't think I should go
through are the questions about this
yeah
oh yeah there's actually so if you go to
youtube and you search for a killer
rabbit Monty Python there's a great
little snippet which is a lot of fun so
there's actually three that's one of the
three things is also do you know
Jabberwocky
so Jabberwocky is vorpal and then and
then elmer fudd was tends to lift his
arse and i guess yeah
so they all kind of fit together if you
think about it right we're hunting the
vorpal rabbit right exactly what the
vorpal rabbit is was not at all clear at
the beginning and they're still not at
all clear but we're hunting it
so as VW has developed its also grown a
user base and I have to say that having
a user base is kind of addictive in some
ways it feels nice when your things
actually get used so there's various
ways to measure this there's a mailing
list with about 400 people on it that's
active there's also a lot of companies
that that use VW in various ways and I'm
sure there are many more the nature of
open source of course is that you can't
really track its use very well okay so I
wanted to give you an example um see if
I can do this so this is let's go here
let's make this a bit bigger
yeah
okay so
if I look at these datasets they're
about 100 32 megabytes compressed this
is an example that Liam Batu originally
did but I'm gonna start with the raw
data from our CP 1 right so this is
predicting whether or not you have a
business article or not so the label is
plus and minus 1 and then this is some
sort of stemmed words in the actual
article I would use even rar data
without stemming but but this is
actually the the raw data which is
available for that data set ok so now I
can say something like there's you know
the data set I tell it that I want to
hash with 22 bits that's adequate I tell
it that I want in grams and I want to
allow skips ABL the default learning
rate is 0.5 0.2 5 works a little bit
better here and then I can let it go let
me actually start over let's time this a
slightly larger ok so now it's turning
through the data you can see that it's
telling you how it's doing is it's going
and it finished right so this is the
squared loss right here I can I can
tweet things I can say I really want you
to report binary loss so the error rate
is going down this is a progressive
validation loss these are this is a
number of examples that it's processed
that's the current prediction and if the
current label and it's the number of
features that are associated with the
example at the point of printout and I
end up with 4.5 percent error rate in
nine seconds right yeah
so you can very easily if you care about
type 1 versus type 2 errors then the way
to express that is via importance
weighting so what would happen is you
would importance weight and say that if
the label is 1 it's maybe it's very
important that I predict it correctly
but if it's label as minus 1 then maybe
it's not so important so you just uh
scaler that you put into things
controlling the number of bits and hash
so each of the individual words and in
grams and skip grams are expressed with
a 22 bit integer which is used to look
up a parameter they could be sent to the
same thing that's right
okay so I already described what
learning reductions do but the the key
problem with learning reductions is how
do you make it efficient enough to be
really useful and how do you make it
natural to program and I think that
we've we've made it quite a bit of
progress here so there was that - binary
flag that I sent this is what that says
binary calls this is a function that you
create when you want to create a
reduction so I'm trying to make it I
mean you want to make things easy to use
both from the viewpoint of somebody
doing machine learning but also from the
viewpoint of somebody who's creating new
learning algorithms and this is a very
natural way to create a reduction you
have a function which has a based
learning algorithm take some examples
input you call the based learning
algorithm you threshold to plus or minus
one and then maybe you compute a loss
and that's it right so this is of course
can be executed very efficiently because
it's just you create a stack of
different reductions inside of the
system that sort of compiles down to the
based learning algorithm and takes it
back up all right so now I'll give how
some time
okay sorry so I've been interested in
structured prediction for a while so my
background is mostly in natural language
processing where almost everything is a
structured prediction problem so what do
we mean so we're trying to make a joint
collection of predictions and we want to
have some sort of joint loss so the
standard example I think in machine
learning land is sequence labeling which
is sort of typify din language land as
part of speech tagging so we get
something like pair of Incan comma 61
years old as our input and we want to
say that you know pairs of proper now
and Vinc n' is a proper now and come as
a comma 61 is a number and so on and so
you know somehow we have this
expectation that being able to label say
the second word is going to help us
label the first word so there are
correlations among the predictions and
we might also believe that the loss
function that we want to minimize this
sum I sort of conglomerate loss over the
entire output sequence rather than
something that decomposes nicely over
little bits so another example that you
might want to do is is translation this
is sort of the the NLP hard problem that
people like to use and here we have sort
of a much more complicated structure in
you know sequence labeling there's sort
of this nice one-to-one correspondence
between input and output and translation
things become more difficult so this
Japanese word becomes the full sequence
of national treasure there's all sorts
of reordering and so on and sort of
importantly you know it's the sort of
loss that you want to minimize here is
really a function of the entire output
sentence not a function of some little
individual prediction within it okay so
lots of people have worked on structured
prediction you know I think the standard
thing that people do is conditional
random fields or variants of conditional
random fields like max margin markup
networks and things like that these
things tend to be difficult to program
if you're not a machine learning expert
and so one of the things that we really
wanted to look at was you know how can
we reduce programming complexity so that
Norm
NLP people eye or vision people or
whatever your application domain is
where you have structured problems can
can make some progress and use state of
the art machine learning techniques and
I guess one observation is that lots of
people have structured prediction tasks
but they don't solve them using
structured prediction techniques because
it's too much of a pain and I think that
this is unfortunate and this is sort of
the situation that we want to resolve
you know sort of a second desert Dada is
that like it has to be good right I can
give you an easy interface to use but if
it gives you crummy accuracy no-one's
gonna use it anyway we also of course
care about training speed because the
development cycle is important and
because you know this is going in VW and
we actually had an early version and one
of my colleagues used it and he's like I
expected VW to be faster and I was like
okay well maybe we need to go back and
fix this because clearly there's this
expectation and then test speed is also
important because we want our
applications to be efficient okay so
here's a couple comparison points so
this is lines of code
so let's say that you want to sit down
and write a part of speech tagger how
many lines of code do you have to write
so there's CR f s GD which is Leon bow
twos package
there's CR f plus plus which some people
may know there's structured SVM these
all takes somewhere in the like that
order a thousand lines of code and here
I'm trying to exclude as much of the
machine learning machinery say the
optimizer part from the actual part that
does the structured prediction in some
cases this is a little bit difficult
because they're they're integrated but
you know if you think of this as order a
thousand lines of code that's probably
reasonable for us it's like eight lines
of code which I will actually show you
in a second in terms of accuracy so this
is this is on part of speech tagging
using tuned hyper parameters
basically the
x-axis is the training time so you know
a minute 10 minutes 30 minutes and then
the y-axis is test accuracy so if you
just do classifications so this is I'm
too lazy to write a structured
prediction system because it's too much
of a pain you're very very fast if you
use vbw but you're kind of crummy you
can get another like point or so
improvement if you use an actual
structure prediction technique with not
much time right so this is 10 minutes
this is training on about a million
words of data and you know other sort of
standard systems fall somewhere else in
the curve so this is CRF SGD this is CR
F plus plus this is a structured SVM
with this dummy DCD solver sorry this is
structured SVM this is 0 + + anyway so I
can talk more about like what this means
if people care but you know I I think
the the point that I'm not trying to
make is that like yay we're so much
better or yay we're so much faster I
mean we're a little bit better we're a
little bit faster but our programming
complexity is essentially zero in
comparison this is test speed so this is
number of tokens that you can label per
second for part of speech tagging for
named env recognition where the last two
bars which are bigger which is good yeah
so this is sort of oh so the question is
what is own features so basically we
were trying to make the comparison fair
by giving every system exactly the same
feature set V W also has the ability to
compute internally some features that
would be specified in the file so we
typically do a bit better when we use
our own features but for fair comparison
we wanted to have the other one and the
reason why own features is slower is
because it's actually computing features
here there on disk but presumably you
had to compute them ahead of time so
okay so here's an example I wasn't
actually planning on running this but
basically it sort of follows the same
framework that John had so you give it
some data you say I have some search
tasks in this case it's sequence
labeling there are 45 labels in the penn
treebank part of speech tagging data set
there are 45 parts of speech tags we
give some sort of interpolation rate for
the way that the structured prediction
works and then this is the own feat
stuff that you just asked about so we
can condition our prediction on
neighboring words and we can condition
our prediction on prefixes and suffixes
of the word that we're trying to label
yeah all right I will add to this in one
second perfect question the question was
interfacing okay so now I have to figure
out how to use John's computer yes maybe
John can figure out how to use John's
computer
something about this wasn't like that
canonical I'll try to answer that I mean
I can show you sort of actually you
should probably go to mine because the
Python version is only in mine just
switched on Langford to hell 3 ok so I
guess I'll try to answer this question
so the question is for different
structure prediction problems say not
sequence labeling how hard is it to sort
of roll your own I think that the answer
is we're trying to make it as easy as
possible so John had an intern this
summer highway from UIUC who was really
interested in this mtv resolution task
where you get a document and you're
trying to resolve different entities
that appear in the document to like
their Wikipedia entry or their freebase
entry or something like that which has
like sort of a very complicated graph
structure and he was able to do this
relatively easily I've been working sort
of behind the scenes on these collective
graph classification problems where you
have some sort of graph and you're
trying to make predictions about nodes
and edges and stuff like that and I
think a lot of complicated problems can
be reduced to sort of these graph based
classification problems so I think it's
doable but actually one of the points of
putting this out is I want to see what
people will try to do with it and what
they have a hard time doing because I
think that's a nice way to push progress
ok so just to give you a sense so this
is the sequence labeling I apologize for
the C++ that's John's fault
so I here is the entire sequence label R
so we haven't initialized which sets
some options
so we say I want to condition my
prediction automatically on previously
made predictions I want to use Hamming
loss by default if you don't want to use
Hamming loss you have to implement your
own loss and then for optimization
reasons we're going to tell it that
we're not going to modify examples
inside of our code we don't have to do
anything at the end and so we have this
structured predict function which is
basically you know how our thing is
going to work so we get a vector of
examples we say for so this vector of
examples is like the words in the
sequence so for each word in the
sequence we make a prediction and if we
want to produce output we produce output
and then that's it so this is sort of I
think the most natural way you could try
to write a sequence label or it's like
go through each word in the sequence
predict the label of that word and
basically all of these sort of the
structured loss and the the the joint
feature space over the over the features
is happening internally automatically
for you okay so we do need to finish ok
so then I will not show you but you can
also do this in Python so there's a we
made a python VW interface and you can
write essentially the same function even
easier in Python and it works as well
but if it's slower because you pay
foreign function interface fees
yeah so the question is for graph
classification what do the input
features look like so I mean it depends
on on what sort of data you're using
right now I'm just using synthetic data
so basically each node in each edge has
like some random features on it the the
sort of problem that was motivating this
was was one of two things so one is
either like a social network II like
problem where nodes represent
individuals and edges represent
relationships and then you might have
personal features of the person or on
the edges you might have interaction
features other people so Stephan ross
who worked with us on this and drew
Bagnall had this nice paper on using
sort of graph classification for solving
image segmentation problems and so you
know sort of eventually we'd like to
work there too yep right so this is what
was hidden in the set options auto
history so there we're basically saying
condition on the K previous predictions
and that's happening automatically for
you if you didn't do that you'd have to
automatically construct these features
yourself and add them to the example
data structure what you could do it
would just take you know a couple more
lines of code obviously for these more
complicated problems you need to do that
yourself it's only for the very simple
cases that this works for you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>