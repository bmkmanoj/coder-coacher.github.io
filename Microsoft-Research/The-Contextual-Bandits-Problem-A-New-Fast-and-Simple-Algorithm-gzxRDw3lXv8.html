<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Contextual Bandits Problem: A New, Fast, and Simple Algorithm | Coder Coacher - Coaching Coders</title><meta content="The Contextual Bandits Problem: A New, Fast, and Simple Algorithm - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Contextual Bandits Problem: A New, Fast, and Simple Algorithm</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gzxRDw3lXv8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
welcome everybody I'm delighted and
thrilled to introduce Rob Shapiro for
the MS our colloquium Rob is a
researcher in microsoft New York City
formerly professor at Princeton
University and before that a researcher
at AT&amp;amp;T Labs he's a preeminent name in
the field of machine learning he
invented a technique called boosting
that if you know almost anything about
machine learning you have probably heard
of it and if you don't you should run
out immediately after this talk and try
to find out about it or buy or buy his
book or just boost a copy from someone
who has the book so he's been you know
hugely influential person in the fields
also influential in writing some of the
seminal papers on regret analysis of
multi-armed bandit problems and today
he's going to be telling us about the
latest and very exciting chapter in that
story ok yeah so well that's fine so
thanks a lot thank you very much for
that nice introduction so I'm going to
be talking about the contextual bandits
problem joint work with Alec Agarwal
Daniel su suck in Calais John Langford
Emily Hong Lee so let me start with a
couple of concrete examples of the kind
of problem we're thinking about so this
one is very well known by now so you
know imagine that you're running a
website and the website when a user
shows up needs to decide on which
advertisements or which content to
present to a user like which news
articles to present and the user so what
happens repeatedly is that the website
is visited by some user and the user
typically arrives with some profile in
other words we usually know something
about the user
person's profile browsing history and so
on and then based on that information
the website needs to decide on the
advertisement to present or the
newspaper article or other content to
present to the user and then the user
responds so maybe the user clicks on the
ad or leaves the page or whatever
there's some response and the goal here
is for the website to make choices that
will elicit some behavior in the user
like typically we want the user to click
on the advertisement ok here's another
example which is medical treatment you
know a toy version of medical treatment
but so what happens in this scenario is
a doctor is visited by the patient and
then again the patient arrives with some
information so the person has some
symptoms and there are some results of
some tests and you know how old the
person is and so on and based on that
information the doctor decides on a
treatment for the patient and then the
patient responds in some way so the
patient recovers or gets worse or never
comes back or whatever and so the goal
again is to make choices that will
maximize some favorable outcome so in
other words you want the patient to
actually get better so abstracting these
problems look something like this so
what's happening is repeatedly there's
some agent so in one example it's the
doctor and another it's the website I'm
going to call that agent the learner
because the goal here is to do some
learning so the learner is presented
with some information some context of
some kind like information about the
patient then that learner has to decide
on an action like which treatment to
prescribe and then the learner observed
some reward of some kind so reward might
be you know whether the patient gets
better or not and importantly that
reward the OLT the learner only gets to
observe the reward for the action that
was actually taken
okay so that makes it what's called a
bandit problem it's called bandit
because it comes from the multi-armed
bandit problem which so in other words
the name seems kind of weird but it just
comes because of the history of the
problem so the learner observes this
reward but only for the action that was
taken and the goal again is to learn to
choose actions that will maximize the
rewards so I would claim that this is a
very general and fundamental problem I
know that these are toy examples but
we're really trying to get at something
fundamental here which is how to learn
to make intelligent decisions through
experience okay so what are some of the
issues in trying to attack this problem
oh I'm sorry I forgot to say that this
is what the contextual bandits problem
is bandit because of this aspect and
contextual since there's context
involved okay so what are some of the
issues in trying to attack this well
first of all there's a classic dilemma
between exploitation and exploration so
on the one hand the learner needs to
exploit what has already been learned so
enough you know based on what you
already know you want to behave in the
way that you think is best based on your
current state of knowledge but on the
other hand we need to be doing some kind
of exploration so that we can learn
about which other ways of behaving might
give better results so on the one hand a
medical diagnosis you want to give the
best drug available for a particular
illness but on the other hand as a
community we need to be doing some
exploration to be discovering new drugs
as well and how do you trade those off
between exploitation and exploration so
that's true of any bandit problem but
here we also have context and so we need
to be using that context effectively and
when you have context you have many more
ways of behaving than you do when you
don't have context because you know in
principle you could be behaving
according to any possible rule that that
takes those contexts and maps them to
actions so there are many many possible
ways of behaving in this kind of setting
and of course you never get to see the
same context twice or you're unlikely to
see exactly the same person a second
time I mean of course you have the same
patient coming repeatedly but you know
what I mean okay and another problem a
very important problem is selection bias
so to do exploration we need to be
gathering statistics about what
behaviors work well in what situations
so we need to be doing that exploration
but at the same time we're trying to
exploit the knowledge that we already
have we're alright we're trying to
behave in a way that will maximize
rewards based on what we know so we're
gathering data but we're not doing it in
unbiased way we're collecting data
that's going to be very highly skewed
because because of the fact that we're
trying to exploit while we're also doing
the data gathering
okay so selection bias is a very serious
problem and of course we care about
efficiency our goal eventually is to
come up with algorithms that will be
efficient right so everything so if we
care about time efficiency space
efficiency and also statistically
efficiency how quickly you learn okay so
in this talk I'm presenting a new and
general algorithm for this problem and
that in terms of efficiency what's
important about this algorithm is that
the statistically its optimal or nearly
optimal in other words the learning is
happening as faster almost as fast as as
possible
in terms of how efficiently we're using
the data that's being collected and in
addition
about time but you learn with the
minimum amount of right right okay
that's the statistical that's why I mean
by statistical performance okay that we
want statistically to be close to
optimal and then in terms of time
efficiency we're presenting an algorithm
which is much much faster than the
previous algorithms and also much
simpler I mean I know simplicity is a
subjective thing but in this case I
don't think anybody would dispute the
fact that this algorithm is much simpler
okay okay so that was kind of an
introduction so what I want to do now is
to come back to and look at things a
little bit more formally so this is
really the same scenario that I had
before of what's happening and let me
just introduce a little bit of notation
so again in the first step the learner
is observing a context which I'll be
calling X T so T is the round number the
iteration number and then based on that
context the learner chooses an action a
T so so in this talk we're assuming that
there are K possible actions that we
just call one through K so we're
thinking of K as not too huge so maybe K
is like 10 or maybe a hundred but not a
gigantic number so the learner selects
an action based on the context and then
the learner observes a reward for that
action that was selected don't we just
finished not at this point okay so this
is this is what we can do at this point
but you're absolutely right
so there's a lot of open directions and
that that's a good one okay so based on
the action that like I said the learner
receives and observes a reward arti of
80 so the reward for the action that was
actually selected and in fact there's
one more step which is happening here if
you want to model this mathematically in
fact what we're assuming is that at the
same time that the context is selected
some some outside entity call it the
environment or nature let's call it
nature nature chooses a reward for each
of the K actions
okay so nature chooses but not does not
reveal a reward for each of the K
actions and so that's denoted here by
this reward vector where it's a vector
where you have K entries one for each of
the K actions and it specifies the
reward a number between zero and one
let's say for concreteness for each of
the K actions okay and then the learner
chooses an action and gets the reward
just for that action that we selected
yeah so let me get to that in one second
okay okay so the goal now is to four now
this is somewhat of a lie I'm going to
change this in a second but for now we
can think of the goal as that of
maximizing the total reward okay so this
is the reward at time little T and we're
supposing their capital T rounds all
together so this is the total reward and
now to come back to Bobby's question
what we're assuming in this work is that
these pairs this pair of context and
reward are being chosen from some
distribution on every RIT so there's
some unknown distribution over these
pairs and on each round we're supposing
that these are chosen independently okay
so I'm not saying that XT and our T are
independent of each other
on the contrary we expect the context
and the rewards to be
we correlated because we want the
context to be useful in choosing an
action I'm just saying that from one
round to another the choices of nature
are independent okay yes well it doesn't
but it makes it kind of a boring Oh are
you saying from one round to another oh
I see so if the rewards are exactly the
same independent of the context then
that's a special case called the
multi-armed bandit problem I'll talk
about that in a in a second but that's a
much easier problem correct
so easiest case that we're starting with
okay good so here's just to make this a
little bit more concrete so maybe there
are three actions and so maybe the
context say a web user is a person so
this person shows up and you know that
this person is a man and he's 50 years
old and so on and so next and the nature
also chooses rewards or kind of
potential rewards for each of the three
actions but does not reveal them to the
user okay so they're supposed to be
hidden the three rewards then the
learner chooses an action let's say
action two and receives that reward and
also gets to observe it and then this
continues so another person shows up
rewards are chosen an action is selected
and a reward receive and so on okay now
and the goal is to maximize this sum of
rewards right okay so special case is
that was studied before this is the
multi-armed bandit problem that's the
case where there's no context which
means that the rewards are chosen in
exactly the same way on every round
independent of the contexts of the cut
there is no context or
context is irrelevant and this is a much
more studied problem and in that case
what you're really trying to do is
you're just trying to do as well as the
best single action so just this best one
action and so tacitly what's being
assumed there is that there's one action
that gives high rewards which really
doesn't make sense in the examples that
I gave that would be like there being a
single treatment which is perfect for
all patients regardless of their
symptoms and the test results and you
know anything else it's ridiculous for
medicine and the same thing for the
other example so in our setting in the
contextual bandit setting we can use
context to choose the actions so how is
that going to happen well in that case
what we're supposing is that there might
be a good decision rule a good policy
we're calling it a policy for choosing
the actions based on the context so just
to be concrete here's an example of what
I mean by a policy so it's a rule that
says if the person is male in this
example then choose action to otherwise
if she's older than 45 to action 1
otherwise choose action 3 ok so it's
it's just a rule a function that map's
context to action and by no means am i
saying that the policies have to have a
form like this I'm just giving this as a
concrete example the point is that it's
a decision rule and in general by a
policy which I'll denote by little pi is
just a mapping a function that map's
context to actions so typically the
learner is working with some space of
policies that I'm denoting by capital PI
that's supposed to be a capital PI and
we're thinking of that space as being
very rich ok so maybe that
policies that the learner is working
with is the space of all decision trees
this is a decision tree just a nested
if/then/else rule of this kind and so
well first of all we're going to assume
that this policy space is finite but
it's typically going to be extremely
large so for instance the policy space
might be the space of all decision trees
which is in a gigantic space if you can
imagine all possible it rules of this
kind of form so tacitly what we're
assuming now is that there exists a
policy in this space that gives high
rewards so in other words we're assuming
that we know the form of the rule that's
what the policy space is really giving
us it's the form of the rule so we know
the form but we don't know which
particular rule which particular policy
is the good one and that's what we want
to learn no X does not need to be finite
so policy space depends how you set it
up yeah very enough okay so for decision
trees it has to be very yeah yeah I yeah
I actually think that this might be the
easiest thing to generalize this part
about policy space being finite to
generalize at infinite ones but that's
when we want to compete with that's
exactly it
okay so we're gonna try to do as well as
the best policy in the space okay so if
they're all lousy then a learner will
also not perform well but if there's one
which is good and we want the learner to
perform well also
okay so the goal now is to learn through
experimentation to do what I just said
to do almost as well as the best policy
in the space and we're specifically
allowing these policies to be very
complex and expressive okay and we hope
that this will be a powerful approach
that we can learn to do as well as
really complicated rules so why is this
a hard problem well it's a hard problem
because we're assuming that the space is
extremely large so exponential in any
reasonable complexity measure
okay larger than we could then we have
computation time for so how are we going
to work with that huge space and also we
need to be learning about all of the
policies in this huge space
simultaneously while at the same time
we're trying to perform as well as the
best one and when an action is selected
remember that we only get to observe
rewards for policies that would have
chosen the exact same action okay so
we're only actually getting feedback on
a very small subset of the policies in
this space so in other words it's the
same dilemma of exploration and
exploitation but now on a gigantic scale
okay so coming back to our formal model
all this part is the same but let me
clarify now what the goal is so the goal
now to be more specific or more precise
is that we want the learner to achieve
high total or equivalently average
reward in comparison to the best policy
in the space so in other words we want
small regret so this is what I mean by
regret so this expression don't even
worry about the math of you don't wanna
this expression is the average loss of
the learner and this is the average not
average loss sorry average reward of the
learner and this is the average reward
of the best policy in the space and so
the difference is how well the learner
is doing in comparison to the best
policy in the space
and what we want is for that regret to
go to zero and we want it to go to zero
as quickly as possible okay yeah well I
guess we don't model that explicitly in
any way but yeah you would kind of
expect that I don't know I'm trying to
look at how I'm gonna get anything out
of context if I don't put any
restrictions on relation between the
bars policy space right you can consider
smooth policies for example and those
will have the properties that you and
Josh if what you said is true then there
should be good some but some good
performing policies yes move over that
means that you're performing as well so
you definitely need some you need to
know something about the policy space
and you'll see the kind of assumption
that that we're making it's of a
different form it's not explicitly about
smoothness although you certainly could
take that route absolutely but you'll
see it's something different okay all
right so this might seem like a hopeless
problem I'm trying to make it sound
hopeless anyway but in fact there was
already an algorithm that solves this
there's an older algorithm called
x4 which solves this problem it's kind
of embarrassing that I was one of the
authors on this paper and I still don't
know how to pronounce it whether it
should be X 4 or exp 4 I don't know so
anyway this algorithm called X 4 which
solves this problem and it does it by
maintaining a wait waits over all the
policies so it maintains one
for each one of the policies in the
entire space and it updates them in some
particular way and the regret of this
algorithm is essentially optimal the
regret is proportional the square root
of K the number of arms times log of the
number of policies in the space divided
by T so it's going to zero at the rate 1
over square root of the number of rounds
which is the optimal rate and nice thing
about this algorithm that if it works
even if the data is not iid it works
even for adversarially chosen data which
is a nice property that we're not even
aiming for in this work so that's the
good stuff
yeah we're assuming that the rewards are
always between 0 &amp;amp; 1 ok they're bounded
reach the rewards
okay sorry I should have made that clear
yeah so that's the good stuff about this
algorithm the problem is like I said
we're maintaining one weight for every
policy so the time and space
requirements are going to be linear in
the size of the policy space which will
be much too slow in our case because
we're thinking of the space as being so
gigantic okay so as we already discussed
it seems rather hopeless to do better in
fully general policy spaces so but
nevertheless when the spaces has a nice
structure of a kind that I'll describe
we're looking for algorithms whose time
and space requirements will only be
Polliwog rhythmic and the size of the
policy space okay so that's where we're
aiming for it so how are we going to do
that how are we going to what kind of
structure are we talking about so to
explain that let me take a step back and
consider the full information setting
which is the exact same problem except
where we get to see the rewards of all
of the actions all of the actions okay
so the setting is like this again a
context arrives nature chooses some
rewards the learner chooses an action
but now what's different is that the
learner also gets to see
the rewards for the actions that were
not taken
okay the outcome if a different
treatment had been prescribed to the
patient okay and this process continues
and again the learner is accumulating
some total reward now it's nice about
this setting what makes it a whole lot
easier is that if you pick any policy
high we can compute the rewards that
would have been received by that policy
okay so we can just go back and say well
this policy would have chosen this
action on the first round and this one
on the second round and so on we can
compute that and then we can figure out
what the total reward would have been
for that policy okay and that's and the
average of those is going to be a good
estimate of the policies expected reward
because the data's IID and then what we
can do is a very simple algorithm we can
just choose the policy in our space
which is empirically best which is best
on the data we've collected so far and
that very simple algorithm will give you
regret which is like square root of log
number of policies over T which is
basically optimal sorry yeah right so so
the point is how do you do this how do
you do this step of choosing an
empirically best policy in the space
well to do that you need something okay
so what we're commanding is that's the
assumption that we're making we're
assuming that you have a way of
selecting the best policy based on
observed rewards and we're calling that
an Arg max Oracle by an Oracle I just
mean an algorithm or a subroutine that
we already have for doing that and the
input to this Oracle this subroutine is
basically what I had on this slide
basically this table of contexts and
rewards that's what all this notation
means contexts and rewards and the
output
is the policy in the space that would
have given highest reward Orion we call
it an Arg max Oracle because of this Arg
max it's also called an ER annum Oracle
in it classification Oracle and this
might seem like a strange thing until
you notice that what we're really
solving here is just an ordinary
classification problem
okay classification learning so context
are playing the same role as examples
and ordinary classification learning for
people who've seen it
in machine learning actions are playing
the same role as labels and classes the
policies are playing the same role as
classifiers and the rewards the rewards
are kind of like a gain or a cost or a
weight that you put on the examples so
classification learning is a problem
that we already have lots of algorithms
for their decision tree algorithms and
SVM's and boosting and oh I can't forget
to say neural networks and regression
regression well it's not a regression
problem but you could use the logistic
regression I guess so so the point is
that if we have a good classification
algorithm for this policy space around
then we can use it at least in this full
information setting to find a good
policy yeah Oracle make some
approximation error or should it really
be a precise aren't nice okay we are
assuming that it's actually finding the
optimal okay and we're hoping that it's
going to be okay if it's an
approximation but that's something that
has to be either tested or more theorems
have to be proved so we're assuming it
can solve it perfectly okay so coming
back to the bandit setting of course we
only see rewards for the actions that
are taken ok so again here's where you
get to see all the rewards and in the
bandit setting we only see the rewards
for the actions that were actually taken
so we kind of still of course compute
the learners total reward
but if you pick another policy we can
only observe the rewards for that policy
on the rounds where that policy happened
to have chosen the same action as the
learner okay so like on round two it
happened that they chose the same action
so we got to find out the reward for
that policy but say on round one and
round three the policy and the learner
chose different actions so we have no
idea what the policy's reward would have
been on those rounds okay so we might
like to use this Oracle but the problem
is like I said we only get to see some
of the rewards and the rewards we do see
will be very highly skewed highly biased
okay which is going to be a big problem
because we're not choosing things just
at random we're actually trying to
perform behave well still this Oracle we
would claim as a natural primitive as I
said we have these algorithms sitting
around for solving it so the key
question we're really asking here is can
we solve this problem if we're given
access to such an Oracle so in other
words is it possible to use this Oracle
unbanded data by somehow filling in the
missing data in this table and somehow
overcoming these bias issues that's what
we need to do and as I said we want
optimal regret and time time space
that's only logarithmic and the policy
space size so I guess we already talked
about this but we're thinking of this
Oracle as as a theoretical idealization
and it's hopefully capturing structure
in the policy space and in practice
we're hoping that we can just use
off-the-shelf algorithms okay so that's
the problem and in fact there are
already algorithms that do this to some
degree so there's an algorithm called
epsilon greedy or epoch greedy and what
this algorithm does is on every round
we choose an action where most of the
time we're choosing the best policy so
far so say 99% of the time we choose the
empirically best policy and 1% of the
time we just choose an action uniformly
at random and this first part of
choosing a best policy can be done using
the Oracle so this is a nice simple
algorithm that just calls the Oracle
once per round and it's regret bound is
similar to what we had before but rather
than having a square root here we're
taking a third root okay so we're
raising to the 1/3 power rather than the
1/2 power so it's not optimal in terms
of regret oh the best is no the best is
in terms of the actual rewards so we're
still comparing to the best rewards even
though the learner cannot observe them
okay so we're comparing to the rewards
that oh I see for the best part right
right right
well there are different versions but
let's say the version where you choose
the best policy based on based on the
rounds where you choose things uniformly
at random okay we and you only yeah you
can assume zero for instance
yeah yeah I'm being admit I'm being
vague here but absolutely okay so so
this algorithm fast and simple but the
regret is not optimal and I you know I
it's arguable that there's a big
difference between 1/3 and 1/2 that you
know it's the difference between and
cubed algorithm and an N squared
algorithm it can make a big difference
in fact there is an alga there is an
algorithm that solves this problem and
gets optimal or nearly optimal regret so
this is an algorithm do to do Deek at
all so there algorithm is officially
called randomized youcb but pretty much
universally even the authors call this
the monster algorithm they call it the
monster algorithm actually because it is
a very complicated algorithm it solves
multiple optimization problems on every
round using the ellipsoid algorithm so
very time-intensive would be I I think
we've spare to say that would be
challenging to implement it but it does
get optimal regret so it does get
optimal regret but because it's so
complicated it's also a very slow
algorithm so the number of calls to the
Oracle is about T to the fourth times on
every round so T is the total number of
time steps and we want to think of T as
being pretty large and the millions
maybe even in the billions and so T to
the fourth time on every round it's
gonna be really really slow but does get
optimal regret okay so finally I can
state the main result of this talk so
the main result is a new algorithm and a
simple algorithm for this problem
contextual bandits with Oracle access
and the regret is nearly optimal so we
get that square root rather than the one
third power there's some log terms in
there that's what the soft though is for
and the algorithm is much faster than
the previous algorithm
okay so whereas the monster algorithm
calls the Oracle about T to the fourth
times on every round our algorithm
actually calls the Oracle less than once
around less than once per round yeah
we actually don't so I'm describing it
as if we do but in the paper we don't
every time the time is a power of two
you get amnesia and reinitialize
yourself and a super time yeah you might
you might be able to change this from
big t to little T but it's not gonna
make any difference
yes yeah oh I was hoping nobody
would ask about that yes this is correct
and we've talked a lot about how to
explain that intuitively and have not
really come up with a good answer for it
I mean okay it looks like I can achieve
fewer amo calls with the same regret no
the regret bounds gonna get worse okay
I'll get worse a bunch of dummy policies
that are completely useless and
recommend the same arm every time and
are just used to inflate the denominator
yeah yeah so it comes from there are
various things being traded off and when
you optimize them somehow this is what
you end up with yes that's probably not
taken into account and the reason why it
calls yeah yeah so right so so on
average the algorithm is calling this
Oracle about square root of divided by t
log number of policies for round so
about one over square root of T times
per round or said differently about once
every square root of T rounds okay so
much faster than the previous algorithm
and well just very fast so what I want
to do for the rest of the talk is try to
sketch the main ideas of this algorithm
all right okay so as I said before
selection bias is a major problem for us
so so you have this basic problem that
you choose an action you see the reward
for that action what about the other
actions you know what case is there any
way of estimating the rewards for the
other actions in an unbiased way so in
fact there's a very simple and old trick
called inverse propensity waiting so let
me just very quickly explain how this
works so suppose you want to estimate
the expected value of some random
variable like you want to know the
probability that some unfair coin will
come up heads and to make the problem
harder we'll say first of all that with
probability P you get to observe X but
only once so you only get to see this
coin flipped once and with probability 1
minus P you don't even get to observe it
at all which is kind of like the setting
we're in because most of the actions you
don't get to see their rewards at all so
the trick is simply to define this other
random variable X hat where we take if X
was observed we take its outcome and
divide by P just divided by the
probability of seeing it and otherwise
you zero and if you think about it
you can see that the expected rate value
of this new random variable which you
always is always defined even if you
don't observe X is equal to the correct
expected value in other words by this
simple trick you're getting an unbiased
estimate okay
so and then so we can use that idea here
to get unbiased estimates for all of the
actions not just the ones that were
observed I'm going through this part
kind of quickly because this was old
stuff prior to our work so but hopefully
you can get a feeling for the main ideas
if you haven't seen it before so
basically we can use this trick to get
these unbiased estimates for all of the
actions and then we can use those tests
amay the expected reward and the regret
of any policy as long as P is not zero
right so okay so it seems like wow this
just solves everything right I mean
what's the point of all this because
you've got these estimates now that are
unbiased that have the right expected
value so are we done well absolutely not
because the variance of these estimates
might be extremely large an extreme case
like Glenn said is where P is zero but
even if P is just tiny the variance is
going to be huge you know and
intuitively it has to be he shouldn't be
able to get some you shouldn't be able
to estimate the bias of a coin by not
even seeing it flipped even once and
okay so the point is that to get these
good estimators we have to control
variance okay that's the point okay so
that's the first observation so we're
going to need some way of choosing
actions randomly or semi randomly and
here's kind of a template for the
approach that we're taking so on each
round we're going to compute a
distribution that I'll call Q over the
policy space so we'll compute a
distribution over all of the policies
and then we'll randomly pick one policy
according to that distribution over this
space and then when a new context comes
along we'll just choose the same action
as the one selected by this policy pi
okay so this is not an algorithm it's
like a template for an algorithm and to
turn it into an algorithm we have to say
how to choose this distribution Q now
already this seems like a bad idea
because it seems like the time and space
even just to store this distribution Q
let alone to compute it it's going to be
at least linear in the number of
policies so already this seems bad I'm
gonna try to make things sound bad and
then worse and worse and worse and then
we're gonna dig out of the hole okay
okay so main problem is how do we choose
this distribution Q well on every round
we want to pick a Q with certain
properties so I'm going to describe
those properties that we want Q to have
so on the one hand we want actions we
when we choose actions according to a
randomly selected policy from Q we want
to get low regret so the first
requirement is we want our regret or we
only have estimates or our estimated
regret to be small makes sense we want
to choose actions with high reward so
like I said we can estimate the regret
of any policy so let me call that regret
hat and then this is the expected regret
if we choose the policy according to Q
and we want it to be small that's the
first requirement the second requirement
is we have to control variance like I
said we need low variance for our
estimates to be decent we want to ensure
that the estimates we compute in the
future will be accurate so to do that we
have to control the variance so you can
write down an expression for an estimate
of the variance I'm not going to write
it down because it's a little messy for
any particular policy PI and we won
that variance to be small and
importantly we want it to be small for
every policy in the entire space okay
because we want to get a good estimate
of the expected reward of every policy
in the space so we need the variance to
be controlled for all of them so these
are the two properties we need for Q and
by the way they correspond to
exploitation because here we're trying
to get high reward and exploration
because here we're trying to ensure that
our estimates in the future are good
okay so it's exploitation and
exploration made explicit all right so
all we have to do all we have to do is
find a distribution queue with these
that satisfy these constraints and in
addition we want it to be a distribution
and I'll just make that constraint
explicit and we can give names to these
regrette constraint variance constraint
and distribute it should be a
distribution and it turns out you can
make the problem a tiny bit simpler
easier by allowing the sum of the
weights on the policies to be at most
one because it turns out it's less than
one you can convert it to a distribution
it's a little trick for doing that so we
call this our optimization problem opie
and this is the idea of the algorithm to
try to on every round to solve this
optimization problem and choose policies
according to them and this is similar to
the technique used in the monster paper
but it seems awful like I said I'm
trying to dig us into a hole and you
know let me just try and convince you of
what an awful idea this is okay so it's
an awful idea because in terms of
optimization the variables are these
queue of pies and you have one variable
for every policy so you have a
exponential number of variables if we're
thinking of this policy space as
exponentially large and you have one
constraint for every policy again it's
exponentially large also these
constraints are not linear they and
of this variance term I didn't write it
out but it's actually a little bit
unpleasant and we don't even know if
this thing is feasible okay
sorry it's actually yeah the way I wrote
it it looks uniform it's actually if you
fill this in it's not quite uniform you
actually can soften up the constraint
for the worst policies you don't need a
stronger constraint for the policies
therapy room yeah yeah but I'm hiding
that okay okay so it looks awful but it
turns out if we can solve it then we can
prove that the regret is essentially
optimal and you know the proof is
basically that the regret constraints
ensure that we get low regret provided
that our estimates are decent and the
variance constraints ensure that those
estimates in the future actually will be
good enough okay so that's two lines the
main idea of our proof which actually
goes on for a few pages but that's the
basic idea
yeah this regret bound comes from
somewhere else and then the things that
you fill in for small and small come
from there what you need to get this
through oh yeah yeah you know right
right obviously with the constants
built-in right okay but how are we going
to solve it well the basic idea is to
find a constraint which is violated and
fix it or attempt to fix it and repeat
simply enough here it is in a little bit
more detail so we can start with Q being
the all-0 vector we can do that because
it only has to be at most one doesn't
have to be equal to one so we can set
all of the weights to be zero and then
repeatedly well first of all if you look
at the regret constraint or the sub
distributional constraint if those are
violated we can just multiply Q by a
constant smaller than one we can just
rescale them and force them to be
satisfied okay we can fix them just by
rescaling otherwise we can find a policy
in the space for which the course
variance constraint is violated okay so
we find a policy pi for which this
constraint is violated turns out and if
none of them exist then all of the
constraints must be satisfied and we're
done otherwise all we do is add some
number alpha to the corresponding weight
for the policy whose constraint was
violated okay so I know there was a lot
of complicated machinery to get to this
point but when I say the algorithm is
simple this is what I'm referring to
this is the algorithm start with the all
0 vector do these very simple tests and
very simple fixes until you're done yeah
so it turns out it turns out that to
solve this to find a policy where this
constraint is violated I don't have time
to go through the details but you can do
it using the Oracle okay so the Oracle
gets called once around
okay so I'll come back to that sorry
sorry once per round of this algorithm
okay so this is happening on a single
round so okay so but once per iteration
of this of this in kind of inner loop
okay so if it halts then it has to be a
solution because everything is satisfied
but how long does it take well to
analyze it we use a potential function
so what we do is we write down this
function and don't worry about the math
the math is just to make the point that
you can write it down okay
that's but otherwise I don't want you to
look at it the point is that's this
function and it takes as input non
negative negative vectors Q over the
policy space okay so not only does Q not
need to be a distribution or even a sub
distribution it just has to be a
non-negative vector it's defined for all
of them
and you can write down this real-valued
function and it turns out that this
function has some great properties it's
always non-negative it's convex and
what's really nice is that if you have a
vector Q that minimizes this function
then that vector has to be a solution to
the optimization problem so at the
minimum of this crazy function we get a
solution to this foam is a ssin problem
and the key step is to look at the
partial derivatives of this function
which turn out to be proportional to the
variance constraint for that policy so
it's kind of like all those constraints
get encoded in this one function and I
described the earlier algorithm in terms
of finding constraints and violate and
fixing them but it turns out that
another way of thinking about that
algorithm is as a coordinate descent
algorithm a different way of thinking
about it is that we're doing coordinate
descent on this same function V so in
other words on every round
we're just adjusting one of the
coordinates which can be written as q of
pi okay and so to analyze the algorithm
we can prove that on every iteration
this potential drops a lot we can do
that just but with the Taylor expansion
and since this function is non negative
that immediately gives us a bound on the
total number of iterations because on
every iteration it drops a lot we know
what it starts out at we know it never
goes below zero immediately get a bound
on the total number of iterations okay
so in particular we can prove that on
each round little T the algorithm halts
after about square root of T iterations
of this algorithm and as a corollary so
we were worried before that the solution
of Q my
itself be gigantic proportional to the
number of policies but in fact hue
starts out as zero and on every
iteration you just adjust one of those
weights so it's going to be a sparse
solution it's going to be zero in all
but this many coordinates so so okay so
in fact we can do things a little bit
better and I realize I'm almost out of
time but I'm also almost done so so far
we assume that we solve this problem
from scratch on every round so that'll
give about T to the three halves calls
to this Oracle and in T rounds in fact
we can do a lot better so first of all
since the data is iid we can actually
use the same solution for many rounds we
don't have to do anything for most of
the rounds because the data is iid for
these long epochs and secondly we don't
have to reinitialize the algorithm at
zero every time we can instead
initialize the algorithm based on the
previous solution that we had computed
before rather than starting fresh each
time
excuse me so we can use warm start and
if we do that so if we only update this
distribution let's say on rounds which
are perfect squares on rounds one forty
nine and so on
then we'll get near optimal regret but
we only need about square root of
capital T calls to the Oracle across the
entire sequence of T rounds okay so we
get a bound on the total number of of
calls to the Oracle which is smaller
than the number of rounds yeah yeah so
what happens is you're getting new data
across this epoch since the last time
you computed the solution and that does
or can cause the potential
to go up but when you analyze that it
turns out it can't go up too much and so
you can you can prove the whole thing
based on that one potential function
yeah yeah oh well Q is not big so Q is Q
is actually going to be nonzero only in
this many places so you can just store
the whole thing yeah and actually it's
true also it's true also across the
entire sequence even if you're using
this warm start the sparsity is never
going to be bigger than the number of
calls to the Oracle okay so that's
really it
so we presented this new algorithm for
contextual bandits gives nearly optimal
regret simple and fast and the number of
calls is much less than once per round I
guess yeah a lot of open problems future
directions some of them came up in the
talk first of all we need to try it
experimentally the other question is
whether we can get an online version of
this so we kind of have a mismatch that
we're getting data online but we're
using this batch data it would be more
natural to have an online Oracle of some
kind you know proof lower bounds and
then a last open problem is to handle
adversarial data rather than iid data so
let me stop there
yeah that would be that would be nice
yeah so so x4 explore like I said
handles adversarial data gets optimal
regret so yeah
so in principle seems like it could be
possible a natural approach that you
must have tried was take x4 and try to
implement its sampling procedure by
calling an a.m. oh yeah do you know for
sure that that's computationally no I
don't I don't know if it's hard I just
know that yes we have thought about that
and couldn't solve it
yeah maybe thinks along the lines have
follow-the-leader something Apollo per
liter or I don't know it would be great
don't know how to do it
okay thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>