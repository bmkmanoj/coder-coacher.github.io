<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>3rd Pacific Northwest Regional NLP Workshop: Short Talks | Coder Coacher - Coaching Coders</title><meta content="3rd Pacific Northwest Regional NLP Workshop: Short Talks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>3rd Pacific Northwest Regional NLP Workshop: Short Talks</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/anbDWBA5bNs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
welcome back to the short talk session
so for this session we actually have
five speakers and we really have a very
little amount of time each so we're I'm
going to limit the questions to one or
two questions and we'll only have two
minutes for questions so I'm really
pleased here to be able to introduce the
five different papers here so we're
going to be starting the first paper is
on approximate parsing with hedge
grammars and it's going to be Masha
yamaha Maudie talking about it thank you
very yeah great thanks okay can you hear
me go on there yes ok I'm going to talk
about approximate parsing with hedge
grammars today first of all thank you
and North was NLP for triggering this
submission to the ACL so I was preparing
this two-page extended abstract for
Northwest MEP then I decided why not
submit the same work to the ACL so I am
Brian an errand and submit that this
paper also with a different title to the
ACL and it got accepted parsing for
hierarchical structure is costly and in
fact some of NLP applications and skip
it altogether and replace it with
shallow proxies and like such as MP
chunks for example and the models to
drive this non hierarchical structures
our finances it equivalent so they are
very fast and however they omit all but
the most basic syntactic segmentation
and in some applications and having some
more degree of hierarchical structure
might be useful in inference even if it
don't end up a fully connected parts
theory at the end so in this time I'm
going to present H parsing as an
approach which provides a full
hierarchical structure for a local
window and by ignoring
any constituent which is out of this
come this local window so we define
hedge parsing as discovering every
constituent of the length up to some
span L this is a parse tree for a given
sentence example sentence that we have
every constituent in these parts we have
and has a span which is the number of
ward it it covers it can for example
this is 54 MP MVP constituents and it is
10 4 s constituents showing that it
covers Edwards and 10 words it's 13 for
this will be constituent here and for
the part where speech tags this pan is
just one which means that they cover
just one word ok to to obtain the
constituents that cover up 2l words we
do a hedge transform to the original
tree so we remove the constituents that
span more than L words recursively and
and connect their children to the to the
parent of the node so this is an example
for a leak 127 these are the nodes that
are subject to remove because they spend
more than seven words and so we remove
those and we connect the children to the
topmost node which is s in this case ok
a very interesting property of huge part
series is that they are sequentially
connected to the topmost node and as I
showed with the dashed line here so each
hedge constituents is the conditional
connected to s and this property allows
us to segment the sentence and before
parsing and parse these segments instead
of parsing the entire sentence and then
we combine the results and as I will
show later and this will give us give us
a huge speed up over the case that we
parse the entire sentence
this plot shows the percentage of
retained constituents when we do the
hedged transform and develop in to rank
journal penn treebank over fifty percent
of constituents and have a span of three
or less and for example for L equal to
15 we have over ninety percent of
constituents retained in the hedge
transformed tree compared to the
original parse tree okay what are the
methods for doing the hedge parsing as I
said H parsing is the approach of
finding hedges edge constituents of an
input sentence the baseline approach
which is the brute force case is to
parse the entire sentence using a full
context free grammar and then his
transform the result as you can imagine
this has the maximum accuracy about
minimum speed or minimum efficiency
because it has the full knowledge the
rich knowledge of context and so it's so
accurate but very slow I proposed to
alternative approaches over this
baseline in both we constrain the
searches face of zik parser and based on
the transfer indent different for hedges
and also use a huge bank grammar instead
of a full context free grammar and hedge
grammar H Bank grammar is a grammar
which is trained from the huge parse
tree as opposed to the fool and parse
trees this is how we constraint the
surface I will see why a parser since we
limit the span of non-terminals we can
we can expand we can limit the social
space of a civic a parser as I've shown
here by closing the cells which are
above the L which is seven in this case
except the cells which are along the
periphery of the chart and the
complexity of cik parsing reduces from
and cute and size of grammar to do this
number of short below I'm not going
through details of how we compute this
complexity but you are very welcome to
join and
I ask my question in the poster session
session which I'm presenting the same
poster okay okay these are the
approaches for her H parsing besides the
baseline the first approach parses the
entire sentence using a hedge bank
grammar and the second one pre segment
the sentence parts each segment
independently and simultaneously and
then combine the results together the
task of hedge segmentation is to change
the input sentence into complete hedges
so we train a classifier and apply it at
each for to decide if that word can
begin a new hedge or not so we define to
task one is unlabeled and the other is
labeled for the label we just have the
beginning and not begin tags and for the
label we also include the type of
constituents like NP or repeat and we
use the discriminative like linear model
using the average perceptron algorithm
to train the parameters and these are
the feature set that we use this is our
experiments experimental setup we use
the world series you're not in a penn
treebank the standard training
development and test section we trained
Berkeley latent variable grammars with
six split and merge we use the Box
parser in exhaustive Civic a person mode
and we use the standalone machine with
this specification to run the
experiments and we evaluated our parsing
result with the precision and recall an
FS Corps from the using the standard
eval script these are the results of
parsing do and development said 16-24
for L equal to 47 you can see that we
get order of magnitude the speed up and
when we parse using the hedge bank
grammar as opposed to parsing with a
full eik grammar in the cost of three
percent of accuracy and this shows the
comparison between no segmentation
versus pre segmentation
before parsing and if you look at the
Oracle line here you can see that
segmentation is potentially very
powerful in a fast and accurate H
parsing well in action we can not
achieve this accuracy obviously so we
achieved this accuracy and even in that
case we got an order of magnitude the
speed up over in the case that we do not
segment before parsing at the cost of
almost five percent accuracy real
reduction this is the result on our test
set or evals it basically the same
pattern holds here and these are the
results for different l values from
three to twenty and the eval sit with
our three different approaches the
baseline no segmentation and pre
segmentation before parsing as you can
see here and we could get a huge speed
up and we do segment before parsing
especially for small else and if you
look at this graph you see that when we
segment we get a consistent degradation
in precision and recall which points the
need for an improved segmentation in our
task I introduce a novel partial parsing
approach for fast syntactic analysis of
the input beyond shallow parsing it
allows for input sent a segmentation
before parsing our baseline segmentation
model provides significant speed up in
parsing but cascade errors remain as a
problem and these are fuel filter and
directions which is to investigate hedge
parsing in combination with the methods
that are available for prioritization
and pruning the cells of a civic a
parser the other direction is to improve
the hedge transform and hedge
segmentation model to achieve better
accuracies and the other is to really
evaluate the hitch parsing idea on a
real task like an incremental
translation thank you so much we thank
you very much
so while the other person next presenter
gets ready we have time for a couple of
quick questions I have the mic here so
have you heard of vine grammars and vine
parsing yeah so how is this different
from that yeah that's a different the
same concept but for dependency parsing
and they use some kind of similar
constraint for the length and the
dependency parsing as well yes what
that's the same idea what I'm dependency
parsing and we mentioned that as a
related work in the introduction as well
okay in time for one more quick question
while we get that set up okay a quick
question here a clarification I guess
when you're you're evaluating are you
evaluating against the true full parse
are you evaluating against a hedged
that's a good question that's against
the hitch transform trees yeah thanks
for clarifying great okay let's thank
the speaker again thank you very much
okay we're getting it set up here for
our next speaker it's going to be
shaoling and the talk is going to be on
context representation for named and
keep thinking i'm shaoling fry
University of Washington this joint work
with samia and my advisor 10 today I'm
going to talk about contacts reputation
for named entity linking so let me just
briefly tell you about what the task of
demented Lincoln is it's about
identifying intimations in text and
connecting them to the course to their
corresponding knowledgebase entries
usually Wikipedia so there are a couple
of major challenges for this problem one
is ambiguity where people tend to use
the same Mandy dimension to refer to
different entities for example we have a
Seattle in both sentences but the first
one refers to the soccer team Seattle
Sounders while the second one refers to
the city the other big challenges
variability also nonsynonymous where
people tend to use different names for
different names for the same entity like
the nickname of Seattle and also a
grenade like msre for microsoft research
so next I'm going to tell you about the
general architecture of a standard
demented linking system assuming the
entity dimension is given like Seattle
here and the count of and within some
contacts like the I'm going to simplify
of the contacts bio using only one
single sentence next time we we're going
to generate a bunch of candidate
entities for the system to select from
to be able to select the best one we're
going to rank them using multiple
scoring functions for example string
similarity to measure the similarity
between the ND dimension and the
canonical form up at the candidate
entity and also we can look at the
context around the ND dimension and
measure how similar it is
you the representation of the entity you
can have other scoring functions as well
and in the end we're going to sum them
up and pick the entity with a higher
score today I'm going to talk about
focus on the contact similarity so the
majority of previous name NT linking
literature has been using the back of
words reputation for the contacts of any
dimensions in the in the reigning
example we have a three word back beat
Portland yes lily and to be able to
compare that to the candida entities
we're going to look at the Wikipedia
text of the corresponding entities and
extract similary bag of words and we're
going to compare the the bag of words
you have to mention to each of them so
we observe a couple of the issues here
first we're going to the words in the
bag or sometimes in precise and given
the Prada to the wrong candidate also it
might include your relevant feature like
yesterday because yesterday it doesn't
help disambiguation of the end a
dimension what's about what matters here
is the verb feet however the this is
beat might not appear in the bag of
words for the for the corrupt entity
well what we're proposing here is to
make use of the dependency the
dependency graph of the album sentence
continue that animation we're going to
focus on the direct dependency I'll link
to the head of the animation we are
going to extract features like those the
first feature basically says the entity
mention is a subject of the verb of the
subject is the subject of the verb beat
and also we are going to include some
more specific ones like the the second
one which is a conjunction feature it
basically says the entity mention is the
subject of feet and also the object of
the
Orlin to to construct similar
implantation for the entities where
again look at the Wikipedia text extra
features using the same procedure and
also as a compliment we're going to look
at a bunch of sentences from the web and
the authors of those were voluntarily
link those important inventions to
Wikipedia to the corresponding Wikipedia
pages and we're going to apply the same
feature extraction and after you the
processing we are going to have a huge
matrix where the roles are the entities
and the columns are going to be the
features and the number of entities we
have in this matrix it ran is three
melon and put the feature of the number
of total feature the total number of the
features is going to be around 700,000
and you might suspect the the matrix is
very sparse and it has lots of missing
values so for example we might not see
the women out observe the expression
that Seattle be some other day other
team in either Wikipedia text or front
of the web sentences so it will be very
difficult to compare this representation
to the any other representation for the
ND dimension so will we propose here is
we are going to perform a matrix
completion task by learning a load
dimension of the embedding of both
entities and features which learn
signals from other correlations of these
features and also from out the similar
entities within the matrix and to
further encourage sharing and
purification of those information we are
going to add 5 500 most frequent
freebase types and failing those values
accordingly so the final matrix is going
to be composed by two two parts one is
the dependency features we extract it
and free base types and we're going to
learn a bunch of low dimensional vectors
for each of the entities in
of the futures so here are some
experimental results first we want to
this is more likely a sanity check we
want to make sure that the entity matrix
is making sense so we kind of we did is
basically picking up some pairs of
ambiguous entities like in this case we
pick up Georgia the state and georgia
the country and we compare the nearest
neighbors according to the ND d matrix
as you can see here it's kind of making
sense because the nearest neighbors for
georgia of the state are mostly US
states of the nearest neighbors for the
country are mostly european countries so
that's nice and then we look at the real
world them into a linking data sets and
the first thing we did is we want to
make sure that the proposed how our
representation is better than bag of
words so to isolate other factors we
basically just use the contact
similarity to rank the candidates and
the and the y-axis is showing the
percentage of one representation rank
the gold entity higher than the other
and the blue bars are the winning
percentage of our work and the green
bars are back upwards so each other data
sets we we tested our proposed
representation actually dumb it's
backwards so got it okay so this is some
preliminary results on end-to-end
linking accuracy of our prototype system
we are still other the whole system is
still under development for example we
don't have a joint inference on
component that considered the prediction
of the links together but still using
using simple string summarily and the
contacts emeriti will propose here we we
have comparable results at least in two
of the seven
and we were optimistic when we further
improve the system it's going to be
better and also just a quick reminder /
contacts reputation is independent of
any system is independent from the whole
system so you can be plugged in any
other name entity linking system just a
quick recap recap we propose a novel
reputation for for modeling the mention
context and to compare sparsity is using
the entity matrix we we perform the
matrix completion task and we show some
preliminary experiment results in the
end so that's it here we have time for
some questions and your hand is the
first one up here can we go back to your
result slide so I noticed that you're
doing well on a lot of the complicated
data sets like tak but something like
MSNBC is like such a easy data set
debate I was wondering why this work is
getting lower performance there could be
many reasons why we are getting a worse
one what one particular reason is the
MSNBC accuracy is is compared is
evaluated over back of concepts within
the whole document while we are sort of
like predicting the links for each end
ed mentioned independently so it could
be for the same entity mention we're
predicting different things that will
hurt precision if you look at the whole
document as a bag of concept so the this
could be one reason but there could be
others
well we the next speaker comes up to get
set up and it's going to be Victoria
Lynn who's the next speaker another
question at the back there
what is the most important between you
your syntactic side of the matrix and
the and the side where the other types
what happened if i use only two types i
don't either the synthetic feature part
or if I kind of try to weight them I
think the syntactic features and the
freebase type features are complementary
to each other I don't think it's
possible to only use freebase types
because there is no such clue for the
end a dimension so that you don't
actually have anything to compare with I
don't have a you know exact answer for
what's the most important syntactic
feature for the matrix and I think it's
mentioning dependence great thank you
very much speaker again while we're
getting set up here I'll have a chance
to introduce the next speaker and the
title of the next talk and what's going
to appear on the screen behind me here
is the title hopefully being leveraging
oh good timing leveraging prior
knowledge of output structure for
learning with incomplete annotations and
the speaker for today I is going to be a
Victoria Lynn so over to you tutorial
for 10 minutes thank you so good morning
everyone welcome to my talk I'm Victoria
today I'm going to speak about a
research project we did recently and the
focus of this project is the scenario
learning with only incomplete
annotations how our prior knowledge of
an ideal prediction structure would help
us to train about a classifier this is
joint work with our group from the
University of Washington with Sam year
look Lou hem and Bend husker by the way
if you're going to search for the
penthouse car family burner phone that
is a term we are going to google for so
the problem we look at his multi-label
learning the go is to allow a set of
labels to each data point this is a
problem we are going to frequently in
counter in different situations for
example nowadays there is a worried
health book social bookmarking website
where users could assign keywords to
their favorite websites scientific
publications are there everyday
photographs multi-label learning can be
seen as a simplest to form a structure
prediction innocence that the output
structure is a flat set of labels
however is just the simple structure
output as a number of labels we consider
grow one could encounter a lot of
different challenges when designing a
machine learning algorithm for it the
most immediate immediate one is
gathering incomplete annotations is
extremely challenging so think about
this basic example for a single image
what has a possible set of words you
could apply to it we see here there is a
castle there are pinnacles of the car so
and and some people might ask might want
to call it a beauty there's also the
life that are sketchy cetera when users
are lying text to these kind of images
no effort is made towards completeness
hence the annotations you get from the
users might look like this in this case
if we treat those missing labels as
negatives our classifier would be
confused because the features
corresponding to the amazing object are
still present in the image hence what we
really want to do is to treat the
mailing labels as question marks in the
sense that we are not sure whether they
should be present or not our model
handle this aspect by excluding them
from the definition of our loss function
and on the other hand we use the prior
knowledge about a desired prediction
structure to help recover those
information so here we present a
mathematical formulation our input is
represented as a design matrix ax and a
label matrix Y each rows of the design
of the matrices here Chris money to an
example so e 0 of X is a feature vector
and each row of Y is a label is a binary
level actor so that y IJ equals to 1 if
example I is tagged with Lego Jay and I
as I said before we really want to treat
those zeros in Y as cross remarks
so the our mother goes by completing the
label matrix as training goes along our
proposed approach is we would have a
inductive multi-label learning
prediction model and also a label
completion model and what we are going
to do is to do joint inference over both
of them during training so the matrix
representation of Y is key to our
innovation because here we see we have
observed some entries of Y which are
which are the ones corresponding to the
annotations provided by the users so by
the theory from matrix completion field
if Y is known to set it's like certain
structure product property with those
samples one is able to recover the
original y matrix with high probability
and the structure assumption we are
going to made about why comes from two
different aspects both are based on our
prior knowledge the first one is the
large number of label is actually highly
correlated it with each other in the
sense that groups groups of words tends
to co-occur together hence we would
accept we would accept expect that
possible labels actually come from a
smaller number of word groups
mathematically this means the latent
structure representation of why should
be closed later low rank and also from
the large number of possible attacks
each example can only be associated with
a smaller number of them hence we also
expect the true and the true label
matrix to be sparse so with those
structural sumption we are able to
design the completion algorithm which is
fairly standard we do this by fix
desired rank a and define the
factorization as a product of the two
slang matrix you envy and we compute the
UN we by minimizing their their error
loss over the complete over the observed
entries are why so that you could get a
low rank embedding that it is close to
your annotation and for the classifier
prediction part we just tried the
simplest binary relevance largest
regression so basically what it does
here is to train a logistic regression
for each label
we modify the standard logistic
regression laws to factorize over only
the observed entries of Y and remember
what we really want to do is to conduct
those two parts jointly so that like
when we compute the matrix the matrix
could take the label matrix could take
feature information into consideration
and also our train the classifier will
learn from the low ranking bedding so we
close this loop by adding a term that
minimize the KL divergence between our
classifier out food and the low-ranking
batting in business we are forcing
consistency between among the classifier
output the lower ranking batting and our
observation the other way to understand
this model is that we have in code the
structure property lower ranking sparse
onto the onto the low-ranking value we
computed and by allowing Q and P to be
close each other those soft constraints
propagate from Q to P his Q actor act as
well as a regular either on the
structure of the output of the
classifier and this is the philosophy of
the technique called posterior
regularization thereby we named our
model as called posterior regularization
low rank short for PR error the joint
objective defined in the previous slides
is non convex and fortunately but it can
be optimized using an EMS Taylor rhythm
so we initialize all the variable
properly and for the m-step with fix our
low ranking bedding and update the
parameters in the e step we fix the
classifier out food and compute the
low-ranking body again both estep and am
step can be two can be done efficiently
using stochastic gradient descent and
the the in rem process can work very
fast within a few iterations so here we
go over some of our experimental results
we can we basically evaluated models
over three datasets all of them are
gathered from social bookmarking website
babe sauna me and delicious
and all of those data sets have fairly
large number of labels ranging from near
200 to near a thousand here shows the
number of training example we have the
number of features and the number of
labels we calculate the evaluation
metrics based on label average like
example averaged f1 mirror both of them
are calculate just using the user
annotation provided so the first set of
experience can is to test the advantage
of doing joint modeling so we compared
with 3 base line models that can be seen
as sub components of our model the first
one is just a naive about binary
relevance classifier which is trained by
treating those amazing labels as an
actives and the second one is basically
saying that the first one but the last
function is defined over only the
observed entries and we also add a
sparse regularization on the output the
third one is to do label completion and
classifier training in two separate
stages we can see and the last one is
our model we can see here like by
ignoring the negative examples we could
have a fairly strong boost already but
on those three datasets our model that
used label correlation and doing joint
hearing inference has a significant and
consistent improvement we also compared
with some of the standard multi-label
learning algorithm which has has
previous published state-of-the-art
results but both of them are trained of
treating all those amazing labels as an
actives and here we see that our model
has a close performance to the best of
state-of-the-art here former especially
on the big test it has as our
improvement is quite significant and
also it's worth mentioning that the
training process of Homer is very
complex so it didn't finish within one
week our largest did it as that on the
other hand our model is quite scalable
so in conclusion I present here like for
complex structure prediction problem
gathering complete
would be an obstacle for designing the
machine learning algorithm and our prior
knowledge about the desired output would
it's very effective in providing extra
supervision for pruning the learning
space and posterior regularization is an
effective way to achieve this goal it
performance by a by enforcing soft
constraints on the classifier outfit as
future work we like to design better
evaluation method especially for
catering in the cases where we only got
incomplete annotations and also we want
to test the programs of our model across
domains and at last we should compare to
some of the recent machine learning work
on learning with incomplete labeling
thank you thank you very much if we have
time for a question here which is happy
I was wondering if you use any prior
knowledge to initialize the M what kind
of prior knowledge oh that's a good
question like I think the answer is you
can't but we didn't we just randomly
initialize all the variables for example
so the low rank matrix here we just
initialize them using random values for
the entries yeah and hopefully like the
learning process would help you to get
from the initialization to a local
minima okay great thanks and i think one
quick question when we're still setting
up there any others yes bringing the mic
t back here
thanks um so the prior knowledge you
encode is that there's a low-ranked kind
of completion will run as far as
physically right so do you imagine
extending the kind of prior knowledge
you can put into that part of the
optimization that like for example
biases in terms of how the data is
incomplete all right right I think the
question is like on the theta might be
in completing a virus the way so like
certain labels turns to miss I think the
answer is still that you can because the
Trentemoller we present here is very
flexible actually we didn't make extra
assumption on how you do the label
completion and how you do the training
so basically like if you have a better
way to model the label completion part
you could just add them into the
framework and I think the same similar
alternative minimization will still
apply here but I think that part is
worth trying because I think that's
really the problem we should ask thank
you again very much okay we have a
couple more talks to go here so
welcoming my colleagues from Vancouver
again back to the podium here we're
going to be hearing a presentation on
evaluating open relation extraction over
conversational texts over to you thank
you hi again well this is a work that
was done by one of her grad students my
saw but since she couldn't be here today
I am going to present her work at the
work is more a preliminary result and
preliminary work about how we can use
relation extraction tools or open
information extraction tools for
summarization and mainly we trying to
look at some results evaluation results
to see whether we can use those tools
for a force a musician or or we need
some adaptation well as I I talked to
you about conversational data in my
previous talk we have very precious a
source of information in
relational data so we are already
interested to dig into those data to to
run some analytics are not non data to
gain some information from such data and
as we know it grows fast it's grows
exponentially and we need some way to
deal with the information overload you
get from such data so the intuition
behind that is is for any kind of text
processing of technologies that you're
going to run over conversational data
what you need is to extract some some
valley information inside the tix so one
of the I think that we look at is
information extraction specifically open
information extraction Verde relations
are not predefined so it's pretty open
and then we decided to look at this and
see if we can extract valuable
information and then from there we run
our summarization and use such
information in also a musician system so
open relation extractions to try to
extract relations or pre-poll relations
entity relation entity I have here one
example like for example Facebook about
what's up argument one Facebook argument
to is about sub and the relation there
are many advantages of using open
relation extraction eloping information
extraction tools and one of them is that
these tools are available nowadays many
groups are working and they're trying to
improve them they are scalable so you
can run and then you can extract the
relations quite robustly nowadays and
you don't need much of any
domain-specific knowledge to run those
tools or turn those systems on your data
so we're very motivated to use open open
information extraction for summarization
especially we have quite a few number of
state of the
ours open I systems calling reverb only
which is an improved version of reverb
so an extra Colonel exemplar but the
fact is that conversational data are not
like news they are very noisy there are
less structured so we're drilling with
more problems with conversation so it's
not like the result that you get when
you run open information extraction on
news would be the same result that you
could expect that you get it over
conversations talking about not only
forums and blogs but let's say treats
they are very short less structured full
of acronyms and and it makes it
difficult for such systems to extract
the relations or informations and we
know actually that most of text
processing takes pre processing systems
or tools that we use for different
applications in our research when we run
it under conversational there are such
noisy data they read the performance
really decreases so for example when you
run naming TT tiger or syntactic parser
over tweets you absolutely get nothing
if they are not trained on the on the
domain specific data or we know that
about eight percent of Miss extractions
seven to thirty two percent of inquiry
extraction in in those famous reverb oli
are from incorrect parsing so if we fix
the parsing we can solve those kind of
problems as well so we know that there
are many challenges in dealing with
conversational data so in this work in
this preliminary work we're trying to to
first of all sample a good data set from
different sources of conversational data
from firms blogs to to emails meetings
and tweets we try to run and the current
evaluation of our open
information extraction system and then
at the same time since some of the
sentences from those kind of data sets
are not really real form or are quite
complex for the system to understand we
try to see if text simplification
techniques can help us to do a better
information extraction or not so that's
basically contributions of our work so
for data set creations we have these set
of data set for conversation we have
reviews we have emails we have meetings
we have blogs and online discussions and
we have social networks so we have all
this said what we need to do is to have
a good sample of each dataset putting
them together and run the system but
sampling in such a kind of valuable and
huge data set is not a simple
randomization problem you're going to
find a quiet consistent sampling method
to to get your data set so what we did
we use started by sampling using some
features some conversational feature and
some other features from the data sets
to have a good distributions that
representing many features that exist in
conversational data sets so I'm giving
you the good kind of set of
representation of good set of features
we have syntactic future conversational
features we we try to use them to
cluster or related says and then sample
from those for text as simplification we
know that complex sentences can be
simplified using those methods so we use
a one of the state of the art takes
implementation systems call for Easter
price which is a kind of statistical
sentence of purification was a very
interesting model so we try to use that
so in summary we have run our one of the
information extraction
call Olli which is one to sit up the art
or our data set we evaluated result
manually we simplify the data set we run
it again and then we compare it to see
how the result will change and that how
we evaluated we have the matrix of
number of relations extracted accuracy
of the relational extraction and
confident six core coming from oly so
for the results I'm giving it important
results so we can see actually most
cases the text amplification system
helped the information extraction which
is one of the things that we wanted to
know two to three acknowledge if you
want to use X simplification before
running information extraction or not at
the same time we know that the results
for different data sets are different
and then we know that the occurs
different modalities when we're on
simplification or not the result differ
in number of the cases for example when
we're on only itself they result in /
saul and bc 3 corpus are the best but
the reveal corpus or divorce and when we
simplify the BC Khafra still is the best
reviews still as they're not very good
we know that quit texting build vacation
is quite effective especially when we
have some data like Twitter data we
showed in our experiments also is not
very is not very effective in an
increasing accuracy off of ollie or
information extraction of information
girish of / datasets there are some
analysis in our paper that you can have
a look so in conclusion in
conversational leaders is we have lots
of challenges we have very complex at
kind of takes in terms of noise and less
a structure we got to know that email
and blogs are kind of easier for
relation extraction than reviews of the
products so probably for product reviews
on relation we should decide some other
ways to use them some knowledge to do
some relation if you come to our poster
we talk about reviews on musician as
well and and for future work we are
thinking of using some systems in
combination or different simplification
methods also we are trying to take
advantage of such relation extract
relations that are extracted for some
relation thank you very much ok we have
time for a question or two thing so by
by extracting relations are you also
interested in relations between
utterances yes exactly we are we are
very interested about that and this is
actually one of the work that we are
working on so for example the rhetorical
structure can be very interesting to see
the structure of any conversation and
use that for some reason we have some
insights and some work done VI the
evaluation phase so if you come to the
review summarization poster I will talk
more about the conversational piece of
using our framework great thank you very
much thank
for our last talk in the morning I'm
pleased to be able to introduce to you
Stephanie Lucan who's going to be
talking to us today about identifying
subjective and figurative language in
online dialogue over to you okay hi
thank you my name is Stephanie Lucan and
I'm going to talk about identifying
social language an online dialogue
specifically focusing on nastiness and
sarcasm so many of the current NLP tools
are based on this mana logic model of
language from traditional media excuse
me such as the news but as we've been
hearing throughout the day so far social
media is becoming more prevalent and the
type of style of this language is very
different consists of dialogues and
emotions and in formalities so in our
work were interested in creating new
models of dialogues by taking advantage
of the abundance of all the social data
in 2012 Walker Adele released the
internet argument corpus which consists
of annotations annotated exchanges and
online debate data it was annotated by
mechanical Turkish they were shown a
dialogic turn like up here and they were
asked to evaluate the overall language
of the responder which is in bold up
here a variety of types of languages
were annotated along the side but of all
these styles we were specifically
interested in sarcasm but how can we
define sarcasm it's very difficult
there's a lot of different definitions
people don't interpret it in the same
way and furthermore there's a
theoretical work that claims you need
the context or you need world knowledge
in order to determine if something is
sarcastic just an important note in the
previous Mechanical Turk task when we
asked people to identify if an utterance
with sarcastic or not we said just we
didn't give them any definition of
sarcasm we just said is the sarcastic in
your opinion so in this work we're
hoping to examine these utterances that
they picked out and
and try and hone in on the aspects that
they believed are actually sarcastic so
looking in our forms data we found
sarcasm is about prevalence in about ten
percent of the data but despite I mean
that that is prevalent but it's still
pretty scarce and it's expensive in
order to have human annotators to
collect more data so we think it'd be
useful to have a technique that we can
be able to learn sarcasm from a small
amount of well labeled and annotated
data so we look at a method designed to
do just this it's from felon and wry
laugh and wry laugh and we be in 2002 in
2003 and we recreate their method by
applying it to our data in hopes we can
learn new labels from the small amount
of labeled data that we have and we also
decide to look at nasty language in
addition to sarcastic language just to
see if this relafen we be method can
generalize well to the domain of
dialogue language so this is their
method in summary they first develop
cues so in their original task I'll
point out it's a task looking at
detecting subjectivity versus
subjectivity in a mana logic domain so
the first thing they do is they develop
cues for identifying subjectivity they
use these learned cues to train a key
base classifier and the classifier the
goal of it is to maximize precision at
the expense of recall they use this
classifier as a first approximation on a
large amount of unannotated data and
then because the precision from this
classifier is high enough they can take
these predicted labels and learn new
patterns from this data specifically
syntactic patterns and then they then
bootstrap this process and learn new
patterns on unannotated data so in our
work we found as we were following this
this process we found that we can't
achieve this high based precision with
just the q words so we we we use this q
based classifier we develop
we also develop a pattern based
classifier and we combine them together
and in the end we achieve a fairly high
precision for this task so the first
step is to develop these keywords for
our sarcasm and nastiness domain but as
I mentioned the theoretical work says
that you may need context in order to
determine if something as sarcastic so
we do have labeled data already from our
corpus we run a simple statistical
analysis to just select you two grams by
grams and trigrams but we also want to
see what providing context will do so we
create a mechanical turk hit where we
show them the quote and the response and
then ask them to pick out things in the
response that people could think are
sarcastic so from this here's some
examples of the highest rated ones we
have our statistical test was
chi-squared mt is Mechanical Turk so
there's some overlap you can see oh and
oh yeah appearing in both of them for
sarcasm but also we point out that in
the chi squared things appear like we
and we think this could just be this is
class just an over training issue so we
use these keys we found we make this Q
based classifier to rank the cues in
order of importance we use their
frequency and their reliability that we
found in our development set we trained
over a variety of theta 1 and theta 2 we
make the classification if two or more
of these cues are present above these
thresholds and here are our results for
sarcasm and nastiness but as I mentioned
before our goal is to create a
high-precision classifier and this is
not very high precision so we do notice
that nastiness does better than sarcasm
so this may be just a first indicator
that maybe nastiness is easier to
identify using these q words and sarcasm
is so okay we don't get a very high
precision but we continue in the process
of next learning syntactic patterns and
we come back to this problem later so in
the pipeline
um okay yes so next we learn these
syntactic patterns and the point is that
they can generalize across utterances
they don't have to be exact surface
matches like the q words and so these
are the templates that were used from
the original wry laugh and maybe work as
some more examples so this this template
now in prep noun phrase can match any of
these examples that we find in our text
what we ask the question are these
templates tailored to the subjectivity
domain which they were developed so we
also look at our data and try and
develop our own sarcastic cues or
sarcastic patterns so here are some
examples Oh adverb oh right and I'm
sorry we see appear a lot in our data so
so we run our pattern classification and
what we call baseline here is just using
the relafen we be syntactic patterns
without our new keys without our new
patterns so there is an increase
especially in nastiness this is very
good and in sarcasm it is better than
the Q based classifier was previously
and then looking at the new patterns we
see that they helped very much in
sarcasm which was what we were expecting
but in nastiness the precision jumps but
we called drops five percent it was
maybe like just ten utterances or some
very small number so in this we did not
develop patterns for nastiness so that
could be a reason is why this is just a
little strange okay so we've gone
through this pipeline but as I mentioned
a couple times now the Q based
classifier is not achieving the high
precision that we need it to to make
this actually work so what we do is we
do have a trained q based classifier we
have a train pattern classifier so we
combine them together so we make a we
make a classification we have two
distinct
and we make a classification if either
if either the Q based or the pattern
base says yes this is sarcastic that's
our or in the table below and then we
have another classifier where if they
both say yes this is sarcastic and
that's the ant and the table so
comparing the Q based classifier which
is the the yellow the 51-percent the or
does much or it's as much better in
terms of recall because we're using both
classifiers we're getting a lot more of
the of the data precision is still
increasing as well and for and we do
very well in terms of precision but as
expected recall is lower because we're
being more specialized and focused in
our classifications and in nastiness we
we see the same results where we have
the or based classifier it is better in
terms of precision then the q based and
then in the end eighty-eight percent is
our precision but with a lower recall of
30 so in conclusion the goal of the
relafen we be work was to develop this
high precision classifier that we can
use is this first approximation and then
learn from a large amount of unannotated
data learn syntactic patterns from it so
we couldn't really achieve that with our
Q based so but our combined classifier
is is on the right track so we've
learned that context is pretty important
for these cues as shown on their own
they're not doing as good as we expect
but the patterns do generalize well
especially our syntactic patterns and in
the future work we're going to now look
at unannotated data and run our
classifier on the unannotated data have
human annotators compare and we're
hoping to get a high amount of agreement
and overlap yeah thank you very much
so we have time for a few questions and
you probably want a nasty question so
you can use it in for you not sure
Heather so uh very interesting thank you
um I was wondering see how the Turkish
review the things and annotate them for
sarcasm water what was the or do you
have a sense with the inner annotator
agreement was like to the Turkish
generally agree on what was sarcastic or
was there a lot of variability there ah
there was I don't know the I can't
remember the exact numbers but there was
so we picked the the the utterances that
we selected for our study we made sure
that there was a high agreement in those
in those arguments we had about seven
annotators / utterance and we picked
ones that had four that were agreeing
that it was sarcastic or more great
thank you very much turning it over to
you thanks very much for this morning
session and announcements relating to
lunchtime right so I
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>