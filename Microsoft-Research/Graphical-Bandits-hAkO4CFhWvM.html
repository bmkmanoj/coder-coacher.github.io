<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Graphical Bandits | Coder Coacher - Coaching Coders</title><meta content="Graphical Bandits - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Graphical Bandits</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hAkO4CFhWvM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon so a lot of us study
various objects on graphs a woman and so
with this particles when the walkers
spins but this time we'll hear about
bandits on graphs from the expert
Niccolo discipline talk about experts as
well so this is a some work I've done in
recently and it's a lot start from
scratch so you will not need a lot of
background knowledge at understand
business so we'll be talking about
sequester the sequential decisions and
we will only deal with no stochastic or
adversarial models so this is the
traditional prediction with expert
advice or nos tokens no stochastic
bandit model and the model and that the
model is very easy to specify is very
bare-bones
so you have just K actions that you have
to and each one of this action let's say
will give you a loss when you select it
but you can you can't avoid it you have
to at every time step in your sequence
of decisions you have to decide which
action to pick and you will incur a
certain loss associated with your
decision so we'll assume that everything
is a no stochastic so there is a some
assignment of losses to actions over
time so there's some some numbers el-tee
and we'll assume that these numbers are
just in the unit interval for simplicity
and these are this is the loss for
playing action I at time T
okay the idea is that some unknown
deterministic process has laid down this
sequence of losses okay for each time
step before each action so this is
action I ranges from 1 to K and T is the
time which is discrete okay and you
don't know you have no no previous
knowledge about this that you had don't
have any any any prior about this
sequence this matrix of losses that are
assigned to the actions and basically
the the goal of the player which is
playing this game is to pick at every
time step T the player has to pick some
action I T in the set of actions and
will incur the loss associated with that
action okay
the player will typically use the
randomization to perform this choice and
that be a time T the choice of the
action I saw T of the index of the
action will be based on some previous
previous observations which refer to
losses of actions that have been
observed okay so we have to make this
precise and we get to these two the
observation the so-called observation
models okay so there are two very
well-known will study the observation
models which are the experts and the
Bandit model so let's see if at every
time step I pick I T and observe okay I
had two options here so in the expressed
model I observed the entire and won t l
kt i observed the entire vector of
losses associated to
to all the actions at bedtime step ok so
I pick action I sub T and I incurred The
Associated loss by get to see all the
losses of of all actions so this is the
call experts mother so think of
investing in in a number of assets you
can see the performance of all the
assets not only the assets that will
have invested money on okay in the
Bandit model I only get to observe the
actual loss the tanker ok so again
whenever I depending on the observation
model if it's an expressed game whenever
I play IT I know everything about the
past I know all past vector losses and
if I play a bandit game when I whenever
I play IT I will only know the past
losses that I incurred but I don't know
anything about the losses of actions
that I didn't pick in the past okay so
here indeed in this kind of non
stochastic setting where everything is
deterministic in the in the let's say
apart for the internal possible internal
randomization of the player the measure
of interest is regret and this is
defined by the sum over a certain number
of time steps let's say capital t of the
loss incurred by the player - the loss
of the best action up to the same time
steps and the since we are assuming that
this the player might be randomized we
will put an expectation over here okay
so we including the possible
randomization on the player we are
interesting in minimizing the difference
between the cumulative loss incurred by
the player which selects a certain
sequence of actions and
best possible the smallest possible loss
that a player could have incurred by
playing consistently the same action
okay that clear
so actions have abounded everyone that
loss so clearly this this thing that
this quantity can grow out more at most
literally with time because action self
constant may have cost of loss at most
so anything interesting can be said
whether when I can control this
difference in in so that it grows the
sub linearly with time and there are
there's a famous there's kind of let's
let's call this quantity by the way
let's call it RT for regret and we know
precisely up to constant factors what
happens in the expert and the bandit
model what are the best possible regrets
against any any possible assignment of
losses to actions over time so we know
that in the in the expressed case the
regret will go like square root of T
with the log of the number let's call it
sorry okay the number of experts and the
best possible regret in the bandit case
will also grow with the same rate as
respect to time I will have a worse
dependence on the number of actions this
is just because I'm observing one case
of the information I observe in the
index face mode okay so now this is a
this is sort of a very nice and clear
pictures we know that these rates are
tight up to constants and okay now I
want to I want to show you some model
which is somehow interpolates between
two these two extremes okay
so I would like to do so by introducing
an algorithm which is a generic generic
algorithm that is able to achieve both
this
regrets these two regrets rates up to
logarithmic factors so I'm willing to to
pay a little bit more here so let me
weaken this by including an additional
logarithmic factor so this is still good
up to logarithmic factors we know it is
just a little bit worse but now I have a
single algorithm that with a slight
modification is able to achieve these
two regrets accordingly it is run in the
expression in the bandit observation
model so this is Algren is a variant of
edge of x3 and it's it's pretty easy to
explain
so the probability we just have to
specify the probability it is going to
be a randomize player so I'm going to
have to specify the probability of
picking an action at time T given the
past observation okay so we denote by
this the Sigma algebra induced by the
past observations so it's it's going to
be something obvious something trivial
in the expert model because this will be
just all the past vectors of losses in
the case of the bandit what the observer
really depends on the outcome of my on
the realization of my random variables
of my random selections here okay so we
denote this by P I T and this is going
to be proportional to e to the minus ETA
and I D minus 1 ok this L hat here is an
estimate of the the past cumulative loss
of each action I so I'm going to pick
action I with the probability which is
exponentially small in an estimate of
the loss that this action suffered in
all pasture steps so we're going to give
an overwhelming from a probability of
picking the the best action according to
our loss estimates but we also give some
a non non vanishing probability to of
picking an action that didn't perform
the best in in the past ok so now what
is this so this is
i t minus one is simply the sum s 1 to t
minus 1 of and I as hat and these are
instantaneous estimates of losses and
these are defendant like this equals the
an I comma s divided by Q is times the
indicator function of L I as observed so
if the according to the observation
model I a time si I do observed the loss
of action I then I will like estimate
the loss of the action using this ratio
where Q I should say what is Q is Q is
is simply the probability of observing
that action the loss of the touch it's a
probability of Li Li yes absurd given
the given the past yes so are the Li T
so for fixed i li t is completely
unrelated over different values T yes
it's it's completely unrelated the idea
here is that the idea here is that if
you give me completely random
arrangement assignment of losses then no
matter what action I play I will
essentially make I will won't make any
difference
because everything is random so this
regret will be is going to be really
small follow you if you only know what
it seems like you can't maybe I'm
missing something it seems like yeah I
choose any information right right it's
quite it's quite surprising the
attraction can I will I will give you a
little bit of
of explanation here strategy B if for
example all of the elves are one except
for one hidden one that's zero
okay all of them are one except for one
is zero and yes yes just I guess I
couldn't imagine so yes okay if if the
optimum you only compare you know if
there if there's one on no random zero
is the one nonzero trend on then it's
it's easy somehow because I will leave
your only I'm comparing I will compare
with a fixed column of this matrix Yeah
right so if there's one just one which
is consistently no zero loss then sooner
or later will sort of identified if it's
random doesn't matter so if there is
some structure I should be able to pick
it up even though I don't observe
everything even the managed model so
okay yeah but but it's perfectly yes
thanks for asking this question so this
is the the probability and right yes
okay dip okay I'll tell I'll tell you in
a moment so this is the next thing okay
so first of all okay let's see it's okay
so Q is for instance we can say Q is is
going to be one
sorry okay so what is the probability of
serving a loss of a certain action it's
going to be one in the band in the
express case because I do observe
everything by definition it's going to
be the same probability of picking
detection because in the Bandit model in
the bend of model because there I only
see what at peak okay so here we have a
and another thing and now you can see
that this definition is clearly gives
you that expectation for any fix that I
asked the expectation of now I should
say I should write as minus one this is
going to be exactly be correct because
I'm dividing I'm putting here this this
indicator function and when I take the
expectation for a fixed tie then this
this will be the probability of
observing the loss which is exactly this
and will cancel so I get an unbiased
estimate of that all right so now I so
now you see that this is M now now you
see okay this is a very specific
observing I mean these are two specific
observation models right I observed
everything of just observe what I what I
pick in general you might be willing to
run this algorithm with using different
observation models okay and I will where
do you get these observation models for
instance you might get observation
models from a graphical information
associated with the actions so I come
down I'll come to that in a minute but
first let me just say two words about a
few words about the analysis of this
algorithm okay so
how do I go about proving something like
like that for these are going here so
essentially I want that the proof is
actually quite short but I don't want to
to spend most of my time on it so I will
just tell you that analysis okay so the
key of that the key to the analysis is
to look at first of all the these
weights here which are the weights
assign it to actions ok the knurl these
weights when I normalize them will get
me the probabilities we wish I'd pick
actions here okay and then I look at the
sum of weights at a certain time step
and then I look at the ratio between
let's say normalization these are the
normalization factors of this weight
which get me the probability and I look
at the evolution over time of this
quantity over here this is a kind of
potential function that allows me to
analyze the the evolution on let's say
effectively gets me a way of controlling
this distortion of regret so I will give
you just some some little hint about the
analysis so what you can prove the
termini deterministically
deterministically i mean consider any
realization of the random choices of the
algorithm as as given by these
probabilities so the argument is playing
according to these probabilities and it
will have a certain realization of of
actions that are selected up to time T
okay now I want to tell you something
about this this this the sequence of
actions so it's it's not
easy to prove something like this some
over time some of over actions of P i.t
i.t had smaller than equal then the
cumulative estimated loss of any action
T and I will be interested in the index
of the best action for the horizon and
looking it because so K will be little K
let's call it J J will be the index of
the action achieving this this minimum
over here okay and then I can plus so
this is basically a very simple
algebraic manipulation of the of the
curve of this quantity here sum it over
time and I'm just using a very easy
second-order Taylor expansion in order
to linearize the the exponential
function over there and very little less
so it's basically algebraic manipulation
and I get to okay okay so this is a sort
of a basic equation which I which I get
which I get to very easily starting just
from the analysis of this quantity here
and this wholes deterministically for
any sequence of actions for the by the
player and it's at the basis of all
analysis of this exponential with the
diagrams so now what I can do I can take
expectation with respect to the random
with respect to the distribution of
these random variables
I know the distribution is this and so I
already know that the these are unbiased
estimates of the losses and I can also
very easily see that the second moments
of these estimates are easily controlled
by by this and one over s can you read
over there
okay so so basically just because the
definition of the it's very easy to see
because of the boundedness of the
lawsuit because of the definition of
velocity estimates I can prove sorry
this is going to be an inequality I can
prove this okay now if I take
expectation here I'm using the unbiased
ninis and that in quality here what I
get is is the following
okay so I still have a expectation here
because these are random variables okay
VI T and I T so the heads go away
because estimates are unbiased and also
heads go away this is the sum of losses
so just by linearity the heads go away
this is a J and this is sorry capital T
okay this is a constant and I have
something here okay I made a mistake
here probably I had the PID which I
forgot here okay and so what I get here
is bi t / qi t okay and this is also a
random quantity because this these are
determinate the random functions of past
observations in general okay now good
okay now here is just this is just the
cumulative performance of the players so
this is the average this is the average
loss of the player which is playing
according to this probability
distribution and this is summed over
time so this is like like the cumulative
loss of the player we can call it L sub
I okay is this quantity over here
and this is the we can just pick J to be
the loss of the best action in the time
horizon so I get mean j JT I just pick
the best one
then I have Ln K over it and then it's
very easy to get to see here what
happens I have when in the Xpress model
Q is 1 so here I have a sum of
probabilities so this is 1 I have a some
probabilities which is 1 and I have some
overtime so this quantity here becomes
it a has T in the expressed case where T
is the horizon I'm summing up and in the
bandit case well Q is P P over P is 1 so
I get 1/8 1/2 T K because I'm summing 1k
k times which is the number of action
okay now by picking data in order to
trade off these two terms experts cases
so this is again expert and this is
again the bandit I exactly get these two
bounds over here ok good so now this is
the first part of the story
so now again I would like to play a
little bit with this observation model
so Oh what-what suppose now that the
actions are for similarities so again
suppose that there is like so my actions
are maybe like this so there is some
graph huh
so these are my key actions and there
are some similarities among them so the
edges of the graph indicate similarities
between actions so maybe no I actions
are ads that I display on some web page
and whenever I put an ad and I get some
information about the the revenue that
that that ad got of the the
click-through probability I will also
know that similar ads would have gotten
a similar loss or similar game okay so
now i i i can may assume that whenever i
play a certain action so pull down with
that i play this section over here now I
only get to see I don't see everything I
see a little bit more than what I
actually played I also see the losses of
actions that are in the neighborhood of
the dice on I picked I could have like a
random a random a random signal that is
yes that's correct but this is a
statistic free talk so he wanted and
don't have any I won't have any
randomization in the model okay but
definite is true indeed we have examples
of that but just just you know as a
philosophy but this talk let me okay so
now it's kind of easy to of course this
generalizes both models no so in the
experts case we have to click everything
I pick
gets a gets me to see everything else
okay okay so the neighborhood isn't a of
every node is the entire graph okay
in the bandit case I have an empty graph
okay whenever I don't have any edges so
whenever I pick something I only get to
see that in general I can have anything
in in between so now the question is how
should be I'm I expect to see some
scaling here that interpolates between
the two okay so what kind of scaling
should we expect here yes
not necessarily not necessarily I can
comment out the let's for the sake of
concreteness we can assume that for now
that the graph is fixed and maybe
unknown you don't know at the beginning
okay so how do you go about playing this
game okay bring this game you just it
don't do any D algorithm is good so you
just use this anger in you can what what
is now qi t so qi t is the probability
and recall is the probability of
observing observing li t given the past
and this is going to be equal to the
probability of picking that particular
guy plus the sum of all the j in the
neighborhood of the guy of the
probability of picking or picking them
so i will observe the loss of this e if
either I pick this or if I pick any
action in the neighborhood of this guy
okay so now excellent so we we had
basically done in a sense that all it's
all its left to do all its left to do is
to study this quantity here because this
quantity will determine the the final
regret okay so how does this continue
because you know we had two easy case in
in bandit experts very easy no work you
just put P and simplify or you put one
in some nothing to do but in general if
you have a general observation model
then it you'll have a little work to do
now so let's see how that it looks
so we are let's look at one of these
guys so just for for a specificity so I
can drop the index so I have P I / p i+
son and this is summit over I sum over J
in neighborhood of I of P G and P 1p K
is a probability assignment over the
vertices of the graph okay so now how
big can this be how can I so one way to
look at this is to take the counting
measure just as a sake for the sake of
our clarity if I put the counting
measure there I get something completely
combinatorial and this is one divided so
it's 1 over K and I just simplify the ki
throughout and I get just the sorry the
size of the neighborhood okay this is a
so give me any graph on oriented graph
and what is the sum of the reciprocal of
the neighbor of the degrees plus 1 okay
okay so this is a well-known result this
result is and this I tell you that it's
upper bounded by the independence number
of the graph so is the largest subset of
the vertices of the graph such that such
that no two vertices in this subset have
an edge in common okay
yeah so how do you prove these things
that this is actually easy and fun to
prove so let let's start from we start
from from ok give me any graph and let's
prove this upper bound over here so what
I do is I let's call a Q 0 this quantity
here I 1 over 1 plus and
okay and now let me get a zero is going
to be the vertex which has the smallest
neighborhood okay now I'm splitting this
some juicy rock I'm splitting it at the
sum over okay let me let me do like this
let me consider I want to consider the
graph eyes ear oh and I want to cut a
hole in the graph so I want to take out
eyes Eero and the neighborhood of i0 so
I cut this out from the graph a zero the
neighborhood and all the dangling edges
okay
and now the semi split in two parts so
what is left I call it Q 1 Q 1 is what
is left of Q 0 in the sum when it took
away a zero and all the vertices in the
neighborhood plus what I took away which
is I 0 Union and say i 0 let me get this
right okay so a sum of a J okay and
there's some 1 over 1 plus size of n J
ok so by can you see if I write down
here no okay so this is a forbidden area
is there no no-fly zone maybe I'll write
down here maybe write down here ok so
what happens now so you see that this is
a kind of unfortunate so let's look at
the quantity here so this quantity here
sorry I wasn't planning so this quantity
here is going to be okay this guy a zero
is the one with the smallest
neighborhood this end it's in the Sun so
I can replace every term of this sum
with the term corresponding term with a
zero
because it has the smallest denominator
so is the largest term so I have some
over J in eyes ear o Union neighborhood
of eyes zero okay and then I have one
divided 1 plus size of the neighborhood
of I 0 okay so now you see that I have a
constant here so this term is just equal
this sum is just equal to 1 because I
summing exactly this neighborhood 0 plus
1 terms that are all constants equal to
the 1 over 1 plus neighbor size of
neighborhood a 0 okay so now I know that
this is at most Q I plus 1 ok so this is
Q 1 Q 1 plus 1 so I and now i recurse on
the remaining graph so I take again at
the the eye with the smallest degree in
the remaining graph in the graph of the
whole I take it out and I again I can
write that this is that most what is
left plus 2 okay how many times can I
take out can I can I repeat this process
I take out the vertex and all the
neighbors ok I can do it exactly at most
independence number of the graph because
this is the largest number of edges that
there are vertices that the largest
subset of vertices that won't have that
I won't remove when I remove any of them
ok you can see this but yes sorry yes
so one eternity between one standard way
to be ten independent second just label
the graph with independent uniformed
variables and take those vertices that
have a map that are local Maxima that
are bigger than all the neighbors and
when you do that the expected size is
exactly yeah there are many there are
many ways there are many ways to of
doing it
many just the randomization gives it to
immediately list in quality because the
expected size of the label we okay I was
planning to use this proof a couple of
other times so that's why I'm using this
specific I'm sure there are yes okay so
this proof is for the counting measure
but you can generalize it for any
arbitrary measure over the graph okay so
now this gets you something which is a
will get you the disco today now will be
bounded by the independence number and
now in the day regret the proof will
immediately give you that you'll get
with scale like T alpha Ln K and you may
immediately see that in the case of the
clique the independence number is 1 so
this is 1 and I recovered the Express
bound in the case of the the empty graph
the independence number is the number of
vertices because I don't everything
everybody is independent of each other
so I have a Ganic I'll get a K here
which is the upper bound for the bandit
so this is nicely interpolates between
the two okay now in the remaining time I
want to take a look at the different a
more general situation which is okay
which is what happens when I have
directions on the graph so this can very
well be no because in just assume that
you have a situation in which
you know that's like the example I make
usual is this you're gonna buy a game
console and you get a recommendation for
buying like a high-definition cable okay
so if you're interested in the game
console it's likely that you'll need a
high definition cable in order to view
it view the game's very nicely the other
way around is less likely if you buy a
future cable maybe you don't have the
console some some other needs ok so
there are there directions now and the
lectures or core of course are reducing
a further the information that you get
okay now in which sense what is the
observation model here the observation
model is that whenever I pick some
action I only observing the losses of
the actions in the out in the out
neighborhood so I am serving the loss of
the action at peak and of all the
actions that are pointed to by edges of
the actions I pick okay so I won't see
this anymore because the edge is
pointing the wrong direction okay so now
the I can write again I can just okay I
can just revise this definition here now
the probability of observing a loss is
just we can just put a notation here
it's just the sum so is the probability
of picking that action plus the sum of
the probability of picking actions that
are in the in the neighborhood of this
so the what is the probability of
certain is loss either I pick this or I
pick any action that has an edge
pointing to me okay so this is now I
have reduced the information and I would
like to know how how what was the
correct regret rate so by the way in in
the previous case where we saw that
regret was killing with independence
number of graph we that's that's tied
for any graph so for any graph we can
provide
and matching lower bound for the band
for the game played it on that graph
that corresponds to beam to the
dependents number okay
so that's that's a sort of a variant of
the standard bandit proof okay so let's
see how do I do this first of all I
hoping okay maybe you know I can still
prove something like that we're here I
just put something bigger which is sorry
something smaller which is the inner
neighborhood however there is a counter
example that rules out the possibility
of getting exactly the same the same
kind of behavior okay so like so let's
see I if I have a directed graph like
this its total order okay so actions and
now I have I just like this and I have K
actions so total order over K actions
and now I have a probability assignment
of which is exponentially smaller on the
so let's say we number things 1 2 up to
K and then I have probability I is 2 to
the minus I so I have a very small
probability of picking an action that
observes that gives me the total
visibility okay if I pick this action I
also the loss of everybody else it's
like the experts case if a pick this
action I all observe the loss of this of
this specific action is like a bandit
okay so if if I have this bad the
probability assignment it's easy to see
that the this sum over here so the sum I
want to k-e i TP I is behind over Qi
this is going to be k plus K minus 1
divided K plus 1 divided by 2 I believe
and but if I look
at this is in the indy if I drop the
orientation of the edges I get a click
okay so the independence number of the
edge without orientation is one but this
quantity here is bounded by something
linear in the number of the edges so I
cannot hope in general unless I make any
any any anything and it's some
assumption I cannot to bound D the same
quantity ahead here now restricted on
the on the in neighborhood with
independence number of the graph
dropping the orientation of the edges
okay so the problem so the problem here
is that well when my one could blame
several you know component several
ingredients of the system so one might
decide to blame the fact that I have it
be too high variance in my loss
estimates so one way to reduce the
variance is the lost estimates is to
introduce bias so an easy fix to this
problem here is to change it to alter a
little bit my loss estimates so my lost
estimates now will be Taylor eight that
to the fact that I my graph has
orientation so I can expect situations
like that where my standardized mates
won't work so now I do biased biased
loss estimates it's biased lost
estimates let me just use the same
notation will be just the same as before
and I T divided by Q I T indicator
function of L I T observed but now I
will add a little bit of a bias down
there okay in order to keep those things
down so now I'm under estimating the the
true losses this is a negative bias but
I can control the variance
in in a good way so essentially if I
redo the proof I did before for the
original estimate I get something very
similar so I get that once I take
expectation they regret that quantity
over here if I use this estimates here
is bounded by something like this plus
gamma and then I have the this same
sorry
I have the expectation here I'm almost
done and then I have the L I t hat
squared the PID okay so the only
difference here is that I have okay let
me write this actually as it should be
so P I comma T divided by Q by T plus ya
okay so get a very similar relationship
that controls very similar quantity
controlling the regret and in the
previous case I have to deal with this
gamma here okay and you see here gamma
is playing essentially the same role as
eita
which is the the one I had in the
parameter ahead in the exponent of my
for my from my probabilities and
although here I I should find a way of
dealing with this quantity here so okay
so now in I can prove for for any any
choice of gamma I can upper bound that
this with something which is again the
twice the independence number of the
graph plus a logarithmic factor which
depends on several things including
gamma of course and alpha
okay so you see here that the the price
the price I pay in order to well I can
still control this quantity here in
terms of the independence number as I
did before but ever that just an
additional logarithmic factor which will
depend by which we depend on the this
gamma term this bias term here I have
introduced so the way this can be proven
it's also interesting I would like to
show it to you and I think I at the time
it's not going to be long and again I
will make a proof for the counting
measure the proof for the counting
measure is kind of silly because if I
know that I have a uniform measure I
know I know that like these things are
ruled out so I'm kind of but this is
essentially the essence of the proof is
already there and then by introducing
this bias term I can I mean I'm able to
generalize the proof to arbitrary
measures over the de Graaff okay because
this gives me enough control on the on
the denominator okay let's so let's see
this proof so I want to that's again
it's it's it's completely combinatorial
a question I have an oriented graph a
directed graph and I want to control you
can find things okay I want to control
the sum over all the vertices over 1 1 1
over 1 plus the size of the in
neighborhoods of these vertices okay so
this is the is like that gamma 0 uniform
measure and then there is a sort of a
technicalities to generalize it in order
to get this upper bound here so how do I
prove this
yes you're right I just described
inequality so the inequality will look
like this so it's again without the if
the graph if I drop orientation every
neighborhood I just know I know that
it's alpha if I orientation and I just
consider the in neighborhood then I have
twice alpha times a lot factor here okay
so now let's see how the proof goes okay
now I'm gonna again I'm gonna pick a
sequence of vertices as before so I'm
gonna take out the eyes zero which is
the word this time is the one with the
largest in neighborhood that's correct
yes and take a zero out and recurse on
what's left okay so I'm just taking a
vertex out all the edges not to the
neighborhood just the vertex okay so
it's again as before but now I just take
out these guys this guy here and they
left the neighborhood intact okay and
all right so let's see let's say reason
a little bit about about this so now I
want to take a 1 relate I want to relate
this with the with the independence
number of the graph without orientation
on the edges without directions on the
edges so this is the maximum so
definitely this is going to be bigger
than the average
okay just because I just I picked the
maximum okay so the average equals to
the number of edges divided by K sorry
this this sum here equals to the number
of edges because I'm counting and not
counting twice because I have
Direction's I'm only counting in so if I
sum over all vertices I just get the
cardinality of the number of edges
divided by K now drop orientations
if you drop orientations you might
instead of counting twice this guy guy
you just count it once but this will
only reduce the number of edges so I'm
still going in the right direction in
this in equal chain of inequalities so
now pretend of dropping all orientations
at this stage and then you use what
torrance theorem trance theorem relates
the density of an arbitrary undirected
graph with the independence number and
this gets you exactly K divided by two
independence number of the graph minus
1/2 it's it's usually it's not within
this way but I just wrote it for
convenience like this okay so now now I
can essentially do it write a recurrence
as I as I wrote before and this is going
to be I'm looking at my quantity of
interest here so this quantity here I
can split now in two parts so the part
that I have there which is so this is
going to be at most okay at most because
this is smaller this is going to be at
most one over one plus neighborhood of a
zero plus the rest I different I 0 1
plus 1 plus okay
it's not minus one this is a - okay so
now just plug that in and I just plug
this lower bound on the size of the in
the neighborhood of i0 and that gets me
an upper bound because I have it a
denominator and this is alpha plus K and
then I have the same thing over here
okay now I i recurse and i just took out
one vertex from the graph so I have a
new graph there with the smaller number
of edges smaller number of vertices and
I keep going so I pick again at the the
vertex with the largest in neighborhood
I take it out and I keep on going like
this so at the end I what I have is that
the the quantity over here writing this
sum over I 1 over 1 plus and I minus is
more than equal then sum over I to alpha
a K I Ketu one I believe so the first
time I have K vertices the second time I
will add K minus 1 and I go down to that
okay so I just get 2 alpha and then I
sum not this harmonic sum over here and
they I get exactly at most one more
another ko okay so essentially write
that then I can take this proof
generalize it a bit in order to get a
control of this term which will include
will be a little more sloppy a little
more loose I have a K squared and and
everything but I can deal with I can
handle arbitrary probability assignments
okay now if I just tune it a properly
tuned eita and gamma properly and the
tuning of beta and gamma will be the
same I can get a bound which is exactly
which is the same form of a regret bound
I had before so again it will depend
it will again depend on the but on the
independence number but I will have
additional additional lock terms a lot
of factors that will be linked a and all
cilenti because gamma will be tuned and
in terms of 1 over T 1 over square root
of T so this is a so essentially even in
the undirected case I can still get a
control on the regret for arbitrary
directed graphs by using two tricks by
using essentially just one simple trick
which is adding a little bias to the sum
and then with the proper control on this
on this quantity which is really the key
quantity that rules the regret here and
essentially get the same result with
just additional of factors so this is
basically the message so so this is I
found this interesting because it gives
us sort of a nice way of blending
between the experts and a and D and the
bandit model yes sure
so in the directed graph model we could
draw the self edges point to myself
explicitly right and I could also meet
some of them so with that break the
proof if I don't point to myself yes so
where can you point if it's disconnected
there you mean what I just don't have a
loop I don't my everybody has at least
one incoming edge but it's not you know
myself no if you don't see your own loss
it's a problem well
well there's a this is a there's a lower
bound which you have a worse dependence
on the on time T 2 T to the 2/3 instead
of the revealing game so you play a good
action but you don't see it in order to
see something you have to play a bad
action I use this but I but you I
thought I proved to be that I knew wrong
so where can you show me where the proof
breaks down where the proof break down
the cue will always include the P this
is this is a so you don't have a 1 here
this is what you mean you don't always
have a 1 which means that you don't
always observe yes that that's that's
pretty crucial to I mean I don't owe you
is always bigger than P if Q is always
bigger than P yes because I'm I'm a
fail-safe to the bandit situation that's
correct so if you can always ensure that
the probability of serving a loss is
always at least as big as probability of
picking the action that corresponding to
that loss okay yeah that would be hard
to guarantee specifically there's one
really great guy whose probability
shoots up yeah I mean it's I mean all
these arguments that don't don't assume
anything about the probabilities this
this all these things hold for I mean
all the comunitaria stuff holds for any
probability assignment so it doesn't
matter what target everyone
ok so this is something that if you
start assuming something about the
behavior the probability of the other
and then it gets really really tricky so
you may also imagine an alternative
observation models in which you
basically you know what observed the
pair really depends maybe on
on the realized loss so if you if you
pick an action and you that action and
zero loss you don't observe anything but
if your action has a big loss then you
get to see something else so you may
also I mean there is southerner and and
and this this grass don't have to be
fixed as I say the beginning you
actually don't need that to know the
graph in advance so suppose the graph
are varying over time then instead of
having a dependence on the on the
constant of the constant instead of a
dependence on that T alpha will have
dependence on some of the the-- on the
sum of the case for obscure root on the
some of the independence numbers of the
sequence of grass so so I can have the
observation mother changing over time I
don't need to sit beforehand
beforehand I just need my probabilities
won't depend on the observation model so
I can pick it blindly and then some sort
and Z okay actually this is what you
observe and I don't need to I don't even
need to observe the integra in order to
update my probabilities in order to run
the algorithm I just need to observe the
the graph up to the second neighborhood
of the action I picked because I needed
to update the probabilities for all
actions whose loss I know and this is is
up to Anita more the probability of
picking so if I pick this guy I need
that will observe the loss of this guy
and I need that to calculate the
probability of observing this loss which
is a function of the neighborhood of
this guy so I need to leave I need to
know some vicinity of the graph of the
action I picked up another integral okay
I'm done okay
thanks for your attention and station
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>