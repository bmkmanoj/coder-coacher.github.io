<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable learning of Bayesian network classifiers | Coder Coacher - Coaching Coders</title><meta content="Scalable learning of Bayesian network classifiers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Scalable learning of Bayesian network classifiers</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HhysL6ex-7Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so as that so I went so is my great
pleasure to introduce professor Jeff web
Jeff is a professor in the Faculty of
information technology as Monash
University Australia where he had the
center of the research for intelligent
systems his primary research area
emotional in determining and the user
modeling his our editor-in-chief of data
mining and not discovery creditor of the
spring exact period of motion earning a
member of the otherwise report of
statistical analysis in determining a
member of the editor part of the machine
earning and was a foundation member of
the editorial board of ACM transactions
on key DD he was a coach PC chair of the
2010 hve international conference on
determining and the culture of the
Tucson health in HB international
conference on determining and received
the 2013 at GE a CDM service award so
today professor Jeff were talk about a
large scale based network of money now
I'm glad to have provided you here thank
you for those kind words am I supposed
to get this out of the way also are
ignoring the online audience of millions
we're a small group here so please feel
free to interrupt as we go along and a
happy to expand on detail so what I'm
talking about is a continuation of the
research I've been doing for a number of
years into essentially extensions Tanev
base so how to take the great
scalability of naive Bayes and expand
upon us and we moves recently on to
issues of how to best learn from large
data I'm sure I don't need to explain to
any of you the ever increasing need for
learning from ever larger quantities of
data but most people's response to the
problem of learning from large data
appears to be how do we take our
existing algorithms and scale them up
and my argument is that learning best
from large data is not a question of how
do we make existing algorithms best cope
with the computational challenges of
large data but rather that we need
fundamentally new algorithm so let me
start with giving you a very simple
overview of why I believe this I'm sure
we're all very familiar with the idea of
a learning curve so as data quantity
increases error typically starts at a
higher level and drops and at some point
asymptotes so you can take any learner
and plot a learning curve and of course
different algorithms are going to have
different learning curves and in
particular we will see that algorithms
that are able to provide very detailed
descriptions of complex multivariate
distributions are going to tend to
overfit when data quantities are small
so they're going to be outperformed by
lower bites by lower variance algorithms
for small data but they're going to tend
to perform better on large data so this
is on test data itís has been done a
learning curves where we take so we hold
out some data and then we do
ever-increasing training set size so
that's all of these examples i'm going
to show a couple of these curves and
they've all been taken on the poker hand
a data set because it provides very nice
your illustrations butts are the same
point the curves aren't always as fairly
smooth as this but the same thing holds
for
many many different types of data sets
Isis is fairly well understood as the
bias-variance trade-off now most machine
learning research has been conducted
with things like the UCI data set where
the majority of data sets are actually
no more than 10,000 fact even less than
1,000 examples I say they're way way way
down the bottom end of these some
learning curves so algorithms that are
going to be good here look extremely bad
in the space where most machine learning
research has been conducted so what we
need is algorithms that can closely fit
complex multi very very distribution but
the majority of machine learning
research I believe has just ignored
these because actually look very bad in
the space where majority of research has
been done who need algorithms that are
both low bias but are also very
computationally efficient right so it's
Lobos being computationally efficient
means you cannot spend a lot of time on
each training example why not because
you've got large numbers of them then
you simply if you multiply large numbers
by a large amount of time it takes too
much time and at some level of
scalability if you're going to use all
the data you have to be processing out
of core now most existing so most
state-of-the-art low bias algorithms do
not scale I'm sure you're all familiar
with this and typical low bias
algorithms state-of-the-art typical
examples are random forest support
vector machines and neural networks
these are all inherently in court you
need to look at every example many times
so you
you cannot process them out of core so
what I'm going to talk about today is
selective kdb classifier which is a
scalable low bias Bayesian network
classifier the structure of the talk is
the introduction of just given I'll talk
very briefly about Bayesian network
classifiers I think most of you should
be pretty familiar with the background
but sir I'll show you the terminology
i'm using and talk about the new
algorithm give you some experimental
results of course being an
experimentalist and then talk about what
next so a Bayesian network classifiers
what are they will they are defined by
two things parent relation so what links
are we going to include and classifier
then the conditional probability tables
which provides the conditional
probabilities that are required then we
classify given some class using the
posterior probability as proportional to
the joint probability of the class and
the set of attribute values which is the
probability of the class given its
parents times the product of the
probabilities of the X values given
their parents and usually what we do in
a Bayesian network classifier is we make
the class appearance of all of the other
variables and adds an illustration of
the nave Bayes classifier which I'm sure
you're all familiar with now one of the
very nice things about these types of
classifiers is once you've selected your
parent relation once you've got the
structure you can learn the conditional
probability tables very quickly by just
look justice uncovering the joint counts
that under light so in a single pass
through the data
you know which joint frequency counts
you need in a single pass and also in an
incremental manner so a very nice thing
is once you've got the structure you can
just keep refining the classifier by
just keep updating the counts you never
need to relearn so the particular
Bayesian network classifier that are we
working on in what I'm talking about
here is the K dependents bays so this is
reasonably old one mid 90s I think our
mayor and hasami mayor and sahar me I
developed this it requires two past
learning so in the first pass you
develop the structure you're going to
use second path simply fills in the
count and in that first pass you collect
counts necessary for both the mutual
information between the attributes in
the class and the conditional mutual
information between each pair of
attributes and the class and so then
order all of the attributes so you sort
the attributes into an order based on
the mutual information with the class
highest mutual information so therefore
the most informative to the left and the
less informative to the right and then
you go through the attributes in turn
selecting parents so the classes of
parents of every attribute and then the
parents have to be selected from those
with higher mutual information and you
select up to k so k is it user defined
parameters are you select the K higher
mutual higher of K earlier attributes
that have the highest conditional mutual
information right so we're looking at
the parents 4x4 look at which of X 1 2
or 3 as the highest conditional mutual
information conditioned on the class
that's the highest mutual information
with X
for conditioned on the clients so the
reason there's no connection export when
x 5 is because the visual permission to
fly the flight so what I've Illustrated
here is Katie be with K equals two so
every attribute can have it has the
class as a parent and at most two of the
prior attributes so there's no prior
abuse here there's only one so it has to
have this there's only two so it has to
have both of them here there was a
choice of two of these three and it took
the two that had the highest mutual
information with x4 conditioned on y so
that didn't include x1 and here we had a
choice of two and we've left out X 4 and
X 2 so that's done in right so the first
pass collects the two-way tables of the
joint frequency of each attribute and
each attribute value and each class
value to work out the commuter
information of the class and then the
three-way joint frequencies of each pair
of attribute values and each class value
in order to work out the conditional
mutual information between each pair of
attributes conditioned on the class with
real so in all of this with doing
discretization so the discretization
we're going to use is equal frequency 5b
okay so one path learned structure
second pass learns conditional
probability tables so this is quite nice
algorithm from many points of view so it
has training space so the first term
there is the size of the joint tables
for the conditional mutual information
so each pair of attributes each pair of
a so wise number of class
a the number of attributes V is the
average number of values / attribute and
then the second term is the complexity
of going are so it's as training space
so the second one is the complexity of
the count tables that you've learnt in
the second part and so they have to go
down for each attribute do all the
combinations of values for each the
class and all of the attributes that are
its parents classification time you only
need that second part you abandon this
part training time you've got so this is
the first phase so you've got the
collecting of the to take so you've got
the collection of the tables you've got
the calculation of conditional mutual
information and then you've got the
collecting of that table and depending
on the type of data you're dealing with
any of these three terms might dominate
we can see that it's linear with respect
to the data quantity which is very nice
in classification time is quite fast you
just go along you need to for each class
you're going to work out for each
attribute you descend a tree over K
values to find the correct account now
interesting thing about katy b is you've
got this variable K and it very clearly
controls a bias variance trade-off so in
my first plot I was actually my first
set of learning curves the two
algorithms there were kdb with K equals
2 and K equals 5 here I've plotted KTB
with k equals 0 which is naive Bayes
through to katy b k equals 1 2 3 4 &amp;amp; 5
we can see that five is still a long way
from SF
okay so we've got a nice way of
controlling bias and variance but we've
got a problem that we don't know what is
the right value of this for any
particular data set the only way we can
work this out is by actually trying so
so r 4 is getting close to sm toting 5
it's going to add some tote out here
somewhere so this I took 4 is here far
to see yeah I was very good yep all
right so it's very clear that some if
you have enough data a higher value of K
can never hurt right because all you're
doing is adding additional links if
you've got enough done enough data then
even if they're irrelevant they'll just
start will tell you that and will factor
itself out all right so adding k only
hurts insofar as the probability
estimates that you develop from the data
in are inaccurate and the the higher the
K the less starter each of those
estimates has taken from so the less
accurate they will be unless you have
enough data so somewhere here this has
to at least join this one if you just
get enough data okay so there's no way
to select for any given data set in
advance right but but some are the
attribute independence assumption might
actually be correct so 0 might actually
always be are the best value the other
ones will eventually asymptote to to
this but for less data they won't so we
don't know from the back pointing us
this is the poker hand data set but when
the same thing has to be
true for any data set because the
additional links are only harmful
insofar as the probability estimates
that in a in acuras because you don't
have enough data alright so if you go to
infinity the higher k is always going to
in the worst case asymptote to the same
value as a lower k and cannessome total
to a better values because they can
describe a wider range of distributions
and also to reduce the variance lick the
growth of your table grows exponentially
with care right yep yep and so there may
still be events that you haven't seen
enough you're not attempting to reduce
the variance of see what do you do with
zeros or things like that but so are we
we're doing an M estimates to seal off
the estimates yep yep we're doing an M
estimates on all of the all all of
probability estimates that we develop
yep spurious attributes may also
increase the error and again if there
was enough data they wouldn't matter but
if there's not enough data they're going
to introduce a slight amount of noise
and again we've got no way of selecting
that now an interesting observation
about the k DB classifier is that a full
k DB classifier actually embeds a whole
lot of simpler classifiers so for any
mij where we're doing is a value for K
and J as a number of attributes so we're
ordering the attributes here by mutual
information except the way in which
classifies developed so any mij is a
minor extension of em i minus 1 j and m
i j minus 1 so for example are with the
katy b classifier we illustrated earlier
you can simply take out
one of the level of links and here we
have a katy b k equals one where we've
just got for each attribute the parents
with the highest mutual information and
you can see I think how to have encoded
the full one we have to have also
encoded the lower level one or we could
just take out the last attribute and
again to have encoded the full one we
have to have also encoded this one so
we've actually in forming the full
classifier we've actually also created
all these other classifieds so the very
simple trick that we're going to do is
in one more pass through the data select
between all these classifiers and we're
going to do it are using leave-one-out
cross-validation so the full model
subsumes k times a sub models antes
number of attributes k as the value of k
actually it's k plus 1 minus a because
you've also got k equals 0 the knave
base model so each of these is a very
powerful model and we're going to very
efficiently select between a large class
of strong model no against do it in
phase through no no how does feet 1 and
2 run phase 1 and 2 is learning the full
kdb model phase three is now going to be
yeah so the leave-one-out is a little
bit of a cheap right no I want used all
the data to learn the structure but when
we do leave one out we're going to leave
it out in order to learn the structure
so depends what you think is being
cheated yeah so possibly but
we're looking at very large data and and
we're not claiming to learn exactly the
same as you that's not a that's not a
covalent yeah yeah yeah well I'm not
really saying that it's yeah it's not
really a fudge I'm not claiming that's
exactly the same as if you did leave one
out cross-validation setting a different
value of the number of attributes and a
different value of k and then compared
all the results not claiming that it's
equivalent to to that and so why leave
one out cross-validation because leave
on that cross validation is a very low
biased estimator of our sample
performance and because bizarre nice
trick makes its are extremely efficient
for bayesian network classifiers what's
bizarre knees trick xannies trick is you
collect the count tables that you need
to estimate the probabilities and then
when you come to classify one thing you
simply subtract it from the count table
so very efficiently you can perform the
leap on that you don't need to learn a
new model each time so because the full
model subsumes all the other models to
evaluate all of the sub models actually
doesn't take all that much more
computation than just evaluating the
full model in this way so it's very very
efficient ah so the result in complexity
well space is exactly the same because
the full model encodes all the models
that we're dealing with so we require no
more space at classification time we can
actually save space because we now might
have smaller values of a and K so we can
drop the unneeded parts out and the
complexity the only increases this final
additional pass
which is some as previously right so
previously we had a number of examples x
number of attributes times the value of
caves we went through are doing the
counts now we're going to have to look
at some for each class in the final pass
to do the evaluation so it's a slight
increase on the complexity often in
practice one of these two at something
sometimes actually just compiling the
conditional mutual information will
still dominate other training time so
this is not the a dominant term and
classification time again we may
actually be fast dropped parts that out
of the model okay so this is back to our
curves for katy b equals 0 1 2 3 4 5
this is what happens if we do our trick
and we don't learn right so we're
setting the number of attributes so
we're using the full attributes we're
just selecting the appropriate value of
k you can see we've done very well we
actually improve a little bit upon the
best k because sometimes the lower k is
performing better right so this is the
average over 10 10 run some runs one did
better than the other once appropriately
selected the wrong one here we're over
fitting and we jump to four earlier than
we ideally would and quite badly fitting
that something that we want to do
something about and something that shows
maybe given us a good point as to what
we should do it oh here is if we just
take katie beers five and select
attributes without selecting k so you
can see we're substantially improving
upon the version without attribute
selection and here is where we do both
together
so we're not doing much in the way of
attribute selection the small amounts of
data and then we have this overfitting
effect where we're jumping to it so
happens that with the attribute
selection k equals 4 n k equals 5 both
track along the line so they're both
both some fairly equivalent
so these are plots of results I got last
weekend right so I haven't had time to
work out the exact reason the
overfitting I've got some ideas about
how to avoid us but yes I don't have a
good explanation of why they're such
clear points right so the it's a sudden
one here someone here someone here and
it's clearly an extreme stream event
maybe something to do with the poker
hand data which is about poker hand so
in some ways a fairly artificial a data
set and there are I think ten classes
which the different types of hand and
with with some of the classes there are
only very small numbers of examples so
these may be the points at which you
appear to get enough evidence to start
to be able to accurately classify of a
class and perhaps it's mistaken in
though sorry how many attributes there's
only a small number of trees ten
attributes yes that's what we searches
at new selection also have that disease
it's jumping remember yep the world may
become smoother may also become worse
because it may that it may become
actually may jump all over the place um
okay so now on to some experimentation
so we've taken the 16 largest data sets
that's for this type of that should
value learning that we're able to get
our hands on most of them have numeric
data so so that's been that's been
discretized and while they're large in
terms of classical machine learning
research going up to 54 million we
should note they're really quite small
in terms of real-world applications
these states but there's a fair variety
in terms of the dimensionality and the
number of classes so we're doing the
discretization as we've already
discussed so first of all let's look at
performance against star k DB so if we
take the mac right so here the selective
k tebe we're always using k equals 5
right for no very good reason so we
compare selected k to be against the
form full classifier this is plotting
root mean squared error below the line
means some selective k DB is doing
better and you can see it's always doing
better if only marginally sometimes very
very substantially this is plotted on a
log scale ah and here what we've done is
we've observed after the fact which
value of K is best artists in some some
respects a theoretical result so we've
taken the best performing k and plotted
is against selected k DB and we can see
that with
sometimes performing substantially
better and this is clearly the attribute
selection I'm just doing this what sort
of Kostas have in terms of training time
so here we're comparing against out of
core Bayesian network classifiers so
naive Bayes we all know and some of you
may know so that's a bit like Katie
beers k equals 1 año de is my groups
extensions too naive Bayes this is the
full k DB classifier and selective k DB
again plotted on a log scale and we can
see training time sometimes the increase
in cost is not too much sometimes going
from katie be there is a substantial
increase in time so very somewhat
classification time you can see we're
doing quite well we're always no worse
than KTB k equals 5 for obvious reasons
usually substantially worse than knave
Bay's but sometimes not even all that
much worse than these bets okay so what
about things performance the accuracy of
these are so accuracy something you
going to it yet so I'm sort of all
you're doing in terms of okay so I've
left out the performance here we're
going to show summary at the very end of
this I'll show the rmse right so we
showed against ktv k equals 5 it's
actually outperforms all of these on
these larger data sets so what about
other out of core alternatives so
stochastic gradient descent opal webos
is perhaps state of the art in out of
core classifiers so here we've used all
of the
default settings accepts in that we've
looked at both squared and log loss
we're using quadratic features which
provides the best best performance so
for those not familiar with what that is
that means we take each combination of
attributes as the base features and
we're trying passes so three Casas gives
us something equivalent to what we're
doing and also 10 passes turned out to
not perform any better so vocal rabbits
you can train through any number of
iterations through through the data
discreet attributes have been made into
binary features which is the appropriate
way of dealing with them and for
multi-class classification we're doing
one against all so a nice thing about
katy b is that very nicely handles multi
features so if we look with the squared
loss we don't easily get a probability
value out of it so the first comparison
is on 01 loss rather than on the
accuracy of probability estimates and
some on the squared loss selective KTB
gets lower error eight times and opal
web at seven times so fairly close you
can see there's one very strong win on
01 loss which is the US Postal Service
extended data set which is a large
amount of sparse numeric data so we're
probably losing out in the
discretization but I think also
sparseness is very
important something vocus is able to
take advantage of that in a way that we
can't and here we are with the logistic
function which does convert the
appropriately into a probability
estimation we're doing root mean squared
error this biggest win is again the US
Postal Service extended and you can see
that the when there is much less extreme
with a logistic function which performs
better observation it requires far more
computation and we've not been able to
complete the computation for two of the
data sets which are shown as exes he and
we've actually used of open web it then
if we look at sour training time you can
see that right so again plotted on a log
scale selectively kdb zone only that one
case where we're performing poorly which
is very sparse data requiring more more
computation right remember this is just
on three passes through through the data
and often requiring substantially less
computation and classification time it
also tends to be faster again this one
out line if we compare against in core
state-of-the-art techniques so I've
chosen random forest as the exemplar of
incorp state-of-the-art because it's not
parameterised we've just taken the
weaker version and run it with a default
settings so a problem i have whenever i
compared against the state of the art
someone always says why didn't you use
this this setting random forest i've
found results in the fewest arguments
along
they say to the art in terms of Bayes
net learners so this is the way car in
core hill-climbing search for the best
bayesian network classifier it's random
forest again below the line is some
selective katie be getting the lowest
rmse above the line is the alternative
winning you can see that we're never
doing substantially worse than business
and often actually doing far better
because we are and perhaps one reason
for this is because we're not doing a
hill climbing search we're actually
taking a very strong family of learners
and are looking looking between them so
it's a little bit like doing a search
from the full classifier backwards
rather than hill hill climbing from
naive bayes up sometimes random forests
does are substantially better these x's
are where we aren't able to complete the
computation on the full data set so
actually only learning on a sampled data
set so it's possible we'd actually
perform so we would be able to get
better performance by learning on the
full data set which is not feasible with
random files but that's a here we've
compared performance on a sample for
these data sets because random forests
can't be completed on the full data set
but we can complete on the full data set
so we could get better performance on
that data set and and and
example vs the best you can do like that
so so we've compared we've made a
equivalent comparison right so both are
working on the same sample all right so
we kept significantly yep if you use the
full data would you be better we would
be better yes
yes so you mean Beezus that's oh yeah so
I'm trying to give a very clear picture
of what our performed is only random
forest if you had more computer I you
guys would be able to run random forest
on its does it's I'm going on the full
data so the we are clearly doing much
less computation and the only claim I
want to make is not that we're
performing at a higher level than in
core classifiers but I'm wanting to make
very clear incontrovertible that we're
actually performing at a very comparable
level two in court and so out of core we
can perform at a level which is very
similar to what you can achieve with
random forest inkl licenses are again on
log scale the time comparisons we are
comparing see coded against a Java thing
so you can't pay too much attention to
this the key thing is that we're out of
core these are in court right and we're
also bearing an extra cost because we're
actually having to load the data off
disc three times whereas these are
loading off disc once and a
classification time we're often faster
than ran fast always faster than Bayes
net so here's the comparison of all of
them to keep Ronnie happy so takes a
little while to get used to what this is
all about we've got color coding of each
of the data sets we've taken the error
and ranked us so a rank of one means
we've got the lowest ERA on that data
set and then each of these plots is one
two three to eight at so we can see
naive Bayes is almost always seven or
eight
I eau de which is improvement on naive
Bayes but really an improvement for
small data still so it decreases the
bias or knave bays but it's good for
thousands of examples rather than
hundreds of thousands of examples it's
some neck so it's getting sort of sixes
28 10 is next so 10 is like kdb with k
equals 1 Bay's nests which can learn
arbitrary complexity networks is next
vocal rabbit our web it performs best
real values as you disk repose for all
of them early know for so so so this and
this right so right so random forests
and vocal web at the ones that can use
numeric attributes are using the raw
numeric attributes and I think that's
why on the US Postal Service they do so
much better here's KTB where you
magically select the best k and
selectively k DB has the lowest average
rank and so it's not always best the
highest rank it gets as a tithe site
fourth place the earth King this is with
an sk so you magically after the events
are giving me worst until I do because
this is selecting both k and the number
batteries is the best came but with all
attributes alright so some observations
we're dealing very well with high
dimensional data perhaps because of our
attribute selection and we're dealing
very well with large quantities of data
right so that's probably comparing with
these the bigger the data quantity the
bigger the advantage we're going to have
over are the ones down there random
forest is performing well relatively
when they're a small number of
attributes and vocal web it has an
advantage for sparse numeric data
because it is
designed to deal with sparsity and it
can extract more information from
numeric data than we can are so that's a
different way of viewing are basically
the same thing so it's the mean and
variance of the ranking also 54 million
examples we would love to have some more
large data sets to play with if somebody
was too kindly given to it so forth at
large what's the large number of
dimensions or for that large one what's
the number of dimensions the largest
number of dimensions is about 700 so so
the largest data set has kidding i think
it has about 200 to 300 so global
comparison of we're at with the training
so the simple bayesian classifiers are
faster training k selective now starting
doubting my own I'm sure why we're
putting ourselves as better here and
clearly at test time we were performing
far better aode handles high dimensional
data very poorly which is why it's got
such a bad special were directly
ah VW wasn't done in wicker
so we've got quadratic features and for
multivalued for multivalued categorical
attributes they have to be binarized so
the dimensionality can rise a huge
amount as a result of that well most of
the data isn't spots
but not all the attributes are like that
right see so so not all the data is
spots okay so step back the key trick
that we've introduced here is nested
evaluation of a large class of count
based models and I think we've shown
that this can be very effective we've
shown us where that class of nested
models is generated by KD be there are
many different ways you could actually
generate that class of nested models so
we've also done the same trick with
averaged independence estimators which
gives us two paths learner which is also
quite good but not quite as accurate as
we're able to get doesn't scale to us
higher levels of innings cases you can
get K so it's such high order
interactions what remains to be done the
issue of numeric attributes has been
raised a number of times it seems like
there should be something better to do
we've looked over many years now have
looked at doing all sorts of things with
numeric attributes in Bayesian network
classifiers and solution remains elusive
um our preliminary observation suggests
that there's some bad overfitting
occurring even with the leave-one-out
cross-validation so have some ideas
about how to handle this but clearly we
need to do something to avoid that
overfitting there are many many many
ways we could increase the space of
models
are you saying that leave without seems
to overfit yep and right so what we're
doing is we're selecting the model that
has the lowest ERA now that maybe just
one example let's so what happens to be
in the training data may result in one
between a very large class of models
that are actually pretty much the same
level getting chosen and where that's
the case we'd actually prefer to choose
the ones the fewest attributes and the
lowest okay we've done some work on what
we call incremental cross-validation
we've observed the same thing and we
actually went to doing ten or twenty
fold cross-validation and found it to be
more accurate I can't you sit you can
still do the same trick with incremental
yeah yeah and everything to evaluate you
come back yeah but we the key point was
you have to allow the structure to
change yep you're doing this thing on
all the data where the FLQ a flavor okay
yeah yeah yeah yeah I'm actually
wondering whether it wouldn't be better
to just do the leave-one-out on a small
sample rather than all the data so as a
anyway we'll see so we can increase the
range of alternative models it's going
to increase the problem fitting of
course all we need to have addressed
that first there are many other ways in
which you can get nested are
particularly Bayesian network
classifiers maybe alternative sorts of
classifiers that are based on count
models I think that we could pull this
back to being just a to past learner
right so first pass is selecting the
structure and in the second we do the
training and just sample a small test
set because I don't think we need the
full data for the model selection in
fact maybe it's even harmful and I think
there's possibility of creating a one
parts learner and so if you've got
enough data then
you can sort of possibly bootstrap up
the complexity of the model using these
types of types of tricks so to summarize
I believe very strongly that large data
isn't just about scaling up existing
algorithms we really need fundamentally
different types of algorithms we need
low bias efficient algorithms I think
we're kind of hamstrung by just trying
to deal with it as a problem of dealings
the computational complexity and we are
working on a new generation of
theoretically well founded so there's
very clear theory behind Bayesian
network classifiers you know exactly why
they do and don't work perfectly which
are clearly very scalable and they're
capable of forming low bias models end
of transmission
questions
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>