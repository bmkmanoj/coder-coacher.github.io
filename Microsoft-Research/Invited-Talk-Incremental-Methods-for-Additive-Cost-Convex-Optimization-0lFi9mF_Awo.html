<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Invited Talk: Incremental Methods for Additive Cost Convex Optimization | Coder Coacher - Coaching Coders</title><meta content="Invited Talk: Incremental Methods for Additive Cost Convex Optimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Invited Talk: Incremental Methods for Additive Cost Convex Optimization</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0lFi9mF_Awo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon we're going to get
started with the afternoon session for
the afternoon session it's my great
honor pleasure to introduce professor
agios de gloire from MIT who finished
her bachelor's degree at Middle Eastern
Technical University in Turkey she then
went on to do graduate studies at MIT
finishing in 2003 and since that time
she's been a faculty member in the
engineering and computer science
department at MIT she's affiliated with
the lids as well as the new Institute of
data sciences and her research focuses
on problems that arise in analysis and
optimization of large-scale dynamic
multi-agent network systems including
communication networks transportation
networks and social and economic
networks today this afternoon she's
going to be talking about incremental
methods for additive convex cost
minimization thank you thank you very
much for the invitation it's a pleasure
to be here it's my first time at nips
I'm very impressed with the number of
participants and I can't resist i'm also
very impressed especially with students
giving their talks in front of this
audience so with that s Daniel mentioned
today I'm going to talk about
incremental methods for additive convex
cost minimization and in particular
present some of our recent work on
establishing new convergence rate
results for several variations of these
algorithms and this is joint work with
Merck gurus balaban and Pablo para los
both from lids ecs MIT Merritt is a
postdoctoral associate whom we are
jointly advising with pablo outlets so I
will be focusing on optimization
problems with a very special structural
characteristics additive cost problems
or in other words we're thinking about
problems where the objective function is
given by the sum of and component
functions fi of X
and these fi effects are real valued and
convex functions so I'm going to refer
to these as additive cost optimization
and throughout the talk I will focus on
unconstrained problems namely the
decision vector X can take any value in
RN and dimensional Euclidean space and
these are eyes in several important
applications probably this is redundant
for this community but a prominent
example is statistical learning problems
assume we have a large number of data
points X iyi i from 1 to M X I
represents a future vector Y I is a
target output and many statistical
estimation problems are based on the
so-called empirical risk minimization
where the goal is estimating the
parameter of interest here I denoted by
theta think about this as a parameter
that permit Rises your model by
minimizing an objective function given
as the average of a loss function over
data plus some penalty on the complexity
of the model which takes the form of a
norm in general and the examples of this
formulation is vast lasso support vector
machines many regression problems and
also classification problems this kind
of formulation this additive cost
formulation also arises naturally when
minimizing an expected value which is
sort of typical in stochastic
programming so we're thinking about
minimizing over some eggs expected value
of a function f w is here a random
variable which we assume it takes a
large and finite number of values in
which case this is exactly that's a
weighted average or you can even think
about W having it more general
distribution and we're approximating the
expected value by a finite average
another important application I would
like to highlight here is distributed
optimization over networks and we're
thinking here about an agent's situated
in a network each having its own local
objective function representing this
private
preferences and the goal is to optimize
the sum of these local objective
functions using local computations by
that node as well as communication over
the network that is connecting these
different agents in all these
applications as I've been trying to
highlight the number of component
functions M is large so if you want to
solve it true as you know standard
gradient algorithm full gradient or a
sub gradient step is costly this
motivates us to use incremental
algorithms which exploits their additive
structure and essentially updates the
vector X the decision vector using one
component function at a time one fi at a
time ok and the goal hope is that this
will have reasonable progress with
cheaper incremental steps just gradients
of the component functions and because
these components functions are processed
sequentially one at a time these are
also clearly well suited for problems
where fi is distributed as well as they
are known sequentially over time in an
online manner an important consideration
then in design of these algorithms is
how to choose the order for processing
component functions f1 f2 FM how do I
choose the order with which I will use
them in my processing this incremental
process ah the media choice is using a
deterministic order simplest one that
comes to mind is a cyclic order that
goes through these component functions
in cycles in a round-robin fashion one
two three one two three or you can think
about another fixed arbitrary
deterministic order in each cycle two
one three two one three two one three
another natural idea is to use a random
order the most popular especially in
this community is the so called
stochastic gradient descent or
stochastic gradient methods essentially
what it does is that each iteration it
picks a component function independently
and uniformly at random from the
collection of functions so you can think
about this as sampling with replacement
from the collection so in each cycle you
can sample the same component function
multiple times to 113 2 3 and you can
now also think about that without
replacement version of this idea which
at each cycle pick a random order
independently and uniformly at random
from all possible permutations of 1 to M
all possible orders okay and this will
be like a sampling without replacement
because in each cycle you pick an order
so you pick each component functions
once in each cycle and we will refer to
this as random reshuffling you reshuffle
the order randomly in each cycle I also
would like to highlight Network imposed
orders which is particularly relevant in
distributed optimization applications
this is basically you have to process
the component functions according to an
order imposed by the underlying network
structure namely if your agents are
situated in a ring Network then you have
to process the functions as 1 2 3 4 just
like a cyclic case if it's another mesh
network that has another different
global order that you have to use which
is also dictated by the neighborhood
relations of the agents another perhaps
local version of implementing those
orders over a network could be around
the money in which the next component
function is chosen randomly from all
neighbors of the current component
function ok and that would lead to the
so-called mark of randomized incremental
methods so in this talk I will talk
about first order incremental methods
meaning I will only use first order
information about component functions
first I will focus on deterministic
orders in which case the method is
typically referred to as an incremental
gradient method the best known results
for incremental gradient method is like
square root 10 verse 1 over square root
k type results where k is the number of
iterations and this was established for
non smooth convex component functions
the first thing I'm going to show you is
I will assume some smoothness and under
the smoothness assumption show that we
can achieve a rate of 1 over K in
distances improving upon this 1 over
square root K result and this using
smoothness translates into a rate of 1
over K square in function values and you
will show that achieving this rate
involves adapting your step size to
strong convexity constant of the problem
so we will also discuss more robust
variance that does not require such
adaptation it's slightly degraded
performance I will then focus on random
orders in particular random reshuffling
the idea that i talked about picking a
random saw a random order in each cycle
now this is used quite often in practice
numerically observed odd performance
chili but there's no analytical results
establishing its convergence rate using
our deterministic IJ results i will show
rate of 1 over K to the 2s whereas lies
between one half to one so this is
arbitrarily close to 1 over K square in
fact you can also achieve 1 over K
square rate with probability 1 in
function values for this method and this
improves upon the existing one over K
min max rate of Sgt and I will show you
how to achieve this rate basically we
will choose a particular step size
function and properly average the third
and then finally I will present another
idea I will go back to deterministic IG
and ask the question can I accelerate
this and we will study an interesting
variant incremental aggregate gradients
and show that it achieves a linear
convergence rate so my goal here is
basically to give an overview of these
different methods highlight important
features of the convergence mechanism
and hopefully also give you some
fundamental ideas and approaches for
analyzing them so if we all be able to
go through my whole plan so let me start
with sort of talking about main ideas of
the incremental subgradient methods this
is very similar to a gradient method
except with the key difference that at
each iteration the decision maker X is
updated incrementally by taking
sequential steps along sub radiance of
the component functions fi ok so instead
of the full gradient gradient F you do
it as gradient fi at each iteration
hence we can move this method in terms
of cycles each outer iteration consists
of a cycle these other iterations are
velvety no
it k and there will be cycles with M sub
iterations over I i from 1 to M whereby
I update this x ik along the g ik which
is a sub gradient of F I ethics ik
normalized by my step size alpha K so
you can actually sorry I can't point to
my slides but so basically you go from X
1 K in the retreat's x 2 x 3 x all the
way x MK xn plus 1 k gets you to the
next outer iterate which is x 1 k plus 1
ok so the odd relation between all the
retreat's can be written as the last
bullet here from x 1 k 2 x 1 k plus 1
what your aggregate is basically the sum
of these GI case and those are the
individual you know gradients or sub
gradients of the component functions
which serves as an approximation to the
full gradient gradient F of XK except
that it is evaluated at outdated eaters
instead of X 1 J it's evaluated at these
inner saboteurs it's a prominent
algorithm that appeared with a long
history that appeared in many several
literature's in AI it has attracted a
lot of attention in the 80s it's not i
don't think it's known as the
incremental method but rather the online
back propagation algorithm for training
neural networks another well-known
example of this method is the so
recently well studied cuts mars method
for solving linear systems of equations
AI prime x my equals bi and this is
basically incremental gradient apply to
quadratic component functions with unit
step size and the IG sub iterations in
this case basically boils down to at
each iteration take your point x and
project it to the hyper plane defined by
AI prime x equals bi and very I
cyclically or using some other order and
before I provide sort of a formal
convergence analysis let me highlight
the main features of the convergence
mechanism let's think about one
dimensional quadratics as our component
functions and following birthday causes
of recent spoken terminology let me
denote the interval
founded by the extreme minima as the
confusion region and the rest is the
far-out region ok so the far-out region
basically represents the region where we
are far from the global optimal solution
all individual gradients are aligned so
an incremental step points you to the
right direction and it's almost as
effective as the full gradient provided
you scale the step size appropriately
where is in the confusion regions the
individual gradients are not aligned so
one takes you this way one takes you
that way so oscillations their eyes and
that necessitates adjusting the step
size so this essentially shows us that
the choice of step size alpha K plays an
important role in the performance of
these methods a decaying step size is
essential to kill these oscillations
that I was talking about as highlighted
in a beautiful paper by law so you need
some decaying step size but that will
lead to slower convergence so in
practice one is also often interested in
approximate solution so it is of
interest to look at small constant step
sizes it won't get us to the optimal but
perhaps it will give us a fairly good
approximate solution and several
influential papers so load off from 98
netted particles from 2000 so that a
constant small step size ensures
converges to a neighborhood of the
optimal solution and in fact actually
there's a peculiar interesting form of
convergence that can also establish for
quadratics in that the sub iterates for
quadratic case do converge to different
limit points around the optimum close to
the optimum so you don't get to the
optimum but you get to a neighborhood
essentially so has I've been
highlighting there's a large literature
on gradient methods on the deterministic
order there are textbook treatments by
bertsekas poly upshore quite a bit on
convergence for differentiable problems
and the rate best rate known is
established for non differentiable
problems to be 1 over square root K
under strong convexity type conditions
as I mentioned before so the question I
would like to
and as a first step is if I assume the
functions are smooth can I get better
results so here are the assumptions I'm
going to adopt so the first one is that
functions are convex and the Sun
function is strongly convex with strong
comex the parameter C ok so think about
this as my function f not lying about
just a tangent hyperplane but above a
parabola see is the strong convexity
constant I assume some smoothness I
assume basically that the gradients are
Lipschitz some of the smooth I also
assume twice continuous
differentiability any each individual
component gradients have a Lipschitz
constant of a lie which translates into
a Lipschitz gradient for the sum
function to be L which is the sum of the
allies so we have Lipschitz gradients
also and also i assume sub gradient
boundedness which is basically assuming
that at every point for all component
functions every sub every sub gradient
ordering the smooth days every gradient
is bounded by sanji ok so i will adopt
all these three assumptions the third
one I'm going to tell talk a little bit
about where I can actually relax toward
the end of the talk I'll be able to
relax the sub gradient mountains so how
do we analyze the incremental gradient
methods as I've mentioned before it can
be viewed as an approximate gradient
method instead of the gradient I take an
approximation with the component
gradients evaluated at outdated points
so as such I can actually analyze it
similar to many analysis as a gradient
method with error so these are remember
the altar iterates X 1 K 2 X 1 k plus 1
and instead of taking a correct gradient
step I take something with an error DK
and EJ is exactly my error between the
actual gradient step and the component
gradients that are evaluated at these
inner iterates exciting so the analysis
of this part actually is simple
smoothness buys us a lot using
smoothness what i will do is replace
this gradient F by some form of
multidimensional mean value
theorem type expression AK times X 1 K
minus X star where AK is nothing but the
average of the Hessians over the line
segment between x 1 k + x star okay so
if i plug in this expression in the
first in the instead of this gradient F
of x1 k i can actually get a nice
iteration or recursion for the distance
k this plus k plus 1 is put an X 1 K
minus X star over there replace a
gradient F with that ok I basically get
that it's I minus alpha kak distance
plus alpha K times my errors ok so now
the whole point of the analysis how do I
control that error using so not that
gradient is the difference between the
error cycle error is the difference
between those gradients now I can use
the Lipschitz pneus and the boundedness
of the gradient to control that gradient
error by a term which is given by step
size l mg remember L is the Lipschitz
constant m is the number of component
functions G is my gradient bound so when
I put things together using also strong
convexity bound on AK which allows me to
bound it by the sea together with the
bound on the error and I'm using now
from now on the step size are over K ok
k is basically a decaying step size I
can actually get a recursion for
distance that way and to analyze that
recursion I'm going to use a very nice
result from the from a seminal paper of
chunk from stochastic approximation
literature also stated in poly acts book
a very treasure for continuous
optimization so think about UK our
sequence of real numbers and assume that
you k plus 1 is less than or equal to
some constant UK plus some error term
note that a is a positive constant there
so we get some nice contraction over
there but there is some error but s is
positive so my error is not too bad not
too big and what the result says is it's
a little little
notation heavy but I just want to
highlight the important piece basically
that UK goes to zero and the rate of
decay which is if you see a is greater
than s its k to the minus s is less than
as its k to the minus a so the rate of
decay is governed by the minimum of ANS
ok now for s equals 1 it's not too hard
to prove this you can actually write
down the iteration evolution of the
iterates explicitly and then you get
this products of 1 minus a over k for k
large you can actually approximate this
as 1 minus 1 over K to the a when you
multiply all of these things it's
actually a 1 over K to the a and on the
other side you have the accumulated
error you can use a similar trick or
approximation for 1 minus a over K that
will be like a.j over K today when you
do the integral approximation you get
this day over a minus 1 times 1 over K
so applying this result exactly to our
previous recursion not here that s is
equal to 1 and we're going to basically
need to be in the first case as is equal
to 1 I want a rate of 1 over K to the
minus 1 over K I need to make sure that
this constant a is greater than one so I
need RC is greater than one and when I
put everything together basically it
gives me the following rate IG method
step size are over k if you choose are
to be greater than 1 over C gives you
this rate of 1 over K and the constant
basically depends on the parameters of
the problem and important thing to
notice here is the one that's
highlighted in red you have to choose
your step size carefully r has to be
greater than 1 over C so no this
requires knowledge of the strong
convexity constant of the problem and
this is actually a very widely noted
problem also in several other
literature's not just deterministic
incremental gradient but also in
stochastic approximation stochastic
gradient descent literature's and very
simple example shows that this is not
just an artifact of the annales
if you don't get our right you can
actually get very very crappy rate this
example shows that i'm not going to go
into the details but one-dimensional
quadratics two functions strong
convexity constant of the sum is 1 over
5 which actually says from the serum you
have to take RT greater than 5 and
suppose I didn't know that I ignored
that and I took our to be one you can
write down the iterations and show that
actually you will get a convergence rate
which is no less than 1 over K to the
one over fist so this could lead to a
pretty bad dagger degradation in
performance so instead of them using
this kind of step size one thinks about
okay how about if we do a one over K to
the S we don't get one over K step at
one over K to this then without need for
adaptation one key to the strong
convexity constant one can actually get
great results which are robust in the
sense you don't need to adapt your step
size to the parameters of the problem
and basically the rate result follows
from a variation of the analysis that
I've talked about before and gives you a
slightly degraded raid 1 over K to the
s.s doesn't call the way to one if you
note enter the constant terror against
similar lmg are over steep so before
moving on to the random orders I'd like
to post a point out one additional
insight that this line of analysis buys
you so let's focus on the specialized
case of quadratic functions and instead
of the cyclic order I've been talking
about let me think about an arbitrary
deterministic order and I'm let me use
also the step size are over K to the s
as taking any value between 0 and 1 so
for quadratics under very little
assumptions mild assumption so this is
just one which is saying strong
convexity so nothing about this gradient
boundedness one can actually shows a
rate result for the distance which is
more refined compared to what I just
talked about in particular the iteration
complexity is still 1 over K to the s so
I'm not gaining on the iteration
black city but I just want you to notice
the constant which I wrote in red there
m sigma that is a constant that depends
on the order you pick so basically this
result shows you the effect of the order
you bitch you picked in processing these
functions on the performance of your
algorithm and that I'm sigma is
basically given there and it's nothing
but the sum of the Hessians waited by
the gradients that you have processed so
far so p sigma j sigma j is the Jade
Hessian that you've process times all
the gradients that you process up to
iteration J sub iteration J ok so it's
an interesting fact that we actually get
this dependence on the order one can
play with this M Sigma to say what kind
of insights this generates one simple
thing is to note that M Sigma is bounded
from above by J times the Lipschitz
constant of the Jade function times G
less than or equal to L M G if you
assume for instance bound of gradient
boundedness angie now a gradient
boundedness with a constant equal to G
so the husband on the right hand side if
you replace basically takes us back to
the previous estimate but the middle one
actually gives you insight basically
what it suggests this process the
functions that has higher Lipschitz
constants first so this is sort of also
in line with some of the randomized
algorithms that sort of process ore
samples the functions according to their
Lipschitz function so there's similar
insights coming from this line of a
house so with that let me move on to
random orders and basically I will focus
on the two popular one that I talked
about SGD and random reshuffling and
they may also repeat what they were SGD
is at each iteration you pick a
component function randomly
independently from the collection and RR
is at each cycle you pick a random or
independently so RR is basically
sampling without replacement HDD is
sampling with replacement from the
collection okay there's much empirical
evidence showing RR outperforms SGD I
actually picked these figures from a
paper of water from 09 it has a a short
very nice paper with title curiously
fast convergence of he doesn't call it
random reshuffling but SGD type mats but
this is exactly random reshuffling and
he actually uses it on a classification
problem the figures show the evolution
of the function values as K increases
for SGD and randomly shuffling and
clearly displays on the left if you look
at the slope a minus 1 so a convergence
rate of 1 over K for SGD and on the
right random shuffle if you just fit a
line you get a slope of minus 2 indeed
curiously showing a 1 over K square rate
for random reshuffling so this has been
posed as an open problem in Bodega co2
also more recently in red and grey and
both basic assembler basically attribute
improved convergence rate of random
reshuffling to the fact that it provides
a more even coverage of the training set
in each cycle you sample each component
function exactly once as of course the
SGD which may actually do repetitions
this is sort of the intuition why sort
of leads to these better rates but the
analysis is hard because of dependencies
of gradient errors in and across cycles
so before presenting the analysis for
random reshuffling let me just say
briefly some of the known convergence
rate results how to achieve them so that
I can actually make a better comparison
between these methods SGD similar to IG
has a very large actually mind-boggling
literature going back to stochastic
approximation literature's Robinson
Monroe key for both of its it's popular
nowadays in machine learning
applications because of its scale
30 and also robustness properties it's
an active area of research i apologizing
for sure providing here an incomplete
list but writing it all would take all
of my slides so i just put some of the
representative papers on achievable
rates robust variant second-order
versions more recently this is a recent
paper heart rate finger looking at
robustness properties of sgv so if you
look at the convergence rate results on
SGD for strongly convex functions and
there are some lower bounds it's a
tricky business so lower bounds so for
stochastic convex optimization SGD has
this one over k min max lower bounds
established in by numerous Givaudan more
recently by aggarwal and co-authors and
one way of achieving that in a robust
way without need for adaptation to the
convex strong convexity constant is the
polyak rupert averaging which is
choosing larger step sizes are over k to
this and curiously taking time averages
of these iterates of these trajectories
without changing the algorithm and for
you can jet ski from 92 has a nice
analysis using central limit theorem
type arguments that shows that if you do
this with this step size and average
sequence and if you look at the distance
of that averaged iterates to the optimal
solution x star that distance scale with
teh to the one half converges in
distribution to a nice normal zero mean
normal distribution so this can be
translated into a rate result for
function values 1 over k rate result for
function values so under the assumptions
i talked about one and two and more
technical conditions one can actually
our recent word provides convergence
rate result for random reshuffling so
let me repeat for average stochastic
gradient descent you get a convergence
rate of 1 over K or K to the one half in
distribution whereas here you basically
get a rate of K to the s for the
distances sorry I was trying to reach
myself so I can do it from here k to the
s rate in these distances to a constant
with probability 1 so this is one result
that shows i think the advantages of
random reshuffling in terms of
convergence rate with respect to average
stochastic gradient descent so this note
that this s is between one half and one
this is in distances so it translates
into a rate of one over K to the 2's so
close to 1 over K squared actually I
will show you we can also get something
on the order of 1 over K square in a
minute in function values with
probability 1 and another thing i would
like to also highlight here is the
constant over there theta star you
recall all the deterministic results the
constant that I have highlighted depends
on n the number of component functions
here you can actually immediately write
down the status star an upper bound on
the norm of the data star to be L times
G okay so there's really no additional
em on this constant and before I go into
the analysis I just want to just show
you serve even and simple simulations
the difference in this convergence rate
is striking to see so let's just focus
on the right panel here these are
histograms for the approximation error
in distances red one is randomly
shuffling blue is SGD and you can show
actually within a number of iterations
which is not too high that you can see
this nice concentrations in a random
reshuffling and this convergence in
distribution which is sort of
asymptotically normal for SGD so I will
try to highlight the converge how to how
to analyze this convergence behavior but
before that let me
try to sort of through a toy example
illustrate the difference in convergence
mechanisms between the two methods so
Sgt again what it does is samples index
ik uniform independently each iteration
K and here basically I wrote it also in
terms of an error but not just not the
cycle error because for SGD you're just
sampling iid so thinking things in terms
of it the iteration error gradient error
is simpler you can also do the FICO
comparison in terms of cycle gradient
there it just becomes more messy so
let's focus for sgv on the iteration
gradient error and let's focus on the
simplest example of two quadratics
symmetric so one is X plus 1 square X
minus 1 squared sum function basically
has an optimal solution at exactly zero
and the gradients component gradients
differ from the actual gradient full
gradient x plus 1 and minus 1 so if
you're sampling these two functions
which I have one half probability at
each iteration your error is essentially
plus 1 minus 1 with probability one-half
right leads and expected error of 0 so
it has a zero mean error variance has a
unit variance ok so you can also show
that you know many typical analysis does
that the error sequence is a martingale
different sequence so you similar to
poly up to this ski you can use
martingale central limit theorems to
show this convergence in distribution
and the rate of 1 over square root K so
if you go to our are random reshuffling
for the same example so now let me write
things in terms of psycho gradient error
because what are our does is at each
cycle samples these orders either 1 1 2
or 2 1 with probability one-half okay
and if you write down this iteration
other iterations x 1 k 2 x 1 k plus 1
you see that there is an error with
respect to the full gradient step and
that error is actually some random error
it probably won't have it's the
difference of gradients of f 2 at two
different points in retreats and which
probably
half it's the difference of gradients of
F's fun at these internet terms x 2k
minus x1 k so now by gradient Lipschitz
pneus i can actually write down or bound
this error as the difference between x
2k + x1 k and note that even in the
inheritors going from X 1 K 2 X 2 K I
take a gradient step x1 k minus alpha
gradient and therefore the difference is
actually on the order of my step size
alpha king okay so the error here the
cycle error this is I think the most
important probably a feature of the
random reshuffling or a gradient error
psycho gradient error is order alpha K
it's expected value is no longer 0 as
SGD but its variance is order alpha chi
square not i'm using a decaying step
size so this is that i have an error
whose variance is decaying and therefore
i have a better and better direction of
descent which will yield faster
convergence okay so that's sort of one
of the key points to realize when you
want to analyze why is are doing better
than its GT so it has reduced various
but the problem is in this case EK is
not a martingale different sequence
because of the correlations in fact if
you write down you can write down EK or
decompose it in the following form you
can write it as alpha k VK where VK is a
sequence independent / cycles minus step
size times x1 k minus x star so as you
see EK depends on the starting point x1
k which depends on the random order you
picked in the previous iteration so you
get these dependencies in these e case
from cycle to cycle but the thing to
notice here is that I can decompose and
that's to provide the proof involves
essentially I can decompose it as I can
decompose it as alpha k BK VK is an iid
sequence sequence independent / cycles
times my step size minus alpha K times X
1 K minus X star but note that from if
you look at our deterministic analysis
determines the canal
this is valid for all orders meaning as
long as you sample all the functions
once in each cycle that now that's
result also holds true in this case
giving you a rate of alpha K 4 X 1 K
minus X star so that gives me an rate of
all of alpha square so I have an
independent part plus the conservation
which is on the order of alpha K square
and that's what allows me to analyze
this method so I how much time okay so
just the next two slides will be a
little technical I apologize for that
but at the same time I wanted to just
show you the proof or summarize the main
ideas of the proof in a couple bullet
points so I just wanted to say you know
should be able to tell you that we can
actually summarize the main ideas in a
couple of steps so the idea basically is
similar to many of the analysis of these
methods you write down the evolution of
the outer iterates and this is exactly
what I didn't I geo so I how do I go
from X 1 k 2 x 1 k plus 1 i basically
get a gradient step it has some error
times alpha k okay so i will do polyak
report averaging so i will average both
of these both sides of this equation i
will do the proof for quadratics the
general case is very similar so it's
easier to see it on quadratics so
because we have quadratic functions i
can actually write down that gradient as
H star or the hessian times X 1 j minus
x start and when I replace that and take
the average of both sides i get the
second equation okay let me call the
left-hand side of my equation ik this is
this x 1 j minus x 1 j plus 1 times step
size averaged and the right hand side is
just the average of the previous right
hand side in the first equation if i
rearrange what i get is from the right
hand side i get x 1 j minus x star
averaged so it's exactly my averaged
sequence X bar k minus X star
ok now I have an H star which will be an
H star inverse multiplying the right the
rest of the terms so it will be an H
star inverse ik ik put to the other side
and also i have here h star inverse
times basically that age eight ER some
of the EJ's divided by alpha JS x alpha
k bar ok so basically i can write down
the distance of the average literate in
terms of the sum of two terms a star
inverse times the first term is very
immersed times the second term and the
proof goes through showing the following
three results this alpha k bar which is
the average step size this is one over K
to the s averaged you can show that that
has a rate of 1 over K to this ik has a
rate of log K over K this ik is nothing
but basically how the different sequence
some of the different sequence X 1 j
minus x 1 j plus 1 evolves and finally
probably the red part is the most
interesting EJ over alpha J that ratio
converges almost surely this to this day
the star that I have defined effect and
I also put down sort of how these
different rate results follows Loki over
K showing that this ik converges with
trait loci over K this is basically you
extract from this difference x 1 j minus
x star you use the deterministic IG
results on that and use the lots of
beautiful algebra or maybe lots of ugly
algebra depends on who is doing at some
people like it some people don't so lots
of algebra and you can establish this
log K over K the other one which is ej
or alpha j exactly follows from the idea
i highlighted in the example so you get
your psycho gradient error you decompose
it into two terms of a kv k VK is the
sequence independent over odd cycles
alpha plus some perturbation which is
order order of alpha k square and
expected VK you can write down as the
status star by strong law of large
numbers we have that ej over alpha j
which is essentially my vijay it's an
independent sequence if I average it and
use
large numbers it converts almost surely
to its expected value also implying
almost sure convergence of the weighted
version which is the ej or alpha J to
the same theta star so this analysis
basically instead of the central limit
theorem type arguments for SGD uses
allows us to use through this
decomposition a strong law of large
numbers type argument and gets us these
rates and if you look at the sort of
blue rate results or estimates here you
have a forget the low-k one over K here
and one over K to this showing us that
indeed the rate of convergence of this
distances are order 1 over K to the s
okay and the next thing to realize is
the first term which is the term which
governs the rate of decay 1 over K 2
days consist of terms that I can
actually estimate its H star inverse
Hessian at the optimum alpha k bar which
is the average of my step size and the
status star which is all in terms of
gradients and has seen that data start
so if I can actually estimate that and
subtract it that's a bias then subtract
it I can actually get a rate of 1 over k
lo k over k but I can also get rid of
that Loki okay and that's sort of the
other idea that we are proposing here
basically this bias term which is all
related to the property that X star can
be estimated with an error of course
let's say we estimate we fix the number
of cycles that we're going to run our
random reshuffling in the last cycle we
estimate this bias and subtracted that
gives us a rate of 1 over K square in
function values and if you actually
simulate this over random quadratics and
look at the errors in terms of
suboptimal T of the function values you
see an order of magnitude difference
between random reshuffling and the sword
of bias removed version of the algorithm
in the last couple of minutes let me
sort of wrap up with a final idea and
that sort of this aggregating the
gradients idea I want to go back to
deterministic orders deterministic IG
now and ask the question can I
accelerate it but while still
maintaining the savings associated with
this incremental
access to the component functions so
remember the problem with IG so
everything is good in the Far Out region
but oscillations arise in the confusion
region because one component gradient
gets me this way one component gradient
gets me this way so the simple idea very
nice is basically still compute single
component crazy that in an iteration but
keep in memory the last and gradients
that you computed component gradients
and using those in your memory form an
estimate or approximation to the full
gradient step and move along the
approximate full gradient step and this
full gradient step because you're going
to be moving closer and closer to X star
will actually estimate your full
gradient better and better and that's
actually an idea beautiful idea proposed
by blood hero and gouge Minh in 07 and
the method basically takes the following
form so you move your ex always along
the direction of the sum of the
component gradients so this is not exact
cimmyt gradient for the full function
and what you do is at each iteration K
you go compute a component new component
gradient at XK and in that memory you
replace that value with the most
recently computed subgrade more gradient
and keep the rest the same still move
along the Sun ok so that's sort of the
idea so in a blood at all established
converges for this method a linear rate
for quadratic case this sort of idea was
also used recently in this beautiful
paper by narrow and co-authors who
studied a randomized version of this
where the index is selected uniformly at
random leading to the stochastic average
gradient and proved the linear rate for
the algorithm stochastic algorithm
there's another paper which looks at
proximal extensions from last year's
nips this is a long proof uses very much
the stochastic nature of the algorithm
and to our knowledge does not extend to
the deterministic case so we basically
analyze the deterministic version of
this algorithm in a deterministic
component order and
showed that with a constant step size
you can get linear convergence and I
just want to sort of conclude by showing
you a very simple proof of this result
it's actually probably less than a page
and it goes as follows basically you
analyze it as a gradient descent bit
errors again the first relation is
exactly what I've been putting since the
beginning same relation for IG the novel
step is the founding the error gradient
error and what you can do is basically
play with the Lipschitz news and so on
and so forth to bound the gradient there
in terms of distances the last k sorry
2m distances and then you can combine
all these relations pick a Lyapunov
function given by the distance and show
that this distance satisfies this last
relation here and p is less than 1 so
you get a contraction there you can get
a linear rate plus a perturbation so the
nice thing to notice is using a recent
result by phase madhavi and I taken in
Johansson if you have a perturbation
here that depends on your recent finite
history you still get linear convergence
but at the expense of a degraded rate
not with P but the degradation is
basically determined by the size of your
perturbation q as well as the history
duration of your history which is given
by 2m here so that's basically the idea
and with that let me conclude so I try
to give you sort of some ideas rate
results for three different methods
deterministic IG random reshuffling
deterministic iag I think this is a very
fertile research area I especially liked
the fact that it combines ideas from
several different literature's
continuous optimization machine learning
networks control and it has a lot of
applications in large-scale networks and
data processing so with that let me
conclude and thank you for your patience
if you have questions could you make
your ways to the microphone so could you
commented on the difference between
stochastic gradient where replacement
versus the random reshuffling if you had
say a deterministic sequence of
permutations how would that different
from the random reshuffling that's a
great question so I think when I thing
to notice is that sorry thank you for
getting this and I actually it's
intentional there's a hole here that's
why I'm sold so the iteration complexity
one over K to the s is the same for
deterministic order so as long as you
sample all of these components once in
each cycle you get the same iteration
complexity what is different is that
constant in the deterministic out and
deterministic orders you get a constant
which is very bad because it basically
it's an upper bound on all orders right
it accounts for the worst order that you
can actually pick so it has that M
dependence and various kinds of
dependencies where is in the random
reshuffling what you get rid of is
exactly that M so it's actually a nice
constant which can be much smaller than
the constant I provided for the
deterministic orders so very good
question
when working with practical industry
problems for example online marketing
optimization other any practical ways to
figure out where the smoothness
assumptions and boldness actually
applies so that's also a very good
question i think in general my answer is
that if i look at the applications that
i've cited before so even if you don't
have smoothness it's basically only in a
small number of points so i think my
answer to that would be i expect many of
these things to hold in practice also
because in general the problems that do
not have too many non differentiability
but other than that i think if you look
at the analysis it does doesn't assume
any kind of smoothness then you get all
kinds of very bad rate results but I
think in practice I would expect to
observe more of the smooth results that
we see here think okay let's thank oz
you again for presentation
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>