<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Revealing Nuances in Data at Scale | Coder Coacher - Coaching Coders</title><meta content="Revealing Nuances in Data at Scale - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Revealing Nuances in Data at Scale</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aek2L2flurs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright so welcome thanks for being here
and thanks for tuning in also it's my
pleasure today to introduce Jeff reso
tarski Jeff is currently finishing up
his PhD at CMU or he's advised by Nicki
couture he's an HCI person who uses
infovis solutions to solve problems in
social computing and crowdsourcing and
jeff has built some really interesting
systems including what is now known as
data squid and published a lot of great
work at Kai cscw whist H comp and
received a handful of best papers as
well from KY and whist Jeff is familiar
around the lab as he did an internship
with Mary Morris a few years ago and
he's also an MSR fellowship winner and I
know I and others here have cited his
work in our papers more recently he also
received the carnegie mellon student
innovation fellowship so i'm looking
forward to his talk today on what he
learned about helping people make sense
of data and i also thought i'd mention
for those of you online i have the
question asking terminal so feel free to
ask questions and i can say them out
loud awesome okay hi everyone Jeff Reza
tarski so I'm just finishing my PhD now
at Carnegie Mellon and like Jamie said I
kind of work at the intersection of data
visualization social computing and
crowdsourcing and it makes sense to
start my talk I think with data so here
are some interesting data sources that
are pretty darn huge so Wikipedia just
recently passed five million articles in
size mechanical turk as a sample that a
couple weeks ago has over 200,000
different tasks for workers to do
presumably organized by many people and
even at a consumer site like Zillow
we've got over 2.5 million different
data points or in this case houses you
could purchase and the thing about these
massive data stores are we actually are
using these in our everyday life in a
really meaningful way so Wikipedia here
at all like me you incorporate that in
your everyday knowledge it's really a
day goes by that I don't use Wikipedia
Mechanical Turk is used in products that
face us it's used for quality assurance
search translation approval corpus
building all sorts of things in the back
end
and even a site like Zillow is ending up
being used for one of the most important
decisions in a person's life buying a
house is a serious economic impact the
problem is these aren't necessarily
perfect wikipedia has an overwhelming
amount of historical data if you think
of a Wikipedia article sort of like an
iceberg you can see the article on top
but there's gigabytes of historical data
underneath it collaboration by thousands
of people that's hidden from you a
mechanical turk if you're organizing one
of those two hundred thousand different
tasks you get a stream of raw results
back and it's not necessarily obvious
which ones are good which ones are bad
how they all contextualise together and
in even a consumer site like Zillow it's
rare that a person making a decision
exploring data has a perfect criterion
or makes the perfect optimal choice it's
always an involved exploratory process
among a lot of data so the common thread
among all these different domains is
that there's really no one-size-fits-all
solution here we need people to
understand the data understand context
be able to make an informed evaluation
of history or which purchase or which
house to buy or which mechanical turk
task is the good one but there's no real
easy way to do this and so what I point
to in my line of work is the idea that
we need to support data exploration we
need to help people not only find the
thing that matches their expectations
but in fact understand what their
expectations are in the first place and
so my core guiding principle in my work
is really how do we design systems that
help users see and use context and
complex data and this idea of context is
really important if you think about when
you're making a decision are trying to
make sense of data you can't just come
into the data source and know
immediately what it is you need to see
and why you need to see a bunch of
examples a bunch of counter examples to
build up a model to build up an
understanding I mean so in my work I
focus on a number of different domains
identifying how we can surface context
to help people complete complex tasks
and make sense of data this might mean
that people can perform more quickly or
more effectively then maybe it would
explore more features in the data at
once or their findings might be better
they may have more satisfaction after
exploring some data
at this time I want to draw a
distinction here between directed and
exploratory tasks so what I call
directed data tasks or ones where you
kind of already know what you're looking
for if you think about Google search you
have some search terms it is well able
to fulfill those requirements if you
have discrete criteria systems right now
we're very effective at giving you
exactly what it is you're looking for
whereas exploratory tasks are much
harder to afford now if you think about
trying to explore it involves building a
mental model you don't come in with the
perfect representation you build that
representation from the ground up and
only then once you've built a model can
you generate insights which are
different from a perfect decision
matching criteria and your decision
themselves are kind of integrative
processes it's not this matches undone
to link this a little bit to literature
on we can consider what cognitive
science is done with sense making so
since making is the process of
constructing meaning from information
encoded in data and like I think
describes us really nicely as an
iterative process some you're developing
a mental model but you don't come into
the data with it immediately you
iteratively build it up over time and
over exploring proliant card have a
really evocative term which is foraging
you're searching around for examples
you're searching for necessary data to
build up an understanding I min per inch
nedderman at a really nice complication
to this which is all of us in this room
are familiar with using statistical
tools can a specific analytics but what
they found is that when you're making
sense of data very often you have to
start with the broad exploration before
you can even apply those statistical
tools so you can imagine a narrowing
where you start with an exploration
where you need to make sense of data
iteratively and only after a while can
you actually apply statistical
techniques to illustrate this even
further let's look at a sense making
model so this is Russell a tallest model
which i think is pretty effective in
conveying what since makings iteration
is all about imagine you have some data
and you come into this task with a
little bit of understanding about the
data you know I maybe have a task that's
telling me what to look for already in
the green box you start searching the
data for good representations in other
words you're trying to find data that
match what it is um you think the data
is all about so if you have an example
if you're looking for a house to buy you
may already have some existing criteria
that you can find houses that Matt
that ideal as you're trying to make
sense of what's out there in the blue
box you take those examples as
representations and you encode them into
your mental model so in this box what
you're really doing is taking those
examples and asking yourself do they fit
in my model how do they fit do these
match my understanding well and if so
that's great but if not you end up with
residue that red arrow so as you explore
data you're not going to find every
single example fits your mental model in
fact some things don't fit your mental
model and as you explore more and more
data you build up this residue stuff
that doesn't fit eventually you hit a
crisis point where you have to adjust
you have to change your thinking about
the data so that you can accommodate
those residue and so if you think about
this process more holistically we have
iteration we were trying to find
examples you try to find counter
examples that are signals that you need
to switch your understanding and over
time you accumulate less residue because
you're a model is getting better and
better this points to a couple of
different ways that we can improve sense
making for data explorers in particular
we could help them find those
representations better direct them to
ones that either confirm or disprove
their hypothesis really quickly
similarly we could also improve the
iteration of the process so how much
data can you cover in a given a loop of
this process this can help people make
kind of better explorations and develop
better mental models but it's even more
complicated than that if you look at
decision making with data people rarely
get the perfect optimal choice in fact
most everyday decisions are made without
a full examination of all the available
options the best option may be missed at
Carnegie Mellon land we call this
satisficing the idea that you make a
decision with the best possible
constraints you have maybe you just
don't have time to find the perfect one
complicating this even further is the
idea that we have physiological
limitations on our data sense making
process so working memory is only seven
ish units in size you cannot store a
huge amount of data in your head as
you're performing a task we have limited
attentional resources so you can't focus
on too many different targets azhar
exploring data without getting
overwhelmed your performance degrading
and even things like feelings of
self-efficacy expecting that you're
going to do well in a data sets making
task actually impacts your performance
if you feel like you're going to do a
good job of exploring data you actually
do so to operationalize this a little
bit let's look at buying a house in
Pittsburgh if you're going to be buying
a home in Pittsburgh which is a data
analysis task I mean these numbers
actually are reasonable for Pittsburgh
for those who are not necessarily
believing me it's amazing so you often
come into a data task like this with
some existing expectations or
understanding in this case I have
bedrooms baths and a budget but as you
look at some more data you realize there
are some criteria you didn't expect to
see but really do care about so in this
case maybe you realize parking is an
issue in Pittsburgh and you really care
about a nice neighborhood however as you
keep exploring you find you know what a
nice neighborhood is actually pretty
expensive I want to live in a nice place
but it's going to cost me so I have to
readjust some of my existing criteria to
match and as you keep exploring you end
up accumulating a lot of different
criteria which speak to a really deep
understanding of the data but as you can
imagine trying to find something that
matches all these is a really hard
process so this idea is really
characterized by exploration you don't
just find the point you're looking for
you build up a model as you explore by
seeing a bunch of contextual examples
there's this idea of hypothesis testing
you think yourself all right what about
a nice neighborhood what would that look
like well let me experiment and see what
that maybe it's an active iterative
process as I mentioned earlier so
switching gears let's look at how it
goes with an existing interface that
consumers may actually use so this is
zillow's visual tool for identifying
houses you can see each red dot is a
house in Pittsburgh and I can actually
pretty readily using their faceted
browsing tools pick out some criteria I
can encode these directed criteria
really easily the houses will disappear
the ones that still much my criteria
stay there but if i want to ask some
questions of the data like what if i
want more bathrooms what if i want a
bigger place or a smaller place i have
to go through a lot of different
interface steps and then see what's
appearing and disappearing in order to
gain any understanding this process has
a sort of disconnect with what people
are actually doing for known goals that
interface is really good
but for exploration it gives you really
hard feedback I'm either points are
there or they're not you don't know why
they disappear why they appear based on
your filters interactions to explore and
test different values involve a lot of
different steps and so what I point
through my worker ways that we can
surface context in a really fluid
natural way that's relevant to the user
so that we can accommodate exploration
on a deeper decision making and I'm
going to do that in this talk in three
different domains where I focus my work
so to begin let's look at wikipedia i
mentioned earlier that wikipedia pages
are sort of like an iceberg and here's
an example of one page i've actually
contributed this so you may not be able
to tell just from looking at it and if i
asked you who's contributed to this are
there any viewpoints that are
particularly strong on this um what are
the cultural background or the gender of
the people have contributed this page
were there any debates going into it you
wouldn't be able to tell just by looking
at its current state Wikipedia is an
immensely collaborative artifact but the
collaboration is hidden from most
everyday users to get to some of this
collaboration because maybe you're going
to make a contribution and to make a
successful contribution need to know
what's already been tried what issues
may be present that you can't
necessarily see immediately so one thing
we can do is go to the little button in
the upper left corner talk which is
discussion among editors I mean for a
big article this actually poses a
serious barrier so here's the discussion
among editors for Scientology which is
the controversial article this here is
several dozen pages long and you notice
the top archives there are 30 different
archives each potentially dozens of
pages long all containing discussion
among editors so if I asked you now what
are people talking about in Scientology
what should we do or not do based on
discussions in the past you wouldn't
really be able to say that much maybe
you could use the search box to search
for the word cult because you think
maybe there's a debate about that in the
past but it's not necessarily clear what
you should glean from this data store
instead maybe let's look at the past
revisions the past things that have
changed in the page I'm an actuality
however for an article like the article
abortion um just the diffs of changes
authors have made over time is roughly
20 copies worth of Pride and Prejudice
in length so we can't exactly expect you
to
into this content either so you can see
this sort of difference between
exploration and directed you could
search the discussion pages you could
search the revision for a specific term
but if you wanted to gain a general
understanding you really have no ability
to parse through this data it gets even
worse so we conducted interviews with
three expert Wikipedians in the
Pittsburgh area and one of the things
they pointed to immediately was this
idea of conflict people are fighting in
Wikipedia they have xela tree about
certain topics and for newcomers
especially this can become a serious
issue if you wade into a conflict zone
unexpectedly your work is going to be
thrown away perhaps in a hostile manner
you'll never come back one of our
experts in fact has received sort of a
Wikipedia version of a no-contact order
because after a battle in one particular
Wikipedia article they were stalked by
another editor the more interesting part
coming out of these interviews is about
information overload so we asked all
three of these wikipedians what do you
need to do to make a successful
contribution and they all said to us
well once you have in the region of the
page or interested in you want to check
the discussion and check past edits to
see what's happened see what sorts of
discussions are having their who the
stakeholders are if there's any conflict
we then ask them go ahead and make an
edit for us and we gave them a couple
editing tasks they did not use any of
those resources immediately after
telling us history is important they did
not use historical resources at all we
asked them why was there this sort of
disconnect why didn't they use them and
they said there's just too much I'm
never gonna be able to find out anything
and attractable on a lot of time so I'm
just going to try out and see and hope
for the best um so we have an
opportunity to do better here but if you
look at existing interfaces we're still
existing kind of on the high level
rather than digging into the actual
substance of the conversations and
activity so wiki dashboard in the upper
left shows you kind of temporal
relationships of different authors who
is contributing right now and how often
history flow right here shows you the
evolution of the page in graphical form
but it's hard to get down to the what
changes are people actually making and
why level and the lower left corner you
see snuggle here which is a tool for
administrators to socialize and interact
with new editors
situated within the contributions these
new editors have recently made but
snuggle actually was appropriated as a
tool to target new users for being kind
of two new too engaged and kind of too
inexperienced so a lot of the
administrators who saw these new edits
in fact we're throwing them away saying
nope this isn't ready yet you need to do
more because they didn't understand
necessarily what the edits were actually
doing in context so with this problem
kind of in mind we took all of
discussions on Wikipedia I'm at a
particular article and use topic
modeling to situate them within a small
section of each Wikipedia article so the
idea is as you're browsing Wikipedia now
with this discussion lens tool within
each section you can start to see
important discussions for that section
that led to its evolution so for example
if you are on a particularly contentious
article which is the article I'd hummus
the food turns out is a very conflicted
article on wikipedia when you're
browsing the section on etymology it's
important to know about this discussion
right here these are people debating
whether the oxford english dictionary is
an authoritative source for the origin
of the word hummus is it a turkish word
is it a an arabic word it turns out that
if you invoke that it's being a turkish
word these authors are going to strike
you down and we see that throughout the
history of this article um so now if you
are reading the article on hummus you
can imagine seeing that as one of the
recommended related discussions you'll
be aware that this is an issue that you
may not want to wait in so we're cutting
down the complexity here by surfacing
relevant contextual discussions within a
much smaller section of the page and in
practice this does make a difference so
we asked participants in a lab study on
between subjects either the Wikipedia
interface or our interface and blue to
write a guide to an article section and
the idea was if you were talking to your
friend and telling them what they should
contribute to this new section what are
some openings what are the stakeholders
I'm are there are some issues you should
avoid or consider for a small article
like the article on hummus our tool
really isn't reducing the complexity any
people are just as able to write decent
quality guides based on history but for
a large article like the article on
Ellen
tering the tool by crunching down and
providing just contextually relevant
information and given section actually
does provide users a much better picture
of historical data this is on the
readers side I'm just now for CCW we're
working on patching into this what the
editor side looks like so based on this
feedback do you actually make a better
contribution to Wikipedia knowing this
extra information this question we're
looking at right now so i talked about
discussion but what can we learn from
past contributions themselves and so
I've also done some work modeling past
contributions to Wikipedia and a lot of
times if you look at this is a stream of
different contributions to the article
on Scientology you'll see comments like
this undid revision by person the idea
is a lot of times in Wikipedia work is
just thrown away wholesale I'm either
because it violates norms as part of a
conflict or isn't wanted by a particular
community surrounding an article and so
he constructed n-gram models to try and
get an understanding of what sorts of
content or value are not valued by
editors using machine learning to cut
through the complexity of this large
historical store so for a simple change
like this changing jumps over to walks
near we can construct a feature vector
that captures the changes they made and
considers whether or not that edit was
accepted by the community or rejected in
other words reverted in Wikipedia
parlance and if we do this over 150
different articles it turns out that we
can actually pretty accurately predict
whether or not contributions are likely
to be thrown away by the community just
by the words that they're choosing to
change so the idea here is not that we
should just tell a person nope yours is
going to be thrown out yours is rather
that we can use this model to gain an
understanding of what things are
particularly risky to do to a Wikipedia
article so this is model waits for the
article on genetic engineering and so
you can see at the top dude is
definitely something you should not
contribute to the Wikipedia article in
genetic engineering surprise what may be
more surprising is shouldn't it turns
out shouldn't is a prescriptive term and
wikipedias neutral tone does not allow
that sort of language by policy the
header you see their genomic engineer
there's a debate in the article about
whether this header should be included
and so our model picked up on the fact
that that was a conflicted area of
contribution whereas of course Monsanto
is a less risky term interestingly
exceedingly and involves depending upon
context could either violate Wikipedia
policy or not so the idea here is that
this model is capturing some really
interesting features within the textual
data store and so the future i think is
a really interesting possibility in the
first line of work I was looking at how
to collapse different discussion to
editors while they edited in a
particular article context and you can
imagine making this into a sort of
recommender system if we know you're
editing a particular section we also
know that changes you're making how do
we present relevant data to you that's
actually going to change the kind of
contribution you make how can we be
prescriptive I'm going to say to people
you're noticed you're adding Turkish to
this particular section that's no good
here's some examples for why and here's
some discussion that's relevant to
actually improve their quality of work
similarly we can use history to direct
people to new interesting areas to
contribute if we notice that we don't
have much recommendations for a certain
section there isn't much activity maybe
that's an easy entry point for a
legitimate peripheral participation as
newcomers socialize and perhaps most
interesting to me right now is being
able to construct ethic using guides
dynamically based on historical data so
if we know what's going on in a certain
article section can we construct a guide
for that section just from the kinds of
comments people are making as they
change work in that section and the
kinds of discussions that are happening
go ahead Mary wondering
so I like these ideas but I also see how
you know pretty good let's say with
Wikipedia right where there is a certain
pervasive culture among among the
editors which is very exclusionary and
presents certain nights of you and these
you know tools and ideas you have might
help someone to fit into that culture or
better an honoree within it but doesn't
maybe adjust the larger question of how
to affect change beyond you have any
thoughts and so I think this is actually
really where um I think we can effect
change so this first one does run the
risk of kind of only enhancing the
orthodoxy of Wikipedia because we're
telling people avoid this it's dangerous
it doesn't change the dynamic at all um
this work draws on work on Wikipedia
socialization which generally says that
newcomers tend to go to Wikipedia
articles make one contribution get
thrown away and never come back the
barrier for wikipedians is in fact these
early edits I mean once they've
socialized a bit more they can handle
wading into riskier areas and taking
stronger viewpoints and so one core
possibility for kind of investigating
historical data is to provide better
entry points so you know these may be
lower risk entry points in terms of an
article section that hasn't been
contributed to that much but this gives
people hooks to begin effecting change
and can help bring in more diverse
audiences in particular right now
Wikipedia is suffering from a gender and
culture problem it's predominantly white
males who are contributing and if we can
provide better entry points that are
less hostile for a variety of different
contributors maybe we can start to
change the culture in that matter yeah
going to sort of a related question so
that yeah the statistics are sort of
pulling out you know a space that
reflects the culture the people that
contributed that and it's all temporal
of course too yeah so my question is
it's sort of specifically about the
system that did the topic modeling is
there a way to kind of wait the
importance of the operates do both
and so if yep I've got it here CC right
yeah so there's a star in every corner
something we're playing with this we
prototype this out to finish it up for
CCW is collaborative filtering I'm so
the idea is that if people actually find
this information valuable we can also
use that to operate it and you can
imagine kind of constructing more meta
information out of this so maybe as
people read discussions or as people
close discussions we can construct
summary information um that's more
condensed and more relevant I didn't
mend yeah savvis then people can go and
kind of in a post hoc way um also we
haven't I haven't really even talked
about temporal issues which are also an
interesting question do things decay
over time or stay consistent and scale
Wikipedia pages singularly like I've
been discussing now or all of the Kapiti
as one model so this is one particular
final vision this is a prototype we're
thinking about right now you can see
we're giving people real-time editor
feedback and the left bar and true
sively they get more information about
what they may or may not want to add so
that's thinking about context in
Wikipedia as the historical context
hidden beneath a page how can we expose
that to people in a tractable way so
they can make sense of data and i'm
using ml and vis to help get us to that
point switching gears to crowdsourcing I
think everyone in this room is pretty
familiar with existing crowdsourcing
marketplaces including Mechanical Turk
we just more micro and up work which is
more contractor organized in short
crowdsourcing workflows kind of follow
this pattern imagine I've got a big
corpus of images in this case adorable
puppies and I want to tag each of these
um I could one by one go through each
animal and tag it or I could give each
picture to a single person in parallel
and had them all do it this holds a
really interesting possibility for
getting a lot of human judgments really
quickly and scale ibly the challenge is
not all results that you get are good
especially when economic motivations in
like in the case of Mechanical Turk
start to come into play when people are
extrinsically motivated they may try and
find ways to gain the system so I asked
people to tag that image and you can
imagine getting really eager answers
some answers and we asked them for three
to five tags they gave us three
or no tags at all hoping we won't notice
that they didn't give us any work this
is good because these people then are
making the most possible hourly wage for
the least amount of effort I mean hoping
we won't notice some mechanical turk
work is called this cheating the idea
that they're cheating you out of
particular value by not contributing and
if you look at Michael Bernstein back in
2010 the fine fixed verify paper they
pegged it at thirty percent of
submissions or of that so low quality
you can't even use them these days ten
to thirty percent is about the rule of
thumb you should use so you think then
we've got to figure out which one would
based on each submission you got is it
this this or this and if you look at the
existing Mechanical Turk interface this
is what you find I asked them to help
name my company I'm so each row here is
a list of company ideas brainstormed by
a mechanical turk ER you notice I can
get the names they've given me so the
raw data they gave their approval rate
in other words have I kind of reputation
system wise approve them before and if I
dig really deep into this interface I
can also see whether they worked for a
long time or a short time but that
number is known to be incredibly
unreliable um so how do you find the
good work existing research has looked
at this kind of two different lenses one
is designed by their tasks so people
have to give you good work which is
really hard um this is usually involves
a lot of iteration and a lot of
incentive design you can also in a post
hoc way analyze what you got and try and
find the good stuff I'm so one way to do
that is by seeding your task with gold
standard questions so the idea is if you
already know the answer to some
questions you can put them into your
task and see which workers get all of
those right now because then obviously
you trust their results more if I asked
you though what's the gold standard
example for a poem what sort of
restaurant of you would tell you whether
they're good or bad worker complex work
this all kind of falls apart it's hard
to understand more creative or more
varied inputs also I might add that
workers are known to gain this crowd
flower in particular has been known to
have pools of workers who learn the gold
standard formula and only answer those
properly you could also have multiple
workers redundant Lee do the same task
for instance if you're transcribing a
video you can just pick the most common
answer or most common substrings
to get a decent transcription of course
there's no most common short story and
having six people do the task of one
person adds a lot of redundancy and your
system and can crunch down a lot of the
diversity that really human judgment is
valued for so in this line of work I
propose a really different signal for
evaluating the quality of work in a
crowdsourcing workflow and even just
understanding in general how a
crowdsourcing workflow is going and that
is thinking about the middle between
designing a task and getting your
results the way workers work can tell
you a lot about their not only their
performance but the task and workflow in
general and so to give you an example of
what that looks like let's consider two
workers taking an acct practice test so
you read a passage can reply to some
multiple-choice questions worker a
accept the task Scrolls down clicks an
answer clicks an answer is done worker
bee accepts the task pauses here Scrolls
down pauses here scrolls up pause this
here puts an answer pauses clicks an
answer is done now to ask you not even
knowing what the answer to this multiple
choice questions are which worker did a
better job you'd probably say B it's not
a super hard question unlike these
questions right here which were pretty
difficult to see brouhaha there that's
pretty tough we associate worker bees
behavior with diligence right those
delays were them checking the passage
our knowledge of the task actually
informs a lot about on their end
performance so we constructed a model
that measured workers work on behavior
while they worked using clickstream data
so here's a worker typing submitting
hello really low level events we had a
bunch of workers complete different
tasks and from those low level event
strings we extracted a bunch of general
features that were more comparable
across workers and more quantitative so
things like how long they spent how much
did they pause to think while they were
typing those sorts of behavioral
features we had workers do three
different kinds of tasks pick the nouns
tag an image or that practice test you
saw earlier I'm going to practice just
looking at the way people work
can really inform give us information
about their end product so calling out
image tagging we had two Raiders rate
whether they thought the person was
cheating us or in other words giving us
bad results intentionally and our model
just looking at behavior got ninety
three percent accuracy in terms of
whether a person was cheating or not
based solely on behavior and we had work
to Raiders also rate quality in a
five-point likert scale for those tags
our model just looking at behavior can
get within about point five and a
five-point likert scale of human ratings
just looking at behavior not even
considering the end tags so this is
really interesting on this business idea
that behavior is a really valuable
signal for understanding quality of work
but it also neglected to consider a lot
of really interesting features so right
now we're crunching behavior down into
just a simple outcome measure pass/fail
or reading what about individual
variability what about different ways of
working or different cognitive
strategies I'm so on building on this
work and the second paper I constructed
a visual metaphor for these sorts of
traces of activity so here the blue tick
marks or people clicking on something
the orange lines are people scrolling up
and down the page the red boxes are
people typing you can see this person
paused in the middle of typing and black
lines what you won't see here changing
focus or tabbing to a new tab they do
here is now we can actually go a bit
deeper than just good or bad so I'm
going to say you know who did the poor
work but now why you probably would say
a these are people doing the ACG
practice question but a didn't do the
poor work because they had a shorter
trace in fact time spent on task is
usually a poor indicator performance
instead what you see in B and C or this
upside-down V pattern these are people
who were reading the question then
checking the passage for the appropriate
answer than scrolling back down again to
pick their answer we're now actually
getting to their behavior in other words
this visual metaphor lets us understand
not only did they do good work but it
start to explain theoretically why that
maybe we can find out some really
interesting things by looking at work or
behavior we ask them to tell us their
favorite color I'm gonna use a color
picker to pick it and describe it to us
there's a perfect near perfect
correlation between the delay they spent
before they told us the color name and
length of the color they gave us because
they were picking the perfect shade in
the color picker more operationally we
asked people to translate from Japanese
to English a particular passage and one
of these is not like the others only one
of them has red blocks indicating typing
so only one of our workers and what I
believe was a 10 or 20 worker pool
actually did any typing while they were
completing this translation task
everyone else used a translation service
and so the most common answer to this
translation passage is this one which if
you can read Japanese does not match all
that closely and if you could read
English you can tell is not terribly
sensical this is the worker in green the
one who stood out in the behavioral
traces um it turns out they actually use
machine translation as well but they
took effort to proofread and correct the
machine translation before they gave it
to us so this still isn't a perfect
result but we were able to pick the best
result possible which was not the most
common one these timelines were part of
a much bigger interface lets you try and
get laid down on the relationship
between the output workers gave you a
number of different representations
their behavior and some quantitative
features valuing their time how much
dumb thinking they were doing things
like that the idea here is that you can
pick a few interesting behavioral traces
out and use distance and ranking
measures that use small ml algorithms to
pick out more people like them so you
can iteratively build an understanding
of different kinds of strategies or
different kind of working habits among
your workers yeah very cool Richard
sense on how this would generalize like
for example if you provided like an API
people gonna see me but people get just
fired data at you with from like any
domain yep and just say look here's
operationalize by me those different
metrics on some of your writing you
menya you're doing whatever tasks mm-hmm
like I don't know what your sins may be
awkward or also a we don't know and this
is something really interesting to me is
kind of getting beyond just performance
to thinking about you know are they ESL
do they have domain expertise are they
checking their email well or not things
beyond just this pure labor pool
performance um
the initial ml work we had really good
result sporting the models so we could
take a model from one place like
nonotification actually accurately
predict whether a person would pass that
reading comprehension test so it points
to maybe they're being archetypes for
different kinds of tasks but a lot left
needs to be done you can think of
similar behavioral or interaction
patterns between multiple classes of
tasks it's almost certainly task
directed as well as application directed
so there's probably some interaction of
the tool that I really like to
investigate further but in the lens past
just pure quality additionally you can
imagine giving this right back to the
contractors working so can we tell
contractors you know we noticed you had
the skill maybe you could find tasks
that are more aligned with this deliver
more value or maybe we notice you're
getting fatigued why don't you try
something fun here's some suggestions so
giving power back to the contractors
yourself awareness similarly we can
actually give organizers a much better
picture of what's happening so in that
acct practice tests you can imagine
telling organizers hey I noticed that
the good people were checking their
answers could you make your task design
such that people check their answers by
design there's a really nice fruitful
cycle where you learn from the different
strategies workers are employing on the
fly as you develop better and better
versions of tasks so context and
crowdsourcing markets really means
discovering human behavior as people
complete a task and surfacing that to
task organizers I'm so they can make a
better adjudication about the
performance or nature of their working
pool so once again is giving them kind
of the information necessary to
understand and then act in the last
portion of the talk on the briefly touch
upon a more general approach which is
helping people see more context in their
own multivariate data so multivariate
data something I think we're all pretty
common or have a good amount experience
with each row here is a brand of cereal
each column is kind of nutrition
information in the back and you can see
if I asked you find a correlation
between carbohydrates and sugar you may
intuitively know there is one but going
number by number may be too difficult if
you're just starting to look so to get a
better visual understanding of
multivariate data researchers have
gone the direction of visualization
techniques so here's an early example
film finder which charts out films on
kind of a temporal and a rating axis the
Nate innovation here is that you may not
be able to see you may not want to see
every single film on that chart you may
only have certain interest areas and so
we can use dynamic querying these
sliders over here to filter down what
you're looking for and the stuff
accordingly pops in or out of the screen
what if you want to see more than three
dimensions at once without scrubbing
that slider to see week instead staff
charts so now we have three different
dimensions of data showing in these
stacked charts and we can use brushing
to help kind of zero in because the
attentional load for trying to find
certain regions is quite high if these
weren't colored it'd be hard to figure
out where the clusters lie at least
certainly the green in the orange so the
attentional load is high we can also use
really advanced visualization techniques
like parallel coordinates which are
really effective at seeing trends where
values change abruptly so each point is
a row now and it crosses these vertical
lines on its values so you can see that
the orange tend to go all down the blue
tend to go all up but if you were
untrained this can be pretty
overwhelming especially if you were an
experienced analyst and if these weren't
colored when you necessarily be able to
see it through all the noise so the core
issues I'm identifying in these sorts of
approaches which I'm pointing to his
limitations on these approaches
certainly are incredibly valid and work
well I've use them hard constraints can
make it hard to track values as they
change over time so as I move those
sliders and film finder stuff appears
and disappears training can be a serious
issue and also they can be high load the
stacked plots are really hard to
interpret at times so interestingly a
really wonderful thing has come up in
the past decade where touch devices have
become not only common but incredibly
used by everyday people you know
everyone owns a tablet or smartphone in
America these days they're just a high
proportion which is a shocker and these
devices have a really nice property they
bind a really closely interaction with
response they occupy the physical space
of a person and they also afford a
really interesting potential in terms of
naturalistic visualization systems so
I'm defining these assistants which
employ interactive
jewell affordances that resemble real
world phenomena and the idea is we can
use touch and these natural feeling
systems to get really close to users um
we're leveraging their inherent
expertise so if I know if I drop this
gravity will pull it to the table and
even further this is a fluid thing so it
doesn't just pop from here to here it
actually fluidly transitions all the way
down as part of its fall these sorts of
interfaces encourage a lot of
experimentation in play if you think
about using a tablet it's generally a
playful experience um and so in this
line of work in the Connecticut project
asked the question what if we use
physics based approaches to help people
explore multivariate data leveraging
this idea of fluidity and little
training or expertise because people
already know the models and using
physics metaphors as applied to actual
data processing so to give an example
what I mean by a physics metaphor here's
one this is a kitchen sieve this is
actually a really great filter for data
so not only do you see the particles
that pass through the filter kind of the
small cornmeal you also see the stuff
that didn't make it and you encode in
the process of shaking this filter out
the active filtering it has really nice
properties in terms of mount on either
end and the action so you can see here
in this video I'm doing the same thing
to data now some things pass through
other things don't and I see both ends
of the filter and I recall it and encode
it because it's an action I take we can
use magnetism difficult points to charts
and we can emergently combine different
physics based tools together to get
really complex data interactions so here
we're filtering out some points charting
them and then highlighting sudden that
match a criterion to understand where
and how these particular physics-based
vis approaches are good we conducted a
small between-subjects user study
comparing Excel to this new approach
participants first received training in
either case some we thought these
conditions balanced out these
participants tend to have more Excel
experience um but Excel is comparably
harder to use so we thought there was
some leveling going on because
Connecticut's trading costs was much
lower once participants were trained
they were given some basic stats
questions to make sure they understood
the tech
knology then completed two different
tasks the first was here's some data
find the perfect car for you here are
some example criteria to go on um the
second task was a set of people for the
Titanic when it crashed we gave use as
an open-ended exploration task find out
as much as you can in this data set
here's an example of what one
participant found in the car buying task
we asked them you know what are you
looking for in a car and they
immediately pushed all the points out
that were hatchbacks apparently they
really did not like hatchbacks but you
can still see them here they graph by
weight because they have hypothesis that
heavier cars do better in the winter and
you can see they encoded this
three-dimensional sort to capture the
distribution of power reverses fuel
economy and filtering out based on their
budget interestingly the participant did
not just go and say you know this is the
optimal one it has the best mileage or
that Porsche up there is the best one
because it's the most powerful they
gauge the kind of bow genus of this
distribution um and said you know I
really want something more like that
because that points in the middle of the
road and a lot of different features i
care about it spoke to their deeper
understanding as opposed to just
matching the criteria optimally and
perfectly that they started with here's
an example of a participant in the
Titanic condition um and they're
actually doing a four dimensional query
here so points are being pulled to a
particular place in the chart in this
case we have cabin class and gender of
passenger they're colored by survival so
the people who died or read the people
who lived or believe they already noted
that more women survived than men in
sorry this is kind of a Cobb example but
they were interested in what about the
children on the boat and so they drew a
barrier that excluded the children from
this set pushed them out but because
they still a field feel uphold their
place in the chart this is kind of
consistent physics metaphor we get
clusters so you can see in the lower
left corner there's a solo red dot it's
the only girl in this data set to die
and similarly there's a solar red dot
among the women in first class this is a
mother and daughter that the person was
able to find because they're outliers on
this four-dimensional split they
wouldn't have otherwise noticed it I
might add a lot of our participants this
was kind of a more environmentally by a
sample so we did not
have a lot of college students a bunch
of our participants had never even
picked up a tablet before and we're
still able to do this sort of task very
quickly looking at participants findings
in general Excel users excelled at these
two types of findings we coated them
with two different Raiders with pretty
high reliability point findings are this
particular data point is aged 40
statistical findings the average age is
50.1 whereas kinetica users were much
more able to do descriptive things so
comparing you know more women survive
than men there's a relationship between
age and survival older people tended to
die more often in the data set I mean
descriptive things like there just
seemed to be more men than women in this
data set this speaks to a more holistic
and general understanding while they
could not necessarily get down to
quantitative features going all the way
back to the old / Aaron Schneiderman
paper I mentioned about broad
exploration moving into statistical
tools you can imagine this being a sort
of wayfinding where you identify
interesting areas to future to further
interrogate using quantitative means
since kinetica I've commercialize the
technology as data squid which has been
really great because it's allowed me
access to beta stakeholders people who
actually use data in their everyday
lives and I can go into detail with this
some more with you later but this also
led to a redesign of interactions so
this is what data squid looks like now
and you can see we've focused in on
giving people applaud at all times
because we realize the core benefits is
providing to people was in terms of
varying different representations
context in the case of Connecticut /data
squid means giving people as many
different views of the same data as
possible so they can build a better
model and notice more interesting trends
and doing this in a way that forces them
to see statistical features like
distribution you can see how these bulge
in different ways and have different
centers without a box-and-whisker plot
in the future I think there's some
really interesting possibilities you
know how do we show a million points in
a small screen in a way that's sensible
to inexperienced users and how do we
represent the fact that if we're
clustering a hundred thousand points
together that there's a stochastic
quality there's an uncertain quality to
those points now there is no perfect
average for there's a hundred thousand
points there's more
detail how do we devote detail on the
screen devote more pixels two parts that
actually matter and high detail where we
know the user may not be interested
additionally i think presentation and
sharing is really crucial in this sort
of information visualization approach
all of our stakeholders for data squid
want to share this with others
immediately and I think this is really
interesting potential how do we help
people curate data presentations in a
meaningful way while they explore or
right after they explore what sorts of
people what sorts of things are people
choosing to present what aren't they
presenting and what's the best medium to
share do we demand our users do this
live in person and speak it aloud can we
generate static visualizations that we
pass on to other people after the fat
what modality works best for conveying
information and of course what does
physics look like for correlation what
sorts of complex analyses have metaphors
instead of the easy to use physical
world and what do those look like so
context in this case means context about
your own data how do we help give you
different representations of a data set
such that you can find interesting
features and build an understanding to
direct your decision making in
particular we find that this data squid
tool is really good for Yelp data
helping people pick their favorite
restaurant because of this over constr
endure constrained I don't even know
what I'm looking for quality one theme
that's emerging out of all my work
moving forward is this idea of use how
do we move from seeing to employing or
using and acting so I propose this
creates a really virtuous cycle we work
with stakeholders to understand data and
develop new data visualization solutions
we can improve the kinds of
contributions in Wikipedia or the kinds
of findings people make with data which
in turn gives us better data to generate
new systems there's a really virtuous
cycle inherent in this process in
Wikipedia you can imagine being
prescriptive telling people you know
here's interesting area to contribute
here are some things to consider if you
want to contribute up there if you're
breaking norms these the people you
should probably talk to before you do we
can do this in more communities such as
web forums you can imagine capturing
what is a flame war and modeling that on
open source projects we can capture what
makes a good issue request who's
contributing to a certain part of the
code and what do you need to know
ordinary contribution there and
crowdsourcing you can imagine moving
towards being prescriptive to task
organizers here's how you should read us
on your task here are some stakeholders
in the crowdsourcing market that'd be
really good to talk to or have domain
expertise that would be really well
suited for your particular project and
we can expand this to contractor or
creative type markets identifying
expertise becomes a really critical
concern as the work gets bigger I mean
understanding the kind of work in the
process of work becomes increasingly
critical and the multivariate models as
I mentioned before kind of scaling up
and thinking about presentation is
important but also what does a
physics-based visualization to look like
for graph data how can we extend this
sort of approach towards naturalism to
new data types and keep people rigorous
so they avoid kind of the problem of
seeing somebody t-tests one is always
true how do we make sure they have an
adequate understanding even if they're
an experienced user of statistical
reliability so with that I'd like to
thank you all very much for your time
and thanks all for hosting me love any
questions yeah Mary so the beginning of
you talk you had an example over zillow
as being in a really hard she's forcing
punch things I was just wondering if you
had had any thoughts on that in
particular in terms of what this
solution might prefer for that kind of
pixs so we're actually running between
subject study right now evaluating kind
of a zillow type home buying task
between a traditional in face like that
something more involved like tableau and
the kinetica prototype the idea here is
that if you don't necessarily know what
features you're looking for because data
squid / kinetica shows you a bunch of
different representations really easily
you can help triangulate on break points
in the data maybe neighborhood actually
is a feature I care about because it
really cleanly breaks the data into one
and don't want another area we're
looking with the Zillow task annotation
right if you're doing a lot of different
analyses steps and different
representations giving you an ability to
kind of carry through
information from each of those different
representations is important so maybe I
tag things that are cheap and in a nice
neighborhood with rat a red color and
then go through and say you know well
these are decent parking areas John the
geographic view tagged as the blue color
and at the end be able to kind of
collapse your information down um I
think in general the only way we're
actually can be able to zero in on what
makes a proper kind of Zillow or
customer decision-making tool work well
is by drowning a bunch of a B and
investing exploratory studies like what
I'm doing now no trying to get at piece
by piece part of the data I part of the
data where is the benefit coming from
and even in kinetica interaction by
interaction why is the tool kind of
performing better than existing ones
yeah a little bit in there about scaling
up the size of the data are there limits
in your mind or in it so what are they
young for both the sort of physical
interaction analog like in Connecticut
but also that kind of cognitive analog
first off you're saying you know we
could pull out of top bags and thank the
sort of math to people's McConnell's
with the millions or billions of data
points complete breakdown and so
something that's kind of lurking behind
the scenes and the Connecticut slides I
showed is that data are not often
perfect or even good quality data are
noisy data come from various sources
need to be brought together and that's
problem only magnifies larger in size
you go so I think one way to start
tackling that problem is to think about
kind of machine learning aggregation how
do we aggregate points into collections
in a way that makes sense one early
prototype I did while I da ting about
this for kinetica was to apply a
hierarchical clustering based on data
features to the points so the idea is
that we can bring all the points
together or we can selectively break
them apart if we know a user's
interested in a certain set of points
the challenge in all these approaches i
think is maintaining focus and context
how do we show the user a lot of detail
about what they care about but still
represent the other stuff in a faithful
way that leads them to kind of
contextualise what they focused on and
that's where i think we may hit limits
so you know if you have a million points
in a screen the focal area may only be
10 points
and there may be nine hundred and
ninety-nine thousand points being
connived condensed in some way we may
not be able to condense those in a
meaningful way especially if the data
are noisy and so another way to take the
work is to also think about other
modalities like natural language
querying how do we help people
interrogate the data not just through
this visual medium but also through you
know querying it properly in their own
language reciting results back when it
makes more sense to be quantitative than
visual kind of mixed media systems
almost I wish I had needed the answer
but you know my lot of my work in this
sort of problem space has been around
conducting design ideations you know
building prototypes and testing on the
troops for these issues because i found
that I've learned the most by just
constructing systems that start to do
this that raise the Saline issues to the
top I'm kind of learning by building
yeah follow up ginger last point you
focused a lot on exploratory tactics but
how do you think about balancing a broad
range of tasks yeah not a real estate
site it may be that where people want to
do is monitor the price of okay or look
in one particular neighborhood yeah I'm
going did search people do a lot of very
simple things so how you balance that
with the broader complexity of how do
you get people to go smoothly from one
yeah and this is even true in wikipedia
where you know administrators wants to
track changes over time so yes I think
the talk has focused a lot in
exploration those are kind of the
interfaces that I've largely focused on
and the challenge comes i think in
changing context like you were saying
you know we can develop a prototype that
really adequately helps people do
directed search or keep track of prices
over time but it's a transitional moment
that's the really hard part so I've been
actually prototyping in the kinetic line
of work time series data and so how do
you transition from looking at one
single window of time to data mapped
over time and it turns out that
transitional point is really really hard
to get right because we want people to
know the consistency right you are
looking at one window now here's that
window in a larger context
and doing that with the kind of fluidity
that users expect in this type of
interface has proven extraordinarily
difficult do we have the points move
based on their time such they're
consistent do we show kind of multiple
representations repeated in some ways
they're domain-specific so I think you
know the tools that I would use to
support a Wikipedia administrator aren't
the same as what i would use here to
show data over time and kinetica arm but
that issue of kind of consistency and
learning what kind of tasks they're
doing whether it's actively or passively
I think are the core questions whether
you have the user declare that are we
try and infer it from their pattern of
use using sort of the crowdsourcing work
that I've done already on behavioral
monitoring that's a super open question
really exciting yeah Mary was I can I go
ahead did you squid
well
what do you feel is the typical adults
cruise not a PhD in computer sciences
Cassidy understad there it skills to
understand how to interact with those it
you feel that there's a need for more
research or tools or curricula on how to
educate people sit internet data and I
think this is a tension in the
Connecticut work where we want to give
this to very inexperienced users because
we know they can begin to use it quickly
but the question is whether they can use
it rigorously or not and how much kind
of statistical education they need
before they're appropriately prepared to
make findings using it and that's a
tough one on one hand what are the
design principles behind kinetica is
trying to make the affordances we used
for exploring data push you in the
direction of statistical validity so
surfacing distribution at every step of
the way through the way the points bunch
up showing filtering visually so you
know how much is being filtered out so
you don't focus on two points when there
are a couple hundred that you're
excluding now I think there's visual and
kind of interaction ways to keep people
rigorous but that's still not enough you
know we can get a 40 or five year old
participant who's never touched a table
before using this in five minutes and
they can pick out a house for them but
is that ethical to have them look at
something for five minutes and find data
points that correspond to such a big
decision and not necessarily understand
the ramifications because visualizations
can read is so authoritative you know
Jamie now we're talking earlier about
the difference between text and visual
and how people are pretty well prompted
at this point to deal with text
resources kind of ranked search results
and know that the top may not actually
be the top that's not necessarily true
when everyday people interact with
visual systems like this and so part of
it is kind of stamping expectations
lower I'm saying you know yes this is it
one way to look at the data but this may
not be the full way to do it and I'm not
really sure yet what that looks like um
you know do we design more systemic
things to say hey wait a minute you got
check these things before you actually
go through with any decision do we just
adjust the interface so it won't show
you things if it's uncertain it's a
whole continuum that I'm not quite sure
about this is a problem even in
wikipedia where people may not be able
to interpret the syntax that people use
in discussions when they're negotiating
down a page if they reference WP colon
peacock do you know what that is it's
actually a policy that says don't use
exaggerated words but you know how do we
make sure that we level these things
properly points to stay on this great
sassy money there but that just is real
but I think that's the sort of tension
this work sits in oh and you know I
don't have an easy answer for because I
think it's a really hard kind of design
as well as a systemic question yes you
know are there like any good data
directive data exploration clothes that
are geared toward kids for example like
Esther yep I see that's an area i'm not
super familiar with their undoubtably
are and i really need to look into that
to understand kind of the issues I think
you're talking about because I would
assume that's sort of the place where
you start oh you're kind of you
because
lady you know so I know what you're one
of the things people often say that
Wikipedia is like oh you know kids
shouldn't they love suppose have rules
like kids should cite Wikipedia a source
in their reports for school this is not
you know authoritative or something
right and I thought that email Dina
Boyd's book that she wrote last year
about teens use of social media for some
reason it wasn't really germane to the
main point of the book but in one of the
chapters was like on the side about
Wikipedia that I actually thought was
one of the best like explanations I've
ever read for why teachers should let
students cite Wikipedia as a source in a
paper and it was sort of focused on the
fact that that all of the background
parts of Wikipedia that people don't
normally read are actually really
educationally informative for helping
students you know young kids like teens
understand the nuances and subtleties of
both the credibility of the information
and how its generated over time and what
is it is controversial and how
corporated all over the PDA and not
business service features could actually
be really important educationally and I
wonder like seeing your system makes me
think about like your system is designed
for you know adults who are contributing
media but i wonder if you know ever have
any thoughts about tools that would
actually help you know middle or high
school students who are confusing me we
have to be more informed consumers of
some of this background contoh a way
that would enhance their kind of
training and education training internal
filter better nothing else I mean I'm
not familiar with the pedia context with
any systems like that but that's a
really interesting angle to take this
and then the question becomes how do you
surface kind of the right because i
think the contextual information you
want isn't a little bit different you
you may want to bias it toward
successful interactions as opposed to
people fighting and coming no resolution
without any other progress or niggling
over a very tiny detail and not an
architectural ii important part of the
page this is something I really have not
gotten too much in the talk but curation
underlies a lot of this work
in the sense of what information you
choose to present and why because
inherent in constructing these topic
models are features that raise or lower
certain parts of discussion versus
certain parts of the page I'm inherent
the crowdsourcing work kind of what
behavioural features are surfacing on
why and that really influences the end
conclusions people make and I think
that's dictated a lot by who the
perceived audience maybe it's an area i
have not liked theoretically explored
much and sounds really really
interesting to dig into awesome thank
you and thanks digital people</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>