<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Designing Controller Abstractions for Software-Defined Networks | Coder Coacher - Coaching Coders</title><meta content="Designing Controller Abstractions for Software-Defined Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Designing Controller Abstractions for Software-Defined Networks</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bj2WEfG6oWk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
my great pleasure to introduce job today
that came from President John josh has
been working on the last couple of years
on a software-defined networks and then
he has basically presented a paper at
nhti last year about parity which is
better way of Zen as the end that paper
want the best paperwork in the
conference one of the many that have a
best best paper community of what the
best paper that gave actually caught in
data for purveyor musi to play with just
all so before that flows at Columbia
where he finished his PhD and during
that work he was involved in a startup
that it's not really start upon as I
understand now it is more of an
established company okay it's doing okay
he is also done a couple of stints at
the microsoft research as an intern in
one of them he did the paper Sleepless
in Seattle but a fortune he didn't talk
about us he talked about machines so a
lot of us actually I Sleepless in
Seattle well and yeah poverty thank
thanks so much for the kind introduction
and thanks everyone for coming out I'm
really thrilled to be here today and I'm
excited to tell you about the work that
I've been doing on designing controller
abstractions for software-defined
networks I've got a lot of material to
pack together so we'll see about the
pacing depending on your questions maybe
we'll skip a little stuff maybe I'll
elaborate on some points but before jump
into the talk let me tell you a little
bit about myself so I like to study
network systems in particular i'm
interested in the way that end hosts
whether those are laptops servers mobile
robots what have you interact and
influence the network that they use to
communicate and vice versa so the kind
of things that i like to explore my
research in the systems i like to focus
on are ones that might take some
knowledge of the network some network
communication and use those to improve
the way that end hosts function how they
perform speed up their storage or access
to storage for example i wanted i like
to do things in the
sittwe which is what you're going to be
hearing about today where we take
resources provided by the end hosts
whether their computational or their
wireless radios or some other type of
feature they can provide that the in
network hardware really doesn't have
access to use that to improve and
enhance the way the network functions
and also new types of systems where we
more tightly integrate the end hosts
with the network and are able to do X Y
or Z faster or better so today's talk is
really going to be focused on how we can
use a controller running on end hosts to
better utilize manage and well control
our network to data point so this is a
hard problem today's networks are really
messes of complexity as illustrated by
the diagram behind me which shows sort
of your typical enterprise network and
the interesting thing about these
networks is while more than half of
their cost goes to the operations
personnel the network engineers that are
managing configuring and winning these
things more than half of the outages
that occur in such networks or because
of operator error so why is this well
there are two reasons at least but
they're too i'm going to talk about
today one is fundamental we want a lot
out of our networks today there are lots
of different interacting components or
at least logic components you want to
provide an 10 connectivity you want some
traffic moved more quickly than other
traffic some traffic may need to be
blocked you might need to balance
traffic across servers or across links
and do all sorts of other things and
this is all in conjunction in
combination with one another so you need
all these things to combine together and
play well each of which may be changing
over time so you've got this problem of
combining lots of different things and
also each of them and possibly the way
in which they're being combined it's
going to be changing over time and then
this is exacerbated by sort of a person
made problem this engineering what I
would say bed interface for controlling
the network and it's not actually one
interface it's
many so they're over right now when I
checked before I came here seventy
internet standards that for managing
network control along with 186 best
practices hundreds of supplemental
semi-official things and who knows how
many proprietary vendor introduced
mechanisms Cisco's stuff juniper stuff
etc and from all of these different
interfaces network engineers today need
to kind of cobble together the network
data plane behavior that they desire
that they intend which makes it no
surprise that there's so many errors
occurring in this process and then it's
so highly human involved so manual so
the key idea in my mind behind
software-defined networking is really
let's go ahead and fix this interface
fix fix this mess of different ways of
controlling the network that all overlap
and instead let our network engineers
focus on the truly difficult portions of
the problem which are figuring out how
to combine lots of potentially
sophisticated functionality that's
changing over time and yeah let's move
on to the next slide this is what the
first generation of SDN really did we
introduced something called open flow it
came out of academia making q and
Stroupe at Stanford and it really has
just taken off of taking off like
wildfire and essentially what open flow
does is it provides us a single unified
way of programming or controlling or
managing the data plane functionality of
our in network devices and this is great
because now we can programmatically
control the network feasibly for the
first time but it has its shortcomings
which is where my work comes in so the
shortcomings are open flows had a fairly
low level so it doesn't really help us
with those fundamental problems of
combining lots of complex software or
sophisticated software and dealing with
and expressing the dynamics
of those individual components in the
way they come together so my works
really about supporting modularity so we
can combine these things and providing
abstractions for encoding dynamic
controllers and two different systems
pyretic which was already introduced and
a newer system we're working on called
kinetic that deals with the dynamic
portion of things I Reddick's more
modularity the flame splits in half
maybe well I think it's a reasonable
icon anyway so the work that I'm going
to be telling you about today really has
kind of two phases to it one phase d
both of these different projects is one
that's in interface so we're designing
an interface upon which programs can be
built the second side is the runtime
architecture that actual computational
machinery that sits on the n toes
controller and realizes the abstraction
is presented by the interface and pushes
them down to the network's data plan one
a one brief note over here today's talk
is because of time constraints going to
tilt a little bit more to be into
probably a bit more to the interface and
then the runtime end simply because a
lot of the runtime techniques tend to be
motivated by concerns in the interface
and well we need to understand the
interface first for them to really come
across but I'm really happy to talk more
after the talk and I'll try to slip in
as much as I can about the runtime so ok
our talk outline what I'm going to be
doing is trying to create a better
interface so I'll need to tell you a bit
about what the situation is now for
managing the network give you a little
more detail to that and how open flow
changes that picture and we're open flow
falls short once we've covered that
background we can really jump in to
talking about the abstractions in both
our interface in our runtime for doing
modularity and dynamics and I'll wrap up
as time allows with some brief remarks
including a little bit of evaluation so
let's just do sort of very basic
networking in case anyone hears from far
outside the discipline if you aren't you
can take a little nap it starts with
in-house they need to communicate so
they're going to be a bunch of links and
boxes that connect them so that way they
can send each other messages in this
case these boxes happen to be cold
ethernet switches but the key thing I
want you to take away from this slide is
that each of these boxes actually has
two lives it has a role as a control
plane and today where it's making
decisions about which traffic should
flow where when and it also has a second
life or a second role as a data plan
today actually making sure that packets
get forward forwarded from one link
incoming link to the next outgoing link
or set of outgoing links at the low
latency and high-throughput that we
demand out of our networks so coming
back to our story our desktops can
communicate with one another but they
need to access services on the servers
so the servers are going to be connected
by their own set of links and boxes in
this case we call them IP routers but
what's what I'd like you to point out to
you is that these boxes aren't exactly
the same in terms of their data plane
but they're very similar the real
difference comes in in the control plane
which is completely different ethernet
switching is very much plug-and-play
learn where hosts are over time adds
something pull something out whereas IP
routing is very much more focused on
being structured and optimized so two
different control plans but relatively
similar hardware and this theme is one
which is going to repeat itself over and
over again so we want to bridge these T
networks that have their each have their
own way of moving packets around and
their own control interfaces so we'll
add a new box a gateway which is going
to run its own stack of logic and have
its own set of interfaces to bridge
between them and now that we have
traffic moving across this network we
may need to load balanced set
our servers so yet another box that's
going to be in the network with its own
interface and own control logic and this
continues on so ISPs comments will meet
our inter domain routers and since
traffic from the outside they're bad
people up there we need our firewall of
course their wireless nodes more types
of boxes will add deep packet inspection
billing compliance all sorts of stuff
out of tsardom and we've got a very
complex thing to manage at the end of
the day each one of these boxes has its
own set or set of interfaces each one's
running some distributed logic and this
is just a nightmare to manage to control
to set up oh let the ad absurdum office
so this is where open flow comes in
openflow really is about decoupling the
control plane from the data plan by
introducing an open standard interface
for having the control plane program the
data plane hardware what's less which
lets us move to a world well rather from
a world in which we have these wildly
heterogeneous interfaces for managing
and controlling our networks data plan
and lots of distributed logic running
around to one in which we program
everything using a single unified API
and we can write applications in that
single unified interface that then can
run wherever possibly on some centrally
logic logically centralized and hosts so
it turns out that this is quite
impactful has gone from academia to
industry in a short period of time there
are all sorts of groups and
organizations and meetings devoted testy
at this point along with a whole bunch
of controller platforms which are fairly
well developed or at least somewhat
mature all of which are open flow
centric and I'll explain what I mean by
that momentarily and you've also got
companies very nice companies some of
them that are developing deploying and
purchasing this technology and really
investing seriously
and getting good results out so this is
all fantastic but it's still far from
ideal in my opinion and to explain why
let me show you what's going on inside
one of those typical OpenFlow centric
controllers I was talking about so in
here you basically have a program that's
been written in some standard
programming language likes each other
Python and it uses a domain-specific
language or an API basically open flow
to control the data plane functionality
of the network switches so why is this
bad well open flow is quite low level
hardware centric so it turns out that
writing programs in it are going to be
rather to write programs in it one is
going to need to go through a tedious
process of trying to figure a very low
level logic that's fairly unintuitive
least two humans and the resulting
products are going to be really anti
modular they're very very difficult to
combine and I'll show you some examples
in a little bit moreover when we get to
and that's without any dynamics when we
get to the world of dynamics we run into
a whole other set of concerns like well
can I predict the future behavior of
this thing that I've written and will it
violate any important properties so can
i verify its behavior how do I think
about structuring it properly and let's
see what else oh yeah optimization
trying to optimize how it actually
installs rules on the data plane all
these are going to be issues I'm the one
thing that we do get nice is we don't
have to spend much time worrying about
architecting runtime this thing is going
to be mostly a shim that just takes the
OpenFlow dsl the OpenFlow api used by
the program and just fix it out as open
flow messages on the wire and I can now
be precise about what I mean by a
controller platform that's going to be
the combination of an interface and a
runtime so everything below the program
on the control
I've shown you so let's go a little bit
further take a little bit deeper and see
how open flow works what the models are
for interacting and controlling with the
network in the context of the simple
example shown behind me we've got is
just a single Sdn switch on port one
connect to the internet ports two and
three or connected to servers a and B
respectively and there's a control
channel perhaps over TCP tunneled over
the network or perhaps on a separate
control network that goes to our
controller host so by default packets
that come into this network are going to
get sent to the controller that's what
the switch is due by default the
controller can do any kind of arbitrary
processing it wants produce new packets
and then if it chooses inject them back
into the network so very powerful model
but it has its costs namely you get a
lot of delay latency increases because
you're processing and you've got the
surround trip time back and forth and
doing that processing and do says
significant overhead on the controller
so the second way of interacting with
the network is to actually install rules
on the data plane and then our traffic
can go through no additional late and
say no overhead on the controller but
this too has its trade-offs its costs in
particular there there's a smaller world
of types of analysis matching you can do
on the packets and actions in terms of
transformations of output that you can
do when you're restricted to your data
plane hardware and you've also only got
a limited amount of space in that hard
way so we're apparently going to have
this trade-off with our controllers over
how much interpretation we do versus how
much compilation we do and I'll define
those terms later on in the talk more
precisely so how do we control the
switch tables what's this Hardware sort
of model that's used by open flow that
we're going to be improving well let's
look at a simple table in an open flow
style so that's the green box showing
behind me it's got several rules or
lines in the table and each one of these
rules is going to have
a pattern that matches on packet header
bets so in this case bits in the source
IP field of the packet it's going to
have an associated action with that
match maybe forwarding on a particular
port and instead of priorities because
one of the key restrictions of these
tables is you only get a packet to match
one rule in the table at most so if we
have two rules like say the top roll in
the bottom most rule which could both
match the packet destined to IPA well
the priorities are going to solve it for
us we'll go with obvious priority and
also when Shaun are a bunch of rules for
the number of counters and the number of
counters each rule has won four bites
and the number of packets that have hit
that rule since it was installed in the
data plane so how does this program work
well if we've got a packet that's going
to a it's going to hit the highest
priority rule at top go ahead 42
likewise B's going to hit the middle
rule go out port 3 and anything else is
going to hit the bottom most rule so
implicitly we've encoded those packets
not matching a and not matching be are
going to be forwarded up or 1 and this
gives us a very simple IP router
matching on IP field and forwarding
packets out now the flexibility and the
brilliance of open flow is very simple
interface gives us lots of stuff we just
change the set of bit bits were matching
on to look at the header bits in the
packet that match on mac address all of
a sudden we have an ethernet switch
which is going to match on mac address
and for doubt ports if we want we can
add an additional forwarding action to
one of our rules and now our switches
multi casting or we could make a table
where some of the rules have no actions
whatsoever implicitly dropping that
packet so we've made a little fire wall
over here for packets that are coming
from host X finally we can sort of
combine more sophisticated patterns with
some it with one additional action
operation modified to be able to balance
our traffic here we're taking traffic
destined to see and we're now going to
rename its destination address tie their
air be thus balancing occur
servers a and pay depending on the
source IP prefix and now we've got our
load balancer so this is a really nice
flexible model for utilizing the
hardware that open flow presents but can
we combine two tables that are made in
this way and i'll show you a first
example involving what i call sequential
composition we'd like to have our load
balancer first modify the destination
address and then our routing module
we're out based on that modified address
not based on the original so can we
combine these rules and remember we only
get one match in a table so as to get
the desired functionality will the
answer's no and if that's not
immediately obvious to give you a little
intuition think if we were to put the
load balancing rules before the routing
rules well every packet is going to go
through the load balancing Rules match
one of them and either get modified or
dropped but none will ever reach the
forwarding rules so we'll balance
without forwarding and vice versa not
good let's say we want to do parallel
composition so we want to route and at
the same time you want to do something
new which is well just in a dress that
has no actions a rule in purple that has
no actions but in this case it's meant
to stand be a stand-in so we can
actually have an entry in the table that
counts packets we won't count the number
of packets coming from host X we're
going to run into the same problem again
and as a result if you look at the kind
of composition facilities offered by
today's control and platforms you'll
actually find they're really kind of
flaky taken from the pox documentation
not all components work together well
but some do what you're left to do with
that I'm not quite sure and this is not
just isolated Tupac's of some references
below feel free to go through the
documentation or I can show you later
question your example is why can't the
two tables that are different colors we
treat it as different as different
openflow tables
you still have that is planed up one
match per table but then they compose at
least in those simple examples okay so
that's a great question um and we can go
deeper into it later but i'll give you a
quick answer in two parts right now the
first part is because we'd like to be
able to implement things as widely as
possible and a lot of the openflow api
so the api's we have available to the
hardware don't actually show us this
kind of multi table setup that's in the
underlying hardware so that's point one
the second point is you don't know a
priori how many different pieces of
logic you might want to say sequentially
composed so if you have three pieces of
logic and you happen to have three
stages in your pipeline and they had
each stage happens to match the order
and the header bits that you want to use
then okay you're great but if not you're
not and if you have more things to
combine together sequentially let's say
four stages and only a three stage
pipeline again you're sunk so ideally
we'd like to at least have the machinery
available to be able to compose these
kind of things together on the same
table if needed so dynamics won't go
into it I'll let you kind of extrapolate
but things get even harder we'll see a
little more detail later but i'll just
show you a quick snippet of what's a
very simple and kind of canonical piece
of dynamic logic that runs on this
ethernet switch is mentioned before over
here written in pox so this thing is
really quite simple and the amount of
code it takes is a bit daunting and the
level at which logic specified well
you'll need to take my word for it or I
can walk it through it on the slide
afterwards and actually of the open flow
controllers I've mentioned before poxes
is probably the most concise example
this particular application so now that
I've finished with the introductory
material in the background you need I'm
going to move on and tell you about our
modular interface design so the key to
providing a truly modular
software-defined networking controller
platform in my opinion
is figuring out how to design an
interface that really supports that
modularity and the thing we need to do
to do that is we need to abstract the
various features that are present in
this Hardware model that open flow
exposes and break them up so we can then
combine them in a modular fashion
instead of the anti modular fashion of
just smashing two tables together and
getting nothing that works so how do we
do this well based on the observation
that a flow table really is essentially
acting like a packet function a function
that's going to take an input packet a
packet at a particular location which
from now and I'll call the located
packet or just a packet for short and is
going to output some set of packets in
this case the exact same packet but just
at a different location and so we can
think of every table is just being a
function that's going to do
transformations and using oh well before
we go there and this is very precise and
abstract so like let's say we want to
make a function called flood that's
going to run on the network Sean behind
me so we've got a packet it's going to
arrive at a particular location sorry
I'd be animation went a little too
quickly let's do that over again packet
arrives at a particular occasion we'll
evaluate the function based on its
current location get some output that
outputs going to get sent across the
network that's what the network does the
Winx do they take something in put it
out and we'll evaluate again this time
we get to output packets they go out one
packet stays within our network and it
turns out this time there are no further
lengths to go we finished flooding so we
drop so very sort of nice abstract way
of expressing what we mean it can apply
to the whole network as opposed to one
particular switch and at the same time
it provides a nice level of abstraction
which oh just a side note so all
piratical actually contain this
information in their header so the
programmer has access to sort of virtual
headers for import out port x which that
they can you
is just like any other header value so
now going to our interface design
strategy what we're going to do is we're
going to sort of take the hardware model
presented by open flow and we're going
to abstract the flow table as well float
abstract the features of the flow table
as either being functions from packets
two sets of packets or operators that
take multiple functions and combine them
reduce a new function from packets two
sets of maggots so let's see how we do
this well the first function we have is
one which we call filter and this
abstracts the open flows matching open
flows matching capabilities so we have a
function primitive function match a
field equals to a value say we evaluate
that on a particular packet well what
we're going to get out is the singleton
set containing that identical packet or
a copy of that packet which is identical
if the packets filled matches the value
and if not well we're going to get the
empty set out what about our next one
well modifications are going to abstract
individual actions so if we modify a
field equal to a value on a packet we're
going to get the same pack a copy of
that packet which is identical except
for one change F is now assigned to
value very and that's going to be output
again as a singleton set and it turns
out this is really nice because we have
those virtual headers in our field for
location so what we basically do to do a
forward it's just a special type of
modified namely we want to forward out
port a that's just modifying the outport
field equal to a ok special filters we
had before our load balancer we kind of
had this bottom little star goes to no
actions and what we really meant there
wasn't drop the packet we met hey pass
the packet on unchanged to the next
module whereas in our firewall what we
really meant was drop so we provide an
identity function which takes the packet
and always produces the singleton set
with that exact same packet
and drop on packet which always produces
the empty set so now we can actually
disambiguate between these two cases so
we're more expressive than open flow and
finally one last set of primitives
before we can move on queries so we want
to be able to abstract the idea of
sending packets to the controller or
counting packets and you'll notice this
is the exact same flow table is the one
we were using for firewalling in the
purple at the bottom source IP equals 2x
no actions out so how do we do this well
we provide to query functions primitives
forward bucket which is going to take a
packet and basically not propagated
through the data plan it's disappeared
from the data plan because we're going
to be sending that packet conceptually
or information about that packet to the
controller where it'll be stored in some
data structure which we call the bucket
likewise we have count buckets that are
going to do the same thing in the data
plane but are going to go to a different
type of data structure and send a
different type of information to the
controller the difference being forward
buckets actually send packets and the
packets are stored by the bucket while
count buckets are going to store
aggregates on packets so counts and
bytes so well for example if you were to
take a filter like let's say you wanted
to know the packets that matched some
particular source I pay right if you
wanted to measure it well yeah well what
you could do is measure the average
packet size you would take a count
bucket and the count buckets going to
give you both the number of packets and
the total aggregate size then you divide
those two numbers and you've got the
average packet size to go inside the
bank you can see the others education oh
so if you want to do deep packet
inspection then then you'll that's not
supported on the data plan the hardware
doesn't do it for us so you use a
forward bucket and actually get those
packets to the controller then you
process them use
a variety of libraries which I can tell
you about later for doing that kind of
processing your language actually
doesn't support it because it doesn't
was hungry exactly like a future yeah
the language you could do anything but
then compiling it and prefer and more
importantly so compiling it becomes more
of a challenge but more importantly
giving the programmer an idea of how
expensive this is it becomes a real
challenge because now if you know if you
use a forward bucket you know you're not
going to get much overhead except what
you do in terms of processing but if I
were to give you this random operation
you have no idea like how costly that is
maybe you'd end up totally hosing your
networks performance so we try to
there's a trade-off here between what we
want you to be able to express which is
everything and what we can efficiently
give you or at least tell you how
expensive it's going to be um so
everything else that's in our table so
there's multiple actions for particular
rule multiple rules running at the same
time at different priorities and this
sort of linkage between matches and
actions are all going to be abstracted
by basically two simple operators plus
well one more parallel composition is
our first operator so that corresponds
to that parallel composition example I
showed you before hand and parallel
composition is the plus sign and
basically when I say two functions
parallel composed or being a vow are
evaluated on a particular input packet
what I'm really what I'm saying is
exactly the same as saying I'm going to
input the first function on that packet
I'm going to input the second function
on that function on that packet and I'm
going to take the union of those two
outputs that is parallel composition and
this lets us do two actions just by
itself and if we want to do multiple
rules well if they don't overlap it's
all we need if they do overlap well then
we may need to use some filters with
negation to make them not overlap and
I'll show you some concrete examples of
that momentarily so negation was that
other operator I mentioned the other big
operator is sequential composition
corresponding to the balanced and
forward example I mentioned
where and here when we say function a
then function be on a packet really
means i'm going to apply to the packet
and then i'm going to apply be to that
output of course there is one little
mistake here namely that B takes a
packet not a set of packets which is
what a outputs so we're going to change
up our definition slightly so we're
basically going to unwrap each packet
produced by a and then apply be too wet
and if they're more than one take the
union of those sets sounds good okay
great so with this we've actually
completed we've actually completed a
total interface redesign going from this
low-level hardware centric anti modular
unintuitive model that open flow
provides to a much more abstract more
expressive and hopefully intuitive and
definitely completely modular dsl for
writing data plane network policy
basically so let's look at this in some
examples we've got our load balancing
example over here in PI retic doesn't
look all that much dissimilar there's a
correspondence each line in the
expression and poetic corresponds to one
of the lines in the flow table but
instead of using ordering and priority
we're actually just using a negation all
those packets that don't match the
destination IP that's going to be
transformed they just get passed through
arm here that an song is actually just
sugar for sequential composition so you
have one match sequentially composed
with another match is logically ending
them likewise parallel composition turns
out to be or now packets unchanged as I
mentioned which is something we couldn't
express in the previous program in open
flow alone what about routing well same
same deal here except note we really
don't care about the ordering parallel
composition is order independent so we
can decide to put our not match case up
at the top what about monitoring oh it's
just
wearing a count bucket and we want to
compose all these different pieces of
functionality together in a prescribed
fashion so we want to balance then
forward and at the same time monitor
well that's all we need a right and this
is something which the programmer can
write down once it doesn't matter how
complex any of these functions are
internally what they're built up of in
terms of primitive functions and other
commentators nor how they might change
in the future which compared to what you
have to do an open flow is very
different this table totally new table
you have to write to combine the
previous tables we saw before with the
logic specified on the previous slide
you'll have to take my word that it's
correct I'll have to take my word that
it's correct because I haven't formally
proven it today but I believe it is but
you have to write this totally new table
it's going to be geometrically larger in
size and hint at what makes dynamics
hard with open flow anytime you change
any of those other individually
precursor functions you're going to have
to rewrite this whole thing which is a
huge mess so this better interface that
we're providing does require something
namely it requires a more sophisticated
runtime to be able to support its
abstractions not enough time today to
really dig into it but to give you a
little bit of a flavor I had mentioned
the interpretation and compilation
before so what exactly is interpretation
interpretation is when we have packets
that end up getting sent to the
controller for whatever reason maybe we
maybe it hasn't been able to compile and
push the the rules down to the switch in
time before a packet actually arrived
maybe there's not enough space on the
rural table or maybe this packet has
some kind of specialized logic that
requires it to be sent so we'll take our
function and we'll evaluate it on that
packet when it comes into the controller
that's interpretation compilation is to
analyze the function and figure out how
to efficiently turn it into a rule table
which is going to require implicitly
doing a lot of the
I check and what a meeting that logic
that the program would have had to
manually do in the previous slide and
then push that on to the switches so the
rules that basically make that function
or on the data flame they are on the
data plane okay so little systems
architecture we've got our open flow
switch there are all sorts of messages
that's requests packet in flow
modifications errors what have you so a
runtime is going to have a thin layer
which is going to take openflow messages
in push open floor messages out it's
going to interact with a couple of
components above it the modules that I'm
highlighting today or the topology
module because we have located packets
an interpreter and a compiler to do
interpretation and compilation and these
are going to be linked in terms of their
behavior I'll show an example about that
in a moment and in engine for our
buckets so on top of this the program
runs and let's look at one particular
program so if match a field equal to a
value on holding these abstract right
now afford a port one otherwise we'll go
into this forward bucket be and at the
same time we will forward a port to
what's going to happen well the
compilation pipeline is going to take
this function and it's going to turn it
over several stages into a table that
can go out to the switch or switches for
which it's applicable and push those out
to the switch so that way when a packet
comes in here a packet where field F
doesn't equal V packet will go through
match on the rule and if you notice
we've got a rule saying forward to so we
do the forwarding to in the data plane
fast don't hold up anything in the
network where we don't have to then
we'll modify the packet and forward it
again to the controller so now we have a
packet in the left back end which came
in port one it's been tabbed and it was
also tagged with the modification we
made which was to put a little bucket
flip a bit saying that this belongs to a
bucket which then lets us when it goes
into the interpreter evaluated against
our policy but realize none of the
forwarding actions in the data plan
are significant any longer so we produce
only one output packet be that goes into
our bucket and we've completed the
interpretation phase of this now note we
might not have had time to install those
roles or been able to install those
rules for whatever reason on the data
plane in which case the packet would not
have been tabbed before going to the
controller and thus the interpreter
would have actually gone through the
full process of generating a second
output packet informing so the
abstractions I've covered for modularity
thus far or the abstractions we provided
work rather on the interface these
primitive functions and the operators
that combine them so total interface
redesign and in the runtime I've shown
you the a little bit about linked
interpretation and compilation though
there's a lot more to say I've pretty
much skipped over the compilation
process but I'm happy to tell you about
it afterwards and involve something
called classifiers and taking cross
products with shadowing and the last one
time abstraction or concept that we've
covered is that of controller buckets so
we can now move on to dynamics and
dynamics again are when the controller's
output an internal state are going to be
changing over time so turns out this has
the same types of difficulties or the
same form of difficulties as previously
there ones that are intrinsic such as
well this is mission critical software
it needs to work right it needs to be
clearly structured for maintenance all
that sort of things so how do we
structure this correctly can we verify
properties about at can we make it run
efficiently in terms of how it interacts
with the data plane these are all
inherently tricky things and then we've
got the exacerbation of the bad
interface open flow as I said before if
you have multiple tables you change one
you have to rewrite the whole combined
table some other stuff but we've gotten
rid of open flow as our interface so I
will with dwell on that instead we'll
we'll just jump right in so I can tell
you about how Mac learner works this is
that canonical Sdn application which I
mentioned before for doing dynamics
showing how dynamics can be
get to work so what does the Macklin do
well we've got this plug-and-play
network we got a bunch of switches they
don't know where they're where the other
switches are or how many links their
neighbors have they just know what links
they have they don't know who the hosts
are yet or where the hosts are located
in the network they need to figure this
all out and it's very naturally
distributed piece of logic each switch
is going to figure this out
independently by first flooding packets
so that way we know that if I put a
packet in over here it'll flow through
the network and get to you but that's
very inefficient use of network
resources so over time each switch is
going to observe the packets that go
through the network and as it observes
them figure out what the shortest path
back to the host that was sending as and
not start uni casting and again this is
sort of a canonical application you'll
see in stn tutorials because of its
simple dynamics but it's sort of robust
huge usage of features let's look at
this a little more precisely you've got
our switch over here again we have a
packet that comes in coming from the
orange host and going to our brown host
so it goes into the switch and the
switch by default is going to flood the
packets out which is great but if for
say an omniscient observer or if the
switch observing what's happening the
switch has learned something namely that
packets are coming from the orange host
on port 1 which means that orange host
is downstream of fort one so we can
install a rule on the table at least
conceptually saying hey packets destined
to the orange host should get ported out
port 1 likewise so we get a return
packet now we're going to unicast sorry
I move through that animation look
quickly so we uni cast out plus we
learned something new namely where the
ground hostess brown host is down
stream's node 3 so packets destined to
brown host from now on can be unicast
upward 3 that's the way Mac learner
looks works any questions about this
logic does it seem relatively simple you
can nod your head or ok I'm assuming
that's
sure excellent you let basically when
the package arrived but and you do it by
sending the party to the street to the
controller the controller figure shot
and that those alone what how do you
forget about them about as sorry about
parts and social disease you mean on the
controller like monitoring them okay so
that's a question I would love to answer
but at the end of the talk or offline
time allowing I'll give you a very quick
answer right now it involves basically a
dynamic function like the one I'm about
to show you but the dynamic function
that involves Geist filters and multiple
buckets so you're going to set up some
derived packet function that is doing in
parallel multiple buckets each preceded
by a particular filter and then as time
passes you're going to remove some of
these expressions from it thus reducing
packets getting sent and that's going to
use the machinery that I'm about to
introduce so in the first abstraction we
use for for doing this kind of dynamics
are simply dynamic packet functions
packet functions whose value can change
over time these are essentially wrapped
in a class in Python so we basically
create a new dynamic function in this
case Mac learner that's going to be
initialized first we're going to create
a bucket in this case it's this packets
bucket which is actually a library we
have that provides just what I mentioned
to you before so we'll only see the
first packet for each switch and source
MAC pair and then we're going to
register an update function defined
momentarily with that and we'll
initialize our function to well flood
and run our query in parallel great so
how does update work well we're going to
updates going to take a packet but
getting went into the bucket and what's
it going to do it's going to change the
packet function its current value self
dot packet function PF to an if
statement which is another derived
statement involving peril and sequential
composition and negation basically if we
if we're matching we're destined to the
mac that we're just
19 then we're going to get forwarded out
the port that the packet we learned on
had come in the downstream port
otherwise we're going to continue doing
exactly what we had done before and this
is essentially all the code you need to
do Mac learning in PI Reddick compared
to that old slide you saw before with
box and this is with the sort of first
of our abstractions but we've got more
because their problems with this first
problem well look at the updates and
texts over here turns out this is a
pretty common idiom so set the packet
function its current value of a dynamic
packet function to be if a special
condition then some special action
otherwise the normal action as before
the problem with this is that what we're
doing with this update we're expressing
it in a mixture of our dsl and general
python which is kind of ugly and creates
some problems with deeply nested if
statements that can even affect
interpretation but fundamentally the
more important part is that it doesn't
provide the runtime any information the
runtime just sees the packet function
was changed but it doesn't really know
how unless it starts to try to analyze
arbitrary Python code which is turn
complete so not not a good way to go how
do we solve this oh and why does the
runtime want to know that this is a
partial change well because it turns out
you can do some very clever things like
cascading recompilation which I won't
have time to talk about today but we can
chat about it after if you'd like so
kinetic our work on dynamics introduces
an abstraction into PI retic that
actually goes into the runtime namely a
new operator update so we're going to
have a packet function we're going to
update it with a with a delta which is
just a filter packet function and a
normal packet function and this
semantically is going to be equivalent
exactly to that mixed Python poetic dsl
before but not only is it nice and
crisper it's going to provide that
needed information so now the runtime
knows and 10 run optimizations that have
been implemented in it so we've got our
old update logic and now we can clean it
up
a little bit with the Delta but as
you'll notice for kind of mixing and
matching source and destination IP s
it's a little ad hoc more fundamentally
it's also hard to look at this and say
okay well what will this thing do in the
future which is where our second and
bigger abstraction in the kinetic work
comes in that of fsm dynamics and so the
idea here is that we're going to make a
packet function which is encoded using a
finite state machine so this finite
state machine is going to have States
each one of which maps to some packet
function in periodic extended with the
Delta update operator for example our
first initial state might be passed
packets on through until a warning event
comes in triggering an egg's agenus
externally created transition to a new
state in which we're dropping packets
you know something's gone wrong we're
going to drop later on we get the all
clear we'll go back to our initial state
identity and beginning forwarding
packets through once again what's really
neat about that so that structure is
going to be useful later on for doing
things like verification but i'll show
you during the evaluation section but
what's really neat is that when you take
these finite state machine encodings of
packet functions and you apply parallel
or sequential composition to them what
you've actually done is implicitly code
in exponentially larger product
automaton where each cross state in the
product automaton is just the packet
function from the first crossing state
composed with the packet function from
the second crossing state so you can see
that in parallel composition you're just
changing the composition operator and as
I said exponentially smaller the number
of states and the number of transitions
so we can get these real a crisp compact
encodings using a linear number of
compact fsms combined using just
parallel and sequential composition and
this is something will exploit shortly
so before i go to exploiting that
property let me show you how we do in
fsm mac learner so this is going to be
specifically
for one particular host and for the
particular switch over here the orange
switch we started an initial state with
three variables one a port just a number
that starts off to be zero because we
don't know what port forwarding out to a
packet function PF which corresponds to
that port in this case we don't know
where which port so we're going to flood
and and 3a a topology change variable
teasing so packet comes in and it gets
flooded out according to the policy but
this also generates an event that's
going to say hey port value n in this
case 1 so we're going to update our port
21 and our flood to forward at port 1
and we're going to stay that way oh and
of course the runtime will then install
the right rules because we're building
this is a library on top of piratical in
till so we'll keep doing that behavior
until say a link fails in which case it
doesn't make sense to keep doing this
which is also going to trim create a new
a new event topology change so we're
going to turn TC to true and because I
roll other variables depend on TC this
is going to create an attack aid of
endogenous transitions back to the
initial state ok limited time so I'm
going to flip through this rest quickly
so conceptually we had that for the
orange fsm will need one copy of these f
SMS / host but if we don't know this a
priority we might run into some problems
so what those problems would be tell you
in a sec but first this concept of
basically taking one fsm per set of
related packets we formalized as an
ocean of allocated packet equivalence
that class this is the largest set of
packets that are all going to be treated
by the same fsm copy and in this case
that set of packets might be match on
the switch that we saw before and the
desk mat being orange and periodic
filters provide a great way of doing
this they're basically just sets we have
the programmer provide a projection
function which is going to map each
packet to its equivalence class so in
this case the equivalence class is going
be the one that matches all packets that
have the same destination MAC address at
that particular switch so how do we
implement this thing well we got the
programmer that provides the finite
state machine and this projection map
and now we're going to create one FS m
copy for each one of our equivalence
classes they're going to be at
equivalence classes we're going to run
them all in parallel and we're going to
restrict each using just the filter the
equivalence class filter and sequential
composition to its set of packets and
there's some event handling stuff I'll
just skip this you can ask me about it
later but the really critical thing is
well we've done an exponentially smaller
encoding here so number of states at the
fsm is s we add n fsms well we're going
from s to the n states that we have to
keep alive in our runtime to s times n
states much much more compact question
or other not it sounds very neat in
terms of a formula in right theater sure
but the suicides of the controller show
doesn't foremost is there evidence that
the number of states when you are doing
in mac learning for reasonable size net
but there's going to be a problem oh so
that's a question we should get into
more a little bit later but what I'll
say right now because the time
constraints is and without the next
trick yes it becomes a serious problem
because a priori and if this is mac
addresses and we don't know the mac
address space beforehand that we might
be operating on you're looking at two to
the 48 being the value of any even if s
is to to to to to the 48 is an
impossibly large number for any control
for any piece of computational machinery
i think to maintain realistically sure
are getting with is mike learning seems
like a very good example because map
learning is one of those rare things
that networks can automatically do very
well without any configuration as i know
it's not your fault not everybody in stn
us
it's the worst example ever with this as
you said Ethan it is plug and play and
it's actually I think that netbooks do
well so I it sort of takes away from the
actual work but anyway yeah well so my
quick answer to that is it is the
canonical one in this area and it's very
good for illustrative purposes you'll
have to take it on faith that this can
apply to lots of other things we can
talk more about that offline because I
just don't have time left to address it
more thoroughly right now so we've got
this tremendous reduction but we're
still in a very large state so we've got
02 to the 48th if we're doing mac
addresses so we need to get that smaller
but it turns out we're in luck each of
our FSN copies is a copy of the same
spec which means that all them start in
the same initial state which has the
same initial packet function so we can
just return as our policy that initial
packet function Delta so using our
previous machinery only by those fsms
that have changed since we started
running our program or even more
efficiently by those fsms that have
changed and not return to the initial
state oh and that pun was intended okay
so abstractions and dynamics interface
so we've got these dynamic packet
functions as Delta update these fsm very
compact encodings um that can be
verified as I'll show you in a moment
the runtime I haven't covered update
state of jang I haven't covered
cascading recompilation but there are
things we have in there that are really
neat and I did to show you how we can do
this on-demand product atomic expansion
so now let me go through our concluding
remarks in like two minutes okay so
bottom line would get comparable
performance to other open flow centric
platforms from applications we took in
the wall took out of the wild and made
best apples-to-apples comparisons we
could in terms of latency and throughput
and that's without many of the
optimizations that I've spoken about you
today several of which are still in
progress or weren't at the time we grand
experiments we get a far more compact
set of programs that are also you'll
have to take my word for it easier to
read example a server load balancer 24
lines instead of 144 in pox and almost a
thousand in floodlight we wrote a state
for a firewall which is actually another
example where you're going to get a very
large expansion potentially because
you're running the stateful firewall
logic for each pair hosts that where one
hosts on the inside one on the outside
of which there can be many many pairs
running in parallel so that's actually a
concrete example application where you
will want to use the tricks I just
showed you and we couldn't find any
programs out there in the wild that did
this type of dynamics I'm going to guess
probably because it's very hard to write
yes also so they said they so these code
lines are just this is just the code in
terms of the experiments before these
were experiments run on mignonette not
on a real network we didn't have access
to a testbed large enough to really do
it with our deadline but were that's our
next step we're planning on doing this
so oh yeah last less point in the
evaluation i want to make dynamics are
verifiable so we can actually write
properties and computational tree logic
or other temporal objects to say things
about the fsm based functions so for
example we can do something like
convergence on all future execution
paths in every single future or current
execution state on any of those paths if
the ports currently 0 which means that
we're flooding then there exists at
least the path where there's a future
state in which we're not flooding so we
can always converge is what this
property says and we can make stronger
statements by tweaking our models
slightly we can also think say things
like stability so for all execution
paths for every single execution path if
we're uni casting to a particular port
then for every future execution path we
will continue uni casting until there is
a topology change today's talk based on
two pieces of work one poetic and a
second one kinetic just in submission
great collaborators and happy to tell
you all about them and which pieces of
the work most of what i present it today
i've tried to focus on those things
which work more my core contributions
but there are a couple of things which
other people really helped out with we
don't need this we can talk about it
later future work you can probably guess
there's a lot of different things and
look at this slide for just about one
more second and I'll just finish off
with quick mention of my past work on
the network to improving host side my
work on accelerating virtual machine
storage basically streaming virtual
machines from storage images is scalable
manner this was a conex to 2012 and did
spin-off that need startup in phinney Oh
a little bit or previous excellent
institution I interned at Microsoft
Research and we did this project on
improving and host efficiency love to
tell you more about that it was
published in music say TC and it's
actually percolated out many generations
later into product i think server center
2008 from the host to the network side
i've actually look mostly at how
wireless interfaces can be used to
enhance the capacity or range of the
network so maintaining the connectivity
of mobile mesh networks won a best demo
award at mobicom will be hawk and also
as percentages a full paper co next i
did that work at columbia and last place
an old work i did a technicolor while
interning there on using opportunistic
mobile contacts to disseminate videos in
optimal ways that accounted for the
impatience of human users and this
resulted in a couple of patents and i am
i've done i'm going to leave that slide
for you to stare at assuming there's any
time for us to stay in the room</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>