<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficient Minimization of Risk Measures via Smoothing: Theory and Applications | Coder Coacher - Coaching Coders</title><meta content="Efficient Minimization of Risk Measures via Smoothing: Theory and Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficient Minimization of Risk Measures via Smoothing: Theory and Applications</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SmUzJb0hwLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone I'm very pleased to
introduce Anka and saha who's here
visiting us this summer from the
University of Chicago he's been doing
some cool internship work with zilla
working on predictive web search now but
today is going to talk about the usual
work he's doing for this PhD on
optimization and i guess some nesterov
method stuff so looking forward to an
interesting talk thanks thanks for the
introduction submit so this has been
stuff that I've been doing for like the
last two years or so and we've been
using some smoothing techniques to
basically optimize risk meshes and look
at various applications in machine
learning where this is applicable so at
the heart of most machine learning
problems you will see a regular Isis
minimization problem and more often than
not this generally results in
optimization problem which is becoming
increasingly larger in size so just to
give you a few examples in the Hadron
Collider you'll have huge amounts of
data gigantic data sets of incredibly
large dimensions if you look at the
field of the internet websites like
Google Facebook Twitter they are
collecting gigantic amounts of data
these days and more often than not these
can often be very skewed data sets with
very large dimensions and maybe very few
data points whereas the converse can
also be true and it's often true
actually in MRI images as well as in
most field of most medical imaging
problems you will actually come across
the large p small and a domain where
basically the data is very large
dimensional but you don't get that many
data points so with this delusion big
data it is very important to come up
with efficient optimization schemes
which are also faster and more
importantly d I mean you want to come up
with optimization methods which are
suited to the specific problem and this
can give you faster rates of convergence
or faster performance on real in real
life so what we do in our work is we
incorporate this new style of smoothing
in various non-smooth objectives I'll
just I'll define all these terms as I go
along
go along so what we do is basically we
give faster rates of convergence on a
large number of existing machine
learning problems and to give you a few
examples are I mean our methods are
applicable to simple binary as VMS
stretch a structured max margin
prediction problems are finding minimum
enclosing shapes and something that
we'll be talking in detail is efficient
smoothing off multivariate score so
problems like minimizing the ROC score
or the precision recall break-even point
and so on so before I go into details
I'll give you a crash course of one or
two slides on what can be I mean
basically the convex analysis that I
need for this clock so as most of you
will be aware a convex function is can
be lower bounded by a linear
approximation so in our talk will mostly
be concerned with a few classes of
convex functions our universe will
mostly consist of Lipschitz continuous
functions so basically the function
value is bounded by some constant times
the actual that actual distance of the
point so basically the closer points
will be closer in terms of function
value as well another particular class
of functions that will be specifically
interested in are strongly convex
functions so these are functions that
are actually lower bounded by a
quadratic rather than the simple linear
functions that the convex functions are
lower bounded by so you can think of any
standard quadratic function as a
strongly convex function as an example
of a strongly convex function but in
particular the definition is that
instead of just the linear approximation
you now have a quadratic term in the
lower bound as well so to just form an
analog the analog is the class of
functions which are upper bounded by
such a quadratic approximation and these
are what we call the functions which
have exactly the functions which
actually have Lipschitz continuous
gradient and they're also known in the
machine learning literature as smooth
functions so these are two particularly
well behaved function classes for which
there are much better rates of
convergence known in the optimization
literature and that's what we'll be
looking at so to give you an example
this on the left hand side is an example
of Lipschitz continuous gradient
function so this is the original
function and at any point you can
actually draw
our quadratic which will upper bound the
function similarly for a strongly convex
function if you take the original
function you can always form a quadratic
which will lower bound the function and
then you can actually optimize that
particular quadratic function to give
better rates of convergence so that that
corresponds to this figure and to just
give you an idea so this is the universe
of Lipschitz live should continue
functioning at and you can consider two
classes of functions as strongly convex
functions and the smooth functions and
there will of course be functions in the
intermediate intermediate regime which
can be optimized much better so I'll
give you some idea of the
state-of-the-art rates for standard
Lipschitz continuous functions Mira
decent methods have already been
existing for the last 20 years or so and
they give you order 1 by epsilon square
rates of convergence to give I mean
basically to go to an epsilon optimal
solution you would require order 1 by
epsilon square iterations for strong yep
sorry so epsilon is within epsilon X
naught and they ask daddy no within
epsilon the F value so this is actually
not epsilon close to the optimal a trait
but epsilon close to the function value
yeah I mean what are all the epsilon
optimum things that I'll talk about is
in terms of function value so in terms
of strongly and so basically for
strongly convex functions symp I mean if
you have both strongly convex as well as
smooth function results then you can
show that projected simple projected
gradient decent methods can actually
converge in order log 1 by epsilon time
whereas for general smooth functions
there have been these momentum based
methods that have been pioneered by
nesterov for the last almost 30 years
now that actually gives you order 1 by
square root epsilon rates of convergence
but a a lot of machine learning
objectives are actually non-smooth and
therefore it remains a challenge is to
how to apply some of these rates into
the machine learning problems in an
effective way so I will show you that
how we can X so these are generally
blackbox rates so you will say ok if
it's a function is strongly convex you
can get this rates of convergence but
yeah
dessert yeah second bullet is used both
strong comeback so basically
well-defined condition number yeah so
what you want is basically that if you
have particularly this particular
structure defined in your problems you
can actually get better rates of
convergence than what exists for the
black box case so that is what we will
exploit in our various problems that we
look at so this is the structure that
I'm actually talking about so suppose
you want to minimize a non
differentiable objective which looks as
this form it is the sum of a strongly
convex function f and the dual of a
smooth function G so I'll use the term
function with Lipschitz continuous
gradient and smooth function as I mean
they basically mean the same things I
will use them alternatively so this
objective is actually the sum of a
strongly convex function and the dual of
a smooth function so as an I mean to
ease it out i will give you various
examples so most regular ice risk
minimization objectives that you see in
machine learning can actually be
expressed in this form in particular
thing of say the standard SVM objective
this is your l2 regularizer which is l2
regular square which is a strongly
convex term and the hinge loss can
actually be expressed as in this form
and i'll show you in one of the later
slides how it can be expressed so most
object most regular is this objectives
that you actually see they can be
expressed in this form and this the dual
can actually be written out like a max
term so if you have an objective in this
form you can come up with I mean I'll
show you how you can actually smooth it
to come come up with better rates of
convergence and what exists in the
literature so if you look at this a
particular objective you will see that
this AG max might not be unique and that
actually results in this function being
non differentiable and causes problems
in differentiating this objective and
getting a well-defined gradient and so
on so the key difficulty as I mentioned
is because of the non differentiability
of this of the dual function g star and
here i was trying to minimize with
respect to W what you can do is
basically push this max over the
variables you
outside and push the min over the W
inside to get what would be the duel
basically the dual objective and that
actually looks like this and it is
interesting to note that if you have
such a well-defined primal objective the
duel over here is smooth so basically
this duel is differentiable and so
basically it has a lectures continuous
gradient and throughout the talk w will
refer to the set of primal variables so
basically the primal objective whereas
you will refer to the set of dual
variables which is the which is what I
have represented here so this is the
this is how the problem geometrically
looks like you have this function J that
you want to minimize it is non smooth so
it has all these kings and there is this
smooth function D which is its dual and
you want to find out this optimum now
convex to ality simply gives you that
this primal will always lie above the
tool and so you are trying to get to
this red star point now one of the ways
of smoothing this basically optimizing
this primal is to come up with a smooth
surrogate of it and you can smooth the I
mean smoother primal in several
different ways i will give you one
particular example that actually leads
to faster rates in convergence so the
idea is you want to add a strongly
convex function in the dual space
capital d so what i do over here is
previously you had this objective where
you had a maximum over this part now i
subtract out of thumb a strongly convex
function in this space so the idea is if
you subscribe to strongly convex
function now you're our max becomes
uniquely defined and once you're AG max
is uniquely defined you can take
gradients so this function jmu now is a
smooth function and you can control how
much strong convexity you are adding by
this parameter mu and if you send new 20
basically you're getting after the
actual objective g yep thank you so i
thought you just said that you have the
primal problem is difficult with the
door solid role you find this
you can see you can saw the private by
solving to do and the doom hole is move
look duel is me but your friend so I
want to solve the entire thing in the
primal itself I don't want to solve the
I don't want to solve the optimization
problem in the duel there are two
reasons for this one is that if i solve
the dual i will give you an iterative
method to converge the optimum of the
duel but that inherently does not give
you a rate of how close how fast you are
going to the primal optimum so there is
a lot of iterations where you would say
there is a lot of algorithms where you
would say i'm going at order 1 by
epsilon rate close to the duel optimum
but suppose i give you an intermediate
iterate corresponding to any particular
do illiterate so suppose you have an
alpha k corresponding to that i give you
a WK which is in the primal this has no
guarantee of how close WK is actually to
w star or even how close the primal j of
WK is to GF w star whereas what I will
end up bounding is the duality gap so I
will end up bounding the primal at the
prime illiterate from the duel at the
doula trade so the distance from the
optimum is sandwich in between so of
course you will be going to the optimum
in that case just because you will have
a specific preference for these
iterative method that only get close I
mean if you if you would really solve
the dual will be done right
I mean I don't know if your tickle
bounds whether actually they can give
you suppose I say that okay I get a
particular alpha cases that D of alpha K
minus the idea of alpha star minus D of
alpha K is less than equal to epsilon I
don't know how to translate that to a
bound from j of WK to GF w star and I
mean they're like that is what is often
done in practice and it is then tends to
give good results in practice but I
don't know if any it rate I mean any
theoretical bounds correspondingly
whereas if you smooth the primal I can
show you that what you end up getting is
a nicely sandwich bound between the
primal and the door yep so this kind of
defines a smooth approximation to it so
this is how the entire thing looks to
you you have this non-smooth primal GF w
what I just showed you is I'm
subtracting out our posit an on-
strongly convex function small D which
is also bounded by a script e so what we
had here is that small D is non-negative
and it's bounded by descripti so what we
getting over here is that j of em you
will always lie below GF w and if you
add new times the upper bound to small D
over here it will always form an upper
bound to the original function so what
we end up getting is we are on well I
mean we're creating a smooth envelope to
our non spot function and fun these
strongly converts a small D strong its
lower bounded by
this has to be a bucket is that it has
to be from your balance yes I see there
is a cute oh so basically new yeah so I
mean I forgot mentioned that yeah we're
working in a bounded domain on the dual
space basically which is awesome I mean
for example if you look at svms the dual
space is basically and a box are I mean
for most machine learning problems that
i know of the duel is generally bounded
or a simplex or something like that so
you end up getting an upper bound as
well so the idea is as you optimize as
you basically optimize this mood
function J of mu and try to send mu 20
at the same time you will end up
optimizing the non-smooth objective so
your rates will actually depend upon as
a trade-off between how well you're
optimizing grm you and how well you are
sending you to 0 so the duality gap now
is basically just the value of the
primal to the duel so the difference
between the two and this can be upper
bounded by this quantity just because g
fw k is upper bounded by this term and
now if you can basically say ok these
are two smooth functions I haven't
handle on this so basically say suppose
I bound this by epsilon by 2 and I
choose mu K such that this term is less
than equal to epsilon by 2 then the
entire thing is less than equal to
epsilon and that is basically the entire
trick behind the smoothing techniques
and what nesterov accelerated gradient
methods to are basically they try to
optimize this smooth objectives J of MU
in order 1 by square root epsilon
iterations and what we found out in the
course of experiments is that if you
actually smooth out the objective in
this way and solve the problem in the
primal you can actually throw
state-of-the-art very fast solver so
what in our experiments we actually
through L bfgs at the problem after
smoothing it and it turns out it
performs amazingly fast compared to
exist I mean compared to existing
state-of-the-art methods over there so
i'll show you a slide for that as well
so
no I we so we basically smooth it out
then we tried various ways of optimizing
the smooth objective which is terms
original algorithm as well we tried CVX
after as well we tried l bfgs as well
and we shall saw that basically if you
all the advantages of fast solvers are
actually captured by this month
objective so to go into some
applications i will basically talk about
smoothing of multivariate measures in
particular i will just talk about roc
area today so the idea is you have all
these measures like roc area and
precision recall break-even point which
are kind of very i mean very important
in areas like natural language
processing speed recognition sometimes
even vision where especially in the
cases where the number of labels the
label classes are very skewed so suppose
you have a very large amount of positive
labels as compared to negative labels it
does not always make sense to capture
accuracy so people often try to capture
our OC area and so on and one of the
problems with these kind of complicated
measures is that they generally come
combined measures over the entire data
set and so more often than not they are
not individually additive over
individual data points so they are not
like a hinge loss that you'll calculate
it over individual data point so the of
more often than not it's very difficult
to apply online learning algorithms to
it so I mean this is kind of a
misleading statement you can actually
use online learning algorithms if you
define measures over pairs of data
points for our z-score area but I mean
I'm right now talking about individual
data points I know but you can choose
from the roller uniform with unstrung
here given person Olin and use online
very happy
oh up I mean you get most other
point I mean one way I know of doing it
is basically if you consider xixi comma
x j pairs where x is a positive point
and XJ is a negative point and see ya
you're going to work well nice very nice
limited to the top but you can still do
that by looking at me now I'm finding
the promotion Iman person okay
construction do it by stochastic values
so I mean that shouldn't say that way
yeah probably that's I mean that was one
of the reasons like a lot of people were
initially asking as well how do you
justify your methods given the fact that
there are a lot of very fast stochastic
gradient descent methods around right
right now so good
okay you can do is to test the gradient
will be one person but this is non
convex so basically end up going to a
local minimum all right so to go ahead
with help I just explain briefly what
ROC score looks at so the idea is you
define these concept of misclassified
pairs so misclassified pair is a pair IJ
such that the score that your model
gives 2x I and XJ is reversed to what
the actual labels of YN y j are so
suppose YJ is a negative point as a
negatively label point and why is a
positively grade level point but your
model is actually giving lower score to
X is compared to XJ so the ROC area is
basically defined as 1 minus the
fraction of such misclassified pairs so
here script p is basically the number of
positive points and script NB is
basically the class i mean the class of
positive points in the class of negative
points and thoughts and yokum's had a
very famous paper which was looking at
max margin formulation for optimizing
such multivariate scores and what he did
was he defined measures in our product
space basically the product space of
excise and x j's so the idea was you
introduce these auxiliary variables Z in
this product space and what you do is
you define a margin based empirical risk
so what is what this empirical risk is
basically capturing is very simple you
want your score corresponding to so I
will always refer is will be an
enumerator over the positively label
points and J will be an enumerator of
the negativity level points what you
want is that your score over the
positively label points should be
greater than the score over the
negatively label points by some margin
Delta so for the particular case the
margin is just one so if that is not the
case so if you incur a loss you want to
punish so you are having these auxiliary
variables Z in by which our band binary
variables in 01 to the M and if this
is actually positive then your zij will
actually be one whereas in the other
case your zij will be zero so if you
incur a loss your zij will be one and if
you don't incur last year's the edge
will be sure so this is just trying to
capture what the losses and then you
will be minimizing over the particular
model that you have so the goal is
basically to find an epsilon accurate
solution of such an empirical risk you
can add a regularizer to that and you
try to optimize it over the data set and
the state-of-the-art algorithms that
were existed before was due to general
cutting plane methods so some something
like bundle risk methods for regular
Isis minimization they used to solve
this optic they used to solve this
problem in order one by eps 1 order 1 by
lambda epsilon time for convergence by
lambda is basically the regularization
parameter and so if you're wondering svm
perf is exactly a cutting plane method
which is basic often considered the
state of the art for optimizing it what
we get is that our smoothing techniques
help us get order 1 by square root
lambda epsilon rates of convergence for
the same objectives and we show that we
are faster in practice as well so we
write down this empirical loss as a
maximum over these binary variables in
the M dimensional space and we note that
this is actually equivalent to
optimizing over fractional variables
beta which belong to the entire space 01
to the M and you replace the CIA's by
the beta H is over here and then we see
that if you look at the entire regular
Isis objective where this Omega can be a
2 norm squared or something like that
this term can now be written in the form
that we wanted it to be so we have we
can actually write it in the cell as a
sum of a strongly convex function f
which is just our regular Iser and this
empirical risk can now be written as the
dual of a smooth function where the duel
is actually as simple as this the duel
is basically I mean this is just some
convex analysis the dual can be written
as an average sum over the dual
variables when they lie in the interval
minus 1 to 1 and they are infinitely
otherwise
and this nice transform comes out that
this a is basically a transform on the
primal variables it is actually just a
matrix such that the IJ the column
refers to this difference between Xin XJ
so we can be then in after we observe
this we basically just apply a nester of
accelerated radiant scheme and use mu is
equal to epsilon by T so note that new
is at the rate at which we decrease our
strongly convex can content in the duel
is dependent upon epsilon so that
actually results I mean that that is how
epsilon affects our solution so i won't
go into the details of the accelerated
gradient scheme of nesterov but what i
want to say is that it is analogous to
gradient descent methods but what you do
is you don't take the gradient of the
previous iterate rather what you take is
you take the gradient add some kind of a
combination of the last two eat rates so
you can think of it as so that is what
is often mentioned what is often
referred to as momentum based methods in
the optimization literature so you take
some kind of affine combination of the
last 28 rates and take the gradient of
the function at that point and it is the
first order method and it is shown to up
to be converging in this rate these many
iterations where script d is basically
the upper bound on the strongly convex
function that we use and norm of a is
basically the norm of justice this
matrix a that I we're talking about so
it is interesting to note that if you
just throw any smoothing function to it
you you will screw up this this
contribution that you have this script d
times norm of a it is not necessarily a
constant it is not independent of the
kind of smoothing you use in particular
if we had smooth without so basically in
our smoothing we had actually used this
contribution X I minus XJ so beta IJ
nicely couples up over the pairs of
positive and negative points whereas we
could have in the use introduced
individual variables for the primal
points and individual variables for the
negative points turns out if you do
something like that you are this
component ends up becoming dependent on
the number of data points so we
you will actually end up incurring more
dependence on the number of data points
then then what we get so basically in
our case the script d times norm of a is
actually a constant so our number of
iterations is actually completely
independent of the number of data points
whatever depending on the number of data
points is we get is basically due to
gradient calculation at every iteration
and I mean so turns out that the
smoothing needs to be done in a pretty
non intuitive way and you cannot just
say that okay we can throw any
particular smoothing at it so to give
you some intuition about how we look at
the gradient evaluations so the gradient
d value I mean since it is the first
order method you will of course go via
calculating gradient at every step and
the gradient at every step looks of this
looks like this very complicated form
what it is doing is basically it is
summing over pairs I mean overall the
positive points and inside the positive
inside that some there is another some
of the negative points and these alpha
hat I j's are actually medians of 10 and
this come this complicated term where AJ
comes from the negative negative points
and AI comes from the positive points
and this basically comes up if you look
at how to optimize SVM perf as well so
basically it is a standard gradient
descent scheme that comes up in
optimizing our OC score in general so if
you actually try to calculate this
gradient naively what do you see is that
you might end up getting order n square
dependence because you're summing up
over all the positive points and summing
up individually overall the natick
points and you have to do the same thing
over here as well see if you try to
calculate the gradients naively you end
up getting order n square dependence so
there was this famous algorithm by
forcing himself which we actually see
that it applies in our case as well what
they'll end up doing is basically it
ends up calculating this gradient in
order and log in time so the idea is you
sort these terms ajs and a is in terms
of in an increasing order and then you
basically calculate this internal sums
in linear time so that is a pretty
simple bookkeeping algorithm which sorts
these particular ages and a is and you
can see that it is very easy to keep
track of them so that you can calculate
these individual sums in order in time
once you sorted them so basically the
entire complexity is due to the sorting
algorithm which gives you order n log
independence to calculate the trading
yeah will you end up with free that
petitioner was there we are so
theoretically you can subsample but
depending upon the amount of skewness in
the labels that you have it can be a
pretty bad estimate it may use the
sample in a way way from these people
others and take us some time on either
service- have it and then probably
escaped we tried simple rescaling
methods but that were not working well
and especially like we tried doing
experiments on the DNA data set which is
actually a pretty skewed one and in that
case the results were not that good if
you if you were trying to calculate this
naively well considering any guarantee
oh I can give you some sample you
the sewing your same program that fall
from which means that these game be
better than drinking of changing the
objective yeah we do not consume and
it's not anything empirically if you
were to subsample how tight they would
be great version of the truth from the
same elevation sir
hip or bill so you have an independent
subsampling for before every iteration
are you just subsample in the beginning
and family yes you just draw a sample
instead of taking Fulton and Lafayette
where men because it's too crusty old
and you can always seek with to stand up
with the musician resources yeah because
the forum changes that we step see that
or you can see Miley kind of an
organization with noisy gradients which
is against the possible solution
so this is the the gradients method that
I was talking about so you end up
getting an order and log and complexity
per iteration for calculating the
gradients and interestingly the same
bookkeeping method actually allows you
to calculate the smooth function value
as well at every iteration so the entire
complexity per iteration turns out to be
order n log n and I just included you
this slide to show you the variety of
the data sets that we were looking at in
particular this DNA is a monster data
set no sorry this was the DNA string it
is a monster data set and it has a very
very skewed ratio and there are some
other very large data sets that we
looked at as well in particular KTP was
a really large and OCR was here yeah OCR
was pretty large as well so what we did
was we compared with the standard
cutting plane algorithms in particular
bundle methods for regulations
minimization and we tried various
possible standard algorithms after
smoothing but in particular the result
that I'll show you is with applying LP
fgs after you smooth algorithms and we
basically calculated our spoon loss and
radiant evaluation using pet CN Tower
libraries from Argonne National Labs and
that helps in faster faster matrix
vector multiplication and stuff like
that so these are some of the curves
that we have I mean the blue curve
corresponds to our algorithm and the red
curve corresponds to bundle methods and
you can clearly see that both the top
curves correspond to basically how fast
the regularized risk is decreasing the
and this corresponds to the
generalization performance so you can
see in terms of both the blue curve is
way ahead of the red curve in terms of
convergence and we are we actually use
like eighty percent of the data for
training and twenty percent for testing
so the generalizing performance is
pretty I mean significantly better than
bundle method spell so yes that's mostly
about the ROC area problem in the limit
in the remaining of the talk I'll try to
show you how this particular smoothing
technique can be applied to various
other problems in
yep idea well is it improve compared
with LPS it's not no we don't come to be
achieved the objective and run ldf GSR
as our optimization yeah so who is
really the blue blue is basically L bfgs
after applying smoothie nesterov starts
did you try this ropes course here
mistress are we tried that but that was
not as fast as our images but it was
still faster than the red line I mean it
was still faster than the existing other
currently methods in general which is
kind of surprising because in various
real-life experiments the bundle methods
actually attained curves which are
closer to order log 1 by epsilon as
compared to order 1 by epsilon so their
theoretical upper bound is order 1 by
epsilon but they are in often in most
data sets their performance is actually
pretty close to order log on web salon
which actually I mean initially V she
was conjecturing that probably it is a
weakness in their bounds but then we
ended up showing lower bounds for bundle
methods as well so that mix makes me
wonder whether this is an artifact I
mean there is some kind of weakness in
terms of smooth analysis or something
like that like this these order 1 by
square root epsilon rates are optimum in
terms of first order methods that has
been theoretically shown by number of
ski but in many real live data sets they
actually perform amazingly better
compared to order 1 by square root
epsilon so if you look at the curve it
is often as close to log 1 by epsilon so
I mean I don't know whether it's an
artifact of and whether there is any
stronger analysis and what I analyst
category of optimization problems
it's just our pub yeah we also did the
particular problem is our bond for the
general yeah yeah but then I'm of course
the lower bound is also for specific
problems that we handpick and in general
cases it might be better yeah so I'll
talk about one other programming one
other problem that we solved pritika I
mean a problem that on the surface looks
completely different so this is the
problem of finding minimum enclosing
convex shapes in particular the minimum
and closing ball problem so the problem
is very simple you have been given n
points in D dimensional space and he
want to find the smallest ball which
encloses all of these points and this
has a large amount of implications in
data mining machine learning even
statistics and I was surprised to see
that I mean I reached this problem by
looking at SVM solvers so there is
something called ball convex machine a
core core vector machine or ball vector
machine which came out around two
thousand seven it was considered to be a
very fast SVM solver and what it did was
basically solve the dual of the minimum
enclosing ball problem and the previous
best algorithms for this are from the
coming from the computational geometry
community and they end up they have this
scheme called finding the corsets which
is like a I mean I won't go into the
details of that that is a pretty
constructive approximation scheme that
gives you 1 plus epsilon multiplicative
approximation guarantees in order ND by
epsilon time where n is the number of
points DS the dimension of epsilon is
the accuracy parameter what we end up
showing is that we can get to an epsilon
approximation in order ND by square root
epsilon and using the same kind of
smoothing techniques so to look at the
particular problem the minimum enclosing
ball problem can be formulated in this
simple way that you have this unknown
radius that you want to minimize and you
don't know the center of the ball as
well so these are your variables and
what you want to do is that the distance
of every point from the center should be
less than equal to the radius so it's as
simple as that so in if you want to
minimize r over r square for all X I you
can might as well write it down as over
maximum over all the excise over this
term
and if you open that out that actually
looks like this and this is the maximum
over excise belonging so basically a
number of excise lying on a set so you
can equivalently replace it on a by a
variable lying on a simplex because the
optimum of a variable on the simplex
will actually lay out the corners so you
end up getting something like this and
you can clearly see that this is the sum
of a strongly convex function and the
exact dual dual formulas formulation
that I was talking about in the past and
now your dual variable actually lies in
the simplex which is again a closed
space or compact space and once you have
this formulation you can basically
smooth it out and throw nesterov I mean
nesterov accelerated radiant scheme
added in particular we used a variant of
next steps accelerated radiant scheme
which is called the excessive gap scheme
for this problem but it is pretty
similar and the corresponding dual
function looks like this form so note
that the dual function is smooth over
here and in this case the duality gap is
again given by this quantity and you can
I mean basically the entire rates is
basically obtained by how fast you are
shrinking the the strong convexity
parameter to another strong term is
sorry the amount of strong convexity
that you adding 20 so basically mu K is
going to 0 at a quadratic rate over here
so that is giving you the rate of
convergence and over in this case Sigma
is basically the strong convexity
parameter of the strongly convex
function that you are adding script d is
the upper bound on that small D function
and L you can just think of L as norm of
a transpose so these are the various
parameters that come in and I mean the
reason I'm kept all these parameters in
the rate is that these these parameters
are important if you don't smooth in a
proper way these parameters will throw
in various dependence on the number of
data points or the dimension or
something like that and will result in
suboptimal rates so it's important to
peak I mean to to optimize it very
carefully so that you end up getting n
to get end up getting the best possible
routes and the reason I mentioning is
that I'll give you an example for this
a bigger Sigma than this you mean yeah
so you should gain by getting a larger
see ya if you if you have such a d
witches which has a very large campus
convexity harm context strong convexity
parameter then this cream d my degree is
well so we ever getting a bound on
script p by sigma
and
so depending upon so basically i will
give you two examples to just illustrate
this point this this small d function
which is strongly convex function is
often referred to as prox function in
the literature so depending upon what
prox function you use you might get
different kinds of rates so of course we
want to add a strong convex function but
a strongly convex function actually the
definition of a strongly convex function
depends on the domain that you are
working on so suppose I am initiate so
currently my domain is just the simplex
but suppose I endow the simplex with DL
to knock in that case the natural
definition of a strongly convex function
to add is the two mom squared so suppose
I had added the two numbers distance
from the central of the simplex in that
case my script the the upper bound
script d would have been sigma x two but
this Lipschitz constraint of the dual
that will actually now have a dependence
on the maximum eigen value of the a
transpose matrix and in general if your
a is an N by D dimensional matrix this
thing can have a dependence on the
number of data points so in particular
the convergence bond that you will get
will be dependent on this script L which
actually will have a dependence on n in
the worst case whereas if you actually
end out this simplex with the l1 norm
and choose your prox function as the
entropy function which is strongly
convex with respect to the with the
simplex in that case your script
basically the upper bound now and incurs
a logarithmic dependence on the number
of data points but this the Lipschitz
concern now becomes independent of the
number of data points so it is just
upper bounded by the the maximum ball of
the individual data points and in this
case basically you end up getting just a
logarithmic dependence on the number of
the other points which gives us the
order star 1 by square root epsilon
rates or tipple dependence so this is
exactly the bound that we get for our
algorithm this will actually converge to
epsilon accuracy in order 1 by square
root epsilon time so as I mentioned the
problem of minimum enclosing ball can
actually be applied to various other
problems in particular it it can be
applied to the problem of finding
the do optimizing support vector
machines so in particular this is this
is the simple objective of support
vector machines you have the hinge loss
plus a regularizer so SVM's are often
solved in the dwell in existing
literature and what was once I mean what
was done around two thousand five and
two thousand seven was people notice
that if you take the dwell of the l2 SVM
where you are actually working with the
hinge loss squared the dual of that
particular problem actually looks
exactly as the dual of the minimum
enclosing ball problem so in that case
people actually use the court the
previous algorithm that were existing
for minimum enclosing ball and they just
used it to solve that as vm table and
the notice that it actually gives you
very fast convergence in real life when
you have large data sets so I think in
at one point of time code vector
machines by the fastest SVM solvers in
practice surround 2007 or something and
they actually so they were previously
using quotes course I'd better algorithm
so they were getting order 1 by epsilon
rates of convergence whereas if you use
our scheme and just plug it as a
subroutine for optimizing this you will
end up getting order 1 by square square
root epsilon the rates of conversions
and so that is for core vector machines
and so one thing of that one thing that
might be of discord is that what we are
doing here it corresponds to l2 SVM so
basically hinge loss squared so turns
out that you can even solve standard SVM
so basically the l1 SVM where you just
have the hinge loss and you can solve it
using these nester styles nothing so
what is obtained is that I mean just to
give you some introduction a previous
state-of-the-art batch solvers so
basically bundle methods also solve the
l1 SVM optimization problem and they
give you order ND by lambda epsilon
rates you can also use stochastic
methods like stochastic gradient descent
or Pegasus to solve the l1 SVM problem
and they actually get independence of
the number of data points but they'll
give you order d by lambda epsilon rates
for convergence theoretical so what we
did was we wrote down the l1 SVM
objective as again a strongly convex
function and the hinge loss as the dual
of a smooth function turns out that this
function the original function G is as
simple as so G of alpha is just
summation of alpha
and if you take the dual of that you end
up getting this objective and in this
case the the the transform a is
basically the product of the y times X
matrix where X is just the data points
stacked up in a matrix and so basically
since your SVM can be written down in
this formulation you can just use nest
ramstein smoothing and then use any of
the accelerated gradient descent scheme
or even proximal gradient descent scheme
which is another variant of nesterov
methods and they'll both give you order
ND by square root epsilon dates of
convenience yeah general relation evolve
epsilon if then l needs to be chosen
roughly something like 1 verse and
square we undo their analysis support
both in any degree and then could you
the running time that you need and you
gain the same for Tennessee what what
happens there and that will be learnt
the correct of this one yeah so I mean
what I mean one of the reasons that like
we had we actually had this around 2009
but we had compared I mean in order to
put this into perspective with Pegasus
what we did was we compared with the
batch version of Pegasus and we beat the
version of texas straight away but then
everybody was like why are you comparing
with the bad version of Pegasus Pegasus
is supposed to be an online algorithm
and our method is strictly a batch
algorithm so comparing it with the
Pegasus algorithm which uses one point
at every iteration was like comparing
apples and oranges so this point that
you made was exactly what we did after
that so it turns out that our method
like in terms of experiments we are
actually kind of at the same league as
Pegasus so I'll show you a quite a few
min quite a few algorithms in the next
slide that we compare it with so these
are a bunch of algorithms that we
compared it with so batch Pegasus is
basically the red line that we have so
this is showing the the primal function
versus the number of iterations that
required required to converge and so
Pegasus is basically the red line and
ours is the blue line so we are
significantly better than the badge
version of Pegasus but there is this
algorithm lib linear which you cannot
even see over here it is somewhere over
here so that is like um that is way
faster than anything else that we I mean
that way faster than our method way
faster than any of the existing batch
that we try to compare it with and I
mean it it basically uses something like
an implicit method in the duel so what
it does is so this is the labeling
algorithm by such activity and chillin
what they do is basically they look at
the dwell and update may update one
coordinator time in the duel so
basically they do a coordinate descent
in the duel and make the corresponding
updates in the Prime and turns out their
way faster than any of the other
problems but then our objective over
here over here was to show that we
actually get faster rates than all the
existing standard 1 by epsilon
algorithms that are existing literature
so yes so that is as far as this is
concerned and yes so these are some
other algorithms these are some other
plots which is basically showing how
fast the primal function decreases in
terms of the time in the particular
seconds so one thing that should be
noted is that our method over here is
bounding the duality gap whereas all the
other algorithms that we compare here
with they work they are basically dual
algorithms so they will give you a bound
on D of alpha star minus D of alpha K
where alpha studies chosen according to
some ground truth and then they just
take that alpha K and get a WK
corresponding to that using the same
formula that is true only at the optimum
so theoretically there is no bound to
say how far that WK is actually from
double strap I mean in experiments they
perform reasonably well when a pretty
well so that is what is generally has
taken as ground truth but our method
actually is efficient that we're both
theoretically as well as practically
because you are bounding the duality gap
and the thing I mean the distance the
optimum is sandwiched over there so I
mean pretty close to my end of my talk
so some other applications where I
actually applied these things is
basically the problem of finding the
minimum enclosing convex polytope which
is again a computational geometry
problem so basically the problem is that
you have a set of points and you have
been given a fixed polytope and you want
to find what is the minimum
magnification of the polytope that you
is required to enclose this points turns
out this problem can also be applied to
certain active learning problems
and it was actually used by dan roth in
finding active learning algorithms for
as basically active learning SVM
problems and you can convert it into the
convex polytope problem and our methods
can also be applied to other max margin
base problems in particular structure if
you look at structured SVM's then our
methods go through over there as well
and it goes through over the entire
basically calculating all the marginals
and sub marginals can be done
efficiently now our setting as well and
we end up getting better rates than the
state of the ad the state-of-the-art
methods for structured SVM's are all
again order 1 by epsilon do to michael
collins and some of his quarters so we
beat that and get order 1 by square root
of solenoids and there's a problem of
finding polytope distance which is
basically look at two positives and
trying the distance between them and
that is also a place where we can apply
our algorithms and get so basically all
of these algorithms previously had rates
of water 1 by epsilon to get to epsilon
close to optimum we improve it all of
them to get to order 1 by square root
epsilon rates here so in conclusion what
we showed was that we can get improved
rates for various machine learning
objectives via this simple I mean this
particular smoothing style that we have
dear nesterov and it gives you if you so
the key messages that you need not
necessarily use nesterov algorithm
itself the key thing that actually
happens is the particular kind of
smoothing that you want to use so
suppose you smooth an objective you can
throw any good solver at it after that
and you can get very good experimental
convergence I mean in practice and this
is true for very large data sets as well
so it is often very I mean often very
relevant so as part of future work we
want to see if these up I mean can
appropriate smoothing mimic the
performance of second-order method since
we are already using l bfgs so can we
basically say that okay if we smooth in
this particular way we can actually just
throw a second-order solver and I mean
not incur the cost of calculating the
Hessian so basically we get we get
performance as good as a second-order
solver but just by doing smoothing and
then throwing something which is cheaper
to calculate at every iteration and
another thing that we have not done up I
mean the another thing is basically
applying the smoothing techniques to
more complicated measures like ranking
measures and BCG or position of the rate
of K or even f1 score so these smoothing
method that I talked about currently we
can do it efficiently only for our OC
area and precision recall break-even
point turns out we have to hand I mean
you have to hand pick the smoothing
technique for each other there is no
generic smoothing Tecna that will apply
across the board so far if something as
simple as f coming as generic as f1
score we don't have a small D technique
that gives you better rates so that is
also a potential future work that is
existing right now yeah so that's stand
of the talk where last experiment is
show that were the best friends
yes so since you already is that's
Liberty new paulinho since you've
already sort of them up here that you
are bench did you try I will be f GS on
that one we tried lbf GS on that it was
it was much faster than our hour so this
al got this crab that i showed the blue
line it is due to nest Ross algorithm it
is not due to lb fgs it comes closer to
live linear but it's still I mean libyan
air is still faster but that is prob I
mean that is because the live linear
code that is available in the
repositories of very heavily optimized
code properties I mean it uses a lot of
caches and stuff like that and thus I
mean it has like a separate cash to do
something and so on I mean I'm not I am
not exactly sure why lib linear is so
fast it says that the theoretical rate
should be log one average superseded
version just to plan
yeah so you just about it correctly it
was faster than the average average yes
yes are you missing woman's convex
Italian John obvious Moses run the LPF
GSF sober then I just be there are you
doing something different from Pegasus
are I don't waste my time in operations
that when I said the wrong words initial
learning worthless to be computed right
I'm are you are watching of the era the
decay and learning anything particular
do in case the same is just a starting
point as well as should be different
yeah we did not compare with and the
usual what you so for your i'll be up
geez how to set your meal here this
collapse long already you can know in
this case it's under surveillance aster
if you want nice selecting the direction
so we I think it's quite all absolutely
perhaps moment so episode excel-based
but that is just a conservative
operations for expend as we tried
various different possible values of
nachos for the injuries on this and so
basically we would start with a musical
epsilon pi square will be and then take
ten times that ten square times a times
1 why we said we have so much he is as
you want to prove the rain and the first
other my server when you applying the
LPF chasers and suspects as if you
starting over starting problem not
attribute values increase rested a
basically no but there is a sweet spot
you continuously keep increasing it at
some point of time it it becomes worse
so basically i think it's an artifact of
the the epsilon that you want to go to
as I mean how close you want to go to
the optimal solution as well and it's
it's not necessarily a well-defined
thing like if you work with 500 points
and you work with a particular epsilon
the MU that you need to set is often
different from you for working with 1
million points so that is
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>