<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Visipedia – A distributed visual system composed of machines and people | Coder Coacher - Coaching Coders</title><meta content="Visipedia – A distributed visual system composed of machines and people - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Visipedia – A distributed visual system composed of machines and people</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YyHHaxIzAdU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I have a body and hello to all the
people who are following online the
topic of the similar today's visit pedia
it's a project I have been working on
with search by lon g and our students
for the last three or four years and I'd
focus on some aspects of it they cannot
cover everything and I hope that people
will want to stop me in raised their
hands because there are some
philosophical discussions here and there
where I always benefit from people's
comments and objections cetera ok so
some of you have seen this slide and
they the question is always can you name
this part of the bird and this helped
has been produced a the issue that often
we have these visually driven questions
and it's very difficult to answer them
given today's technology and so the the
knee-jerk reaction is always to go to
Wikipedia at least for me but if you
don't know what to type it's difficult
so in this case I e-type beak find out
what the thing was and then it went to
pigeon and then I went back to be canned
after a while I found in a small plane
the word that made me think that maybe I
had found it any media had found it and
it the name is fear of that structure ok
and it's all not only vegans have it but
also some parrots and some Hawks have a
seer this is another instance of the
same phenomenon this is some email some
SMS that my father-in-law sent me that
email from his cell phone and he wanted
to know if we can eat this particular
mushroom because he knows that they pick
mushrooms on the Alps and so I by the
look of it I thought he should not even
touch it but I wasn't quite sure and and
this was a difficult not to crack I went
to to Wikipedia I thought roughly knew
what it was but I wasn't able to make
any progress from the entrance page and
the page because I thought there would
be some
some Phil guy that would help me but no
they tell you buy yourself a paper field
guide if you want to identify the Damaja
and so it turns out that these guys
don't work and in fact we have
experiments that I would not show today
the demonstrator Phil guys don't work
and if you're interested on why and and
how did we find out ask me later so it's
a problem and and we are so used to
being able to find all the information
wanted a fingertip in fact we say it
often in our talks oh isn't it beautiful
today we can find anything 12 the figure
it's not true ninety-nine percent of of
the material or bits or bytes that are
on our hard drives on an Internet are
images and video and that's completely
dark matter we cannot access it and all
of us in computer vision know it and
we're working very hard to solve the
problem and so just to go back to the
mushroom the page is there it's an
Amanita pant arena and and so one
interesting remark is if you read it
they tell you that Europeans are
convince Utley will send you to the
hospital within hours of eating it and
the Chinese have a way of cooking it
that makes it very delicious and you can
eat it no problem at all and so I'm very
curious to ask my Chinese friends to to
teach me how to deal with America packed
arena but I'm not sure that will sample
it afterwards okay so um so this was the
first point which is not true that the
web is fantastic and we find all the
information 12 there's in their
fingertips whenever we have a visual
query we are stuck very often and unless
we can think of words that go with a
query in which case we can make progress
and yet information is there and so
there is a bridge to be built somehow a
second point is that is made by this
slide here until here also this little
stories I was at the golden conference I
think it may be r equals not therefore
what I usually drink at all of this
small nice meeting and they it was near
the park of Yellowstone and was walking
around Yellowstone and I found the body
of an elk eaten up by the wolves and I
found a big bone and i thought this
is the femur of the elk but I wanted to
check it and for sure there are places
where there are labeled emerged of Elks
I'm sure there are comparative anatomy
so teach this in class but if you go to
Wikipedia the only picture of a female
find is a female of a human which is
taken from grey's anatomy and all the
infrastructure that we have of making
pictures clickable and hyperlink and so
on has not been used this is the
beginning and the end of of your
investigation of femurs if you go to
Wikipedia at least it was two years ago
anyway this slide so so what's going
wrong I mean there are people who work
on Wikipedia all the time give us lots
of interesting info why can't we click
on this particular femur and get to
ostrich femurs and elk femurs and
chicken femurs and why can't we
understand what is the nature of those
different landmarks on the femur and
what kind of tendons insert their etc
all the technology is there but simply
people don't convey information because
it's very boring so you wouldn't have a
comparative anatomist to spend this
night making this thing clickable and
people are willing to type because
typing is quick and is efficient but
when it comes to visual information they
are not willing to conveyed in visual
information they have and yet people
lecture on these topics all the time
there are if you attend the lecture in
comparative anatomy they will go through
the skeleton of different animals and
compare them and so the information is
there and people are trying to teach it
in some way and if you think of how it
is thought its taught through images so
it's all visual information and in fact
I would make a claim that in many
disciplines visual information is that
the majority of the information is being
conveyed and so we are not doing a good
job not only at serving visual queries
but also in allowing experts who are the
depositary service information to take
it out of a brain and put it out in a
computer friendly way so that we can
share it and deal with it organize it
and search it and this slide is making
the same point
so what would like to be and so in some
some limited domain this could be a
vision of what we want to be so we take
the picture of the object we're curious
about by virtue of the picture have been
being captured and uploaded to some
central repository it all becomes
clickable and hyperlinked and so we can
access the information about this year
at the touch of a finger and this is
precisely what we're doing when we asked
a friend who knows about birds what is
that you know it's as simple as that and
want to be able to access information
and so it feels like what we want to do
is to build a bridge across this divide
between information and and pictures and
so we call this bridge the busy pedia
which is a play on the word wikipedia
and so what needs to be done and people
like us work in visual recognition you
know know what needs to be done so what
we need to do is to have allow
collections of images to be posted on
the web which is you know already
happening and then we need for example
unsupervised learning to figure out all
the corresponding parts between these
pictures with maybe a little bit of help
from some expert and and then somebody
will come in and label one of the
pictures with the different parts and by
virtue of the pictures having been
putting correspondence the labels will
propagate to all such pictures
especially pictures that that user
submitted queries and so once the user
has submitted the picture as a quiet
very becomes clickable it can be linked
to a mall August regions in other
pictures and by virtue of that link to
labels which link to Wikipedia pages and
so it's all done right so this is
exactly what we should do and so this
talk is about telling you that this is
not all and so there is more and there
are interesting questions to be resolved
and so I will not be spending much time
at all on a visual recognition which is
of the workhorse of my group but I will
talk about the compliment of that yes
are using them for search our finding
out information that is mainly image
driven in this are you proposing for the
text and images which all what they want
okay so so this slide is about the fact
that if we had good unsupervised
learning that could figure out all the
correspondences between the pictures
then the amount of work for an expert it
would be much reduced because instead of
labeling all the pictures that are out
there they could label only one and you
could automate the process of answering
user queries because it would be
automatically linked to this database of
information so at least that seems to be
reasonable and this was so if you had
asked me in the year 2000 you know what
is that we should do it which have said
this is it you know this is a man that
is what we want to do and once we have
done it it's all solved and and this is
the type of work we're doing in 2000
which was indeed discovering a mall
August regions between different images
of objects belonging to a certain
category discovering the categories
themselves and so discovering this
platonic concepts that are behind the
pictures and that unify entire entire
data sets of pictures like motorcycles
or airplane that's better so this is
from Rob Pegasus eases and and this work
is being so that if we call this a
constellation model consider mother is
still alive somehow Pedro of essence
valve and Deborah manan improved it so
they added discriminative Lee train
parts which requires supervision but
they work better and so that this is a
whole thing that is going on but my
point today is this is not enough and
and so while I thought that the final
truth is in the images and so humans
should not even bother to perturb the
pure information coming from the images
because the images are the only honest
honest party in the game and so if we
have statistics and the images then we
find out anything that has to be found
but that's not true and so I realize
that
humans that need it because humans have
many other means of investigating the
world and images are just one facet of
the way we acquire information so the
fact that we call that part of a body a
throat well it's not visible the throat
right so any side of the neck looks like
any other side of the neck but since we
can cut people up we can find out that
one part of a neck is a bit different
and also since we can if we get hit at
the playground on your children here it
feels a bit funny and painful well if we
get hit on the side it's less painful
and so we know that it's a special place
and so there's a simple example so here
is it another example of what goes on so
this is a card made by the Louisiana
forestry department to help people
identify that I already billed
woodpecker and so here there are some
people so as you know the ivory-billed
woodpecker has not been seen in the wild
provably since 1930 but there are every
year one or two people who say oh I've
seen one and so the Forestry Service has
put out this car to help people realize
that there are lots of other birds that
may look like an ivory-billed woodpecker
in some configuration of light and so
you know how long would it take to
gather all of this information the
images and so on you need experts in the
field who have the experience who has
are able to see through the images I
don't believe that with unsupervised
learning we would be able to solve this
task unless we were paving the woods
with camera etc and you know for 30
years not possible and instead of the
world if you open your eyes is full of
attempts are at conveying visual
information and here you know nobody who
hasn't taken apart a a locomotive engine
is able to tell you that some panels are
engine access and not the bathroom for
the driver etc so it's simply impossible
so again you know you need in-depth
literally information in order to be
able to to label these pictures but this
is all valuable knowledge that we can
accrue on the visual side and so a
question is how do we get this knowledge
to come in the
open and to be discussed in to the in
computer friendly format so the picture
I would like to draw is a bit augmented
with respect to what we had before so
not only we have information and a
bridge between information and visual
queries if you wish but also we have to
take into account the fact that we have
lots of partially labeled images out
there and this is something that people
in computer vision have been aware of
and so maybe we can take advantage of
all of this partially in partially
labeled stuff computer vision people
build automata and so it will be nice
that the Berkeley segmentation engine
was online and working on any image that
I submit as a query and the Caltech UCSD
bird fine grain classifier was there
available and etc it's so we have lots
of automata now as I was saying before
we have experts in all domains and right
now in the field of computer vision we
always have the computer computer vision
person is also thinking if he's an
expert in teaching the system but it's
not true you simply don't know there are
no people you know there is so much
knowledge out there that we have not
even the first inkling of and so we'd
like to build a system where the experts
like in Wikipedia they can come in and
it can interact with a system without
assistance of a computer vision person
and they can convey the knowledge at the
same time the system should be like a
vampire soon as an expertise around it
should it should not only facilitate but
it should suck all the knowledge it can
out so that it's available to everybody
else even to other experts because
experts have different degrees of
expertise and also different schools of
thought and come back to this concept a
little bit later now all of us have been
using paid annotators online and we know
that we can crowdsource visual task and
I'll talk about that a little bit and so
somebody's laughing in the back of the
room because they're afflicted by this
at the moment and and then of course we
have vision scientists around and so
wouldn't it be nice if the the smart
student who builds a good piece of
software couldn't check in
piece of software that processes images
and does something useful in too busy
pedia like you check in a new page of
Wikipedia and other people could edit it
and the system could figure out if it's
reliable as much as it has to figure out
if an updater is arrival or a expert is
reliable okay and so first of all we
have to notice that there are four
different groups of people on this slide
okay and each different group of people
has a different function different
properties it's a different resource or
a different user type right second we
see that the so-called ground truth is
shifting so while we are used and it
come back to this in the last slide
while we are used to the idea that there
is some expert who has annotated the
data set and provided a ground truth
that we can use for cleaning in
algorithm here there is no ultimate
source of a ground truth while you could
say it's the expert well the experts
will disagree and and we know it and
they may have different views on how to
address a certain visual task and so
there is no ground truth any longer in
some way the ground truth is a platonic
hidden lated concept that we wish to get
at if any exist and so we'd like to
discover what are the different ways in
which we think about images and and
different styles of thinking a different
experts have and see if there are
agreements hidden somewhere and if there
is agreement spring them to the open so
it's a whole set of of new tasks and
again there is no ground truth and we
hope it to emerge and so the question is
can we build such a system with networks
machines and people and helps the
community community the world to share
visual knowledge manipulated organize it
etc so I do now so here you've got the
overall philosophy now I'll go in depth
into a couple of projects two out of
maybe eight that are ongoing or have
happened around disappear and then at
the end I'll try to draw some
conclusions okay and so today what I
would like to do is I told you it will
be orthogonal to computer vision I will
focus on the question of how do we know
whether another
ters and the experts are giving us the
truth and what is the quality of their
work and how do we make a system be
better than any one of the experts or of
the annotators and somehow become the
the sum of all knowledge I shall start
with paterville endures Jesus a piece of
it would be was publishing nips doesn't
in 10 and so the question that I'm going
to talk about him up as we are
collecting the first bird collection
called cube242 hundred species of birds
mostly found in California and this was
done again in collaboration with church
bell on g and the way we collected a set
of birds was we looked up the latin
names on wikipedia and we use the latin
names as indices for flickr com to
download the pictures of the birds and
so i must say it was both we use the
latin names but also the english names
and so that that was the source of much
trouble when so and so a fella TV i'll
type of trouble is if you type pelican
you get lots of pictures of actual
pelicans but then if you get boats
called the pelican and restaurants
called the pelican and you have to kill
all of those images that don't belong
right a more subtle problem is this one
the indigo bunting and so the the indigo
bunting is a beautiful blog little bird
and it happens to be very similar to an
unrelated bird called blue grosbeak and
so there are almost indistinguishable to
what the untrained eye and so here you
have some of the ones and some of the
others so so these are results of typing
indigo bunting into flickr.com the years
ago four years ago so you get some
images that are clearly not correct and
some that might be ambiguous and some
that are absolutely obvious and you
would like to filter out the images that
don't belong and be left with the indigo
bunting images and you do that with
laters now if you do it you obtain the
commonly known phenomenon of
inconsistent labels so if you have four
people have labeled your images some
images have agreement and some images
have disagreements how what do you do
when your experts or your annotators is
agree and so the common wisdom is that
you take majority voting and it may be
you collect some more annotators labels
if there is no clear majority and the
theme of the next ten minutes is no you
can do much better but really much
better if you think a little bit
carefully about what's going on and so I
want to go through what we what we did
so first of all I want to remind
everybody probably no need but there are
two types of errors that people make
mrs. and false alarms now we happened to
annotate we I think carefully they
indigo bunting data set so we have a
ground truth and so we're able to see
how different annotators did on the task
so each dot here is the performance of
an individual annotator and you don't
see error bars here but some of these
annotators annotated maybe 200 images
and some did only 10 images so there are
bars are a bit different throughout the
the plot and they i wish i had a plot
with error bars so um here the axes are
slightly different from the usual roc
because I flipped the vertical axis and
so we have the best performance is the
top right of the of the plot okay so the
hit rate is on the left and the rate of
correct rejection is on the bottom and
these are equal error equal error curves
so these are the ROC curves that you're
used to but flipped around and as you
can see there are a few people orbit
around ten percent error rate but the
vast majority is around maybe 25 or or
more so there's a first realization
annotators that you train with
few training pictures are not very good
it turns out that when you and I'll show
you pictures later in most asked I know
of when you ask experts experts also
disagree thirty percent of a time
identified is that not an outlandish so
when you find expectantly ninety percent
of a time then they're not even worth
day their salt because then it's an easy
task okay so so how do we think about
all of these dots and so the point of
this graph is to convince you that if
you simply average the opinions of
different annotators you're going to do
terribly so here first of all we have
the good one the competent ones up there
now as you can see there are some that
we call bot where most likely somebody
was watching TV and clicking at random
the yes and no buttons or somebody had
written software to reply automatically
to our queries in earning money anyway
now you have people who are optimistic
and always say yes some of them might be
really good they just happen to be
biased towards things and some of them
may be like bot and so you have to be
careful about that and some are
pessimist and so it's I was surprised
that you find all of the Bob sprinkled
somewhat evenly throughout this plot and
you find a few a few adversaries as well
so I was telling my students to do
energizer more this must be MIT students
who are trying to mess with our without
daytime but in fact it's prolly people
who were confused by the GUI and click
the s where they should have clicked no
and vice versa being is slightly
different than this big and this energy
memory so in this case we're showing
them example images of what the bird
should be and of course we cleaned up
two hundred birds and so we had no
automated process of generating the
instructions page we later learned that
it's better to spend 20 minutes 15
minutes to help them get qualified but
then that would require in this case of
200 now a thousand birds to require
quite a bit of work in figuring out
which birds could be
fuse with the current Berg if you are
watching so it's a little bit of it so
that's also another very interesting
question we are getting at which is how
do you train then updaters in a way that
it can be automated and so that takes
advantage of the errors and make you
know to generate there and this is
something that jenn patterson and fact
andreas Klaus is also working on this
passion at the moment through contact
recently and give a talk nice time okay
so so once we realize that the
annotators were all over the map then we
decided let's think carefully about what
the no taters are doing it maybe by
thinking and having a model what they do
maybe when derstand better how to
integrate that information and it in a
productive way and and this is what we
think they know theaters are doing and
so let's see if I can get you to follow
me people like bubble diagrams they will
have a great time in the next few slides
Z is the binary variable that denotes
whether the object are looking for is
present or not so I should qualify this
is about binary tasks at the moment and
so later asked me if you want to know
how you generalize okay so Z is a binary
variable and the index i denotes the
image i and so the image eyes i I and
it's a product of two sets of variables
one is a binary valuable Zi it says the
indigo one indigo bunting is there and
then you have an enormous number of
nuisance variables that you don't care
about and yet they determine the
megapixel image that you will capture so
which viewpoint what was the weather
which specimen of the species 12 it said
okay now let's suppose that you've had
the best possible bird watcher the bird
watcher and any human looking at birds
with reorient themselves you know ignore
all the brush or sky or ways and so on
look at the bird and identify the
different parts of the birds and carry
out a few maybe ten maybe five
measurements on the bird what's the
color of the plumage in shape of the
beak the overall shape of the bird
estimated size a few things like that
okay so X sub I
is a vector a small vector maybe five or
ten scalars which are the measurements
that your best bird watcher would carry
out on the image now an assumption and
so I anyway so an assumption is let's
think of excise a scalar for a moment
it's a vector but the scalar for a
moment an assumption is that the
measurements that water will make are
sufficient to discriminate most of the
time the burden are the only thing you
can go by so here we have one
characteristic x i such that condition
on z being one the value is high in
conditioning z being 0 the value is low
and so it's clear what the annotator
would do they would find the threshold
and decide based on that so if you had a
picture like this it would have a very
low value of x I it would be an
unequivocal know if you had ok and ok
what happens now what happens now is a
true annotator one that you hire will
not necessarily be perfect and so their
readings of the bird will be corrupted
by some amount of noise ok so so in the
head of the vlm rotator that is a
variable Y sub I sub J we're now J is
the index of that annotator and it's
corrected by some noise process sigma j
which we model is gaussians with right
both gaussians and Don pill
distributions doesn't seem to be me
making a difference of them ok so now we
have what is in the head of the
illuminator so a real annotator given
the given that the ideal one would have
excited ahead while they may have a
certain spread of outcomes for that
specific image and depending on how good
is in the theta the spread will be
smaller or bigger now if you have a
clear indigo bunting then it would have
a high value of x I and if you have an
biggest bird if nobody would know would
be able to classify it may be in the
middle between the two distributions and
and so the different annotators would
have a different spread around those
target values if you wish I'm coming to
that yes it's the next I thank you very
much and so so this is the bias idea so
you set the threshold to make it take a
decision and if the threshold is high
then you're one of those pessimistic
annotators if the nation is no you're
one of those optimistic and updaters in
its threshold advice yes yes and so the
point is there are two separate
variables one is the bias and one is the
incompetent if you wish and now I will
convince you that there is a third
variable which we call a school of
thought and it's actually the classifier
so now let's remember that those
measurements are multi-dimensional and
let me use the hawk and seagull example
just to make the point clear so how
might you decide whether a bird is
either hawk or a seagull and so one
thing that most people notice is Hawks
are brown and seagulls are gray and on
the matter what the sub species would be
and the second one is the hawk has a
more robust and hooked bill then the
seagull which is a more straight in
smaller hook okay and then of course
there are other differences like the
talons versus a web fee etc clear for
other obvious differences okay so here
we haven't we are in a
higher-dimensional space and it's clear
that you could have people who train
themselves are playing to discriminate
the birds just by looking at a color and
didn't notice the big and people who
might instead have noticed a beacon not
paid attention to color and people who
may be able to use both at the same time
okay so if you want to come up with a
label you have this high but not too
high dimensional space of measurements
and you will use a classifier which in
our model is a linear classifier so
no particular reason other than
simplifying the crow that we use for
estimating these models okay so here is
our our model and so on the Left we have
the variable Z I which is what we would
like to know on the extreme right we
have the label lij which refers to the
image and then the theater who has
produced the label the labels are
observable and there is another
observable node which is the pair's IJ
of which annotator J observe which image
I so we have labels and we know who
generated the labels and that's the only
thing we know the labels are binding
what we'd like to know is the binary
value of the eye which is down there
clear for bubbles below and so how do we
do it in so the claim that we make is
that if we're able to estimate the
latent variables of the images and of
the imitators we're better off at
estimating the CI and why would that be
the case well because annotators will
see many images and so if we are able to
estimate how good or bad and the taters
are and what biases they might have from
a sex subset of images that propagates
to other images and so that helps us the
same some images may be easy in some
very difficult and not even the Albert
Einstein of birdwatchers would be able
to resolve them and so it may pay off to
estimate this latent variable X I that
tells us if the image is right at the
edge between the two the two
distributions of the two categories
within to discriminate or whether it's
fully in one or the other of its
distribution okay so in some way the
latent variable X helps us explain away
errors that seminar theaters will make
and still let those annotators be good
annotators so so it is that you start
from the labels you want the z eyes and
you go that you do that through
estimating the excise and then the in
competence of the no theaters which is
the noise that they are
mentions yes so in our experiments the
number of dimensions was not let the
free parameter we imposed dimension to
because we wanted to visualize well the
data and for some experiments I'll show
you later on clustering we had four
dimensions it turned out to be a good
one okay so let's exit point yeah so
ideally would have a prior on the number
of dimensions and estimate the number of
dimensions as well we don't do that and
so the the other two characteristics of
annotators i would like to remind you
our tau j which is the the bias and WJ
which is the orientation of the
classifier plane and we call that the
school of thought of the annotated a
somewhat grandiose way but it
illustrates what what it is about yes
exactly so that's that's what is in the
guts of algorithm but politically they
are different things okay so now let me
show you experiment so first of all you
know the raw performance this is what we
get for the indigo bunting data set and
so we had to compare to previous ideas
on how to do things and the green line
is a majority vote and the blue line is
when you use the model and you see quite
a big difference namely with around four
annotators the blue model does as well
as the green model with maybe 20 or 25
annotator so it's clearly there is a big
advantage and there are other
alternatives so I'm
so we carried out a few synthetic
experiments to see if if the idea would
work and if the code was correct by the
way the code is online it's called Cuba
Cuban see you be am and i'm noah
sniveling of cornell told me he don't
really used it and it works so it's a
medician get some he also has used it i
don't know what your what your
experience whether chungi has used it
and stopped using it so maybe it doesn't
work so much okay so so this is
synthetic experiment in which we we
produced 180 of these fuzzy ellipses
each one of which had an orientation we
need 0 and hundred and seventy nine
degrees so one degree at a time and we
show them to annotators asking them is
the lips more vertical or more
horizontal meaning you know when it's 45
degrees it's right in the fresh hole but
if it's more than 45 degrees it should
be labeled as vertical and so of course
you would expect it when we show them
ellipses are at 45 degrees then they are
difficult and so we asked elevators to
label them and okay and so on the x-axis
here you see the actual orientation and
confusingly better decided to use zero
for the 45-degree case and minus 4 less
than 45 and plus for more than 55 and on
the y-axis you see the latent variable X
I and in this case it was a one
dimensional X I and what you see is
indeed X I is around minus one when the
orientation is let's say 10 degrees less
than 45 degrees and plus 1 when the
orientation is more than and it is about
45 and so do you have a certain plus or
minus 10 degree confusing area where the
estimate of X is quite smoothly changing
between one and the other and so from
this plot you see that they
ensemble of the innovators the average
of the imitative is not biased either
way and so really at 45 degrees if you
hit zero and some annotators have some
signal but it's not enough univocal in
some way still ambiguous up to the point
where when it becomes easy and vice
versa now why does X is a curate here
and so this is because in the infinite
so I didn't tell you so how do we infer
the latent variables and so we use some
variational blah blah blah technique
that all of you have had the pleasure of
this pleasure of using and and so what
we do is we we impose a prior on the
value of x I which is a mixture model
where X I has a mean of minus 1 or plus
1 and a sigma of 0.3 or something like
that and then we let the algorithm
adjust it but the prior influences the
extremal values and when the information
is unequivocal sorry so i should say it
better so you have a one degree of
freedom that you can never discover
which is the distance between the means
and simultaneously the size of the
Sigma's because you could scale
everything up or down by set an amount
and so what you can measure is d prime
which is the ratio between the distance
between the means and the Sigma's and
and so you set the two means to be at
minus one and plus one just to normalize
everything in to regularize their
estimate and so when there is no doubt
of what the outcome should be the means
go to minus 1 and +1 and these are the
so while here the dots refer to sorry no
dude animation again so here the dots
refer to individual images so we know
how easy or difficult they are here
don't refers to refer to actual
annotators and now we know what their
bias was
because we can measure each one of them
independently against the ground throws
and on the y-axis we find on the Left
plot we find the estimated bias and as
you can see it's a fairly faithful
estimate of what the real bias was for
the imitators and the same is the
incompetent or competence we defined on
the right hand side okay so since we
produce the ellipses synthetically we
knew what was the real orientation of
each ellipse and we knew what the real
answer should have been from each
annotator on each on each query on each
image so we could compare for each
annotator the labels they gave us at
each orientation with the ground truth
and so we could estimate their bias and
so we saw that indeed individual
imitators have some bias and so a
mystery to me is it's quite clear that
the majority of the imitators so that
you have a group that has no bias at all
but then if you look at bias ones most
are biased on the positive side on the
negative side meaning remember well all
the signs that you tend to see and you
tend to over over say vertical now and
so earlier we saw that there was no bias
in the X I for the image and so so that
I don't know that's a curiosity have
never looked into and here you see that
so most annotators have a d prime around
three but if you are quite poor okay
these plots then it's a toy example but
they tell us that at least the software
is working an idea may have some merit
this is the second experiment we thought
of once the first one went through try
and see if we could estimate the school
of thought of the different annotators
and so we generated these little cartoon
characters some of which are more green
and some of which are more yellow some
of which are taller and some of which
are shorter and then we added some
random splotches on top of them to
confuse the matter a little bit further
and and so either they're green and tall
or short and yellow and we instructed to
populations of annotator
is differently to some we said click on
the greener ones and to some of them we
told them click on the taller ones and
so each one of them was instructed to
pay attention to one characteristic and
not all now we didn't tell this to the
algorithm so we mixed up all the
annotators together and we fed all the
labels to the algorithm and the
algorithm produced this description of
the latent variable X I so you remember
each dot here is one of the images and
so the variables X 1 and X 2 represents
something the detonators might be using
but the algorithm of course doesn't know
what is it because the algorithm doesn't
see the images it only sees their labels
now if you superimpose for some of these
dots the the picture of the end of eight
of the the picture that the annotators
saw you see that there is some order to
these pictures so somehow the greener
ones then to be on the right and the
taller ones then to be on the top now if
you if you put down the ground truth
that was the ground truth so indeed the
latent variable and separated out the
ones and the others and if you look at
what the w's are w's and Taos so it's
exactly as you were asking earlier there
are these and if you color coded by what
instructions they're not they turn to
see if you see that indeed the algorithm
was able to estimate plausible plausible
schools of thought or criteria for
annotating
more interesting experiment is this one
so what Peter did he mix together five
types of images wood ducks mallards
Canada geese Gribbs and water pictures
with no birds mix them together and he
asked annotators to click on ducks so
what we're expecting was yeses and
mothers and wood ducks and nose and
everything else and I'm sure the people
from Seattle would be able to make this
distinction very clear keep it in mind
now this is a latent space that we found
and these are the color codes of each
dot as to which category they belong to
now as you can see something strange is
happening here if then I taters aren't
perfect then we should have a mix-up of
on one side of the docks and on the
other side the Greaves the mat they
geese and the the water pictures there
should be no way to discriminate between
them but in fact in the latent space
they are separated out apart from the
Ducks which are mixed together so what's
going on and you can superimpose the
pictures it's interesting so we had a
GUI at some point where we clicked on
individual dots and we saw that indeed
the pond at the transition our own
biggest ones like birds against the Sun
and so on what you doin up it's all very
clear with you so the interesting thing
comes when you start looking at the
annotators criteria and that will
explain why the geese and the graves
pull apart where they should all be
mixed together in some sense and so here
is what happens you have a first group
of allah taters or very good at
discriminating ducks and everything else
darkness of each line okay the darkness
means the so we had so that where the
model estimates not only the plane of
that sorry the let me tell you better
the darkness is the level of competence
namely one over Sigma yes
it's always a challenge to visualize
everything when one thank you now you
have a second set of lines which is this
one that separates out the Grebe's and
the Ducks from the geese and everything
else so clearly there was a set of
people who thought that the Grebe's are
ducks and that's not surprising because
the Grebe's are a minutes in some way we
had seeded this disaster them because we
picked something similar to that and
then you have a whole set of people you
see 25 spread out there but there are
not many points so spread out doesn't
mean that they disagree it's just means
that the estimates have happened to be
that way of of people who separate out
the non bird pictures from the bird
pictures but they don't discriminate
between the bird species so what we
discover here is that there are three
schools of thought I should be careful
in what they say because the algorithm
truly doesn't know that there are three
shooters hot because if there is no
process for grouping this line and
seeing that there it's our I that tells
us there are these three groups and so
there is one piece missing in the
algorithm which we haven't yet put in
which is if you think of a bubble
diagram I showed you the statistics of
how the annotators are distributed you
could think of it as coming from topix
model or some something that groups them
and so it's yet more jim crockett which
we have yet to have the gut of putting
in an experimenting with saying that
their schools of thought is a reasonable
inference especially given the lines but
if you ignore the lines for a moment
when I first saw I color code
is that grieves are harder to
distinguish and geese are much more
different there was really this diagram
just looked like the diagram visual
similarity yes so so it could okay so
that's a very good point so in my mind
the reason why the Greaves ended up
being separated from the ducks and from
the geese in this latent space is
because there were the different schools
of thought because the annotators were
well spread out in the way they
separated out a space and therefore the
vinyl labels allowed us to spread out
the different species apart from each
other namely you have if you think of a
grip you have a population of annotators
that says yes the population law says no
you think of a doc's everybody says yes
the geese are the ones where a minority
of annotators say yes and the water
pictures nobody says yes so there is
means for the algorithm to pull them out
this way
ok so the annotators are required anyway
because unless you have the labels you
you cannot estimate your exit now the so
how do you verbally explain how these
things spread out you could just say
it's sufficient that the annotate so in
on difficult what you're saying is on
difficult cases then I taters will tend
to the degree and that will keep those
difficult cases apart from the easy ones
which are the ones where you have all
zeros are all one and so it will make
the difficult cases over in the middle
which was what we saw for they lips it
and here I say I think we need something
more more structure than that I think we
need a schools of thought to see this
but it's only words I have not proven it
to you I admit it's suggestive but not
not proven so that would be an
interesting question okay so briefly to
show you that these things are are going
to be used this is a collaboration we
have with a bunch of people in the
medical school at Stanford we look at
sleep traces so if you have trouble
sleeping you spend a night in a sleep
clinic you get instrumental in different
ways maybe some of you people they don't
have had this experience and the doctors
the next day have eight hours of EEG
places such as these ones where they
identify different phases of sleep and
different events within the phases of
sleep and in order to sticker fit in the
water and one way or another we focused
on sleep spindles which are events where
it looks like the cortex is
communicating with the deep brain in
some in some way and so there are higher
frequency events lasting for maybe half
to two seconds and counting the number
of these sleeping spindles in stay in
phase two sleep is an important metric
for some cases so what does the doctor
do the doctor will not be able to take
it all in because it's too long so they
hire a technician who is certified by
the American sleep association or
whatever it is to do it and so it's
somebody who has been trained for months
and gets paid with
well to do it in so the annotator will
sit there for a few hours and go through
the place and deliver counts and a pre
diagnosis the doctor who will then look
at these digestive data and together
with talking to the patient we decide
what to do it so we're curious to say I
happen to sit at dinner once with this
guy manuel me know obviously clinic and
tell him oh we can solve this you know
they have to so the annotators come once
per week and so the patient has to wait
for a week before his place gets
digested the least important patients
may have to wait for a month sometimes
annotators are not available it's a big
pain in the neck and so everything is
delayed and I told meet him you know
with Mechanical Turk you can do it in at
night and he said oh I don't believe it
and so we started the study and this is
the bot yes I'll show you what how well
it works on out here I that was also my
reaction it's very easy and he says oh
I've been working with people in the
electrical engineering department in
Stanford for three years and they
haven't been able to crack this problem
so it conveniently forgot with it that
they work with so these are all the
annotators and Mechanical Turk so each
dot gives you saying so we have a whole
process to find some semblance of non
truth and being able to analyze this
data is not easy at all but anyway so
the dots here and the darkness is how
many how many sleep instances in the
annotate and as you can see the darker
dots which are the people who work
harder are a little bit worse than most
of the light of the many of the lighter
dots which means that lighted dots that
are in the top right of just flukes
they're lucky people who did annotated
three or four sleep phases got easy ones
and got away with flying colors but they
are not necessarily the good ones so the
people who work a lot are have precision
recall around 0.6 if you wish and the
curve is the consent the aggregation of
all of the work of all of these people
and here we have a comparison with the
paid technicians and again the darker
squares are the paid technicians who
work most and as you can see that my
technicians are either equal or a little
bit better than a consensus of the paid
workers but for diagnostic person
purposes they're indistinguishable
they're quite similar so this is seems
to be a useful thing sorry and the
algorithms are the blue the blue dot
different settings of the algorithm okay
so let me skip this one how much time
are you thinking that I should take so
up to noon or after now or I have
another piece of a thought but I can
skip it easily a quarter to another five
minutes ten minutes okay so okay so as
you may imagine so that the next thing
to try out is M for some images you see
that they're easy after the first three
or four annotations everybody agree so
you should retire them and some images
have difficult you should leave them
there for a little bit of time the same
for the annotators annotators as soon as
you know their parameters well you can
use you can kick them out if you're not
good or you can use more of their
services and you already know how well
they're doing and so the labels they
give you are much more informative and
so you know as the system takes off and
is being used it becomes better and
better and so you should be able to make
do with fewer labels per image and so
that's something that we've looked at
and so you can use you can use the
sequential probability ratio testing to
decide if an image is is a positive or a
negative and so as you receive your
votes the probability that it's a
positive and a negative changes for
every vote and at some point original
confidence if we can retire the image
and I'm not showing you the math here
but the math is very simple so I'm happy
to
to do it on the board later and what we
find is the x-axis here is is images
that we have put into the system and on
the left side is the first few images
where labeled and on the right side is
the last few images that were labeled
and what you find and the vertical axis
how many labels do we have to accumulate
per image and what you see is that at
the beginning that is a burst of maybe
100 or 200 images on the left what we
need to obtain 10 or 20 labels each to
be able to reach a conclusion because
we're not going to be sure about where
the resonators are good or bad and as
they annotators become more and more of
a known entity the number of annotations
drops and here we happened not to retire
the baton of datas and therefore they
still a little bit of noise but not much
okay let me skip this so we have a whole
set of thoughts on how to not only do
binary but also do a continuous
annotations like clicks and so here we
had an experiment with which we counted
all the taxes in manhattan by
downloading the big google earth
pictures and counting them and I think
they're worth 25,000 on that at the time
when we counted them and so this is an
easy image but as you may imagine there
are difficult cases like here on the
Left whether it is shadow and so the
color of the cab is not so obvious it's
certain happen ok I'll give you very
quickly another similar but quite
different thing that you can do and so
here is the story suppose that you had a
naturalist with her graduate students
who goes to Costa Rica and once account
the species of beetles in the forest so
they go out with good cameras they spend
a month and they come back home with
30,000 images of beetles and leaves and
so on so how could you figure out how
many species that are
so if you look at the papers on beetle
specification you see strings of papers
will be three or four papers in a row to
decide if Corsica has three Beatles or
four in a certain region and so they did
they look at every single hair on the
cutter pass and Evan the legs they've
gone and this and that long discussions
so how could you quickly sort things out
maybe in a night by outsourcing task and
decide how many species that are again
if you did it by hand it would be many
years of work and so um so the
difficulty clearly is that if you can
source it the each annotator would not
have the context they will not see all
the picture that we don't see a few
hundred pictures it will form an opinion
based on very few and also different
annotators may have different ideas on
what to look for whether it's a four
legs the hind legs and so on so how
could you do it so let me go quickly
through there so for clustering you know
one possibility is to embed things in a
euclidean space and to use it a distance
or you use an affinity know how do you
do this and so first you know experts
use different techniques and so how how
would you do it and so it I must admit
that the this project was suggested to
me when I bought this wristwatch and I
thought roughly that I knew what they
wanted and so I went to Amazon and I
fine so in june 2011 that we're 67,000
men's watches so they're lies okay this
is horrible it's never going to go
through luckily they have a few little
dialogues on the left you can choose the
diameter the color of the bezel and the
thickness and the weight and so on it
turns out that they don't capture well
my own gut feeling of what I was looking
for so um for example and so the idea
would be asking people to cluster these
watches by looks so that then I can
specify and why nice sort of like and
can look a similar one like people do
want internet for example
it's a very organic form of organization
and so here for example though you could
imagine it some people might say that on
the left you have we have black watches
and the right you have white watches and
somebody else might say oh no it's sport
watches versus dress watches it counts
and so you know both are valid and so
how do you how do you think of it so in
order to work on this we devised a GUI
where we asked people to cluster things
in a free-form way and so we had lots of
objects from I think Zappos and and so
people are annotators could pick out so
they would see the six by six panel and
it could pick out arbitrary colors and
use the colors to denote last time they
saw and then they could also have the
option of leaving some things and
cluster if they didn't belong to any
cluster and so so they would an update
for example shows in bad handbags in
blue and jewelry in green so um so
what's the what is going on in ahead of
these people and so what is the parallel
of the model we saw and we think that
the thing to be added what we saw before
is a metric of their space what is
similar and what is different and so if
you think of these birds maybe you think
of them perching vs. flying or you think
of the species of the seagull as
important and so what you could think is
that different people have a different
metrics and people have a metric like
this and when people have a magic like
this so this is what the additional
ingredient for for annotator that we
didn't have before from the metric they
use for the space and so a bubble model
is similar to before but it has this
notion of automatic so earlier we asked
them to classify things and now we ask
them to decide what is similar and what
is different from how and so the labels
now are a little bit more complicated
because the labels are labels on pairs
of objects and other individual objects
and so there are a number of changes to
the bubble model that you saw before
and I'm not saying that they're
obviously took us some time to figure
them out but somehow the philosophy is
similar and so you can think of a
situation would he ask people to tell
you in patterns of six by six what is
similar to what and then you run
influence on a certain model and you
derive some clusters on the whole
collection so no single elevator has
seen even five percent of a whole
collection and all the elements of a
collection have been seen by seminar
theatre in different groupings each time
and so we try to mix things up and to
get lots of measurements so I'll show
you quickly some experiments of one we
did wasn't a popular 15 seen category
data cited for losing of your vision and
so this was in four dimensions and will
only show you two and so it looks like
the clusters like six and two or five
seven ten and three are superimposed but
in fact if you look at the assignment
entropy is very low so the clusters are
separated in the other dimensions and so
this is what people find is that fast as
if people find which are unsurprising
but the surprising part is if you look
at the confusion mattox with respect to
the original proof of each annotator all
the confusion markings look different so
each annotator did have a different
criterion for doing the clustering and
so this is the first class thing as you
can see em so the first class there is a
column that yellow labels are rows so
what you find is the first cluster has
three categories together kitchen living
room an office in doors the second one
has open country and forest together the
third one has told building and inside
CT and the other ones are separated out
so what we discovered is that if we ask
people so for each inferred cluster at
this stage if you ask people to do the
task once more but just limited to those
images of a cluster then we get a
further round of clustering and so what
you can do is you can keep Rick lasting
until
one classf is found and so you have a
tree structure of the taxonomy of the
images so indeed the first class that
gets separated nicely in the second
round if you get kitchens offices in
living rooms no problem at all the
second cluster mountain by itself and
okay so this cluster here which was the
class that the street the highway faster
which was no problem at all if you look
at the gun throws got separated out into
three classes by annotators and clearly
what they see is that there is there are
some scenes with lots of traffic some
with no traffic at all and some with
some traffic and so this is a new
glasses amazing so this is to cover
something it was not there in the
original data set with lots of
experiments and birds etc but I will
keep them in the interest of time okay
so so this picture you've seen so there
are a bunch of papers and other aspects
of you that have not been able to
discuss today and you will see that in
this lies that I gave you maybe one last
thought this is not in a slide I gave
you because I made these slides
afterwards is the following our our
traditional point of view on what we are
doing in a profession is that we are the
computer science experts and by using
machine learning and computer vision
techniques we produce a black box which
maps data into labels for some for some
user and we obtain some set of labeled
data by asking an expert to label the
data for us and then we have sort of a
ground no become truth they run through
together with our expertise in machine
learning the revision allows us to train
this black box and once the black box is
ready we pass it on to someone who will
use it they will be either happy or
unhappy okay that's it and so the
computer vision problem is this right so
that's what we think so I hope that at
the end of my talk you would have a
slightly
different point of view which is that
first of all the picture is more
complicated and so let me go through all
the differences so first of all experts
are multiple and they will disagree and
so the label data are not you neva call
second we routinely augment the data
labeled by experts by shipping them
should be the data off to Amazon Turkish
to produce very large training set for
our for our machine learning algorithms
and that is even less safe as a ground
truth but we use it so we have a bigger
data set then we train a black box and
what happens is and this I didn't go
over but as soon as the black box is
they're the experts have using it to
explore it the data and we realize that
they disagree and they realize that
there are lots of things that you can
discover that it in thing off before so
they will change their judgment as to
what are the real ground truth label I'm
very very happy to give you details
later and so so here you have a loop
that keeps going on and they keep
changing their mind in terms of what
they want to label really furthermore
there are multiple people in in our
business so there are lots of different
possible machine learning computer
vision boxes they produce different
black boxes that can be used and they
should be used the users are willing to
do some work some time to obtain
information and this is something that
Steve branch on it exploding and it
turns out that the users can help the
machine a lot in achieving its goals for
example the machine gets confused when
he looks at a picture of a bird of where
is the bird and it can ask the user
please touch with your finger the eye of
the bird and so the user in just 200
milliseconds will give invaluable
information the black box that will make
it possible for the black spots to
identify the bird for the user and so we
have a loop here and the machines
themselves can be used as a third round
of labeling for data that can go back to
the experts and they let the experts can
check whether the labeling czar good up
now and give them to the machine
algorithms to to to digest and so so my
view is that this is a visual system and
so we should broaden our horizons and
embrace the big mess that is here on
this slide I find it very interesting
and we should make it work for us thank
you fema or you know you guys are no
FEMA and then some solo even when she
kind of stopped right there you're
making point that even if I you get the
text right yes you get stopped because
in other texts Jeff doesn't hasn't got
lucky motion for you there yeah so I'm
education there I mean to me I am
working on text processing so text
ambiguity just as much as visual image
and the popular way you're talking about
their character so accept that so um of
course I mean if you do some just kind
of limited lately scenario they text
hobbies forgive you political community
does your goal to get those in effects
so the question really is that our text
processing visual information can be
used as as grounding to help
disambiguation the text and acuity just
as the other way around yes that's a
nice nice so that you say the picture is
even bigger than that yeah there's only
thing is there's a whole world of pecs
and it's the world should talk to each
other yeah exactly that's hot okay
because I'm not on text to think that
I'm acuity properties as much even more
so than the DVD
see that text you have to think about
your brain one there's our so how is
your thought about bringing them
together that's to put you up by your
earlier be surprised in place so i have
an example of something that which i
don't think we published in our
practices pieces at some point we had
figured out that in order to so we're
looking for cutting corners for cleaning
classifiers for the objects and realized
that if we type words into google images
or being we get an enormous number of
images maybe twenty five percent of
which are related to the concept okay
until we discovered that our
unsupervised learning algorithms you
could set them off on these collections
of images and since what was related to
the search words was consistent and
everything else was inconsistent so
think of Pelican you have the birds and
then you have restaurants boats whatnot
my t-shirt with a pelican on top and so
the things that don't that don't enter
into the category and are there just by
chance are inconsistent with each other
while the category things are consistent
and so we could learn the visual
category from this this inconsistent set
of images because the consistent part
was lots of signal now there was one
exception we found which was sometimes a
search queries produces a number of self
consistent clusters and this is where
you were we're going for example if you
type Madonna you will get a pop singer
and you will get paintings from the
Renaissance of the Virgin Mary okay and
so there are two separate clusters of
things that refer to Madonna and so
using vision you could discover that the
word has a multimodal meaning and and so
now I mean this is very simple example
what you're talking about but I saw I'm
aware of it and it's but nobody has yet
exploited it or exploded yeah yes
ok but</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>