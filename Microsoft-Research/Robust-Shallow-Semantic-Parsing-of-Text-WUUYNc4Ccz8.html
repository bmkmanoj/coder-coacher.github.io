<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust Shallow Semantic Parsing of Text | Coder Coacher - Coaching Coders</title><meta content="Robust Shallow Semantic Parsing of Text - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Robust Shallow Semantic Parsing of Text</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WUUYNc4Ccz8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone this morning it's my
pleasure to introduce deepen jaundice
who's visiting us from CMU his talk will
be on robust shallow semantic parsing
which is a task that many of us have
spent a long time thinking about in
which we all think is very important for
upcoming possible applications of our
natural language processing techniques
he's about to graduate as advisor is
Noah Smith he received the best paper
award at the ACL 20 sorry 2011 for a
paper on unsupervised part of speech
tagging but we since the majority of us
had already seen that talk we asked deep
and John to speak of his other work but
but it certainly the part of speech
tagging is very noteworthy work that
you've done so thank you very much
thank you ok firstly thanks for inviting
me to give the talk here I'll talk about
this work which is a major like part of
my PhD dissertation it's about shallow
semantic parsing of text using the
theory of frame semantics so this talk
is about natural language understanding
given a sentence like I want to go to
Seattle on Sunday we can the goal is to
analyze this by putting layers of
information such as say part of speech
tags if we want to go deeper we can look
at dependency parses however here we are
interested in even deeper structures so
given predicates which are salient in
this sentence for example the word go we
want to find the meaning of the word go
and also how different words and phrases
in the sentence relate to the predicate
and its meaning so over here
travel is the semantic frame that the
word go evokes and it just encodes an
event or a scenario which this word
means over here and there are other
words and phrases in the sentence
that relate to the semantic frame that
go evokes so over here these roles are a
participants that basically relate with
this particular frame which the
predicates go evokes so I is the
traveler over here to Seattle is the
goal where the person wants to go and
time is an auxiliary semantic role so
there are multiple predicate argument
structures possible in a sentence so
over here the word want also evokes a
semantic frame called desiring and there
is the subject I that fulfills the
semantic role experiencer so this talk
will be about the automatic prediction
of this kind of predicate argument
structures mostly it has been described
in these two papers from Knakal 2010 and
ACL 2011 but I'll be talking about some
unpublished work as well okay so one
thing is that please stop me if you have
Kara fication questions so broadly my
talk is divided into these three
sections why we are doing semantic
analysis and I have subdivided it into
the motivation section why we chose this
kind of a formalism for semantic
analysis and finally applications the
second part of the talk is the core when
I talk about statistical models for
structured prediction and this task can
be divided naturally into frame
identification finding the right frame
for a predicate and argument
identification which is basically the
task of semantic role labeling finding
the different arguments once the frame
it is is this ambiguity and importantly
each of these sections will focus on the
use of latent variables to improve
coverage of frame identification and the
argument identification will we will
note the use of dual decomposition but a
new twist on dual decomposition where
which we call dual decomposition with
many overlapping components
and the final section I'll talk about
some work on semi-supervised learning to
improve the coverage of this parser and
I'll primarily focus on some novel graph
based learning algorithms okay
so this is the outline so let me first
start with why we are doing semantic
analysis and motivating it so given this
sentence bengals massive stock of food
was reduced to nothing let's put a
couple of layers of syntax on the
sentence part of speech tags and
dependency parses now a lot of work in
natural language processing has focused
on this kind of syntactic analysis some
of my work is on part of speech tagging
and dependency parsing however we are
going to look at deeper structures
basically syntactic structures such as
dependency parses are not enough to
answer certain questions so over here
let's look at the word stock which is a
noun it is unclear from the dependency
parts whether it's a store or a
financial entity so it's ambiguous
moreover if you want to ask questions
such as store of what of what size whose
store these are also not answerable just
from the syntactic analysis similarly if
you look at the verb reduced we can ask
similar questions like what was reduced
to what that is also not apparent from
the syntactic analysis so what the type
of structures that we are going to
predict frame semantics parses can
easily answer these questions so things
like store the frame store which is
evoked by the word stock says that it is
indeed a store and then there are these
different semantic roles which are
fulfilled by words and phrases in the
sentence similarly there is this frame
and its roles for the verb reduced okay
now I will take some time to trace back
the history of this kind of work like in
a couple of
slides basically it started with the
formalism of Ches grammar from the late
1960s by Charles Fillmore so given a
sentence such as I gave so this is a
classic example I gave some money to him
Fillmore talked about cases that are
basically words and phrases that are
required by a predicate so over here I
the subject is an agent to him is a
beneficiary and so forth so it talked
about three salient things the case
grammar theory I talked about the
semantic valency of a predicate it also
talked about the correlation of
predicate arguments structures like
these with syntax and also talked about
cases or roles like obligatory cases and
optional cases so we are mostly familiar
with this kind of this this theory now
case grammar around the same time in AI
Marvin Minsky talked about frames
basically for representing knowledge and
film or extended case grammar with the
help of this theory about frames to
frame semantics in the late 70s and
early 80s now frame semantics basically
relates the meaning of a word with world
knowledge which was new since in
comparison to case grammar it also
basically presents the abstraction of
predicates into frames so for example
the word gave in the previous slide
evokes a giving frame it has several
participating roles but this frame is
also evoked by other words like bequeath
contribute and donate so basically all
these predicates associated with this
giving frame and they can evoke this
frame in their instantiations now frame
semantics and other related predicate
argument semantic theories gave rise to
annotated data sets like frame net prop
Bank verb net and currently on to notes
which is a popular popular corpus that
people are using and now we are doing
data-driven shallow semantic parsing now
very roughly in the world of AI
give rise to at the same time the the
theory of scripts was developed by shank
and Abelson and template filling
information extraction which is
extremely popular came about with the
help of these annotated data sets like
mock is and so forth so broadly these we
can partition this into CL and AI
computational linguistics and AI work
but today machine learning is bridging
these two areas and we are not really
doing very different things in both
these areas structurally shallow
semantic parsing is very similar to
information extraction okay so enough
about motivation and the history so why
did we choose this linguistic formalism
so I have like I'm representing semantic
analysis in this spectrum from shallow
semantic parsing to deep analysis now on
the shallow end so this is also like a
very approximate representation it's
don't take it very seriously so at the
shallow end
we have prop Bank style semantic role
labeling which is an extremely popular
shallow semantic analysis task so given
a sentence with some syntactic analysis
this kind of semantic role labeling
actually produces it takes verbs so over
here the word reduced is a predicate of
interest and there are symbolic semantic
roles like a 1 and a 4 which are
arguments of this verb predicate now
these semantic roles are only six in
number according to prop Bank the code
rolls and they have specific meaning for
these labels however there has been some
work which has noted that these six
semantic roles since they take different
meanings for different verbs they
conflate the meaning of different roles
due to oversimplification so from a
learning point of view you are learning
classes which have different meanings
for different verbs so which is not
really desirable
on the other hand in the deep side there
is semantic parsing into logical forms
which take sentences and then produce
logical structures like say lambda
calculus expressions now these are
really good because they give you the
entire semantic structure but these are
trained on really restricted domains so
far and has poor lexical coverage so
basically we cannot take a logical form
parser and just like parser strain on
these restricted domains and then run
them on free text so our work lies in
between these two types of popular
parsing formalisms and inflamed semantic
parsing there are certain negative sites
it doesn't model quantification and
negation like unlike logical forms but
there are certain advantages like deeper
than prop Bank style semantic role
labeling because we look at explicit
frame and roll labels there are more
than a thousand in number
semantic roles and we also model all
types of part of speech categories verbs
nouns adjectives adverbs prepositions so
forth we have larger lexical coverage
than logical form parcels because the
our models are trained on a wide variety
of texts and much larger in number than
the restricted domain logical form
parsers and finally the lexicon and the
supervised data that we use is actively
increasing in size so every year we can
train on new data and get better and
better performance so it's an ongoing
moving thing and in tandem we can
develop our statistical models ok so I
come to applications next basically lots
of applications are possible for this
parser the first is question answering
so let's say the example that I showed
you before if we take a question that
tries to extract some information from a
large data set basically few parts both
the question and the answer with frame
semantics parses we will get isomorphic
structures that can be used say in in
features or constraints in
whatever way to answer questions so
moreover if we use other lexical items
like resolve instead of stock since
frame semantics abstracts predicates to
frames we can actually will get the same
frame semantic structure and we can
leverage the use of frames in question
answering so this type of work has been
done previously by Bilotti Italian
information retrieval where they use
prop Bank style semantic role labeling
systems to build better question
answering systems and right now the deep
QA engine of Watson which is partly
developed in CMU is using my parser for
question answering another application
is information extraction in general so
if there is lots of text and if we parse
it with a shallow semantic parser
basically we can get labels like this
the bold items are the words that evoke
frames and then there are semantic roles
underlined and all of these can be used
to fill up a database of stores and
there are different roles that can form
the database columns and yeah
so there is so there is this below th
out paper basically gave the idea of
including it in the pipeline of deep QA
so they have quantification of using a
prop Bank style semantic role labeling
as to how it can improve question
answering but there hasn't been any
quantification in the Watson system okay
right so the last thing I'll comment
about about applications is multilingual
applications so let's take the
translation of this sentence in Bengali
which is my native language it is an X
in tactically divergent language from
English it is free word ordered but the
roles and the frames actually work most
of them work in Bengali as well so we
can use wide alignments to associate
different parts of the English sentence
with the Bengali sentence and we can do
things like translation cross lingual
information retrieval or fill up
multilingual knowledge bases so this is
this is just some hand wavy suggestions
as at doing multilingual things with
frame semantics parses okay so I will
next come to the core of the talk of
statistical models for structure
prediction most of this work has been
described by this in this paper from
Knakal two years back but some of this
is under review right now okay so before
I go on to the models I'll briefly talk
about the structure of the lexicon and
the data that we use to train our model
now the lexicon is basically this
ever-moving
thing called framenet which is a popular
lexical resource so this is a frame
we're placing is the name of the frame
it again encodes some event or a
scenario and there are semantic roles
like agent cause goal theme and so forth
and these black ones are corals and then
the white ones are non core roles these
non cold roles are actually shared
across frames so these are like the argh
em roles from pro bank there are some
linguistic interesting linguistic
relationships and constraints that are
provided by the expert annotators so
over here there is this excludes
relationship I will briefly I'll talk
about it later on these are binary
relationships between semantic roles and
there are potential predicates that are
listed with
each frame that can evoke this frame
when instantiated in a sentence so
arranged bag base now Ben these are
things that can evoke the placing frame
there are several frames in the lexicon
actually there are like 900 in number
currently so I'm showing some like 6
examples over here there are some
interesting relationships between these
frames they form sort of a graph
structure and the red arrows over here
is it is the inheritance relationship
which says that the dispersal frame for
example inherits its meaning from the
placing frame and there is this used by
relationship also and there are other
other multiple relationships that the
linguists use to create this frame and
role hierarchy now there was this
benchmark data set that we use to train
and test our model now it is a tiny data
set in comparison to other semantic role
labeling Stute rain other semantic role
labeling systems so basically there were
around 665 frames the training set
contained only around 2000 annotated
sentences with 11,000 predicate tokens
now we use this for comparison with past
state-of-the-art very tiny data set now
in 2010 there was a better lexicon that
was released it nearly doubled the
number of annotated data and there were
more frames and row labels more
predicate types also in the annotations
so this this is now being increasingly
more information is being added and it's
easy to train our systems as more data
becomes available so we will also show
numbers on this data set right
or the friends being created in
conjunction with the Train yes right
exactly yes so previously what happened
was before these full we call these full
text annotations that was released and
same eval 2007 before that they used to
come up with frames without looking at a
corpus so right now after the
development of this corpus whenever a
new frame a word or predicate cannot be
assigned with a new frame they add new
frames also that works for roles it
comes in our model it comes both from
the lexicon which was present before the
corpus was like annotated as well as the
corpus a union of those two and these
are not only verbs there are nouns
adjectives adverbs prepositions there is
a big chance so we work on the
assumption that these nine hundred
frames that we have currently they have
broad coverage that's the assumption but
it's interesting it can be an
interesting research problem to come up
with new frames automatically so we will
see some work where we assign or
increase the lexicon size by getting
more predicates into the lexicon so that
will be the last part of the talk but
not frames so we assume that the frame
set is fixed this is what we work with
currently so this is released in 2010 I
think they haven't made a formal yes yes
yes yes
so the lexicon is the frames roles
predicates and sometimes I
interchangeably use lexicon with the
annotated data also
yes nine thousand two hundred and sixty
three each not necessary yes that's a
great question yeah it is not the case
so I will have I have semi-supervised
learning algorithms that can handle
those predicates which were not seen
either in the lexicon or the training
did
yes you'll get partial score but that's
another great question we do not exploit
the relationships of frames during
learning that can be an extension where
you can use the hierarchy information to
train your model get better like an
example would be like if you use a max
margin sort of training your loss
function can use the partial like the
related frames for example okay so this
is the set of statistical models that we
use to train our system so I'll first
start about frame identification which
will also address
Lucy's point of these new predicates
that we don't see in the training data
or the lexicon so let's say we have this
predicate identified as the one evoking
a frame and it is ambiguous we don't
know which frame it will evoke so the
goal is to find the best frame among all
the frames in the lexicon so we can use
this simple strategy by selecting the
best frame according to a score that
scores a frame a predicate and the
sentence basically the observation now
we like probabilistic models so let's
say that we use this conditional
probability of a frame given a predicate
and a sentence this can be framed using
a logistic regression model now there
are certain problems with using a simple
logistic regression model here look at
the feature function it takes the frame
the predicate and the sentence now what
happens when when a predicate is unseen
you did not see it in the lexicon or the
training data the feature function will
find it difficult to get informative
features for predicates that you did not
see before moreover so unable to model
unknown predicates at test time this is
the first problem the second problem is
that if we look at the total feature set
it's order is the number of feature
templates capital T multiplied by the
total number of frames multiplied by the
total number of predicate types in the
lexicon so this is the number of
features in the model now for our data
set its turns out to be 50 million
features and we can handle 50 million
features now but we thought can we do
with less number of features and can we
do better so instead we have a logistic
regression with the latent variable
where the feature function doesn't look
at the predicate surface form at all so
the feature function is basically the
frame something called a proto predicate
the sentence and the lexical semantics
relationships between the predicate and
the proto predicate and so basically we
are assuming that a proto predicate is
evoking the frame but through some
lexical semantics relationships with the
surface predicate okay so there is the
frame there is a proto predicate and
then there there are lexical semantics
relationships with the actual predicate
and since we don't know what proto
predicate it is we marginalize it in
this in this model is this kind of clear
with example which I which I'm going to
show you now it will become clear yeah
yeah so that was like nearly
unsupervised like fully mostly
unsupervised so you take prototypes and
then expand your knowledge it is kind of
related but yes yes so here is an
example so let's say for the store frame
you saw cargo inventory reserve
stockpile store and supply but if your
actual predicate was talk which we saw
before you just we just use lexical
semantics relationships with these with
stock and the features only look at
those relationships which are only three
or four in number okay yes it is not but
you will get feature weights
correspondence notes let's say that in
training store was the predicate and
your proto predicate that you're
currently working with is inventory the
lexical semantics relationship will be
something like a synonym and your
feature wait for the synonym feature
will be given more weight for example so
you will learn feature weights for
features that look at a proto predicate
and a lexical semantics relationship
that's tied you
English tokens or other English texted
with both si yes exactly they're not
truly just something like very dropping
yes so it is the list of predicates that
appeared in your supervisor at the neck
see now okay does not look at the
predicate surface form now let's take a
concrete example as I already probably
became clear if the predicate was talked
the proto predicate was stockpile then
the lexical semantics relationship is
synonym and let's say feature number one
zero two four five if is fired only when
the frame is store the proto predicate
is stockpile and the synonym belongs to
this leg same set okay
and this for us comes from word net but
you can use the lexical semantics
relationships but you can use any
semi-supervised distributional
similarity type of lexicon
right lots of predicates
I mean make your training discriminately
so maybe not frightening maybe each
other weights are gonna be reduced a
little bit but yeah that's a good
analysis which I haven't done actually
this does well in comparison to no no it
doesn't do really well it does well it
does well in comparison to a model that
doesn't use latent variables but it does
much worse than you can like a
semi-supervised model and we'll see how
we can do better
um any more questions yes
no but let's say this stockpile
predicate there will be a feature with a
high weight that will get like if the
future was stockpile with synonym it
will get a high weight during training
ideally and that feature with high
weight will fire during test time when
it sees this new predicate so from that
perspective you should get the right
frame
I'll come to that number of features now
is the number of feature templates x all
frames and the max number of proto
predicates of a frame comes to only
500,000 features 1% of the features we
had before okay
now aside is that I worked on this
problem of paraphrase identification
which was which is of interest to
researchers here as well which where we
used all these similar resources like
dependency parses lexical semantics and
latent structure to get good paraphrase
identification we trained this model
using l-bfgs although it is non convex
and for fast inference we have tweaked
basically if a predicate is unseen only
then we score all the frames otherwise
we score only the frames that the
predicate evoked in the data and this is
becomes really faster when we do this so
on so I would give you a caution
beforehand the numbers are not as good
as other NLP problems because we have
really small amounts of data but there
are some frames and rules on which our
model is very confident and those
structures are good so on this benchmark
data set in comparison to UT Dallas and
LTH which were the best systems at same
eval we do better significantly better
now this evaluation is on automatically
identified predicates and all of these
three systems use similar heuristics to
get the predicates to mark the
predicates which can evoke frames now in
a more controlled experiment where we
have the predicates already given the
frame identification accuracy is 74% and
when the data set doubled this goes to
91% so which is good because we only get
one out of ten frames incorrect and
without the latent variable this number
is much worse so basically we are
getting better performance
as well as reducing the number of
features yeah so that's a great point so
this number actually uses this partial
matching thing if you only do exact
matching it is 83% so that is also still
pretty good because we have like
thousand frames to choose from now often
there are some really closely related
frames and it's hard to actually find
the right one so there is a question of
inter annotator agreement they're also
like what is the upper limit and I think
it is just like 92 or 93 percent so we
are doing pretty well in terms of frame
identification especially for known
predicates this number is abysmally low
for unknown predicates and we'll come to
that at the final section of the talk
no no actually this model I don't use
unsupported features that is a great
question
so unsupported features are the one that
appear in the partition function so if
you include those then it is 50 million
but if you don't use them then it is
some a few million basically okay the
unsupported features they actually help
you a lot in this problem which is what
we observed given the correct predicate
for brain no it's the given a sentence
you can mark which predicates can evoke
frames so if you do that automatically
using heuristics that is the non-trivial
problem so if you saying you tell it
this is the predicate but you don't tell
it it doesn't necessarily match in
practice you still have to do your we
still have to do free modern diffusion
so
three out there for that pretty piece so
would that be a case where you have
different words that hopefully won't
but example do you have
I'm excited
mr. clay
so you are asking let me just rephrase
it so in a question in a sentence
whether two predicates can evoke the
same frame is that the question that
happens but not quite often but we do
not since we do not jointly model all
the predicates of a sentence together
that from a learning or inference point
of view we don't care about what other
predicates are you looking but that can
also be done like whether at a document
level especially we can place
constraints or soft constraints as to
whether like different predicates in a
document which are the same or similar
meaning they should evoke the same thing
yeah it's like whether there is a
lexical semantics relationship of
synonym whether there is a lexical
semantics relationship of - ooh -
Anasuya
just curious I suppose you are trying to
the Mexican symmetric relation derive
yes you don't necessarily get by
so this model can handle that like a
real-valued feature there should not be
any problem but another hack would be to
just make bins of distributions like 100
bins for attendants and use those as my
images
okay this is the more interesting part
and I think that we have done some good
modeling over here in comparison to
previous semantic role labeling systems
so let's say we have already identified
the frame for stock and now we have to
find the different roles so there are
potential roles that come from the frame
net lexicon once the frame is identified
so let's say these are these five roles
over here and the task is to from a set
of spans which can serve as arguments we
have to find a mapping between the roles
and the spans so this is just a
bipartite matching maximum bipartite
matching problem from our point of view
and at the list at the end over here we
have this Phi symbol which is the null
span and when a few roles map to this
null span it means that the roll is not
overt in the sentence or it is absent
from the analysis
it's just a convention that the
annotators use it's strange
it is like yes it is a choice that both
prop Bank annotators and frame it and
order lefty like modeling the labeling
the entire prepositional phrase for
independancy parsing also these are
there right because conjunction is a
case where you don't really know what to
what the head should okay now there are
certain problems this is not just a
simple bipartite matching problem
because we made mistakes I may may make
mistakes like this if we map supply to
food over here
this is linguistically infeasible
according to the annotators because it
violates some overlapping constraints so
basically two roles cannot have
overlapping arguments so this is a
typical thing in standard semantic role
labeling work for example Christina had
this paper in two thousand four or five
where they did dynamic programming to
solve this problem now there are other
things which are more interesting and
have been explored previously like the
mutual exclusion constraint that these
two roles cannot appear together an
example is basically for this placing
frame if an agent places something there
cannot be cause role in the sentence
because both the agent and the cause are
ideally placing the thing so two
examples are here the waiter placed food
on the table and in Kabul hauling water
put food on the table so these are very
different meanings but hauling water is
the cause while agent is the waiter and
they cannot appear together in an
analysis there are more interesting
constraints like the requires constraint
which means that if one role is present
the other has to be has to be present
okay in the analysis so over here this
frame is similarity so resembles is the
predicate evoking the frame similarity a
mulberry resembles a loganberry the
first one is entity one the second is
entity two but assess
sentence like a mulberry resembles is
meaningless so you have to have both
these roles in the structure okay so
there are more things more such such
linguistic information that can that
will have that I have to come
the the maximum bipartite matching
problem so it's a constrained
optimization problem okay so this is
sort of other people have also done this
but we are doing this semantic role
labeling task in one step which is just
one optimization problem so what we do
is we use this course over these edges
the edges look at the roll and the span
so remember that this bi-directional
arrow with roll and span is the tupple
that we operate on and this score is we
can assume it to have a linear form like
it's a linear function that is there is
a weight vector and there is this
feature function G that just looks at
the roll the span the frame and the
sentence which I have omitted over here
okay so it's a standard thing that
people do now let's introduce this
binary variable Z it's a binary variable
for each roll and span couple Z equal to
1 means the roll and span that span
fulfills the role as 0 means that span
doesn't fulfill the roll and let us
assume that there is a binary vector for
all the roll span tuples so it's the
entire z vector for all the rolls and
spans now we are trying to maximize this
function basically sum over all the
rolls and spans z rolls and multiplied
by this score okay with respect to Z and
for all the rolls we have this
uniqueness constraint which says that a
roll can be satisfied by only one span
so this is the constraint that expresses
that and there may be more constraints
like this one which is a little more
complicated it constrains each sentence
position to be covered by only one roll
span variable so it will prevent overlap
and there are many other structural
constraints that imposes the requires
excludes these constraints and people
will be familiar with this this is an
integer linear program okay now there
has been a lot of work on this so this
is one of the seminal papers on semantic
role labeling that uses ILP
to do this this inference problem but
ILP solvers are often very slow and many
of them are proprietary and my parser is
public and we want to make it open
source so that people can contribute so
we use this thing called dual
decomposition with alternating direction
method of multipliers that solves this
ILP problem it solves this relaxation of
the integer linear program using a very
nice technique that doesn't require us
to use a solver so this has been
developed with colleagues at CMU and
this is a paper under review so
basically we introduce this thing called
basic part a roll and span topple forms
a basic part so the entire space is all
the roll and span tuples that we operate
on so what we do is we break down the
whole bipartite matching problem into
many small components like find the best
span for a roll like for for a sentence
position find the best roll span couple
and so on these are really simple
problems that can be solved really fast
and at a global level we impose
agreement between these components so
the entire structure is there is a
consensus between these small problems
so I think people who are familiar with
dual decomposition it's a really
trending thing in NLP what people do is
that they have two big things two big
problems that can be solved using say
dynamic programming and then there is a
consensus step at the end but this is
very different from that because we are
we didn't break the problem down into
two big things that is not really
possible for this task we have many many
small things and then we try to impose
agreement so for each component now we
define a copy of that binary vector so
this is the superscript component so a
component is basically one of those
small things and each constraint in our
ILP can map to one component so there is
one component for each constraint in the
ILP so this is like in graphical models
language
each component is a factor okay in a
factor graph so for we have this Z
vector for each component so basically
we define this function called bolt
score that scores this entire binary
factor this binary vector and it uses
that roll span scored that we saw before
so given an entire z component vector we
have this function that gives us a score
now the ILP that we saw before can be
expressed in this way so we sum up over
all the components with this core and we
have this constraint where all the Z's
for all the components rolls and spans
they have to agree and that agreement is
done by using this u it this witness
vector U for consensus so basically if
all the Z's are equal to this u for the
different components we have a consensus
okay please stop me if there are
questions now so this is the primal
problem where Z are integer now we try
to solve an easier problem which is
primal prime which is where the integer
constraints are relaxed okay so it's a
linear program now we convert that to an
augmented Lagrangian function so this is
a new thing in comparison to the collins
work where we augment the Lagrangian
function with this term ok this term is
a quadratic penalty that actually is I'd
should be ideally 0 because the Z's are
equal to the U so it doesn't make a
difference to the primal the LP but it
actually brings consensus faster so
that's the reason why people use it so
if this is this is a standard trick that
people do in the optimization community
now this augmented Lagrangian function
which looks like this the saddle point
can be found using something called
alternate alternating minimization which
is again another standard trick in
optimization and what what is nice is
that the saddle point which we seek can
be solved using several decoupled worker
problems so the so it's basically an
iterative optimization technique
where there are three steps there are
Lagrange multiplier updates there are
consensus variable updates and there are
Z updates now the Z updates can be
solved at decoupled workers okay there
they can be solved in parallel now what
do these decoupled workers look like so
remember that one component is basically
one constraint so let's say 1z update
that we need to do is for each role we
have a worker that imposes a uniqueness
constraint so this this is an again a
small a tiny optimization problem and
that has to that solve the Z update and
it looks like this and let me just state
that this is just a projection onto a
probability simplex and it can be solved
using a salt operation which is really
fast and the challenge for a new problem
where you have such constraints and you
want to use our technique of dual
decomposition we have to define fast and
simple workers that can bring
optimization faster
okay so advantages of this approach is
that there is significant speed up over
ILP solvers and we don't need a
proprietary solver and the speed ups are
really marvelous we get nearly 10%
speed-up in comparison to SEAPLEX
which is a very strong state-of-the-art
solver okay and that is paralyzed and
this is not okay we get also certificate
of optimality like exact solutions for
more than 99% of examples so back to the
yes
numbers they're not for this problem for
dependency parsing we have numbers so
this was this paper by Martine set up a
TM LP but we haven't done any analysis
in comparison because yeah it will be
slower firstly we don't want to use that
because the subgradient approach of
collins it uses these two big things or
two big problems we don't you cannot
break up this problem into many things
there is there are some possibilities
but we haven't found any way like the
dynamic programming trick to find reduce
overlap remove overlap can be one big
component but I have no way to impose
those other small constraints in a big
okay so learning we use straightforward
learning we use local learning actually
to learn these weights by just taking
roll and span pairs we also have done
some experiments where we do global
inference using dual decomposition and
use a max margin trick that doesn't work
very well for some reason so we stuck
with maximum condition log likelihood
training so here are some numbers just
for argument identification we get a
precision boost of two percent using
joint learning and these actually these
numbers do not so we are doing better
than local inference but these numbers
do not reflect things like whether the
output is respecting the linguistic
constraints that we placed so we measure
those the local model makes 501
violations that include overlaps
requires and excludes relationships
while we make no violations yes
yes yes so we have some numbers about
that uses beam such these accuracies are
similar but it makes like five 50 or 60
violations
a great great question yeah we also
wondered about that so basically let's
say we are trying to there is this
requires relationship and the local
model predicted entity one and didn't
predict entity - okay now the the script
that measures performance it will give
us give the local model some score for
entity one on the other hand this model
is very conservative and did not produce
either entity one or entity - so it got
lower scores for that example this is no
this is just argument identification the
frames are given
well casual
that doesn't happen now then we can
change the script to do that but we
don't do that it is not really a partial
score it will so when the local model
shows entity one and it predicted it
correctly like mapped it to the right
span the the the script gave it some
points but this model neither predicted
entity 1 or entity 2 because it respects
that constraint that can be here that
that we haven't done
all right we can we can we can continue
caesarean
just as you state to
I won't return any of that that happens
actually
we call
we can we can discuss this more because
I want to have numbers which reflect the
better quality of okay yeah I'm really
scared about this this is a good point
because the reviewers will also point
this out okay on a benchmark so this is
full parsing like identifying frames as
well as arguments the again auto
predicates because like these these
systems actually identify the auto
predicates and gave us their output so
we do again much better nearly five
percent improvement which is significant
on given predicates we do even better
like 54 percent while on the new data it
is like 69 percent now we can get much
better than this
and we are training only on very few
examples in comparison to other SRL
systems and as more data becomes
available we are hopeful that this will
get better now the last part of the talk
is about semi-supervised learning for
robustness I am really excited about
this topic because I'm interested in
semi-supervised learning now this has
been presented by this paper from last
year's ACL but I've done more work on
this and I'm going to present it as well
so unknown predicates frame
identification I did a cura see is just
47% which is half of what we get for the
entire test set so on out of domain
predicates new predicates the model has
really where if badly and this is
reflected on the full passing
performance also because once you get
wrong frames your whole parsing accuracy
will go down
so what we do is that the nine thousand
two hundred and sixty three particles
that we saw in the lexicon in supervised
data we only have knowledge about those
while English has many more predicates
we actually filtered out around
sixty-five sixty-six thousand from
newswire English which can evoke
predicates yeah so these are basically
words other than proper nouns basically
and some other part of speeches which do
not evoke frames in the lexicon okay so
we used what we do is both interesting
from the linguistics point of view as
well as from an NLP task so we are doing
lexicon expansion using graph based
semi-supervised learning so we build the
graph of our predicates as vertices and
compute the similarity matrix over these
predicates using distributional
similarity and and the label
distribution at each vertex in the
language of graph based learning is the
distribution of frames that the
predicate can evoke so here is so this
is very similar to some work that I've
done with Slava Petrov on unsupervised
lexicon expansion for
POS tagging so here is an example graph
a real graph that we use the green
predicates come from the lexicon and the
supervised data and I'm showing the most
probable frame over each of these green
vertices similarity is on the right side
and unemployment rate and poverty is on
the left and the black predicates are
the ones which can potentially serve as
predicates and we want to find the best
set of distributions with a set of
frames for each of these predicates and
we call the green 1c predicates and the
unlabeled predicates of the back the
black ones and we want to do graph
propagation to spread labels throughout
the graph to increase our lexicon size
and this is the iterative procedure that
continues
until convergence so brief overview a
graph is like lots of data points the
gray ones are so we have this in the
geometry of the graph by this symmetric
weight matrix that comes from
distributional similarity for us and
there are high weights which which say
that these two predicates are similar
and low weights say that they are not
now these are 1r5 on the gray predicates
as the gray vertices are the ones which
are come from label data so we have
supervised label distributions are on
the labeled vertices and the q are
basically distributions to be found on
the vertices now we use this new
objective function to derive these Q
distributions so this is a some work
this is under review right now so
basically if you look closely what we
are doing here the first the first order
term in the objective function that we
are trying to minimize its meaning it
looks as the Jenson Shannon divergence
between Qt and RT for the labeled
vertices so basically for the observed
and the induced distributions over
labeled vertices should be brought
closer using the first term okay as the
Jenson Shannon divergence is basically
an extension of the KL divergence which
is symmetric and smoother the second
term over here which is more interesting
it uses the weight matrix to bring the
Jenson Shannon divergence of the
neighboring vertices distributions and
finally this is another interesting term
that induces sparse distributions at
each vertex so this penalty is called
the L 1 comma L 2 L L 1 comma 2 penalty
that was it's called the elitist lasso
in the regression world so basically
what it does is that it renews tries to
induce vertex level sparsity and the
intuition is that of a predicate in our
data
and evoke only one or two frames so out
of the nine hundred frames in the
distribution you only want one or two
things to have a positive weight and
rest of them should be zero so that's
the idea and we have seen that inducing
they're having this parsecity penalty
not only gives us better results it also
gives us tiny lexicons so the the
lexicon size will be really small which
can use an like store and use later on
much more efficiently than a graph
objective function that doesn't use
parsecity so the constraint inference
it's a really straightforward thing if
the predicate is seen we only scored the
frames that the predicate evoked in the
supervised data else if the predict is
in the graph we scored only the top key
frames in the graphs distribution and
otherwise we score all the frames okay
and this is an instance where
semi-supervised learning is making
inference much faster on unknown
predicate so instead of scoring hundred
frames which score only two which makes
the parts are fast so we get huge
amounts of improvement but still not as
good as the known predicate case so we
get 65% accuracy on unknown predicates
in comparison to 46% for supervised data
and this is reflected on the full
parsing performance also okay okay so
I'm at the end of the talk wait that's
distributional similarity just take a
parsed corpus with dependency parses
look at subject object relationships of
predicates then use point wise mutual
information and take cosine similarity
it's standard distribution of similarity
but great things can be done there also
you can learn those bits we are working
on that but we haven't got any results
on development data I cross validation
to so I turns out that if you fix one
you can tune the other one yes so if you
so an interesting so basically why we
diverge with from previous work is that
in label propagation people treat these
things that matrices and do updates and
we don't do that we take gradients we
firstly we make the distributions
unnormalized that's the first thing we
do so none of the distributions q are
normalized so that makes optimization
much easier for us so it becomes really
really nasty if things are normalized so
you can't I mean there has been some
very interesting work at u-dub on this
by amar Subramanya now we make thing
unnormalized and then if you take
gradients with each with respect to each
Q's component then the problem the
gradient updates become really trivial
so you can just operate on vertices and
make updates and it is trivially
parallelizable across scores also so
it's so on this graph with 65,000
vertices and 900 labels running
optimization on 16 cores takes 2 minutes
it's really fast
okay so it's conclusions we did some
shallow semantics any more questions yes
but do you get information
yes yes other relationships other soda
gang linz dependency corpus he has a
lexicon for similar he has adverbs and
adjectives there too that this is from
the 90s and we make use of that in this
work yes so with the graph construction
or the similarity learning if we can use
the framelit supervised data and learn
those weights that will be really
interesting but I have I have not there
is an undergraduate student working with
me for that problem like generic weight
learning problems or metric learning
problems for graphs and if we can tune
the distributional similarity
calculation for framenet the frames it
will be very interesting
okay so shallow semantic parsing using
frame semantics reach our output than
srl systems more domain-general than
deep semantic parsers and on benchmark
datasets we get better results in
comparison to prior work we do less
independence assumptions for just have
two statistical models there are seven
supervised extensions and we saw an
interesting use of dual decomposition
with many overlapping components in this
work lots of possible future work we can
train this so one trivial extension is
to train this for other languages these
six languages have same semantic
annotations use these techniques for
deeper semantic analysis tasks like
logical form parsers have this problem
of lexical coverage whether we can use
this type of semi-supervised the
extensions for like merging
distributional similarity and logical
form parsing in some way and use parser
for NLP applications so right now
actually our parser is being used to
bootstrap more annotations by Fillmore
and its team which is interesting
because it's it's closed as a circle and
gives us more data and the parser is
available it is an obscure task but
still 200 people are down have using a
problem and thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>