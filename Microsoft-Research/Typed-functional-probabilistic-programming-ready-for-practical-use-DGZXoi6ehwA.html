<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Typed functional probabilistic programming: ready for practical use? | Coder Coacher - Coaching Coders</title><meta content="Typed functional probabilistic programming: ready for practical use? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Typed functional probabilistic programming: ready for practical use?</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DGZXoi6ehwA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
my pleasure to introduce Tom Nielsen who
just out the blue got in touch with me a
few months ago via roundabout route
because he'd seen some of our videos to
do with that tool distribution model
iran on youtube or on the web somewhere
and then we had a really nice chat a
couple of weeks ago and I was really
struck by and what a multi-faceted
interesting sort of person tom is
actually the brief chat we've had this
morning has only reinforced that so I
didn't realize for example that he was
here at Cambridge as an undergrad he
started off doing maths in his first
year almost instantly switched to
engineering got a bit bored with that
switch to philosophy for a second year
got a bit bored with that and switched
to psychology in his third year then
went off to do a PhD in computational
neuroscience a couple of postdocs
including Harvard Medical School and no
doubt someone else interesting before he
decided to go all entrepreneurial and
develop some software which is called
Bayes hive to do with probabilistic
programming Bayesian statistics and
predictive modeling and all that kind of
stuff so he's a kind of a bit of a rare
rare breed I think Tom if you don't mind
me saying so anyway so I thought it was
worth inviting Tom here so I'm and
without further ado I'll let you let you
get on with the talk tom so thanks for
coming thanks drew and thanks for coming
I hope not to waste your time i'm going
to talk about some work we've done in
designing a probabilistic programming
language that's really built for
practical use for data analysis before I
get into that I want to talk a little
bit about a hidden assumption in in data
analysis or data processing and so what
I really mean by by data analysis is
that you've got your you start off over
here on the left with some data that
you've observed so this can be in a
variety of scenarios in science the data
may come from a telescope or microscope
or from an amplifier or in retail this
may be everything that you've sold or in
finance we have a lot of historical
pricing data but really you want to get
over here and
maybe you want some sort of insight out
of your data you maybe want to measure
some constants or some ratios or maybe
you just want to take a decision you
want to ask you know this is my data
what should I do next should i buy this
should I sell this what price should i
buy that or maybe you want to calculate
a risk what's the risk that we're going
to be bused in six months or you may
want to predict something about the
future or about some new case about
which you have incomplete information
and so now the hidden assumption in in
much of data processing is that the way
we do this is by setting up some sort of
transformational pipeline some sort of
computational pipeline so you might
start say this is some amplifier data
you might start off by filtering it with
a bessel filter love Butterworth filter
then you might do a fast Fourier
transform and then you might want to
look at that power spectrum and you may
want to measure how high and how wide
some of those Peaks are and then finally
you've got a bunch of numbers that you
can stick into a statistical test maybe
a t-test so really what you're doing is
reducing complex data to a handful of
numbers that you can interpret and I
think this is very natural to us and I
think most people do this without really
thinking about it but there are some
problems with this paradigm and so the
first one is that I don't know which one
of all of those possibilities I should
choose should I choose the best law
Butterworth also should I which corner
frequency should I use I want to come to
measuring how should I exactly measure
these peaks in the spectrum in which the
testicle tests should i use and i don't
think we really have a good way of
choosing these computational procedures
other than that this is something you
meant to learn through some sort of
apprenticeship through the elders which
is the same as saying that we really
don't know
but this is a particular problem in data
analysis when we have a arbitrarily set
conventional threshold for what is
deemed publishable that is the five
percent significance level because there
is a flexibility in these parameters I
can change the corner frequency of my
filter and I will get a different
response at the end a different final
score and so if I don't get the results
that I like I can just choose some
different parameters and I can try again
and eventually I might find something
that is deemed partial publishable and
this sounds awful but this is something
that people do so so it didn't come out
well but as a paper here which shows
that undisclosed flexibility allows you
to present anything a significant and
there's lots of evidence that in
industry research where you have to
reach some threshold for approval for
efficiency for instance for drugs or
medical devices this kind of flexibility
is widely abused now so the second
problem here is that in s computational
scientists we we love to to write models
and what these models do is they go from
some parameters and using a
computational procedure they generate
fake data and if you can do this well I
think that shows a lot that that you
know a lot about the underlying system
if you can do a faithful simulation and
yet this picture has no place in that
transformational pipeline they're
completely separate activities of course
you can analyze the simulated data and
see whether these parameters that you
use can come out at the other end but
there's no real principled way to do
this in general which is a shame
and so this third problem which is
really the most serious is that it is
really difficult in this view to
propagate uncertainty correctly so the
raw data has its own variability and
noise and that means that the final
numbers that come out at the other end
should really have some kind of
uncertainty some kind of error bar our
bar and to do that is is really
difficult you need to you need to take
the partial derivative of each
transformation and that's really hard
right so what's the what's the partial
derivative of the Fourier transform with
respect to to one of the the corner
frequency of the filter that you use
that's really hardened people end up not
doing it in practice so you end up not
having that uncertainty or you end up at
least dropping the uncertainty from the
earlier stages of the data analysis
process so you might ask is there an
alternative to this view is it possible
to do data analysis in a different way
and and yes it is so there there is an
alternative it is a probabilistic
programming and it is what I'm going to
to talk about for the restaurants talk
so the idea here is that at least
initially we we take this idea of the
model and place this on on a pedestal
and all the analysis will flow through
that model and this model is somehow a
mapping from the parameters to the
observable data I'm not saying it's a
function it's some other way of getting
from the parameters to the data of
generating the data from the parameters
and then by virtue of working in a
probabilistic programming environment we
have a procedure some way of inverting
that process so we can go we can flip
this arrow and we can somehow go from
the observable data to the parameters so
we can learn what the underlying state
of this model is
giving the data that we have actually
observed the real data and so I think
it's important not to stop here and
consider everything else that you can
also do so based on these parameters
that you have now obtained you can now
do a variety of tasks of instance you
can forecast the future you can
calculate the probability that something
awful is going to happen and you can
take decisions if you can define a
utility function and these decisions the
outcomes the actions chosen from this
decision procedure in this way are
provably optimal if you have the right
model now we never really know whether
we have the right model so one thing
that we also want to do is to somehow
figure out if this is a good or bad
model and this should also be somehow
possible in a probabilistic programming
environment so but that may be other
things that you want to do that I can't
think of so we better make sure that
these are not primitives that these are
built using primitives that you can then
use to define other kinds of operate off
of transformations the other ways of
exploiting this parameter estimation
risk only accept a model and not when
the parameters and not also some
predicate saying what is the condition
are you do we're estimating oh yeah it
does it does so so sorry yes you're
absolutely right so you want to to then
define some function from maybe just
from the parameters to a probability
depending on what exactly how you're
calculating risk or you may actually
want to preda gate this risk on the
future so you may want to define it on
so different ways there will be
different risks and you'll calculate
them in different ways
okay so the only thing that these are
actually quite easy to define they easy
to calculate sometimes computationally
the only thing that might seem a little
bit magical is is this estimate
operation so you might wonder how you do
that and so actually we've known for
some while how this can be done so they
all depend on these two methods depend
on defining a likelihood function and
that is the probability of observing
your data given the parameters so if you
can write some sort of expression maybe
a function for the probability of that
data given that these parameters are in
scope and that you pretend that they
have concrete values then you can run a
maximization procedure and find the
actual parameters that maximizes this
probability this is a principle that you
can adopt if you like and it comes out
with a set of numbers and you might also
get out some sort of uncertainty but it
is not a probability distribution the
advantage of using Bayesian inference
that you do get a probability
distribution over the parameters given
the data and so since our model has to
be written as a probability distribution
that means you only have one kind of
thing and we know what kind of thing a
probability distribution is so that for
me that's the chief advantage of using
Bayesian inference you get the answers
as a probability distribution over the
parameters given the data but it is a
little bit more complex so it depends on
the additional term which is the prior
which is what you already know about
these parameters and so you do have to
define that and this is so complex that
we you should just ignore what which we
can fortunately okay so before I sell
this too hard I think we should also
think about when when do not want to do
this so you may not have a model at all
you may have some data and you may not
be in possession of a model so you may
want to do some exploratory data
analysis in order to formulate a model
and then when you have a model it may
not be efficient to estimate the
parameter
is in this way it may be efficient and
that's very hard to predict how
efficient that's going to be however it
is unlikely that you can do this and yet
it doesn't really help you much in
whatever data analysis problem we have
very often knowing the parameters will
give you crucial information to give you
whatever insight or whatever estimation
or forecast you want to do based on the
data prediction another reason why you
might not want to do this is that the
technology is simply not up to scratch
yet so you can do something like this so
this is some bit from this is a bit of
code from all from the programming
language are and so I've got a piece of
data here this is the time series data
and I can run the ARIMA ARIMA stands for
autoregressive integrated moving average
to the time series model against these
data and then I get the results back so
the only things I have to type on this
slide to do a forecaster these two lines
is that you answered only two lines of
code to do something now I can i can ask
what the results are and i get a bunch
of acronyms that some people know what
means and i get some parameter estimates
he said that that's this one right and
then I can use that result to do a
forecast and I get then these are the
next points into the future with their
confidence until also or spreads or
standard deviations or standard errors
now very nice that this is two lines of
code but actually that that tells me
that it's too short these are black
boxes and they're not very flexible so I
cannot fit any model I like here this is
a very restricted class of models and
this forecast you know might know
something about the future might know
that something will happen more might
think that something will happen which
is not in the historical information I
might want to incorporate that in
forecast but there's no flexibility for
doing so here
so I'm looking for something that's
actually a little bit more books than
this that allows me a little bit more
flexibility than just a black box
forecaster so you know let's see what
this error in my model is so it has
three kinds of terms the autoregressive
term integrated moving average tab I've
gotten all the integrated and moving
average sound a little bit more
complicated but they roughly follow the
same same form as this so these numbers
up here they tell you how many terms you
allowed of each kind so one of those is
the number of auto regressive terms and
so if I have one auto regressive term
then what my model says is that the next
mod the next time the next value in the
time series is distributed by SS the
number that that you get by multiplying
the previous time step with a parameter
this is a parameters is a number real
number and then adding a randomly chosen
value and if I get to do to auto
regressive time steps then I get to look
too bad right so this is the next time
step this is the previous one and this
is the previous one before that I get to
multiply these two by parameters of the
model and then draw a normally
distributed error term and so but even
when you incorporate bees and the
integrated moving average integrated
moving average times you still have a
pretty narrow restricted model for
instance I'm always stuck with a linear
combination of terms and I'm always
stuck with this normally distributed
error term which may not be what my
model is what if I wanted to do
something completely crazy yeah if I
wanted to say this is really crazy right
so the next time step is drawn from a
gamma distribution where the two
parameters the scale and location
parameters of this gamma distribution
are some crazy nonlinear functions of
the two previous time steps yeah if I
ever wrote this on a statistics exam I
would fail for is there probably is no
process in the world that is described
by this equation I mean it's it's it's
ridiculous but I think if we're
providing software for data analysis for
modeling you have to let the users X
power
in with the grotesque and you so if
nothing else then because then they can
figure out from themselves why this is
wrong if it is wrong so well probably
this probably I mean I think you should
be allowed to do this and run this
against your data but I think this is
probably not going to work very well
because there will be no continuous time
approximation for this time sir so and
heavily on your discretization also the
basil is no no I'm just saying there's
no process there's no physical process
out in the world that is described by
this by this equation yeah it this is
not a law that describes anything in
society or in physics it might be i
might be wrong right I think you should
be allowed to find out i think you
should be allowed to enter this and and
and and and find out if it is wrong
always this is exactly the kind of thing
that starts editions don't care about
but it seems to be like that's the one
thing they never ask is actually is much
mobile this does it correspond to the
mountains of reality right right okay
your eyes also there's also no physical
process that influenced the first two
that that may be true okay now so i
think the first values did the first in
data analysis environment that i know of
that that allowed you to do crazy things
like this and allowed you to run any
model against your data was the wind box
software the box software was actually
written hand cambridge at the MRC
biostatistics unit and so this is an
implementation of that crazy model so we
have got my mic gamma here so that's
that's the you drove the exit the next
time step is drawn from a gamma
distribution so here the tilt means
distributed that so that means that this
variable is distributed as a gamma
distribution and that relies on two
local variable cylinders these are
deterministic calculate calculations
right these are not probability
distribution this is just a simple
equation I just say I'm not really sure
this is really valid wind box in fact I
think
started closing parenthesis just type
this out but this is roughly wind books
with bugs implementation of that of that
process there's a so these are these
calculations and they get put in these
local variables or deterministic
variables and then use that to calculate
the next step of the gallant versus an
au pair as your prior yeah these are the
distributions for you four parameters
and so you can really write probably any
or a huge range of models in this
language because you can say that any
variables distribute according to any
probability reason you can do any
calculations you want to unfortunately
that the only thing you can do you can
only describe models for 444 data that
you've observed and so you cannot do any
of the operations that I showed you
before forecasting risk estimation
prediction decision you can do some of
those by doing some tricks so you can
sometimes add an extra parameter that
corresponds to your forecast and you can
add that in this way here but I think
you know what we're looking for is not a
bag of tricks here that you can stretch
this with a little bit more we're
looking for something gives you much
more flexibility in the kinds of models
that you write and so so we have to look
somewhere else for some inspiration and
and that is looking at functional
programming and and realizing that
statistical models are magnetic programs
so in functional programming we try to
calculate with functions that really are
just mathematical functions so functions
here are pure mappings between two
different sets of things and so for any
given input you always have to get the
same output that's one way to define a
pure function so we write this F here
which is the arrows as the type of going
from the Diamonds to the stars the type
notation for that and so no matter how
many times you look at this arrow here
you always get the same output for the
same inputs and so for instance when
you're writing a function called prompt
this is the prompt from JavaScript for
instance that asks the you
so for value like this could be how old
are you and this would be the users
reply that prompt function cannot have
this type because for the same question
the user might give you two different
answers so that the way you do that
instead is to say that that this prompt
function takes a string which is the
user which is displayed to the suit is
the string which is displayed to the
user how old are you but then it returns
an action in in the in it returns a
string but wrapped in an input-output
operation so that you cannot for
instance then write print hello plus
prompt what is your name because this is
not a string this is an input-output
action which will ultimately yield a
string so functional programmers some of
them like to tell you that it is
extremely powerful that you cannot write
this and indeed this can be used through
some spectacular effect because you can
generalize over these you can write your
loops for instance but I think the jury
is still out on whether it is always a
good idea to always restrict the user to
this this style of programming where
this is banned but i'm going to show you
that when considering statistics and
probability distributions this is a
really good idea so we define a type of
probability distributions as a random
number generator that is a function that
takes a seed and then yields a value of
some type and then we implement what's
called the Monad instance for this and
what it means to be a monad means that
you define a sequencing operator this
bind here in red and what that means is
that you can draw a value from one
probability distribution and then you
can send that to a function and then you
get to use the value that has been drawn
from that probability distribution
as if you knew exactly what it was so
inside this function x-scissor is a
number there is a real number and so you
can sequence draws from probability
distributions using this operator let
let me show you how that would so I
don't know how many of you are used to
looking at gamma normal distribution so
here's some pictures of them so the
gamma distribution is sort of gives you
always positive numbers and in this case
it looks like something that's likely to
be small and more less and less likely
to be higher a larger value so so this
is a histogram of values drawn from the
gamma 11 distribution if you change
these numbers a little bit sometimes it
can look like a bell curve here but the
gamma distribution always yields
positive numbers a normal distribution
you probably know most of you this has 0
mean and SNS are a variance of one and
so this is the the shape of the famous
bell curve and now if I sequence if I if
I compose these two probability
distributions in this particular way so
they say yeah such that i used the x
that has been drawn from a gamma
distribution as the variance component
of the normal distribution then i get a
histogram that looks like this now this
is a little bit torturous to work with
so fortunately the functional
programmers have invented a syntactic
shortcut this is some tactic sugar and
what that means is that you just
basically flip this bit so you're saying
you pick a value X from gamma here and
then in the rest inside this do block
here you get to use that X as a real
value as a sres is an actual number in
the computations that follow so this
becomes a little bit more readable than
this but it really means the same ok so
i want to show how how we can use this
to build a real statistical model for
example example so let's pretend we are
economists and we've got some data or
were working we want to simulate some
data first
for the relationship between inflation
and growth and so we want to generate
these kind of points that have a steep
slope eye donation which way it should
actually go but you know we're just
simulating data here so I'm going to
build up a real statistical model for
this kind of data so we're going to use
this distant ation here so first I'm
going to pick a inflation from a uniform
distribution and then I'm going to
calculate the growth from a normal
distribution with mean is calculated
based on the inflation that I've just
picked and the slope and a an offset
this Grove 0 is this deal intersect with
you x axis of the lined fits the state
these data and then as a variance and
then I'm going to return the inflation
and the growth so I have a point now
return sort of in a certain way it shows
it tells you what the output of this
sequence of computations is some of you
know that it's a little bit more
complicated than that but you can use it
in that sense you can use it to say with
the output of this these computations
are so this gives me a probability
distribution for one point because I've
only picked one inflation and i've
calculated one growth picked one goes
from a calculated normal distribution so
I want a probability distribution for
all of these points so for instance I
have a function called repeat which
takes a number that number of
repetitions then it takes a base
probability distribution and then it
gives you a probability distribution
over lists of things right now this was
this base distribution here was the list
of all points so I have a list over
points of things here ok so now I have a
distribution of all of these points now
the problem here is these parameters
parameter there are three parameters
slope the offset and the variance and
promise how are they going to come into
scope looking at it from a programming
language theoretical perspective right
here if I if I ran this then this would
fail because there's no slope in scope
that that variable is not accessible
it's not defined so it could just be a
global variable but that's not going to
work very well because
we will later want to go and estimate
these variables but but this would work
if you just wanted to simulate data they
could also be arguments so now my model
becomes a function from these three
parameters to a probability distribution
over lists of points and and I think
this would actually work pretty well if
you were building a framework for
maximum likelihood estimation I would do
this i would say that models are
functions from the argument from their
parameters to to the data that they want
to generate but i'm not i'm building
model for bayesian inference in the
previous slides i kind of thought that
the is this monadic approach will be
that either it's got an analytical way
or if somehow automatic you're choosing
the sampling way that it that it
complaints all of these distributions to
produce the distribution at the end but
this seems to suggest with the repeat
100 are you now saying just sit is it
simply taking 100 samples or is that a
bit more to it than that this is four
ounces of it I mean this in a certain
way I mean it's building a yes it this
is a very when you're just looking at
generating data it is a very simple
interpretation and it is just I mean
taking is not taking 100 samples it's
building a probability distribution is
creating a pseudo going to certain 100
it's creating a new probability
distribution over tho those datasets
it's not really sampling anything you
need to sample from this because it
ended up being a probability
distribution if you use this hundred is
not a number of XY pairs and you'll see
that later that will come later that's
then that would be the number of points
in the simulated data set that would be
100 I see right where is how it's
actually doing the completing the
distributions is is not to do with that
that's separate right it could be done
relatively or it could do it you could
have different interpretations of this
Monette but here we are actually doing
things in a very simple way and i'll
show you that when you estimate things
get more complicated yeah so here we
really are just simulate
data points okay so so one thing you
could do for the Bayesian inference is
to put these parameters inside this this
do block so you pick the parameters
first and then you use them so now we
have a probability distribution over the
parameters which is what we needed to do
base in inference for prior and we this
is also a self-contained computation but
you now have no longer have any
parameters you can just run this it will
generate a slope an offset and a
variance and you'll generate some data
this particular way isn't going to work
because I pick 100 different slopes I
want to pick just one slope and
calculate my data based on that so so
with what we'll do is we'll put put
these drawers of the parameters outside
the do block and then we will pick so
you can read this as a computation right
you can read this as an imperative
program that first generates a normal
and an another normal and a gamma
distribution and then generate the data
here and so you have you have a complete
procedure here for for generating fake
data and this is this is this is what we
want this is the final formulation of
that apart from sorry I didn't thank you
i forgot to say i've changed the arrows
to tilt to avoid confusing statisticians
because here the arrow means
deterministic assignment so i but yes
apart from that it is entirely haskell
syntax okay
however in the real world it's not
enough just to simulate fake data right
so we have so much real data out there
in the world and we don't just want to
generate more fake data we want to
estimate and so the question is how are
we going to do that how are we going to
use this procedure to estimate these
parameters and so I should probably now
introduce the language that we've
developed it is called basic which is a
probabilistic functional programming
language I didn't make this up for the
occasion I promise in the second slide
you pointed out the model was one of
these funny functions or vanity
functions from parameters to the
observable data so it seemed that you
have committed on your previous design
where you you were setting the
parameters know so so this is so as I
said it is not a function the model is
not a function yeah I said it was a
funny function so actually what what
you're doing is you're generating those
those parameters internally in the model
so it's actually the model is just a
probability distribution over the final
data and it generates its own parameters
internally and the show you later how
you can change them so what we want so
let's say we have the data the observed
data and we get that somewhere from some
database or some other way we have this
in scope and we've defined this model so
what we want now is a joint probability
distribution over the model parameters
and by joint I mean that they are
estimated together and that you retain
information about the correlations and
that's really important so here I should
have done that I fits it some some some
lines to the data points and you can see
that these lines are not all parallel so
the lines that have the greatest
intercepts with the y-axis have the
shallower slope and ones with the lowest
intersect with the x-axis the y-axis
have the steepest slopes and so so what
I mean by that is if you squint and look
sideways
looks like a bowtie right it's wider
here narrow in the middle wider here and
so when you look at the parameter
estimates so these are the parameter
estimates and every dot is a possibility
that these parameters could be a
consistent possibility and so you see
that I guess Australia Day their
probable values if you see that if the
growth if this x-intercept sorry y
intercept is high then the slope must be
low and so this correlation between
parameters is really important if you
start calculating things from these
parameters estimates and and you know
you have to do this if you're doing it
correctly so when we go and estimate the
model parameters based on the data that
has to be returned as a probability
distribution over something it cannot be
returned as three different or two
different probability distributions if
these were repair returned us two
different independent objects then you
would lose that correlation information
so the way we did this we said first of
all for the first condition is the data
has some type T in this case T is lists
of pairs of numbers the model is a
probability distribution over that same
type list of pairs of numbers and we
give you then a and operator that we
have to add to the purely purely
functional programming language called
estimate which takes us argument to the
model the data and gives you back the
parameters and so the type of the
paramus the type that is given by
estimate is a probability distribution
over records and the names the field
labels of these records are the
parameter names and and this is not vary
from a programming language perspective
this is this is weird because these
these are our record labels and these
are just arguments to functions really
that was they were they were the
arguments when you flip the monadic do
the bind operator
is the argument to that function so
there's some kind of you need some kind
of meta programming facility to set this
and yet I i I'm rather convinced that if
you want to do it this way that if you
want to have a procedure called estimate
then you you have to return this for for
using if you want to use this
practically for large models models that
are much larger than this you have to do
this you could return a probability
distribution over vector over vectors or
tuples but then you would quickly lose
track of which index is which parameters
when you when you have a large model and
also remember it's some of these models
are nested so some of these parameters
that are estimated to come inside lists
and you want to you want to keep that
information well here is like a set of
samples I yeah okay a good question so I
showed you earlier a definition for prop
which was a program yet that's right and
and that doesn't actually work
practically so let me find it where were
we here we are and let me see can
actually maybe I can just go out of it
when we hear so actually what what you
get is in for practical purposes you
actually need to have a second
representation for probability
distributions which is a set of samples
that becomes a set of eight and that
that you have to do that to do efficient
manipulation of the output of estimate
okay okay and so you can you can ask for
word for this value you can ask for it
at the console and so the IE for those
of you know haskell you can say that it
has to show instance and and so when you
asked for it you get you get it sort of
gets displayed as a record of
probability distributions but that's
that's just a way of typesetting money
it's very difficult to to show the user
in textual form and mostly dimensional
probability distribution so so for that
summary when we output at the console
you just get the mean and the standard
deviation but in this parameter is
retained all the correlations and all
the skew and all this is just a summary
but but notice how there aren't any
acronyms in here and and all we are
doing is getting the parameter estimates
and we're trying to do data analysis
just by doing the parameter estimates
nothing else from say inflation what is
this my model it takes another parameter
outside of the repeat but that's all up
to me apparently double should be part
of the model so how does it know what
what I want get out of parameters and
what I want to keep inside the model
yeah because this is absurd yeah so your
data is a list of pairs of numbers and
so this is this is observed both of
these two are observed and they help to
shape the likelihood function by being
observed um the fact that this is
observed actually had no information to
likelihood it just change the offset
because
this has no further parameter it's in
the data yes yes the monistic function
and then turn over vastrapur okay okay
good question so this does not work for
every single function every single model
you can work work because we're doing
Basin inference there are restrictions
on what you can do so what we say is
that the data the final observation has
to be a simple company combination of
drawers from probability distributions
and what we mean by simple combination
is that it is either a direct draw from
a probability distribution or it is a
pair or some other constructor does
build up yeah any kind of constructor
but and so so we don't support that you
do arithmetic operations on it that
wouldn't principle be possible so you
could do invertible any invertible
function you can do you could in
principle apply here but but we haven't
done the facility for doing multi
components of the model with things
depending on other things and all of
your data might consist of you know
combinations ratios between and so on
things in that model as long as you
supply enough data to sort of well given
you've given the data that you supply a
bunch of these things will drop out as
being in the data that's correct and
those that are left you say well they
must be the parameters to the app that's
practically so actually so actually i
applied i'm sorry i wasn't going to make
a compass really have to finish soon
this isn't necessarily true this can
this restriction that these two have to
equal can be loosened and that can be
used to great effect if you're doing
practical difference so for instance if
this is a list of records then you may
not want to and so this may be about
people and one of those records who
might be the name and I don't want to
build a statistical model for people's
names so you you might just this record
might be
moeller in some sense and then you can
you can run this so there's a special
relationship between Jesus we call it
that they have to be structurally
similar and there can also be too much
information in the model and then that
becomes unobserved and it becomes
estimated in crumbs with the parameter
of the model so I'm gonna have to make
some hard choices about what i want to
say now okay I'll do this very quickly
so one of the most of for me at least
beautiful consequences of taking this
magnetic view is that you see some
things that are very powerful but
looking at correspondences between
functional programming and statistics so
it turns out that also regression equals
unfold em so unfold is a classic
Combinator from functional programming
it allows you to build lists so you
build lists of a's by having this
function which takes an accumulator
which is not in the final list and for
each step it generates a new a and
that's the one that gets put in list and
it keeps track of this accumulator
throughout time and you can use this to
build up lists in various different ways
here's a complicated way of counting to
ten and here's an even more complicated
thing where I'm incrementing the
difference between lists over time why
would you want to do this probably
because you've been asked to for your
first year computer science homework I
don't know why else you would want to
use this but it turns out for statistics
this is really powerful
a great direction equal to some fold yes
so I look at the magnetic equivalent of
this we are again building up lists of
things in them in a monad by running a
computation in a monad and you can think
of this when we switch to the
probability Mona you can think of this
as a hidden unobserved state which is
not in the output and it turns out that
this generalizes time series data
analysis so most of all of the
discrete-time time series techniques
auto regression erima dark all of all of
the ones alien of them can be written in
some way using this unfold em company
turf or different instantiations of this
function so for instance here's our old
friend sorry al one this can be written
using a simpler version of unfold them
where we just iterate over things and we
just look at the previous step and the
next step is drawn from a normal
distribution where the mean is
calculated from this from the previous
step multiplied by a constant if we look
at our friend al to where we're allowed
to look to back would you need to full
unfold m and so the state you are
accumulating over becomes the previous
step and didn't end it the previous two
back step and the previous step and so
the next step is just calculated from a
normal distribution where the mean is
calculated by this linear combination of
the two previous values and then we
return first the next observation and
then the next accumulate accumulate so
state here and then that X next becomes
X proved 18 proved one becomes X proved
to on the next step and so you can write
I don't know if you can write all
discrete time time series analysis
techniques using this way but you can
certainly
a lot of them and this works with
estimate so so then you can you can then
estimate these parameters here all right
Sam I'm going to have to skip so this is
what they look like and we have to skip
how we build differential equations in
two basic I want to just quickly show
you how we we use the output of estimate
and so here our we have two friends here
one is is f map which allows you to
transform a function transform a
probability distribution by applying a
function to every value in it and so we
use this a lot for for instance for if
you want to do risk or if you want to do
any calculation based on your parameter
estimates then you can use you can use
this function so for instance let's say
you've got your parameters from estimate
here and that is a probability of a
records where there's a slope and an
offset and then you want to know if the
slope is positive what's the probability
that the slope is positive and so here
we just use F map and here this deter a
function which will then be a record
which is a parameter sample and then you
just say is a positive let's get the
syntactic sugar that makes this a little
bit easier second friend that we have
here is update and this is it again a
little bit more magic and that needs
unfortunately metaprogramming innocent
in this sense and so here you have
update takes one probability
distribution and model takes a
probability distribution over records
that can come from anywhere and it gives
you a new model where the parameters are
changed such that they come from this
probability distribution so for instance
here what I can do is if i've estimated
some parameters I can take the models
that have estimated parameters for and
update that model with the parameters
that were estimated and that gives me
it's called the posterior predictive
that is the probability distribution
over fake data sets that have been
generated using the model and using the
parameters that have been estimated from
the model so there's like a
very nice technique for you using for
for checking your model so what you do
is you you sample some fake data reflect
lots of it and then you try to probe
ways in which the real data is
inconsistent with the probability
distribution over fake data sets that
have been generated through your model
and through the parameter estimates that
come from the estimation of the real
data would look like the Delta X and I
would have exactly the same as a sample
it's not distribution and then then you
could then go on to repeat these yeah
you could actually up so you could you
could then compared is directly without
sampling from it so you couldn't use it
yes no maybe my model fade data look at
the pars to you can pair of pants two to
pause and see whether you could do that
and see whether you get an unbiased
estimate you can do you can do that you
can do that yeah yeah also all those
things let's skip the way in which
there's no escape from uncertainty so
let me just quickly show you how this a
little last bit so how we've built a
platform around this and and so what
we've done here is we've built a
environment in which you you write basic
code let me show you the document that I
generated here for for these talks so
what you do is you you first of all you
have an editor where you can write code
here and what you mix English language
written in markdown hello world with
code but then you also don't have
questions and so some of those questions
could be the probability distributions
here this is a probability distribution
and so you ask that and then it shows
what i showed you before where were you
well show you in a second but some of
those questions in this document which
mixes English code and questions can be
plots and so you can if we view this you
didn't get the plot here and so this is
the plot and this plotting library I'll
else read on the second has been written
this code here the way that these plots
are defined has been written to
understand probability distributions so
you can plot probability distributions
so here is
see I've got a really simple one I know
I cannot have a really simple one but so
here's the output here i'm plotting one
of the parameters from the estimation
but but you know i could also open so i
could also just type but a probability
distribution to type this plot normal 0
1 and i get i get that probability
distribution typesetters i sa as a
histogram now so then you over here on
your left you've got data so you've got
various different kinds of data sets
that you can visualize so and then you
can do plots of them which is all very
nice doing scatter plots and histograms
of your data sets but the real the key
thing here is that we would try to make
it easier to build models and so there's
a lot of a GUI a wizard-like thing here
for building models so I believe that
this is somewhat similar to something
you're working on and so I can hear
build OOP I don't even know the back
button works what it does you you can
build here as I now i'm building a
linear regression model for this data
set and i am going to say that the
output is predicted by the capital here
and so i can set my priors and see if
this works and so now i can just change
my priors here for these values oops and
change them around and stuff and the
camera is always fun to play with you
can see here how so as I change this
around yeah so now I get something
that's slightly different i can just
drag this one out let's let's do let's
do this window and now i can do various
different things i can insert some tests
so there would be like saying is the is
the slope greater than 0 or something
like that into various plots and and now
I
I can then just paste the code for
defining all of this that I want to do
let's do some plots let's do this test
whether this is true and then I paste
this into at the document that I was
working on before and this is now
running this would be at the end and so
here's the code for the model that I've
built here's the test is the test it's
greater than zero 99% sure that this is
right and then here are some of the
plots like so data plus fit and so this
code is generated by by by clicking
through a GU Rica let me see if I can
get back to the talk yeah okay so let me
just say that the graphics here that
we've done is we've built a plotting
library for this where where we've used
angular and and angularjs and d3 to
create our own HTML plots this is really
nifty so you can just define a plot here
in the HTML code just by these
extensions plot extensions that we've
used them so we would go through this
this is called radiant let's open source
and so the way this works is that we've
defined in the basic language would find
plot combinators that correspond to
various different things and you can
then nest these plots arbitrarily the
way you want to let me skip that and in
fact let me just a job to thanking some
people so in Ross built lots of this
Garrett Tobin is working on some of the
a markov chain styles and so would be at
we're building a he's a bit article as a
practicing psychotherapist and we're
building some facilities for calculating
changes in his patients mood and fitting
random walk models to that that's a lot
of fun and then we thank the people at
Leicester University who have helped
project and Henrik who is on the board
of the company thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>