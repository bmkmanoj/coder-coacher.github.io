<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Finding Dense Subgraphs | Coder Coacher - Coaching Coders</title><meta content="Finding Dense Subgraphs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Finding Dense Subgraphs</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nX3-Og8OZW0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay good morning everyone so actually
in know all of algorithms finding then
sub graphs in the graph is one of my own
favorite topic so I was delighted to see
the title over details talk please thank
thanks luck you what so I'll be talking
about finding dense sub graphs and some
other problems that are related to it
and okay so the general theme of what
I'm going to be talking talking about is
that you're given a huge graph and it
could arise out of a variety of
situations and we want to find induced
subgraphs which are dense and in a vague
sense it has applications to a lot of
things for instance detecting
communities in social networks is a
natural application and also it's been
used for detecting what's called link
spam in web browsers okay so and of
course it has a clustering feel to it so
it's used in a lot of clustering back
problems so let me describe some of
these a little bit before we move to the
actual problem so okay so first is
example I'd consider social networks so
we have these graphs that arise from
like friends of people and things like
this and so usually finding dense sub
graphs here means you're finding
communities let's say people who belong
to an institution and things like this
and they usually have much more edges
inside the community than they do to the
outside so this is one kind of an
example where you'd want to find then
subclass and finding such things are
useful because you know maybe they all
have some common feature that you would
like to exploit and the second example I
spoke office what's called detecting
link spam so this has been used by some
researchers in for some are trying to
avoid the problem where people
keep linking to each other's pages just
so that their page rank improves so one
way of detecting this is to find small
sets of vertices that have too many
edges among them so then they're
probably just trying to fool the system
and one one thing about this example is
that it's different from the earlier one
in that you really have a small you
really want the sets to be really small
ok so maybe there are big organizations
where there are legitimately pages
linking to each other and you don't want
to find those ok so how do we formalize
such a question it's so here's a first
cut so this is a problem that has been
called the max maximum density sub graph
and the problem here is so you're given
the graph and you want to find the sub
graph H so that you maximize the ratio
of the number of edges to the number of
vertices yeah so this ratio we call the
density throughout this talk and we want
to maximize this and turns out that this
can be solved efficiently yeah so in any
graph you can find the sub graph that
maximizes this quantity and it's non
since the 80s and I can give you the
rough and so it's based on a flow based
it's a flow based algorithm and there
are also very fast algorithms that have
say approximation ratio to for this but
so note that there is no restriction on
size which turns out when people
actually care about there are some
applications in which we don't really
want we don't want to output graphs that
are too big so one one way of capturing
this is to have explicit restriction on
the side this was a problem i'm going to
consider more detail is one where you
want to find a sub graph age or at most
K vertices and you want the number of
edges to be as large as possible so it's
a very natural problem and it's what's
called the densest k sub graph problem
and as you can see the key here is that
it's some natural optimization problem
you have the small support constraints
you want on the cave
so too and these are it turns out this
is a general challenge to handle
constraints of this nature okay so so
far we've seen practical motivations so
what about motivations inside theory so
here's a problem that has recently been
studied in connection to the unique
games conjecture so it's what's called a
small set expansion problem it's really
simple to state it so you're given a
graph just off average degree D let's
say in fact let's assume it's regular
and has degree D and you want to
distinguish between the two cases the
first is when there is a small set let's
say of size Delta and Delta is given to
you in which almost all the edges are
actually inside ok so 99% of the edges
in from inside this actually stay inside
in the second case your promise that for
any any H of size at most Delta almost
all they just go out ok and so this is a
promise problem and you want to
distinguish between these two cases and
it turns out that we don't know how to
do this so the small cell expansion
conjecture so it says that it's hard to
distinguish between these two and it
turns out to imply the unique games
conjecture which is something everyone
is interested in game but as you as I
stated it it's easy to see that it's
actually a sub case of the densest sub
graph problem because if you can solve
densest sub graph you can solve this
efficiently so so it is it strictly it's
strictly easier than lenses I grow but
we think that I will try to convince you
during the talk that dense a sub graph
is a much harder problem than this ok ok
so what do we know about densest sub
graph and
let me point out first the negative
results okay so we know it's NP hard
because generalizes click or trivially
and so it's been shown that there is no
polynomial time approximation scheme so
you can't get a 1 plus epsilon
approximation for every epsilon so this
has been shown and further it was proved
that if you assume something more so if
you assume something more than NP
hardness it's what's called a random
3sat assumption so then you cannot
approximate it to a factor better than
1.5 okay and the first voice first is
you just wash code and others to refi me
and okay so what do we know with respect
to algorithms right so the best known
algorithm before what I'm going to say
is for something just close to an end to
the one third factor approximation okay
this is due to fire code sales and
felling around 15 years ago okay now
notice the big gap between the two
results right on one hand we can rule
out a factor 1.5 approximation even
assuming some fairly fancy hypothesis on
the other hand we only know something
like an end to the one-third
approximation and and this is true for a
reasonably natural problem right okay so
what do we okay so that's what we know
if so then what do we conjecture is true
about this problem ok so we conjecture
that it's actually hard to approximate
to some small fire to some factor n to
the C so we think that the algorithms
are actually tightened our machinery to
prove hardness is probably not as well
developed and in fact so one of the
messages in the stock will be to say
that beating even a factor of n to the
one-fourth is a difficult challenge okay
so there is a natural barrier a 10 to
the one-fourth is what I want to see
okay and the other point is that this is
a problem which even seems to be hard on
average ok so I'll describe a
distribution over inputs or which we
don't know how to solve this problem
and so all that all the known
algorithmic tools fail and it turns out
that people have even used this as a
complexity assumption okay so they
assume so if you assume that denser sub
graph is hard then you can show some
other thing okay so so in general if a
problem is hard and out it's something
that algorithms people feel sad about
but you know there are some people who
feel happy about this and turns out that
cryptographers one such bunch okay so
and so so these systems there is a
public key cryptosystem which is based
on the average case hardness of densest
sub graph okay and so how does this
what's the rough idea so the crypto
system uses a bipartite graph in which
you have some I just going across and
somehow the den sub graph hides the
private key so you can think of a graph
is being publicly everyone knows the
graph but unless you know the dense part
you can't quite decode so this is a I'm
lying a lot here but that's sort of idea
okay the other other paper which uses
this is it's about pricing financial
derivatives okay so this is a recent
work by Sanjeev or and a few others at
Princeton so the problem is the
following okay so you have a bunch of
assets and the question in these
derivatives is that you want to bundle
these assets together and you want to
form derivatives out of this and the
problem could be that if you know if
someone who is bundling knows which
assets are bad then he can gain by
bundling the bad assets together okay
and but you know the person who is
bundling them will just say look i
bundled them randomly and if you think
if densest sub graph is hard then it's
hard to tell between the two so he'll
he'll tell you that this is how i bundle
the assets and they look random to me
and you can't contradict it so this is
the one thing that there's one other
application of hardness okay so here's a
rough outline of theta
i am going to give an end to the
one-fourth approximation for the problem
and the key thing about this is not the
improvement in the factor but the fact
that we somehow have an average case
version which we can solve and using the
ideas from there we can move to the
worst case okay so that's something that
I think is interesting about this and ok
as I said we don't know how to prove
hardness for this problem so you know
one thing we could ask is ok so we know
all these techniques like linear and
semi definite programming hierarchies so
do they at least say something right and
can we say that these things don't work
so that's the I roughly speak about that
and I'll also talk about other
continuous relaxations that you can
write you get to what these mean in a
second but ok so the last two talks will
be a little last two party little
speculative lot of hand waving involved
and at this point i should mention my
co-authors in a lot of these books there
Moses charcoal Eddie clamp touch good if
I gay urban then who's also a fellow
student at Princeton and when get guru
Swami and you want soup ok but what I
want you to remember is that about this
problem is the significance of the
average case so somehow it seems to be
the key to understanding the problem ok
so without further ado let's go into the
problem so first consider this simple
problem on random glass ok so so I want
you to distinguish between two classes
of graphs one in which I just give you a
random graph with GNP degree n to the
one third ok and in the end in the other
case so I take GNP with the same
probability and I take a bunch of
vertices let's say I take square root n
vertices and inside them I add something
like n to the one-fourth edges ok I'm
sorry n to the one-fourth times
sighs edges so I make the degree
artificially into a 14 and to see that
you've actually done something non
trivial note that any case I subset here
has degree very small you can take Union
Bonta it's a simple Union ponca city ok
now so here you know it looks like a big
gap right so here any case I said you
have very small density and here the
degree is in to the one-fourth now a
question is how do you yes yes yes let's
say I even make every degree and to the
left oh yes yes yes I mean that wrist in
the number of forwards
okay so how do we tell between these two
and so one way is ok so let's think for
this slightly simpler question ok so I
have two random graphs both of Edward II
fees once I one you have degree n to the
one-third here you haven't won haha do
tell well you can see the degree and
obvious things but I want something more
but the question I asked is suppose you
pick two words is U and V and I asked
how many lengths three parts are there
between these two so in the first case
you can do a simple calculation to show
that for any fixed U and V the number of
the number of length three parts will
only be log n with high probability will
be off log yet while in the second case
you can see that there will I mean that
in general the number will be something
like square root n in particular there
will at least existed where where where
the number of length three paths is root
n this is simple calculation and now how
does this fit in right so so now we have
a big graph now we have a graph in which
the degree or two once inside bits
there's something planted right now the
key is to observe that this graph H
which was a random graph with degree
this right so this is like a mini copy
so it's a graph on KY disease and its
degree was scared to the half so this is
kind of like the second example here
except that it's a k instead of em right
and luckily for us there were the number
of length three paths they were scared
to the half which is still much larger
than poly log in ok so so it's important
that in this test we got log in as the
answer here and enter some polynomial
ananas answer here and so so simple test
to distinguish between the two is check
if there exists some you and we with log
square n line three paths because in
this case there will be no nothing and
here only things inside H will there
will always be some sessions yes three
edges
four vertices yeah including you and we
all right so this is a simple test okay
and yeah you can also check that we
don't really need H to be random okay so
because the test we are doing is if
there exists two vertices with many
lengths three paths turns out by
accounting argument that as long as you
have k to the half edges inside this
case of graph you don't really I mean
there will always exist some pair that
has K to the half okay so Oh in this
case i want the average degree with that
so we really need average degree but you
can think of the minimum degree it's
fine all right so this is a simple
planted version so as you could see this
exponent of K is what mattered here so
that's what we will make for Molina in
our definition so we define the log
density of a graph to be to be the log
of the average degree divided by the log
of the number of vertices okay so that
means if you have a graph on n vertices
you say it's log density is Rho if the
average degree is n to the row and ok so
and the general result that we will try
to prove is that if you have a random
graph g NP with log density some
rational number are over s and in
another case you have random graph with
something planted inside but it has log
density slightly more so towers plus
epsilon then you can distinguish between
these two so that will be the claim so
note that in the previous example it was
one-third and half and let so in this
case what we could show was that you
look for pairs you envy with many
lengths three parts and if the log
density is more than strictly more than
one-third and you can distinguish it so
one-third was kind of a threshold here
so anything more than one-third you can
distinguish there will be many lengths
three points now the National question
is can we had is there
this threshold thing work for any row so
that would be the art claim okay so
another example I'll give us if you want
log density three fifth okay so this
means the degree is n to the three fifth
and it turns out the right things to
count so earlier we were looking at pet
to to Boyd disease in looking at length
three paths now the right thing to do is
to look at four tuples of vertices and
look at structures of this guy so you
fix UV W and X you look at how many such
structures are there in the graph so
this is one and maybe this is another
and so on ok so you count these
structures and the flog density is
strictly more than 315 there will be
many such structures ok and in ok and in
general if you want so what we do is we
can't appropriate tree-like structures
not that forklift we had we have the
stream and and we count the number that
are supported on a fixed set of leaves
if I supported I just mean these are the
leaves and there exists some tree in the
graph which is of this kind ok and we
pick these such that the expected number
of structures for any set of leaves is
roughly constant so the structure are
chosen carefully like this but we also
it turns out we need some kind of a
concentration result because we want
sure that for any set of leaves this
bond is true that the number of
structures is not much more than poly
log n so this the so this means you
can't use any tree you have to be
slightly more careful ok so and in
general what kind of trees work so turns
out that we can take inspiration from
allisonville and cat what are known as
caterpillar graphs will turn out to work
and so caterpillars are graphs that
roughly look like this I won't describe
how general how they look in for general
are over s but roughly speaking it has r
plus 1 leaves and it has so many were
doofy ok so i won't say more this and
the properties that we will have our
that if you take look at GNP with log
density slickly strictly smaller than R
over s then you pick any r plus 1 top
there will be only poly log in such
caterpillar and and if you pick any
graph with log density some epsilon more
than are over s then there will exist a
leaf double with more than K to the
epsilon caterpillars okay so this will
be the distinguishing test and so this
leads us to the claim that I said that
the distinguishing so now let me give an
example of how we prove a statement of
the first kind okay so i will take an
example of a caterpillar and show how we
show properties like this so it is a
simple probability calculator so i will
just do it for one value of the
parameter so let's think of our or SB
tube 15 this case it turns out
caterpillar is like this so we fix uvw
we want to find how many such structures
there are with these with you vws leaves
and the idea will be to bound the number
of candidates for each of these internal
nodes where internalized i mean these
air b's and c's and each of those would
be at most log in and that means that
the total number of structures you can
have only log Q okay so let's look at
some guy see now okay so how do you burn
the number of things right so we want
the guys for a to be neighbors a few
it's so so they're only like enter to
fit the degree was into the to field
okay and once you fix you you look at
how many things for beer there into the
452 level neighbors but of these the
candidate bees are all neighbors of this
fixed vertex be so the number of such
guys is only p times this because you
need an edge to be and so the number of
candidates for be at this point is n to
the 1 5th and then you get the number of
seats that you can have only with this
information is n to the three fifth
because they're all neighbors of this n
to the 1 5th size set and butt off those
you only you only want things that are
adjacent to W
and by the way we chose the caterpillar
it turns out that p times n 2 350 only
one okay and you can make each of these
statements with high probability
statement so you can say that the number
of neighbor these numbers will be
concentrated around that value and at
the end you can get this kind of a
statement that the number of candidates
for be the pro force see probability
that it's more than 10 log in is
something like 10 n 2 that means for
every choice of uvw this holds and this
finishes the proof yes okay so so we can
do this for any caterpillars are over s
and this is the theorem that we get so
that if you have the platter something
planted slightly more you get you can
distinguish in 10 10 tips okay so now
what about arbitrary graphs right so
that was all in the random case now what
can we do an arbitrary graph so okay so
we want a row factor approximation to
dances sub graph right that was our
stated goal and by that we mean so okay
let me fix some parameters here let n
and let n be the number of vertices in
the full graph and I'll denote by
capital D its degree and for now I'll
think of it as regular and being degree
D can easily reduce it to this case and
H will be a graph on now so H is the
optimum for this problem and it has its
own k word disease and it has degree
small D so just remember n and capital D
for the full graph and opt has scanned
small D and H is what I'll call dog and
the aim will be to find an H Prime with
average degree little do a row and by
this is the definition of a row factor
Brock's me yes so these are things we
can assume with very easily so and we
can assume these without loss of
generality here so we can think of G as
being bipartite so we are interested in
the density of graphs up two constants
now you can assume the graph is
bipartite we are interested in n to the
one-fourth approximation so
Constance don't really matter and we can
think of the of the graph the full
graphs being d regular and another thing
is we can assume that this optimum graph
it has minimum degree little D instead
of everything so we can do the usual
thing of removing vertices that are
smaller than D by 2 and also we are
allowed to return a sub graph with the
average do which is of size smaller than
K that's because we can just remove it
and recurse and so this assumption is
nice because we'll always will keep
looking at sets of word disease and
their neighbors and it's nice if they
don't intersect okay so another crucial
assumption which I will not justify is
that we only the the only interesting
case is when K times capital D is n so k
was remember a parameter given in the
problem so it was so k is the size of
the sub graph that you want and so this
is the only interesting case from the
point of view of the approximation okay
so other cases can only have a better
factor so this I will not justify but
the one implication of this is that if
you pick a random H then it has density
around constant and our aim will be to
beat this constant by some factor and
okay so this is an assumption we can
make and okay so now let me state my
state the theorem so suppose we have an
indie and let's think of capital D as n
to the row for some row ok now the
theorem says that if little D is some
sea x ke to the row then I can find a
sub graph which is on small number of
vertices with average degree at least
see ok so C is the factor by which
little D is more than K to the row if
little D small then this is no
guarantees I'll just return one I less
return some arbitrary matching or
something
and note that the approximation ratio
here is K to the row because the receipt
after you're returning see and by the
way we chose parameters Saudi was K
times T was n so the approximation ratio
will be something like this and he'll
always be better than 14 and well at
this point is no real surprise that the
proof will involve caterpillars because
I said it's very inspired by the random
by the random case and it'll involve
caterpillars that correspond to row now
row need not be a real need not be a
rational number which is why we get into
the one-fourth plus epsilon here it's a
technicality for now let's pretend row
as a rational okay so a simple
observation is that so we want to return
a sub graph which is only on K vertices
but it turns out it's fine even if we
return a bipartite sub graph which is of
the right density so it has number of
edges to be C times a plus B so that's
the number of edges divided by the
number of vertices and one of the sides
has smaller than K so once you do once
you have this it turns out you can
easily get to both sides being at most K
ok so this what we'll do ok so now I
will prove for approximation guarantee I
will prove the theorem that I stated in
the case of one example which is are
over s being one third so yeah this
would be a little technical but no as no
gas is often a talk is supposed to have
one joke and one proof and it's good if
they're not the same so I think so this
would be the proof and hopefully there's
the other jokes okay so all right so are
aware s is one-third and so the setting
we have is that the degrees into the
one-third and little DC times K to the
one-third and KN to the truth because we
thought of K times D as being n
okay so this is the setting and now the
idea is the following so in general
we'll be looking let's start with some
vertex you and look at it set of
neighbors okay so we have the following
so we know that the set of neighbors has
size at most deep because the graph had
degree D now what we know is that use
that that if you was inside edge then
the minimum degree in H was at least
small D so we know that the intersection
of the neighborhood of you with H is at
least small D so let's pretend we guess
some you which is inside H first of all
for purposes so so then we have these
two bounds now I will claim that if we
look at the neighbor of neighbors of you
then we have a fairly strong guarantee
on the size of s so what do we know
about neighbor of neighbors we know that
its size at most d squared because there
were d neighbors and can expand to d
square and i claim that if you look at
its intersection with h which is the
optimum then it has a size at least
little d squared over small fee that is
so there was this graph our and there
was this set are and i want to say that
our almost expands fully in age so
inside H the degree was at least small D
now and the best it could have expanded
by was something like our times D they
want to say that it's close to that
alright so why is that true so this is
just the copy of the game okay so
suppose that is not true okay so now
let's look at this sub graph which is
formed by r and s so now that sub graph
should have had a ratio of number of
edges to its vertices to be at least see
right because we assume that s was small
we assume that s was smaller than R
times D over C and the total and the
number of edges between the two is at
least d times are because minimum
inside H was little d okay so so we have
a bond of so so we have a sub graph of
density at least see inside how do we
find it now we can solve this max
density sub graph problem that I earlier
defined I said that that can be solved
in poly time the problem there was that
it was giving so it could give graphs
that have much more than que ver de fief
right but that's okay here because k is
into the two thirds and the best you can
come up with from here is is something
where you have n to the one-third guys
on one side and then that's fine right
so so okay so if there existed such so
if this bond did not hold then we are
done so this will be the style of the
argument we will have some inequalities
of this kind we will say that if those
bonds do not hold then some local
solving like this will actually give us
a denser so we can continue this line of
reasoning you can say that now work with
this look at its set of neighbors and we
can say that if you looked at T which is
the neighbors of s and intersection with
h h was optimum sub graph then it has to
have a size at least something like this
if you work out the parameters we can
show that the size of tea should be at
least little d cube by c square and t is
this set is the intersection of this
third neighbors with h now okay if you
stand carefully this can't be right
because the way we picked parameters the
size of H was k while this d cube by c
squared is strictly more than say it's
more than C times K and T was the
intersection of some set with key it
can't have a size more than K right so
this means that at some step you must
have found the seed n sub graph and this
finishes proof so the so one of these
things should hold otherwise you will
have found inside before you get a
contradiction and are in this race
the smaller side is always yes and right
that's crucial so we should note that
the second side is that one of the sides
is smaller than K and this will hold
because of our choice of parameters so
all the parameters should be chosen
carefully so that one of the sides will
always be kept smaller than K and then
we do this right okay and and even for
arbitrary caterpillars this will be the
general outline outline so we'll always
have some set of vertices w and we say
that we will do some simple procedure on
it and if it fails then we will come up
with a good bond that holds yeah and if
we continue doing these bonds and
eventually we get a contradiction so
this means that some step you should
have found an M sub graph okay alright
so this finishes the algorithm for them
sub graph or at least this is all I
would say about it and okay so now what
about lift and project methods okay so
i'll define this in a minute okay so so
these are a class of techniques that
have been studied pretty intensely of
late it is okay so here's a standard
formula we use for for solving problems
right so we can write an integer program
which is you want to maximize some
objectives subject to things being 0 1
and we usually relax it so that you have
conditions between 0 and 1 the variables
between 0 and so they can have
fractional values and if the integrality
gap is small then it means that the
object of value is approximated recently
it turns out many for many problems the
gaps are huge in one way of dealing with
this is to add more constraints to the
to the linear program and there are
these ways of systematic ways of adding
constraints which is which I want to
call lifting operations I won't define
them in particular in in general but the
idea i somehow that you'll have
variables that correspond to pairs of
events and things like this pairs IJ and
so on
and so they'll turn out that this has a
strictly smaller integrality gap or at
most as we can into Galilee gap as the
region as this linear program okay and
there are the systematic ways of writing
more and more constraints okay so think
of this as a polytope that you would
define so the and you are repeatedly
chopping off parts of it and the hope is
and there are ways of defining these so
that if you do n steps of this then you
actually end up with the integer program
so then all the corner points will be
the actual integer points that you
started off with and there are many ways
many systematic ways known of doing this
so the two things are commonly used or
what I call the Cheryl Adams hierarchy
and the Lasserre hierarchy so we won't
need the details of this but this is the
general outline of thing okay and the
crucial thing is that the kids level of
the kit the level in this hierarchy can
be solved in time roughly n to the K so
that's why it makes sense to ask this
question so after small number of rounds
of this do we have a very small
integrality gap so big because then
let's say after some K rounds we have a
very small integrality gap then we can
solve the problem roughly in time n to
the K and so we ask this question for
densest sub graph starting with a
natural relaxation for it and it turns
out this that you can't do this okay so
we show that even after some tea rounds
of shirali items but t is fairly big it
is something like log n so solving this
would already need the quasi polynomial
time so the integrality gap remains
roughly n to the one-fourth n to the
one-fourth is what our algorithm could
achieve and somehow we don't seem to do
better than that and also the gap
remains end to the epsilon even after
close to linear number of rounds of love
the lesser herrick so I'd want to say
that lesser hierarchy something that is
sort of as a really strong sdp hierarchy
and most of our algorithmic tools
somehow seem to be captured by a small
number of rounds of it and the fact that
even so many rounds of it do not help is
strong evidence that the problem is
actually hard okay so it in fact shows
that we need really new techniques if
the problem can actually be done better
so natural things like counting spectral
based techniques don't work and it turns
out that the gap instances are actually
random for this guy's which is nice
because which is interesting because it
sort of reaffirms our belief that random
instances has a difficult one and then
entry let me finish the lenses sub graph
part with this interesting question
right so it's so let's say we want to
distinguish between these two cases one
you have a square root n degree random
graph another case where you have a sub
graph H inside with degree something
like n to the 1 4th minus epsilon and
not that here you if you look at any
root n size set its degree is only at
most log in sorry this should be log in
and the question is can you distinguish
in poly time and this we don't know okay
so and this is why the end to the
one-fourth roughly comes into the
approximation so if we can do this
better than we think we can solve even
the general problem better than okay so
now what about other continuous
relaxations for problem one such thing
that I will talk about is is what I
called mixed not but let's get into that
so okay consider this problem given a
graph I want you to find a set of
vertices s such that the number of edges
in inside s divided by the size of s to
the 1 plus Delta is larger than 1 so
this is something we just saw this is
like saying find an s of log density
more than Delta and but you can rephrase
it in another way you can say try to
maximize this quantity so X transpose ax
for
01 vector measures how many edges that
are inside s right so if s is the
indicator vector for some if X is
indicator vector for some sed s this
measures how many edges they're inside
and this roughly measures size of s to
the 1 plus Delta wrote this down so that
it's invariant and the scaling although
it doesn't matter in this case but okay
so it turns out that this kind of a
problem has roughly the same feel as
problems of this kind these are much
more well study so well so x transpose
ax is roughly like a x2 squared so it's
actually for the square of the matrix
but anyway so let's not get into the
detail but but these problems have
roughly a similar field and turns out
these things are well studied so they
are what I call hyper contractive norms
okay so so you want to maximize over all
of our end now so / 0 1 2 then in all of
Ireland doesn't matter much that similar
up to a log n factor and so if you can
sure that this norm is small so if so is
there destined C matrix of a graph you
can show this norm is small then you can
certify that small sets in the graph
actually expand so it basically means
that this quantity is small which means
that there is there are no dents
portions in the graph and motivated by
this and it's I'll mention some work
which is kind of different from what
I've been talking about it's about
computing norms of this kind so this is
one other question that I've tried to
study just you again some matrix and you
want very natural problem you want to
just maximize the P norm of a x divided
by the q norm of X and you want to find
out when you can do this so not that it
generalizes the largest singular value
and also this well style problem called
the gross in dec problem which you can
get for certain values of P and Q now
okay so the question is when can you
approximate these it turns out
that this is very badly understood and
the one case where we did understand I
have just mentioned in a bit but in its
full generality it's maximizing a convex
objective of a convex set so you want to
maximize ax norm p subject to the q norm
of x being small right so so this
something we cannot do by normal convex
optimization but there are some cases
where we can actually do this and this
is in a joint work with Ivan last year
so so if you have such a problem and be
smaller than Q then you can solve this
problem and the entries of a are all
non-negative so if you have like the
adjacency matrix of matrix of a graph or
something you can actually solve this
and it turns out that even though it's
not a convex optimization problem there
are some features of it that you can
actually solve okay that you can exploit
for instance we can prove that the level
sets of of this function so you look at
this function and you look at its level
set they actually don't be connected so
so they're not convex but they are
simply connected and they're also then
they have this nice property local
Optima to go beyond so you can actually
solve it and it turns out the solution
is by a fixed point iteration within
which we can show convergys fast and so
this is when entry Sophia are
non-negative we also know what happens
if you allow arbitrary entries so here
it behaves so you can reduce from max
cut and show that it's hard to
approximate to this looks complicated
but if it had been one here then it's
polynomial it's something like n so it's
something as kind of like polynomial so
this is the kind of guarantee that you
often get by parallel repetition you
should know so okay so we know it's very
hard if you allow arbitrary entries and
if you have non-negative entries we can
actually solve okay all right so what
are some directions for the future so we
ideally like to understand the
complexity of this sparsity constraint
because in this somehow seems to arise
everywhere right so it's not just
small sub graphs in graphs but they're
also applications like compressed
sensing where you want to show that some
matrix has for instance the r.i.p
property where first pass vectors if a
first pass vectors that actually is an
axe it's like an isometry and things
like this and it's also related to hyper
contractive norms as I was saying and
it's nice to have a better understanding
of these for instance it's not known if
computing these is hard case and this is
something would be nice to know another
feature of our algorithm was that it
that we did something on an average case
and then we could do we could carry over
such techniques for the worst case right
so it's this kind of paradigm useful in
any other context so that would be nice
okay and another thing is that you can
try to relate this thing to do more
exotic problems like there's this
question of tensor maximization that a
bunch of people have said it also
appears elsewhere so it'd be nice to
relate to these and okay so since it's
kind of an interview talk let me try to
let me generally mention other things am
I kind of think about so one of the
questions which I was kind of which I
was talking about way a little earlier
is the question of eigenvectors in GNP
when P is reasonably big so a lot we
know a lot of things about eigenvalues
and like there's this big nough
semicircle law and so on but you know
somehow no I mean people don't know much
about eigenvectors okay so for instance
our entries well spread out and things
like this and we try to take some basic
steps in this so we can do it for some
very simple cases then its joint work
with sanjeev so it's kind of like a
foray into this but we have only
preliminary results i'll be happy to
talk about these okay and another thing
that i have recently worked on is using
some tools in convex geometry to study a
differential it rise in differential
privacy so you know basically it's like
I used to think of privacy as being
something mysterious you know there's
differential privacy but yeah this is
one so I know something about it now and
okay what do I mean by unconditional
here there's something called the
hyperplane conjecture and convex
geometry and so there was an algorithm
there was a mechanism for privately
answering certain queries and it was
shown to be good if if this conjecture
hold and we showed that you can find a
different mechanism which actually
avoids depending on this conjecture so
so that is the work I did with revision
krishnaswamy and kunal tour what
actually you guys must have seen we are
city but okay yeah thanks a lot yeah
so can you only like is it is these so
you call these caterpillars is that if
you want talking possibly some other
structures yes for the land in case you
can actually do suppose you have a rant
I mean random graph of some bigger log
density inside another random guy you
can count lot of things there are all
these before they cause I mean you can
count like four cycles and things like
that and they all have some thresholds
at which they start appearing so you can
do with those but somehow caterpillars
we found a very useful in the general
case so we don't know how to handle this
also those this other problem there was
this other good thing about using trees
which was that we could show that the
planted thing need not actually be
random so it could be any sub graph that
has logged in setia tlie strictly more
than the original graph so that is that
happens only when it's a tree because we
use this counting argument and then you
get that there has to be some lift up
which there are lots of things so that's
the reason we just got a pure silver
it's reason please that's the recently
right but yeah actually I guess the fair
answer is there might be other trees but
these were the simplest we could find so
I'm so easy currently oh yeah there's
please sunny but could it perhaps
improve the so I'm trying to distinguish
three particular powers then there might
be a better yep you a nice of laxity
right right right this right now we pick
some rational number between Delta and
this plus epsilon and we only have this
one over app into the one over epsilon
you could bring it down to something
better that would be nice but I don't
know how to do that
or dreams
I guess the pleasure miss P by Q right
or something later you show up is our
leaves an s total also it doesn't the
caterpillar you paid that I guess if you
pick a neatly with these two things ah
oh no no it doesn't because I mean so
you need this concentration bond that I
was saying so if you can see the board i
can draw one figure that will not so
suppose I pick trees that a kind of
lopsided right so this will look
obviously lopsided but it's an example
so let's say i pick a tree i pick a tree
like this ok i came that this is the
wrong tree while this is the right one
the point is that if you picked any UV W
X and this is for log density being 350
this is the example that I showed so
that means P is n to the minus to fail
because the degree was in 23 fit now so
what happens here is that there will be
some four tuples uvw x for which there
are lots of such trees so in expectation
it's still true that the number of
traces for some UV W X is only constant
but whenever these three guys have a
common neighbor there will be lots of
trees and because as a test we are using
is if you know if there exist some fort
Apple that has lots of trees so we
cannot use things like this so somehow
we need these numbers all to be between
in this case the problem is that this
the expected number of candidates for
this guy smaller than one and whenever
it is one whenever there is some common
neighbor tanza there are a lot more for
the others
hey Sophie nice embarrassing but yeah
there are still lots of an aspiring
writer at it I think they will work well
though always holidays thank you yeah I
think as long as you ensure that if you
do some calculation and find the
expected number of guys for each node if
you find that that numbers between 1 and
s and right it's not strictly it's not
less than 1 or something then I think it
will work
you're getting
yeah not that I know sorry
nothing
yeah that would be very nice</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>