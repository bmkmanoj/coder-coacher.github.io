<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Randomized Block Krylov Methods | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Randomized Block Krylov Methods - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Randomized Block Krylov Methods</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/igy1_Ly9JG8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so we have our first contributed talk
it's the title is randomized Brock
blocked relief methods for stronger and
faster approximate SVD in the line of
recent work on randomized linear algebra
that's become very popular and the
speaker is Cameron misko and the authors
are Cameron and I guess his brother
Christopher vehicle all right um so I'm
talking about randomized methods for
approximate singular value decomposition
the singular value decomposition all of
you probably know it's the decomposition
of any matrix into the product of three
matrices u which has orthonormal columns
the left singular vectors of the matrix
V transpose which has orthonormal Rose
the right singular vectors of the matrix
and then this non negative diagonal
matrix Sigma whose entries are the
singular values of the matrix are
typically ordered in decreasing order
from Sigma 1 through Sigma D the
singular value decomposition finds
applications in all sorts of places in
data analysis it's typically used for
dimensionality reduction and
specifically for low-rank approximation
and principal component analysis so
these applications are based off
well-known optimality properties of the
singular vectors as sort of the
directions of greatest variance within
the data set so specifically the
singular vectors are the eigenvectors of
a a transpose and a transpose a
respectively and due to these optimality
properties you can read off an optimal
low-rank approximation from the singular
value decomposition so if I just zero
out all but the top case singular values
in their corresponding singular vectors
I get this rank K matrix AK which
minimizes the error a minus B 4 B Gnaeus
norm for all rent k matrices B and this
actually minimizes the error when you
measure it in terms of any unitarily
invariant norm so an important other one
is the spectral norm air now AK can also
be written as the projection of a on to
its top K principal components so I can
write that as UK UK transpose a now the
singular value decomposition is somewhat
of a complex object what does it
actually mean to approximate it so
I'll just review some standard
approximation metrics the SVD gives
optimal low-rank approximation so we can
ask an approximate VT to give
near-optimal low-rank approximation
specifically I want to return a set of K
singular vectors which I'm downloading
here is U tilde UK tilde and I want when
I project onto these singular vectors
and look at my furby Nia storm error
with respect to my original matrix that
this error is nearly as small as the
optimal for beam is norm error for a
low-rank approximation now this is very
useful in some circumstances but for
beanie is norm error can be sort of a
weak measure of error this is because
the furby Gnaeus norm of a matrix is
equal to the sum of its singular value
squared and the Frobenius norm of the OP
the difference between a and its optimal
rent k approximation is equal to the sum
of its sync squared singular values
below the top K singular values no
typically datasets display strong
singular value or spectrally case so
these small singular values are small
but if you're in very high dimensions
this total size of the tail can actually
very large it can be very large in
comparison to the top singular values of
the matrix so the right hand side of
this inequality can be sort of large in
this approximation guarantee can be weak
unless you set your epsilon to be
extremely small for many data sets you
can basically return any u tilde k and
get pretty good for be Nia storm
approximation when you measure it in
this sense so people look to give
stronger notions of approximation and
one way to do that is just use the
spectrum norm rather than the Frobenius
norm so the Frobenius norm is the sum of
singular values of a matrix the spectral
norm is just the top singular value so
in this case the spectrum of a minus its
best friend K approximation is its k
plus first singular value due to
singular value decrease mall if you're
choosing k to be large enough and so in
order to achieve this notion of
approximation u tilde k really is going
to have to project off the directions of
largest variance in the matrix it's
really going to have to align with your
top principal components so this tends
to be a much stronger notion of
approximation than for being a smart mer
a final sort of strongest notion of
approximation we can talk about is we
want our approximate singular vector u
tilde I to nearly have as large of a dot
product that product over a transpose as
the true singular vector a a transpose
okay so that's approximately a singular
value decomposition we can do this all
with classic full SVD algorithms like
the QR algorithm these types of
algorithms run in n d squared time they
have super good dependence on error so
things like log log 1 over epsilon error
rate but unfortunately n times d squared
can be pretty expensive for large
matrices so did so the question is can
we do something better especially if
we're doing principal component analysis
or low-rank approximation we're really
just talking about computing the top
case singular vectors of the matrix so
the question is can we go faster if we
just want to compute this partial SVD
the answer is yes if we going for the
week approximation goal for be Mia storm
error we can use like classic stronger
including QR algorithms and get with
Anna polynomial factor of the error we
can also use johnson lyndon strauss
random projection style techniques these
sparse subspace embeddings and get
within 1 plus epsilon of the optimal for
penius error with the very faster on
time something proportional to basically
the number of non zeros in your matrix
plus some lower order terms depending on
K and your epsilon but as I said for
beanies norm errors week in many
circumstances so we want to do something
stronger and if we want to get stronger
error things like the spectral norm or
PCA error really the only game in town
are iterative methods for the SVD and
specifically the power method in the
land trust methods the runtimes of these
algorithms are something like n NZ a
which is the number of non zeros in your
matrix is the time to multiply your
matrix by a single vector which is a
basic primitive of these iterative
algorithms k the number of singular
values your computing or the dimension
you're reducing to and then iterations
typically depends on your error and on
some other parameters K is typically
much smaller than di either to mention
you're reducing to is much longer than
your original dimension so as long as
you keep your iteration count reasonable
these algorithms are going to be much
faster than doing a full SVD using a
classic algorithm and so they're widely
implemented and you can use all over the
place I'll briefly mention there's been
a lot of recent interest in so passing
methods for PCA in dimensionality
reduction I'm very interested in these
methods and I've been doing some work on
them and happy to talk about it offline
but i'm not going to directly compare
them to the iterative methods because
i'm mostly talking about the general
matrix regime where these don't get
necessarily give speedups over the
Asik iterative methods okay so let me
just quickly review the power method
which is the most basic iterative method
most of you know the power method for
computing the top augen vector of a
matrix and the block power method which
is also known as simultaneous or
subspace iteration is the direct analogy
of this for computing multiple top AG in
vectors or multiple top singular vectors
in our case so you start with a block of
K vectors some initial a turret in an
each iteration you're going to multiply
by your matrix you're going to
orthonormal as the result and this is
going to give you your next iterate as
you multiply by your matrix the
directions in your vectors that are
pointing in the directions of the top
augen vectors are the top singular
vectors of the matrix are going to be
stretched in comparison to the rest of
the vector and eventually your iterates
are going to converge disbanding the top
subspace of your matrix so this is the
basic our method with the block power
method okay the run time for this power
method you can give by the cost of a
single iteration which is n NZ a times K
so this is the cost to multiply by K
vectors and then the iteration count
it's going to converge geometrically so
you have this log D over epsilon term
which is basically coming from if you
have a random initial start matrix you
have polynomial the initial error and
you get down to epsilon error and then
the convergence rate depends inversely
on this gap parameter which is basically
saying how much larger is the K it's
singular value from the pic k plus first
singular value now traditionally this
gap is sort of assumed to be a constant
but in many data sets it's actually
going to be the dominant factor in the
iteration count so as i explained most
data sets display strong singular value
decomposition and after about the
thirtieth singular value or so the
spectrum looks quite flat if i look at
the median gap amongst the top 200
singular values it's only two out of a
thousand and if i look at the minimum
gap so if you choose the worst possible
k if you get unlucky lucky this minimum
gap is something like one out of twenty
five thousand so if we plug this into
the runtime bound we gave before we're
getting this 25,000 multiplier on our
run
which is really bad and this doesn't
really jive which what's seen in
practice the power method seems to work
very well in practice even if there are
very small singular value gaps so this
phenomenon has been largely explained by
recent work which gives gap independent
balance for the block power method with
randomized start factors specifically in
Lodi over epsilon iterations you can get
within 1 plus epsilon of the optimal
spectral norm error for the singular
value decomposition so essentially what
they're doing is you're replacing this
gap parameter with your epsilon
parameter your error parameter and this
improves on the classical bounds
whenever epsilon is set greater than
this gap which is often in case in
practice there's been a long line of
work on this probably the most
well-known paper is this hulk omar
instant drop overview in sub what's up
went to the release of these results
there's been a lot of renewed interest
in this randomized block power method
it's a very simple algorithm it gives
simple runtime bounds and so it's been
implemented all over the place recently
this is sort of in contrast to the fact
that in the numerical linear algebra
community crilla and LAN shows methods
have been used over power method have
they been long preferred over power
method in LAN chose methods are really
the standard implementation of SB DS for
large matrices so matlab spds this is
using a lan chose method not the power
method so why is this this is
traditionally because the cria of and
LAN chose methods essentially allow you
to put a square root on this gap
parameter that I was talking about so if
you have a small singular value gap
these are going to give significant
acceleration in comparison to the power
method the recent work on the randomized
power myth it is is removing the gap
dependence on power method and putting
an epsilon dependence there instead so
showing it works in the small gap regime
but up until now there's been no gap
independent analysis of prelab methods
so that's what we're providing our
contribution is basically showing you
can get the same acceleration when you
have a gap and defend it bound you can
get the same acceleration when you have
a gap independent bound getting the
square root of epsilon parameter okay so
our main result is you there's a random
a simple randomized block free lap
iteration that gives all three target
error bounds in this runtime the number
of iterations is log D over square root
of epsilon this is giving a bound that's
independent
our matrix a so for in classical
analysis if you happen to have a gap of
size zero you get literally no
guarantees on your performance our bound
will still be useful in that case it
beats the runtime of block powers method
significance for small epsilon again
improves on land trust whenever your gap
is smaller than your epsilon parameter
so the first step in understanding how
we can remove the gap dependence is
understanding where it comes from in the
first place it basically comes from the
fact that classical analysis all depends
on convergence to your true top singular
vectors or your top subspace and they
measure error using a potential function
something like UI minus u tilde I the
two norm this is a simple potential
function once you can show that you make
this potential function small it's easy
to show the type of error guarantees we
talk we care about but there's a huge
negative in that in order to get you I
minus u tilde I small you inherently
require an iteration count that depends
on your singular value gaps so
specifically if I apply the power method
when I have a small singular value gap
I'm going to converge quickly to the top
singular vector but I'm gonna converge
very slowly to the second singular
vector because it's only boosted in size
a little bit at each iteration the thing
that we can notice though so this seemed
like an issue we have this inherent gap
dependence if we want convergence but
the thing that we can notice is that
convergence becomes less necessary
precisely when it's difficult to achieve
specifically if I have a small singular
value gap it really doesn't matter if I
can distinguish the second from the
third singular vector in terms of
principal components or for low-rank
approximation these two singular vectors
are pretty much as good as each other so
while minimizing this UI minus u tilde I
is a sufficient thing to do it's far
from necessary especially when you have
a small singular value gap so this is
why the intuition for why you should be
able to remove this gap dependence how
we do it technically speaking is that
instead of thinking about convergence
between subspaces or singular vectors
we're viewing iterative methods as
denoising procedures for random
sketching methods if I have an actual
low rank matrix of rank K matrix a I
choose a random Gaussian matrix with K
columns I look at the span of a times
this Gaussian matrix this is equal to
the span of a with probability 1 if I
project a onto the span I'm going to
exactly recover the matrix and I'm going
optimal for penis or reconstruction for
this low rank matrix now if my matrix a
is not actually low rank so it has some
distance from low rank a minus AK I'm
gonna instead get an error that's
polynomial in this optimal for being
sore approximation polynomial a minus AK
this is basically giving an error bound
for single a single iteration of the
power method it might seem very weak and
it is very weak and it so unless a minus
AK is very small but it gives us a
starting point because now what we can
do is instead of applying this very
simple sketching method to a week Andy
noise a and apply instead to this matrix
a QA powered up to the q with power this
is exactly what black power method does
you iteratively compute a Q times G and
then you return its span as the output
of your algorithm 8q looks exactly like
a except that all of its singular values
are raised to the q with power so if we
look at the spectrum what's going to
happen is in AQ the large singular
values are going to be significantly
blown up in comparison to the small
singular values if I scale the spectrum
so they match on the top singular value
basically a ques tail is extremely
relatively small if I look at its
optimal for penius norm reconstruction
error so a Q minus its best rent k
approximation this is the sum of these
very small singular values and is also
extremely small now setting q equal to
one over epsilon is sufficient to kill
off any singular value that is below
sigma k plus 1 in comparison to any
singular value that is above 1 plus
epsilon x sigma k plus 1 so we know that
if we choose our output to be the span
of a Q times this Gaussian matrix in
order to obtain even this very coarse
for Venus from error insured by the
sketching method because this low-rank
approximation error is going to be
extremely small u tilde k is really
going to have to align with large
singular vectors of a queue I already
said we can't get convergence we don't
know that aligns with actually the top K
singular vectors of a queue but we know
that aligns with some set of good
singular vectors and it gives us exactly
what we need for spectral norm
reconstruction era and for PCA air the
final step is we know that a &amp;amp; AQ have
the same singular vectors so if u tilde
k is good for a queue it's good for a
and this is the basic algorithm
we use some there's some details in
converting from this for Venus or marrow
to the spectral arm and / vector error
and importantly we always avoid talking
about actual convergence of singular
vectors or subspaces so crea of
acceleration the understanding of why
this works is actually very simple
basically there are better polynomials
for denoising a then a to the q instead
of raising your matrix to the one over
epsilon power you can apply a chebyshev
polynomial with degree 1 over square
root of epsilon in this degree saving is
exactly where we're getting our square
root of epsilon gain over this over the
power method basically the same argument
goes through for applying this chebyshev
polynomial the only difficult thing is
how do we actually compute this
chebyshev polynomial and basically what
we do is we notice that we can just save
the intermediate computations of the
block power method into what's called
the career lab subsidies which contains
all the all the components of this
polynomial I think I'm running out of
time so I'll just give a very brief
summary of what we end up doing instead
of computing the actual chebyshev
polynomial we compute the what is called
the best the best span within the curl
of subspace and this gives us the error
bounds we need so we'll be around during
the poster session for if you have more
questions and I can finish up explaining
this thank you very much
well thank you for a very nice talk
before we start any questions may I
request all the speakers for the
spotlight sessions to start lining up
questions now
so let me ask you one question land
choice is of course intimately tied to
this three-term recurrence which is
invoked the theory of orthogonal
polynomials and makes things more
efficient so in your methods are you
using the three-term recurrent so I
explicitly we're not so yeah really our
method should be called acrylic method I
guess instead of Elaine chose method you
could use the three-car recurrence the
issues with the three-term recurrence is
there's some instability so in our
actual implementations because we're
working with a relatively small crea of
subspace we're just directly orthogonal
izing it without using this recurrence
and this allows us to get a stable
algorithm um you could use the
three-term occurrence but then you have
to deal with all the stability issues
that are inherent with this sort of
approach so it's not necessary to use
the recurrence basically so with these
new methods could you give a sense for
with the you know the current computing
power how large of a matrix you can you
can use practically you know for out of
time I probably can't give you a very
good sense of that um I know that in
doing experiments for this paper if you
have a sparse matrix with orders of tens
of thousands or maybe a couple hundred
thousand rows you can do this extremely
quickly using the LAN choice or power
method so you can do I can do this on my
laptop essentially um but i'm not sure i
don't i haven't done experiments on
really large scaling of these things but
other people have done this especially
for randomized power method okay let's
have one last question by motives yeah
quick question a great talk em so um the
the method you analyzed refers to the
queue with power of the matrix a and so
it's not really numerical equivalent to
what you actually implement where you
have like a normalization step after
every iteration so it seems like to
analyze that you would need some kind of
numerical stability at least or cope
with rounding errors and it's the same
for the helical martin trop analysis
yeah right so um you don't really deal
yeah even in the hokum artisan chop
analysis it's not really giving rigorous
stability bounce if your or Thor because
you really do need ortho normalized not
if not after every step you really need
ortho not Norma is relatively often
after every few steps so you don't lose
rank of that subspace so we don't have a
full stew
analysis but we have experiments and I
can show you later basically showing
that these lantra safe methods are as
stable at least as subspace iteration in
practice and it would be interesting to
think more specifically about what sort
of stability analysis you need to get a
full analysis okay let's thank the
speaker again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>