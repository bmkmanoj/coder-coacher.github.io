<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Online, Opt-in Surveys: Fast, Cheap, and Mostly Accurate | Coder Coacher - Coaching Coders</title><meta content="Online, Opt-in Surveys: Fast, Cheap, and Mostly Accurate - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Online, Opt-in Surveys: Fast, Cheap, and Mostly Accurate</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lCsgFGLRtqo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I didn't this about this area in general
and the the conference I think has been
really phenomenal so far and in
computational social science is really a
particularly interesting area I think
it's using kind of computational methods
and big data sets is kind of
revolutionising the way people do social
sciences and I'm not going to really say
too much more about that i'll give the
speakers more time and so let's let's
just get started such a short
introduction and I didn't get ready to
jump on so now I'm good so David
Rothschild I'm an economist at Microsoft
Research in New York City and thank you
guys for having me here today and this
is joshua lee what I'm gonna be talking
about his joint work with shrod who's go
well who was from Stanford is actually
next speaker so you can blame him for
whatever you don't like and Adam who's a
grad student Columbia oh wellz a lot of
it also is built off of work that we've
been doing with Andrew Gelman of
columbia as well and so it's gonna be
talking about polling and the image on
the left is an advertisement for Clark
Stanley's snake oil which was sold
widely in the United States up to a
hundred years ago when they determined
in a lawsuit that there's neither snake
oil in it nor does it do anything good
and from that I'll jump into traditional
polling so traditional polling is a 21
billion dollar industry and it is ripe
for disruption by people who understand
statistics and that's where we're
jumping into and what I have up here is
something which I pulled off of the
website pollster and they take a bunch
of polling and they aggregated and show
what's going on this was from a month or
two ago and you can see two poles one
done by Gallup wamp done my mammoth
University both done with the the best
methods that people utilize a random
digit dialing which means that they try
to get a random sample of Americans by
calling phone extensions and then
supplement that with some randomized
hitting of people on the internet
through email lists and what you see is
the presidential approval and you'll see
what's interesting is that Gallup had a
fifty percent approval and Malmuth
University at
virtually the exact same time over the
same type of population had forty two
percent and what they publish both is a
margin of error of plus or minus three
percentage points and so this happens
all the time and you probably wonder if
if there was the true the margin of
error of plus or minus three percentage
points was true this would seem rather
implausible to happen on a regular basis
the margin of error is generated by
assuming that what they do is a simple
random sample so they interview a
thousand or 1,500 people and they assume
that they actually have a perfect simple
random sample of the population that any
single American adult in this case is
equally likely to have answered the poll
now of course that is not true no it's
not sure and the reason it's not true is
40 sorry to be clear they don't actually
believe that a true but they report
margin of errors that are based off a
simple random sample it's a it's not a
super innocent but it don't kick like
this in brenham some value to
adjustments it's okay sorry it turn on
jumping onto the onto the breaking of
the way it's guys that yeah you so okay
on top of the raking waits to be clear
but it what they're not cutting into the
sample so it should be served but
they're not putting into the are to be
clear is coverage error and non-response
error so essentially coverage error is
assuming that the general population is
not actually completely in the
possibility random sample and
non-response error is what happens when
people do not answer it equally or not
as likely answer equally response rates
have fallen to about five percentage
points which means that ninety-five
percent of the people who they attempt
to contact don't don't answer so and
there's also a survey which on top of it
survey our wishes basically saying that
different polls actually interact with
people slightly differently the question
of accuracy is one that's very difficult
to think about because there actually is
no ground truth there's no ground truth
to the approval rating of
the president so it's hard to think
about how how that works so I will curry
and I started to to oversimplify and
I'll talk about the how waiting folk
focuses in on it but you can assume that
the margin of error is something that is
built off of the random sample plus how
they end up waiting in the end with
absolutely dropping off dropping off the
impact directly of coverage error and
non-response error and I'll talk about
how it can and cannot be included in a
second I don't want to get too much into
the weeds on it because it's actually
not the main second the main point of
today's talk but i would say that we've
you know done a lot of work just looking
at just kind of calculating how the
error the true errors for those things
that do have outcomes relate to the
simple random sample as a kind of
hardest baseline and essentially looking
at the top is what we're looking at is
presidential senatorial and
gubernatorial elections over the past
few cycles we took all the polling that
was publicly available off of major
websites and we looked at the root mean
square error versus in the dark lines
about what actually happened the dotted
lines was if you assumed just even the
most you know perfect simple random
samples to see assuming the most perfect
random samples what we see which is very
interesting to us is that if you compare
the outcomes of what you get in the
polls to what you'd get if you actually
had a random sample the variance of what
you get is actually fairly similar let's
say it's in the same ballpark of it what
you're really losing out a lot on is
essentially bias wherefore if you look
at the polling within a given election
where you have a certain number polling
for a given election the error is not
randomized on both sides of it you
generally end up with polls clumped on
one side or another okay most polling
though you don't have the benefit of
having some sort of answer those were
poles that happened also in the last
three weeks of election polling also
suffers from being somewhat slow
somewhat expensive so standard polling
costs about one dollar per response in a
sense and somewhat inflexible
and the fact that if you're working a
system which has telephone as part of
the main outcome use case then you have
some sort of inflexible ability on how
you ask questions and I jumped in
questions I don't want to get jumped
into the weeds of exactly how the margin
bearers were determined because but all
I want to say is that there's a lot of
things that are left undone because we
don't normally have outcomes you can't
separate bias from variance and also
because there is a lack of full
understanding about how non-response and
coverage actually changes by questions
it's not something that is easy to
incorporate a systematic way they're the
opposite type take the kind of other
extreme is thinking about
non-probability bullying so
non-probability polling is when rather
than even attempting to get a random
sample you grab samples of convenience
and in 1936 the literary Dodgers had a
monumental failure they went out they
sent out ten million plus postcards to
people who were contributors to the herb
subscribe to literary digest or at
automobile ownership and they were
amazed when they got back about two
million of those and they were pointing
heavily towards the ventral loser in
election landon vs roosevelt and the
reason of course is that they had a
terrible sample they had very poor
coverage and they had non-response all
of it they didn't account for but a lot
has changed over the years that actually
took several weeks now we have internet
collections that are fast and flexible
very scalable and of course there's
analytics that can go on top of it and
this this distinction between
probability and non-probability polling
becomes a lot more blurred as you look
at the traditional polling in which
people attempt to create random samples
versus once which you don't attempt to
create it there is essentially you have
on one end a theoretical probability but
in the other hand you have something
like the man on the street and the
question comes down just are we making
an artificial binary distinction between
these if there is a lot of non-response
and coverage air
are in your ability to try to get the
best probabilities and the analytics is
somewhat constrained and has been
constrained because of when you try to
get a probability sample you want to
demonstrate that the data just works and
so there are certain limitations on how
much analytics that people put on top of
the data they collect and so that's what
I'm going to jump into today you can
kind of think of it quite simply in some
ways is if you think about in the far
left this is going to be your cheapest
your fastest and your worst samples if
you go and you get extreme samples of
convenience versus on the far right
where you think about standard methods
in which you try to get as close to a
perfectly representative sample the
census is able to do that with mandatory
reporting through the ACS as well you
have places where people go out and they
really try to bang on doors and get
people to actually respond RTD is a
standard methodology of randomly calling
different extensions on telephones you
have companies like you go which are out
there subsampling from internet panels
that they have created you have a lot of
companies out there that also willing to
make a quota sent boat which means that
they will simply hit their panel up to
certain marginal demographics generally
focusing on say gender and age versus
say looking at the man on the street and
then of course you have the question of
analyze yes so the continuum is arranged
according to what I would say this is a
range at thinking about generally when
you talk to people in the polling
industry you have a hard cut off right
here in which they're going to fir two
things on the right here is an attempt
to make a probability sample and things
on the left here are generally
considered non probability if you look
on the sample you're going to assume
down the sample as you move to the left
things are going to be faster probably
to collect they're going to be cheaper
to collect and as you move to the right
you're going to have a better sample
you're going to have a sample that is
less biased and more representative for
the population that's what I'm asking
because you complete ignore the issue no
response bias right mandatory does not
mean people responding act accurately
it's quite a well-known for the census
sometimes you know people give you
answers
having more people not necessarily mean
it's better and there's a property
summary the people you know don't miss
retail you sure so you know the senses
you have the benefit of asking questions
that are not based on sentiment they're
based around demographics there's
probably less lying or concern of that
there are many other variables which can
be on this team so I don't want to claim
to cover all i would say that on those
three variables of kind of cost time and
essentially sample completeness you're
looking at on this continuum on this
continuum you're looking at basic what
the basic analytics looks like right now
you know you have a lot of polls out
there that are just showing raw counts
you have essentially where you're
matching marginal demographics which is
the main method of of what you're using
in survey research and you have kind of
more advanced methods that people are
now testing out and kind of questioning
about looking at regressions and then
post ratification on top of that there's
been a lot of look into understanding
this in the research and i'll point out
to recent studies that have been very
interesting on this and the first one is
something in which a bunch of
researchers went out they did the same
sort of questions asking about
demographics actually using both
telephone and internet based probability
samples and then a range of what they
referred to as non probability samples
more of a convenience what is
interesting to to this and is thinking
about is that they did a simple raking
method on top of it in which they
matched to marginal demographics shall
mention them the next few pages and they
look at errors that we range from about
3 percentage points to four to five
percentage points in this paper they
looked at the statistical significance
the difference and said look you're
getting a much better answer when you
use probability sampling and what we're
going to be dressing today is whether or
not that would be different if they'd
use more advanced analytics on top of
the data that you see in this section
and second of all being that for a lot
of questions we don't know the brawl
truth that we don't know the ground
truth we're going to question whether or
not four or five percentage points is a
serious concern or whether or not this
is actually in the same sort of realm of
meaning in the long run for most people
similarly pew went out recently and was
able to ask the exact same questions
both on the internet and the phones and
they determined
median difference of about five point
five percentage points one of the things
that I focus on also at the end is
saying look this was a question of not
one being right or one being wrong but
then being different and trying to get
in to determine thinking's is five point
five percentage points a big deal they
never do it for us what sorry so this
was looking at exact same questions they
had about 50 questions and they look to
see just basically if you ask the exact
same questions on internet and on the
phone how many percentage points
difference were they so for the for the
average question meeting questions I
we're going to look at four datasets
today the GSS which is a very
impressively done expensive survey where
they go and repeat to try to find as
many people as possible an RDD poll one
in which was done in quota sampling
where they just stopped when they reach
certain marginal demographics and one
which is completely opt-in kind of some
background kind of data on it to think
about there's no Tazz looking at that
continuum as you were mentioning earlier
on the great basis of kind of money and
time GSS is something that takes several
months to complete in essence and it
costs about three dollars per person per
question standard RDD you're looking at
about one dollar per person per question
in order to do quota sampling on the
internet using so pull fish or some
similar thing like SurveyMonkey we're
looking at about ten cents per person
per question and in order to have a
fully opt-in sample which took about two
and a half hours versus you know several
months on the top we're looking at about
2.5 cents per person per question which
is about a hundred full difference from
what you're looking at in the most
extreme situation so a lot of time
difference a lot of monetary difference
in that outcome of course if we compare
our most opt-in situation as far as even
on the most basic thing which is not
even thinking about the biases but
thinking about just looking at the
representation of the sample we get it's
not surprising that it looks very
different if we just allow anyone to
come in and answer the question what
we're looking at here is a reasonable
geographic divide a somewhat reasonable
divide an ethnicity much more liberal
more democratic more higher educated in
this case more males and a much younger
skew so we have that going against us
so the analytics basically vary
depending on what's pulling company
we're talking about but the basics of
what you do in random digit dialing and
probability polling is you're going to
march a match marginal demographics and
they've this gone a long way in the last
probably ten years ago it was based off
simply matching say the age and gender
maybe race maybe one or two other things
education to the consensus essentially
the most recent census you can find now
you're going to be looking at a few
interactions they're going to take an
iterative iterative waiting process and
simply match the marginal demographics
and then if they're looking for a
subsample of people they're simply going
to drop people who are not in that
subsample so if you're looking for
likely voters this can pick sometimes
done with a probability sample sometimes
not but the most part what they're going
to do is akin to essentially saying if
they think you're likely voter then
you're going to keep you if they don't
think you're likely voter they're going
to drop you may be the same sort of
thing if you're looking at a subgroup of
demographics the way that we've been
addressing it is to take approach where
we model and we post ratify and so
essentially this can be done with a
bunch of different types of modeling a
multinomial logit is one way to do it if
you're looking at something with
multiple sets of answers to any given
question and what we're going to do is
we're going to essentially take a few
key demographics we're going to break
them up categorically we are going to
then run a simple model of some sort
some sort of model is going to give us
some sort of basic probability that
people are going to answer for any given
demographic a certain way and then we're
going to post stratify that to some
group of the general population we've
become a lot more constrained in some
sense and how we post ratify if we're
doing this right at this case trying to
be as clear transparent as possible but
a lot of times we're going to model this
part as well so we can model what we
look think the likely voter space is
going to be or model what we think the
general populations could be or model
what we think a Microsoft employee looks
like depending on what we're doing and
so simple regression I'm not going to
get into the details of different
choices and then post ratification and
this is a affectionately mainly known as
mr. P when we do this looking at using
Mechanical Turk data and compared to GSS
or Pew so this is we've pooled together
the answers to 49 different questions
that we found on both the GSS and Pugh
the GSS is kind of a gold standard app
you is another major polling company
that does our DD and we have the online
survey estimates on the y axis and the
GSS and pew on the x axis what we find
is something that looks well we're not
really sure right so it looks decent
it's not like doesn't blow us away it
also doesn't freak us out completely and
we have about a median difference of
about seven to eight percentage points
so this is not overwhelming and it's not
devastating but we need to learn a
little better what do we think the
accuracy should be well in this case
there's a couple things that I'll jump
into first and say that the mean ever
the mean absolute difference was about
seven point four percentage points using
mr. P using traditional polling
technique using the traditional raking
the mean absolute difference was much
larger it was nine point eight
percentage points this not surprise us
too much we weren't able to match in
this case GSS and pew against each other
in all the different questions but we
did have 12 questions of this sample in
which GSS and pew it asks the exact same
question within a year of each other and
when we did that the error that they had
to each other was actually eight point
six percentage points what does this
mean this means that even among the gold
standard even among sort of our DD
polling and an even better type of
polling asking the exact same question
there's a little bit of time difference
but asking the same question they
actually reported answers that were even
slightly further apart than what we were
getting from our
optin sample comparing to pew and GSS
answers we know that's not the exact
same sorry wait yeah so this is a much
larger set of questions boss and the end
what I'll do in the next slide actually
show we we did this with both the PO
fish data and with the mechanical turk
data on the same 12 questions what you
get is a pretty similar type of thing
we're a little worried in this case that
maybe this is not the best
representation of g SS vs pew we're
actually pretty surprised at how far
apart it was so we tried to think a
little more of it there just weren't
that many times and what's the same type
of questions we're matched and it was
very easy to do so we did think about
how this would look kind of
theoretically in a sense if essentially
you took a simple random sample and you
assume that the GSS and q were
essentially were the actual truth how
far apart would you expect any given set
of poles to be if you asked a thousand
people these axiom questions at the same
time with two different polling
companies about 3 percentage points you
know what we're seeing from different
works that people have done is somewhere
in the realm of four and five for a lot
of the comparisons in which they've done
which include the actual server error
that happens the actual coverage and
non-response error that goes on top of
simple random samples so then it comes
and begs the question again that if we
can sit there with the worst opt-in
samples and have seven point four
percentage points difference it's mostly
accurate it's a lot cheaper it's a lot
faster and it's up to decision makers
decide if that's close enough for a
fraction of the cost and a lot more of
the speed yeah
literally you hope you need those who
account for a fraud or someone
maliciously and getting data into those
holes like robots and then probably is
the cost of cleaning that I mean yeah
this act right so basically for what we
blow it on and trying to all systems are
doing this kind of a regularly trying to
evacuation systems that are going things
into things that have reputation systems
which would try to block against that
thinking about having login is mandatory
for different things so that you can
match and try to do basic counter
measures to ensure that people are
people and not bots but yeah those are
those are things as an opt-in system has
to face that a random system would not
have to face is the fact that you can
ping it and you can keep hitting it
again and again when it comes to sub
demographics when it comes to sub groups
it's very hard to determine whether or
not you're good or not in a sense that
we know that the GSS ARP you or any
other companies estimates for a detailed
kind of sub group is going to be pretty
off but we can look at is something
which we did previously which does have
a ground truth which was that we looked
at the 2012 election on xbox and on xbox
we asked several hundred thousand people
about 350,000 people who they were going
to vote for in the upcoming election we
looked at this right before about two
days before the election this was the
Xbox estimates using a few key
demographics and this is what the exit
polls showed two days later this is
looking at a combination of any subgroup
of two demographics like white males or
over 50 year old women over 65 year old
women and we were pretty comfortable
that this was this was pretty close this
was actually a pretty nice thing for
something that I said a very low
marginal cost yeah so we were using
actually this was it live so we're using
at the time the 2008 voters as the post
stratification space so this was to be
clear after this was done in November of
2012 using people who are on the Xbox
modeling them there
bowling and then matching them out to
the 2008 voters sorry so what is it goes
so the larger circles are how large the
demographic group is and all of these
demographic groups are two dimensional
sub groups both race age gender
education and party affiliation so any
sort of any double combination of that
into combination of that I think they
kind of key take away from what we're
looking at is to say that essentially
for the last you know 80 years or so
it's been a race to try to get the most
perfect samples and spending a lot of
money and spending a lot of time and a
lot of effort and what we're looking at
over the horizon is a race to find the
best analytics because it is going to be
increasingly difficult to reach those
samples regardless of how you try the
cost will continue to rise the time will
continue to rise and I'll leave you with
a thought from Andrew gentleman who is
concerned that the president of the
American Association of buggy whip
manufacturers that was his call a tow
arms against the survey research
community that was fighting against it I
wouldn't go that far but I think it is
it is kind of one of those turning
points in which basically you need to
have a dimensional shift in how they're
how they're thinking about it moving
away from this question of sampling
which they're holding on to and pushing
towards more more intense analytics so
thank you jelly just one suggestion
dinner one question a suggestion that
you start by putting everything some
against the simple item sound boy I
don't think that's very effective first
first that you basically anybody has an
ology that few the world will see that
you are trying to beat a dead horse the
literature that gone far above anup
singh random sample stratified samples
non-response bias these all well you
know well understood but i think your
point is well made the point is like in
practice people probably still
synchronizing random sampling they're
doing a simple random sampling but but i
think in terms of the
in terms of the way you try to move the
research forward you probably shouldn't
really think that you're just improving
pop simple random sampling or even use
that as the at the benchmark now I think
yeah take a total enjoyment that I don't
want to be looking like i'm using a
false baseline and i understand that
they've moved moved past it i do think
though when you look at up and i should
make it more clear though but when you
look at what they're publicly kind of
saying as far as theory goes debt yeah
exactly that's what they're trying right
but the most serious question here is
really i agree with you it's not a
probability humorous personnel probably
the key element here and i think what
you're saying is that coalition prob is
much lower than people realize is the
correlation between the value you want
to look at and the probability they will
report that value given given that about
it that's that for all these samples
that the bias is the college the
correlation between the probability
response and what they what they're
reporting so what you're saying is all
these opening surveys themselves the
coalition actually is quite low and the
problem is is that without having ground
truth it's hard to be able to determine
what that is and the problem then the
reason why that the survey research
community is pushed so hard against
things in which we don't have the basic
theory is to say look most things we
don't have a ground truth if you don't
have a statistical theory like you do
have a simple random sample or
suppressant 0 plus some sort of
estimation of non-response and coverage
error and how can you ever determine
what's bias and what's the variance
based off something which you're trying
something totally do and don't have any
sort of background information on and
our argument is to say that look it
that's a problem all over the place and
we're not really sure how much worse
these opt-in samples are or better worse
or different as it were then as far as
the tail end a probability sampling yes
but you can take the serum food though
right by because the shoe is to the same
serie right it's the simple random
sampling will probably simply just make
that coalition see reticle again to be 0
sure in practice is a top because it
don't response by risk yes but you're
showing evidence probably some of the
research you can devoting to studying
that the property people I'll bring you
know that type of Mecca drive how to
call later and that give you a base to
set up the seer
are you you know moving away from the
simple random sample top of serie but
moving to that direction I'm saying that
we we should move on you gotta kiss on
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>