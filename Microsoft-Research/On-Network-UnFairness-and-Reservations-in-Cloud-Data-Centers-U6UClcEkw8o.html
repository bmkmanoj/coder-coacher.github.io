<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On Network (Un)Fairness and Reservations in Cloud Data Centers | Coder Coacher - Coaching Coders</title><meta content="On Network (Un)Fairness and Reservations in Cloud Data Centers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On Network (Un)Fairness and Reservations in Cloud Data Centers</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/U6UClcEkw8o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's my pleasure to introduce summoner
who's I won't waste much time
interstellar researcher he's been an
assistant professor for you for six
years now and he did his PhD from UCSD
and has done some really good work on
data centers and on isp networks so
he'll talk about soph his latest word
really hot off the press in today's top
thanks to me so the title of today's
talk is on network fairness and
reservations in data center networks so
but before I talk about this particular
talk I will actually just give you
quickly a summary of the things I've
been doing about the things that I'm not
going to talk about today so the first
topic is actually the things that I'm
going to talk about the second part is
actually mitigating a vm consolidation
impact on tcp so it turns out when you
have multiple virtual machines sharing a
core processor which is increasingly
common in many of the cloud environments
what you can have is virtual machines
are actually scheduled in a round-robin
fashion it introduces CPU latency and
that actually impacts DC piece
performance because packets are you know
take much more longer to actually get
processed and that impacts tcp
performance so we are a whole bunch of
hypervisor based solutions where you can
offload certain responsibilities of tcp
light for example in the receive path
you can actually offload the
acknowledgement generation into the
hypervisor to speed up TCP transmission
and TCP transport performance so on the
other transmit path what you can do is
you could actually flood packets into
the driver domain or the hypervisor and
let the hypervisor actually trickle this
data on the on the network that actually
improves transport performance
significantly we also looked at various
scheduling based algorithms to improve
latency or processing when you have
multiple VMS sharing a core and these
are some other topics that I've worked
on and the other major area of research
is about high fidelity latency
measurements so do you actually obtain a
micro second level latency measurements
in switches and routers in commodity
you know data centers for example or in
storage interconnects or in trading and
in other platforms where latency is very
very critical for diagnosis purposes you
need these kinds of Statistics
collection natively to be present in the
network devices and we have a whole
bunch of algorithms to actually do this
efficiently in terms of both aggregate
latency is well as per flow Layton sees
and diagnose any kinds of anomalies in
the network and so today's topic is
mainly about data center networks on
which I will actually focus on two main
things one is actually fairness issues
and the other one is about reservations
and data centers we also have some more
work on intelligent intelligent packet
spraying in data centers that will
actually work well the TCP I'm not going
to talk about that today much okay so
the main motivation right in a large in
many of these data centers usually a you
know think cloud or you know even
private data centers a lot of times you
have memory and CPU strictly sliced
across jobs or applications or ten ends
okay unfortunately network sharing in
many of these data centers tend to be
more marketplace like approach where TCP
flows are actually competing with each
other to grab as much network bandwidth
as possible okay so broadly motivations
were two questions right so one is
actually if you run TCP in these kinds
of networks so what are the various
problems associated and we discovered a
new problem called tcp outkast problem
which actually relates to unfairness
when multiple TCP flows compete okay so
the next part of my talk is going to
focus on the question how do we share
them the network across multiple tenants
such that you know you can provide some
isolation properties across various
doughnuts and a part two of my talk is
going to talk about a new abstraction we
call temporarily interleaved virtual
clusters okay so these are two parts of
my talk so the first part some point you
mentioned you
well not really do you see rise actually
in the open flow market well that is
actually more isolation in a management
perspective right you want to have a
physical Network actually isolated well
from a management perspective or mints
isolation it's actually it's like they
all actually are sharing the same data
plane okay the thing that they don't
well the the way they want to like
actually market it is to tenants can
actually have access to the individual
switches and they can deploy their own
routing protocols they can deploy their
own stuff to manage that address space
performance is actually right so so
although they don't actually quite I
mean they give you a bunch of mechanisms
but those mechanisms are still the
capabilities and real switches is like
really bad right now so I'm going to
talk about a slightly bigger abstraction
I think here what we are trying to talk
about is like in terms of tenants in a
cloud right what kind of network
reservations can you give these tenants
such that their jobs get predictability
okay so that's the broader concept but
you're right i mean maybe there's some
you know philosophical similarity but
they're operating in a slightly
different granularity okay so the first
part is actually about this new problem
that is we discovered it is called the
tcp outcast problem essentially a tcp
there are two properties of tcp that we
often take for granted right so one is
actually work conserving if there is
spare capacity TCP will try to use it
and the other one is actually some
notion of fairness right if there are
multiple flows competing for some bottle
in capacity would assume that TCP flows
will actually get a fair share right so
question is do they actually to hold
true in data center networks it turns
out there is a TCP famous TCP in class
problem which actually you know reflects
on the first aspect of TCP which is the
work conserving aspect right so when
there are multiple clients when there
are multiple servers contacted by a
client and you have a sequence of
responses coming back it is
ganized fashion that actually overflows
the switch buffers causes the gas
congestion collapse the reason it
actually affects the work conserving
properties because some flows actually
enter time out and even though the span
capacity opens up these flows are not
actually responding rapidly enough
that's why there's all kinds of
retransmission timeout fixes and so on
and so forth to deal with it okay
there's been a lot of interest but but
this doesn't actually you know talk
about the fairness aspect okay so but
what about fairness right so in order to
actually understand fairness so we said
like you know let us build a data center
network which is organized in the form
of a some sort of a factory multi rooted
tree topology so in our testbed what we
did was we built a simple k equals 24
fan tree out of 36 HP quad-core servers
running Linux and we basically had 20
switches composed out of these net FPGA
for port switches and we connected into
no caicos 24 factory topology with
routing configured using open flow okay
and we ran a very simple traffic pattern
we call it the many-to-one essentially
this is like the equivalent of a reduced
phase in the MapReduce type application
here we have s1 the source s1 s1 the
server s1 is that is the destination and
all the senders s total as 15 are going
to basically transmit traffic or TCP
flows to to this destination as one so
in terms of routing I mean you can think
of s2 is in the same tour t1 so in this
case it's actually a direct route s3 and
s4 actually go why are the aggregation
level and the rest to actually go all
the way to the code to come back to the
to the destination s1 okay so there is a
natural you know there are three classes
of flows there's like 12 half load to 24
Hawk flows and like the rest of all six
half flows essentially going all the way
to the core and then coming back it's a
very simple traffic pattern and we just
focus on this tour switch mainly okay
and if you actually run the TCP
experiment what we find is basically a
very interesting so the throughput here
is showing that the red flows which are
basically the ones we
our farthest away from the from the
destination get close to about 70 to 80
megabits per second while the one which
is the closest one which is the blue
flow gets about 10 megabits per second
okay so that is about 7 X less than the
farthest flows in the network and the
four hop flows are somewhere in between
they get about half the half the
throughput as the the the farthest ones
in there to the six half plus resort
like maybe there's something wrong with
the rtts let's check right so so this
actually shows that indeed the the
particular flow which has which which
starts at the server which is in the
same tour has the lowest RTT followed by
the for harp and followed by the six hub
ok so the arteries are in sync with what
you would expect except that the
throughput is actually completely you
know off we completely counterintuitive
because it's actually exhibit in verse
RTD bias naturally you would you know
you would think of rtd bias RT devices a
well-known property but this is actually
just the inverse of it so what's going
on so why is this why is this happening
ok so this is what we actually call the
tcp outcast problem ok so the outcast
problem is basically some flows are
getting outcasted when multiple flows
are actually sharing a particular
bottleneck link and in many cases we
observe like you know these kinds of
order of magnitude difference between
certain fleurs ok and we conducted a
large number of experiments to just try
and isolate where the problem actually
lies and one of the simplest experiments
here is when we vary the number of six
hop flows with two half loss okay so if
you have m65 flows and ends and two half
Louis when we have 26 hop flows and one
you know six up to have flow right here
this is the average throughput obtained
by these flows when you actually look at
0 2.1 seconds 0 2.2 seconds 0 2.5
seconds as time evolves in some sense
okay so the first here the red here is
actually showing the average throughput
of a to hop flop from time to you know T
equals 20 to the point 1 seconds so
really what is happening here is the two
hop flow is more aggressive than the six
half flow
as you would expect but as time
progresses 2.2 seconds and point five
seconds the two hop flow actually loses
out so it seems like you know as the the
amount of time you actually spend
competing the two the two have flow is
like sort of getting squished by the
other other guys and actually for first
when when you have 66 off flows and 12
hablar you can see that actually even at
point 1 seconds there is a fair bit of
difference and yeah so so as as you
increase this to almost 12 6 off loes
and 12 hop flow this unfairness goes to
almost like 7x and in many cases
actually can even go up to 10x so that's
a significant significant variation and
this is the instantaneous throughput as
you can see all these six half flows are
like you know they have their peaks they
have their valleys they keep oscillating
this that and the other but the last guy
the black guy is like completely dead it
just makes up every once in a while and
since a few few packets and then after
afterwards it's dead ok ok so we
connected a whole bunch of experiments
with like you know when we staggered the
start times one of the problems then
cast was when you had synchronized flows
actually starting at the same time so he
said like let's stagger the flows and
see if this problem persists yeah it's a
good question it's not like we were
discovering this we just build the
testbed we run some very simple
experiments we said no we had absolutely
no intuition we were actually very
perplexed no we wanted to actually start
with some MapReduce type profiling and
we had like two years back we had a
vision of oh lets you know think about
these network reservations and data
centers so in order to any
experimentation we need a test bed and
the test but needs to be realistic with
some realistic topology so we've
invested in buying all this equipment so
we said okay fine let's just create this
test bed run some basic benchmarks ok
run some very simple many-to-one all to
all you know all the traffic patterns
really like this can't be right go and
check right we see like this huge
variation
this can't be right right why is this
actually happening then we started
digging deeper deeper deeper and finally
we got to where we are yeah it's it's
one of those things we hit upon so
sometimes I think I think a lot of times
you don't actually in production
networks i think maybe by you know you
observe some you know high-level
application level normally and you start
digging deeper but we didn't have such
an application we didn't have a
production data center either so yeah so
when we actually varied a whole bunch of
things right so staggered arrivals with
larger number of flows this is actually
interesting initially I showed you the
12 ease 21 split you can actually try 1
22 10 and even then this would actually
happen so if you keep the ratio actually
the same right as you have larger number
of flows competing with smaller number
of flows it just seems like the large
number of flows always will respective
of whether there is background traffic
whether the bottleneck is in the network
whether we vary the switch buffer size
switch buffer size actually has some
effect on delaying it a little bit but
it doesn't actually completely remove it
and even when the RT DS are exactly the
same that is all flows are exactly the
same it's just the flow differential in
terms of number of flows arriving on one
port versus the other port that's what
is causing this this problem okay with
psycho option with multi way NP tcp the
multipath TCP and you know having
multiple contending ports when you have
three input ports draining onto one
output port even then this this problem
occurs they're all output good yes that
is actually tail drop tail drop is what
matters actually this is exactly the
conversation as having with with g2 and
it will be interesting to try and
understand that we haven't actually
completely you know studied all the
parameters they just way too many
parameters but but that's an interesting
dimension to look at what you know
switch architectures are prone to this
kind of an effect on DCP that's an
interesting question to follow up okay
so this is the one question is like why
does this happen right so so we using an
you know using simulations using the
real traces
collected from these these these test
bed right we try to hone in on the
actual reason the actual reason is
basically something like this very
intuitively explainable so if you have
two ports a and B and draining on to a
port see and what happens is both these
ports are individually clocked and
there's they're like basically they are
not actually synchronous right so they
actually can start at arbitrary times
but they all have enough traffic so
which means there is enough back-to-back
packets and the gap between these back
to back packets is roughly the same
because it's actually a switched
Ethernet with no contention from any
other any other any other device ok so
the inter frame spacing is exactly the
same and all these are basically
aggressive really flows they have enough
packets to send they're all trying to
drain on to the same output port with
same capacity and everything the so the
timing sequences are exactly the same
they're all actually heavy big size
packets if this happens what happens is
you could have one port running slightly
ahead of the other in this case let us
say port B is slightly ahead of port a
ok so when the port when the packet on
port B arrives it actually occupies the
last slot into the queue just gets in
and then the the the the packet on port
error is slightly later it actually
finds to the queues empty queue is
filled up there is no more space so it
gets dropped and this sequence actually
repeat several times so you could have
actually consecutive packet losses okay
so so a consistently lose this packet
here here and here okay and this can
happen to poor both port a or port be
there is no bias against any one
particular port it's just that two ports
when they contend is equal likelihood
that like in any of these ports actually
entered into this situation and we call
it the the port blackout yeah why is
there not enough data to because they're
all aggressive right they're all
actually trying to send a train of
packets
randomness in the scheduling that's
right no this the scheduling is
basically just a simple round-robin type
scheduler right you just basically
variation in the back inside there is no
variation of the packet size because
there is enough data to send it is a
bulk flows they're all trying to max out
the empty you see sure that may actually
distort the timing a bit but you may
still have a train of packets which are
not your acknowledgments but you are you
know you just congestion window right
you're sending a whole bunch of packets
it may have some mixing with other flows
but that that is why we actually try it
out with all kinds of background traffic
in this and that that actually
completely eliminate the problem it does
as soon as you introduce randomness sure
it will alleviate the problem to some
extent but it not completely eliminate
it right so doesn't make sense purport
fair for input port there that is what
is happening in this case no this is all
it is saying is you will have see the
number of drops does not actually quite
matter as much as the number of
consecutive drops the number of drops is
purport fair you are absolutely right
but if they are happening at random look
random instances then it will not affect
anyone tcp connection much know if it is
randomly spaced then the chances of you
having consecutive packet losses from
money any any particular TCP flow is
less consecutive packet losses actually
are the more devastating ones because it
will cause you to go into a timeout
because you do not have any more do you
do packs coming into it
yes a little bit a little while just for
a little while and then there's some
randomness that will actually break it
and maybe later on a will actually get
into the sport blackout it's just that
this spurt yes yes absolutely break that
that sequence of drops yes if these are
actually somehow artificially
interleaved yes the gaps are so even if
if there is some differential right so
so I mean there's still only a small
finite window but you can actually still
actually very it a little bit here and
there but still it will actually end up
hitting the case where the queue is
filled up only if it actually goes ahead
of be right so the randomness has to be
enough that it preferential
preferentially goes in in favor of a or
b rights that that is when the the thing
actually gets distorted small variations
actually will not help you much ok
probably one you any privacy conjugative
now when you say probability right it's
also got to do with intensity because
there's only one flow which means no
well that won't work
there's only one way to go up there
multiple rains that are coming in and it
fish head in are you doing something
like this this guy's always going to be
get much more it's going to be reverse
reverse exactly this is like red this is
exactly like red what you are talking
about is if you actually had instead of
a drop tail you had an active queue
management like basically like a red
right if you do red essentially you are
not waiting till the queues filled up
you are actually having random
probabilistic drops the probabilistic
drops will affect both ports with equal
probability right so that actually does
not solve the problem that will actually
get you back to the TCP RTT unfairness
the process also problem in the sense it
will it will it will get rid of the
inverse RTD requires it will be unfair
the other side make everybody else
compete for you see thing about it like
you're in a gang of friends and only one
of us et cetera nothing is stopping the
other guests from going ahead where is
the other input port has just one flow
and that guy go to drop is going to
suffer this side you have nine floors
say boys u10 you have nine plus now we
have no drops and you are continuing no
actually one thing I'm actually saying
is a few drops don't actually hurt ECB's
performance you can still recover from
it really fast what really hurts it or
what kills it right in some sense is the
consecutive drops as you lose an entire
tale of the congestion window it enters
a timeout and occasionally tries to send
of one or two packets and if that gets
unlucky then you won't even ramp up to
have three new packs actually okay
equals you won't see the problem you
wouldn't see the problem I mean this
should happen even if it happens so here
is the thing if you look at this yeah
yeah i think i can actually you know i
think if you just quickly look at this
right see even if there are two flows
right it is not like all of all the time
all of them are getting you know it is
like same throughput or anything you
just lot of variations right overall it
aggregates out right because you know
there are occasions when a
lo X actually goes ahead of Y and vice
versa but it all averages out the
unfortunate thing is here this is on
this is this goes into the timeout and
after the closer to the time out it is
dead it every once in awhile it actually
tries but it just cannot recover back
that's the problem okay okay so this is
actually like evidence of the port black
or be found in an s2 as well as like I
said you know in real traces as well so
existing workloads meaning like
mapreduce so the you know we don't have
any real production data centers to
actually test it out it would be great
if you can actually like you know yeah
this may happen yes yes yes I think it
will be good to validate it on a
production desperate and I don't think I
can claim anything beyond our little
desperate but this is at least one thing
that you may want to look for in a
production desperate sure so this is
actually just you know corroborating
what I just said except in ns2 I mean
this is actually even reproducible in an
ester simulations you could see that you
know these are the these are the drops
on the input port these are the NQ
events on the input port B and these are
the DQ events on the output port p so
you see that this guy gets dropped this
guy gets in this guy is just trained
basically and so on so this actually
pattern repeats many many times over if
you actually carefully look at it we
have found similar things even in the in
the real test fit as well okay so the
impact of Port blank out like i said is
all ports exhibit this kind of a thing
and i actually compete for an output
port except that the port that has
smaller number of flows eventually
actually loses out and we actually did
some very simple analysis to figure out
if you know how these consecutive drops
actually impacts tcp and there's just a
simulation results showing as we vary
the number of consecutive packet drops
the trooper drops down for one flow much
more drastically than two flows and then
the if there are large number of flows
then it actually like it is much more
resilient because conjugative packet
drops are now smeared over many many
many many flowers but you may say well
you know i could actually keep the ratio
one is to 10
all and actually increase it from tennis
to 120 right that would actually mean
that the consecutive packet drops will
now be spread over multiple flows it
turns out actually this the duration of
the port blackout actually increases
once you have larger number of flows
they will compete for large larger
amount of time but eventually the weaker
guy always loses out it just that the
amount of time it takes for it to get
killed increases as you have large
number of flows so here we have looked
at that's right that's right i think you
know i mean it's it's a continuous trade
off so i can't put exact I can't off the
top of my head tell you the numbers but
for one is to 12 we could see 7x
differential between the top guy and the
lower guy for one Easter 12 flow density
ratio it depends on several factors just
the ratio plus also you know the amount
of switch buffer size the rtt you know a
whole bunch of factors that's why it's
not like it is one number I could go
with the paper has a lot of details
about all kinds of experiments but but
yeah so so that's a good question i mean
there's no one number so all we know is
like you know if you actually have you
no one is to ten type of ratio between
the number of flows coming on one port
versus the other you would see at least
an order of magnitude close to an order
of magnitude differential between them
okay so how do you actually mitigate
this problem one is actually to go with
you know things like red but that will
actually break you know twist or you
know shift the the the bias in the
opposite direction the short flows or
the two half Louis now become more
aggressive than the others then there is
a TCP pacing unfortunately pacing is not
so much of a you know it will not work
that well in data center environments
because the bandwidth delay product is
actually very very small there's not
much room to pace especially if these
are actually sending traffic it's very
hard to pace and you have to do it in a
dynamic fashion adapting it is actually
quite hard
stick fair queuing if you do fair
queuing of course you know that is a
network layer solution to ensure
fairness right obviously this will work
well this is TCP pacing results sort of
you know you know decreases a bit but
you know it's still one easter to kind
of a ratio if you do read the opposite
effect happens the short flows actually
become significantly more aggressive
than the others one thing we actually
try it out is he said just let us get
rid of the flow of the hop length
differential between all these flows and
make it all uniform across the board so
there is no shortest path routing
anymore you just basically even route
the flows within the same tour all the
way to the core and come back this is
like we will be extreme okay well we
will be without the shortest path yeah
without shortest path shortcuts exactly
and this you could do for fat retype or
provision or full fully you know full
bisection bandwidth type networks
because there's enough capacity at the
core and the advantage of this is
basically because you are forcing the
local going to go all the way to the top
it has better mixing with other flows
and that allows you to break the rhythm
a bit okay as opposed to because it's in
the same tour right the sequence is
really predictable really really
predictable and you want to break that
by somehow pushing it all the way to the
top so you mix with large number of
flows and you don't have any unfair
drops so it's sort of a you know it's
kind of a simple clever you know
whatever hack that you already can
effort in a full bisection bandwidth
network ok so just basically if we do
this we can see that the differential
has reduced quite a bit i mean is still
some variance across flows depending on
how long you actually keep them but its
secret significantly reduced compared to
what existed before so can stick fire
killing is obviously i mean it's a
network layer solution it makes the
switches complex but it will boil
fairness is important which one
this one oh this is just basically the
t1 the problem i showed before was a t1
but it also happens in a one i mean if
you actually have persistent s2 to t1
interface no but it makes you go all the
way to the top and when you actually
have when these flows the the green and
the blue flows interact they do so out
here they don't actually interact here
that's right that's right that's right
so all the big thing happens at the core
so the mixing when you when you have
enough like you know because you have
enough number of flows at the core it's
like you know you have enough mixing
that you break all these all these
random you know these synchronous events
okay that's the summary of the first
part we discovered a new problem I think
we have discussed this so let me move on
to the second part which is basically
about incorporating time-varying network
reservations in data centers ok so in as
a simple motivation long-term viability
of cloud platforms is really dependent
on cost and performance right so you
want some predictability in performance
otherwise enterprises may not be willing
to actually move to the cloud and all
obviously it has to be cost effective as
much as possible right so the question
is how do you make it predictable and
cost effective so one of the things we
argue is that providing Network
guarantees is important because
especially in oversubscribed data
centers if you have one is to one full
bisection bandwidth even then maybe it
is important but it is not as important
as if you have an or subscribe cluster
right because the tenants will actually
get significantly varying amounts of
bandwidth and that may need that may
mean that your MapReduce jobs or
whatever your applications are may not
have any predictability in the
completion time okay so question is how
do you actually do this and there's a
lot of prior work in the space and so
this is actually MSR Asia this is
actually here
this is ms our cambridge so really in a
theme park yeah so yesterday is giving
this talk at university of washington
right it's like then I was being honest
like I miss Alicia I mean sir then I
realized all of these elements are right
anyway so so the common observation
among all these things is that they they
usually make any reservations right
fixed reservations throughout the entire
duration of the job that is they don't
assume that jobs have varying temporal
requirements like for example you know
during the MapReduce jobs they don't
actually really require the network
during the reduce phase once the shuffle
phase happens then you know there's no
more usage of the network all the
computation happens locally yes actually
see wall and but they don't actually
very the weights along the duration it's
not fixed reservations per se but they
don't but I mean no I what I meant was
they didn't consider right none of these
papers actually considered sure you know
it's like it's a new dimension so the
how next explore this new dimension you
could say that like it we see could also
do that but sure but they didn't right
so that's the observation no sure sure
that is why actually our work is closely
related with virtual cluster the VC
thing is what we actually extend by
putting it more putting the temporal
dimension we call a temporally
interleaved virtual cluster but you know
I guess the reason why included all
these things is because they are sort of
broadly related to the problem of
isolation network isolation so it would
be unfair for me to not include it i
mean i'd be happy to not be included but
but i think the reason i included it all
of these is because at least you know
identified that traffic isolation across
across individual tenants is important
so that is the broader the-- sure but
our focus is mainly in the virtual
cluster pipe fixed bandwidth
abstractions right so you allocate one
fixed bandwidth throughout the entire
duration of the job
but when you actually profile a whole
bunch of these MapReduce type
applications right so here is a Hadoop
sort with 4 gigabytes per vm that's
about thirty two waves essentially a lot
of traffic and after that like you know
this very little traffic if you look at
hive aggregation it has some small
amount of traffic here significantly
higher amount of traffic here Hadoop
word count has periodic you know
behavior and then finally this is hype
join which has some amount of
requirement here then there is a period
of lull and then there's some extra
activity and so on and so forth in other
words a lot of these applications
exhibit significant temporal diversity
in their requirements the question is
can be tamed this temporal diversity
right and and basically a bit more
number of jobs and how does it actually
even help right so here is a simple
example it is a simple tree right so
there's a bunch of physical machines
here so it's a it's a it's a three level
tree right said the leaf level you have
for physical servers with to vm slots
for physical server and here you have
agree to aggregation switches and one
for switch and this is a non
oversubscribed so in this case it
happens to be you know 500 mega bits per
second here and one gigabit per second
here ok now let's say you have a job
which requires about 500 mega bits per
second per vm ok and you have a four
node job now you can't actually put both
these nodes on the same physical machine
right sorry both these both these VMs
cannot be occupied for this job because
you know you this is a 4 vm job so
you're actually required to span two
different physical machines and that
would require basically you know about a
gigabit and you only have 500 megabytes
so you can't actually fit this job this
way the way to actually fit this is by
putting one vm or occupying one vm slot
on each of these machines so that you
can have you know extra correct you can
use the connectivity the whole
connectivity the question is can we
actually improve the utilization here
because we cannot fit any more jobs of
this type because the network couldn't
actually allow you to ok so yeah this is
just basically saying you know if you do
it this way then if you have a steady
stream of these kinds of jobs you'd have
scheduled them one after the other okay
so the question is can you actually fit
them in a temporally interleaved fashion
okay so so for some reason this whole
thing is cut from that side but
hopefully you can see most of it so the
idea is a so so the idea is instead of
doing that you know you can run the the
first job from 0 to 10 but you can
actually start scheduling the the second
job from five to 15 because as we saw
before it's really using the the network
in the first half of its job so so you
can actually interleave these two and in
that way you can actually improve the
utilization of of your data center of
your VMs essentially and in this trivial
example you can actually see that you
can actually get fit almost 2x the
number of jobs yes yes yes there's this
is the insight I mean this is really the
inside I just wanted to give you a toy
example to sort of give you the insight
as to why this may actually be useful of
course the reality is like you know
there's a whole bunch of jobs then the
question is how much can you actually
pack and you know how do you actually
determine how do you how do you profile
or do you actually understand all these
things are questions yeah absolutely but
I was just this is there's a thread of
an idea here right so this is the main
main example okay so essentially you
know we want to like leverage this
insight and come up with abstractions
which allow you to like you know express
this temporal variation okay so these
are some of the some of the
possibilities so this is actually the
octopus type you know VC model the
virtual cluster model right so we're the
same bandwidth is actually available
here here we can actually extra
parameters we have extra parameters like
T 1 T 2 T and an extra bandwidth here
and a base bandwidth of be right because
you always need some communication with
the Hadoop job master or like you know
job tracker and so on and so forth right
so you need some base bandwidth but you
need peaks at certain times okay yes
that's going to get some tasks from the
master depending on the task that's
right that's right that's right so so
you're trying to learn what is the
pattern exactly haha mean isn't because
our experiment here in our sort of
MapReduce clusters I mean you can if you
look at these big jobs they have the
different many many stages ready have
matters reducers joins and all that
these are typically like a bunch of
views right but if you look at single
task then that might have pretty
predictable profile right but inviting
your you're looking at sort of the
aggregate Oh usage shut off the whole
machine but if you look if you split it
up my task then you'll see that this
task always has this little but that
this attic task has always this much so
you wouldn't have to infer it and split
it up artificially but what you're
saying is that the factor is task
dependent right that's even asks yeah
sure tasks are dependent on the data
right so this summer mood rescued
depending on the data there's some
amount of skew depending on the timing
there's a whole bunch of things that
that can exist we will have to pack VMs
you would add the tasks because sir and
in each task would behave more like a
rectangle as opposed to a vm that
behaves in these ways right so the
reason why we didn't actually go down
that route is because there is a
overlaps between waves right so that
they're not actually strictly sequential
waves so you actually you know once the
the five percent of jobs actually
finished if I five percent of slots
become free you start another fresh wave
and so on so for this like all these
parameters that are very difficult to to
model so one of the things we actually
did was when you do the profiling you
actually understand or we fix some input
size let's say four gigabytes okay so so
long as you actually keep the same
amount of data in the production run but
you scale the number of VMs you get
roughly similar performance x
barring a few extra you know linear
factors like the the shuffle phase grow
slightly because the number of TCP
connections are slightly higher and you
know you can model some of those okay
but the thing that we didn't want to go
into is actually modeling very in detail
all the Hadoop parameters like half a
dozen couple dozen Hadoop parameters
like in memory sort threshold you know
and all these things we don't want to
like in a less silly model we just want
to take the application from the user
run it in a quick local profile generate
the local profile and then spawn it off
on a big giant data center okay so
that's why we don't actually exactly
model these individuals but then don't
extract the bandwidth usage for at the
vm level but sort of adverse a process
level or task level right and then you
can maybe even put different task in
different VMS because the user doesn't
know that's right so in this case
actually we try to keep it as regular as
possible sure you have all these
flexibility about you know on one
particular vm you could put five tasks
and other vm actually can have three
tasks we deliberately try to avoid that
actually who actually you do you can
pack it it turns out if you actually
shift right you can actually pack right
so if this this this this peak this
width is actually really small if this
the Swift is really small but the gap is
actually really high you can actually
slightly push the next job and your
peaks are no longer aligned so you can
actually fit more number of such jobs
but these are just like abstract models
that I am just trying to say like you
know your your applications will fit
into one of these models essentially
yeah the bandwidth Peaks yes these are
bandwidth Peaks
right but in this case when we actually
figure out what these B value should be
how do you actually even obtain these
these parameters right it now is a
complex problem now how do you actually
obtain all these parameters right you
don't want to like necessarily ask the
user to specify these no way that you
want to actually have an automated way
to obtaining these and that's what
actually I will get to really fast you
don't run the task and measure the bed
with you run the task and measure how
much data you're transferring ha ha ha
just how many bytes is that how we are
doing is it's potentially different way
to do it right so cuz then you know
you're done it may be throttle down the
particular job or particular task just
so that you can fit right so in other
words I think I think the question is
okay so one of the things one of the
assumptions we had was we didn't want to
like tamper with the existing Hadoop
framework at all we didn't want to put
any hooks in the Hadoop framework like
that would be the next obvious step
which is you know put at various stages
saying that hey this stage is finished
now we are moving onto this stage this
is when I need this reservation right
you can actually break it up into
arbitrary phases and let the Hadoop
framework actually control the
configuration even we don't be right now
we have a very black box approach we
just say we want to like profile it get
this traffic trace and fit very simple
square waves because square waves
actually allow you to like you know
stagger these in time and fit many of
these jobs in time that is the that's
the main intuition like you're
absolutely right i mean it would be nice
if you can actually somehow program it
into the Hadoop framework so it can
actually say hey I am running a little
late so can you delay this slightly
right that would be really nice to
actually achieve but we haven't actually
done that yet but that's a good
direction yet okay so the idea is to
just basically take these and also this
is actually a profile from one node okay
so different if you actually take if you
profile it over eight nodes and you take
eight different races they have very
different characteristics what we do is
actually we take the envelope profile
the profile which actually gives you the
usage across these eight nodes and we
actually put in superimpose all of them
and take the envelope of of these okay
so that you can actually factor in all
the skew and variations and things like
that in addition we can actually even
add cushion to make sure like you know
any timing offsets right like all the
stragglers maybe one note is just
running it slightly behind you want to
capture all that using some cushions can
still do that I mean we haven't actually
done that yet but but you could actually
add some extra cushion okay so so this
is basically saying okay our approach is
to just take take these applications
takes ample amount of data run it in a
local cluster do the profiling collect
the trace and use the the trace for
model generation by fitting these simple
square waves okay the couple of
important questions one is actually what
bandwidth cap to use during the
profiling arm right you could run it
unconstrained or you could put a cap the
thing is you know as you actually put a
cap let's say you you know when you run
it unconstrained on a gigabit link you
see that the max peak is like you know
maybe 400 megabits per second if you put
a cap of 10 megabits per second on that
that will actually lead to significant
stretching of the job it will cause
basically the shuffle phase to get
arbitral elongated and anytime you have
any TCP connections they're getting
significantly less throughput which
means they all get stretched it turns
out actually very hard to predict what
band which leads to how much run time
for the particular application so we
figured you know this is too difficult
to actually model okay so what we do in
our in our practice is like we run it
using different peaks and we choose a
peak where the job does not get
elongated any any any further or the
elongation is like really minor so then
we say okay this is a safe peak and this
is what the cloud customer would like
okay and then the other question is how
do you actually fit fit the tiv see
model from the traffic trace like I said
once you actually get the traffic
profile the envelope traffic profile you
can you need to now fit what square wave
approximates this traffic profile and we
can actually try a whole bunch of things
to do this this is the one I was talking
about to the respective impact of
bandwidth capping when we ran with
different bandwidth caps like 100 200
300 400 and so on you see that at 100
you can see significant elongation but
like after 300 or so the elongation is
so minor that you know you may say okay
this is this is good enough for this
particular application that is not
actually longer the job any further
and we call this the the no longest
bandwidth threshold ideally it would be
nice if we can factor this prediction in
somehow but it's too complicated okay
the other thing is choosing sir so well
I already mentioned all this this is not
a part okay so the other thing is model
generation now you have some some
profile curve like this and you want to
like fit the various parameters so you
can start with the base bandwidth which
is really low and that will actually
mean that you'll have a significant be
cap in that slot lasting for a
significant amount of width so the
alternate is to actually increase the be
the base bandwidth right that will
actually mean that you really require
this for a shorter amount of the peak
for a shorter duration of time so there
are multiple choices when you actually
come up with like you know these
parameters so the way we actually do
this is using this efficiency thing if
it's defined yet so the efficiency is
basically the application traffic volume
or the reserve bandwidth volume which is
basically the curve under the square
wave and the curve under this the ratio
gives you a sort of a utilization or an
efficiency and you can use this
efficiency to figure out what is the
best fit you can try a few things and
you can actually say okay this is a good
fit okay right so this is actually
telling you that like you know this is a
six percent efficiency here ten percent
efficiency here you can never get too
close to a hundred percent partly
because these are like really sporadic
right if you actually try to like reduce
your time resolution for allocation
right that then you can actually get
close two hundred percent but if you
reduce it too much then provisioning it
becomes really really hard so there is a
trade-off there so you have to actually
try and and you don't want to like
frequently change things because that
will also mean that you can't fit any
more jobs so you want to actually you
know lie on at least the higher order
bits you do not want to model the lower
order bits yeah sure
I think shortly you will apply but it
would be a success yeah i agree but that
also means that we we would it bit jobs
which are strictly sequential the waves
are strictly sequential because the
superimposition is what causes a lot of
nonsense it makes it harder it may be
okay sure sure no III think I think okay
so here is the way I'm thinking about
which is which is probably what you are
also suggesting I think we have the
current model of execution we are trying
to actually extract what we can out of
out of the current model of execution
but given that like we have seen all
these challenges associated with
actually extracting these parameters now
can we actually go back to the customer
and say look I will force you to
actually operate in the sequential
fashion and that will actually maybe
make my case actually much more simpler
right so in other words if you actually
enforce certain certain restrictions on
how the cloud customer can actually do
these MapReduce jobs that will actually
give me some much much more advantage in
terms of modeling as well as in terms of
actually provisioning complexity and
that is a good that's a good point I
think I think we haven't actually
considered that and but we will we will
we can consider but that involves Hadoop
framework changes right
that's hard right because when you
actually come up with a client the
client comes in it says hey here is my
job here is my data and you just figure
out how to actually do this correct novi
profile it we take the sample data and
profile it it's an abstract again and
you you want to not be in the business
of three of their sort of how you
allocate resources how you weird if
everything is running you know what is
sort of you want to look at silk black
box torture but yeah these advantages to
both I think we haven't explored the
other angle yet this new methodology is
applicable it doesn't change whether you
do task waste or you to be in place and
that was that to me is moon me
personally was more interesting that's
right a lot of choices exists right
right so so one of the things actually
we observed is if you actually go for
the the absolute peak this is actually
like you know still six percent
efficiency it's not like a strict you
know high efficiency but this actually
forced us to we could we cannot fit jobs
of this type once you have this so
instead what you can do is sometimes you
can actually relax this a bit reduce the
reduce the bandwidth peak and this
actually allows you to actually fit more
number of jobs because you can stagger
them really well so the question is
whether reduction of this is is
impacting the application or not and
that's right that's right yeah some of
it is like right now I think right now
we have a bunch of heuristics to do this
like the heuristic is like you know if
the efficiency is less than some gamma
which is like six percent then we
actually reduce the thing so that like
you know the efficiencies close to an
alpha I mean become of these parameters
this is sort of an arbitrary thing based
on our you know our experience with
these applications it would be nice if
you have a larger set of applications
and so that we can learn these and we
can come up with some rule that is
broadly applicable but we haven't
actually good because you're only
running the application ones you can
sort of figure out how long the actual
execution time soon downloading the data
and so then you can sort of pretend
maybe let's say that this task is
running slower at the lower bandwidth
you can sort of estimate how much longer
that will take and what effect you have
so I've got any good simulators I think
so I mean orianna do is I know there are
simulators but I'm not so sure if
they're any good even very simple
simulator will be better than just my
produced simulators right I mean all you
need to do sort of let's pretend you
have try rate this much data yeah
actually one of the things right right
so but but wouldn't that actually
require you to work with the data and
run the application right so which is
then exactly that oh like a projective
model okay so right now our projector
model is actually like I said although
this paper doesn't have the projective
model we have a follow up we submit it
to hot cloud but this this paper is
about once you actually have a profile
run with like four eight and sixteen or
something locally how do you project it
to let's say 10 to four and one of
things we observed is as long as you
keep the amount of input data the same
right there the four gigabytes per per
vm or whatever that actually gives you
very small scaling factors right like
for example like i said the tcp ramp up
effects the number of connections these
are the things that actually influence
in the they sort of push your final
production run for you know completion
time slightly but it does not actually
change it a whole lot actually turns out
so as long as you keep this regular you
can actually project to from from four
nodes to 10 to four nodes as long as you
keep the amount of input data the same
across knots so so yes the more complex
the more fancy we can get to that will
be great it's just too hard to get down
to very very detailed models and but we
haven't actually like really looked at
simulation yet completely so maybe that
is a possibility maybe that is a
possibility right so this is just
basically giving you the overview of of
the things profiling model with run with
different bandwidth caps obtained you
know a cost model associated with like
different bandwidth caps then we want to
actually give like a cost bandwidth
matrix of some sort so that the customer
can actually pick this is the bandwidth
cab this will give you this much and
this is a cost for the bandwidth right
so these are the things that you can
actually project to the customer and the
customer will actually pick whatever
like you know he likes and then you can
actually go ahead and allocate it and
there is a whole allocation algorithm
which is a simpler which is not it is a
simple extension of the octopus
allocation algorithm actually at this
point we just consider the temporal
dimension when we consider whether we
can fit a job or not we actually also
need to worry about whether the temporal
allocation vector right matches with the
available capacity in the in the links
or not so that is a you know we need to
have extra dimension of searching but it
turns out it doesn't actually like and
increase the search time much because we
have a faster dynamic programming
algorithm to do the allocation really
efficiently so it's not actually so much
of a complexity issue and then you
actually run it on the production data
center which is the sequence of steps so
one of the other things we were also
considering is how do you actually
enforce these right so octopus actually
uses the hypervisor based enforcing
right so they actually collect a whole
bunch of things from the network every
RTT and try to compute the maximum fair
and
we'll all kinds of things all kinds of
enforcement at the hypervisor so I'm you
know we thought like a note then the
question is like can be actually
implemented in switches turns out
actually you know the key observation I
think which makes us believe that it can
be implemented in switches is that the
number of concurrent jobs that need to
be at intermediate switches like
aggregation or even core is really
really small because a lot of jobs
actually fit within Iraq they don't need
network reservations or you know the
need very minimal network reservations
so for the larger jobs you may not
actually fit within a job and the number
of such large jobs is actually not a
whole lot so the more you actually go up
the tree the number of concurrent jobs
is quite small so if you are like in 100
jobs you can actually easily define per
job aggregate that you can actually
configure in switches and modern switch
is actually support like you know
thousands of rate limiters you can
actually do simple rate limiting yes
what is the source of this observation
this for pure than 38 it's just based on
a 16,000 of sorry 5,000 jobs simulation
on a 16,000 machine 3-tier ternary tree
with 40 machines per rack and and so on
this is actually using a mix of any jobs
but the sizes of jobs is actually drawn
from a distribution that octopus uses
okay then thank you so much for 4b it's
an almost like you know I just have a
bunch of evaluation results to show you
that like you know it actually works
really quickly I can show you right so
so performs much better than VC this is
the blue curve is a VC this is a
completion time and TI vc is actually
much better in terms of completion time
when you have batched arrivals of jobs
with increasing your subscription you
have even better because you are
actually utilizing the network better
with increasing mean job sighs this is a
means we used of mean jobs as of 50 but
as you increase the job Sonny's the gap
also increases quite a bit because bc
cannot fit more larger jobs because that
requires straddling rack boundaries
whereas the IVC can actually fit them
more efficiently so this is actually
showing how many rejected
requests exist and we can see that the
tiv see rejected rejection ratio is
actually lesser and as the load
increases beyond sixty percent there is
a significantly higher almost 2x
rejected requests from the in the VC
model as opposed to the TIV see model
and this is also saying higher
concurrency in terms of number of jobs
you can run but the larger jobs are
actually hi the extra seven percent is
actually for the larger jobs which means
you have larger utilization in terms of
number of VMs which leads to twenty
eight percent higher revenue using some
simple models I mean you have to do what
you have to do no I meant in terms of
like you know at least having a baseline
model because there's no such model that
exists that we can directly use we use
the same model that we see guys have
actually used so in that sense we
haven't actually innovated much and the
utilization of the band with a reserve
more but use less we reserve less and
use more that is the reason why you get
more efficiency out of your network and
this is different charging models the
provider can make more money as well as
the con the cost of individual jobs goes
down if you keep things this is a
comparison between tibc and VC and
finally I think I just want to quickly
go to this one last time we had a test
bed using net FPGA switches there is a
three-tier topology with with the three
nodes at the bottom connected to a door
switch and we have all these net FPGA
rate limiters configured appropriately
and we have a bunch of jobs and what we
found is that TI vc outperforms
twenty-seven percent less time is
proposed to vc and also the
predictability is really high both vc +
h dt IVC get reasonable performance but
baseline actually with no reservations
the jobs actually take much more time
that is a job time completion ratio is
actually much higher the shift to the
that's the thing that you lose right if
you have no no reservations in the
network essentially that's pretty much
what i have so any questions
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>