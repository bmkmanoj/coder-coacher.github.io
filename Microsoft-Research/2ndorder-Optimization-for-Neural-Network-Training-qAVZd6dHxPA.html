<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2nd-order Optimization for Neural Network Training | Coder Coacher - Coaching Coders</title><meta content="2nd-order Optimization for Neural Network Training - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2nd-order Optimization for Neural Network Training</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qAVZd6dHxPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright thanks for the introduction
alright so I'll be talking about second
order optimization for neural network
training so neural network optimization
is an important problem is I think most
people here know each layer of depth in
a neural network allows you to express a
larger class of functions or
distributions efficiently which
motivates their use and there's actually
many rigorous results that support this
idea such as these ones including to my
own papers you know of course also and
this is what matters most at the end of
the day it seems to help a lot in
practice on various data sets such as
image net unfortunately deep networks
are harder to optimize than shallow ones
now standard first order methods like
SGD are quite slow and training time is
days for deep nets or even weeks in some
cases versus minutes for shallow ones of
course people don't train gel and that's
these days but I do sometimes and it's
extremely fast so in this talk I'm going
to address the question can we actually
do better with second order optimization
can we get the training time for deep
nets down so just a little bit of
notation first for the rest of the talk
in this talk the parameters are theta
the network's output function that is
the function it computes that it's top
layer is f the loss will be L and Y is
the target Zed is is what the network is
is outputting so it's it's prediction
although it doesn't have to correspond
directly to the target it could be like
a primp the parameter of a distribution
that predicts that target and I'm gonna
assume in particular that it's the
negative log problem density R and
that's that's fairly mild assumption and
then the objective function is just the
expected loss over the training set
which is Q here so just briefly I'm
gonna go over a second-order
optimization in general so the idea of a
second-order optimization is that you
form a local quadratic model of your
objective function H the model will
donate in this talk by M and it takes
Delta which is the change in parameters
and it models the the value of the
objective function as a quadratic like
this here you have a curvature matrix
term this is the quadratic term and then
the standard first order term which
depends on the gradient and the constant
term typically if you want to
approximate the function locally in a
Taylor series sense you take the
curvature matrix to be H but there are
other choices H here being the Hessian
so the idea is that once you have this
quadratic model you optimize it to find
the optimal Delta and this gives the
well-known formula which involves the
inverse of the curvature matrix times
the Hessian and you apply this update
now the issues with this idea are
numerous but the two main ones are that
for high quality bees that is ones that
are not approximated in a very
particular way it's hard to actually
compute or store or and especially
invert this matrix because of course it
has n squared entries and can be a very
large number neural networks it can be
millions or tens of millions so we must
respect and the other the other problem
too is that it's this idea of just
proposing updates from a quadratic model
is sort of naive you have to account for
the fact this is actually an only an
approximation of the objective function
so you employ various methods to
restrict this minimization to a local
region where it's actually a good
approximation
so to address the first problem that is
this big curvature matrix there's
various ideas in the literature one very
popular line of work looks at diagonal
approximations this was think that the
first word for neural nets was done in
the late 80s by Alma Kuhn and
collaborators and there's various other
proposed methods now the experimental
evidence that I've seen actually
indicates that there's almost no
improvement in practice if you actually
tuned su-24 mentum very well compared to
these diagonal approaches so their
benefit seems to be mostly in terms of
making it easier to choose learning
rates but perhaps doesn't be provide any
sort of fundamental benefit beyond that
there's also low-rank approximation so
you can do to the curvature matrix
l-bfgs of course is the most well-known
example of this again there hasn't been
a good demonstration that these methods
can actually significantly help neural
net optimization another approach which
is different from these is a block
diagonal approximation in the in the
work of Nicola rudol in 2008 tonga and
this works by having a block diagonal
matrix where one block you have one
block for every unit and that
essentially corresponds to though that
units incoming weight parameters this is
actually not practical for large
networks
unfortunately because you have a lot of
these blocks you have one block for
every unit and the blocks well they're
easy enough to invert if you only have
to do it once or twice if you have to do
with thousands and thousands of time for
every time for every update it becomes
prohibitive in that work they even go
further and do low-rank approximation of
those blocks but then you're sort of
getting into low-rank approximation
again and it's still quite inefficient
so some work I did in 2010 looks at a
kind of truncated Newton or an exact
Newton or matrix free there's a lot of
different names for this kind of method
it's a it's an optimization method which
where we're gonna address this problem
not by making an approximation to the
curvature matrix but instead we're gonna
actually
approximately minimize the poetic model
so we're not gonna have to compute that
inverse because we're only gonna get the
an approximation to its optimum and this
is accomplished in this method as it is
with any of these kinds of methods
you've seen preconditioned linear
conjugate gradient with a particular
diagonal pre-conditioner this works
because CG linear Contra gradient only
needs matrix vector products with this
matrix B and you can get these quite
efficiently for various choices at B
such as the Hessian and the cost is
roughly that of competing a gradient
okay so more details about that in in
that work I'm actually using sure well
that work only looked at standard
networks because it's fairly old before
commnets came back but um it principle
could apply to almost any network I mean
there's yeah I mean they're they're just
- you're forming the blocks explicitly
so weight sharing patterns wouldn't I
think mess it up pretty sure about that
right although there could be you could
imagine funky architectures or maybe
this sort of idea would become undefined
to have you know associate a block with
it you know the incoming weights to a
unit
maybe units would have sort of one
incoming weight and then what do you do
I don't know anyway so in HF we're gonna
actually use instead of the Hessian
we're gonna use a different matrix the
issues with the Hessian are that it's
indefinite for non-convex objectives so
you have an unbounded local kinetic
model which could
be fine if you're doing constrained
optimization of that model but it leads
to other problems and that's a hard road
to go down and the other problem is that
it actually just doesn't seem to work
well in practice for neural mats and I
think this is an interesting research
question as to why exactly this is the
case instead we're gonna use this matrix
that was invented by Schroeder F in 2002
although it's probably finds its origins
earlier in the optimization literature
called the generalized gauss newton
matrix this is defined like this this is
s here is that is the set of training
examples and we're just going to take an
average over what is essentially the
Jacobian of the network's function f
times the Hessian of the loss times the
Jacobian of the network's function f and
this is called generalized aus Newton
because it's a it's a generalization of
the standard Gauss Newton
matrix it when you take the loss
function to be the squared error but it
works for arbitrary convex loss
functions it's always positive
semi-definite when L is convex
so that's good you don't have to worry
about this indefinite issue and yeah as
it seems to actually work very well in
practice for reasons that are not
entirely clear although I have sort of
various intuitive theories so that's so
so that's the matrix we're gonna use
there's this other issue in second
second optimization I mentioned before
where you have a local quadratic model
that you can't completely trust so
remember that computing this update
normally it's just like essentially
optimizing the quadratic model
completely and yeah that doesn't work
very well in many situations so we need
to constrain it and we're gonna use in
particular here a adaptive ticking off
scheme which essentially means that
we're adding a multiple V identity to B
and you adjust the strength of this
multiple according to various rules some
of these that are theoretically
justified and various old optimization
papers that so-called live and birthmark
card rule this is you can show is
actually equivalent in some sense to
choosing a trust region and optimizing
over that trust region although we never
actually compete the radius the trust
region because that becomes too
complicated and there's also this aspect
we're doing a partial optimization via
CG is actually itself a form of damping
or regularization that is you know get
the full inverse so in some sense it's
weaker but it's also maybe better
behaved and again you can there's some
analysis you can do there in particular
looking at C G's convergence and
analyzing how it respects different
eigenvalues you can show that it
actually favors big ones first and so if
those are the small ones that are sort
of correspond to L conditioning will
sort of get addressed last and maybe
those ones will not appear if you do
early truncation and CG this is this is
discussed in a paper I have so for
experimental results with this method HF
we consider the deep autoencoder
problems from the original deep learning
paper from 2006 these were at the time
believed to be impossible to train from
scratch without the pre training
technique proposed in that paper so we
show in fact that it does much better
than the baseline that they were using
which is essentially just pre training
plus nonlinear conjugate gradient so we
can actually optimize these nets fully
from scratch free training does help a
bit that's this line here but in fact we
are starting this line here but we can
do pretty well just just without that
optimizing from scratch it's also much
faster now probably the most interesting
aspect of this which doesn't show up in
this graph because this is just looking
at time is that in fact we're using
orders of magnitude fewer iterations
than a first order method in particular
the one they use but also other ones you
could be using they know we don't want
this this graph doesn't actually look at
full batch there's it's using larger
batches but there's a certain point
where like if you make the mini batches
larger for SGD you get almost no benefit
and that point is quickly reached in
these in these experiments by the
baseline but these experiments are
important to dwell on because this is
old stuff and the baselines have gotten
better since then
as as I'm about to discuss yep yeah so
each iteration involves running CG I'll
talk about sin a couple slides and that
of course negates a lot of the benefit
right if the if the iterations were just
as cheap as is a great descent you'd be
doing thousands of times faster but
they're not and one of the challenges
going forward is how do how to address
that issue right that's right yeah this
is just looking at time this is this
observation about iterations it's just
aside information but anyway so more
recently people have discovered that in
fact you can optimize deep inner or less
with with simpler methods like SGD and
in fact the historical reason why people
thought this was impossible was mostly
coming down to the fact that the random
initializations they were doing using
we're very bad as if you use good ones
in fact sqd starts to work now the
various issues that people or
observations people have made about deep
nets having issues like vanishing
gradient set cetera this is related to
issues of curvature in the objective
function that's that all remains true
but it turns out that if you have
momentum it seems that you're addressing
those adequately enough and then
sometimes you don't even need momentum
so these days SGD mentum is still the
most suitable approach for optimizing
neural nets with very large data sets
although HF is actually still useful for
certain problems in reinforcement
learning or buried deep temporal models
where it seems like second order
optimization even if it's done
relatively primitively in comparison to
new methods which I'll talk about later
it does seem to provide enough benefit
over SGD that it's worth still worth
using there
well there's there's so many different
flavors of it we usually mean yesterday
vimentin sometimes you could on the
other learning rate I don't like that I
think averaging is a lot better people
have their own people everybody has
their own sort of secret sauce version
of it though I wouldn't know I wouldn't
say that it solves them it gets around
them in kind of a backhanded way I mean
well it probably would be the case that
the benefit would be better for networks
that are sort of haven't been designed
to make SGD work better like that the
benefit you'd get would be larger but
the nice part is that you wouldn't have
to worry about doing those things but it
could also benefit those architectures -
I don't know I haven't done the
experiments so again probably the
take-home message from this this work is
that despite SGD momentum still being
the the most widely used and best method
in most situations it's HF does have
this property that it's using far fewer
iterations in SGD and so this helps
support the idea that a second order
method can help a lot in principle that
is you know if we could compute these
iterations more cheaply they really are
much more effective than a first order
iteration and so we can identify some
issues with HF method the main one of
course is that each update we're
computing using a run of CG and this
typically requires you know tens or
hundreds even of iterations
now CG is a first order method in sort
of the classic sense in the sense that
it's a linear produces a linear
combination of previous gradients to
compute its updates and so it's not
immune to certain pessimistic lower
bounds you have four such first order
methods as applied to certain
quadratics which you can construct now
the local quadratic models that you get
when you're optimizing neural nets do
appear to be pretty difficult in this
may be an even in this technical sense
not as hard as the absolute worst case
but probably not nice enough that CG is
getting a huge advantage from say
clustered eigenvalues or something in
particular these hard difficult
quadratics are characterized by sort of
a heavy-tailed distribution over
eigenvalues and see these nice
properties that you get from CG might
not actually kick in that's bad because
that means that CG might not actually be
that much faster than well tuned sgdq
mentum at optimizing these quadratics at
which point you have to say well why am
I doing this why don't I just apply SGD
to the original problem without the
quadratic approximations and that's
that's that's that's a valid observation
the other issue with HF is that you use
you compute estimates of the critics
you're using the current Mini badge and
this is an issue because if you spend a
lot of time computing this very nice
update with multiple iterations of CG in
some sense you're overfitting to that
small amount of data that you've used
for that to compute that Crippler
estimate and that's bad and that leads
to updates that are essentially do very
well on that data but don't do well on
the other data that you haven't used in
the curvature estimate and of course the
solution you might think well why don't
we just change the estimate of the
curvature for every iteration of CG
cycling through the data using different
data each time unfortunately that
doesn't work CG is not stable to that
kind of hack unfortunately and there's
not it's not obvious how to make it
stable it's probably not possible so
going forward we have this new challenge
can we compute high quality second order
updates as in HF
but without resorting to either a crude
approximation of B like a diagonal
approximation and without using a first
order method like CG to solve the local
quadratic subproblems and so that's the
work I'll talk about for the remaining
remainder of the talk the method is
called
factored approximate curvature this is
joint work with Roger gross at U of T so
in this method we're gonna have it's
gonna be a second order method where be
our curvature matrix is an approximation
of the Fisher information matrix
associated with the network this
approximation will not be low rank we're
not gonna make it sparse or diagonal and
we can actually compute it store it and
invert it most importantly very
efficiently using essentially unlimited
amounts of data
so unlike HF we can make the curvature
estimate depend on any amount of data
you want without affecting the
efficiency of the algorithm and this
approximation turns out to capture a lot
of the core structure of the true Fisher
it's not it's far from exact but it
seems to actually be very very good
considering how cheap it is this method
gives cheap iterations that whose cost
are about the same as SGD at least
computed on a medium-sized mini batch or
methods that use diagonal approximations
and we need to gain far fewer iterations
so that should be necessary for mentum
it's not as few as HF but it's certainly
making a big savings which if you
combined it with the fact that the
iterations are about the same cost you
get an algorithm which is actually
overall much faster in our experiments
even on a single machine so for the rest
of the talk I'm gonna have a little bit
of notation for neural networks which
will be important so here we have a
simple two layer neural network a for
each layer is gonna is going to note the
the values of the activities so you can
think a for activities and the subscript
just refers to the layer index zeroes
for the input which is also X here and
our on top of the a is just gonna mean
that we're going to append a coordinate
of 1 to that vector that's just a trick
so that you don't have to have a
separate bias parameter it makes the
math work up much more nicely so W is
this weight matrix but it also includes
what is traditionally thought of as the
bias vector as the last column
and you compute like going through the
standard matrix multiply times the
spectra you compute the inputs to the
next layer which will denote by s S will
be the inputs to that layer SSI that
goes through the nonlinearities which
for this talk won't be very important
and then you get activities outputs for
that layer and then so on and so forth
you multiply by the weight matrix etc
and the output is just the network's
function f at which is that the
activities at the top layer which is L
so one thing to remember is in fact this
if you can find it with the loss
function it defines a conditional
distribution over its output that'll
become important in a little bit but
essentially you can think about the top
layer not not predicting a target per se
but predicting the parameters of a
simple conditional distribution which
which will then give the distribution
over what the network thinks the target
should be so I mentioned the Fisher
information Fisher information matrix
before that's the matrix we're going to
be approximating so what is this how do
you I mean how do you Fisher information
matrix associated with distributions
well it's the it's the distribution
associated with in the neural network
that's what we mean and this you can
actually expand out this is given by the
expectation of the outer product of the
network's gradients essentially which
we'll just using this notation and here
script D is just going to be the
derivative of the loss with respect to
that variable V that'll be going forward
for the rest of the talk so so yeah it's
this it's this nice expectation of outer
products therefore it's also a PSD which
is a nice property in fact you can go a
little bit further than that with this
matrix you can you can say that if our
conditional distribution of the output
is a generalized linear model
with natural parameter Z in fact F is
equal to the generalized gauss-newton
matrix which we talked about before so
it's in fact it's the same matrix that's
used in HF it's just formulated in a
different way as a kind of a statistical
object and it could also of course be
viewed as the expected Hessian under the
models distribution on targets whereas
the Hessian of the training objective is
simply the expected Hessian of the loss
under the training distribution of the
targets so just that one one difference
in the word actually makes all the
difference here this is not the Hessian
but it's something which is close and
these in these two different senses but
it's but it's PSD critically so it's a
nice matrix to use it's also related to
the natural gradient which probably most
people here have heard of the natural
gradient is classically defined as just
the inverse Fisher times that has sorry
the gradient and natural gradient
descent is just doing gradient descent
with that vector in fact this is just
second order optimization right with the
choice of the curvature matrix being the
Fisher of course it's not typically
thought of in the work of omaree as a
second order method but really that's
what it is especially if you if you
think of the Fisher as a approximation
of the Hessian of course there are
another nice properties that don't
really come out of that interpretation
that are more due to the classic
interpretation this is work done and a
by omaree and they show for example that
if you look the learning rate go to zero
in this and this update you have these
nice properties for example you are
invariant to any smooth repair
memorization of the network that's a
pretty strong property right because
even in classic Newton's method you're
only invariant to affine transforms also
if you think about what this is actually
doing its following essentially a path
in distribution space the most direct
path where you measure distance in some
sense
locally by the kale diversions globally
it's not kale divergence but if you if
you go if you essentially define a
Romanian manifold where the local
definition of distance is approximated
by the KL divergence you get this space
over distributions and this is what your
you're optimizing essentially within
this space and taking the most direct
path in that space when you do now a
gradient descent so that's nice
now of course the issue with this theory
is that it doesn't hold if the learning
rate is is essentially a larger than
zero so for practical purposes it
doesn't really make sense but you can
actually show that these various
properties hold approximately if you
have a larger learning rate and this is
done an mtech report I have okay so
naturally in a sense nice it's the
connection to second order optimization
is nice but how do we actually use this
to compute an approximation of the
Hessian or the Fisher
so the first step we're gonna decompose
the Fisher into blocks this is just done
on a per layer basis as opposed to a per
unit basis so these are big blocks still
very big blocks actually and you could
not invert them explicitly even if
that's all you were doing is just
computing block diagonal matrix so we
break the matrix into blocks each block
corresponds to again a pair of layers
III and J and back here is just this
just means vectorization this is this
quantity is a matrix quantity because
it's the derivative of the weights I'm
sorry the drivet of the loss with
respect to the weights we vectorize it
as one does when you convert it into a
single parameter vector and you just get
this formula for each block this is
that's just trivial so that's the first
step
now before discussing the next step just
a little bit of extra notation I'm going
to define G I to be the derivative of
the loss the respect to the inputs to
layer I remember the inputs to layer I
was si this is this just means the
derivative the loss with respect to that
quantity so you can think she stands for
gradient here
because it's the gradient essentially of
the unit's associated the unit input and
the Kronecker product which will be very
central in this talk is defined like
this it's between C and D you
essentially take D you tile it over to
create a much bigger matrix multiplying
it each copy by the corresponding
element of the matrix C so you get a
much much bigger matrix when you do this
operation okay so given that notation
immediately what you can do is you can
write the derivative the loss with
respect to the weights at layer I in
this form normally this is an odor
product between the back propagated
derivatives G and the activities from
the layer below at I minus 1 but you can
write this just as a Kronecker product
between two vectors tribulus
so this lets us essentially go expand
the formula for each block with the
Fisher like this you just this is just
definitional here essentially and like
but the transpose inside use this
identity here to actually read it like
this now it's the expectation of the
Kronecker product of two matrices okay
well that could be useful I mean
chroniker products are nice but this is
not a Kronecker product right it's it's
a expectation of a Kronecker product and
sums of chronica products quickly become
structureless if you sum over to many
things so that's not immediately helpful
and this is essentially the main idea
the talk is that well let's just pretend
that this is what we have let's actually
approximate this expectation as the
Kronecker product of the two
expectations and that is a nice form we
can work with this
so if you write this now if you write
these two matrices as a capital A and
capital G with appropriate indices then
now we have this block corresponding to
a layer I and layer J
approximated as a Kronecker product okay
and then this is a game nice because you
can now invert these blogs because of
various well-known properties of
Kronecker products we'll discuss later
so just looking at this initial
approximation it turns out to be pretty
decent here this is a visualization for
the Fischer matrix for a medium sized
neural net that's been partially trained
on amnesty and you can in fact see that
the exact Fischer which is here has a
lot of the same structure as our
approximation here the pattern of the of
the of the entries is fairly similar the
difference which is plotted over here
certainly does exist and certainly
important but it seems to correspond to
sort of a finer grain structure might
not be as important in practice okay so
that and certainly this looks a lot lot
better than any kind of diagonal
approximation you could do or any
low-rank approximation you could do
these ended up being I mean had to be
small enough to compute explicitly so it
was several hundred oh yeah oh the size
of the difference is the size of the
differences are fairly substantial so
it's a it's about 30% of the total size
so it's not insignificant but of course
being wrong at that level is not
necessarily important like you could be
you could dish to have a scaled copy
they have seen that would also be 30%
different but that would be a great
matrix if you can have it yep
so I'll talk about the computational
costs associated with using this later
but it just is it to go to skip forward
it multiplying by this matrix is about
the same cost as computing the gradient
on a medium-sized mini badge by
medium-sized I mean the size the mini
batch is roughly this the same as the
width of the layers sure you know it's
not linear with the size of layers it's
unless you because the gradient is is
already linear so it's a quadratic yeah
yeah yeah the exactly be fourth power
yeah okay well that I guess that's more
of a claim that's not necessarily
supported by looking at this picture but
so there used to I used to have a
version of this slide where it looked
much much better that's because there
was one low rank component which is very
strong and we were approximating that
one very well so in fact there was the
air was like 5% you can change the
picture essentially by changing the
network architecture such that it
doesn't have this big low rank component
and so that and then you know what's
left over is something that's more
interesting and harder to approximate I
suppose I would have to it would be good
maybe to have like an actual picture
computing the best low-rank
approximation you know as done by SVD or
something
yeah that would be that would actually
be a good good thing to add I think but
it it it's it's sort of a claim that I
can make based on the performance of
methods that try to do this try to
actually approximate this matrix even
low rank methods which so far has been
pretty visible so but yeah maybe there
could be something I think that's worth
doing actually but I think the matrix
and the matrix is far from low rank of
course it might you could argue all it's
not lowering just because of this
diagonal term but I think it's more
fundamentally far from a low rank matrix
for other reasons
you can well you can look at the eigen
spectrum for example and we have looked
at the eigen spectrum and to get to be
well approximated by lowering matrix you
want some large eigenvalues and some
much smaller ones if it's heavy tailed
you don't you're not you're never gonna
hope to succeed at approximated
approximating it well with the low rank
matrix and it does appear to be heavy
tailed yeah which is the same reason
it's hard for a cg to optimize
essentially if you have a low rank
curvature matrix you probably need any
of this stuff anyway I sure yeah there
could be there could be some differences
there I it seems that the Fischer
preserves enough of the interesting non
well rank structure but yeah it the
points well-taken okay so one thing i
sort of i talked about that
approximation before didn't really
justify it we can't justify this a
little bit so in so consider for example
two-way trip moves in the network just
to arbitrary corresponding to arbitrary
layers arbitrary units doesn't matter
they're given by the activity from the
layer below times the back propagated
gradient from the layer above in both
cases these are just two arbitrary pairs
of those of those quantities in the
network and what we're really doing is
we're approximating that if this
expectation as factoring like this we're
doing this per entry essentially that
corresponds to the Kronecker product
approximation so this is really just
like assuming that we have independence
between products of pairs and activities
versus back propagated rivets which okay
that's kind of trivial what's a little
bit less trivial is that in fact you can
write out the approximation error like
this and this actually this expression
actually uses properties of the network
it's not this doesn't hold for arbitrary
variables and this expression depends on
these cumulant terms which all don't i
by kappa you have three three terms a
fourth order term a two-third order
terms
and so if you write there like this it
becomes apparent that really the air is
all in these higher-order cumulants now
the intuition with cumulants in some
sense their higher-order generalizations
of mean invariants and they capture what
physicists they like to think of as
interactions which are intrinsically
higher-order as opposed to other
interactions which you could write as
sums of lower order interactions this is
what they talk about in that literature
so these are terms for example that you
could almost never hope to capture
exactly without a say a third order
tensor or a fourth order tensor which is
out of the question for neural nets so I
like the way I like to think about this
result is that we're what we're doing is
we're capturing this sort of all the
statistics that you can capture feasibly
about the Fisher for neural networks I
mean maybe you could somehow approximate
these terms using another technique but
you never you never get them exactly
where as we get everything below third
order cumulants we get it exactly in our
approximation but yeah maybe you could
approximate this with a diagonal term or
something in fact in some sense our
method implicitly does that but I'll get
into that later and so yeah the other
thing to note about cumulants is in fact
there's zero for multivariate Gaussian
so if these variables are magically
distributed according to multiplication
it would be in fact our approximation
would be exact so I've talked about this
Kronecker product approximation that
we've done on the matrix that's fine but
it doesn't actually necessarily yield an
algorithm for computing the inverse
efficiently so we still have to do a bit
more work and now we'll talk about the
second stage of approximation which is
where we look for structure in the
inverse of the Fisher and more generally
structure in the inverse of covariance
matrices because of course the Fisher is
just the covariance of the gradients
from the models distribution so in this
general setup again I'm gonna I'm gonna
consider just a multivariate
distribution over these fees whose
covariance matrix as Sigma
and for our example again V is just that
the entries of the parameter derivative
vector and the covariance is just a
Fisher so if we define a matrix C where
each row of matrix C is going to be the
vector of weights and the optimal linear
predictor of VI from all the other V J's
then in fact you can show that the
inverse covariance is given by this
expression here which is a diagonal
matrix or the inverse of a diagonal
matrix times this term identity minus C
and so there's a relationship between
the structure of C and the structure of
the B inverse covariance and what does
this give us well gives us the basic
intuition that the size of the of an
entry of the inverse covariance
corresponds roughly to how useful a
variable V J is for predicting a
variable V I given all the others so
useful in a sort of an optimal linear
predictor since this is a little bit of
a subtle concept it's important to be
clear about it this is not equivalent
for example the degree of correlation
between VI and EJ so if say you had that
VJ was a noisy version of some VK to
some other variable then in fact it
would not be useful for addicting VI
given the others because others includes
this VK even if they're very highly
correlated why is that well because if
you any optimal linear predictor you'd
always want to shift more rate weight
onto VK because it's a strictly better
version of VJ with less noise so it's
better for prediction up so so this is a
subtle concept in particular
correlations can be explained away
essentially by other variables now
intuitively if we think about the
usefulness of entries of the gradient
for predicting other entries of the
gradient as if you think about them as
random variables well it's probably the
case that entries in the same layer are
most useful and entries in adjacent
layers
are sort of at the next level of
usefulness in terms of predictive power
so on that reasoning our approximate
inverse Fisher should be well
approximated as either block diagonal or
block tridiagonal this is an
approximation or gain of the inverse not
the matrix itself and so does this hold
up in practice what it seems to here we
have our approximate Fisher this is the
first stage of approximation has been
applied the Kronecker product
approximation this is this the the
picture from before essentially and
here's its inverse and you can see there
is a clear block dominant structure in
this matrix again white it corresponds
to big in these in these pictures so
that that seems to actually hold up
quite well so for the block diagonal
case if we want to compute the inverse
well that's pretty trivial right you
just invert each block and we can do
this so you get this and we do this
block inverse efficiently because each
of these blocks for Kronecker products
and you have this useful identity that
says the inverse of a chronica product
is just the Kronecker product between
two inverses and because we can invert
these two matrices efficiently
relatively efficiently we can get the
whole inverse efficiently which is much
much much more efficient than say doing
this explicitly for a matrix that that's
this big now we don't actually want to
compute the inverse right we actually
want to compute the product of the
inverse and an arbitrary vector like the
gradient but this is straightforward to
do too because we have this identity for
carnotaur products which is in some
sense definitional and this allows us to
compute this this matrix vector product
with fee using this formula essentially
pre and post multiplying capital v which
is sort of a vectorized version of V at
layer I by the corresponding matrices
from the from the whose that correspond
to the factors in a contra product for
that layer so this mapping between V and
these
alvey's is just the same as the mapping
between the parameters and the weight
matrices for different layers and
critical observation is that this is
actually no more expensive than
computing the gradient over a medium
size mini batch that is a mini batch
whose size is roughly the size of the
the layer width the number of units this
is because it's just matrix matrix
multiply this is the operation you do
when you can meet the gradient over here
so that's good now the block tridiagonal
inverse is a bit more complicated I
won't discuss it in detail I'll just
outline the basic idea so we're gonna
view the variables sorry the gradient
has been generated from a undirected
Gaussian graphical model whose
covariance agrees with our approximation
on its tri diagonal blocks and you can
convert this to a directed model using
standard techniques which involve
computing some matrix multiplies
essentially some inverses and this
formula you get respects enough of the
structure of chroniker products that in
fact the only operation that you really
have to do that's expensive at the end
of the day is computing inverses of sums
of two Kronecker products I said before
that expectations of Kronecker products
are not good in general this is sort of
an expectation to sum of two actually
you can invert this efficiently as well
that's sort of a miracle of mathematics
that you can do that for three it
appears to be impossible so it just
works out nicely and so to look at this
the quality of the approximation in this
second stage well on the top here we
have the block diagonal version again
we're computing to assume a matrix is
inverses block diagonals to assume the
matrix itself is block diagonal so we
just get this and we're quite good on
the block diagonal blocks but of course
we don't get any of the other structure
what's interesting is that for the block
tridiagonal inverse
in fact we recover the full matrix right
was we're assuming the inverse has this
structure so the air even on the off tri
diagonal box it's actually quite low
of course what we're really interested
at the end of the day is not the matrix
but its inverse and so it's more
interesting perhaps to ask how well are
we approximating the inverse again we're
doing pretty well and the block
tridiagonal version here seems to be
quite quite a bit better in terms of
approximation quality but both are quite
good actually
so now I'll discuss some details of the
implementation of this method first we
have these factor matrices remember
these are just expectations of outer
products of quantities from from the
network activities in this case and back
propagated derivatives in this case and
and we have to maintain estimates of
these and we do this well using
essentially just a stochastic estimation
procedure it's pretty heuristic so for
the G's of course we have to get
gradients from the models distribution
not from the training distribution so we
do a bit of extra back prop there and
then we use exponentially decayed
averages essentially to average these
quantities as we get them so as we're
computing updates with mini-batches
we're just cumulating better and better
estimates of this stuff decaying it
exponentially of course because as time
goes on it becomes stale as you get as
your parameters change so this is just
these are just standard tricks to
maintaining this kind of stuff and the
other important thing to talk about is
how we handle the the update damping or
the regularization that deals with the
issues of trust in the quadratic model
remember taking it up there like this is
just like optimizing this crit model
completely so the game you need to apply
some subset of these kinds of methods
taking off damping regularization as
it's called trust regions truncated cg
optimization which we can't apply here
so the approach that we settled on
is a-tickin off approach where we add
where we add game on times they didn't
need to ask before inverting it which is
equivalent to adding this to the
diagonal block of course there's a
slight issue with this in that you this
actually destroys the Kronecker product
structure if you if you add the identity
to a Kronecker product you don't have a
chronicle product at the end of the day
one solution is to add square root gamma
times the identity to each factor and
this you have this that you then invert
and so you're doing this essentially and
you can do this this is not the same
thing as inverting a cron b + gamma
times the identity but it's sort of it's
sort of a it's a crude enough
approximation that works actually quite
well but and you can actually justify it
and make it a little bit better and this
is done in the paper there's actually a
way to do it
more exactly for the block diagonal case
I won't get into that it's a bit more
complicated but it can be done right
yeah you're right it doesn't have to and
in practice this the solution of adding
to each factor seems fine yeah of course
it's harder to understand maybe what's
going on here every every stage of the
competition we do here it preserves it's
just as efficient as computing gradients
for mini batches that are the size of
the layers even the matrix inverses here
right that's that's still just a cubic
operation no no but it is Schleyer you
you I mean when you do back prop right
you're computing matrix matrix products
that's a cubic operation in the
dimension of those matrices and that's
that's that's all we ever do here yeah
although we actually save a lot of
computation in various clever ways along
the way to reduce the constant
but but asymptotically it's no different
right and so in addition to this ticking
off stuff well we have to do a little
bit more we found it's not quite good
enough in particular gamma has to be
quite high and this actually seems to be
it seems to be that it needs to be high
in order to compensate for the
approximation we're doing not just from
the approximating the objective is
quadratic but approximating our
quadratic with a different quadratic
which is somehow impoverished by our
approximation of the Fisher so we do a
little bit of extra stuff in particular
we actually what we do is we we view
this this update as a sort of a proposal
update which is just again that the
approximate inverse Fisher times the
gradient this is gonna be a proposal
update into a sort of a meta second
order scheme that actually uses the
exact Fisher as computed within an exact
quadratic model essentially M so what we
do is we just feed this update proposal
into M as computed using the exact
Fisher and we want to optimize alpha
which will just be a learning rate this
essentially just allows us to scale this
update down to help compensate a bit
more for how this is an approximation of
F because normally you can just take the
learning rate to be 1 in a second-order
method and it's fine this we don't we
don't have that property unfortunately
but we there is this nice method that
you which you can actually compute what
is in some sense an optimal learning
rate without tuning it this is a one
dimensional minimization so you can do
it essentially with only one matrix
vector product with the exact F and we
can get that that's just that's just old
HF stuff you'd have to estimate using
the current mini-batch but it's such a
it's such a minimal amount of
computation that it's sort of it doesn't
matter that much you can essentially
we're all you to competing with it as a
scalar so it it can be a bit biased and
sorry going back there's also an extra
damping term here for the exact Fisher
which is separate from the one that
we're using here for the approximate one
this one we adjust using standard
makes for second-order methods and this
works quite well this is the same method
that's used in HF basically it makes it
bigger if the qur'anic optimism the
quadratic models are not fitting well to
the objective it makes it smaller if
they're doing good soda just Sencha Li
how much it trusts the quadratic
approximation based on empirical
observations and there's also justified
in theory and it works well in practice
there's also kind of momentum that we
have in the method which is a bit
different from standard momentum when we
update when we propose a update to take
Delta it's actually going to be this raw
update proposal which is the are
approximate natural gradient essentially
F inverse times the gradient multiplied
by that learning rate but also we're
gonna combine that with a multiple of
the previous update that is the update
computed at the previous iteration of
this whole technique and we optimize a
quadratic model over jointly over alpha
and beta the intuition here in fact is
that over time we're building up a
better and better solution to the
minimization problem corresponding to
the exact kinetic model that is without
our Fisher approximation and you can
view this in some sense like HF it is a
kind of similar type of approach an HF
used a hot start of a CG iteration as
well but we're only doing one CG step
essentially just computing one only
computing learning rate and a beta once
we're not maintaining any other sort of
quantities and this actually makes it
similar to matrix momentum but at the
end of the day forged radix effectively
this is just a type of momentum that's
optimal it's like C it's like basically
like doing CG so I'll talk a little bit
about some theory in particular I'll try
to extend the theory that I talked
before for the net about for the natural
gradient method I remember that in that
work it's shown that if you follow a
natural gradient methods follow the
smooth path in distribution space
and this path is invariant to smooth
realizations of the model which is nice
but of course it only works for for
infant testimony small learning rates so
we can't apply that because we're taking
bigger steps and we're using an
approximation the Fisher we're not using
the exact Fisher anymore but you can
show this is done in tech report I have
that in fact again this stuff holds
approximately for large learning rates
and it can be extended to other kinds of
matrices other than the exact Fisher
provided they have certain basic
properties even verify so - so to talk
about the kinds of invariance is that
you can prove for this method we're not
gonna be able to prove arbitrary smooth
reaper amortization invariants but we
can prove something and so to describe
this class of transformations I'm going
to talk about transformed networks that
is a network where you essentially
reaper ammeter eyes it so this doesn't
affect what it computes but it does
affect how you the definition of the
parameter within the network and we'll
just what we'll do here is we'll just
pre and post multiply each W by these
two matrices which will put Sumar
invertible so you if you fold this into
the network it's like you should have do
this matrix multiply here by this
invertible matrix before you multiply by
the white matrix and then you also post
multiply again by another invertible
matrix if you choose if you transform w
when you add these matrices in such a
way you can actually just preserve what
the network is doing so it's it's it's
it's just a reparent ization
and what's nice is that we are in fact
invariant to these kinds of repair
relations that you could do so
particular this means that we're
invariant to the choice of logistic
versus tan aged units because those
those networks actually are related by
such transformation I just described
we're also invariant to any kind of
linear linear pre-processing of the data
that you could do like whitening the
method doesn't care about that
and this also implies a certain
interpretation of what's going on in
particular you can some sense say that
what we're doing is we're taking a
gradient step in a network where we've
Reaper a motorized it so that the
implicit activities and the implicit
back propagated derivatives and by that
I mean the quantities that appear here
and here so to the weight matrix they
appear like the standard activities and
back propagated derivatives but not to
the units so those implicit quantities
are actually now centered and whitened
with respect to the models distribution
so from the perspective of the optimizer
it's getting in activities and back
priority derivatives that are in some
sense at the head that have been pre
processed in this way but done
dynamically at every step at least from
the perspective of the optimizer as
gradient descent okay so now I'll give
in to some experiments the most the main
part of the experiments your our game
will be the deep autoencoders from the
old deep learning paper this is the same
problems I discussed before again these
are well-known hard optimization
benchmarks and they still hold up as as
sort of as benchmarks for what is
considered a hard optimization problem
they're easier sorry they're harder for
example to train and confidence are
usually and for the baseline we're gonna
use a well-tuned version of SGD for
Menten with the emphasis on the word
well-tuned and in particular this uses a
special schedule for the momentum decay
constant this is motivated out of the
work of unaccelerated methods nestor ups
various papers and we also use a type of
inert averaging so this allows us to use
actually a constant learning rate and
still achieve optimal asymptotic
performance and then in this this set up
actually just works very very well in
practice it beats pretty much any
diagonal method we've ever compared
against so this is actually a very
strong baseline and implementation i've
just done in a matlab
with a GPU package it's pretty
rudimentary so for the first set of
results I'm gonna look at training cases
processed versus error to get some
intuition about how this method scales
in terms of mini batch size and what's
interesting is that for that for the
baseline what you can see is that as you
increase the mini batch size going from
two thousand four thousand to six
thousand the performance is actually
getting worse it's actually taking
longer and longer to optimize so you
you're throwing and this is and this is
in term at least in terms of the amount
of training cases you've seen that's bad
right because that tells you that you're
not getting a good return on your data
investment you're throwing more and more
data into each update but you're not
you're not getting a much better quality
update out of that k faq on the other
hand seems to have a much more favorable
relationship between mini batch size and
progress per iteration which gives an
almost constant relationship between
training cases process versus reduction
across different mini batch sizes
exactly and I'll emphasize that point a
little bit more later I mean hopefully
this I mean we haven't tested it and
then and that's setting yet but
hopefully this will translate into much
faster distributed optimization so
there's certain there's a certain point
at which this this can't possibly hold
anymore right like if you make it so big
such the data becomes like completely
redundant right well it'll hat it'll
depend on met various factors it'll
depend on where you are in optimization
how late you are because you actually
you can see that even in the early
stages we're not we have that like you
if you zoomed it on this part right
we're actually we have about to sit I
mean it's not as bad as su D but but but
the bigger mini batches are doing worse
here but that the trend actually
reverses later in the middle it's about
even so the stage of optimization is
important also the various qualitative
aspects of the data set are very
important to how redundant is the data
that'll tell you a lot about how I mean
how how far you can expect this property
to go right if it's very redundant
adding more data is not gonna help
you'll get diminishing returns for this
data set anyway the data you fit it
closely enough that it becomes non
redundant after a while essentially as
you optimize the more your method cares
about finer and finer distinctions
between the data cases right why is this
happening well I think I think the
reason this is happening is because it's
because really second-order methods are
actually really really good but noise
screws them up badly so what you're
seeing when you have this is in some
sense the potential the second-order
method being more fully realized these
are already big mini batches right so
it's gradient descent a sort of like
yeah you get more iterations but each
iteration is sort of like the trade-off
of SU D is always like all the
iterations are really bad and instead of
poor and badly scale but but you have
lots of them and they and also you don't
need a lot of data it really don't get a
lot of mileage out in data and because
it's sort of like that the performance
of your system is always gonna be
dominated and sometimes by the weakest
component right a second-order method
the update quality is very strong but if
you give it very noisy data it won't be
any better than a first order method
because it's sort of garbage in garbage
out kind of thing right it it doesn't it
doesn't matter to be exact about
curvature if you're
right yeah I mean you essentially second
I mean you can you can actually analyze
these kinds and methods a supply to
quadratics and you can show that in fact
there's a certain decomposition between
a deterministic term and a stochastic
term and and if the stochastic term is
made small enough the deterministic term
dominates and then the performance of
your method is essentially proportional
to how well it works in the
deterministic setting and they are
second order methods clobber first order
methods so it's part is part of what to
discuss that more a bit later I guess
yeah so oh these networks are between 13
and like 8 layers or something this a
missed one that goes like 10 or
something like that
it's these optimization problem
confidence because in some sense you
have this constrained code layer that
makes it things a lot trickier
all right so zooming in again on this
late the late so this is zoom in in this
area here on this late stage we actually
see that in fact there's it's
advantageous to use larger mini-batches
if you go far enough into the
optimization in terms of amount of in
terms of efficiency of training cases
process versus error reduction and so
this motivates a technique which we
actually use in the rest of the
experiments where we're gonna actually
increase the mini batch size as we
optimize
oh sure I'll get I'll get to that this
is just sort of leading up to up to that
because the actual experiments we do on
time
involves this other stuff too okay great
uh I should be done before then and so
yeah so we'll increase the mini batch
size as we go
there are various heuristic adaptation
schemes that have been proposed recently
we'll use a different strategy although
this is probably worth exploring in
future work where we're just going to
increase em according to a certain fixed
exponential schedule so that and this
formula essentially all this means is
that we're gonna max out at the training
set size we recording to an exponential
function which starts out at the value
m1 at iteration one this will be our
initial mini batch size so the curve
ends up looking sort of like that and
then it just stops and so by iteration
and in our experiments by iteration 500
perhaps you're doing full batch training
and of course if you were applying this
to a very very large data set you might
not do that right you might stop it at
some other large but reasonable value so
this is the computation time on a single
machine using this implementation in
MATLAB and it appears to be doing quite
well in terms of time versus error as
you can see
yeah we're Optima this is this in some
sense this graph if you extend it
arbitrarily the advantage becomes bigger
and bigger if you stop it at a
reasonable point or about I don't know
10 times faster yeah yeah it's a very
good baseline like in our experiments
this baseline beat pretty much
everything else we tried and so so this
is this is this is a this is a hard
result to get actually I've been
optimizing these these particular
auto-encoders for years and years is a
bokken a test and it this this result
was actually very hard one like to get
it
good our water method has no free
parameters its total totally self tuning
the there are a couple parameters you
could say are free but in practice we
never tuned yeah going further the size
of the mini batch that we tuned yes
although although although actually I
really just tried like you know I tried
to maxing out at 500 tried maxing out at
a thousand that was pretty much so I to
two points essentially and we use the
same thing for every data set even
though actually it's probably better to
tune them per on it per dataset basis
that would be something which you could
probably get rid of if you explored say
for example the methods that do it
automatically well there are some of
those some of those details yeah there
are I mean there were some free
parameters but in fact most of that was
actually you could actually tune pretty
automatically but it was never it was
never a really great implementation out
there and here
well the all the damping stuff is done
again that's all done automatically
the only thing after tuned there is the
initial value of the thing of the
quantities that get adjusted and yeah
that would require tuning but it's a
sort of tuning you do on the basis of
the performance of the method and like
the first iteration right you don't have
to wait you're after like run a whole
thing to find out if it's good yeah by
the by the end yeah this is the full
sixty thousand and by the end of
optimization we're doing batch over the
full sixty thousand listen what's really
amazing is that you went you get data
efficiency even in the batch setting at
the end of this optimization hopefully
yeah why in some sense it's I mean in a
raw efficiency standpoint it's better
than HF in terms of computation time
right oh yeah yeah yeah yeah right or
even slightly worse on a single machine
it could be it could be slightly worse
it the advantages of a method like HF
only really become apparent if you can
use big big mini batches but then it
would also still be much worse than this
new method so yeah yeah no I mean this
is HF I think is only sort of more
useful in a raw efficiency standpoint
for a certain set of problems which
these days does not include
auto-encoders so so this baseline is
pretty much the best I think that you
can do compare against this is just a
different auto encoder on a bigger data
set what is I guess interesting and
these past graphs is that in fact the
Green Line is our method without this
momentum and so in fact it really is
important that you have that too
you might say well that's I mean how
much work is the momentum doing right
well a lot but if we if we didn't
include momentum in the baseline the
line would be like up here it would be
terrible these problems really really
really want to use momentum
eventually I think they probably cross
they probably cross - the momentum is
very important it's hard to it's it
start to under emphasize that all right
so hard to overemphasize that so perhaps
what's most interesting in the in these
experiments again is the iteration
versus error you can see that using what
are essentially values that work fairly
optimally for the respective algorithms
the amount of iterations that were we're
taking is just tiny in comparison to a
first-order method and that again makes
it ideally suited to distributed
implementation because iterations
correspond to synchronization steps
yeah
yeah well if we use two M equals 500 for
K FAC it would be it would be not much
better than the baseline in terms of
iteration efficiency it would be son it
would be more efficient but the
advantages would be eroded substantially
it is important to use bigger mini
batches in our approach it wouldn't be
much better than this graph and the only
reason I should have done that for these
plots the reason I didn't was because it
was like insanely slow like full batch
gradient
I got it like it would have taken months
but yeah that's that's a slight
oversight in fact in our follow-up paper
we do that experiment that kind of
experiment better maybe
in fact so this is I'm just gonna talk
briefly about some new results for
convolutional nets I've described an
algorithm it only works for standard
networks that is we assumed structure
that really only exists in a standard
before Network but in fact you can apply
these kinds of ideas for commnets and
we've done this so I'll just just just
look at the graphs from that paper this
is this basically the same baseline and
this is this this is our method and on a
single machine again we're doing we have
we have a stanchion improvement it's
probably not as good on a single machine
for these auto-encoders are sorry for
their combi nets versus the auto encoder
is a game because autumn quitters are
actually easier to Train combi nets or
easy to train but it's still it's still
it's still quite a meaningful
acceleration this is a different data
set this this uses drop out so
overfitting is not much of an issue here
again we're doing it substantial
acceleration to two to three times
faster maybe more again though I don't
think that's sort of the the more
interesting thing to look at I think
what's most interesting is again a look
at iterations vs. progress and here
we're doing much much much faster than
the baseline and this actually this
answers your objection in fact we're
using the same mini batch size for both
and this in this plot it's something
like several thousand
that's that's just cake that's just the
name of the method for confidence yeah
it's just the same thing basically it's
this includes this is slightly different
like some aspects that have been tweaked
a bit some of the details have been
tweaked but it's basically the same
thing mostly we didn't do internet
because neither Roger or I are like big
engineers and it was it became apparent
that that would be a set of a mammoth
task if you didn't use one of these
libraries and we couldn't use the
libraries because we're sort of really
digging into the guts of optimization we
need quantities that are not even
returned by automatic different shaders
I don't know it was an early design
choice that maybe it wasn't the best but
I it constrains us in terms of what we
could do so we picked the smaller data
sets but I agree it that's it's that
would be like the real killer
application is to look at imagenet so
directions for future work with k fact i
think are numerous things you can do we
can handle stochastic city in a more
principled way right now we just
increase the mini batch size there's
probably better way to do that extent
other kinds of neural nets for
confidence we've done this but there's
our own ends and various other kinds of
nets where you can't naively apply the
same ideas you have to do a bit of extra
work it's yeah it I mean there's a naive
thing you can do there that kind of
works maybe the main issue is the weight
sharing the fact that you have multiple
sort of input like you essentially have
multiple layers in the network ie time
steps that each contribute to the same
set of the gradients for this for the
same parameters and the problem is if
you applied the approximation the
cartographer approximation to each one
of those contributions give us some of
chroniker products which isn't actually
not invertible easily yeah yeah yeah
exactly so this is the weight sharing
that complicates it in both examples and
anyway so a better implementation of
course would be great one that is not as
crappy as my MATLAB one there's all
sorts of opportunities for parallels and
we didn't
here in fact all of us could be made
much more efficient like we just do the
invert matrix inverses and all these
things sequentially with everything else
in practice you could you could
obviously just have this running on a
separate machine - or even just a
different thread there are other ways
maybe maybe yeah although there's I do
have a solution for our Nan's it's sort
of preliminary but it's yeah in any case
and so and finally perhaps the biggest
direction is to now compute high quality
gradient estimates using massively
distributed computation and this should
I I think lead to big gains in terms of
overall performance versus async SGD at
least I hope so
and that's it
it seems like it is it is very different
so I think well certainly there I mean
the interaction between optimization and
and a model fitting is is there and it
complicates a lot of things like for
example we you know you can you can
tweak the parameters of this of this
optimizer and it'll get slightly worse
generalization errors you tweak tweak
them again and get slightly better
generalization errors then SGD and
they're certainly
right right I mean it's probably true
that as the networks get deeper which
also makes them harder to optimize this
method should help more that's probably
true well the test performance is again
it's a funny thing though right like I
mean if you preserve the number of
parameters and make the light the
network sort of narrower and deeper
isn't necessarily a good thing right
like it there's a there's a very I mean
there's a you could study expressivity
of networks I have actually and it's
complicated the relationship is
complicated test set air performance
like I personally try to sort of stay
away from when I'm looking at
optimization just because it's it it's
yeah exactly exactly exactly but there
is a certainly an interaction like if
you you have a stronger optimizer and
you say take it apply it to some model
and you're like oh well you had
optimized faster but the training that
the test there's actually a bit worse
this sucks right you have to sort of
step back and say wait a minute okay I
used a struggler optimizer and it fit
more and I it didn't generalize quite as
well that maybe that's not a surprise
maybe if you use a stronger optimizer
you have to also increase the
regularization a bit or do something and
that certainly seems to be what happens
in practice
right we didn't do much of that simply
because sort of at the beginning we
wanted to sort of compare to existing
baselines that had existing
regularization schemes so that we
wouldn't be accused of sort of doing
that kind of adjustment of the problem
to make our method look better maybe
although I mean if you want I think if
you want to get noise into your
optimization process to get kind of a
regular izing effect there's probably
better ways in fact that that's some
future work I want to explore well if
it's random neural random neural
networks tend to produce very
uninteresting distributions and
arbitrary neural networks can produce
like cryptographic hash functions which
are impossible to learn provably so we
I've never done that kind of that kind
of recovery experiment I think it's
because it's very hard to come up with
an interesting distribution over the
networks weights that is both random but
also produces structure that's learn
about and not and also not trivial I
think that I mean I don't think anybody
has a good actually understanding of
sort of what the distribution is of
trained networks what that looks like in
terms of natural datasets I don't know
yeah in fact I had a slide about
efficiency I think it got eaten or
something
I didn't skip it did I I and yeah I must
I must have forgot to add it so the it
is certainly like so there is a there is
a case where if you have a very wide
layer one very wide layer and all the
other layers are pretty narrow this
method can use a lot more resources both
in terms of space and time
that's because you're gonna be dealing
with this one layer who's say with this
am it's it's gonna be M by M right
whereas the parameters are actually only
M times something much smaller right and
so there could be a difference there
substantial difference but as long as
that's not the case as long as all your
layers are you know are roughly
comparable it it's basically it's all
just a constant multiple it ends it ends
up being like five to ten I I think it
could be yes sometimes oh yeah for sure
exactly a some some networks get to the
point where even having like two to
three copies of the parameters is
already prohibitive and and yeah and our
method would probably not I mean there's
probably a lot of tweaks I mean that
five to ten is based on a really crappy
initialization really crappy
implementation there are certainly ways
to optimize it you probably would
probably always be bigger than SGD
though you public you you are paying a
price there so there's a certain upper
limit of networks that maybe our method
is inaccessible to just based on that
for that reason alone well I think
papers been on online since March of
last year but it's difficult I guess it
is I posted it in February the only
person who's been suited I've heard he's
been using it it's been Ilya we worked
together for a while he knows like these
kinds of methods pretty well too
hi he say he said he said he was trying
very some supervised learning problems
and it seemed to be helping their
substantially but I don't know the
details</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>