<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intersection Workshop - Are we ready for autonomous driving? | Coder Coacher - Coaching Coders</title><meta content="Intersection Workshop - Are we ready for autonomous driving? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intersection Workshop - Are we ready for autonomous driving?</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ur76F2wNN8w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright so so yeah I sorry I started by
putting actually a controversial title
so that you know we can generate
discussion and the first thing I want a
survey sayin is what do I mean by week
which i think is you know the first
question to answer and what they mean by
we here is computer vision okay so
robotics you are safe i'm not going to
say anything bad about you alright so so
they vie for so it was a you know asking
for an application so here's what i
think is a very nice application for
computer vision algorithms okay so i'm
interested in autonomous driving and one
of the reasons is because actually I
don't drive okay and I live in Chicago
so this is actually fairly bad alright
so in in terms of we will see about that
okay so so what what can be done in
autonomous driving so you know so you
have polo a little the literature and
larvae challenge so robotics people know
how to do very well localization some
type of things but planning obstacle
avoidance and what they do typically is
that they heavily rely on for expensive
systems such as the velodyne system
right nowadays it's two hundred thousand
dollars to actually buy one of those and
they also have detailed maps of the
environment and so this is nice right
they can actually do a very good job we
have all heard about the Google car and
what actually inspired it'll be the
title of this talk is that I was
visiting a Google last summer and they
show their their car for whether you
actually use computer vision and the
only thing that we show that I use
computer vision for was given a tiny
little patch on the image can you tell
me the light the color of the traffic
light ice I further you know it's a
shame for our community to vs you know
providing this for such a nice
application suggest this okay so now if
you want to do Tanner was driving using
computer vision so what do we need to
solve so we need to reason over 3d right
so we should be able to pass stereo we
need to reason about motion right
optical flow we need to understand how
are we moving we need to the visual
odometry structure from motion
we also need to detect objects
particularly dynamic objects we have to
do this in 3di we have to interact with
we need to know where we can actually
drive we want to do this over time right
so 3d tracking is something also that we
need to do and in general we also need
to sort of solve through this in
understanding right we would like to
know what is the road where can I Drive
what are the buildings etc so it
actually is an application on provide us
with everything that we want in order to
evolve it you know while we are
currently working on in computer vision
or most of the things that we could only
working on ok so let me tell you a
little bit about what we actually did
for this project so far so we created
the kitty dataset ok which is a ittt I
ok this is very council and in order to
to collect this data set basically we
took advantage of our autonomous driving
platform so here is the car and we
mounted it with two stereo rips one
scholar one is great skill is pretty
high relatively high resolution wise
1400 x 5 500 and we also have a laser
scanner valid initial scanner as well as
gps and IMU localization and this is
going to help us actually create ground
truth for the benchmarks that we're
going to develop and so far we have
collected six hours a time frames per
second is actually on the order of two
terabytes ok so one of the next things
is that everything I'm going to show you
today even the raw data is is or it's
going to be available in the next few
days ok so let me show you a little bit
about I want our benchmarks so let me
just demonstrate is visually so here's
our autonomous driving platform and what
you're gonna see first are going to be
the benchmarks for a story on optical
flow so this is you know the kind of
scenarios that we capture the data from
so you have all sorts of very cluttered
scenes you know as well as highways and
more you know residential areas so here
is the stereo analytical from vitamin we
have 400 images to kind of training 200
testing we don't give run through for
the test ok so this and it provides
enough training later to actually get
learning algorithms to do this so this
is the
so the visual odometry and this is the
case of the 3d detection and tracking
benchmarks okay so you can see that the
level of annotation is actually very
very accurate and I will tell you a
little bit more about how we are noted
these things and how we do this okay so
well this gives you an idea of what the
bank to Mars actually comprise okay so
I'm going to tell you a little bit of
the challenges that we needed to solve
in order to do this so the first big
challenge of headache is actually how to
get the calibration right and so in
order to do this actually my colleagues
in the guttering sitting at the
technology developed some of the
automatic camera calibration which
actually is very stable you only need to
calibrate it once for the full day we
also develop tools for a calibration of
velodyne to camera to get register these
two sensors and here the challenge is
actually the velodyne right Alicia is
actually rotating so you had you need to
compensate for your ego motion in order
to register these two things so this is
the only part of the calibration
pipeline that is not fully automatic it
requires a few correspondences the other
thing that we need to do is also
calibrate gps and velodyne you need to
register this these two types of sensors
this is actually something that has been
done in the robotics literature so we
follow a set of procedures for that and
it's again it's fully automatic okay so
what was the second headache is like
okay great now we have everything
calibrated we have this massive amount
of data now what so one of the things
that you know I really wanted to do is
3d object detection right and in order
to do this we need to label this massive
data set and so we did this with
actually a hybrid approach so where we
use undergrads in order to label very
culturally the bounding boxes and so
that cost quite a bit of money and then
we also use Mechanical Turk in order to
get a collision labels for every one of
these 3d object detection okay and i
have to say that the Germans are very
precise is very impressive okay so here
are some statistics of the
of your data set so as you see we have
almost 200,000 cars in 3d label we have
all sorts of bands tribes so we have
more than 100,000 pedestrians as well
cyclus etc so we have a whole bunch of
different things and you can see us on
the right a statistics of actually the
occlusion labels okay so we have visible
our polluted and truncated yes so the
way we do this is that we created a open
deal the special tool that uses
projection in the images as well as the
ledge of data ok and the annotators you
save all this and trajectories over time
in order to get very precise labeling
yes it is everything you can transfer to
everything everything is register yeah
what do you you are not eat something
anything thank you Alyosha even though
it's registered the great leisure it's
too far away from the camera sees
different things so sometimes you have
things that are so I mean you have
definitely there's a field of view of
the cameras where you don't see let you
see with the velodyne but so we didn't
sit too much of a problem in terms of
let's see so the so there is between the
cameras there is 50 centimeters and the
laser is in the middle ok so it's not
zebras yeah and we actually triggered
everything via hardware so that is well
register ok so the ok so one more peg it
was actually that you need to actually
get you know a relatively sexy web page
so that people will will actually look
at you and you know how to actually
evaluate all the different techniques
and some more things that we need to do
is also a value idea the state of the
art in all these different much more
action this is something that undress
and philly the students working on this
did a great job so they so you can
actually access kitty you can download
all these things and I'm hoping that you
will actually dinner so maybe
relevance as soon as possible okay so
let me talk a little bit about the
arrangements and I don't want to spend
too much time because i want to show you
also samisen understanding stuff i guess
i will just highlight some of the things
okay so I don't think after Michael
stock i need to actually convince you
that millbury is not everything right
that we need to go a little bit
beginning this and ok so this is just an
example to even reckon with you more
where you know this is percentage of
pixels that have ever bigger than 1
pixel in middlebury so you see that is
very very small and we took the same
type of approach and then we apply it to
kitty so you can actually see sometimes
some catastrophic errors here it gets
fifty percent of the other pixels have
another bigger than three pixels in this
case okay so there you know from this
you know laboratory type of setting to
actually the real world there is a big
difference okay so maybe in a sense we
are not as already as we thought okay so
yes to highlight a little bit the
differences right so from laboratory we
go into taking this from a vehicle you
know things are not so rich in texture
right to hear Sam some houses that yeah
it's very is very difficult to actually
get a stereo from the sensor saturation
we have high resolution images therefore
we have much more disparities that
actually makes a problem for you know
some of the graph cat based approaches
and the the planes are actually pretty
slanted here right so that also can I
can make your life a bit more difficult
sigh yes I know that some of you don't
like it rankings but I'm just going to
put them anyways okay and it's a fear is
an example for the stereo ranking how it
is as of right now and so the nice thing
also is that we force everybody to tell
what they are doing and what are the
exact parameters of a use for their
method okay so that everything is
reproducible and this is very important
I think for making computer vision
events and so here's an example where so
the living method passivity which
actually something that I really like is
that learning is a learning based method
okay so so the fact that we have a
training sale actually is important
that's fairly well and I compared to the
others and let me just highlight some of
the when it works and one
doesn't work ok so so that's really
where when you have naturally since lots
of textures no objects and you have a
couple of wrong pixels at things like
things surfaces like folds and the
reason is because this particular
technique that's your reasoning over
segmentation as well as stereo now when
the does he have really travel is that
when you have this inner city scenes
where you have lots of objects and you
have texturally surfaces you have the
huge central saturation fear you have
reflections like in the caravan on the
right right so this is very very
challenging scenarios ok so so let me
tell you all you get is more about the
flow flow data set and it's a pretty
thin and Michael is not here anymore
maybe they want to hear the competition
but so so I asked andreas to actually
update the table so that we have you
cannot see anything but so we have also
the average of over all the pixels and
actually the method of the microbus
talking about is the second method here
and we have an average 11.7 pixels
difference on the underflow vectors
right so this is very very large so one
of the things that we saw is actually
that yeah fro algorithms are really not
ready for this kind of you know
scenarios and here we split the tables
into two because the lower part actually
uses visual odometry in order to come up
with her people are lines ok all right
so so we took the best results than by
bishop group and here you see that it
works very well when you have a small
flow vectors why not surprise me and
when you have in a city scenes with the
slow motion something like intersections
right when when you're not driving too
fast now the difficulties are in
difficult lighting conditions again when
you're driving on highway you have you
know large flow back close up to you
know hundred fifty pixels and so the
current methods cannot cope with this
this kind of motion so we also have
visual odometry we have the initial
batch Marr has 40 kilometers we are not
focusing lock closures ok so this is
like Islam base kind of thing okay so
this is just more to evaluate odometry
and here the interesting thing was to
come in with a metric that actually
doesn't prefer certain methods over
others so what we did is that we
averaged the errors and we support into
translation and rotation errors over
paths of different lengths okay because
one of the problems here is that you can
meet a little mistake at the beginning
right us look at the end point they're
going to be very far but it doesn't tell
you anything about how how with your
method is ok so we also did a few
experiments with object detection and
now finally the application data set is
going to be the full 3d object detection
is going to be available so this is just
showing that actually in the AC cases
when you don't have a collision actually
so the UFC TTI detector works very very
well ok so this is the purpose of a dive
in McAlester development and russ
Tkaczyk detective ok so so this is all I
want to show about the ventura also
wanna since yes
yes one year to students and I have to
say the two students were particularly
Andrea's there yes amazing and I wish I
mean this is much more than I hope for
and I think I mean it's just a lot of
work they also have you know a lot of
expertise in culture they have the car
driving around for a while right so all
that is already built in not not
everything they have flash cameras we
have to even you know engineer how to
Rome you know multi-donor rock and
whatnot so so parts of the stuff was
their parts whether or not there but
they have been working on this for you
know ten years only yeah yes so this is
one of the another headache I guess I
didn't talk about and so we try to have
diversity okay and for this we create
some descriptor that you know recent
cyber you know the flow content or the
stereo content and then what we did was
using classroom in order to select
automatically you know sequences that
are actually different and I mean it's
the best really we could came up with we
didn't want to select by hand because
then you have a prior right which is
your own bias okay yes ma'am
yeah yeah so that's a very good question
I mean so we are currently looking into
what I'm going to talk about a little
bit more now in check how you know
having a rough prior you know a map you
don't know exactly where you are but
more or less can this help you and this
is where you know cheap sensors can
actually help but yeah we haven't really
done anything it's very specific and
other action yet very good question yeah
okay so let me tell you a little bit
more about you know some of the stuff we
have no person understand and since this
is the scene understanding track right
after worship okay so and i'm going to
survey telling you a little bit in this
scenario what are things that one might
want to look into okay so so this is 3d
scene understanding for autonomous
driving right and the idea is that ok
going to drive around just a little bit
i'm going to have you know maybe 10
cycles always have a video and i want to
infer from that the topology on geometry
of the sin ok so here is an intersection
this is a four-way intersection and so
you can see outlining the weather and i
will also like to reason about the
actual semantic information with the
traffic situation in the same ok we like
to know other cars coming in this
intersection from where to where are
they coming right in order to maybe you
know have a decision in my internal
drivin so the way we tackle these things
is by using prolific generative models
and we build different models for
actually monocular on a stereo and i
will tell you a little bit of this to
two mothers in a bit ok and what was
important was actually to try to model
the dependencies between static and
dynamic information ok actually when we
have cars moving this is a very very
important q that we need to use in order
to do this estimation okay so the first
thing you need to do is actually try to
define was it the political model that
you want to do an obtained so in this
case we focus on for this case
intersections of three different types
in terms of topology so we have two
armed intersections three are my forearm
intersections which model most of the
intersections in classroom and we also
have a geometric model in order to have
a priest
is definition of the layout so you can
think of this as in an extension of the
keyboard layout and endorse you actually
the other scenario right we are you are
really interested in to these these
routes so what we have is we have a
random variable K which is the number of
intersection arms we have the center of
the intersection we have the different
widths of the roads as well as our own
rotation with respect to the
intersection and the relative
orientation of these things ok and then
what we since we have a private priority
queue native mother what we need to do
is actually define good priors and
define a good likelihood function right
so for the priors we use sophisticated
priors for these relative orientations
we build nonparametric positioning
statistics basically there is the
process mixture models for this and we
will actually likelihood functions using
very simple image information so in this
is what we use was a coupon secrets
which is something that people do you
think robotics very often well and we're
going to always risen in Versailles
perspective and you see the picture on
the right ok so you can see our own car
this is very sight perspective right i'm
looking it from from the ceiling and you
have the feel of view of the car and
then you can see color coded the one I
think that things are occupy their free
or I have no idea because there is an
occlusion or maybe I don't see it ok and
we use a stereo in order to compute this
a compass Ingrid's the other thing the
other thing that we use is actually flow
we assume that things that are moving
actually cars in this initial model so
it was very very simple and you can see
there that actually flow gives you a
very a very good indication of where the
streets are gonna be right and what type
of intersection you see here that things
are two people are turning therefore it
has to be a line on the on the left side
ok so I'm not going to go into the
details of how the model is built or
anything is going to show you some
videos so here's an example and actually
the duration of the video is the only
thing that we have in order to do this
estimation so here is a particularly
difficult case because we are not moving
so we have less information that if we
were moving and I'm just going to give
you a kind of like a photo pop-up type
of
encoding of what's the situation so you
see here in red actually where the
system estimated RS activity where cars
are driving and you see also destination
of the intersection so some of this is
seen are actually pretty pretty clutter
and actually that's a good thing for us
so you see that I videos are very very
short bus it's actually a fairly
difficult problem even if we have a
stereo okay so the next thing that I
wanted to do was to actually try to do
this with monocular imagery and the
first lesson that I learned was well
your previous model was nice we solve
this nonparametric stuff but this is
just too complicated right and in
particular the information that we are
interested in is correlated in the
direction of death so it's particularly
difficult in this case so we simplify
our model now we have a set of fixed
apologies and the other thing that we
did was actually two shell parameters in
the intersection so for example we only
have a single with four other the
streets and we assume that the roads are
collinear so there is only a single
degree of freedom encoding the angle of
one Street respect to the other ok now
i'm not going to go to the details of
this well just to give you an idea what
is the image cues that we use so we use
semantic segmentation ok if you can
segment things into building sky and dro
this is a very good cue we use vanishing
points you know you cannot detect them
very reliably but when you can detect
them this is very important and we use
also vehicle track lights so we use a to
the detector will link trajectories and
then we hypothesise hypothesize things
in 3d ok and the other interesting thing
of the model is that since we are now
going to have a vehicle detector a car
detection then we needs also reason
about where these cars are really
actually in 3d so so another model is
not only doing estimation of the layout
but is doing joint inference we also the
position of every one of our chocolates
in 3d and we need to reason about
whether these cards are actually park or
these cards are in the interim in
this is very important to get the weave
accurate okay and without really telling
you much I'm just going to show you the
video and you would see the kind of
thing that we that we we can do so this
is from 10 glory module you okay and the
video again is the only thing that our
system has so you see here on the right
it's again Versailles perspective ok so
is for 3d and you see you an estimation
of our own motion as well as the motion
of the vehicles in industry as well as
the layout so sometimes you know things
are not you know super good I in this
case right we got that car that was a
little bit too far up and the reason is
there you know if you get a couple of
pixels brown there the segmentation
right this is in 3d is actually very far
and but you can actually get further
noise estimation of the traffic here
right so you can think of you know
reasoning about you know what should i
do if i see this this this traffic right
the first thing that you notice is that
okay i can probably turn alive like I
should have stopped by going as the
continuous right yeah so so we use
boostin soon for that and boostin to
further yes our classifier okay and then
the jointing for us is conditioned on
that ok so we do it rid of it first yeah
yeah yeah this is something that we
should properly change soon okay so you
see there one of the cars park etc okay
cook and all right so so in this case
the serious comparisons of how good we
do respect to the two if you use a story
obviously Mona Clarke's not going to do
as well but now we have this rich set of
features that actually I think are going
to be very important to make the stereo
work much much better and the
interesting thing is actually we are
very accurate and estimating the
rotation of the relative rotation of the
different streets which i think is one
of the important things on your
interesting
ok so now one of the questions I want to
actually even ask you is that so we did
Kitty right I talked about this van
tomorrow we benchmark certain computer
vision problems but Kitty has much more
it's very very rich we can do a lot of
different things and let me show you an
example of what we have in mind this
place so in this case this is open
street maps and this is the evolution of
the annotations over time okay so you
see that is growing actually again this
is capturing cultural Germans love
technology so they annotated almost
everything and the annotate not only the
roads but they also annotate buildings
okay so it's relatively coarse
annotation so here what you have overlay
is actually the where the rods are you
have the network you can see the actual
three lines and you also where they have
an attitude the buildings okay so it's
not super accurate but we are going to
work into using this in order to get
accurate annotations of this thing so we
can now benchmark with with kitty with a
lot of data actually the type of layered
approaches that I was showing you before
now we have we have four cameras we have
gray scale we have color we have lesser
range we have GPS trace and now you can
think of you know trying to to look into
which one's of this sensor is important
how many things i need is color really
important try for all these different
things so what some of the things i
would like to do and hoping to do in the
next few months it's actually get also
semantic segmentation and this will be
transferred directly or so to the point
clouds okay so there is you know a lot
of interesting things that one can do
here and you can see here like you know
the reason is you know you can do things
like traffic analysis right outlier
detection etc okay so yeah so i have
five minutes four minutes three minutes
whatever some minutes i guess i'm going
to talk about one of my favorite things
okay which has nothing to do with
autonomous driving and that goes well
with what robin de llegar showing
or and then I will just stop after that
okay so so yeah one of my favorite
problems is this problem that I guess
David and Derrick and bar so I started
to work out a few years ago which is
like a given a single picture can you
estimate the layout of the room and so
this is actually a structure prediction
problem right where you are interested
into giving some features which are some
potential which are built from image
features trying to compute the are marks
of the layout okay so the question is
how can you define the features and the
parameterization such as you can do this
problem very very efficiently and very
accurately okay so there has been
multiple parameterization of this
problem and the so one of the
parameterization actually i think is
very nice is that given the vanishing
point you can parameterize this problem
with only four degrees of freedom okay
and now the second question was how you
compute this the image features for this
so in this case what we use is following
the set of their orientation maps and
geometric context which basically give
you for every pic so was the probability
of the confidence that you have of which
one of these walls is representing okay
so now the question is how do you
compute the potentials and whether these
these potentials so what typically you
have to do is so this this features five
here what they do is that for a
particular hypothesis of the layout it
count how many pixels on my hypothesis
left wall we are actually yellow how
many other green how many are blue etc
and then you will try to have a scoring
function that says okay that we left
wall should yes align with the yellow
pixels right the front wall along with
the green pixels etc now if you look at
the yellow wall in order to specify this
you need to three degrees of freedom
right there is three lines and you need
to to draw in accordance with this
vanishing points you know let's just
basic by this if you want to do the
front wall then you need four degrees of
freedom sorry it seems that this is
actually a micro random filled with very
high order potentials
okay so I'm just going to show you one
trick that you can do in order to solve
this very very efficiently so think for
a moment of integral images ok so in
integral images you have you want to
compute with the value inside the
rectangle and the way you do this is by
computing differences between things
that depend only on two variables right
which are the typical that define a
particular corner of the box so you can
extend this concept to Visser enable 3d
game between a similar manner right so i
can actually count what is on the left
wall or qualities on the front wall but
yes accessing some integral accumulators
with now are in accordance with the
vanishing points and by doing this what
I can actually do is I can obtain the
same potentials of people using the past
but now they are the compostable import
wise potentials and they're exactly the
same energy function there is no
approximation whatsoever here there is
no letting very well there is nothing
like that ok so what is the nice thing
about this is that now you can do
efficient difference in this model and
so the fact that you can do the fusion
inference is important not only because
you're going to be faster but also
because before since you have very high
or the potentials you have to discretize
your problem very closely ok so here our
results that are going to appear
disappear with our technique and the
interesting thing is that you only need
one minute to train the model and you're
already between the state of the art and
you only need for 140 milliseconds to do
inference over each one of the early
minutes on average ok so it's very very
fast it does very very well now ok so
one sentence ok so this is just an
answer to one of your questions that you
asked me a few months ago ok so I
present this stuff in the Illinois
vision workshop that we do up in the
Chicago area and then I guess me ok how
good is this solution and it is possible
to do it better and I just want to
answer the question that you put me
there and so it turns out that you can
actually do this even bar and what you
can do is say there is so we can have
come up with a way to do exact inference
in this problem ok so now you're
guaranteed to get the optimal
let's say 50 to the four possibilities
and you can get the exact solution in
only seven milliseconds okay so so you
can actually do this in France very very
fast and all right and anyways so he's
going to conclude with already dress Mia
and yeah so you know what I would like
to ask you what you would like to have
in Kitty and I mean a question we need
to ask ourselves is that I would really
ready for this it seems that some of the
you know classical algorithms do not
work out and I want to say that all this
is possible yes because of these two
students which are really fantastic and
and the rest will be in the market on a
year he is just great okay thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>