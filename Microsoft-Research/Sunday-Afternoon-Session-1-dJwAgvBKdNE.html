<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sunday Afternoon Session 1 | Coder Coacher - Coaching Coders</title><meta content="Sunday Afternoon Session 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sunday Afternoon Session 1</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dJwAgvBKdNE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so the first paper this afternoon will
be about variable focus video it's paper
authored by neate Ashraf a showgirl
gavin rama chellappa you she Taguchi on
cell to cell and amit agarwal and Nitish
will be giving the presentation hello hi
my name is Natasha off and i'll be
presenting our work on reconstructing
depth and video for dynamic scenes this
is joint work with rice university and
Mitsubishi Electric research labs before
i start the dog let us quickly review
depth from focus or different d focus so
it's a very well studied problem for
over a court over a quarter century now
here given a static seen you you have
certain focal setting you take an image
you change the focal setting you take
another image you change it further and
capture under image so the focus strike
first captured is then fused to extract
depth map information and this is and
this uses blur as cue to estimate depth
and there are multiple advantages of
death from d focus over other depth map
extraction algorithms for instance this
is a passive method you don't need any
additional equipment and the
correspondence is obtained for free but
there is a big limitation the limitation
is that it the scene has to be static
while the images are being captured a
very very related problem is that of the
extended depth of field where the goal
is to have large range of depth in focus
here again you capture a focal stack and
then pick pixels from image where it is
sharpest but here again there is a big
limitation of the static scene so the so
the goal of this paper is to extend
depth from d focus and extended depth of
field to dynamic scenes but then the
first question is why do we care
so here is the commercial camera we know
all these commercial cameras have
autofocus setting to it but this
autofocus setting is highly
underutilized and if you were to utilize
it it's carefully you could capture a
video like this here you can observe the
variation is due to two reasons one is
due to the motion in the scene and
another is the focal setting varies
between two consecutive frames and now
if we if we have this video and if we
can extend depth from D focus or
extended depth of field to dynamic
scenes what we can do is we can obtain a
depth video and then all focus video
both at 30 frames per second from this
captured video so this is the motivation
so have I motivated enough maybe not so
let's rewind two days back when
processing I had presented this and he
motivated it very very well and I was
definitely very motivated about this and
he used a the coated in the quotation by
John Steinbeck about the moment and he
talked about the duration focus stack
the moment focal stack where hit rates
of the temporal resolution to maintain
high spatial resolution so in so
basically what we are doing is just
playing with the data he captures and a
V and so basically playing with the data
that he has captured and trying to get
both a depth map video and an all focus
video so this problem as like other good
problems H is challenging because here
the correspondence is lost because of
the motion in the scene and the motion
in the scene could have been because of
multiple objects or because or could be
due to the camera motion itself and as
we will see further the presence of
blood makes the motion estimation even
harder so let us now analyze the effect
of motion in depth from D focus so here
is a static scene and here is the and
the focal state captured with this on
this focal stack you can apply your
favorite depth from defocus algorithm
and you extract a depth map with no
correspondence I
effect but what if you had motion in the
scene if you blindly apply your death
from defocus algorithm on this what you
will obtain is correspondence artifacts
and here is the magnified version of the
artifact you can see there are multiple
copies of the V in the in the all
focused image that was obtained so then
the question is how do we overcome this
let's say somebody gave us the motion
information so the question is how do we
use this motion information we can use
this motion information to create a
virtual focused at what we mean by this
is let's say the focal stack had five
images you can warp in frames two three
four and five to frame one to create a
virtual focused act like this or you
could have worked of frame 1 3 4 and 5
to frame to to create the second virtual
focal stack and now if you apply your
favorite day of the algorithm again you
will get nice depth maps so what we have
done is we said if motion information is
there everything seems nice life is good
and but then the problem boils down to
that of motion estimation so you can see
there are two here there are two images
from the same focal stack one is focused
at the front on the train and one is
focused at the back and you could see
one point in the scene has red channel
intensity value varying from 158 to 150
to 150 43 intensity value change so
clearly the brightness constraint is
violated and this is the basic
assumption behind optical flow
estimation so you can see that this
leads to spurious flow even in the
static region in the static region you
get around 2.5 pixels flow so then the
question is a okay we want to estimate
the optical flow between frame 1 and
frame 5 so let's say somebody gave you
the picture map and depth map for frame
5 what we could do is use the depth map
to reblog the texture map such that it
the the focal setting of frame 1 and
frame 5 matches and then the optical
flow will look
nice because the brightness Constance is
satisfied so now you can see the
variation is probably 158 and 154 and
hence the optical flow is improved in
the static region you have only 0 point
4 pixels flow so too in short reap
learning improves flow estimation so
what we have done is basically created
this chicken or egg problem what we said
was if we have motion information we can
do well we can get depth map and then
what we said was if we have depth
information we could do well and obtain
motion information but then the question
is how do we estimate them so when one
way would be a unified framework to
solve motion and depth together where we
would want to solve multiframe optical
flow and multi-frame death estimation
together but as you could guess this
would be a very hard optimization
problem and because of the large number
of variables involved in this another
way another nice way it would be the
iterative reconstruction and this is the
framework that we follow we first
capture a focal stack video we first
compute an initial optical flow I will
come to the details of it use this
optical flow to align images on the
anthers create virtual focal stacks on
and then we can do depth and depth map
extraction on this virtual focal stack
and then once we have this we can go
back and refine our optical flow
estimation and once this converges will
have nice depth map and extended depth
of field videos so so now looking at
each of these blocks so the compute the
optical flow block in the in the first
iteration we don't have depth
information so the initial flow is going
to be course we could apply few
heuristic approaches or make a remnant
about the scene for instance if you if
you assume a constant velocity
assumption between let's say the frame
frame let's say the focal stack has five
images you could you could create you
could compute optical flow between frame
one and frame six because they are at
the same blood level and then if you
were if you could assume constant
velocity you can interpolate the motion
between frame
between frame 1 and frame to using the
motion between frame 1 and frame 6 but
this is just course initial flow and in
the later iterations we refine it and
using the Reeb learning approach that I
that I just discussed once we so this
was the optical flow and so then is the
depth map extraction and the extended
depth of field image extraction we
create the virtual focal stack and then
texture map can be you can be created
using the photo montage algorithm
proposed by a sim Agrella in 2004 or and
for the depth map you could pretty much
use any of the shelf depth from defocus
algorithm our formulation is based on
the graph cut on super pixels where the
data item is rebelling term smoothness
term is also between both both the
temporal smoothness and special
smoothness and here is the experimental
prototype that we built in our lab
instead of changing the focus setting we
move the remove the sensor a bit and
this changes large large focus changes a
large amount of blur in the indie
captured images and and here here is a
result on on the can rolling data set
here if you were not to compensate for
the motion you can see you get a large
amount of artifact in the in the in the
texture map and with motion compensation
if the texture map is much better and
here is a captured video this is how the
captured video looks like and this is
the all focused video you can see some
artifacts at the boundaries and here is
the depth map I let it play once more
here's the captured video the all
focused video and the depth map
because we do not have the ground truth
so we can show this using using now
we'll be rendering result and you can
see it pretty much looks okay and and
here is a result here's the result on
another scene data set and here is the
scene with a camera motion you can see
the camera is moving here and this is
the all focus video created by this and
there is the depth map you can see
they're still artifact on the boundaries
and the captured video again and the
depth map here so this definitely has a
lot of limitation still it inherits
limitations from the traditional depth
from D focus algorithm which is it neat
scene texture and then also from optical
flow where these the motion between two
images should be a good enough so that
the optical flow can still work and it
also has additional limitations in terms
of the depth range of the scene and the
choice of initial flow estimation will
will introduce another extra limitations
to the algorithm in our future work we
are working currently on the robust
robustly reconstructing the boundaries
and do better occlusion handling in the
paper we discussed the operation
handling also and also characterize the
fundamental trade-off in this scenario
between the temporal resolution and the
spatial resolution so to summarize in
this in this talk of what we have
extended we have tried to extend the
depth from D focus to dynamic scenes we
carefully analyzed the effect of wedding
blur in flow estimation we proposed an
iterative reconstruction algorithm using
rib learning approach and then we showed
some results with experimental prototype
and the hope is that this will convert
and off the self camera into a depth and
video camera thank you for listening I
will be happy to
I'm curious if you get into trouble with
the is the magnification your changes
when he refocused yeah yes we will do
the homography calibration okay thank
you the second talk is the second talk
is about depth aware motion deblurring
its work by Li Shu and GI Joe and Lee
will be presenting thanks for the
introduction and in this talk we're
talking about depths our way I motion
deblurring the term ocean deburring used
in our paper refers to removal process
to remove blur caused by camera shake
and this happens when you take images
using handheld cameras and under dim
light conditions recently we have
witnessed the rapid development in
motion deblurring and the methods can be
generally classified into uniform and
non-uniform produce and you in today's
money is talk there are very good there
are a local papers and talks in today's
modern session basically representative
method assuming that the psf is constant
across all across the image and the
obvious f is spatially smoothly varying
in the image so we discuss another
problem the motion deblurring we find
that depth variation do affect the blob
formation process on the first place
team points with different herbs may
have psf changes and the psf changeable
varied the scale of psf will change
largely evidenced by the point
trajectory in the three patches on the
second fan sharp depth discontinuity
door cause abrupt psf changes and wake
which makes it difficult to be estimated
even use local patch base
now this is one example using recent
non-uniform and uniform deburring
methods we cannot produce very good
result and the closeup sure that they
are still contains large adverts
adventures and this indicates that the
depth variations in discrete example
cannot be simply ignored actually each
point do not have a not only has the
depth and also has a perhaps for the
SPSS to be estimated so a solution space
is very large position we tend to use an
extra depth map to facilitate the psf
estimation and we turn to the studio
camera solutions this is because the
steerer camera has a long history and in
last several years 3d technologies has
experienced origins and there are many
commercials 3 3d digital cameras
available on the market and we use the
fujifilm fine series which can capture
to high-resolution images at the same
time as shown on the right so our free
McCleary like this first we take images
using 3d cameras and then use these two
images to roughly compute the devs map
further we use this map to help ESF
estimation in a hierarchical estimation
schemes and finally we store the images
we find that even given the ground
through steps map incorporated apps into
psf psf estimation is not trivial and we
were shown later and our framework is
iterative process and TT in clear latent
image can be further fading to adapt
estimator to refine at the apps and PSFS
you first see how we can establish
correspondence between depths forth
between flood frames to via stereo told
us that to correspondence between two
rectified images can be established
using the epipolar geometry
and that's the disparity is the inverse
proportion to the depth map and it can
be used to guide psf estimation when
considering a broad frames if we assume
that translation a camera motion that is
a camera undergoes translational motion
a correspondence between two broad frame
can be still established and seen in
this example to obtain the course depth
map is like this this it is to come
teams many artifacts and it's better
cause this is because that the devs
boundaries may have a crew shins and
below also affected devs estimation
however the accurate boundary will not
likely affect the psf situation in a
hierarchical scheme and the depth can be
ameliorated to some extent you see now
obtained the clear latent images so now
we see the major difficulties in
incorporating the devs map into the psf
estimation and our hierarchical scheme
given the depth map a straightforward
method is to apply the existing blind
deconvolution methods to each step
Slayer however this is not easy taking
this for example this region includes
all pixels for one debs layer are using
our previous blind deconvolution methods
good adult result is like this the
colonel contains obvious artifacts and
is noisy and the restored images have
have some raining artifacts and this is
explainable because the size of this
region is not the large and the small
size depth layer do not contain
sufficient pixels for psf estimation and
thus it and feel most of existing
interpreting approaches nothing so when
we consider that include more pixels for
for the blind deconvolution and we apply
the similar approaches and get a result
like this we can see that the psf has
been improved to some extent and it is
sparse and do not
many noises however the restored images
to have large raining artifacts this map
because that it contains many pixels
from different data layers and the
restauration is not that easy so what
can we obtain accurate the PSFS for
small size adapter region that's our
questions and we use interested in it we
find that we can if we can use the
inaccurate psf estimation as
initialization using the similar
algorithms to obtain a result like this
the colonel has been largely improved
over the image relations and the
restored images is artifacts free so it
reveals the two important factors
applying the conversion at first we need
a large regions to collect sufficient
pixels to perform a robust the psf
estimation in a second if you won't have
a good PSFS for small sized regions we
should have a good initialization and
these two points motivates our
hierarchical estimation scheme based on
the novel region tree we see how we
control region she is based on the app's
map the depth map we use in different
color to label different regions and
notice that each regions do not have
very large size the number below
indicate the disparity value and then we
merge regions with adjacent depth map
adepts values to form a larger regions
for psf estimation and this process
continues until we get sufficiently
large regions for psfs tunisia it has
some several advantages although each
each regions have has pixels with
different tabs values the app's
variations would not be very large since
we only merger adjacent apps values and
also they are large enough in this
example we have three region trees and
the psf estimation is performed in a
top-down fashion for the root nodes in
the region cheese we just estimate the
psf independently for each levels in the
region cheese
the PSFS mission is performing two steps
the first is the psf computation and
using the using the psf from the parody
notes as initialization as we talked
about that the psf installation is very
important however the psf might not
always been proved because the existence
of textualist regions are noisy or very
small size regimes so we should have
refined these psf and we call these are
psf selections as we use informations in
the region trees refine the psf the psf
computation step is similar to existing
blind conversion methods except that we
use examples from the two images and
jointly as meet the colonel iterative
operations in our closed form the
colonel solution is the melodrama
previous work now given the computed psf
for each node we need to identify the
problematic psf as image then refine it
we use a PS I've entropy to identify the
identify them because aaronus psf
usually contains noise in and not that
fast compared to space motion Kono's
rather than simply drop in the Colonel's
we just use it to construct a psf
candidate set and using these kernels
and also that the kernels from the
parent and sell bring those then in it
ISA Graham in this example we have a
kernel set containing three kernels so
the problem becomes becomes to select
one PSS from us from the psf candidate
set and naturally we can use a
reconstructing arrow to select a clattuc
on condos by measuring the residue
arabic between reconstruct images and
the blood was however it is known that
the such kind of reconstruction relative
errors well-favored a smooth
interpretation and thus smaller kernels
will be selected so we propose a novel
short fielder in various measures this
is based on collision of two signals
when they
they reconstructed images and the other
is a shot fielded version details on the
track filter universe measures can be
fine in our paper and here we only give
some intuitive explanation which are
true signals when is blurred and when
it's sharp and the shock filter try to
turn the shaft attended blood signals
into a shop 1 by finding the infraction
point and thus it ordered that bro
signals a lot so the correlation between
the brothers signal Nana shock filtered
version is always smaller than the shock
and then the sharp sharp signals and
it's filled in Russia and we can use the
provision to find a correct colonel and
in practice we use the corrosion of
gradient magnitudes for the sake of
robustness which was a validation here
are true patches and we bloody deduced a
Gaussian kernel of size five and we
computed the reconstruction error as
well as a shock filter in various
measures and the reconstruction error is
plotted in the red line you can see that
they cannot find a crack kono and always
favor the smaller kernel size in our
shock filter in various measures find a
corrective sighs and by replacing the
Gaussian kernel with the space motion
kernels we have obtained a stimulus alt
and these are not two special cases we
have actually experiment with many
patches and different curves and
experiments always suggest that our
shock filter in various measures
outperform the classical reconstruction
errors so based on these psf selections
we can find correct the psf for each for
each node so this is this stage we have
the PSFS and the depth maps we need to
reconstruct the cleared image however
the pixels near the depths discontinued
is my not in complying with the
commercial model so it might have some
learning artifacts so we just increase
the regularization weighs nearly stabbed
it taps taps boundaries using these
steps map
and the PSFS we can reconstruct the
final clean image our framework is the
iterative one and after we obtain a
clean images we can fail it into the
depths estimator to refine the whole
results and we repeat this step twice
now we see some results and comparisons
this is a typical reference image and
it's matching view know that the points
either FR n is less blood due to a
distance and this is our costly
estimated abs and region trees by using
the PSFS estimated top level we get the
result imager like this and please know
the notice the strong reading artifacts
on the on the cup and the box of milk
milk by propagating the kernels to the
middle level the results has been
refined we can further propagating there
and propagate the kernels to the button
levels and the quality of the images are
getting better this is a result from the
first pass and our second pass helps to
further refine the result the close-up
shows that the second path can be reduce
the strain in artifacts in near textures
and as examples and our restored your
style images the characters under the
boats foreground and background can be
where recover this is a matching view
and the restored images of the matching
view we can notice that there are still
some artifacts in near the devs
boundaries
and the the desk map and PSFS from here
the psf suggest that the pier the real
blurry miss actually varies for
different depth map Adam depth depth
layers finally our method can be also
applied to sinker image non-uniform
tipperary by assuming that appears have
now varies smoothly this is done by just
the partition regularly partition images
into four patches and construct a
hierarchical estimation tree then the
psf is estimating top-down fashion this
is a result and the resulting in psf
kernels they do very smoothly and this
is a blurred image of franzia da from
the siggraph in our restored images we
can produce similar results with that in
a prefer and this is a bradley major
front group diet or where cameras and
the girls are translational motion and
our restored images close up from
different positions sure that we can be
our country we are recovered details at
different positions and only will
conclude that work in this paper we have
proposed adepts of we are devouring
framework and 12 knowledge it is the
first attempt to incorporate depth into
a blinding commercial methods currently
our master has and limitations first we
only handles translational camera motion
and maybe the filler Direction include
extend current models to general not
blind a general not only from the
brewery using a method of right or the
similar models like that but it might
involve much computations so we learned
from this this project that psf
initialization is very important for
blind deconvolution and also
hierarchical estimation scheme is very
useful for non uniformity bourbon
especially our region tree structure
finally the shark filter invariance
measure always not performing the
classical reconstruction errors
and that's it thank you very much you
have time for questions you mean that
and actually the weed we do not
intentionally as a noise but the images
still companies and noise and I think
since we use the samples from the true
images and are the joint in the kernel
estimation is a little robusta to noise
oh you mean shock filter typically we
can apply some smooth before the shock
filters but in this step we do not find
the noise will largely affect the shock
theater scene especially in measuring
our shock filter you very special
most curious is the variation of the psf
is it usually due to depth of field
effects usually translations of the
camera yeah actually currently we handle
the PSFS caused by camera motion and it
is fast so we just reinforce them spaz
spaz priors who has makin or something
like that but it can be extended or
different plus costs bifocal blood yeah
yeah alright the last talk in this
session is going to be about super
resolution from internet scales keep
seemed matching its Bilibin son and
james hayes and live in i will be
presenting the paper what did i do
alright um can use this Mike Vargas
accuse this yeah I think that's yeah
okay can you all hear me I hi my name is
Labine son I'm from Brown University
today I'll be talking about a super
resolution from internet scale see
matching this is joint work with james
hayes my advisor so before we even begin
to the next slide I want to make the
distinction that the super-resolution
we're talking about right now is
different from the one we've been
hearing about in the past two days so
here we're concerned about upsampling a
single image so after down sampling the
high-resolution image the high frequency
information is lost the only way you can
sort of go from lower dimensionality to
a higher dimensionality is by
hallucinating possible details and this
is very different from sampling light
feel and then actually recovering
recovering the true high frequency
signals right however this is a very
well-defined problem and there's a lot
of paper on this the image formation
model goes like this you start from a
high resolution image apply Gaussian
blur downsample add noise then you have
the Observation why
is a low resolution input and basically
we're trying to do is go backwards from
the low-resolution image back to the
high resolution image and here you want
to have more pixels that's the first
thing you want to do and then you want
to also have sharp edges and possible
details such as textures or object level
details so mathematically this is
definitely a very ill posed problem on
top of that there's this vision hard
aspect about this problem that's very
challenging so for example if I show you
this low resolution image on the left
and ideally you want to go towards the
right by considering all these
high-level informations in the image
that's very specific to this scene you
know you want to insert snow grass
mountain textures and possibly you know
not textures from this indoor scene into
the into that image now let's take a
take a brief look at some of the
previous work and see how people have
been trying to tackle this hard problem
alright so we we argue that there are
three aspects governing the quality of
Fae a single image super-resolution
algorithm so the first one is
faithfulness is not in the plot because
this this is basically the capturing the
image formation model and this is
relatively easy to satisfy by most of
the algorithms nowadays and on top of
that would claim that there should be
two other things that we should look at
one is level of detail which is on the
x-axis then there's also realism so an
ideal algorithm should be on the top
right corner so far out there so it's
saying you should have a lot of details
at the same time being realistic and
then you can perform very basic
interpolation method that gives you
minimum artifacts but at the same time
there's no detail synthesis synthesize
it at all so this bicubic interpolation
should be on on the y-axis and there are
a lot of work on compact parametric
models such as gradient profile priors
or sparsity priors things like that if
you applied those you can you can sort
of reached
or the edge is pretty well it gives you
sharp clean edges but these parametric
models are very limited in terms of the
way it's in terms of being expressive it
doesn't give you texture which is a more
complicated phenomenon so there are a
lot of data driven and patch based
method that try to hallucinate textures
in smart ways I won't go into detail
into some of them but I'll bring bring
out a few very soon so what we're trying
to do here is basically build on
existing method and try to expand the
horizon a little bit and push the limit
and see how far we can go in terms of
inserting detail and at the same time
being realistic so you know towards
towards the other spectrum of the detail
axis you want to have sort of like
object level details given the correct
data and the algorithm so if you look at
textures and objects these are very high
this carry very high information a
high-level information that sort of
tells you if you can do some sort of
recognition and you can sort of do the
problem pretty well and this is actually
I've been tried by this paper from ICCP
two years ago so what they did is that
they have a database of textures and
then the require this classification of
textures first however this is a very
hard problem given low resolution you
can't really recognize the material or
the texture so they require a human
intervention to make sure the
classification step is correct and then
after that the synthesis step will be
constrained you know by drawing patches
from only the relevant texture category
another drawback of this of this method
is that the texture database does not
contain natural transitions between
textures this is very important because
in a natural scene these are some of the
very salient boundaries in the scene and
they resort to a different pipeline to
handle this these edges and boundaries
another state of the art method comes
from soon that's happened this is two
years ago from CDP are so here they do
not require any heart labeling of the
class
or classification of the texture rather
they start from a input query credit
image segment and they find similar
segments in a large database in terms of
similar filter responses so the hope is
that by constraining the problem with
similarly you know contextually similar
image segments you can find similar
patches there and hallucinate details
that are appropriate however this
doesn't always give you the correct
semantically correct segments for
example if we look at the first row um
it's very likely you will be inserting
patches from ceramic plates into a zebra
this is not ideal although sometimes
visually you might think is okay so an
ideal algorithm should really consider
this high-level information but it's
very hard to do so given the low
resolution other than these data driven
methods there's also this body of work
that focused on sort of this patch
redundancy and self similarity in
natural images this is very interesting
and very cool property of natural images
however I guess there's sort of this
inherent bound in like how much this can
go because they only resort to the
patches available in the input image
itself so the signal present there is
sort of limited and there's probably
some kind of upper bound towards how
much you can desert but nevertheless
they do this is the very cool very cool
stream of work so our work is sort of
similar to some of the previous method
in that we rely on external images and
we try to hallucinate details by drawing
patches from other images and the way we
try to do it is by looking at seem
matches and the major contributions we
claim are the first so we're the first
to consider using see match for a single
image super-resolution and we show that
see matching can be done reliably well
at low resolution and then this is a
cool result so we make comparison to
this cvpr paper last
year and we claim that see match
statistics can be favored over internal
statistics which is the opposite of the
conclusion that that paper had and
finally we want to say that we have
competitive results and we show that we
can handle texture transitions were
really well beyond the capabilities of
previous methods this is just a brief
overview of like how much data has been
you know used in previous work so for I
guess the the first seminal paper of
data-driven single image
super-resolution came from Freeman 2000
there was six images used and towards
the end of 2010 some of the
state-of-the-art methods start to use
more data to push push the limit and
here we're considering internet scale
images of millions of internet scale
images and the natural question you want
to ask here is how do you even leverage
on so much data is it even doable so
we're proposed to use scene match sort
of as a filtering step to filter out
maybe like ninety-nine percent of the
images that are sort of irrelevant given
this input image so this is our pipeline
and let's talk about scene match first
all right see much sort of matured in
recent years and there has been a lot of
papers and in this field showing a lot
of different applications however this
see match this idea of see matching has
not been sort of thoroughly explored in
the context of super resolution so we
want to apply that and see how much we
can we can do from there so this is a
very hard problem from the beginning
because given these input images so
these input images are 128 pixels wide
these are really tiny like sort of like
a thumbnail and if you consider like the
features you can extract from it they
are very limited especially like texture
regions that are degraded by the down
sampling step so see matching might be
hard from here but however if you
consider a combination of features
including global features that sort of
survive this down sampling step combined
with other low-level features
actually doable and we show that giving
you know a large database of diverse
scenes you can actually find similar
scenes so on the right we'll show some
examples we also show a failure case in
a in a later slide so I guess these see
matches are reasonable and certainly
give you some hope in terms of inserting
details because you know the textures
you need on the left are available on
the right and on the right side we have
like a full resolution image so we can
build like a Gaussian pyramid and grab
the relevant patch if you can find it
alright so see margin is definitely one
of the novelties in the pipeline so why
don't we are so I'm going to spend some
time sort of trying to go to the bottom
of this and talk about the properties of
a scene match so we want to make a
direct comparison to Jean tag and irani
from cvpr 2011 here we consider this
property's image statistics from
different databases there are four
databases that we consider the first one
is on the left that's the internal
database if you're familiar with this
paper it should be a very simple concept
but if you start from the ground truth
image suppose you have the input image
with the high resolution ground truth
you form the Gaussian pyramid so each
database is a collection of patches
grayscale with mean removed so you form
the first one which is internal database
all scales you collect all the patches
from the pyramid that's your database
but in the context of super resolution
you only get to observe part of the
pyramid which is the lower frequencies
and from there you get internal database
limited and then same as that paper we
form the third database which is
basically images from the berkeley
segmentation database you draw patches
from those training images and that
gives you the third internal database
and for the fourth database we're going
to replace those bsd images with our
scene matches so in our experiments
we're going to use 20c matches for each
input image 20 is just a number we chose
we don't have a way to justify it
but it sort of worked and we're going to
compare the properties of these four so
that city PR paper actually claimed that
internal database have a lot of
desirable stable statistics behavior and
should be favored over number three and
what we're going to show is that number
four is actually better especially in
the task for super resolution so the
first measurement we want to consider
this quantity called expressiveness so
what does that mean right so first you
choose your database one of the four is
a collection of high resolution patches
and then you start from the ground truth
image you pick a patch and you ask the
database what's my nearest neighbor and
what's my error in an SSD sense and you
do the same for all other patches and
you aggregate the results so these two
curves are from the internal database
the y-axis is the error so lower is
better the x-axis is the mean gradient
magnitude so on the left you have smooth
ones on the right you have like sharp
edges and maybe in the middle of these
are textures so it's no surprise that
internal all scales is pretty is a lower
curve however this is dotted because
they sort of don't participate in the
competition because they they capture
much more than what you can observe in
the super-resolution scenario so what's
really competing is the solid red line
and this is by using 20 random bsd
images and this curve shows the 20 c
match your sort of wins marginally but
it still is that rather a stable
behavior now another quantity you want
to measure is called predictive power so
this is a little more complicated but
this is more consistent with the
scenario of super resolution so here the
database becomes a giant patch pairs of
low and high resolution or frequency
patches you start from the low
resolution observation and you pick a
patch and you say what are my nearest
neighbors and what's my estimation of
the high resolution so
is basically a weighted average of the
corresponding high-resolution patch and
you consider the difference between
these estimated patch and the
high-resolution ground truth and record
the error and do that for the other
patches in the image and you then again
you plot these error against the meaning
gradient magnitude so again these are
the internal database curves this is
where the bsd images are and 20c match
now becomes way lower than random images
from berkeley segmentation data so this
is saying if you have seen specific data
that are similar to your input you can
do a better job in terms of prediction
given a limited resolution image
observation so I guess that's a very
cool property and hopefully that'll give
you this idea that it's promising to use
scene matches for this problem and now
we're going to move on to the rest of
the pipeline so the rest of the pipeline
are pretty much inspired by existing
work next step we want to do is
basically further constrain the problem
by providing a region level context so
we're going to do segmentation and this
is one example of the segmentation
results this is a very simple image so
you can see all the curly image a query
segment on the Left column and then we
show the top five segments matches so
one really cool properties of see match
is that textures co-occur in the c match
so if you have sky against vegetation
chances are you can find similar
boundaries or similar co-occurrence in
in the c match so what we're doing here
is basically expand the mask a little
bit in practice so that we can capture
this fine transition between textures
and we have examples available for those
patches we don't have to resort to like
other pipelines to make this work and
then the the heart of the algorithm is
how we actually make use of these data
right so the first step we take is sort
of like a tiling
process to synthesize possible texture
over this image and we took a greedy
selection of pixel candidates this is
actually very similar to the single pass
algorithm introduced by Freeman 2000
paper and also this is sort of like a
standard practice in a lot of texture
synthesis algorithm so what's happening
here is a pic a first patch based on the
low resolution observation alone you
pick an example patch and then you
condition on the existing high frequency
patch you will move on to make your
subsequent choices so there is this week
conditioning between the choice of
patches and after this tiling process
every location in the image has a number
of pixel candidates and we can plug in
this nice energy minimization framework
introduced by so needs happened so this
energy function has three terms the
first one captures the formation model
saying it should be faithful then the
second term is where the data come in
you want to say my output pixel should
be similar to one of the candidates so
that it's sort of natural and you can
hallucinate textures and then finally we
have a sparsity prior sort of using
student T distribution to make sure the
image is heavy-tailed in terms of filter
responses and we run our experiments
using a test set of 80 images of diverse
photographic scenes so we're going to
look at some cool results I guess I
should turn off the lights just bear
with me um is it good looking okay so
we're going to show you four images on
every slide top left corner is bicubic x
88 times up sampling in matlab and then
we show hours so what's happened is
possibly the state-of-the-art method for
data-driven methods that makes use of
external database and then glass not at
all is a very flu shot paper that sort
of represents the best result from using
internal database so these make very
cool comparisons I'm going to show three
close-up crops from each image and you
can sort of compare them and see which
one you like better
so this is vegetation and you can see
some of the high-frequency details along
the boundaries in our image does that
look like water texture to you a little
bit and then here I guess those are like
little pads or something I don't know
but hopefully that's sort of looking
convincing to you and this image is
pretty cool because we have in our see
match we actually have basically a
objective match it's phone booth again
sort of the same architecture in London
but not the same location so given you
know given the correct data we can
actually produce very fine detail object
level texture for example the corner of
the bricks and here part of the phone
booth you can see sharp edges and more
natural images you can see transitions
actually come out really well sky
vegetation there's high frequency leaves
there you can see the grass along the
boundary and this is this interesting
case where we have instance level seem
match so we have like photographs that
are taken from a similar view point of
the same scene you can see the rooftop
sort of takes on a more well-defined
structure compared to the others alright
so I guess that's all the results I have
and I'm going to move on to some of the
failure cases that could be interesting
so this is more intuitive right this is
when you have bad scene match it's like
garbage in garbage out there's no hope
of inserting useful textures this
happens quite a bit for indoor scenes
for outdoor scenes we can do a pretty
good job and the sort of looks blurry
and straight lines are not well
preserved and this is I think the most
interesting failure case so this is not
saying our see matches bad on the
opposite we have
perfect see match like all the textures
you need on the left can be found on the
right but the problem is the input image
is not so structured like the tree trunk
that's that's the only main structure in
the image and it's just really hard for
the algorithm to produce coherent
textures and this will we get it looks
like a painting you can sort of see like
there are leaves coming out here and
there but they don't look great so we
think as texture transfer methods sort
of mature or advanced we can we can
improve the results even further finally
I'm going to talk about evaluation so we
did a perceptual study based on
responses from 22 participants from our
CS dept CS students and here we show the
charge by looking at two categories
right so we have ten test scenes and ten
of them are sort of good seeing with
good scene match and the other with sort
of bad see match and it looks like based
on the votes we get good see matches
actually produce way better results so
every time we ask the subject this
question which enhancement has better
quality and we're comparing against
anton's happened because in a previous
study we found that a glass noise result
tend to have lower lower votes so we
ignored that every time we asked the
binary question which one has better
quality and the votes will be showing
that in the chart and then for the bad
scene match case we're sort of doing a
tie it's not clear who is doing better
yeah so I guess that's that concludes
everything I want to share with you and
just to reiterate my conclusions are
three there are three conclusions we
want to draw from the study the first
one is that where the first one to apply
see matching to super resolution and we
show that it's actually reliably done it
can be reliably done and then somatic
status as statistics can be favored over
internal statistics so there could be
nice properties that people can use for
similar applications may be beyond
super-resolution and finally we show
good results especially texture
transitions and our future work will
sort of bring this further and make more
realistic images yeah oh yeah we have
time for some questions oh yeah I will
look into that thank you very much sure
um are you looking at expressiveness or
or predictive power
this slide right right sure um so so
yeah one thing I want to mention is that
for this experiment we do not apply any
transformation to any of the database so
in the zantac paper they actually they
claim that because for the internal case
you have very limited number of pixels
so to be more fair you're going to apply
a few transformations to them and keep
the transformed patches in the database
as well so I guess in our version it's a
weaker version of the internal limited
case we actually have the plot but I
sorry I don't have them in the slides we
looked at it and whatever the case c
match is doing better so we apply
transformations to all database as well
we have a very very large of
transformations including mirroring
contrast change and very fine detailed
rotations and we apply that to all yeah
yeah um yeah I understand I guess this
is something we haven't explored so
you're saying aliasing in the low
resolution input right so the hope is
that these features that we extract will
be robust enough to sort of sidestep
those problems I haven't looked into it
but I yeah I think it's a very relevant
question so i don't i don't have a good
answer right now yeah
what kind of depends on the ratio
between
I guess that depends on how the input
image is formed right so in this study
we don't claim to know how the image is
formed other than the idea that there's
this general image formation model
Gaussian blur down sampling add noise
sometimes you're given an image that's
low resolution and you have no idea how
its formed and then from there if you
want to do app sampling I guess
sometimes you can't consider the image
formation model like that and also like
so we're consider the times eight
super-resolution just because this is
actually a common now this amount of
blur can be commonly observed in other
scenarios such as d focus our motion
blur so it doesn't necessarily have to
be super resolution yeah thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>