<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ultra-Low Power Computing Workshop 2014 - Devices and Cloud | Coder Coacher - Coaching Coders</title><meta content="Ultra-Low Power Computing Workshop 2014 - Devices and Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ultra-Low Power Computing Workshop 2014 - Devices and Cloud</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rCMAx9sVUgU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
ok.thank sticking around everyone this
is the beginning of the last session of
the day and we have three great speakers
talking about some larger systems and
infrastructure and some programming
challenges in ultra low power systems
our first speaker is jie liu and he's
the head of the surge group here at
Microsoft Research and he's going to
talk about how we can hide heterogeneity
in a way that is useful so ji what thank
you so just want to share some of the
experience and efforts we had in this
space and probably more questions and
challenges then we have solutions about
so we talk a lot about specialization of
silicon of devices essentially entering
a world that we have lots of wearables
ourselves and devices in the environment
and all these heterogeneous computing
capabilities and communication and
sensing how do we manage those and how
do we enable people to leverage those
heterogeneity to deliver our energy
efficient systems that's the question
want to ask here in some sense this
paradigm has been driving mobile
computing for the last at seven years
right touchscreen experience of being
reactive you have a device that you can
put it out with you very nice user user
experience on delivered on the
touchscreen what we are looking into I
think in the next wave the next
generation comes in our we're bows and
devices in the environment that enable
the computing systems to be proactive to
understand the context I understand who
you are where you are what you're
interested in but how you get to this
state and try to predict try to help
always there and always be available and
with that vision right people
essentially have to deal with this sort
of complexity you have lots of sensors
right different sensing modality
different form factors somewhere some
the environment you have a quite
heterogeneous platform or network
devices in terms of computing capability
some on the wearable device themselves
some on the phone that to go with you
some may be on tablets and laptop that
just you access it in could be
intensively but intermittently cloudlets
pieces of the cloud that that may be
offloading your computing from mobile
devices and laptops and enabled and
backed by the entire cloud
infrastructure building on top of that
all the intelligence people are thought
about right being able to identify who
you are being being able to understand
your context your location being able to
deliver reminders since your mood your
shopping task that the list just goes on
and on and on and then on top of that we
have both first party developers who
actually build these devices and deliver
or imagine certain services you would
use but also want to open it up to
third-party developers that would just
want to build apps on so far has been a
single single devices but pretty soon
we're going to see apps that we're going
to scale across multiple tiers of
devices so how do we manage that
heterogeneity is the is a very
challenging question so we're going to
start small like we started at just
looking at a single device and once we
have hydrogen air processors this is
work we've been we've done a number of
years ago that looks at the the mismatch
between the sensor energy consumption
and the processors energy consumption
and look at the the typical architecture
back then was you have the phones
application processor connect to all the
sensors every time you want to use the
sensor this is being the reactive model
right the phone has to be on and a the
application processor would drive and
rate those sensor data in we're
basically dealing with two two kinds of
efficiencies one is the big processor
although the sensors themselves could
consume only milli
many watts of power the big processor is
very power hungry the second is all this
week up and sleep overhead you have to
spend some energy pull out of the deep
stack to have the big processor to be
all and then do very simple task and go
back to sleep again back then right
we're talking about the skip it and now
this is quite standard that you have
this this processor on mobile devices
that has a dedicated core for typically
called the sensing core for continuous
sensing right you wire your sensors to
those processors to those micro or low
power microcontrollers and then do
low-level processing there before you
wake up they the more powerful processor
here's a simple example that shows the
the an energy benefit right just for a
continuous activity tracking right those
are the accelerometer readings on the
top and this is a person who was sitting
in the office left office and and and
drive the car actually there was taking
a bus and take a get off the bus walk to
his apartment and sit down again so
there's about 15 minutes of the activity
collected using accelerometers and we're
looking at depending on the the how
often the system wake up and collect the
data if we run everything on the
microcontroller this bare minimum our
part we're talking about point2 long
average point to millions of power
consumption versus if you run it on the
typical phone you get 700 million power
consumption just big difference but
there are some interesting tree tops
depending on half and you wake up you
can you can choose to do the cycles
that's just just one example right there
since then people many groups are
looking into what's the right
abstractions we provide for
heterogeneous multiprocessors from the
very extreme and say don't do any
intelligent things just buffer and
periodically wake up and to a slightly
fancier and say we're going to put some
popular scenarios heart
wired in the low-power Michael core and
those are the high level event could be
activity detection could be geofencing
could be something that people believe
ahead of time that's useful and the main
processor can program when to wake up
from those fixed events to us more
flexible cases where you have a fixed
library in there and have some way of
warming them up probably using pops up
kind of logic and say if there's nobody
interested in activity don't even bother
wake up the main processor to I think
Lynn Jones group in rice did the share
memory abstraction that you can prove
you want to provide end programmers the
same abstraction that you had before but
inta more intelligent map different
things into either the high power low
power course we did some study to try to
characterize the benefit of being able
to reprogram a map things onto the low
power core and we use this notion cut
energy proportionality factor where you
can you can assume that there's a sort
of ideal execution that that takes zero
energy for this processor to wake up and
do something and then go back to sleep
versus in reality you have to spend some
time to wake the processing a processor
up do the execution and and a tree and
also spend some psychos to store the
state and go back to sleep so all those
overhead are waking up and going back to
sleep become the inefficient and then
non-proportional part of the energy
consumption so that's one of the traffic
the metric you can use to talk about
different types of processors or a
processor cores in terms of their energy
efficiency the second would be the
slowdown factor if you have different
families of these processors depending
on how complicated it is for more
complex task it may actually spend less
energy executing it because it's it's
it's theirs it's designed more powerful
to process them versus the
the low-power ones maybe just so slow
even though you're low power but you
have to spend more energy to run it so
there will be typically a break-even
point when you choose to be one side or
the other side of the curve we did some
simple profiling and this is for very
simple arithmetics and just you know
simple additions and multiplications
divisions and so on this is between a
msp430 family oppressor and the arm
seven core and and we showed that for
most of the stuff that if you're just
buffering and doing simple processing
processing actually more energy
efficient to run on the the little
processor this is to give like enough
buffer between the burst of the
execution so to capture the wake up in
sleep energy as well but once you get to
the more complex tasks a typical example
is FFT a reasonable size F F fitty it
become obvious that it's actually
cheaper to wake up the main processor
and do it that if you think about a
specific task in this case a continuous
sound sensing task that you do the
microphone sensing and do FFT and do
malfunction classify a coefficient
extraction and then classification to
detect whether you're talking to someone
somebody you know or not the main
benefit actually come from just
buffering the data and differentiate
whether it's is silence versus there's
some noise in the in the in the ambient
and then we have the main processor of
course by doing more intelligent
partition you're going to be more and
more energy efficient but at some point
it becomes diminishing return you
basically don't get anything back so
this simple example actually indicate
that just are floating very simple stuff
into the low power microcontroller
actually brings you the most benefit so
the more advanced stuff of dynamic
wiring
of share memory and those stuff probably
not going to bring you much more than
the the simple approaches going a little
bit up the stack we've worked on
something in terms of the semantic
optimization if we talk about all these
contacts are these applications that
want to use inference from the engine
and all these inference have some cost
including the cost of invoking certain
sensors to to get the data one of them
one on one way of abstract the
heterogeneity in terms of sensing itself
is to provide a very high level API to
say to get the app users to talk at very
high level semantics rather than to go
directly to the sensors for example
let's say we have a bunch of inference
algorithms that can infer whether at
home or in a meeting or driving and
running and so forth these are this
become the components in the system if
one app wants to get whether you're
driving and the information knows
nothing about it it would just invoke
the raw sensor detect whether you're
driving say using accelerometers and
answer that but the system now has some
notion of that you're driving and within
a short period of time it will persist
that information if another app asks
similar information it can answer it
directly if another app asks for weather
driving that information was cash there
and can return immediately the system
also can maintain accurate through data
mining of these context certain
correlations right if you're driving
you're not at home or if a running
you're not in a meeting right all these
conflicts context if another app that
asks whether at home because we know
you're driving and because we know this
rule now we can answer that immediately
say you're not at home if a fourth
fourth app asks something that whether
in a meeting which we don't have that
information now we have a choice we
actually can do just like speculative
execution we can do speculative sensing
if sensing certain things is cheaper
than others if we sort of anticipate or
even optimistically think that somebody
may ask what
at home we could be sensing whether
you're running and and that has a high
chance of telling us whether you're at
home or not so if the fourth app happen
to ask whether at home and we happen to
have that information that you're
running available we can answer that
with some fidelity so that can be built
into the inference platforms leveraging
the semantic level of contacts to do
further optimization now I want to end
us this part with this slide that things
are not as rosy once we have a network
of heterogeneous devices now there are
multiple devices that can sense whether
i'm at home right I could have a PIR
sensor on the wall I could have my
living room connect that have webcams
and my phone location and so on so forth
if some some app wants to ask whether
i'm home and multiple choices of using
one of these sensors and some maybe
depending on the context more energy
efficient or accessible some may not and
to understand how to prioritize these
these sensors they cost between sensing
and communicating and speckle even
speculative use these devices become
very very challenging and interesting as
well as the state sharing problem if you
want to scale this the story of the the
mining the context and the mining the
ruse among them and now we're talking
about sharing some state optimistically
or intentionally between a multiple
devices finally we're the app run if
somebody asks this send this curry of
whether the user at home and you have
all these devices or these resources
which one to pick to run those is it's
interesting and at depending on the
energy sources and the information they
can collect and how they how they can
communicate to each other whether you
have the cloud to backup and so on the
decision may not be a very trivial so
I think it's moving forward there's a
lot of complex complexity in terms of
how to deal with a heterogeneous system
to try to hide those complexities to the
end programmers so they can talk at a
very high level abstraction but also
optimize the execution the street
sharing and the communication in the
system to achieve the end-to-end or raw
system efficiency so I'll end from there
and there will be two more talks I think
it's related to this particular topic
did you do this okay our next speaker is
Roz buttock Roz's from UC Berkeley and
today he's going to give us a give a
talk on how to synthesize programs for
low power reconfigurable architectures
so Russ
so bows down to designing programming
models and compilers for these
architectures so this is work done by
manko on a particular compiler called
chlorophyll and you will see more from
the project because she just started and
she's great she'll have more interesting
results coming so general-purpose
processors are easy to program but they
are also power-hungry if you want to
make them smaller and more
energy-efficient what do you do you lose
a lot of the stuff that you put in them
for programmability like you may lose
the cash you may have much less memory
you may have a narrow bit width in your
words to be more energy efficient and
programming becomes harder then you may
need to do spatial and temporal
partitioning of your programs if that
CPU is a many core ok and especially the
cores are tiny the partitioning may
become really hard the interconnect
between these cores may be limited you
may not be just able to send data from
any core to any corner ok so things even
harder and if you add to it an unusual I
say for example a stack architecture
rather than a register architecture then
your head will blow out ok so of course
the solution that has been time-tested
is to create a good programming model in
which you can write the programs for
this architecture and that will isolate
the programmer from the idiosyncrasies
of the low-power harder of course this
doesn't end here because even though the
programmer is isolated there is a person
who needs to deal with this and that's
the compiler writer the person who will
translate the good programming model in
whatever language you want to embed it
into the particular target architecture
and so there is somebody having to deal
with this and the thing is harder
because there will be many different
architectures with many different
features and in fact there will be many
different programming models so you
really have end-to-end problem where you
will need to write potentially many
different compilers and they'll describe
later in the talk we cannot quite use
what we have learned about building
compilers over the last 60 years
because the programming models are
somewhat different than what we have so
far we can essentially compile
sequential functional programs and data
parallel programs really well and also
we know how to support traditional
architectures register architectures
maybe data parallel anything else really
we don't have compiler recipes for so we
need a different way to build compilers
this project is our first modest step in
that direction of making it easier so we
think of it as a synthesis aided
compilers this is not the hardware
synthesis which essentially is a
compilation from high level description
to whatever RTL or gate level or the
target architecture for hardware people
synthesis in programming languages is
essentially a way of obtaining the
desired programs by searching spaces of
alternative programs looking for one
that is correct in the sense that it
needs the desired specification so how
does that help building a different kind
of compiler in a classical compiler the
way you do the compilation is that you
apply a sequence of heuristic
transformations each transformation
takes the program and the rewrites it a
little bit for example it does some
optimization maybe replaces an
instruction with a constant because it
can prove it some analysis legality
analysis that that instruction always
computes a constant and the actual
generation of efficient code is also
just the rewriting from some high-level
intermediate representation into the
lower level target representation of the
hardware and the sequence of these
transformation is ordered by some
heuristics and the result is as good as
the quality of the heuristics which
orders these transformations and also
the completeness of these
transformations if you don't have enough
free rights in your system to obtain
good code you're not going to obtain
good code because not all codes that you
may want will be obtainable through any
sequence of rewrites that you may have
no matter how good the order them and so
the building effort of these compilers
have been proven to be high because it
does take a decade perhaps to write
enough transformations and analysis for
something like Java to have a good
compiler
so what we try to do in the synthesis a
bit compiler is that we are not going to
be rewriting that program through a
sequence of transformation but instead
we will define a syntactic search space
which means really a syntactic template
of the desired program the template will
design a huge set of programs most of
them incorrect in the sense that they
don't do the same thing as the source
program but we will search in that space
efficiently ideally for a program that
does behave like the source code program
and it also ideally is fast and has a
low energy usage and all the other good
things so what do you need for that you
need to somehow define that search space
syntactically through a template or some
such thing you need an equivalence
checker to show that the program that
you found indeed behaves like the source
code and you need some abstract cost
function so that you can actually pick
one that is not only correct but also
fast perhaps fast in terms of speed or
energy or memory usage or whatever you
want to use and this has proven to be a
little bit easier to use because
equivalent checkers are somehow easier
to build I'll show you how and the
syntactic search space is easy
essentially it's aivan program that
looks like this perhaps instead of one
loop the stew nested loops representing
tiling over a matrix okay so the first
prototype we have is for this green
arrays ultra-low-power processor its
many core processor with about 144
course and what you see here is a demo
which we did it runs on a little lemon
bleach battery so it's not quite energy
harvesting but it's getting in that
direction okay what does the hardware do
in order to be so low power well it is a
stack based architecture so there are no
registers there are two stags the data
stag and a control stack and it's an
18-bit wyd word if that sounds strange
thing over the 16 bits plus to carry
bits you can of course do with them
whatever you want because they are all
software control there are hundred
forty-four course they're really small
each of them has about 300 bytes of
memory that's not kilobytes trilobites
there is no cash no shared memory at the
communication there is really no
interconnect you can write a word into
the latch of your neighbor so you have
four neighbors north south east west and
you need to block until the neighbor
reads from that latch and there are only
32 instructions so there is say a
multiplication but there isn't a
division you need to build it on top of
it that's partly for efficiency of
encoding so two problems you need to
deal with in this word is bit slicing so
if you have operations on 64 bits you
need to translate them into four or more
potentially depending on the algorithm
of those narrow bits and you need to
partition even functions you want to
break them into a pipeline of operations
ideally with just a few per core so this
is sort of what Doug talked about this
morning about coarse-grained
programmable architecture so what is
chloro fuel that's a compiler that comes
with its own special programming model
and it is flexible in that it allows the
programmers to decide how the operation
should be placed across that fabric and
we'll show you how to build a compiler
ideally in a relatively simple fashion
we built it over a years and it performs
quite well in terms of the quality of
the code and the results are that it is
within two acts of expert written code
and yeah it was easy to build okay so
here is the workflow we phrase the hole
compilation problem as a sequence of
synthesis problems you cannot take a
source code and directly translate it
into the target code in the sense that
you will be searching for a code in the
machine code of that many core
architecture checking that it is
equivalent to the source code that
problem is too hard because the target
program is relatively large and the
third space would be huge so you
decompose it into a sequence of steps
and in the first one you partition it
into you could say logical program so
you place parts of the program into
logical course then in the layout you
pin them to physical course where we
have this fabric
and we also do the routing so if those
cores want to communicate at that point
we actually decide how their
communication will happen through which
core other course the lines will go then
we actually separate the code into
single core pieces that's the separator
and obtain single core programs and then
we generate efficient assembly machine
code in the last step on a per core
basis and the first three I mean if this
one this one and that one uh synthesis
problems this one is actually a
traditional transformation in a rewrite
sense and this was the one that actually
we had to rewrite three times before it
was correct whereas the others one
relatively trivial to implement even
though they solve our problems such as
placing and partitioning and routing and
what you see on the right are the three
different of synthesis problems that
solve these individual pieces now tell
you about the first one in the last and
so that you get the flavor of how these
compilers will potentially be written in
the future so start with the programming
model how do programmers actually want
to program these architectures they want
to have some control over how the
algorithm is going to be partitioned a
thing of sort of floor planning of the
computation they say will have data here
and here they may know about where
certain computation should happen but
they don't want to control the rest and
definitely they don't want to generate
low-level communication code so if you
declare two variables three actually
including the result you may want to say
that I want a and B to be at core van
and the result the answer at code three
okay so this is how the variables are
placed now you need to also say where
the multiplication will happen so you
may want to indicate that it will happen
at core two and then the programming
model is going to generate the
communication automatically for you you
may want to do the same for arrays and
you may want to place the entire array
at the same core or you may wanna
partition it lengthwise and put this one
here and this one there or you actually
do the bit slicing where each word is
going to be placed into two different
cores and here are the upper bits I'm
here at the logo
it's okay now as it happens as I
mentioned the programmer doesn't
actually want to specify everything
maybe the program are only wanted to say
that the answer needs to be at core
three but the programmer doesn't care
where a and b will be inverted
multiplication will happen that should
actually happen in such a way that it
minimizes costs and other constraints so
you can think of it as the place of
these other variables is not determined
that's left up to the synthesizer to
decide so this is really the programming
model in which we program the machine
currently but you can think of inventing
other programming models and there will
be many to see you there needs to see
the domains to see the programmers that
you are working with ok so now we are
left with this programming model which
is partially annotated and we need to
come up with places the locations for
the other variables and that sort of the
role of the partitioner so how do you
write the partitioner in a very simple
fashion without taking too much time to
do it and that's now we are coming to
the question of how do you efficiently
build these compilers and so the goal is
to infer these partition types you can
think of these question marks as types
which determine various variables live
and where the computations happen and
you want to infer them in such a way
that you minimize the communication
costs the number of messages that you
sent across scores as opposed to inside
and you more and minimize some space
constraints because this course have
small capacity you cannot put more
variables into the course then is the
capacity of every core and that's small
300 words right bites and way we did it
was on top of rosette which is a
technique for building the synthesizer
essentially what magpul did the author
of the compiler is to write just the
type checker which is something that
assumes that the entire program is
annotated with these partition types
there are no holes and you go through a
program which is represented as a tree
essentially vogue bottom up and check
that the capacity of every core is not
exceeded you compute during the type
checking the cost of communication
whenever you send data across course and
that's your type checker now to obtain a
partitioner you simply assume that some
of these types are not known think of
them as symbolic variables are unknown
and the rosette tool will translate it
into automatically into a synthesizer
for you which will compute those places
for you to minimize communication costs
subject to the capacity constraints so
this is how the first part of the
compiler was built by writing a simple
traversal of the ASD I want too much
about the second problem except that
that happens to translate into different
set of constraints in the quadratic
assignment problem the code separator
was the one that is one piece that's
traditional and was most buggy so of
course we want to show how to do that in
a synthesis fashion as well and its
future work and cogeneration is done
through super optimization which is an
old technique where you are given one
specification of a problem and you are
looking for a really efficient ideally
the best possible implementation of that
function by searching through the space
of all possible programs okay until you
find one that is correct and is fastest
now as the correct means behaves the
same way as the specification okay so
we'll use that to replace this
traditional architecture so this is the
traditional compiler back-end where you
have an optimization or a sequence of
transformations each of them taking some
intermediate representation and lowering
it until you obtain the optimized
machine code so you replaced it with
super optimization by saying we are not
going to do any sequence of rewrites you
generate naive code in the target
architecture target machine language and
it looks something like this a control
flow graph and then you feed it through
the super optimizer which does the
search and generates the optimized code
and of course if the super optimizer is
done well than all these optimizations
that you have here are going to be
automatically discovered because in
principle we are searching the space of
all possible programs looking for one
that is equivalent so that sounds great
but probably too good to be true and the
reason is that the super optimized are
scaled to perhaps 10 20 50 instructions
right so you cannot quite give them
large enough function than say search
for an equivalent program because the
problem is too large so you somehow need
to decompose it okay so how do we
decompose the super optimization we'll
do it on a base
broad basis so this is basic blog one
piece of code without jumps in it it is
a sequence of instructions and we find a
block a segment of about sixteen
instructions this is something for which
our super optimizer scales reasonably
well and we'll try to find an equivalent
the best possible in fact sequence of
target instructions okay now naively you
would do it like this so this is the not
so great way of doing it you would
partition the basic block which is
relatively large into these segments
each of them would be translated through
the super optimizer and the result of
the super optimization would be put here
and you would stitch the result out okay
of course sometimes the surge is too
slow and things time out okay well maybe
it's not so bad you can take the
original sequence and just put it here
because it's equivalent to itself you
can use it in the output code a better
way of doing it is you do a sliding
window you try to find a better sequence
for the yellow segment if you find it
you use it if it times out you shrink it
and you try to run it on a smaller
sequence with will scale better and run
faster okay if you find a segment no
segments with lower costs what do you do
you don't copy the whole thing instead
you just move the sliding window you
copy this one instruction and now you do
it on the next instruction sort of
sequence next sliding window and that
turns out to do a bit better than the
original one so that's our way of
decomposing the super optimization so
let me walk you through a few
experiments at validate that you can
actually build compilers this way as the
first one tries to compare this compiler
with respect to sort of a typical
traditional compiler that does rewrites
and uses heuristics so we look at the
four phases of the compiler and find
sort of corresponding techniques and
evaluate them so we compare sliding
windows with fixed windows and the
partitioner in the first step is
compared to a greedy partitioner which
would be a typical approach of doing
this and what you see here is how the
performance of gets better as you start
applying synthesis rather than these
heuristic rewrites
I think overall maybe forty percent
benefit is obtained with that so not
only you can build faster the
performance also seems to be relatively
good the second hypothesis is how well
we can do compared to an expert so we
took md5 which is the largest believe it
or not program written for this
architecture because it is really hard
to write it uses about ten course it is
quite efficient because it uses
self-modifying code a circularbuffer and
other things and not all of them
supported by our compiler so we would
consider it to be a success if it can be
within 2x of the expert code that was
our goal it turns out that we are about
65 slower and maybe seventy percent less
energy efficient than the manually
written code which was acceptable but
now with a few small improvements I
think we have it in twenty or thirty
percent of the expert finally we look at
productivity before an intern to one
summer to implement one benchmark to
compare this architecture to msp430
which is one of the best
microcontrollers out there and then the
double compiler we were able to create
in two hours sort of three variants of
it running on two cores 4 cores van core
essentially exploring a space of
alternative algorithms so it seems that
it leads quickly to alternative
algorithm implementations and so the
summer is that it looks like you could
use program synthesis as the structuring
architecture for building new compilers
for unusual programming models going to
unusual targets and do efficient
compiler construction while still
getting good code on the output and what
I didn't say is that this
microcontroller of course it's a many
core has high bended it runs about 600
megahertz with more than 100 course and
it seems to be about 10 times more
energy efficient than msp430 so it could
potentially provide sort of the compute
power that you need on low power devices
such as helicopters and so on but we
didn't get as far as actually building
any controllers with it thank you the
first question that I had was is there a
compiler for that thing so you guys did
all this work is there is there like a
compiler that comes in the box with the
chip no there is there is no compiler
for this there's nothing you said well
there is an interpreter kind of thing a
virtual machine so the whole thing is
programmed in a tiny subset of fourth
the stack based language that you may
remember in fact the whole ship is
designed by a team around the person
check more who designed forth and so
this is the 32 instruction subset there
is a high-level fourth which they can
interpret for you but of course you
don't get the advantages of running fast
at low power so there are a few of those
benchmarks available there is a memory
controller which is part of this
interpreter but that's it so this is the
only compiler therefore we don't have
any other benchmark except for the few
manually tweak benchmarks and they have
written other questions okay I'm gonna
ask one more than so you omitted
hardware from this workflow you said
we're not going to be synthesizing
hardware but that's something else that
people synthesize so if you have this
kind of flow it seems like putting in
some parts of the hardware if they were
reconfigurable might give you more
flexibility in the stuff that you do in
the software actually so you might get
an easier to solve problem where you
could make up the kind of parts that you
need to do exactly what you need to do
or you might end up getting something
that's a lot more efficient or you guys
you guys of looking at anything any of
that we are looking at it we haven't
tried it and assume you wanna do static
reconfiguration of the hardware in the
sense that you are faced with the task
of which 32 instructions do we select
forever is a right it looks like those
guys had some intuition about which
story to instructions they need for the
program's
they typically right and there are some
idiosyncratic ones that if you know how
to use them you can get good code but
this is based purely on intuition rather
than some exploration so you could given
a workload a set of kernels look at what
should the instruction set architecture
be in order for you to express those
kernels efficiently on top of those
instructions so you will be synthesizing
sort of parts of the processor and
kernels on top of those picking ones
that sort of work together really well
you can think of it as now your baseline
is not the ISA but some Micro Micro is a
right and you are now synthesizing the
ISA on top of it so that is possible in
principle of course the third space is
now much larger because you are
searching the space of Isis as well as
searching the space of implementations
for them but perhaps doing one
instruction at the time to instruction
at the time seems plausible ok let's
thank rise again all right our last
speaker by the way thank you everyone
for sticking around as our last talk for
the day after this we are going to
congregate out by the door over there
where us and several other workshops are
going to get on a bus and that bus is
going to take us to a restaurant where
there is beer and snacks so everyone
should be enticed by that our next
speaker is Ben Lee he is a professor at
Duke University and Ben has been looking
at how we can make large data center
sized systems more power efficient and
more energy-efficient and today Ben is
going to be talking about how we can use
game theory to help make some decisions
in data centers that make those data
centers use energy more wisely so Ben if
you're ready
right thanks so much for the invitation
to speak I'm really excited to be here
this afternoon to talk to you about some
of the work we've been doing at Duke I'm
a computer architect by training and in
fact my advisors right there David
Brooks but this work really reflects the
the transitions we've made in
understanding how to deploy energy
efficient systems in data centers and
actually some of this work started when
I was a postdoc here at MSR looking at
energy efficient processes for data
centers and we looked at using mobile
processors in data centers to run the
Bing search engine and we found that yes
we could run the search engine on
processes originally designed for the
phone but we're going to run into
difficulty when we try to convince the
data center operators to adopt and
deploy a mix of these high performance
and low power cores and that really led
us to some directions that we're
pursuing now in terms of managing the
risk associated with deploying many
energy efficient components into into
data centers so the work is economic
mechanisms and for managing risk in data
centers and really the goal here is
energy efficiency and we have two broad
strategies as system architects that we
take for efficiency the first is
heterogeneity we've heard a lot about
heterogeneity a bit in jazz talk and a
bit throughout when we've talked about
specialization in this context we're
tailoring a hardware just software
capability and in doing so we're getting
much better energy efficiency the
problem here is that heterogeneity
complicates resource allocation and
scheduling how often do I give you a big
core how often do I give you a small
core mix of big and small cores should I
give you and this introduces risk into
the system this is why we very rarely
see data centers deploy many different
types of processes they often want to
keep the number of unique processor
types in the system very small a second
strategy use often see system
architecture architects take is sharing
sharing divides hardware over many
pieces of software tasks and in doing so
we amortize the fixed energy costs of a
platform over many more useful pieces of
work
this complicates a task placement and
colocation how often do we co-locate two
different workloads together is it a
safe colocation are we going to
sacrifice performance in the pursuit of
efficiency and again this introduces
risks so we have to prevalence
strategies when it comes to to
extracting efficiency from data centers
the question is how do we mitigate the
risks associated with these two
strategies I'm going to outline three
pieces of work that we've looked at in
this context at first is yes hydrogen
heterogeneity is efficient we found that
if you build heterogeneous data centers
by deploying a mix of server class and
mobile class hardware you will get
significant increases in energy
efficiency second we can mitigate the
risks of heterogeneity using market
mechanisms head which in this system
agents will bid for heterogeneous
hardware and the market will allocate
resources that will clear based on the
bids that it receives and it will clear
and allocate heterogeneous bundles back
to the users and these agents will hide
the complexity of the heterogeneity from
the human users in the system and then
finally we talked about sharing in game
they what happens when multiple agents
come together and share HM multi-process
or a large big memory server how do we
allocate and partition the resources in
that setting we show that you can use
agents to share the multiprocessors and
provide provable provable game theoretic
guarantees with respective fairness so I
want to give you an overview of the
first to quickly and then dig a little
bit deeper into the third piece which is
the most recent of the three so with
respect to heterogeneity we performed
two different studies the first study
was done in 2010 what we looked at using
atom processors for the bing search
engine and the second study was done in
2012 where we looked at mobile dram GM's
i go into your phones and deployed those
in data center for data center workloads
as well so first with respect to the
processors we found that there is a case
for processor heterogeneity when we ran
web search on bing web search on atom
processors we saw that the queries per
Joule went up by a factor of five this
is because we're using this
the datapath slower clock frequencies
smaller caches well so this is really
good why wouldn't we want heterogeneous
systems well we're going to need to
mitigate the effects on latency so this
figure shows the cumulative distribution
function for web search queries on a
normalized access and if you would run
on a high-performance Zeon's you're
going to get very tight latency
guarantees ninety-nine percent of your
queries will complete before execution
on this cutoff in contrast however if
you were to run on the atom processor
ten percent of the only ninety percent
of your queries hit this cutoff and ten
percent fail it turns out to be rather
problematic that these fail because
these are these going to be the more
complex queries if you have logical
conjunctions or disjunctions in your
query that's going to put you in the in
this region here if you if you require
language translation that's also going
to put you in this region here so you
can get the simple queries done well but
the hard queries will require the high
performance nodes so again we're going
to have the simple cores and we're going
to have a complex course and we have a
case for mixing these two in a system we
performed a similar study work for
memory systems for DRAM and this study
was done in collaboration with Christos
costarakis and and Mark Horowitz at
Stanford where we looked at d Rams and
looked at the energy per bit required to
transfer data to the processor and the
conventional technologies is this ddr3
technology here and ddr3 is notable and
that it gives us very high sustained
channel bandwidth if we take the high
performance DRAM interfaces we can go up
to twelve point eight gigabytes per
second for bandwidth but the problem is
that if your workload actually EULA
utilizes really little of this band with
the energy per bit cost will go up very
significantly this is the reason for
this is that ddr3 actually has very high
fixed power costs they have delay locked
loops they have on determination and
those expensive interface circuits would
give you the very will give you good
signal integrity at these high data
rates but because you can't turn those
circuits off if you transfer a very
little power you're going to run into
problems began
that fixed power cost is amortized over
very little useful work in contrast if
you were to take LP ddr2 this is the UM
interface that you see for mobile phones
you get very good energy proportionality
across the board you consume roughly 40
Pico joules per bid of course the catch
is you've eliminated all these expensive
interfaces and therefore can no longer
transfer at these higher rates so you're
max out at 6.4 gaben gigabytes per
second our observation was that LP ddr
works great for web search it works
great for memcache d and it works great
for these workloads because we're not
actually getting a lot of bandwidth
utilization in these scenarios for the
Bing search engine we found that less
than ten percent of peak bandwidth as it
was actually being used and as the same
was true for memcache d if you're doing
distributed memory caching your
bandwidth is going to be limited by your
by ethernet not by the memory channels
but if you're doing conventional high
performance database workloads HPC
workloads you're going to need that
bandwidth from DVR now that's a second
case for heterogeneity so now that we
know we want heterogeneity in data
centers the question is how do you
mitigate the performance effects or the
performance risks arising from
heterogeneity and this is the result of
two pieces of work done by one of my PhD
students Marissa Bell guevara so we're
going to present a framework where
agents will bid for heterogeneous
hardware in a mechanism that maximizes
welfare we want this because today's
approach to mitigating data center
heterogeneity is is pretty broken and I
originally had a figure showing amazon
ec2 where Microsoft I'm gonna show
Microsoft Azure but in both cases what
we have our menus provided to human
users and effectively these humans will
have to go in and pick what sort of
machines they want what sort of virtual
machines they want and most often human
users are not capable of making the
right decisions they don't have enough
insight into the workloads that are
running to figure out the size and
capability of the machines they need
here we're talking about virtual
machines but physical machines could be
diverse as well for the reasons we've
just described and the
is that heterogeneity is exposed and not
only did are they asked to assess the
machine price or the machine type they
also has asked to assess the machine
price whether it's a good deal for the
performance of getting and that's pretty
hard for them to reason about as well so
we want to take a step away from this
current status quo which is that which
requires a user understand hardware
software interactions so how do we
mitigate this performance risk well
let's first define risk very simply as
the dictionary defines it risk is the
possibility that something bad will
happen and when we're talking about
heterogeneity we're talking about the
resources of risk what type of hardware
do you give to the user how many of each
type do you give to the user and what
allocations do you provide and how do
you share these resources all right and
we're going to we're going to mitigate
these sources of risk with a market
allocation mechanism and this mechanism
will ensure service quality it will hide
hardware complexity and heterogeneity
and it will trade off performance and
power which means that we're going to be
allocating low power cords or low power
memories whenever possible right okay so
we have this market mechanism and on the
left hand side we're going to have users
their users are going to specify some
level of activity they want they're
going to specify some value that they
derive if you satisfy their service
level agreement and on the right hand
side we're going to have our
heterogeneous processors and each of
these processes will be associated with
some cost this caused my include
amortized capital expenditure and it
might also include operating
expenditures so the user will specify
value for performance and and based on
this valuation function we have proxies
that generate bids automatically and
these bids will be issued into the
market based on these bids the market
will clear and the processes will be
distributed to these various uses the
key idea here is that the market will
shield the user from the heterogeneity
the user doesn't actually have to reason
about the processes in the system rather
they can just specify their performance
and the system will figure out what
processes are required to go back to the
each user to give you a little bit more
detail into proxy bidding I'm not going
to
going to this into a lot of detail but
I'm it suffice it to say we're requiring
very little information from the user
the proxy inputs there are two pieces of
information we need from the user the
first is what's under the man have you
have you requested overtime and the
second is what's sort of a service level
agreement do you want how much how much
are you willing to pay if your response
time is give is a particular value given
these two pieces of information the
proxy will perform analysis to predict
future demand predict latency as a
function of processor allocations and
then with these latency predictions
figure out the value derived based on
the service level agreement and with
these three pieces of analysis we're
able to generate automatically bids for
heterogeneous mixes of processors we've
demonstrated this for high performance
and low power processors and we've
explored the data center design space we
illustrate the design space using
ellipsis to represent hardware types and
points that represent combinations of
processor types so for example if you
have a homogeneous hi performance system
a data center with only high performance
course that would be your design point
here if you have a data center with only
low power cords this would be your
design point here and for a fixed power
budget you can have a variety of
heterogeneous configuration so we have a
mix of high performance and low power
cores and each of these points
correspond to some mix and based on that
makes you're going to get some quality
of service number here so here the
colors show the quality of service
violations and you want to be in these
dark regions where you have very few
quality of service violations the reason
why the high performance and low power
homogeneous configurations are
insufficient is that low power processes
alone aren't enough they don't give you
the latency guarantees that you need in
contrast the high-performance machines
aren't enough either because you simply
can't pack enough of these power
intensive processes into your data
center to handle peak loads somehow
you're going to need to pack more
processors into the system and you can
pack more processors get more throughput
by using low power cores
so there's some design space here and
you can pick the right mix of high
performance and low-power processes for
your particular application I'm not
going to generalize this to many more
processors except to say that we can we
can look at four processor types also
construct a Venn diagram here we have an
out-of-order 6 wide 2.4 gigahertz
processor 3 in order processor cores and
we can explore the design space and
figure out which of these heterogeneous
configurations actually gives us the
best quality of service I'll only say
that the best configurations here are in
sectors GL &amp;amp; E and these are
heterogeneous combinations and if we
were to pick the right one we could
reduce the number of quality of service
violations pretty significantly the
complexity of this Venn diagram is is
really a motivation for constructing a
new metric where you have designed for
manageability if we as architects want
to build new processors we've got to
anticipate whether or not these
processes can be manageable in larger
complex systems and that's something we
looked at in hpc 814 so I sort of
breezed over the first two projects
which is really motivating the case for
heterogeneity and then talking about the
how markets can help us mitigate the
risk of that heterogeneity I want to
close by a detailing some recent work
that I'm really excited about when we
talk about sharing in game theory so in
this setting agents will share
multiprocessors and they will get game
theoretic guarantees on fairness and
this is something we published just a
few months ago so the case for sharing
is energy efficiency as well we have big
service today and these servers are
typically underutilized we're going to
have sharing as i said earlier because
we want to amortize the fixed power
costs of this machine over more work in
this particular case we have a six core
processor and there are at least two
types of resources being shared we have
a shared l3 cache and we also have the
shared memory controller which gives us
off chip memory bandwidth all right so
in both of these cases we want to be
able to share these multiple resources
effectively between multiple you
users heterogeneous users that tasks are
diverse and users are complementary so
sometimes if one uses memory intensive
another's compute intensive we can
exploit that as well we should exploit
that as as well and finally we have two
objectives the first it allocate
multiple resources the second is to
guarantee fairness so how do we do this
in the game theoretic setting let's look
at an example suppose allison baba
working on research papers and each of
them have ten thousand dollars to buy
machines Alice and Bob have different
types of tasks and they might have
different paper deadlines so they're
amenable fish into sharing now suppose
that Alice and Bob are strategic right
and because their strategic they're
going to ask a couple of different
questions the first question is is it
better for me to buy my own small
separate cluster or would I rather wish
pool my ten thousand dollars with Bob
and by largest shared cluster and
suppose Alice and Bob answered this
question and they decide to share then
they're going to ask are the allocations
in this shared system fair and if i were
to lie about what i wanted could i
actually benefit and get more than my
fair share of this of this allocation so
so these are the questions that we're
going to get once we have strategic
users in the system this is actually a
little bit different than the
conventional wisdom in computer
architecture conventional wisdom in
computer architecture says that first
users must share and all that's left to
do is mitigate the penalties associated
with sharing this approach overlooks
strategic behavior because if alice and
bob are looking at your allocation
mechanism and they don't like the
allocations that they see they're going
to say i'm not going to participate in
that mechanism i'm going to go off and
buy my own machines so you don't even
have that problem of performance
mitigate pen penalty mitigation if your
incentive structures aren't on properly
provided in prior work fairness policies
equals slow down that is if two users
share the same framework or share the
same system and if both of them see a
ten percent penalty associated with
sharing probably work will say that is
fair the problem is that equal slowdown
fails to encourage envious that users to
share if in this setting if you
equal slowdown mechanism it's very
possible that Alice will envy Bob's
allocation and Bob will and V Alice's
allocation in that setting you're going
to lose your incentive to share and
finally prior work focuses on heuristic
mechanisms to enforce equal slow down
whereas we have a mechanism that can
provide proof of provable guarantees so
we rethink fairness in this setting and
we draw on a game theory and the
definition economic game theory says
that if an allocation is both equitable
and creative efficient that allocation
will be considered to be fair Hal Varian
defined this notion of fairness he was
actually an economist professor at
Berkeley for many years and it just so
happens that he's now on chief economist
at Google and and this is the classical
definition of fairness from the game
theoretic perspective so what does this
mean for us well based on this
definition we came up with an allocation
mechanism which we call Ref resource
elasticity fairness ref is an allocation
mechanism that provides game theoretic
desert errata 4shared chip
multiprocessors so first of all we want
sharing incentives we want users to
perform no worse than if they had equal
division under those resources of those
resources we don't have sharing
incentives the users will not
participate in your system rather rather
they will go out and buy their own
smaller machines and you don't have a
cloud essentially you don't have a data
center anymore second we want NV
freeness NV farina says that no user
will envy another allocation that means
Alice will look will prefer her own
allocation to Bob's and Bob offer for
his own allocation to Alice's third we
want Pareto efficiency subject to those
fairness constraints so given env
freeness we also want to do the best we
can with respective performance and that
says a no other allocation will improve
utility without harming anyone else
finally we want strategy proved pneus
which means that no user will benefit
from lying or misrepresenting what they
what they can get from their allocation
the performance they get from their
allocation so let's walk through how we
can actually come up with this mechanism
breath
ref is premise is based on cobb-douglas
utility functions and all the guarantees
require the performance to fit this this
model here we have utility for example
performance instructions per second
perhaps and the ex is here our
allocations for particular resources
like cash or memory bandwidth and the
alphas are fitted parameters which are
referred to it as elasticity for
resource are informally you can imagine
the alphas simply being the sensitivity
of how much our user benefits from a
particular resource when it comes to
improving utility so cobb-douglas
actually fits her preferences in
computer architecture pretty well the
exponents will model diminishing
marginal returns which we know exists my
am loss law is a form of diminishing
marginal returns increasing the cache
size beyond what is useful based on an
applications locality that also reflects
diminishing marginal returns so that
office can be fit to capture those
effects and then the products the
product here model substitution effects
so if I are there scenarios where I can
give you more cash to compensate you for
less memory bandwidth and vice versa we
actually get those effects with the
multiplication of these exponentiated
allocations and the reason and you can
see that by taking the partial
derivative of U with respect to X and
seeing that it's a function about other
allocation parameters so we have
cobb-douglas utility functions and
suppose that we have two users with
these particular utilities these are
actually actual utilities that we
derived for parsec benchmarks running in
a shared chip multiprocessor and user
one prefers resource x1 and user to
prefers resource y 2 in this case the
the use the utilities are going to be
performance the x's are going to be
allocated memory bandwidth and the Y's
are going to be allocated cache size so
we have these two users and we now want
to distribute a shared chip
multiprocessors resources between them
right so we have two users 12 megabytes
of cash last level cache and twenty four
gigabytes per second of off chip memory
bandwidth this figure illustrates all
possible allocations so in this
particular exam
Apple user one receives eight megabytes
of the of the cash and six gigabytes per
second of bandwidth and user to receives
the remainder right so that so here
we're partitioning the resources in a
very particular way any point in this
space corresponds to an allocation so
how do we find it find the fair
allocations in this setting well first
of all we want to find env3 allocations
and again env3 allocations are such that
user one prefers her allocation to use a
tues allocation and if you were to ask
you the one which allocations can I give
you such that you experience no envy
user one will respond give me and give
me any allocation in the blue region and
I will not be envious likewise user to
will check this inequality and say if
you give me any allocation in the red
region I will not be envious so the
together these constraints tell us where
the allocations will be where the envy
Envy free allocations will be secondly
we want to find Pareto efficient
allocations and again these are
allocations such that we can't improve
anyone's utility without harming anyone
else and the blue blue curve here shows
the space of envy for the set of envy
three alek sorry the set of Pareto
efficient allocations and I'm not going
to talk about how we get that but that
relates to indifference curves and
marginal rates of substitution so but
we're going to get the setup fredo
efficient allocation is shown here so
recall that fairness is envied freeness
plus Pareto efficiency so we can
superimpose all of these constraints and
we'll get our set of fair allocations in
the middle these fair allocations are
going to be envied free for user one and
be free for user too and also Pareto
efficient it turns out that a couple of
different fair allocations that reside
in this in this space so the question
now becomes how do you find these fair
allocations what is the mechanism that
we have of finding these fair
allocations and that now I'll close with
the the algorithm that we use the first
step is to is to profile preferences the
second is to fit the utility function
after we fit the utility function we
normalize them in some way
and then we allocate in proportion to
what we fit and if we do all four of
these steps so we're going to get our
four game theoretic guarantees that we
want and I'll refer you we actually have
proofs in the paper and I'll refer you
to the paper for proofs of each of these
properties for filing preferences how do
we know what each user prefers how much
cash these are prefers how much memory
bandwidth the user prefers well we can
do offline profiling maybe with
synthetic benchmarks maybe we have
ballooning in the application it with
the benchmarks we can look at offline
simulations we can try to simulate the
application running under different
hardware settings or we could try to do
machine learning where we initialize the
elasticity and then update them as we
get more information once we have this
data once we have the performance for
variety of allocations we can perform a
linear regression to find the alphas and
the way we find those offices we fit to
a linear regression by taking a log
transformation once we have the log
transformation that offers are very easy
to find there's just classical linear
regression and this is just simply to
say that for a variety of cash architect
cash capacities and provided memory
bandwidth allocations weibull to fit
using cobb-douglas utility functions IPC
very accurately so we're able to fit
those performance trends according
according to the measurements we've
received once we fit the utility
function perhaps we've got a function
like this we're going to we're going to
normalize it so that the elasticity sum
to 1 this normalizing process allows us
to compare the elasticity that we fitted
for two different users all right so so
this allows us to put all these alphas
on this to the same scale so that we can
now compare two different users and
their elasticity and then finally the
last step is actually pretty simple once
we've normalized the elasticity is we
simply allocate in proportion to them
right so so here we have user 1 user 2
again and suppose that we wanted to
allocate resources x1 well X 1 user
one's elasticity is point 6 user 2's
elasticity is point two and we simply
allocate in proportion to them so user
one will get to be cool
hours of the 24 gigabytes per second of
bandwidth user too will get the
remaining one quarter of that memory
capacity memory bandwidth and you can do
exactly the same thing for cash too
pasty and it turns out that if you
follow this algorithm and you
implemented this this allocation policy
you get the four properties that we
described now how does this perform
against a mechanism where you have
equals slow down right what happens if
you wanted to implement this algorithm
where you have what you want to
guarantee the same performance penalty
for two different users it turns out
that equal slowdown doesn't give you
sharing incentives nor does it give you
envy freeness and we can see this by
looking at two applications this is kiel
and barnes conil is memory intensive
workload and barnes is not and if you
wanted to equalize the penalties
allocate the resources to them that they
both see the same performance penalties
you would come up with this allocation
here where conil gets less than half of
the cash and it also gets less than half
of the memory bandwidth and in this
setting you lose sharing incentives
because conil will look at its
allocation look at the policy and say
why am I participating in this mechanism
I'll go off and buy my own machine is
going to be half the size of the shared
machine but at least i'm going to get my
my my guaranteed allocation so we don't
have sharing it set of incentives in
this scenario and conil will envy
Barnes's allocation in contrast if we
implemented resource elasticity fairness
ref ref revised both sharing incentives
and envy farinas and in this particular
case it does it by providing more cash
to conil which compensates it for a very
small bandwidth allocation so we're
accounting for substitution effects here
and this allows us to partition the
resources in such a way that you
compensate conil for less of one
resource by giving it more of another
resource and this gives us both sharing
incentives and envy freeness last i'll
comment on performance performance what
sort of performance penalties do we get
for fair allocation for variety of
workloads which i will point to the
paper for hours the point of this figure
is that there's really little different
in weighted system throughput if you
would try to maximize welfare without
fairness implement equal slow down
without fairness or implement our
mechanisms all of them perform roughly
the same when you're calculating
weighted instruction throughput in the
worst case we're going to get a ten
percent performance penalty but we're
providing these very strong game
theoretic guarantees without any
appreciable our performance penalty so
in summary I talked about the economics
of managing risk in data centers we have
two main sources of risk both of them
are rising from our strategies to pursue
energy efficiency heterogeneity and
sharing we talked about how
heterogeneity can significantly improve
energy efficiency by a factor of five
for both processors and memory but we're
going to need mechanisms that mitigate
that risk from heterogeneity we probably
proposed a market mechanism that allows
agents a bid for heterogeneous hardware
this gives you performance we also
provide a game theoretic framework that
allows allocation for shared resources
and this gives you guaranteed fairness
I'll say that we've only just started
this work and we have a mechanism for
for performance we have a mechanism for
fairness and we're now trying to figure
out what the trade else are between the
two and whether or not there are
interesting and other interesting
concepts that we can draw and to make
strong statements about welfare and
shared systems so that's all I have
thank you very much for your time and
attention I'd be happy to take any
questions you have
I always envied farina's defined for the
kids been here more than two years so so
envy phoenix is defined as I if you're
Alice and you have Bob and Charlie right
so Alice should not envy Bob no she she
and be Charlie but she will probably
only then the two other resources are
put together yes that's that's true and
this raises other interesting questions
in with respect to game theory because
suppose that Bob and Charlie are
actually the same person with two
different accounts then potentially they
could try to gain the system and and try
to get more so that's actually something
we're thinking about it that's some
false name proof pneus I think it's the
concept and we're trying to figure out
how to guard against that yeah that's a
great question very nice work by the way
thank you what's that speaker again
thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>