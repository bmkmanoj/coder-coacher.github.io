<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bandit Learning with Switching Costs | Coder Coacher - Coaching Coders</title><meta content="Bandit Learning with Switching Costs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bandit Learning with Switching Costs</b></h2><h5 class="post__date">2016-06-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0tFELG60BaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright I guess we could start so let me
thank to you all too for finding time to
stop by and giving the talk so it's much
appreciated I guess it's a very old
subject but still with new and
interesting results so bandits with
switching costs space good afternoon
pleasure to be here so so this will be
the Bandit here referred to the
multi-armed bandit and the kind of a
machines and casinos where you pull you
pull an arm and hope to get a reward and
they differ from other formats of online
learning because of the type of feedback
that you get each time that is you find
out what happened from your action but
not what would have happened had you
done another action and this is joint
work with offer Decker from MSR who
introduced me to the subject age ending
now at the University of Chicago and
tomor Quran who was in intern with us
from the Technion so I know we have a
mix of some experts and some newer test
our resolve to you know go through the
some background getting the main point
so the we're going to discuss online
learning with K possible actions in the
it's often called also the learning from
experts problem where you have experts
proposing to you and several actions and
you have to choose each time what to do
so you have the player or the learner
and has to choose each time between K
actions mostly we'll think of care
that's more constant and focus on two
but everything applies when k is bit
larger as well so these are the actions
the learner can choose he can choose to
whether to buy stock a or buy stock be
or maybe he can choose
which route to drive or which route to
send his packet each time and a priori
he maybe doesn't know how well the
different options will perform but he
learns over time and this type of
problem is considered in kind of a
bayesian setting with some distribution
over the environment and in a worst-case
setting where the player wants to see
how he would how he does compared to
maybe sticking to one of the to the
advice of one of the experts or one of
the actions so the will consider an
adversary which is a priori could could
be in the worst case so so each time the
player chooses an action and receives a
reward or a loss so we'll think of these
as losses so in so the cost of doing the
action of course you can shift by
constant and think of this as a reward
so so each time you make such a choice
he gets rewarded this goes on over time
and well the problem is what what should
he do and we will see we see several
different settings depending on how
powerful the adversary are is and what
kind of feedback the player gets in each
round so in order to deal with a
worst-case adversary it turns out
important for the player to be able to
randomize because if he goes according
to some deterministic strategy then
there is a worst-case adversary who
would exploit the strategy and lead to
you know bad losses so so there's
different types of adversaries there's
adaptive adversary's which take the
players previous actions into account
and there's oblivious adversary's that
set the loss values
in advance or in any case independent of
the actions of the player so if we think
of a small investor in stocks then he
might reasonably think that his own
actions are not affecting the prices but
you know a major investor his actions
does do influence the prices okay and
the basic feedback models to most basic
ones are well the Bandit Fedak model
I'll talk about today where the player
only sees the loss resulting from his
choice he doesn't know what would be the
lost happy than something else but
there's also the full feedback model
which is sometimes reasonable where he
says he knows after the fact what would
have happened had he made another choice
him so so there are many examples of
this type of problem so one which offer
particularly likes and is featured here
is relevant to in display of news
articles in automated newsfeed and one
has to choose which ones to display at
every time to maximize the clicks and
another one is that I mentioned is
obviously the choice of stock
investments so the setting will be a tea
round Thea round game between a player
that can randomize and here we can think
of a deterministic but possibly adaptive
adversary and also randomize the
adversary the so the player has one of K
actions think of K equals 2 for most of
the discussion and before the game the
adversary chooses a sequence of loss
functions and and in the game for every
time the player chooses a distribution
over actions and draws one from that and
he suffers and observe also his own loss
which is
the function determined initially of his
actions now in the most classical
setting his loss is just the function of
his current action but we're going to
consider also a function of a past
actions as well now if the situation is
full feedback when the player observes
at every time what would be his love his
loss if not just for his own current
action but with different actions
currently combined with his past actions
in the past so again the classical case
is when ft don't depend on X 1 to X t
minus 1 but just on the current action
and then it's just a you observe all the
possible losses in this stage so
obviously adaptive is much more
dangerous classes of adversary's and so
the players cumulative loss is what he
wants to minimize but is going to
compare it to a benchmark so so the
players regret well one kind of
non-trivial aspect when people enter
this field is what's the right benchmark
because you can think well maybe the
right benchmark is the optimal choice of
accident every step but that's kind of
an impossible benchmark so more
realistic and still challenging
benchmark is the best constant action so
so as so think of these a not
necessarily just as different stock to
pick every day but also maybe it could
be a collection of ack experts each one
advises you what to pick every day so i
knew each time you go with one of the
experts and follow his advice and there
are a bunch of experts and what do you
want but before the game you don't know
which one really knows what he's talking
about which is a real expert in which is
a fake expert so what you want to do is
to do as well or almost as well as the
best expert so so this is as
but so this means almost as well as the
consistent choice of the same action
every every step so this is this
definition of regret so a so we compare
the e by the cumulative loss to the
minimum over all choices of actions of
just repeatedly using this action all
the time so since this is Los the best
is the minimum and what are called no
regret or should be called you know sub
linear regret algorithms are a
algorithms that assure that the regret
is negligible asymptotically compared to
time so this means that the player must
be getting better over time and really
learning from his experience because you
know initially when he knows nothing he
will say in this setup get kind of a
constant order one regret every step if
he can eventually get it to be
subliminal that's a sign of learning but
then there are refinements of this so it
turns out that in you can ever get
better than all the square root of T so
this is a this is kind of the best a
regret we would want to achieve and
that's easy to see i'll come back to
this point just if the environment is
just independent and random you're going
to incur this loss of routine no matter
what you do let regret of rooty is
pretty good in tif steps the other
extreme is when you can't get sub linear
regret so the regret is all dirty and
then the problem is unlearn nabol and
this is here i'm so r of t was defined
for a particular algorithm our star of t
is the best overall algorithm or
randomized two-player strategies so
there's right so it's info for player
strategies and then soup over adversary
loss sequences of the resulting expected
so one thing which is well developed our
algorithms that achieve a good regret
bounds and there are several follow the
leader follow the perturbed leader and
maybe the most famous is the
multiplicative white algorithm which is
very simple and there is prehistory
going back before these papers but the
the basic idea of the algorithm is we
each time we see the losses of the
different actions and we we wait them by
him so there according to their loss so
so we punish an option that was the
Qaeda big loss by a factory to the minus
gamma times that loss so a good case to
think about is when the losses are just
zeros and ones that's already a rich
case just pure 01 in that case what do
you so every option either you lost
nothing or you lost the unit and in that
case what the algorithm says well if an
action lost you unit then or if an
expert lost unit then you give that
expert a lower weight by a factor e to
the minus gamma and in general if the
minus gamma times the loss and so each
time you adapt the weights this you
adjust the weights this way so at time
and after you know t minus 1 steps every
expert has a weight which is
proportional to this expression right
see you some the losses of that expert x
minus gamma exponentiate so this give
you wait you normalize them to a
probability distribution and you choose
your next action according to this
normalized probability so mutti is just
the normalization of this of this
expression and you can calculate or you
can estimate what is the regret of such
an algorithm and it turns out that if
you know the horizon is capital T the
optimal gamma is of order 1 over root
tea and it yields a regret of order
p log k now notice to implement this
algorithm we need to be in the full
feedback model because i have to know
for each expert or for each action what
is the cumulative losses until now in
order to calculate this normalizing in
order to calculate these factors so in
the full feedback one that i can
calculate this and then it turns out
this classical without the regret scales
like root of the time and there is a
dependent slow growth with dependence on
the number of actions okay now perhaps
perhaps surprisingly in the case of a
small number of actions you can do as
well with bandit feedback and I still
find this surprising this is it was
found more than ten years ago what you
do is you apply the same in the same
multiplicative way its algorithm with
the actual losses replaced by an
unbiased estimator so every time aim so
every time you instead of using the lost
time T of action I which you know only
for the action you took what you do is
for the action you took you take the
loss and you divided by the probability
of taking it so this is a number you
know and force the probability you know
because that's been calculated from the
previous step and all the actions you
don't talk take I'm sorry all the
actions you haven't taken you just give
them wait wait 0 so this LT hat is
replacing LT and you can see if you take
expectation of this with respect to new
t you get back to the vector LT that we
were looking at before so this is an
unbiased estimator of the true loss
vector but it looks a little strange
right since we were supposed to take the
vector of the losses and we're taking a
vector which is just as a single nonzero
entry which is rather larger than the
loss because we're dividing by a
probability that could be small and all
the other entries are 0
and kind of amazingly just plugging in
this and running through the proof it
works so so as I said the proof of this
is not hard but I still find it
surprising yes the scaling is tight so
so that's a change from so when you have
more and more actions then there is a
cost to the fact that you're not seeing
the different day wait so this is tight
and right but I'm going to focus on you
know small number of actions where the
difference between the root K and the
root loci is not important but when you
have many actions yes this is the this
is important so okay let me I'm going to
maybe skip some of this and go to the
issue of switching costs so this has
been a optical as people have noticed
for many years in this topic which is we
didn't distinguish the cost for the
player of playing the same action as
yesterday and switching actions in many
applications is these are really
fundamentally different for instance in
stock if you have to switch the stock
you're holding there's often a
transaction costs in various robotics
application if your robot has several
actions that he can be doing if you have
to reconfigure it there's a cost and
also in these news article applications
there is a cost to switching switching
the action nevertheless a colliding Adam
calyin Santos went up when Paula in 2005
shows that that an algorithm in the full
information feedback one can still have
a regret of order root t even if you
have add to it the switching costs so
you have so the setting is we have a new
regret expression which adds a so it's
the regret from before the loss in a
time p a plus a new term
which depends on not your just your
current action but the last two actions
it's one if you switch to actions okay
so this is the new a form of the loss
and then note that this extra term is
not going to cost if you to our
benchmark so our benchmark was fixing
the best action and sticking with it the
whole time so then there is no switching
so this imposes a cost on switching
nevertheless they could show a regret Oh
fruity by a different kind of algorithm
called a follow the lazy leader of all
of the perturbed leader so it's very
natural thing to say well at every time
let me just choose the action that has
done best so far and but this is bad
against in the worst case adversary
because he would then know what you
would do and he would thwart you in the
next step but it turns out if you a each
time if you do a perturbation so you add
to the actual losses some control random
perturbation and after the perturbation
you choose the best action so far this
is a this turns out to be much harder to
thwart and not it works in the full
information setting even if you impose
switching costs it doesn't switch very
often and I actually talked to Santosh
when Paula and he said you know they
proved this for the case of full
information feedback and he said he's
had on his agenda to prove the same for
the Bandit feedback but you know it
hasn't worked out so far it looks very
complicated and we see a good reason for
that soon ok any questions
cause they seem to be uniform so you
just switch from any I to any other
right so you could have different so if
you write you could so one could also
vary the cost for switching and we have
some extensions for that but I want to
stick to the simpler setting okay so
m-maybe there's further feedback models
which have been considered but maybe
I'll skip those and so one can consider
a longer memory here we've considered
things of memory one in two and can
extend that but in the end memory that
just think of of two memory so that's so
the switching is an example of that so
so let's focus on the top of this
diagram so so for bandit bandit feedback
and oblivious adversary and we know so
doesn't without imposing switching costs
we know rooty regret but what happens
with switching so so let me go on to
explain sir our main result is that
we're switching the truth is T to the
two-thirds which is kind of a new
exponent in this subfield so there was
some previous work i'll come to buy
chesa bianchi decalin cameo but they
considered unbounded losses which really
kind of breaks the rules in this kind of
model okay so
see there is what I was hoping that
there's no white board to write anything
on here this is no no belief in their
old style neck okay so all right so okay
so what so let's start with 82 the
two-thirds upper bound when you have
switching costs which was well known
before and actually is an easy
consequence of previous results so T to
the two-thirds is a can be done just in
a black box fashion by blocking so what
so how does one see the upper bound you
have T rounds split them into blocks of
length B just equal size block length B
use the this expert three the
multiplicative way its algorithm to
choose actions for each block so we just
commit not to switch in the middle of a
block so we think of so really our DC we
make one decision per block so the
number of rounds effectively goes down
from T 2 T over B we only make one
choice per block and okay so let's let's
see what what this tells us I'm so they
what is the regret of such an algorithm
if we do the optimal choice or even just
the x3 choice but only after each block
so we have what do we know about the
number of switches maybe we switch after
each block but the worst it could get is
t / be the number of blocks now the
number of rounds we said was t over B
and so we get regret of square root of
that but we have to scale it by be
because each action now could cost me be
instead of one because the whole the
loss of a whole block could be as high
as be so we get this expression so we
write it as T over B plus root TB and
the block size is something we get to
choose so optimizing the Sun we choose
beta which is T to the one-third and
that will lead to t to the two-thirds
here
okay both terms equal and give these the
two thirds all right so so this looks
very simple minded when questions can
you do better question was can you do
better and in particular if for some
reason you could prove that the usual XP
algorithm without blocking doesn't
switch too often then you could improve
this T to the two-thirds because here we
kind of took a worst-case view of the
switches we had tea / b rounds and he
said well maybe we switch every round so
if you could say well the algorithm is
not that fickle then you could get an
improved bound it turns out it can be
fickle so but a consequence of our
result is that in some for some
adversaries in order to perform
optimally any algorithm will have to
switch very often okay so so the
question was what's the correct lower
bound is it routier or t to the
two-thirds and offer deckle had worked
on it for a long time and he arrived at
the conclusion that the more in delicate
stochastic process was needed because
prior in this theory the way people got
lower bounds was just picking things at
random and in some cases just doing a
random walk so let me go back to the
simple case without switching costs how
do you see the lower bound of square
root T so very simply both for the
online learning and for the Bandit what
you could do is just pick the losses iid
uniform for the toot and say we have two
actions for the tool for both actions
and then so at every step you really
don't benefit much from switching
because both actions will give you the
same distribution of loss in the next
round
and but the two actions if we look at
their cumulative loss will get a random
walk a sum of independent random
variables and we know some random walk
will have some mean and will fluctuate
around the mean with a deviation of root
T so one action could deviate that by
root T the other action might deviate
down by root T so the difference between
the bottom and the top and expectation
is order rooty and I have no way the
player has no way to avoid this kind of
regret so a whatever he does his his
algorithm will miss some you know they
he will there's one of the actions will
be taken less than T over 2 times and in
those T over 2 times maybe the action he
didn't take out performs the action he
did date and so that will cause a regret
of order routine but picking things
independently at random really in it
doesn't doesn't work as a lower bound
for the problem with switching costs
because the adversary you know in this
case can just stick if he sticks to one
action so doesn't switch at all even if
it's the worst actually the worst action
his regret is still routine someone
needs a different kind of process
so the first okay I'll explain this
anymore so an early idea that was
studied by deckle prism donkey and and
shamir was to make the losses themselves
a random walk not so before we said the
partial sums the losses were in the muck
the losses were iid so he thought well
if we make the losses themselves around
them walk then and we're in banded
feedback so what he thought is we make
the losses for the 1st armored random
walk for the second time it won't be an
independent random walk it will be
shifted by some epsilon ok up or down an
epsilon will be small it turns out the
optimal epsilon is T to the minus
one-third so you have a random walk and
you shifted by epsilon up up or down
what happens there is that you get no
information unless you switch about
which is the better arm because each
time to go from the loss at time T to
the loss at time T plus 1 what what
happens is an independent Gaussian say
if it's a guy steps are Gaussian
independent Gaussian is added and that's
true in both arms because both arms are
evolving the same way they're just
shifted globally by 1 epsilon up or down
so in order to get information on is
this epsilon up or down which is what
you need to know in order to find out
the better arm you really have to switch
so they did this analysis and it turned
out that the regret was indeed T to the
two-thirds the problem is that this
sequence of losses you get is unbounded
because if you're doing a random walk
with steps of order 1 then you know it
gets to be order rooty and the rules of
the game are the losses at every step
should be bounded otherwise you can
cheat so
so it turns out that this is because
somehow the random walk sequences is too
deep so and what we found is the right
approach is to do a kind of to build the
lost sequel since by a kind of
multiscale random or current to explain
next so the general blueprint will still
be as i said before so we'll have the
losses for one arm will be some
stochastic process i'll describe soon
and the other one will shift to be
shifted up or down by epsilon so
there'll be a fair coin toss if to shift
the other one up or down by epsilon and
epsilon the optimal epsilon will turn
out to be T to the minus one-third okay
but but they'll but the way the law
sequence will be constructed will assure
that it's bounded so in reality water
described here will be bounded by a log
T by but log is easily handled if you
just scale things down by log then you
get the lost it with two-thirds over a
log factor so one thing I've been
pushing under the rug is in fact we
still don't know the truth exactly we
know it up to a log factor so so the
kind of processes will be considering
have the form maybe it's in the previous
slide aim right so that's written here
so so for every time p we're going to
define some parent row of T so so the
arrow here points from the parent to the
node so here say they know the parent of
four is zero parent of two is also 0 and
and the loss at time T will be the loss
at the parent plus an independent
standard Gaussian and it turned so if
you if the parent function is just t
minus one you get a random walk and so
if each time you add two if LT was LT
minus one plus a Gaussian you'd get a
random walk
and promised this would grow too fast if
the parent function royalty was always
pointing back to zero then you would not
grow because you just get a sequence of
independent choices but it turns out
that then the the player can get too
much information without switching so
one has to balance these two things the
random walk was ideal in the sense of it
gave no information to the player
without switching but it was bad because
the losses increase too fast the
independent sequence had the other
shortcomings so here so this I will
explain this function this turns out to
be the right choice of the parent
function I'll explain what is in the
month so but just to get the setup row
of T equals 0 so each vertex if its
parent is all 0 then we just get the
independent sequence and rough t is t
minus 1 which is featured here just
gives us the random walk so so we can
define for every parent function its
depth and its width so the depth is you
know how many iterations you have to do
till you get to zero in the worst case
so here it will be or dirty that's too
deep and the depths means that the total
losses will grow out of bounds and the
width of the parent function represents
if I look at some intermediate point and
ask how many of these edges cross over
it and I also don't like large width so
I want the width also to be a polynomial
I'm sorry logarithmic not it not to be
linear or or polynomial so why is which
bad so the problem with with is if I
have a lot if I have some time you know
time 100 and have many arrows going over
100 what does that mean it means that if
i switch a time 100 there will be lots
of observations for me where I can
compare two variables
and their differences either a Gaussian
or a Gaussian with a perturbation of
epsilon and I can use all of these to
test whether the perturbation is up or
down in you know in this random walk
example if i switch here then you know i
can only compare the step 1 to the past
and one from the future and that's
really the only information i have due
to the switch so if you do if you have a
Gaussian and a Gaussian perturbed by
epsilon there's a you know the
information content of that is epsilon
squared so you'll need 1 over epsilon
squared such switches before you can
tell if the perturbation was up or down
but if the function is very wide like in
the ID case then by just switching you
get a lot more information so you can
kind of average the observations before
the switch and after and that will
really a zero down on the upturn you can
do that without many switches so what is
the function here row of tea so it's
written this way but it's better to
think of it in binary notation so given
every T write it in binary and look at
the least significant one in binary and
erase it that's row of tea ok so if T
and if T was was 6110 then row of T will
be 4 100 so just erase the least
significant one so in particular you see
that let's a go back all right so so
this is a part of this picture right so
all powers of to their parent is 0
because you raised there's only one
single bit which is one you erase that
you get back to zero so all powers of
2.20 but here six its parent was four
and so on this is the function and if
you think about this function it really
has both height and width logo which are
logarithmic so from every integer the
height is just how many times you can
erase ones until you get back to zero
well they're only you know logarithmic
number of digits
so obviously the height is logarithmic
and it's also not hard to see that the
width of this function above every point
is just it's just logarithmic and that's
going to be that turns out to be key so
the actual analysis is based on your
chain rule for relative entropy and pin
scores in quality relating total
variation to entropy and then see okay
so this is explaining how these two lost
sequences behave but what in the end we
get is a inequality in total variation
that says the difference between doing
the if I haven't done enough switches so
if I've done less than T to the
two-thirds switches then I can't a tell
between if the second arm was moved up
or down by epsilon that's that's the
conclusion of the technical analysis so
we really bound the Toleration distance
by the in terms of the square root of
the number of switches so and really
create this dichotomy so either you make
at least 3 to the 2 3rd switches if you
do then that's enough to tell if if you
switched up up or down by epsilon but
making so many switches already cost you
t to two thirds if you do less than T to
the two or three switches you don't have
enough information to distinguish which
if between the arms if the second arm
will switched up or down and so whatever
you do you have a constant probability
to just play too many steps on the wrong
arm and if you do every time you go to
the wrong arm its epsilon below the
other note it's crucial here that we
have banded feedback we never get to
examine that because if we got even one
time to examine the two possible losses
you see 01 is epsilon higher than the
other and the game is over but every
time we only see the loss from one of
the actions and so if we can't
distinguish the arms then we're going to
make a constant fraction of mistakes in
expectation and so will pay epsilon a
payment of epsilon order T times so that
will give us epsilon T and because they
said it will take epsilon T's minus
third will get a cost of t3 to thurs in
this case so you know damned if we do
and damned if or for the player if he
does many switches he will know the
truth but he's already paid too much for
this knowledge and if he doesn't do
enough switches he doesn't pay much but
his ignorance cost him ok so that same
self study to determine a gaussians
accuracy epsilon you need young one over
epsilon squared switches so and if it
has standard deviation Sigma you need
Sigma R epsilon squared switches so the
whole point is that this low height of
this process allows us to choose Sigma
which is essentially order one or like 1
over log T because after all if you want
the losses to remain bounded you have to
take into account that you're summing
summing gaussians here so you don't want
to sound too many but because here the
number of summands is at most
logarithmic then if you make the Sigma 1
over log T then you're still ok while in
the random water example in order to if
you wanted to keep the losses bounded
you'd have to make this thing but so
small that you did you know the number
of samples you need to differentiate
would not be so big
okay so aim so this repeat some of the
points I mentioned so the width is the
maximum size of any vertical cut in this
graph and switch contributes at most
width row samples where I can see a
variable which is a Gaussian plus or
minus epsilon so that's why I needed to
control the width and on the other hand
because we want the loss to remain
bounded we have to set Sigma which is
like 1 over the depth and we want the
depths to be small so that the Sigma is
not too small so so again repeating a
eat sweets gives at most Withrow samples
the loss is bounded in 01 so so they
will take the variance to be order 1
over depths and the conclusion is that
the number of switches needed to
determine the better action will be a
tee to the two-thirds over this this
product so if you make fewer switches
you just can't tell and of course if you
make so many switches you've already
paid this cost because the width and the
depth or logarithmic you pay essentially
to the two-thirds so this function has
both depth and width which are
logarithmic
ok so I'm so we now know that that
bandits and switching are hard in the
sense of this T to the two-thirds and so
one can reduce some other models to
these and any deduce it they are also
hard maybe I'll skip that let me just
say what is the dependence on K so if I
focus on the case of two arms or two
actions but if you have more actions we
can also analyze it and see that a the
dependence is now k to the one third on
the number of arms so similar ideas and
this paper will appear in stock and it's
available on the archive if you want to
see you know the actual details of the
proof and the dependence on the number
of arms and okay thank you for your
attention I see if I see both costs
every now and then then I should be able
to break the lower bound because I know
what it is for this for the particular
adversary we constructed if you see it
even once yeah you know it would be
different huh here this is time to I
mean for you or this strategy it's clear
it's sort of intuitive why but this is
it are there algorithms which could
actually take advantage of sort of a
small amount of extra information to do
that aim so you think suppose the
algorithm got to see in a few tah right
so the question is are are these times
known in advance also to the adversary
or not it would hide yes oh so that's
actually in an interesting variant there
are these times chosen at random or
chosen by the player right we could
think of both models so there may be the
player has a small budget where
of times where he can somehow pay off
someone and get information on the
losses of the other actions or it could
be that these are rare random events
certainly he sees a the results of the
other action so that's actually an
interesting variant we haven't thought
of that but it reminds me of this book
reviews in Amazon Canada you know the
story so you know people submit reviews
of books and the reviews are under
pseudonyms but Amazon insists these
people you know send their real names
and emails in order that are verified in
order for these reviews to be published
but then they published under the
pseudonym so nobody sees them however
there was some glitch in Amazon Canada a
couple of years ago where suddenly all
the real names appeared under reviews
and suddenly it became clear that many
of them exceedingly positive reviews of
books were by the author's themselves so
so yeah so in an unexpected glimpse into
information you are not supposed to have
ten can be quite the interesting yes if
your small questions are the use of
modesty other books so to prove a lower
bound in your case I have standard it is
like you're showing lower bounds in
general ah well it's only you know I
think it's a new idea ND setting this
type of random walks you know I've so my
coster John and I and many others have
studied them in probability for other
reason so let me give without proof just
another way one can obtain this type of
multiscale walk so start with the binary
tree and label it with Gaussian
variables and this corresponds to a
branching random walk so you think of a
particle that splits and each time the
you know the two children move
independently Gaussian and they split
and so on so these branching random
walks have been studied for decades and
you know a lot is known about them so
this creates this tree structure now
suppose you take this tree and you do a
depth-first search on the tree and each
time recording the sum of the variables
from the root to where you are so now
you'll get a sequence of variables and
it turns out this is exactly this
multiscale random walk in another
alternative description this is
something that has been looked at in you
know the probability literature
literature for other reasons we're
interested in how the branching random
walk behave so along every path of the
tree you just have a random walk so we
know how to be here but you have a big
collection of correlated random walks so
they're interesting to study have a
maxima and so on so this this is a
process we've studied a lot so kind of
it was something on our mind when we
heard the question you can speak to your
mouth full I'm so</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>