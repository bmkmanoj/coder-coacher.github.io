<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Median Hypothesis | Coder Coacher - Coaching Coders</title><meta content="The Median Hypothesis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Median Hypothesis</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dDcxBd7BD-s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm glad to have already our speaker
today Randy's a researcher in the
Michigan Department of a maxillary
search and that before Joe in the
department here was a prior research E&amp;amp;P
and he obtained his PhD from Hebrew
University in computer science Thank You
Danny so I'm going to be talking today
about the median hypothesis this is a
joint work with Chris purchase and to
begin with I'm kind of to lay the ground
I'm going to discuss the park beijing
point of view about learning and then
will you that in order to define the
median and actually will find out that
we need also to define a depth function
by which will define the median and once
we have it will be able to provide
generalization bound using for for the
median hypotheses are actually what we
are called eep eep offices so this would
be the first part of the talk and in the
second part will start discussing
algorithm so the first one would be how
do we measure the depth often apophysis
and and and then we'll discuss how do we
find that deepest hypothesis which is
actually the median so this is what we
are kind of the outline of the talk so
let's let's start by looking at the pike
pack of asian point of view on learning
so many learning algorithm look kind of
like that we have some sort of a scoring
function i will call it the energy
function and we give a score for every
hypothesis based on some regularization
term you know pick your favorite l2 l1
whatever and a loss function on some
some observed some observation that that
we had from from from park beijing point
of view we can look at it in the
following way is we can say that
actually the regularization term is
actually represent some prior belief
that we had
think about the regularization term as
the log likelihood of the prior belief
that that we had and then given the
observation we generate a posterior
belief and and this is what we you then
to select any pathogen and make a
prediction now no it's it's very
important to note that these are beliefs
so it doesn't mean that we assume that
something in the real word really behave
according to these probability measure
distribution or whatever you call them
this is just a representation of our
internal beliefs on one what's going on
out there so from from kind of the way I
would like you two to view it is that
the process of learning is made of three
stages we begin with some prior about
the world and then we see some evidence
so for example it could be labeled and
unlabeled examples from which we
generate a posterior and then once we
have this posterior belief we select any
purposes with which we'll go ahead and
make our forecasts and predictions and
actually my talk today is going to focus
on on this part once we have our poster
belief what apophysis should we choose
to use to make our predictions so let's
look at some common methods of selecting
the purposes once we have a posterior so
one option is to use the maximal
posterior or this is actually what SVM
and the suit does actually minimizing
the energy function is equivalent in in
my language to selecting the maximal
posterior another option is to use a
base function right which means that
every time I want to make a prediction I
will just hold the vote among all
hypotheses I'll do a weighted voting and
make my prediction I can use Gibbs
sampling which means just sample at
random one apophysis from according to
your belief and I can use in
balls which are kind of an extension of
the gibbs sampling which is instead of
just selecting one run in hypothesis I
will select several and just hold the
voting between them so i would like to
compare these methods but this is very
qualitative comparison and to make sure
that you don't think that there is
anything accurate about it i do it by
hand just to emphasize that and i want
to look at 2x is one of the run time so
when I want to make a prediction how
complicated is it to make this
prediction and the second axis is
accuracy but think about accuracy more
is how well does it capture my beliefs
so I had the posterior belief does a
path as's how well does it capture you
know all the structure of my beliefs so
let's let's look at some examples so if
we look at the gibbs method which is
just you know I got my belief i selected
at random hypothesis and then I'm just
going to use it to make prediction so
the runtime is really easy i just need
to evaluate a single apophysis and see
how well it works but does it capture
all the complexity all my belief a to
very small extent i would say if we look
at the maximum a posteriori it seems to
capture more but still it it just looks
at the peak right so my belief can
change dramatically and instill the peak
as long as it remain in the same place i
would pick the same one so it doesn't
seem to capture all that I've learned in
the learning process on the other hand
if we look at the Bayes classifier it
uses a lot of information about my
belief but it's in terms of front time
you know I need to really hold the vote
every time I want to make a prediction
and semble method provide kind of a
trade-off right if I use an assemble of
just one hypothesis then this is just a
Gibson plank so it's going to be fast
but not very accurate whereas if as long
as i get my assemble to grow larger then
i kept your
more structure for my beliefs but it
becomes lower of course this is because
for example for linear if your classes
if your descriptions of the linear
classifiers and you can show that the
Bayes optimal i think is also a self
linear classifier so somehow the fact
that the base the base is not linearly
is not if you're making a binary
classification so it's true if you if
you the it's not true that the the base
if you take the base classifier over
linear classifier is a linear classifier
it's it's simply not true distribution
when they do the journal when they use
them so you have packed bayesian
generalization bound for spm and in that
setting so turns out the base classifier
is in factories fast like again this is
very you know I'm this is why I didn't
you know I drew it by hand it it's a
thing thing about the concept not I'm
not about the very specific cases in
which things might be different but
generally speaking computing the base
classifier is hard although there are
examples in which it is easy generally
speaking for general class with general
distribution this is this distant to be
hard okay
I think about mr. gazing perspective
there's two maybe some uncertainty
there's model uncertainty
and there's parameter uncertainty and so
it sounds like you're talking about just
model uncertainty in this schematic
diagrams
I'm just talking so again I'm isolating
the fur all the problem of estimating
the parameters and everything assume
this is after i saw i had my my prior
belief and i saw some evidence and this
is currently my belief this is my belief
and you know this is it but now i'm
asking myself ok given that this is my
belief how should i make predictions
excusing distribution to remodel classes
and the brand edge gets down yet let's
get both model can pre-register heads up
so what I'm about to propose is a new
way to select any policies from my
belief which is to use the median now
the median is going to be just a single
hypothesis or in terms of a runtime it's
going to be equivalent to the Gibbs and
the maximum posterior but I will select
this appositive that captures as much as
possible information about my belief so
this is where we're heading to ok is it
so now I i want to start defining what
do i mean by a median but it turns out
that in order to define a media and
we'll need to define something which
will call a depth depth function so
let's start with uh with just the case
that we all know about the
one-dimensional univariate median so as
you might have this sample here of
points and i want to find the dominion
of these points so one way to look at it
is to say the following every point kind
of splits the sample into two parts all
the point that are to the left and all
the point that to the right of this of
this point right and we associate a debt
with each point being the size of this
smaller the smaller part so for example
for this point it had the deck three
because here on this side there are only
three points right where this point has
depth one because yeah if I split the
space only this point is on this side
now given this definition then this
point is the deepest point and it is the
median right once we define the median
via a depth function we can now move to
multivariate cases
I don't consider this no no just count
the number just count the number right
so let's try to see if we can extend
this definition to the case that the
multivariate case so when we move to the
multivariate case this is called the the
two key depth function and the two
comedian so how do we define it in the
multivariate k so i assume again I have
this sample but now it's two dimensional
and I want to measure the depth of every
point so what I do I look at hyper
planes so if I want to compute the depth
of this point I look at all the hyper
planes that go through this point every
hyperplane split the sample into two and
I take the smaller part of it and I take
the minimum over all hyper planes and
this would be the depth of this point so
for example you can see for this point
the depth would be too because every
hyper plane we'll have this point and at
least one additional point with it where
for this point the depth would be three
because every hyper plane will have at
least this point in two other points
with it so so this point is deeper than
that point and again once we have a
depth function automatically we get a
median associated with it which is just
the deepest point now a word of advice
is you have to know that in the
multivariate case actually there are
many definitions of multivariate medians
so we can define different definition of
a depth function each one of them will
lead to different median yeah
in that example if your points are in a
convex chain right then all of them are
minions oh jeez you're saying no because
all of them we have at one point week so
remember that the median doesn't have to
be a sample point so if I have points on
a circle right so let me draw that right
I have points on on on a circle right
this is my sample so you're right to say
you're correct that every one of these
points has a depth of one but actually
that point is much deeper
also if any i get i can i can compute it
to any for every point in space right
point and in this case in this case you
can actually see that the dip the median
is actually the center so it's what you
expect it to be like if you can computer
what if you cannot compute your function
so that dot that function that are
defined only on your sample and our
depth function that I defined everywhere
there are different definitions each one
of them has its pros and cons right it's
it's it's so I don't it's an interesting
topic it's it's very interesting topic
but I'll have just keep on on all of
that so I just wanted to point you out
to the fact that this is not the only
way to define a multivariate median but
this isn't one that I'll be inspired by
so this is why I brought it up and not
other possible definitions hi okay so so
for example foot so again like the
median like the univariate case there
could be more than one media right you
know it from the universe that the
median is not a single point it could be
but here you can you can show for
example that the level sets of the
depths functions are convex and
therefore if you have more than one
median it always will be a convex set of
points which are the median so you can
show all sorts of things so there's a
lot of study on multivariate media and
you know just in as an anecdote I will
tell you that but ten years ago I it's
the first time I encountered that and
III I thought that I invented the
multivariate median because I searched
for multi-dimensional median and when
you search for multi dimension median I
couldn't find anything and I thought you
know this is my big thing right and then
I don't know I know why a friend told me
oh you know that in statistics they
don't call it multi-dimensional they
call it multivariate
so yeah so this is why I just brought
this just just to mention the fact that
there is a lot of literature a lot of
different definitions and and unlike the
univariate case in which when you say
the median we all know what you're
talking about when you go to the
multivariate case listen no longer true
but now I'm going to motivate it by this
very specific definition of median that
that would just so I'm going to define a
median for function classes so one thing
that is important to note that unlike
what we discussed before in which we
thought of a sample of points and we
wanted to find the median of these
points now we're talking about functions
right we're not gonna we're not going to
find the median of art our sample but
the median of our hypothesis so just
just a little bit of notation that we're
going to use so our sample space is
going to be I'm going to use a letter X
for that and the function class is f and
the most important ation is Q which is
our belief or posterior belief this is
the distribution over our sample class
and here is how we define the depth in
in this clay case so first I'm going to
look I have a function that I want to
measure its depth so first of all I take
an instance and define the depth of this
function with respect to this instance X
to be actually the probability of a
random function agreeing with the label
of F that emphasized to X ok and now i
define the depths of F to be just the
infimum over all instances so think of
it it's similar to what we did in the
word in the two cadet in which we said
okay every hyperplane defines some sort
of a depth because it splits the space
into two and I take the smaller part and
then I take the infimum
overall all hyper planes this is what we
do over here family this it had to be a
current education
actually i can i can compute the deaths
of a function which is not in my
function class for example it's defined
for every function right and again once
we have a definition of a depth function
we have a median right which is just the
deepest point the previous time said
that the range of F is just my
compartment writer so there's just two
depths know for every again x 40 x it
could be 4x is either 1 or minus yeah
but then I take the infimum overall X
ever given X yeah yeah yeah there are
just two types of x1 that are deep in
one that are shelling except okay wait
wait but there's a probability withdraw
a private random q yeah but but but what
office says if I have four for certain
point and we'll see it in a second for
second point say eighty percent of the
functions say that the label is plus one
and twenty percent will say that so this
is so it's either either you are in this
life or in that class there's two selves
this terms yeah but but once once you
take the infimum then this function
becomes continuous right
you're absolutely right
now you're taking into over all of X
regardless of you yeah so there's an X
with there's no probability of occurring
you're still used to the question wait
45 minutes
beast exactly so for example you can see
that if I'll compute the depth of the
base classifier it will always be at
least half all right do you see that
because the base class if I always take
the majority vote
right so for every X at least 50 you
know fifty percent of the population
will agree with that and therefore the
infimum is has to be created and half
right so this is and keep that this is
this is an example to keep in mind so a
depth of half is kind of the best we can
hope for in any realistic setting right
and the question would be you know how
close do you have can we get
just uh I will not go over all the
details here but we can actually see
that the two key depth that we discussed
before it's actually a special case of
this definition so when the hypothesis
class which we deter the some sort of
linear classifiers then actually the
trick adepts becomes you know a special
case of this definition but there are
some interesting things about the two
cadets so for example we know that the
two key depth if we work on on on D
dimensional space there is always a
point with depth greater than 1 over d
plus 1 regardless of the distribution
there is always a point with depth 1
over d plus 1 and we know further that
if the distribution Q is low concave
then there is always a point with depths
1 over e regardless of the dimension
right now 1 over e is very good remember
that kind of the optimum we hope for is
is one-half so 1 over e is very close to
that so the base point is enjoys the
base function isn't always in this space
f that's what you say it sounds like it
could be an unrealizable so you might
not be able to realize in the space of
the book with a major q that that you
didn't have I CSS okay now
if we go back to the beginning of our
discussion and we said you know many
learning algorithm look like that you
know the energy function looks like that
and we actually like the loss function
to be convex and the regularization
turned to be convex which means that the
energy function is convex which means
that Q is low concave so actually in
many of the cases which we really work
with we are exactly in this setting
where our posterior belief is log
concave and therefore we are guaranteed
to have a deep function you know as a
function with the depths of at least one
ovary
so I hope that you know you get some
some intuition with respect to the
median and the dash and what what does
it mean and now I would like to convince
you that the median hypothesis is
actually good right but but before I do
that again a warning don't always use
the median but in our case is actually
it's actually good to you the median and
so here the first result so let's say we
have some some target so the word is
distributed with some distribution knew
that I don't know about it and and say
we just will user our new of F is just a
generalization error of F so we could we
can prove that there for every function
f is generalization error is bounded by
one of its depth time the expected
generalization error of if I would have
just sampled hypothesis from Q okay and
actually the proof is trivial right the
proof we have to consider just two cases
pick a point there could be two things
if the majority vote is is done with
large majority then my function since it
is deep must agree with the majority
right and therefore I you know my my you
risk my loss on this would be very
similar to the risk of of just a random
hypothesis the other case is in which
you know Q is not decisive in which case
if Q is not decisive my competition
right which is just a random hypothesis
would err a lot regardless of what is
truth and and so for I can do I can do
much worse than that so again this is
kind of vaguely speaking but when you
write it down it's just two lines and
you get the proof
then I think about it that we can
connect that with with a kind of more
traditional package and analysis so we
had just about a month ago we have my
own my child over here and actually what
we what he showed is something of this
type of a theorem and trying to clear
things around basically what it says
that which like with large probability
that the generalization error of just
selecting new processes at random the
expected error of a random hypothesis is
bounded by a term which is made of two
things one is its training error and the
other one is kind of like the KL
divergence between the prior and the
posterior and now we can just take this
result take the theorem that I just
showed you and plug them together and we
get a generalization bound let's say
that for every prophet is its
generalization bound is bounded by one
of its depth time the same term that we
had before
but but now there is something really
annoying about a definition and offer
asked about it is that we looked at the
infimum why when we define the debt we
looked at the info moment this seems to
be kind of very harsh right it could be
could be that we have a function that
the majority agrees with the majority on
almost all the points but you know say
one point so it seems not the right way
to look at it and therefore we can
define a relaxed version of the depths
and what we say instead of taking the
infimum we say okay you allow to put
aside a proportion Delta of the points
and compute the depth only on the rest
of them so when we look at this relaxed
relaxed that over here actually what it
means that for the majority of the
points your depth is greater than that
but there might be a proportion Delta
and which actually you do were than that
but that's fine and again we can repeat
the same kind of proofs that we've seen
before the same kind of theorems using
this relaxed version of the depths the
depth function so we can we can bound
the generalization error in terms of
this relaxed relaxed depth function and
have a delta term over here and again we
can plug it in the park beijing bounds
and get a generalization bound in terms
of this relaxed depth function
we can choose Delta frankly whatever I
want
I think you can but I think you can I
think you can but I you know what I need
to verify that
I think it does
so you know I think I hope that by now I
motivated you that finding a deep deep
hypothesis is something worthwhile and
now we start looking for you know to
find a DP processes and then the first
thing is actually to be able to measure
the depth of any policy so if someone
comes with an apotheosis can I know what
what its depth so and we want more than
just estimating its depth we want a
uniform estimation in meaning that if i
have a class of function i want to be
able to estimate the depths of all of
them simultaneously and make sure that
my estimate holds for all of them so
here is what we're gonna do so assume i
have a bunch of of points and as we
discussed earlier for each point i can
look what is the size of the population
that labels it plus 1 and what is the
size of population that labels 8-1 so
say these are this is my these are my
sample points and now i want to evaluate
a special type function so this function
for the first point is provide the label
plus 1 right so i know that it steps on
this point is twenty percent because
only twenty percent of my functions i
agree that the label he is plus 1 and
then i can take the label on the second
point and again see what is the site of
the population that agree with it on on
the second point and so and so forth and
i can go over all the points and each
one of them mark what is the depth with
respect to this very specific point and
eventually the depth is just the minimum
over that alright so in this case on
this point I had that the smallest
agreement and therefore that depth of
the of this function is 0 point to right
and this is what actually we're going to
do so actually what we're going to do is
instead of computing the infimum overall
points we're just going to take a sample
and if i'll you get the depth of only on
this
and one we want to evaluate you know
what is the agreement here again we'll
take a sample of functions and use them
to evaluate you know what is the
proportion of the population that labels
plat 1 and what is the proportion of the
population that labels minus 1 and this
is actually what the algorithm does so
you take two samples was it one is
unlabeled samples of instances and one
is a sample of functions from your
belief and you actually compute for
every for every point X you compute yeah
you complete the what is the proportion
of the population that agree with the
function that you're trying to evaluate
and you take the minimum and this is
your estimate so this is what we call
the empirical depth of the function f
and we can prove the following property
and again I'll try to clear up all the
the details here but the main thing is
that with high probability will have the
following the depth that the empirical
depth we compute is going to be greater
than the true depth minus epsilon and it
going to be not larger than the relaxed
apps plus epsilon so it's going to be
somewhere in this range but this holds
with high probability told
simultaneously for all functions so it's
it's uniform this is a uniform bond that
holds for all functions and if we look
at the probability term what it what do
we mean by high probability let's look
first over here though we have here the
fifth funky the Phi function the gross
function of VC dimension that we used
you see and we see that we have some
polynomial term here but exponentially
small term over here and the same thing
over here so we can choose the sizes of
the sample that we that we need said
that the probability will be as as high
as we want as close to one as
as we want to see ever hit certain
parameter right let's go back the
definition of relax death there's
there's there's perimeter you choose it
don't don't ok so that Delta since out
the Year Zero there's this year
and and again the I'm going to skip that
the proof is again very simple is the
main ingredient is is to note the
following that if I have an in my sample
so we have this likeness between the
true depth of a function f and it's kind
of relaxed depth all right we have a
sort of a slackness here if my sample of
points consists of a point for which TF
of given X is somewhere in this range
then my estimate would be smaller than
the relaxed depth and my estimate will
always be greater than this steps
because this is the instrument so all
that I need to guarantee is that my
sample is actually a heating set which
means that it hits all these regions so
I have instances in all this region for
all the functions so this is called the
heating set and in the kind of in the
machine learning literature this is
called an epsilon net so we know that if
the VC dimension is finite a random
sample will actually heat all these
regions with high probability so this is
actually you know the mean ingredient in
this proof
so now we know how to measure that then
we know how to measure it uniformly for
for all the functions in a class and but
now we would like to find the median so
or approximate the median we want to
find a deep point and and and and this
is what we're going to do now and again
I want to start with kind of a
motivation so I'm going to show it by
pictures and then go over the algorithm
so again I have this sample of points
and and actually i'll tell you in
advance what we're going to do instead
of trying to find really the deepest
point will go the dip function we're
going to find the function that
maximizes the empirical depths and not
the true depth and it turns out that it
is easy to find this function so
assuming i have this sample of point and
only which one of them i know what is
what is the size of the population that
labels it plus 1 and what is a sizable
population labels it minus 1 so i would
like that the deepest possible points
will actually agree with the majority
vote on all these cases right so the
kind of the Bayes classifier will give
the label minus 1 here and plus 1 over
here and plus 1 over here because the
majority says plus 1 then so if I can if
I can find a function like that this is
the best that I can hope for but now I
assume that I cannot so I can't find any
function in my class that actually give
all the correct labels on all these
cases so I'm going to eliminate one of
the points from the sample saying that I
allow the the function that i'm looking
for to kind of Miss label this point but
which point do I want to eliminate if
for example I'll eliminate this point I
might get a classifier that labels here
plus 1 and its depth is going to be
point2 so this is not very favorable
sure these drugs is something with the
scariest
so I want to live I want to eliminate
the point on which there is a smallest
margin in inter voting because they know
that for this point so for example for
this point although fifty-five percent
of the of the classifiers labels it
minus one but still 45 says plus one so
if I got if I got the label plus 1 this
is not too bad so I can delete this
point and say can I find a classifier
which agree with all the remaining
labels and I can keep going and every
time i will delete the point on which
there is a smallest margin and i'll keep
going until i'll find a consistent
classifier and not only that that one's
like once i finally found one i know
that it is actually maximizing the
empirical depth i I can also know what
is the empirical depth and the empirical
death is actually the size of the
minority set on the last point I deleted
so and this is basically it so again
that the algorithm itself will just have
will receive two samples one is
unlabeled sample of points the other one
is simple functions for my for my belief
distribution and the output would be a
function f which actually maximize the
empirical depth and it's a protocol
depth the little sample is something
that I the only thing that I get is is a
posterior you might have used that I
don't know what you can use I don't know
what you have you used to reach with
this posterior maybe you didn't use
labeled sample maybe you have some
Oracle that gives you some hints about I
don't know what but once you have this
posterior this is what I need in order
to select the median apophysis does it
make any sense I can use this I can use
the label sample but it doesn't add
anything I don't use the labels I mean
what I'm and uses having a little sample
doesn't
I mean you know when you're training and
i will tell you have a set of label
points so using addition unlevel points
it's not adding to anything either you
asking whether if I trained using a
labeled example can i reuse the sample
again for this for this part and I think
you should be you should you should be a
you can kind of value you the Delta term
in the confidence you / children
you have tea you can build it ensemble /
team is it true that this ensemble will
always be better than your meeting
I don't know it needs to be evaluated
but again this ensemble in terms of when
I want to really use it it is much
heavier than having just a single
hypothesis right it's ten times easier
but on runtime and here i use it only in
training bench then yeah but but is it
true that it will be it that the best
you can do the best you could hope to do
is as good as the example on this guy
because you're using yeah it makes it
makes sense but can I prove it i don't
know if i was in your son will just take
always zero but the me sir you can have
any ensemble always zero but one
but user interface approach is very easy
run if they see a crude Q prime which is
not the posterior but it's the empirical
question just posterior is defined by
this and in sample of teen is the base
up to move on that empirical posterior
it's true but it's not my butt but then
you have to prove that this empirical a
posterior is is is better than that
that's optimal classifier on an average
it hard to believe in love
to pry but but we're asking 1q right
what about what a bitch I think the
answer is that that is upper bound it's
this this is all it's better than the
best I provide you can get but we don't
have lower bounds yeah so just you oh
well it's going to say I mean the case
of the base point machine is a classic
one right where it's known in your midst
you're referring to this earlier where
in fact you know the actual the BPM so
just think is an approximation because
the actual classifier issue you may want
to get it may not live within the
function budget and again from Steve it
could be that the best and solve a lot
of tea you know does pretty well that if
you happen to have a cute crime which
you're using not letting against that
they had that that many you can
do better should what was actually an
example so
and if you can use an assemble yep and
if you can use the base classifier
probably the best thing that you can use
but if you can or this review is rich
cross which were called trip thing
working name is gigantic loaded Oh mega
classifier the mega and some concern
trigger misma share very well have you
call it the trek your your Drakkar you
make the ginormous sort of classifier
out of everything in the kitchen sink
their you ensemble selection the
ensemble selection so so that's sort of
your ab and if that sort of a man is
going back to Chris Meeks question of
like what is this is this like a
parameter space know if it's really
hairy weird space over you're saying
that your belief is I don't know how I'm
going to try to search trees all sorts
of things and that's sort of what your
sort of giant the leaf domain is and
instead of what you're doing is you're
you were sort of i think if you were
boosting or averaging together he said
you could train the little classifier
using his method maybe and this is sort
of a guidance of how he would trade the
little plaster fire indian summer cabin
is you have to find one guy who agrees
with other guys but this is usually but
this is usually easier so you know if
you think of the test usually when we
say we want to find a classifier that
minimize the number of errors stuff like
that this is hard right but finding so
what i need is an algorithm this is what
we call here the consistent learning
algorithm is an algorithm that either
say I finding apophysis would agree with
everything or I failed right if you're
building ensembles usually building on
some physical
in relatively simple basic us both you
should everything you're very imaginable
algorithm was it is also your ensemble
is much more expressive than any one of
these great things correct and here
Ronnie's saying I want to find one guy
it tends to agree let's seem to use all
these things also access to simplify the
notation I assume that the functions
here are from the same class from which
I'm trying to select the finite
processes but actually you don't have to
make the fridge okay just it just it
just it just you know it otherwise I
have to hold to function classes and all
the discussion is becomes cumbersome but
actually it could be that the example is
made of some storms and then you try to
really actually hit beginning this picky
yeah seriously so we also da but still
ektron you know what you're missing some
so if you take a million things and you
build and ensemble over them and then
you're trying to find one thing which
tends to agree with that ensemble yes I
agree that ready saying it could be like
a big huge treat but it's gonna be a big
you tree so you're gonna pay again with
computational complexity because if you
want the very expressive function that's
you have to make it home that's right
that was the beautiful thing that that
rich fellow because he had been shocked
of it I mean he trained all this
insanely complicated ensemble I mean
they were just enormous at a hundred
times slower than you would ever want to
use that he would label in fact a very
large training set it's sort of
different from here you would label very
large unlace me to the side and then you
can you could you can get a much larger
it is that here he's saying well maybe
what you do is you just find the largest
the largest training set that you can
get zero training error on that matches
the labels of the triad mega ensemble
from a different way of creating rich is
johnny thing on something you can either
say yes but I think you're missing a
very good let's take it off there's a
problem with what you're saying you
can't you can't eliminate the bites by
experience street we can't get something
no if I did that with you if you don't
have any limitations in runtime then
yeah use your more sophisticated
somebody is your base optimal right we
you know I agree with you but if you
have if you have limitations in runtime
and you want to moderate the size of
your your hypothesis because you have
this limitation so this is how our
proposed to do that it's exactly the
motivation for some salsa tactics you
know bill you can build you can build
during the training process right you
can build this huge ensample right and
this is actually here here this is it
the ensemble but now still on the
training process you have to prune it so
you can look at it I think this is what
John is is suggesting you can view all
this as as a pruning process is a
richest motivational differently because
i missed it completely I thought I'd
like my perspective on maybe this is
grab a bat my protective enriches riches
work with but like I mean I beautiful
softly feels very someone but he was
kind of like okay year's Hall said and
on this small set I can't you know train
a really simple thing to generalize well
but I might have a really complex agua
a new job so beautiful you know ton of
them and then I trained and then I lost
by a bunch of unlabeled data with that
so effectively expand the size of money
sample data and then a train simpler
classifier on it may but the here it
doesn't seem to be so much the person at
least the software perspective doesn't
be so my said like oh I have too little
data to work from it's more than well
but it feels like you're trying to do
something about generalization being
well maybe that may become sentient same
thing
basically I separate the task of you
know the date of the training day I just
out of the picture over here right it's
just it's just out of the picture and
the same thing is and what reached it
because you know once you train these
little models and you now use them to
labeled knew that you didn't add new
information right you did you didn't add
new information to this process this is
just a method by which you actually
select your final hypothesis so here I
just I just you know it instead of just
putting gluing them together I said okay
do whatever you want to do in the first
part here is here is what you do on the
second part and I'm just analyzing to
the second part do whatever you want to
do on the first part yeah I'm just
trying to understand your other than a
little better how does it prevent
situations like that more than
that either the Year set of sample
functions
none of which are particularly
so again when I the function that I I'm
going to find eventually does not have
to be one of these and functions it's
not going to be one of f1 to FN it could
be any any function in my class so I had
this algorithm at a for which I'm going
to feed it with samples and it just
either tells me no I can't find I'm
going to feed it with labeled samples
points already labeled by this example
right and and then I'm going to feed it
a samples labeled samples and this
algorithm will either tell you know I
can't find any purposes which is
consistent with the sample all say hey
here's an apotheosis which is consistent
with this and this hypothesis doesn't
have to be from from this f1 to FN could
be anywhere and actually you know and
this is I didn't want to talk about it
but it we already discussed that it
doesn't have to be in the same class
actually
and and algorithm okay just you know up
dalga them what what I do in first i
compute for every point kind of what is
what is a proportion of the classifier
in my ensemble actually that label it
plus 1 and this is the p i+ and then i
compute the size of the minority group
on each point and sort the point set
that the first point will have the
largest minority group and the sizes of
the minority groups are actually
decreasing i compute the label of the
majority vote kind of what did the label
that i really want to have and start
iterating so the first thing I'll just
put all the points in with the labels of
the majority vote and see if I can find
a classifier that just classify
correctly all of them and then so i'll
call the algorithm and see if it can
find it can find a consistent hypothesis
if it does not i'll remove the first
point the first point is the one with
the largest minority so i'll remove it
and again send it to the algorithm and i
keep going keep going until eventually
the algorithm will return a function
church why do you let your skill seems
like you're asking you can you can do
that just a small this log and reg
number of traders actually yeah
but it doesn't matter for for us namitha
knocks out half the yellow is a hill a
neural network or whatever hey yo cookie
all right yeah it is it guaranteed that
well I couldn't it be the case that
that's not you know actually you could
not
and and and eventually once once the
algorithm return a function which is
consistent then I can just compute its
depth using a formula here and and and
we can prove the properties of this
algorithm so first of all it will always
terminate so it could be that you'll
keep the leading points and end up with
an empty sample right and the reason is
very trivial on the last point you give
it a single point with the majority that
the label that the majority gave this
point right so there must be a
hypothesis which agree with that and we
can we can show that the function that
this algorithm returns is actually the
Maximizer of this empirical death and
the depth that it compute is actually
the correct depth for this function and
again trying to clear the clouds around
this formula that the important thing is
the following the relaxed steps of the
return function is at least as big as
the depth actually the depth of the
median so this is a supermom of overall
the depth so this is actually the depths
of the median point minus two excellent
all right so so actually it does
approximate that the median
so before I conclude just just few small
nodes so one is we discussed in the
beginning actually the motivation for
the definition of the depths were the
two key depth in the two comedian so now
we can go back to that and say that
actually the algorithm that with
algorithms that we describe actually if
we use them on this very specific case
they approximate that Tookie depth and
the to comedian and there are polynomial
algorithm and accurately this is a new
result so all the previous result on
approximating that took adapt and to
comedian are exponential in terms of
dimension and this is a polynomial in
the dimension but you know I might say
that usually the approximation that they
consider is different than the type of
approximation that I'm considering here
but but nevertheless this is something
that is it's a new result we can also
i'm not going to go over the diesels
here but we can also discuss that
geometrical properties of these steps
function and you know if it is convex in
some sense but i'm going to skip that
but this is the big butt right so as you
know that i didn't have any empirical
study over here which is kind of why
don't you have any you know if it's good
sure that it's good and the problem is
that i made the assumption that i can
sample functions for my belief so all
the algorithm used actually you know if
you look at the algorithm that that
approximate the median we needed three
things as inputs we needed a some pants
a sample of unlabeled instances which is
usually easy to to get we needed and
consider Casilla kosis consistent
algorithm which in many cases is easy to
to come up with but what is hard to come
up with is just sample of functions for
my belief this is something that for
interesting belief distribution
is not so easy to to achieve and so we
can do your wrist Ock's right so you can
think about begging and random forest
and everything like that this is
actually what they do so we can do your
wrist aches we can also show that if I
can sample from the true distribution
but I can sample with something which is
close to the distribution that's fine we
can correct for for these mistakes but
the the reason why I I didn't yet do the
empirical or studies is once you start
using heuristics and you know if it
works great if it doesn't work then is
it because all this method is broken or
is it because the heuristic did not
deliver what you expected it to deliver
you you won't be able to say so this is
why I preferred first to you know
complete the theory and know exactly
what I want to achieve and and then
separately try different heuristics and
see how they perform with respect to
this method Oh cured by maybe to this
game
is any way to measure how good heuristic
is with respect to it sound like you
sure it's base its Bayes error is low I
mean you know the lowest one but you can
compare heuristics but compare they
conserve your it sticks yappin a bit can
you tell appearing
but you can't compare it you want to
compare it again you want to compare
this Bayes error so if its base there is
low then you're good but if it's not low
is it because the heuristic is broken or
because your posterior is not very
decisive or not very good it's it it's
hard to isolate these two things and
this is why I want to isolate the theory
from the empirical because there is this
additional unknown over it over here by
the are you coming to seem like a lot of
pieces Sam fuse market
to approximate so so even if we probably
have a question is whether or not it's
convert or not so for example even if
you have in the simple case where you
have uniform distribution over a convex
fire right so yeah there are polynomial
algorithm for something from that and
their polynomial and they have improved
so that the complexity is some-something
so the first algorithm were something
like with the complexity g to the power
23 and they managed to reduce it all the
way now it's something like three or
four but the constants the constant here
is something like 2 to the power of 100
Rachel on you know a finding me instead
of the posterior and there are
approximate algorithms like EP or in a
loveless approximation that should I
should actually you don't give you the
medical distribution of Q with respect
to the kind of family that is so good
again there are there are a lot of
heuristics on how to do that and there
are evidence that in many cases that
work fine but the only result that I
know that it's actually actually samples
from the distribution at least in some
cases they have this complexity so again
if once once you start where did you
know that the Markov chain is mixing
you're making of it you're making a very
strong assumption that you have a sample
for a polytope and you're only sampling
of you know continuous values repulsor
but that's one specific but that but
yeah but then if you say okay so let's
sample only from cues from which I want
to sample so maybe these cues so if you
know imagine now I'm going to apply to a
certain problem so you know first of all
let me just before we dive into this
discussion you know the first thing that
I want to do is really go go ahead and
evaluate this right though so this is
you know obvious right but there is a
reason why I wanted to isolate it from
this theoretical work and because once I
want to do that I have to either
restrict the type of cues you know the
posterior that I'm allowed to use to
only cues from which I know how to
sample from or say I'm not going to
reach the restrictions but I'm going to
be used some heuristic to sample from
this distribution anyway you know if it
works fine that's great but if it
doesn't work what does it mean does it
mean that this full method is broken or
does it mean that you know if I
restricted my my family of distribution
yeah its treated in a way so that you
know this distribution do not work well
for this problem by definition since
your method has to say thank you it's
broken if it doesn't work yeah but but
then you see you could say you can say
okay it's it's it's you know okay so i
can say there is a another item now is
how to sample from Q right so I went
that far and obviously you know there is
a still a way to go right and you know
I'm very clear about it but if I just
evaluate now and see you know if it's
working you know we are happy right but
if it doesn't work what does it mean
that this whole line is broken or that
we need to improve our methods from
sampling from Q
probably the latter so for example local
cake yes a lot on exactly the kind of
form your the energy function you have
yeah David Dunn off work in trying to
approximate the posterior put this back
at least in the in the in the Bayesian
inference community yeah but perfectly
even with like svm loss functions with
logistic loss with probably loss and as
far as I know when I will be happy to
know that I'm wrong is fine and no none
of them has provable guarantees on the
difference between the true distribution
from which you sample and the sample the
distribution that you're trying to
sample for you don't have so you know of
them are well justified seems to be
working but for none of them apart from
this result for all of us and see one of
its and stuff like that and I don't
remember who else none of them is really
provable so here's the problem right so
you talked about some sample selection
from the supposed to distribution and
you're saying that your method of
selecting media is a good one right and
then i have i have methods to evaluate
in the mode i have the methods to
evaluate the main and certain cases
people have shown that mean is might be
better than mode ready and i cannot
pinpoint where does this lead in LA in
that space unless there is some
empirical
valuation right and hands hands my
questions right what what would it
require you for you to sort of show that
it is as competitive as those days so
definitely I'm going I'm going to
evaluate that and definitely i'm going
to try this heuristic methods either of
sampling with the anchors in accuracy of
the sampling process and see if it works
or otherwise restrict the family of Q is
set that due to families from which I
know how to sample but I wanted to
separate that from from this work
because you know I I'm once i do that i
added another unknown and and that the
evaluation would be problematic here's
another way for here it may be the
sampling when i said a posterior i say
that really is my superior because i can
adjust my prior it's whatever i say so i
could just by prior maybe if there's
certain conjugacy things I don't know to
say that is my prior it's a crazy prior
but how can you argue with me it's my
prior in the prior happens to match
where the posterior has the same that's
it but again but again you know again
like you know you are now to design an
experiment right you are now to design
an experiment and you think okay I'm
going to design an experiment let's see
what are the possible outcomes right if
the outcome that it does not generalize
well is it because of your sampling
technique or is it because the algorithm
that I described here is maybe the small
think you just insert it i think it
doesn't make any sense to you it would
have been inseparable it would have been
inseparable if i simply knew how to
sample from cue from a general cue so
for example from from the energy
function of svm if i could sample from
this energy function right and and then
i can compare the result further of the
result of svm then I know exactly you
know this is that's one choice yeah this
is one choice of prior but important one
I can trace a fire which correspond it's
soup but then but then again what is the
broken a component here is that the
prior is broken or is it the algorithm
broken so there is a problem in running
this kind of experiments right it's it's
an inherent problem in this kind of
experiment that you cannot control
all the free variables right so this is
why I prefer to go all the way to
establish a theory and know exactly what
I wanted to achieve analyze it and then
once i do that yeah i'm going to run the
experiments but i know that there are my
ability to evaluate the result of this
experiment is going to be limited
function correctly you're saying that
the depth of the Bayes classifier is
have in the new bounds have won over the
debt so you're doing a factor of two in
your air compared to the gibbs sampler
no matter what so I mean the whole you
remember when my how much on board over
here is that we get this annoying to
factor for underlying the Bayes
classifier we try to get rid of it and
didn't manage to do that yet the two
goes to
at least here when we're studying on
this stage he said that we have this
annoying and our knowing to factor and
we try to get rid of it and we couldn't
and here's ok but you can see cases in
which you can get better than to for
example if this deep the relaxed depth
so we can say why is it too is too if
you have many points on which kind of
that the majority vote is is very
marginal right but if you actually for
most of the point the majority is very
decisive actually this bound will show
you that you can get better than in fact
that you kind of two is the worst case
for the base classifier it could be
better
but even in their analysis actually what
they show the best bound that they get
is for the expected peeps classified and
then they say okay the base classifier
cannot be much worse than that and this
is the same thing that would you over
here so reminded show you this result
where the two girls do the other thing I
want to say is that the trendy thing in
learning theory now is to talk about the
importance of strongly convex regularize
and mention this at all I'm really
required with the local company of the
exterior now what happens I don't know
what it will be called we have a
posterior which is defined as normalized
Expo strongly convex option rather just
a convex function which would be
achieved by our strong connects
regularizer and I think what you get is
that the guarantee that is guaranteed
and said that there exists a function
which is at least of depth at least none
for you
one over yet i don't need the strong
convexity right i'm saying i think that
if so 1 over e is in the general case if
you have a ball you actually have had a
strong convexity will exactly interplay
between the two right this will be
analogous to this so you taught me this
result that for general context body the
center of mass
kind of stuff that at least no breeze on
one side I'm assuming that's like this
one for you but if a ball you can
actually cut it in half and if you have
a strong comeback circular is like a
nail to an advisor think of a think of
the fun think of your last function is
just there's the Justin deltoid birds
are no perplexed so the weight of the
empirical laws is zero you just do yeah
okay think of a book they not have have
interesting direction I like you know
it's not something that I can come up
with from the top of my head actually
you can you can also show that if the
function is not log concave actually
there's a larger families of
distribution which is called rock
concave function and for each one of
them you can you can get a bound which
is a functional fraud and you know
they're earth you can refine this is
also i'm sure
so there's a question about the sort of
energy function that you should write
the regularizer plus the loss function
is it does it always do the valley
property distributions so if you just
exponentiate the energy now you always
have I think for svm probably that's not
so you always have the partition
function yeah but I think or something
or someone so consider the case when you
have you know like SVM case when you're
regularizer is a norm where is your loss
function of the end loss so say that the
integral might go to infinity and you
cannot yeah could be but then if you
found it if ubound it if you bonded it
now you know so fond of it your
oscillator somehow
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>