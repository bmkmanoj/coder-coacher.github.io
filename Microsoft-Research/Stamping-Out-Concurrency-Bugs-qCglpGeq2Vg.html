<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stamping Out Concurrency Bugs | Coder Coacher - Coaching Coders</title><meta content="Stamping Out Concurrency Bugs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stamping Out Concurrency Bugs</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qCglpGeq2Vg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay that's a let's get started I'm not
sure I need this so it's my pleasure to
introduce barrack Kasich's did I say it
right sounds about right I looked it up
yeah and so Barris is finishing his PhD
with George candia at EPFL and he's done
a bunch of different things but all
roughly along the lines of finding
concurrency bugs so he's going to tell
us all about his most recent work on
that all right well thanks a lot to
shell thanks for the introduction thanks
for inviting me here it's it's really
great to be here it's my third time here
so all right so this talk is going to be
about getting rid of concurrency bugs
and I guess it's no surprise to this
crowd that bugs are a big part of
software development but to give you an
idea of how big a part they are I'll
provide you some numbers now just this
past year about thirty-five percent of
all the IT expenditure went into quality
assurance so that's activities like
testing debugging and fixing of software
and this number is expected to rise to
all the way up to fifty percent by 2018
according to some projections now
because quality assurance is so costly
developers basically use their resources
for dealing with bugs and they cannot
use these resources and their time to
actually develop cool new features which
is a major problem now one particularly
challenging and notorious class of bugs
is concurrency bugs and I mean we all
know that the concurrent program is one
in which multiple threads of execution
communicate and share resources such as
memory and it's great if you can write
correct concurrent software right your
your code will run faster on parallel
hardware then if it were just sequential
but it's tricky to to write correct
concurrent software and developers make
mistakes such as data races atomicity
violations and deadlox when writing such
code and these are concurrency bugs
essentially and these bugs have caused
losses the human lives material losses
security vulnerabilities and not just
that but they're also known to be time
consuming when compared to other types
of bugs in that they take it's harder to
reproduce them and it may take days or
even up two weeks to
early crafted fix for such bugs now all
these extra challenges make tackling
concurrency bugs a scientifically
interesting problem and to better grasp
the ordeal that concurrency bugs have
become especially over the past 10 years
I'd like to give you a quick historical
recap now this black curve of course is
the transistor count trends over the
years as otherwise known as Moore's Law
predicting that we'll be able to double
the number of transistors that we can
pack into a processor approximately
every 18 months right so this trend went
on well all the way until the early
2000s and all the while we had only a
single core in processors and after the
early 2000s due to various physical
constraints processor designers had to
increase core counts to keep reaping the
benefits of Moore's law and they managed
to do so so they work great some people
called it the multi-core revolution
right these architectures are mainstream
today so my phone has two cores I'm sure
some of you have better phones that have
four cores or eight cores this was great
but the problem is that this ship
happened rather quickly and so there
were only a few people basically writing
code for such highly parallel
architectures such as super computing
experts or people writing code for
multiprocessor systems but all of a
sudden there were a lot of developers
writing code for such parallel
architectures and they weren't
necessarily well prepared to write
correct concurrent software that could
take advantage of the parallel hardware
and they basically unwittingly
introduced concurrency bugs in their
programs now I'd like to come back to
this previous graph namely the and focus
on one particular trend that is the core
count trans over the year now remember
that there was a sharp increase after
the early 2000s so what I did is I went
ahead and asked Google Scholar how the
number of papers that mention
concurrency bugs in their titles varied
over the years and so you proposed this
information on to the core counts graph
and as you can see there's a there's a
there's a clear trend and in which the
after the shift of multi-core
architectures there's a proliferation of
techniques that at least match
concurrency bugs in their in their
titles right so this is actually a
fortunate situation I'd say because it
means that academia actually took rapid
interest in the growing real world
problem of concurrency bugs as evidenced
by the publication count yes the the
name of paper strategist Europe the year
is also so when the exponential rise no
no we didn't I mean it was just per year
but absolutely yeah that would be there
will probably be the case as well yeah
we didn't take into account this graph
just shows per year how many it's highly
misleading because if you double the
number of papers that we perused for a
year so then actually sort of the rice
evenly to say do the currency but I just
because they use more papers okay no
it's it's I mean basically so for every
year you have more more more papers that
are produced right so it means that
every year there's actually more
interest in some sense so that's the
trend yeah all right so to give you some
examples of forces how concurrency bugs
can compromise system security i listed
a couple of attacks that that actually
use concurrency bugs in order to craft
their exploits and these are in in
popular software such as the linux
kernel and apache web server now these
attacks can lead an attacker to gain
control of the system exfiltrate
arbitrary data or you know gain gain
control of the system of course there
are other types of bugs that cause
security vulnerabilities but it turns
out that existing defenses against
security exploits can fail if attackers
can actually if Thakur's will actually
use if that could actually use
concurrency bugs in order to craft their
exploits which is another reason why i
think it is sign it is scientifically
compelling to tackle these type of bugs
all right so having identified the
increasingly more relevant problem of
concurrency bugs in my dissertation i
develop techniques to identify and fix
these bugs and the approach i took is
one in which i
study real systems and the issues that
developers face when building concurrent
software I then design techniques to
solve these issues and the key theme
that recurs in these techniques that I
design is a hybrid static dynamic
program analysis approach now it is
possible that I'm I'll tell you briefly
why this hybrid approach is powerful and
it's that it's possible to build dynamic
analyses that are generally accurate
they give that that give the correct
answers but because dynamic analysis
typically operate when the program runs
they tend to incur a lot of runtime
overhead where a static analysis they
run offline so in that sense they don't
have a runtime performance overhead but
because they like access they lack
access to execution context information
of an executing program they may
actually be inaccurate and it turns out
that a carefully crafted mix of a static
and dynamic approach can be both
accurate and efficient and I'll give you
a detailed example of this during the
talk now having designed these
techniques I then build real systems
that actually solve these issues for
real-world software and when building
these systems I strive for three key
guiding principles the first guiding
principle is that of striving for low
overhead low performance overhead
because as we'll see in a moment the
techniques I built are geared towards in
production use right so to be used in
the pin the devices that we use every
day or in a data center and so
performance is a key design constraint
the second design goal is achieving is
aiming for high-accuracy right we don't
want to build inaccurate techniques and
provide okay occasionally provide wrong
results to to the developers because
they will go on a wild-goose chase and
then they will lose time and ultimately
not end up using the techniques and the
tools and the and finally I try to to
make sure the techniques I do are are
usable on commodity hardware so the
developers can actually quickly pick
them up and start using them right away
now as I'll talk about the various
techniques that I that I developed in my
dissertation I'll connect back to these
guiding principles and show them show
you how they instantiate in the in the
techniques I develop
in my thesis alright so using the
aforementioned approach in my
dissertation I do techniques for the
detection root cause diagnosis and
classification of primarily concurrency
bugs and below each component of my work
you can see the venues where the related
papers were published and in practice
these three steps are essential to
finding and fixing bugs right so
obviously the opportunity to detect bugs
in order to be able to to fix them but
really more often than not they need to
understand the precise conditions that
led to a failure that is associated with
a bug that's what we do during debugging
and that's what root cause diagnosis is
about and sometimes there's just a lot
of bugs so the open can not deal with
all bugs and they need to classify bugs
according to their severity to address
the really pressing ones first now these
techniques can be used in house during
testing but I primarily strive for them
to be usable in production and the
reason for that is this way actually we
can detect bugs that impact real users
so it's a benefit for the users but not
only that we can actually help
developers because now it may actually
be the case that developers are not able
to reproduce the failures that recur in
production in their own testing set up
so it will be a win-win situation now
targeting in production bugs increases
the complexity of the problem because we
need to design efficient techniques to
not hurt user experience and that's why
I also design infrastructure for
efficient runtime monitoring that
enables some of these techniques to be
usable in production alright at this
point I'd like to emphasize another
angle of my work namely the practicality
and the importance that I give to
technology transfer now in particular I
integrated the root cause diagnosis
technique that I'll talk about in detail
in this talk to gdb when i worked at
Intel over this past summer and this
tool is still being used and maintained
the Intel and we're actually working
actively with Intel to release the whole
integration as open source to the public
as of now also while I was working at a
microsoft research in 2013 I used the
efficient this efficient runtime
monitoring infrastructure that I
designed to build a cold code coverage
tool for windows and this code coverage
tool was a very efficient in that with
our test with all windows 8 system
binary so that's almost 700 binaries it
incurred really small our heads of one
to six percent and and thanks to these
advantages I mean after I left this this
product was this this this line of work
was also continued within Microsoft and
you know last time I heard it was still
being worked on but you know I haven't
kept up with the read more recent
developments in though in the last six
months or so all right so in the context
of today's talk coming back to basically
this this this this map of my
dissertation research I will focus on
the technique I designed for root cause
diagnosis of failures in production and
although this technique is general in
that it can perform root cause diagnosis
for Hong Kong currency bugs in the
context of today's talk I will mainly
focus on concurrency bugs now this
technique is called gist because it
conveys the gist of the failure to
developers which is the root cause
itself and when describing just I will
first give a background and overview for
gist and I will explain that these the
details of the design of gist and I
finally present some evaluation results
that we obtained when applying just the
real world software all right so before
I delve into describing just let me
clarify that root cause diagnosis is is
a hard scientific problem for several
key reasons now the first challenge is
that is is that of an efficiency
challenge right so performing root cause
diagnosis requires gathering significant
execution information from programs that
that execute
out there in the wild and this may
actually be really expensive so this may
incur significant runtime performance
overhead sure sure sure so I will
absolutely I will sure sure absolutely
i'll actually get to that and it's a
really question so in our work we make
the assumption that there's actually an
observable means to detect the failure
and so when I talk about future work
i'll suggest with ways to deal with the
limitations that arise from this
assumption but for the context of this
work we're going to target things like
yes crashes or you know an assertion
firing or a hanger something with
something that we can observe that we
can actually associate to a failure it
may be way in the past that's your root
cause yeah well we'll see that a
statistical definition works better it's
really hard to make an absolute a claim
for what the root cause is so people
have been working on this for a really
long time you know defining what the
cause of something is but it as I'll
show a more statistical basically events
that I'm primarily correlated with your
curses or failures it turns out that
they are actually in practice root
causes of failures that you offers end
up removing from code I'll get to that
all right so the second challenge is
that of an accuracy challenge so of
course the Oprah's don't want false
positives or false negatives in enroute
Coast diagnosis right so in the case of
root cause diagnosis the false positive
would mean pointing developers to a
wrong root cause and false negatives
would mean entirely missing the root
causes of certain certain failures now
finally targeting in production bugs
actually aggravates the efficiency
challenge right because of the stringent
requirements on efficiency for a
production code and not just that but it
may actually not be possible right as I
told before to reproduce the failures
that occur in production in a testing
set up so this is an added challenge
that comes with targeting in production
box
now there's a significant body of
related work that dealt with the various
aspects of root cause diagnosis of
software failures that range all the way
from collaborative approaches the
approaches that use test cases to
reproduce the failures to isolate their
root causes approaches that rely on
record reply techniques or runtime
checkpointing techniques or rely on
hardware support to perform not cause
diagnosis and we do really build upon
all this prior work although it is worth
mentioning the assumptions that prior
work make for root cause diagnosis and
in particular some prior work makes the
assumption that there is some sort of
special hardware support or special
runtime state checkpointing support that
can help with root cause diagnosis which
may not necessarily be the case in
practice but perhaps more importantly a
lot of the prior work makes the
assumption that there's actually a means
to reproduce the failures that occur in
in production in a testing set up in
order to perform both coasts diagnosis
which may also not generally be the case
pretty much all prior work makes the
assumption that the failures can
actually be detected so there's an
observable means to detect failures and
in our work we actually revise these
assumptions to target our use case of
finding the root causes of reproduction
failures and in particular just makes
the assumption that there is an
observable means for it to you take
failures and as I mentioned I'll get
back to this assumption and talk a
little bit about it in future work and
how we can deal with the limitations
that arise from it alright so the key
component of the design of gist is a
hybrid static dynamic program analysis
approach and essentially a heavyweight
in house static analysis is an enabler
for a subsequent lightweight dynamic
analysis and its really the synergy that
allows just to perform efficient and
accurate root cause diagnosis in
production alright to give an overview
of just I'll first talk about the
software usage model today so you know
developers of some programs and users
run these programs and so at
at in their mobile phones or in the
cloud in a data center or on the on
their personal computer at the end of
the day these are endpoints where users
run their programs now I'm sure some of
you are familiar with this occasional
error message that pops up in Windows
systems this is an old one actually the
other systems have similar messages as
well so Mac OS X I mean have similar
infrastructure as well Mac OS X and
Linux parent also have similar error
reporting infrastructures and if you
click on send error report after a
failure the systems on which this
failure occurs will will ship back some
information back to developers and the
uppers can actually identify this
information to debug their code to fix
the bug and improve the quality of their
programs and as mentioned previously
this this this this detective effort is
finding the actual root cause of the bug
is actually a if hard thing so it may
not necessarily be possible right for
for failures that are not easy to
reproduce and so on now again in the
context of this talk you know when i'm
talking about root cause is what i mean
is is the statistical definition of a
root cause so events that are primarily
correlated with the appearances of
failures are root causes for our
purposes and i'll show that this
practical definition is actually useful
for for real-world box yes so you're
going to expand on this definition is
the leaf so I'm not going to expand on
the definition but I'm going to show
that this approaching the definition of
a root cause in a statistical way is
useful that's what i want to show yes so
i'm not going to get into details we can
talk about I don't have time to do that
but basically we use methods from
information retrieval to associate
certain patterns with the appearances of
failures so it's that that's actually a
little deeper than that we can talk
about it or fine if you yeah sounds good
all right so with that that's a really
good point so the primarily actually
that's what is trying to say
yeah so basically this this this whole
detective effort is a hard thing and
just actually target precisely targets
this problem so it is a technique that
automates this difficult debugging
effort by creating what we call failure
sketches and informally failure sketches
are representations that convey the root
causes of failures to developers and
I'll describe a little more in detail
what failure sketches are in a moment
but you can think of them as
representations that the developer can
look at and then see the root cause of
the failure and then I'm just gonna give
an example the next slide yeah yeah yeah
yeah so then yeah I mean without further
ado perhaps let me show you what every
other sketch looks like and an aside
from formatting this is the output of
just so this is for a real bug so in
this the representation basically time
flows downward and the executions in an
execute and I'm sorry the steps in an
execution are enumerated along that flow
of time the failure sketch shows to the
statements from two threads that are
related to the failure and their order
of occurrence with respect to the
enumerated steps now I'd like to look it
I'd like you to look at this
representation and perhaps tell me what
the bug in this program is mm-hmm yeah
exactly so it takes you one second to go
to spot the bug but I mean if you're
even if you're not a really good
developer then then actually the failure
sketch actually provides you more more
information so in particular this these
red boxes and the arrow between them say
that primarily in failing executions the
the free statement from from thread one
occurs before executed before the mutex
unlock statement in threat to now maybe
yeah many other things yeah sure there
might be thousands of things yeah but we
get to that but we know the patterns to
look for basically so this pattern of
actually you know it right basically
followed by a read let's say or free
followed by a HD reference and we know
to associate that pattern with the
appearance of a failure so we will know
I will get to that indeed I will get to
the details of it but we will know what
patterns to seek for in in actually the
executions that we monitor and that
actually is really helpful in
determining what are the key ones that
we should highlight on the on the
failure sketch but maybe you can
actually tell me how to fix this bug now
like one way to fix it what would be
don't don't unlock yeah I mean
presumably without altering the
semantics of the program that much I
guess what's that freeze on ready to
fremont ready to yeah so basically order
some or drink in some sense right to do
the freeing in threat to that and I mean
I guess it is it is it is clear but you
know it turns out that actually in this
case and also in other cases according
to our experience and evaluation these
differences that are shown on the
failure sketch point to the root causes
of failures and as you mentioned so one
way to fix this would be to free the sin
thread to and another way which the
developers did is that they actually
basically waited for thread to to join
thread one before freeing the mutex and
although it actually took them four
months to fix this bug so from the time
the bug was filed time it was fixed it
took them formats now I'm not saying
that they actually debug this thing for
four months to remove it from their code
what I'm saying is that if they had such
a representation it would have taken
them much shorter time to actually
remove this bug from their code all
right so i'll begin by
this is a beta sketch exactly which
shows you somehow you're able to ignore
the thousands of things that might have
occurred before that free yes and you
identify the three as being me yes
where's it's just another procedure
course and that's always free magic
somehow no free is not magic so you
could think of it you could think of it
as a right as well so let's say you
right now to a value and then you could
do reference we could have found that
out too I'll get to that in the detail
but the basically the the key idea is
that from the failure point when we
monitor only a small number of events
and we use a combination of static and
dynamic analysis to know actually what
the monitor and then basically it will
narrow down the space that we monitor
based on this general observation that
in most cases root causes are close to
the failures which may not necessarily
be the case so I'm going to talk about
that too but it turns out that in
practice that's that's that's a useful
way to at least start tackling
concurrency bugs yes I'm going to talk
about it yeah yeah so but I'll first
begin by describing the high level
architecture of just by explaining how
the solution that we propose actually
fits into the usage model that I
described and the basically just as a
client-server architecture so the server
side performs heavyweight static
analysis and the client side performs
lightweight dynamic analysis now the
server side takes as input the program
source code and the failure report and
so the field report could be the core
dump the stack trace or the instruction
pointer of the failing instruction and
then it feeds these inputs into a static
analyzer that computes a static slice
based off of these inputs now I'll give
a more precise definition of for the
static slices in a moment but you can
think of the the static slices
containing statements that are related
to the failure now the then basically
what just does is it it instructs the
runtimes in there in its in its client
side to gather more control flow and
data value information from both failing
and successful executions by
essentially taking into account the
static slice that the static analyzer
computed now just essentially gathers
this control flow and data flow
information and uses these uses this
information to refine the static slices
now in particular refinement removes
from the static sighs statements that
don't get executed during actual
executions and it adds to the slice
information such as access or drinks now
this is essential to do because static
analysis actually lacks access to this
type of dynamic information that is
basically only available at program
execution time now another server
component of just then uses these
refined slices from failing and
successful executions to determine
salient differences between them and to
finally build a failure sketch all right
so this was the high-level overview of
chess so I'll start by describing the
details of design of just and then for
that I'll actually first talked about
how static analysis is being performed
with interest now static analysis yes
this is a constant yes so that would
bootstrap the processes you add the
instrumentation you redeploy it is
modified you can we can dynamically
instrumenta program I mean we don't have
to redeploy the entire program yes once
in our using multi that is correct that
is correct yes so there's and it will
aimed at that one bug yes did that
doesn't show up again then yes okay yes
all about you are we the thing is we can
do the instrumentation for multiple
bucks at the same time and I will show
that because we because the overhead of
actually gathering these runtime traces
is fairly low we can actually target
multiple bugs at the same time to
increase the probability that we will
fit yeah so yeah again so the static
analysis of just basically build static
backward slices with the primary goal of
reducing the subsequent overhead of run
time tracing now a static backward slice
will start from a certain
statement of interest in this case it
will be the failing statement and
include statements that the failing
statement depends on and when I'm
talking about dependencies I'm talking
about both read and write dependencies
in this case now because just exclude
statements that the failing statement
doesn't depend on it will allow
subsequent runtime tracking to be more
efficient now finally static analysis
injustice interprocedural because
failure sketches can actually spin
function boundaries so it just has to
account for the for this fact and look
at multiple functions and the function
calls among them alright took better
understand yes so you yes every girl in
good works well for multi really yes I
mean it works well in the sense it is
not accurate at all it needs to be
certainly refined it really depends on
where the failure is so if the failure
point you have a certain type that
allows you to prune a large portion of
the statement large number of statements
from your slice then it works well but
if you're on a void pointer for instance
it's not going to work well and it's
going to have the slice is going to have
a lot of statements and you would
require more run time tracing to
actually refine it it really depends on
where the failing state yes yes data
structure analysis from llvm so Chris
letters thesis basic all right so to
better understand how slicing works I'll
walk you through an example and I'll
continue using this example to explain
the rest of the operation of gist's and
on this example there is a cleanup
function that prints some debug messages
and deletes the memory allocated to the
state object s and there's a display
size function that actually again prints
on debug messages and then displays is
the size of the state object s is of
course other code in this program and in
particular code that calls these
functions but they're irrelevant for the
purposes of understanding this example
now it turns out that certain users
observe this program to crash when the
display size function actually prints
the size field of the state object s and
I'll explain to you step by
step how just will did help us determine
the root cause of this failure all right
so in this first step just compute the
static backward slice starting from the
failure point and in our example it will
remove the function calls upon entry to
two functions clean up and display size
and this is because the failing
operation namely the loading of the size
field of s it is not influenced by these
statements now the key takeaway from the
static analysis of chest is that it
helps following dynamic analysis monitor
fewer events that it would otherwise
require monitoring in particular it
actually reduces the cost of control
flow tracking by a factor of 20 when
it's combined with an adaptive technique
that i will i will talk about in a
minute why don't i delete you look
leaner the why don't you so we do use a
global alias analysis that will tell us
that actually the s in here will good
elias 2's in here so that's why yeah
yeah so there was a question i thought
yeah ok so the analysis interprocedural
that's what i meant basically it will it
will have a global elias analysis that
will allow us to resolve these
interprocedural dependencies actually
yeah sure to tell whether a functional
amateur world procedure could possibly
be the same value as a function family
of another procedure oh that's a little
better yes sure yeah and that's going to
be pretty conservative it is going to be
conservative yeah although we do have
some optimism but it might even be a
completely different time yeah I'll
style yeah sure sure sure sure sure yeah
didn't perturb the city snack so we do I
mean this particular example we do have
access to this the source code of lock
so whenever we have sort access to
source code of law of any function we
will proceed but if we don't have access
let's say if and if we know if we all
this we want to assume that a certain
function is not going to have any side
effects we will just annotate it as such
an excluded from the analysis it's a
really good point
this is a whole program enough yes it
takes all the program including all the
law yes all the way down into the
operation as much as as much code you
yeah everything as much cool you have
basically or otherwise you'll have to
make assumptions conservative
assumptions in the sense that if you
don't annotate something is not having a
side effect you left assume that it will
have a side effect and that it whatever
you pass in longview functions in some
way that you couldn't it yes you can you
can say exclude this from the look you
say printf does not have side effects so
and excluded from your analysis yes you
designed in practice in your world so we
didn't annotate certain library
functions it is important in practice
yeah yeah all right so now that I
explain how just perform static slicing
I'll talk about how both dynamic control
flow tracking and data value tracking
that just performs using instrumentation
helps with the refinement of these
slices now control flow tracking allows
just to perform slice refinement by
identifying statements that get executed
during actual executions and removing
from the slice statements that don't get
executed assume that this control flow
graph actually represents the control
flow in a static slice so the edges are
branches in the slice and nodes are
basic blocks so contiguous sets of
statements continues statements you got
without any branch statement and so what
just does is it it actually attracts the
control flow and refined static slices
using a new hardware technology from
Intel called the processor trace so the
processor processor tracing essentially
allows just to determine the actual path
of the program take during execution so
in particular for instance in this
example the blue path turns out to be
the actual path that the program
executed at runtime so yes across entire
yes this is costing time program
procedures yeah yes so this is right to
the unrolling procedures in this yes
exactly so it will have a it's a dynamic
trace of the branches that are taken in
the program yes so that's what Intel
Intel processor tracing will give you
basically yeah yeah
millions of instructions it will give
you a face it depends on how you
configure it so you can what's that in
any program yes it's a little more smart
as smarter than that there's a
compressed way of actually logging the
branches so it will do smart tricks
because it will actually offload most of
the test the decoding time so it will
not actually record everything but
reconstruct the execution yeah but in
practice you can actually get that
information that you're talking about
Wow yeah actually execute from an actual
execution house in execution yeah well
yes I mean depends on how much you love
of course yeah it depends on your use
case so it turns out that for instance
if you do full program tracking for we
tested this for a broad range of desktop
and server applications we observed
overheads of around 40 runtime
performance or heads of forty percent
which is quite better than if you were
to have a software only solution but
it's still not acceptable if you want to
deploy this in practice right and
basically basically what what what just
does is using a combination of static
analysis and an adaptive technique that
I'll just talk about in a minute it will
reduce this forty percent overhead to
like two percent three percent overhead
will make it more acceptable to be
deployed in production so coming back to
the example that I previously talked
about what happens is that when when
just tracks the control flow it will
determine that the logline that's
printing the the address of the the
pointer s is never actually executed and
this is expected because in most cases
when people deploy code in production
they will disable for both debugging
messages and so just is able to remove
that line has never being executed in
the executions that it monitored alright
so maybe at this point it's important to
emphasize the synergy between static
analysis and this dynamic control for
tracking so these two techniques
collectively help narrow down the set of
statements that a developer needs to
actually look at the reason about the
root cause of a failure in particular
static analysis removes statements that
are not related to a failure and control
flow tracking removes from the slice
statements that don't get executed
during actual executions
alright so for tracking data values just
uses another hardware peach feature
namely watch points so watch points can
observe a certain address in a program
and the codes the CPU to trap if there's
a read or a write access to that to that
address it could be configured we just
read just right and so on and it can
actually do all of this with a low
overhead and another really important
thing that we can do with watch points
is that we can actually track the total
order of execution of statements
accessing the memory and augment failure
sketches with this information now this
information is critical it's critical to
obtain this ordering information in
order to reason about failures for which
to the root causes a concurrency bug yes
your processor haha sis out of the
behavior processor in terms of the week
now as you can see so it will cause yes
them to be flushed it can it can
actually serialize the execution in a
way that you will not observe certain
behaviors that you would have otherwise
observed it can happen yes which weekend
is the the instrumentation might conceal
about the instrumentation may mask about
it may also expose a bug yeah because
it's effectively altering the schedule
the thing is of course there's not
there's not a formal guarantee or
anything but it is minimally I mean I
don't want to claim minimums but it is
less intrusive than a software only
solution basically yeah so yeah coming
back to our example just will
essentially go ahead and place a watch
point at the address of the state object
s and it will monitor the values of s
and as well as the order of accesses to
2s and we'll determine that essentially
in successful executions that's the ones
that don't end up you know observing
this failure the statement that prints
the size of the state object s actually
execute before the the delete statement
and these two statements execute
different threats now i'd like i'd like
to note that there's a notion of threads
at this point right with static analysis
with wood was like a flat program
structure so now we have actually a view
of threads after doing this dynamic
analysis and if the cleanup code of
course execute before the logging of the
size field of s will encounter the
failure sure yeah that's a good point so
I mean for the sake of this example we
can assume that we compile the code in a
way that it will actually whenever we
free the memory the next time we
reference it it will actually cause a
let's say it will throw an exception so
I believe even Microsoft compilers
actually can allow you to do that so you
can actually pad the values when you
free when you free them so the next time
you reference that we would actually
know you would actually detect the user
after free bug but even if that's not
the case what can happen is that if
somebody else allocates that memory
which can happen if there's a lot of
churn in the program that you will
actually access some other memory and
you could incur a segmentation fault it
may not happen always but it can't
happen does that make sense okay this
came back for your first done it was a
crash yeah you said the crash happen is
you dereference the you know there's bad
value in the area location let's watch
that memory location but the next time
you run it might allocate in a different
place or you relying on fu shins being
sort of a beatable no no we wouldn't we
wouldn't watch an address we would do it
this dynamically basically so yeah yeah
so how do you know what to watch maybe I
should ask that question so so we I mean
whenever we're refining the slice we
would actually watch memory accesses in
the slice basically so put a watch point
whenever wherever there's going to be a
memory access essentially so to the
address of the memory access so the
instrumentation will make sure to
resolve the proper address at run time
to put the watch point and watch it
accordingly does that make sense Lotus
absolutely so yeah sure there could be a
lot of time and so aside from anything
aside from things on the stack we have
to watch all of them and there's only
limited resources limited number of
watch points that we can place which is
four on x86 for instance what we will do
is we will place we'll watch whatever we
can for a given execution and then we'll
we will actually distribute the watching
task to multiple users that you cover
the whole basically portion of the slice
that we're refining and do essentially
watch values for for all the memory
accesses in the size but across multiple
executions so we will this basically we
will use the crowd in some sense because
we lack the resources to actually look
at memory accesses for a given execution
for you know all the memory accesses yes
you would not randomly so you would make
sure that you would actually cover like
if you're finding a slice right you have
10 statements you would look at the last
four from the failure point basically in
one execution and then you keep going on
to so that you cover all of them so it's
not random yes you do a thousand
executions yes yeah that's true all
right so yeah so what just does though
is when it's tracking this control flow
and data values it will actually perform
refinement in an adaptive manner and the
reason for that is the slice can grow to
be quite large and so performing
refinement on the slice despite using
this hardware support can actually incur
a lot of runtime performance overhead so
again I'll just talk about how actually
just adaptively refines the slice to to
it to get a better picture of the
control flow of the program but actually
doing the same thing for data values
this is again in I mean refinement for
data values is done in a similarly
adaptive manner as well again this is
the control flow in the in the slice so
the the basic block where the failure
occurs is also seen on the is also seen
on this slide and so what just does is
it start
is tracking a small number of statements
from the slice based on a common
observation from prior work that in most
cases root causes of failures are close
to the failures themselves now just then
builds a failure sketch using this
refined slice and continues refining
increasingly larger portions of the
slice until it can provide developers
with a failure sketch that contains the
root cause of the failure now this
technique is effective because as I
mentioned previously turns out in most
cases root causes are close to the
failures but this technique is still
useful if that's not the case so if if
root cause are actually far away from
failures far away in time and space at
the technique will still work in which
case this will have to monitor even
larger slices and potentially incur more
on-time performance overhead so that's
the trade-off there basically all right
so we saw how this performs control flow
and data value tracking to refine the
the slices so let's look at how it
monitors multiple user executions to
determine the key differences among them
to isolate the root causes of failures
now again consider our running example
so a deletion of s is followed by
another threads dereference of s so in
abstract terms this is a right followed
by a read in a particular order in two
different threads that leads to a
failure so this type of failure is a
common type of failure in multi-threaded
programs it's called an order violation
what just does is it seeks in refined
slices of basically it beats seeks in
refined slices this order violation
pattern and also other patterns of
common concurrency bugs such as autumn
ECT violations and data races now
remember that just seeks these patterns
both in failing and successful
executions it can therefore
statistically determined that that they
can therefore Cecily determine patterns
that are primarily correlated with the
fail either with the failing executions
and essentially highlight them as root
causes on the failure sketch
so coming back to our example just goes
ahead and monitors multiple user
executions and it determines that indeed
in failing executions the same failing
ordering is exhibited whereas in
successful executions this ordering is
not observed geez then computes that
statistically speaking the pattern where
the deletion of the pointer at the
deletion of the pointer as followed by
its dereference is the best predictor of
failure at this point just has all the
information necessary to build the
failure sketch that shows the root cause
of the failure which essentially looks
like this alright so we discussed the
design of just now I'd like to briefly
talk about how we evaluated it so we
evaluated our just prototype using
real-world systems such as the Apache
web server sequel light embedded
database and memcache a distributed
object cache and in this section I'll
basically answer the questions of
whether just as effective and whether it
is efficient now to answer the question
of whether just as effective we had to
determine whether the failure sketch is
built by just actually allow developers
to perform root cause diagnosis and what
we did for this is we manually analyzed
the failure sketches for 11 failures
that just for which just automatically
build failure sketches and we determined
that the root causes identified by gist
are actually the root causes that the
open ended up removing from their
programs by examining the patches that
they actually put in to put in place an
interesting result here is that just
reduce the average number of statements
that a developer needs to look at the 27
in order to identify the root cause of a
failure which is orders of magnitude
smaller than the size of the programs
that we looked at which is why we
believe just just was useful we then
evaluated the efficiency of just by
measuring the runtime performance
overhead that it imposes on the
executions that monitors now on the
y-axis you can see the runtime
performance overhead of just across all
runs and on the x-axis you can see the
number of statements in the slice that
just
letters and the unit here is in terms of
llvm statements in the static sites
because that's the intermediate
representation of the compiler framework
that we rely on to do our static
analysis now perhaps unsurprisingly the
runtime performance overhead incurred by
Geist increases monotonically with the
size of the slice that just monitors
during actual executions and this is
what this graph shows the good news is
overall just has low average on-time
performance overhead of below five
percent and the overhead values have
actually very small variance of below 0
point one percent so that's also another
good indicator for just performance now
to recap this section I talked about
just which gathers data from failing and
successful executions and uses a
combination of static and dynamic
analysis to build failure sketches that
point the root causes of failures to
developers now there's more information
on this web page about just such as
related source code and we're working
again as I mentioned so there's this
integration of just with GDP that we
internally realize that Intel and we're
working with Intel to release this
integration to the public which is
coming soon all right so we talked about
a little bit for about over overview of
my work and in detail about root cause
diagnosis at this point I'd like to say
a few words about some current work that
I'm doing and some future work that I'm
very excited to to be doing a in the
near and the law and in the long term
now basically some of the techniques
that I developed I didn't have a time to
talk about detection but you know versus
root cause diagnosis actually relies on
gathering execution information from
from programs running out there in the
wild right from users and then using
this information to improve the quality
of software and this quote this clearly
comes with privacy implications right so
I'd like to work on techniques that
actually like respect the privacy of
users all the while improving the
quality of software in a meaningful way
so currently i'm actually taking the
root cause diagnosis line of work and
extending it to tackle security
vulnerabilities and i believe that what
we can what we will be able to actually
gather execution information from
failing and successful executions let's
say and not just that but you know just
any execution basically and using some
machine learning build a model of good
versus bad executions and in in that
 in this context what bad for
instance could mean is is an execution
that has a security vulnerability for
instance now basically using this ml
approach we can we can basically build
this model and then essentially
deviations from the model can
potentially be indicative of bad
behavior and essentially this this this
ml approach can allow us to go back and
revise the assumption that we made
initially when we were designing just in
that we were actually relying on
failures to be detectable there in and
out in an observable way and so
essentially we can tackle that
assumption and then allow just to become
more powerful by using a machine
learning approach yes this model look
like I mean you you observe traces or
several traces in parallel so you have
like some onion unlimited tag or
parallel sequences to as input of the
model so how many process that a machine
or inside so you would have to have a
sufficiently powerful representation for
instance a Markov model probably
wouldn't work because you couldn't you
couldn't probably encode the basic the
procedure model but you would have to
rely on certain a certain model that is
powerful enough to represent an
execution yeah so I don't necessarily
have a good idea i'm not an expert by
any stretch but this is right for
collaboration basically for me yeah yes
last question about it view the
performance of georgia's so you said
that you are assuming that the heat has
been replaced with wellness of a cheap
so that every time something is free
that you all mark the virtual dresses so
that you can catch
references like the S sighs and make
your your detective failure detectable
so of the five percent overhead how much
of us overhead is due to be replacing
the eastern fermentations so we don't
replace the eve implementation I can
talk about in detail what I meant by
actually when I was when I was answering
your question so I guess I couldn't
communicate that well but let me just go
through this I'm almost done and then I
can I can talk in detail offline about
that yeah again so one other thing is of
course concurrency is present not just
in the single note level but it's it's
it's there in the distributed systems
and our reasoning and understanding
capabilities regarding regarding these
systems can improve actually care can
improve significantly I believe if we
can apply some of the lessons learned
you know in the in this prior work so
finally basically the overarching goal
of my work has been to help developers
deal with the challenges that they face
due to emerging technology trends and
one particular challenge that I address
was the challenge you know in software
development due to shift from single
quarter multi-core architectures right
so there are all sorts of other
challenges that the operas are facing
today due to other emerging trends such
as heterogeneous computing systems and
Internet of Things now these challenges
present themselves in the form of
programmability challenges as well as
security challenges and I think we need
to rethink the the system stack and the
programming ap is and abstractions to
basically allow developers to take
advantage of these emerging trends with
what all the while avoiding the possible
pitfalls and I'm also really excited to
to work on this too before concluding
I'd like to thank a lot of people that
actually helped me with with this work
so it wouldn't have been possible to do
all this work without the help of all
these people there's a lot of Microsoft
people as well
and so this brings me to the conclusion
of my talk so i present it to you an
overview of the techniques i worked on
today i talked in detail about root
cause diagnosis i also mentioned how the
various techniques i developed were were
you know used by some technology
companies and i showed that the
techniques I developed are actually
efficient and effective and as a closing
thought I'd like to mention that i
believe complexity is unavoidable in
software systems and its ever growing
and i believe we need techniques to
better understand and tame this
complexity and the techniques that i
built primarily in during my
dissertation we're geared towards
allowing developers to better reason
about concurrent programs and i believe
that the results that i showed show that
I taken a positive step towards that
direction thanks for listening and
coming to my talk I can take more
questions if time permits yeah you're
lying the static analysis for slicing
yes and this one is feel sensitive well
it's not control sensitive otherwise it
won't scale yes and so it looks like
sort of a pattern of that shows up in
your examples is that your free x + 1
Fred and then you access X dot you know
fool in that afraid and so the if the
sort of the probably there are not that
many pointers that have full so the
figures pointer dot for year-round
Dakota and then it'll be the other
Chanel's is recognized that free necks
he actually but now is going to take the
eggs from all something got for
occurrences and if the number of the
fact those occurrences efficiently small
yes sir then okay so your slice is going
to be sort of manageable yes yeah so
Japanese of the estimate have you looked
at actually sort of how effective is
point
this in sort of Bruni yeah so it's done
I honestly for large programs it's not
very effective so it's not uncommon for
instance for a single call site let's
say in a program like apache 2.2
hundreds of functions whereas in
actually and run time it's just going to
be it's a function point it's just going
to point to a single function or two
functions for instance so as the program
size gets large it doesn't work that
well information keep saying yes and so
the wood so yeah looting that it's very
effective in sort of narrowing down
function pointers to just one it is not
very effective yeah the dynamic stuff
yes so and so what are the other
dimensions of sort of narrowing down in
pruning yeah so I think another one so
basically there is another way of
looking at it so when is static analysis
very effective basically yes but that
will actually identify places where
dynamic analysis Kingston so static
answers help for instance when I with
things like when there's like a clear
type that can actually prune a lot of
the things when that doesn't happen
dynamic analysis kicks in basically and
it's actually helps you better if we
find a slice in some sense so whenever
the types that you're operating on our
yeah you know it's a common types not
nothing specific then the then the then
the Elias analysis doesn't work well and
the slices grow to be quite large hope
you will see why because you go cost yah
so what's this deference the types so I
mean I don't know the internals of how
data structure analysis works but I
think I believe that it relies on some
type information to prune out okay so
let's meet ya thread sanitizer thread
which dimension of translators with a
trace detection tool which is so
threaten hazard data is detection tool
so it just detects data races so divine
information yes yesterday so tread
sanitizer is
I would say like fully accurate in that
it doesn't have any false positives so
whatever it will report you is actually
going to be you know dynamically
observed that it will be an actual let's
say data race but it just works for data
races right doesn't work for other types
of concurrency problems it so it will
keep track once you can handle a safety
purpose if you can go down as well it's
not that easy i would say for ya imagine
you have a data race and so you have
your slice in sort of you trim the sort
of it a lot using the dynamic
information so the is it plausible to
feed the result of suppose it's nice and
improving into design yes definitely so
you can't combine anyway static analysis
with a dynamic analysis to improve the
both the performance behavior and also i
mean i wouldn't say accuracy of data
restriction but accuracy of root cause
diagnosis and that's something we did
actually in prior work so our detection
i don't have time to talk about it but
we essentially combined something like t
sin like a dynamic detector together
with static theories detection to
basically allow it to to have much lower
overhead than it would have otherwise
had in production because something like
T San has maybe like 20 X overhead on
any rate in any any reasonable size
program yeah stop the official talk now
and you can continue with that is he
turn our nature if you ever saw</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>