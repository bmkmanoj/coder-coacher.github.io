<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Algorithms for Online Stochastic Convex Programming | Coder Coacher - Coaching Coders</title><meta content="Fast Algorithms for Online Stochastic Convex Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Algorithms for Online Stochastic Convex Programming</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UFkYdVWXSok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay its one start so it's a great to
have any kill dem owner today speaking
about fast algorithms for online
stochastic convex programming so Nikki o
is an expert on approximation algorithms
online algorithms and algorithm in game
theory yeah and today is talk about
against online now go right Thanks
Kostya thanks it's nice to speak here I
think speaking after a long time so it's
nice to be the podium and I will talk
about this work which is joint work with
ship progress from MSR India so as many
of you know I spent about a year in
India and this is one of the two papers
I wrote when I was there really like
both of them and it is a very simple
talk I will define the problem and give
you the solution and point to some
future work so what is the motivation as
again many of you know I worked been
working on problems coming up in dis
cloud rising now this is the things
we're vaio days on people's t-shirts is
killing and previous to this work
already the algorithms we design are
actually being used by Microsoft in the
past few years to run this disk
lanthanides and here is the basic
problem formulation the basic decision
you have to take in this problem is you
get an ad slot or an ad opportunity and
it is called an impression so this is a
technical term and you have to assign
this impression to one of many competing
acts so which ad do assign the
impression too so if you assign an ad j2
impression T then you get some value V
TJ that depends on this pair now you
could say okay why don't you just assign
each at the edge impression to the
attachment with the highest value you do
not do that because they're there are
constraints the constraints are each
ad has a target number of impressions
and you do not want to assign more than
these impressions these many impressions
to this I have judgment okay it's still
an easy problem if you know everything
here is in a his LP relaxation you can
solve this LP relaxation you can also
round it very easily right so you want
to maximize the total value generated by
the location this is a capacity
constraint it for every add JS and and
you cannot assign more than GJ
impressions and every impression you can
assign to 19 ok the difficulty comes
from the online part thing is you do not
know this entire thing ahead of time the
impressions come one at a time and you
have to assign the impression without
knowing what impressions are going to
get you the future so this uncertainty
about the future is a difficult is what
makes a problem difficult one it's just
one impression just assign it to one of
the ads 1 / dirigibles I'm sorry what
does yeah it's just impression by
impression there is a son of algae it
should be changed oh this is jay oh
sorry sorry yeah that's what that's what
that's what happens when you copy paste
yeah there should be some oj yeah i
change this thing here didn't change
this ok so we are in this paper joint
with value and others we designed a near
optimal algorithm in a particular
stochastic model I will mention what the
stochastic model is later and this is
algorithm with some modifications that
is being used by Microsoft and then we
met this guy called reality so I'm
exaggerating a little bit thing is this
was a essentially linear formulation and
in reality there are all these nonlinear
things at Creepo so one thing is called
the under delivery so the way this works
is this target is not just an upper bod
it is essentially upper and lower bound
so this is the number of impressions you
promised you promised
wiser and if you do not give these men
impressions then you know you pay a
penalty so if you promised a million
impressions you only gave ninety percent
you know you have to pay a penalty for
this ten percent and if you assume that
this penalty is linear then it still
becomes a linear program it is a linear
formulation but in reality this penalty
is non-linear it's some kind of convex
so if you give fifty percent then it's
no it is really bad so it is a nonlinear
penalty the other thing is I dresses
they require a mixture of different
impression types and it's if you deviate
away from this mixture then it's not so
good but ok so how does it look like see
it again it's not the linear
relationship it's some kind of convex
penalty so the farther there is an ideal
mixture let's say you want 5050 of men
and women but the further you deviate
from this ideal the worse it is and
again it's not linear yes we know a lot
of times most of time you know because
they logged in there they have told us
what your gender is so that's just an
example we might know other things you
saying it's not linear but it's well
defined I mean someone tells you exactly
how bad it is to have forty-eight
percent men and fifty two percent women
there is a score that defined by some
human yeah so you can we can model it
can you know it's a little fuzzy so I
Otis's don't really give you this
function but you can kind of capture
this so I restaurant tell you here's a
penalty but you know you you know so
someone who knows the business can model
this and say okay look you know if you
deviate too much then this is how bad it
should be so some human will come up
with this function under the lyrics
precisely no I didn't agree yeah this
actually this is part of the contract so
this is part of the contract so you know
exactly free is always a designer yeah
we'll photonic this is like this which
is given so this function is given
this function is given okay and another
place where this comes up is this value
that I said act so there are many things
we care about a revenue of course also
relevance and maybe clicks conversions
are all kinds of things we care about
one thing you can do is just take a
linear combination so then it becomes
this linear objective again but you may
not that may not again capture what you
really want so you might again what you
really want to optimize might depend in
some non linear way on all these
different objectives okay so there are
all these things that are not captured
but the nice thing is either they occur
as a concave objective or a convex
constraint and this is like a convex
programming version right so from linear
program you can go to connect program in
the offline world we still know we can
solve this so there is hope so in
practice what we do is we you know that
we took the linear programming algorithm
for linear programming and you know you
do some hacks you do some changes at
kind of make sense and it still works
pretty well but as a theoretician it was
not a new it was not a principled
approach so I really wanted to figure
out okay how can we handle these convex
things that come up so that is what
drove this problem ok so now I'm going
to give you a very general problem
formulation that is going to capture all
these things and we call it online
convex programming so this is we thought
of online convex optimization but that
was already taken so we had to settle
for online convex programming but I
think it is a very obtained so here you
are given a convex function f and a
constraint set s a time horizonte this
you are given ahead of time and this is
not changing so this is what you are
talking about the algorithm is given
this objective function f ok this is
input
now this definition it is going to look
very abstract so bear with me for a
moment and I will show you how this
corresponds to the kind of problems we
are talking about so abstractly in every
time T a request arrives and what is the
request is abstractly just a set of
options and what the algorithm is
required to do is pick an option from
this set what is an option each option
is associated with a certain vector okay
and the sector can correspond to cost or
reward some dimensions can respond to
cause some dimensions can correspond to
reward and so on so you encode
everything in in this vector okay so set
the choice set is just a set of vectors
not ready and diagram has to pick one
vector from the set so pick one option
from this choice set in every time and
then again this is online problem so in
every time you do not know what the
future is going to be you have to do it
in an online fashion and what is your
goal the goal is to maximize the this
function f that is given off the average
of all the vectors a tripling so you can
think of it as a function of the sum but
it's just more convenient to think of it
as the average for us but the two are
equivalent if you know the time Horus
and you can just scale everything okay
so this is and then the constraint is
that the average has to lie in s so this
is a global constraint and a global
object player because it binds across
the decisions you take across time so
it's not part time step it's across what
you do through the entire time horizon
right so the average of everything has
Linus strain corresponds to some
capacity constraint yeah and does it
make sense to average it over time I
mean I don't want to violate any
capacity is wrong time so yes that's a
very good question and that was a linear
formulation linear programming
formulation was you had this capacity
constraint and when you specialize it to
that we can actually make sure that you
always satisfy it
but if you have arbitrary constraints
then it is not possible in the interview
intermediate steps it may not be
possible to satisfy everything so it may
be possible to only satisfy everything
in the end so for the for this
generality you need that everything is
only required to hold at the end you
know the time because otherwise I could
end any time and expect you to be in all
of them yeah you know the time I think
of the opposite of packing the covering
in that case you can only hope to
satisfy at the end for instance so let's
go back to the yeah so the thing you
don't know is a is eighteen eighty you
don't know what it is on if you knew all
the 80 times then it would be just a
standard works like a convex programming
yeah why do you know ABT I'm oh ok
assume each 80 is convex if you want you
know then it becomes convex programming
otherwise it is not if you ats are
discrete it is still a notional you know
this issue of discrete versus continuous
but because we are averaging you know
that is only like a one-hour t thing i
think that i should say is this is not
really we assume this is bounded who is
not really are to the deeps minus 11 to
the d so then this discreteness issues
not a big issue so it essentially connor
program okay
so think of 80s just a polytope say
minus 0 vector covers how connection of
business so we're going to eat more
rounding issue so it's going to be easy
rounding all right so what about the
diss flat problem so this is a
generalization of the displayed problem
how is it generalization here's oh my
can encode it the vector has n plus 1
dimension so if there are n advertisers
it has n plus 1 dimensions the first n
dimensions encode which I wrote I sir
this corresponds to it so there is a one
in the dimension corresponding to a
driver in the last dimension corresponds
to the value ok and your objective is
just the last dimension which you just
care about maximizing the total value
and what is the constraint side the
constraint set is this capacity
constraints is or box constraints right
I just divided by T because I'm the
constraint is on the average ok so for
in the JH dimension you know you cannot
exceed more than GJ waiting ok so if you
pick this vector it corresponds to
assigning this impression to the sizer
that uses up one capacity and it gives
you a value of e TG nature it's clear
how this encodes the displayed problem
right and now let us say you had an
under delivery penalty a nonlinear
hundred originality here's how we would
change it instead of defining FX to be
just the last coordinate you would have
this penalty here so GJ encodes how this
penalty looks like GJ only depends on X
J which is the count of the number of
impressions you are signed to a dresser
J ok so you would have something like
this and if G is convex this is concave
freaking maximizes
this also generalizes a more general
linear version called online packing you
know you have this packing constraints
and you know you can do a similar thing
one example of an online packing is a
network routing problem where the aight
request corresponds to a request to rot
one slow from a node SI to know ti in a
network you have h capacities are given
ahead of time so that is a packing
problem another example is a
combinatorial auction so bias arrived
they have combinatorial valuations four
sets of items and it's a you have many
copies of each item so buyer arrives
here some valuation you have to maybe
offer prices or you know do something
else you ask him to tell you your
valuation in some way and then you
assign a bundle of items to him and you
charge him something and then the Xfire
arrives so this is also an online
packing problem so these kind of things
can be encoded in this format and even
though I said for the online packing
this is a linear version we this is
already solved even for this we get an
improvement implement is technical so
earlier the dependence this guarantee
that we get going to show later it
depended on the number of constraints
and the number of constraints in in the
in this formulation for instance like a
network routing formulation the number
of constraints could be exponentially
the number of variables right because
you know if no graph is ma jizz in our
of paths could be exponentially name so
the number of constraints would be
exponential and we get our dependence
that's only depending on you know only
on the number of variables so even in
some cases we get improvement in even in
the linear cases and there are
potentially other applications we've
been we are looking into this and people
are studying similar problems some of it
fit exactly in our model some of it
needs some work and some of it no it is
not clear the fit hopefully we can
extend these techniques to that so
so the lot of work in especially
operating operations research community
right that look at similar problems okay
so one thing that I haven't told you yet
but mentioned is this thing is in a
stochastic model so what is the
stochastic model so this is not the
usual worst-case model of competitive
analysis so the model we consider is
called the random competition model so
this is this should be familiar to
people who have seen the secretary
problem this in this model is collection
of the sets 80s is adversarial so an
adversary picks the collection of 80s
but the order in which these are
presented to the algorithm is random so
it is a uniformly random order and that
is a random permutation of order and
this is the stochasticity is only in the
order in which it comes there is a
related model called iid model in this
case there is a distribution on sets of
vectors it is not a distribution on
vectors it is a distribution on sets of
vectors and every time this 80 is
generated from this distribution is an
ID sample and now the difference between
these two is like sampling the
difference between sampling with and
without replacement intuitively and it
is known that this is the more difficult
model so any problem that you any
algorithm at work so this also works
with this not necessarily vice versa and
there is also a model with time-varying
distributions this is like the
generation of iid that is not covered by
random competition so random competition
is inherently stationary we can allow
the decisions will change over time but
I will not go into the detail of this so
for this talk I am just going to talk
about the random competition model so
just think of this necessarily known to
be unknown to you all you know is that
they're coming in a random order
okay and how do we measure the
performance of the algorithm we look at
an additive notion which is called the
competitive difference so this is
basically difference between opt which
is if I knew everything what should I do
i would i would pick some vectors to be
so that the average is in the set and
maximize this function of the average
that is opt so offline optimal solution
and you look at the difference between
that and what we get the thing is we
cannot make sure we cannot to make sure
that our average is actually in the set
s but we'll make sure that it distance
from the set is small so we have to it's
like a by criteria we have to relax a
constraints a little bit so both so both
of this will go to 0 as you will see
soon and in the special case so the
traditional thing is this computer ratio
which is a multiplicative thing and this
is what is used in the linear version
for instance and the difference there is
again as ish I said the constraints have
to be satisfied at all times in this
packing problem this is too strong for
the general frame of the left side but
when you specialize it to this we can
make sure you satisfy the constraint at
all times and you can also make sure
that we get a competitive ratio so it's
not like we are kind of playing some
games with this it's just the nature of
the problem because of this generality
we have to do editor so if you
specialize it to the race our technique
also gives you multiple ative errands oh
why is something is is there some
rational f except where it's congress or
its Lucas delipcious okay so what has
been done previously I said mostly
people I worked with the linear version
and this online packing especially and
there are a bunch of Frizzles that let
our duel based so essentially the idea
is to learn an optimal dual variable to
this packing problem and uses duel
variables to do the assignment this is
efficient the sense that it has to solve
a batch LP maybe logarithm in the number
of time steps so log T times it has to
solve a batch LP but every step picking
this vector from the from the soy set is
is fast so in that sense it is efficient
but it is suboptimal so the guarantee
that you get are not optimal and then
there was this paper that i mentioned
earlier that is used so this is
something called the hybrid argument it
is a both efficient it also solves a
batch LP lock times and it gets a
optimal bound for the iid model but for
random competition model it's suboptimal
so we actually really know any bounds
for random foundation so it only works
for the iodine model and then last year
there was this paper by castle I'm at
all that use the primal approach so
there are no duals here and it was
optimal for both the random competition
and ID model but this is really
inefficient it had to solve an LP in
every time step so every time step to
pick this vector it has to solve an LP
and okay this is polynomial time but
this is not something we would ever use
in the IO tracing application or most of
the applications for that matter and so
this result gives the best of both words
and also generalizes to the conic
program so it is efficient and so it is
a primal dual approach we have to solve
a batch LP or a batch convex program
intern in the convex programming case we
have to again solid to lock times but
for LPS we have to solve it only once so
it improves even on these things that
have to solve lock times and is optimal
for both RP and random function and ID
models and also another thing that I
like a lot about this is this earlier
ones all of them the proof is quite
complicated it's not clear what I mean
is not completely clear what's happening
and here we give a very simple and
modular proofs so we have different
components in it's easy to see how this
components interact with each other and
simultaneous or later tour work people
did similar things but this is only
again for the linear version so they
give similar results for the linear
version that match ours and this is also
again a linear version but they have
some local convex concave objectives not
think the global concave object yourself
I have okay so what do we get
essentially we get close to optimal
results we get this compared difference
of 1 by square root T so this goes to 0
as T goes to infinity this is this looks
very this should look similar familiar
to the people in online learning square
root II regret in particular for the
foot objective we get this one or square
root e times Z plus LLS the Lipschitz
constant of the function that you asked
about and Z is another parameter of of
the problem is a problem dependent
parameter that I will talk about later
and for the constraint there is no such
thing it's just 1 over root e everything
all the waiters are in the minus y bar
you see it seems just some universal
constant I don't know why I have order
and seeing here and the thing is we get
even better for some special cases so if
a function is smooth if the objective
function is smooth casually get log
regret and this is not known earlier
this is tight in general even for linear
problems but if the function is smooth
you get log regret and this comes from
the corresponding log regret for
strongly convex functions for ya for
online learning so you get the
corresponding things here and you will
see why we get this because of this kind
of a modular proof so we use this online
learning as a black box and any
improvement in the regret their
translation to improve
yeah so smooth so if so smooth here
translates to strongly convex in the
online learning because that's the
essential conjugate of this so
enlightening will have the fenchel
conjugate of F so smooth becomes
strongly convex there and then it in to
cyllage will be able to see this mean
that the French of conjugate of smooth
Islam is fine but why you get the door
no I haven't told you so you shouldn't
yeah when maybe you can but I couldn't
okay so yeah and as I said in the
special case of online packing we get
the optimal computer ratio that was
known before okay so now to the good
part the algorithm right what I'm going
to do is I'm going to consider a very
simple special case it is a one
dimensional problem okay and there are
no constraints this is only an objective
is all you're given is I am for the sake
of the diagrams to be nice i'm going to
switch to minimizing a convex function
okay it is equivalent so you are given a
convex function ahead of time and that
is all you are given and in the number
of time steps and in every in every
period you see a set of points so again
this should be minus 11 so you see a set
of just real numbers you pick one number
so you see some numbers on x-axis you
pick one and then you repeat and the
goal is to minimize the value of this
convex function on the average of all
the numbers that you picked okay so this
is this is a game and you know we
measure this comparative reference we
compare it to the optimal choice on
hindsight okay so here is an example
this is your given this ahead of time
and in the first time stuff you get two
points you have to pick one
and then next step you get say two more
points to pick one and you repeat okay
in every time step you get two choices
you have to pick one okay and these are
the process are the points you picked
now we're at the end of the algorithm
and we compute the average of these
points look at what the function is on
at this average this is this is what we
got and you can look at all the points
all the choices you had on hindsight
after everything is done and can say
okay you know if I knew everything I
should have done something else and that
is your optimal solution if you had
picked these should have got this as
average and you have got something here
and this should be competitive
difference so earlier we used to call it
regret and somebody pointed out this is
called complete revolutions not regret
so computing the best thing in hindsight
how would you do that doesn't matter
just you reduce exponential time and do
it hopefully this could be done so you
can approximate it very well once again
could solve all like a convex program
and you can round it and it's going to
be very close so but for this sake you
could you might as well take exponential
time and compute the optimal thing
doesn't matter you're going to see how
we're not really going to care about
when we grow the analysis is somehow
going to get away from having to
characterize this exactly so you will
see how it doesn't matter you have that
self as here for the greater so here
there is no constraint this only only an
object a so this is like a very special
case but I just picked this to
illustrate the algorithm main ideas and
everything that I said generalizes very
nicely ok so this choice of the average
and regretful and on your random samples
like if you did the same
so in a random permutation model the
optimum solution does not depend on the
stochasticity because the order does not
matter in the iid model it would matter
then you take the expectation of the
optimum ok so here is algorithm in every
time step the algorithm is going to pick
a tangent to this convex function so
this is a tangent i am going to
parameterize the tangent by the slope so
the slope of this tangent is theta 1 and
if you fix it I when you think of this
as a linear function of X and I am going
to pretend that this is a function that
I am really optimizing I'm going to
forget about edge I'm going to just try
to optimize L ok and in one dimension it
just means that if the slope is negative
you pick the point on the left the slope
is positive or whatever the vice versa
slope is positive you pick the point in
the left because I'm minimizing pretty
negative if the point on the right but
can you imagine in many dimensions this
is going to be non-trivial right so you
know in many dimensions I'm going to
have a hyperplane I am going to have a
linear function and given all the points
the choice at ATM going to optimize this
linear function over the choice ATT so
here you will always choose either the
left most of the right post point yes
should it be clear to me that is a big f
function right notice that the offline
is take the offline problem not often
this is the algorithm now I understand
but yeah now go to the hindsight you
have the awesome problem yeah the one
problem is the original problems to the
optimal location the second one is to
the optimal location but you're
restricted altogether oxidize left or
the right one yeah not even there again
there shouldn't be because as of this
should be clear to us wildness it
shouldn't be clear it's not clear a
purely that's a very good point it's
only because because this works they
shouldn't be again because here in this
case angular ms only picking one of the
end points where it's a convex function
so
yeah that's a very good point it's not
clear a purely that you can restrict the
algorithm to do one of the two end and
get something close open yeah it's 80 is
always you know zero and next stop oh
wait for the x 0 x value is I mean maybe
you are messing with do something soon
is yeah Ferguson we do something smart
you'll just choose X star ability no no
no no no no what they try at the
beginning equal to zero I mean we'll
take you to offer just enter the open
question you know everything yeah what
are we stretches engine stuff to do
three points you have to have three
points or more than two points for this
means there are only two points of
course you're either picking the left
mouse or right one so you have to have
three points or more for this to be
non-trivial so you can imagine actually
that if you have many points and maybe
the points in the middle you always want
to keep picking the points in the middle
this algorithm will keep picking the
extreme points and it started all clear
why this could do as well as the point
in the middle I mean that's a that's a
very good point it's not real clear so
only through the algorithm that you know
it has to be great are you looking at
this is the current average no I am I
have I have somehow picked theta one
that's so and I'm looking at the tangent
to H with slope theta 1 and think of
this as a linear function of x and i'm
going to pick the point that minimizes
this linear function is your card
effects oh so now I'm going you know
think of this as a linear function of X
i have i have three two choices which
one minimizes genie is there is the soul
of the current it's the slope of my
current linear function that i'm going
to pick this on this way how do you pick
it yeah okay yeah oh yeah first time I
pick some more okay so i just picked the
one that minimizes now how do i pick it
in the next time step i'm going to use
this online learning as a black box i'm
going to feed what
happened essentially this XT to that and
that is going to spit out the next slope
ok and I'm going to use that slope now
so it spit this slope and now when i get
these two points i'm going to pick this
one ok so very big one so okay I tricked
kata one somehow arbitrary it does not
matter and that told me to pick this
point ok now I'm going to feed this to
this online learning black box exactly
how this thing is going to work I will
tell later but some black box I'm going
to feed it and that black box is going
to tell me what to do what slope to pick
next so it's going to be it's going to
say ok pick the slope now I'm going to
next pick the tangent with this slope
right and i am going to repeat so now I
get these two points and this thing says
I should pick this one so I picked this
point then I'm going to repeat it and
then I again go back to the black box
that's going to tell me what the next
book should be so now you see where the
venture duel comes in there's something
alright we keep saying is changing yeah
as a very specific so actually the
y-intercept doesn't matter because movie
does not matter for the choice but it
matters what analysis oh yeah for the
choice it doesn't matter night if you
move the tangent function up and down
the y intercept is the use like the
phase where the fenchel conjugate comes
so that's essential conjugate opposite
you look at where a wine with the slope
touches yeah you'd usually there's an
explicit form Lloyd so this is always
going to be the essential conjugate of F
star of theta so it's just a set of
theta plus X times theta the black box
takes is just this fall already yeah so
yeah it takes this XT is all that
matters really oh oh
assuming it knows H it knows H it takes
I'll tell you what the exactly what the
black boxes Gator think of it as a black
box and it uses this is a black box and
is inside another black box of online
learning so think of it like that you
find some kind of reward yeah I use this
to define some kind of reward in that
time step and and that's going to tell
me the next one the evil the right past
include various lows also the previous
text email first yeah I'm going to tell
you exactly what that is so for now you
know wait have some patience for that I
will tell you about it you know it's
coming soon okay the algorithm is clear
edges some black box that keeps updating
the theta and that uses online learning
what I not told you is how to define the
reward ok so that is algorithm every
time I just do this repeated that's how
I pick the points ok so we are done this
is average and this was a optimum on
hindsight and this is computable friends
so now what is happening so here I want
to look at the average of the linear
functions that I used ok so i use this L
of X comma theta T this is what I
pretended I was optimizing and I want to
look at what happened you know in this
case and what I am going to argue is
this is an lower bound on this H of X
star ok and then now I only this is an
upper bound on the computer completely
dif so i only have two bound this gap I
really do not care about what extra
lives ok this is assuming that this is a
lower bound it is not clear why this
should be a lower bound ok so this is
what is this gap this gap is the actual
function H at my average so this is the
actual function that I should have been
minimizing and this is my
retains function that I was pretending
that I was minimizing so what is
difference between the actual function
and my pretense that this is a gap that
I have to pound and this gives me a
bound on the computer because you
dependence know the lower bound uses
stochasticity it is not true always
because I want it to be extra you will
see a little bit i will show you why
this is a lower bound it is a tangent
yes but there is more to it because this
is only true so now think of I said you
think of random permutation but for
moment this holds actually only for the
iid case ok so think of your drawing
think of the empirical distribution
right you've seen everything and now let
us say you draw one of them at random
you think of you are sampling from the
empirical distribution and it is
considered the expectation of this draw
expectation of this L of X T comma theta
D ok for each sir each you know each
pair the one that we picked minimized l
and so it is smaller than the one that
would have been picked in opt so x star
is an average of the optimum choice so
here's you know because they are
optimizing for L we pick the best one so
it is better than the one in the art and
that is why this expectation of this is
less than L of X star of theta T you're
so as these your choice
what is this expectation hole this
actually your choice of XT is my choice
activity seems like you show without the
expectation gives you this piece so
actually okay so why is it not true
without expectations you're just saying
XD is optimal for 30 so it's better the
next door thank you so actually hear
what it should be is necessary extra is
is an average yourself yeah see so if
you fixed set a tee and now you are
picking these pairs at at random and you
are picking the optimal one so for every
pair what you would pick like point
where is your better so here in this
when you take the expectation you just
get X star this is this logo which runs
oh so this is you fixed it a tee and you
pick expectations / you pick the 88
random the which set your optimizing so
this holds for the iid model because
there every time it is a random sample
and the this is exactly the difference
between random competition and I ID so
for random pump tation the expectation
is going to be slightly off and what we
need to do is we we have to look at what
the difference between the expectation
for random competition and I ID is
cumulative over all the time steps and
that's exactly we get an extra error
term because of that and and that is
also like 100 root T so everything works
out so this is exactly the point where
random clean and ID differ so this was
one of the things where we didn't
understand okay why do they differ the
early analysis was didn't throw any
light but as whereas here we capture
exactly where the two different Ruby
also we can lock to your key for a
smooth yeah no no actually for a smooth
version only was riding because this
rooty is going to be there raya so that
only works when i disprove here
this is the only place where you will
use a stochastic model yeah you could
also say that eighty could be completely
at their settlement you just want X out
to be in the convex hull of the ati 30
times that it was true them yeah yeah
which can be yeah this is only step of
users to catch the screen and yeah this
is because they are using a tangent but
this step is clicked on non-trivial step
where we use this recipe okay so that is
why in expectation this is a lower bone
it's not true point wise you know some
point it may not be but in expectation
is true you can also convert it into
high probability so it's fine okay so
the summary of all this is we can forget
about X star we just have to bound this
cap now the gap between the H at the
average and the average of this linear
functions okay and this is there okay so
I already give some of the proof so this
is where online learning comes into
picture okay and what is online learning
I think most people here know but let me
make a let me do a quick summary you
know you want to make accurate
predictions using what's happening what
happened in the past so at any time T
you want to predict some vector theta T
in some domain and after you predict
this you get a reward some reward LT
sympathies has a concave function and
you want to maximize so here again these
are reward vectors you want to maximize
them and the regret is if you knew all
the reward vectors then what would be
the single theta that you pick on
hindsight versus the Thetas that you
pick the reward of the cheetos you
picked online okay so this is this is a
problem this is online learning problem
so notice here that these are local
unlike in our case where things were
global so this is only some of some of
TLT right and there are many algorithms
available
here that essentially get this one over
square root irrigate so here i defined
everything in terms of average so that
becomes 1 over square root tea okay so
how do we use this in fact it turns out
the wave have set it up the gap we had
to bond is exactly the online regret if
you define the this else correspondingly
accordingly okay so
ok so the reward that we use is this is
this tangent ok so now think of this as
a function of theta earlier we were
fixing theta and we're think of it as a
function of X but now i am going to fix
xt and think of it as a function of
theta so this is a convex function and
this is a reward as a function of theta
that i am going to feed back into the
online learning ok so what what is this
function if you i fixed xt and as you
change theta the tangent changes and it
is a value of the tangent at XD so this
is this is a concave function because
here we are maximizing gotta send ET
right what ya you x rays what I picked I
picked it I picked some some point X T
now I have to go to the online learning
and define what the reward function is
for this time step for it to spit or the
next one right and what is the reward
function it is a function of theta right
now fix XT and look at this L of X T
comma theta that is a function of theta
tiny mean confusing me to fix the code
change amitabh 2x Israel you have the
slope and at some points if I tilt it
sideways and it's another experience no
no excuse whatever I picked in that the
vector that I picked in the last time
step so this objectivity plus 1 this is
X T dagger oh I think I'm doing so my
angers and pick the particular XT in
time step T so here you pick utility
plus warm in here i'm picking 1080 plus
1 yeah ok the algorithm picked up
particular xt now i'm going to use this
XT to define the reward for the online
learning okay so people had questions
about what this reward is what it uses
so this is what it uses so if you fix xt
that defines a function of theta which
is you look at the tangent with slope
theta and look at how much this tangent
is at this xt
and and that is that is a reward and
this is a concave function again sure
okay so now what is this online learning
regret so it is a reward of this optimum
choice on hind side minus whatever the
algorithm a plane so this is exactly the
second term you know in our gap this is
exactly the lower term right that we had
and I'm going to show in a moment that
this term the optimum on hindsight is
going to be exactly this H of the
average ok so this regret is exactly the
gap that we wanted to bond why is this
so note that L is linear in X ok so for
any theta so theta star look at the
optimum one rate or any theaters now nu
theta this average I can just take the
average inside because L is linear in X
it is a linear function right if you fix
theta okay so now what is this function
this is you look at the average and for
any theta you look at the tangent which
sloped theta and what the tangent is at
this average now you want to pick the
theta to maximize it so what would you
pick you would of course pick the
tangent at the average itself right
because if you pick anything else if you
would only get a smaller value so
obviously the best thing to do is to
pick the tangent at the average and then
you just get the value of the function
yours streets you so now it is non zero
slope no no this what is this function
this is this function is you pick any
tangent look at what the value of the of
the tangent is at the average that is
this function focus for either has to be
I mean this is this is my definition of
L right well L of X T comma theta as a
function of theta is if you take the
tangent which loop theta
what this is tangent what is the value
of this function at X T for Kovacs yeah
yeah convex sets of course right so this
this best thing on hindsight is exactly
h of the average so this regret is
exactly the gap you wanted to bomb okay
so that is that said so note that there
is a switch here in some sense this
thing the best thing on hindsight is
somehow the performance of the algorithm
right how much algorithm got and this
online learning thing is somehow the
lower bound on opt so there is a switch
which is interesting but maybe it's
natural i do not know okay so here is
the overall algorithmic approach the
same thing extends to many dimensions
now consider the case where like in the
special case you either have only an
objective or only a constraint so if you
have only a concern it's like having
only an objective which is just a
distance percent okay so if you have
only the objective then many dimensions
you do exactly this and so these Thetas
are are the essential conjugate so these
status give you some kind of linear
function you can interpret it as shadow
price and so on based on the specific
scenario and and all you're doing is
you're picking you are optimizing this
linear function given by theta over the
Streisand okay and then you getting this
and you're feeding this to the online
learning you defining the reward
appropriately you know it is the same
thing you know you take the tangent in
this dimension and then it is a linear
function or sorry it is now still a
concave function of of the Thetas and
and that's going to tell you what the
next theta next slope should be and then
you repeat this it's the exact same
thing that I said works in in any
dimensions use all this for finding
theta 1 so okay in this case when you
have either only an objective or only a
constraint I don't have to solve the
nail piece I have to solve LP is when I
have both even in the earlier work we do
not have to solve any LP if there is
only an object you are only considered
right ok so now what happens when you
have both objective and constraint so we
go to run two separate instances of this
war actually in the previous line ok so
for the object if I am going to have
this theta as for the constraint I'm
going to have the lambda
but now how do i pick the vector i am
going to combine these two and i am
going to combine just using a value z
okay so what is this z it is a trade-off
between the objective and the constraint
it says if I violate the constraint by
Epsilon how much does the objective
change so opt of epsilon is I take an
epsilon ball around s so I take all
points which in epsilon distance around
s and look at the opt of this relaxed
problem that is opt of epsilon opt of 0
is a storm and Z is this derivative of
opt of epsilon with respect to epsilon
so this is what I said it known if I if
I violate the constraint a little bit
how much should i expect to get in the
object and you are assume that you are
given this V ahead of time or you can
estimate from a sample so this is where
we need to solve like a batch LP or a
batch connect program and the thing is
we only need a constant factor
approximation to use so we do not need
very accurate estimate so we need a
constant factor to Z so it's it's much
easier estimation problem okay and then
what we're going to do is in every time
step we're going to combine these two so
we have the theta and then we have the
lambda you're going to combine them
using the Z so this is again a linear
function and then we can optimize this
linear function over the same okay this
is also the reason why these often great
example is a small sample you get a
constant factor anyway so they go to
sample very good point so for the LP
version you only have to solve it once
to get a constant factor small service
yeah small sample using now and only
once is enough to get a constant fun so
like I just need epsilon squared order
epsilon squared fraction of all the
requests so if the rhetoric plus I take
epsilon square t samples and that
already gives me a constant factor
approximation k so I only have to do it
once
so why does this work okay so analysis
is a little more complicated and I do
not have the time to go through this but
this works okay so let me summarize here
are the I think salient features of this
algorithm it is very general works or
arbitrary essentially arbitrary convex
constraints and concave rewards if you
need lipschutz continuity and this
fenchel duality I didn't say things in
terms of finchel 20 but that is what is
underlying it it seems like the very
powerful or at least the right tool to
attack this problem and for a long time
we were struggling with this random
permutation our society we you know it
should be the same or very similar but
we didn't really have a good explanation
this captures exactly where this comes
so iÃ­d the expectation is the same
throughout random foundation expectation
is changing and it's just a difference
so you know if you take the random
permutation in every time step you take
the expectation of the remaining and you
see how different it is from the
expectation of the whole you sum it up
and that's exactly the extra term you
get for random competition so we mean
really captures the difference between
the two and the other nice thing is this
modular proof where we use this online
learning as a black box and we do not
really care what algorithm you use for
the online learning earlier problems
were very specific to and either even
the other work that I mentioned for
simultaneous or later they were very
specific to the particular algorithm you
used to solve online learning but here
we show that ok does not matter all i
care about is it has no regret and this
was actually conjectured even back in
two thousand seven when metha at all
came up with AdWords problem they said
oh there should be some relation to this
experts algorithm and people all the
subsequent ones hinted at this
connection but there was no formal
connection and we make the first
connection and the other progresses
beyond this any any other advantages if
the learning problem can you can get
better regret and that translates to a
better compute
reference as we saw with the smooth
functions and of course this is only
limited to iid interessant you know so
random permutation this extra term is
already won over 50 so it and children
okay I think this would Lou here one
direction one yeah it could be useful is
to go beyond these stochastic models
that we have so we have this ID random
competition we have some time change of
distributions but these are mostly
stateless so maybe some kind of
Markovian process or something you know
might be able to extend it the other
problem is so this is a scheduling
version of this which does not quite
fall in in the central framework that I
mentioned and I think figuring about
similar algorithms in similar guarantees
the scheduling would be very interesting
we're working on this with Paolo and
others and for me personally another
thing I would like to see is more
applications of this so it seems like a
very general framework so I am
interested in okay what are the other
problems that people have worked on that
we can formulate in this framework maybe
that will point to some some other ways
of extending the model so maybe they
won't fit in exactly but it is close
enough that we can extend it so thanks
questions can you go the other way I
give the recognition like you're
supposed to do some father's work it's a
good question um maybe just like you
just told us is duality possible I
wouldn't rule it out I actually haven't
thought about that but it is definitely
possible there's some reason for coming
and in the worst case analysis online
algorithms convallis course yeah how are
these algorithms related I mean I know
this the problems are a little different
the resulting algorithms are there is
similar not sure I think the others are
the similar robbed of an art the problem
themselves are different so yeah there
you have to keep covering at every time
step so you have to maintain so that
makes a difference villains so you have
more freedom intuitively slowly yeah we
only have to satisfy it at the end so it
yeah I can say we have more freedom and
also we are crucially using the
stochasticity maybe not so crucially as
a sec mention yeah actually that's a
good point oh you can just 82 the
cognition yeah but also this only is it
this it holds only in expectation in
some sense so it doesn't follow that
condition may be too strong right it's
like two statements either this or that
hello you can see this I mean that you
have a distribution that the thigh
assistance distribution could be giant
brother yeah yeah okay okay yeah yeah
that's a good one you know it's a
factory and seriously with the factor is
to be sausage so this is what i'm saying
is actually very similar to this time
varying distribution model that i
mentioned but again say it's very
similar to that
in some sense that's what this time
varying distribution does still each
step has to be independent of other
steps but that distribution in some
sense has to include this X star Y dude
is offside so yeah so even saw the
linear version this one over root T is
time for unconstrained as well yeah yeah
unconstrained so you can I guess this
lesion Rajan you can just think of it as
a distance from the set kind of thing I
think I don't know value even for just
minimizing a function yeah I don't know
just but it is for the packing problem
you know you have a linear function and
we have packing constraints it's tight
ferences for smooth functions it's not
when you can get this locked earring so
for instance no very patient ok that's a
good it's a good question so the thing
is even though it you know it's only a
linear version that we saw and the rest
is hack the algorithm works very well so
it works so well they're so happy with
it that they are not developing an
algorithm anymore so they are focusing
on other things so currently they are
happy with the algorithm as it is so
maybe at some point of time they come
back to it and want to revamp it maybe
then we you know get them to use this
but as of now you're quite happy with it
so right now
yeah yeah don't need for solvent can it
be you're not talking and I mean
reduction so yeah they are solving an LP
the soldiers LP offline and then that
kind of feeds the online the online they
don't have any repeat justice often is
big coming you know what they are below
they use so actually they're using the
same anchor them what tools are used so
they're using the same algorithm when
you have to solve it offline you can
shuffle the order so you know you're
getting a random order so in that is all
that yeah that's what shattered this
well I think there's another interesting
model so in your ear of course your
balance provocas fun in terms of T goes
in pinning your you have this is fun but
the thing about being variety is just
not a really English faint but that
means you're 80 is actually as is a
finance is someone from a fan aside so
if you someone from finance side we
start distribution then they you will do
is well over T something you re going
the suitors same things repeated can you
some learning and BK you gain some film
you go with it yeah in the same requests
I can assert time but that's the thing i
don't think the support of this
distribution is smaller right it's a
distribution on sets of vectors and the
support of this distribution I don't
think is small I think the support is
much bigger than T some sense that's how
I was more to it right or at least as
large st this first one if the support
is small then yes you could you could
actually learn the distribution and and
do something simple but every impression
is essentially different in so a so the
support is huge and that is why we need
to do something other than learn the
distribution
see you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>