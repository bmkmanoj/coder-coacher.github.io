<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning to Construct and Reason with a Large Knowledge Base of Extracted Information | Coder Coacher - Coaching Coders</title><meta content="Learning to Construct and Reason with a Large Knowledge Base of Extracted Information - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning to Construct and Reason with a Large Knowledge Base of Extracted Information</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KFwLcbttFR8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hey so I'm rich Caruana it's my pleasure
to introduce William Cohen who's going
to give a presentation this afternoon so
let me just give you a little background
on on Williams so let's see you're
currently the president of IMLS right
the international machine learning
society you've been an editor or an
action editor for like all the journals
basically so so for many of the machine
for many of the machine learning
journals let's see icml your general
chair once and program chair twice two
different ice emails I was gentle share
once program sure once and then once
when it was like just one kind of
combined okay back in the 90s gotcha
yeah so um and twice you've been chair
of the AI conference on web logs and
social interaction and so one of them
just happened yep got months ago right
so Williams also and I a triplet I
fellow and then you won the Sigma award
right for the test of time for a paper
you did back in 1998 I'm guessing but I
don't know what paper that is yeah it's
the only sigmod paper I've ever
published so they gave me an award 10
years later so yes really sir ok so it's
my great pleasure to introduce William
and I think his his greatest trait which
he just reminded me of a few minutes ago
is that he follows in my footsteps so
twice William has moved into offices
that I left once it just research in
Pittsburgh and once in wean hall at
Carnegie Mellon so tonight I think
that's why so successful that's right
that's right it's all the aura right and
then after that actually moved into an
office which had been vacated by
Sebastian throng who's also gone done
pretty well so you know ah yeah okay
cool okay so I the work of want to talk
about is joint with a whole bunch of
different people so Tom Mitchell I guess
is really one of the driving forces
behind this Nell project and kind of
here's a rough outline so when I found
out I was going to be talking this late
in the afternoon on Friday I tried to
cut things back a little bit I still
tend to kind of pack a lot of things in
but don't let me kind of do too much um
stop me when I spend a couple of minutes
talking about this never-ending language
learning system and just some of the key
ideas in it and maybe give some a little
idea of what it is and then most of the
time I'm going to talk about the problem
of doing inference particularly
inference in this system which is it's
kind of very challenging because of
course this is all constructed with
information extraction techniques and
there's noise and redundancy and and
completeness and so on so i'll talk
about two projects for that one is a
joint work with one other people William
Wong who is an intern here with Eric
orbits until recently and another one is
promotion as inference which is joint
work with Lisa couture who's going to be
here at the faculty summit if you want
to hear more about this later I think
she's talking about this and then I'll
kind of wrap up very briefly so like I
said this is kind of a key part the rest
of this is just sort of Prelude and
let's go ahead and move in so
information extraction I know I can't
see everybody that's going to be
listening to this talk there's some
people are listening online but how many
people know kind of roughly what
information extractions you've heard
about it yeah okay all right so this is
about the right level of detail so it's
basically extracting facts about the
world by reading text ok so now like
many information extraction systems
basically it's based on learning so it
learns how to recognize facts in text
and then it will aggregate the results
okay so it doesn't actually do any
detailed parsing at this current time so
it doesn't actually do a lot of deep
analysis of any particular piece of text
the key thing is that sort of by
aggregating even sort of relatively
shallow
tools for recognizing facts the
aggregation the fact that you've got big
data will kind of help you get good
results so Nels a semi-supervised system
so it doesn't need a lot of labelled
training data and label there so it
learns about five or six hundred
concepts in relations so it's a closed
set but a fairly large closed set and
for each of these things it's got a
handful of examples say 10 or 12
examples for each relationship like
capital city of and each concept like a
person and it also know something about
the relations between those relations so
it knows for example that if your
celebrity you're probably a person it
knows that the first argument of capital
city of is a deal geopolitical location
I know someone knows something about
things that are disjoint so it knows
that political a location is not the
same as a person and it has a bunch of
web pages about half a billion web pages
uses to do with information extraction
also those live queries it's been
running for about three years okay it's
got about 50 million candidate
extractions which we call these beliefs
there's about 1.5 million high
confidence ones about eighty-five
percent of the high confidence police
are correct and a picture being worth a
thousand words here's kind of the knell
site and this is just a random sample of
things that it's learned recently so
recently means sort of in the last few
iterations right so in last three almost
three and a half years sort of gone to
about 700 odd iterations and knows
things like Tim Taylor Stanley is a
person a Canadian person in fact special
subset of person okay it thinks there's
a restaurant called fix in Las Vegas
anyone know that's true no idea okay I
karnataka bank limited as a bank in
india sounds right and of course it's
got some snakes those got iraq which
thinks that's a name of a country
okay so i could actually go in and sort
if i want to i could sort of say that's
really not quite right here's another
subset just kind of at random let's see
beautiful too khoa river is a river new
jersey turnpike is a street in the city
west okay that's probably an error and
so on okay we could also go in and just
look at some of the things that's
learned I so um so since we have an
expert in ornithology here I'll look at
some of the birds so I so it's got a
little gloss for each concept and in
those as I said something about the
relationships between those things and
it also knows something about how to
recognize of instances of birds so some
of these are features of the noun phrase
itself okay so you know to take an
example if it ends in HR ush like thrush
or RB le are like Warbler it thinks
maybe it's a it's a bird or here's
another one is like eat like parakeet I
guess right and then there's a bunch of
patterns like blanks fly abundance of
migratory something African waddled
something aquatic birds such as
something so each of these are sort of
kind of shallow syntactic patterns that
will suggest that something is a bird so
these are all patterns that it's lose
don't let its learned over time I'm just
trying to figure out there's any more
interesting things here let's maybe
let's go back and look at a particular
bird cat among
so so it must be doing deep learning as
well hahaha right so let's see we've got
I don't know let's say a blackbird so
here the bird the patterns that were
used on blackbird it also looks at
semi-structured web pages and tries to
extract information out of
semi-structured web pages so for example
there are um let's see here's a list
which it found you know apparently
several birds in I don't see where that
is but maybe it's in this list of
related species right here I another one
it found is this one right here which is
kind of amusing so it's a list of birds
from Vietnam from Wikipedia so somewhere
down here it so there's a list of birds
and it sort of figured out how to wrap
these so so this is basically what what
Mel is and what it looks like I now will
kind of switch back to talking about how
we get there skip over this here since
we've have done this so what are the key
ideas in the system ok so like as you
can sort of guess from the list of like
people have worked in the projects big
system the two key ideas I'd highlight
our it uses coupled learning and also
has a multi-view multi-strategy learning
approach so what's coupled learning so
let me give you an example of uncoupled
learning by contrast okay so let's think
about these lexical patterns it learns
like blanks fly to recognize birds so
how does it start so how does it work it
looks at some initial set of seeds and
then it looks for patterns that are
supported by those seeds ok so for
sadies you might come up with something
like this right and they'll come up with
other things that sort of match those
patterns so mayor of San Francisco might
appear in textin we're live in denial my
purim text and then you learn those then
you bootstrap and learn some more
patterns then you learn some more things
and what you see eventually in many
cases the semantic drift you'll see a
drift toward things that aren't real
like the original seeds if you gave it
so one problem with this is if I just
think about one concept or even a small
number of concepts its kind of under
constraint right we don't really know
why those things are wrong and the
reason those things are wrong is there
really an example of something else so
if you have a whole bunch of something
else's then it makes it easier to learn
any individual concept so the idea is
kind of paradoxically you know sort of
easier to do like 200 things or a
thousand things at once and to just
focus on one if you're a machine
learning system so again if we're like
trying to learn the concept of a coach
right so there are a whole bunch of
other things that are related like
coaches and athletes and teams right and
we know that you know teams are
different from sports and they're
different from people write a coach
coach is a team so there's a connection
between that concept and that relation
right and and they're also there are
other sorts of things like if someone is
an athlete they're probably a person
they probably play for a team and so on
okay so if we look at all those
constraints that gives us a lot more
information about what's not true as
well as what is true it so by pushing
one concept learning problem off against
the others you get more accurate results
and less drift so this is kind of one
key idea it's easier to learn lots of
interrelated tasks than one isolated
task the other idea is it's easier to
learn using many different types of
information so I sort of suggested that
when i looked at the demo so it learns
rules about recognizing birds based on
where those phrases co-occur like do
they appear in a pattern like you know
puffins fly and also on the actual
structure of the word and also on how
they appear in semi structured pages
like the list of birds from wherever it
was Vietnam okay so they're all those
are all different views of the data okay
so that's the other ideas that you have
this multi view multi-strategy learning
system and sort of the big kind of heavy
lifters and null is um finding these um
text extraction patterns these like
lexical patterns finding patterns that
are based on individual web pages so
basically sort of page specific wrapper
from semi-structured pages these rules
about the you know prefixes and suffixes
and morphology of the noun phrases okay
and then the last one is inference rules
okay so this is just another component
of null it will also basically do data
mining on its learn facts and come up
with plausible rules by which you can
take those learn facts and for new facts
and you know the cycle is basically you
take your initial seed set you apply to
the data and get some examples you do
learning in each of these spaces you do
classifications use these spaces to come
up with new things you combine the
results you get some more facts you look
back so let's talk a little bit more
about this inference process okay so
talk really about two kinds of entrants
we're doing and one is basically
inference sort of in that little box
another straight learning strategy just
sort of another way of coming up with
facts that are probably true likely to
be true that you could somehow derive
from what you know already so I just as
a little bit of background I'm going to
talk about a gem more general problem
which is learning in graphs then I'll
talk about a particular algorithm for
learning and graphs and then I'm going
to kind of go off on a tangent talk
about sort of the latest stuff I've been
doing which is sort of like I extensions
to this last bit okay question yeah the
the inference stuff does that for
so you have to give it an ontology does
it stay with the mount ology or does it
propose new concepts or relations that
seems to be miss it stays within the
existing ontology so it's coming with
new facts in the existing ontology okay
yeah all right so um so a while back i
sent a intern to microsoft research and
in at minkoff was what's your name so
she was working with me on this kind of
nice problem so if you think about
information management let's say
managing your email you know a very
simple way of viewing it say well we've
got two types of things we've got
documents and we've got terms you can
organize them into a bipartite graph and
we can do like I are or something like
that a more complicated way to say well
there are lots of other things they're
like people their dates their meetings
you might go to their email addresses
and these things are all connected so
there might be you know people that
attend meetings that are associated with
email addresses and so on and so forth
okay so if you take that view if you
think about you know the your world of
personal information as a big graph with
lots of different entities and lots of
different types between those entities
you know the question is what can you do
with that okay so so a map came here and
work with toot innova who had done some
work as a grad student on learning
parameters of a certain kind of graph
traversal process so this is
personalized page rank so guessing most
people know about page rank so a
personalized page rank is essentially
the same process you have sort of a
random surfer it's going to the graph at
any point it either takes a random arc
out of that graph or it jumps back not
to any random node in the graph like you
do in regular page rank but to a
particular personalization node okay so
the end result of that is that you're
putting weight on every node in the
graph and the weight depends in some
reasonable way on the distance or the
similarity of that node to the start
node okay so once we have this
similarity metric for nodes over a graph
and once we've got some ways about of
taking that and tuning it for a
particular application then we can talk
about
solving a number of interesting kind of
information you know needs by doing
simple similarity queries so the
simplest kind is basically you give me a
type let's say email addresses and I
note index okay and I want to find other
nodes of the right type that are similar
to that okay or a slight extension would
be Hilly's give a type and a set of
nodes find things of some target type
that are similar to the set of nodes so
there are lots of things you can do like
this so let's take one example so let's
say we want to find out what the
referent of that name is ok so I've got
two objects I've got the name itself and
I've got the context the name appears in
which is a message so my start query is
um are these two things the term Andy
and the message ID of the file that
appears in and what I want out of this
is a person ok so maybe a contact record
would be a rien a person alright and so
this is this is by similarity query and
if I start off without doing any
training I'll get some level performance
it turns out to not be terrible in many
cases and then I can do learning to sort
of improve it and kind of tuned it to
queries of this particular type so if I
have a bunch of disambiguation queries I
can tune that relationship for that task
and we look at a bunch of other things
like email threading and finding aliases
and finding email addresses of people
that would likely to attend the meeting
object and that worked pretty well so I
subsequently I had another student that
came by Sony Lao and he looked at
basically the same problem the same task
of given a type and you know a query set
of nodes finding related things ok and
his contribution was basically a look at
new and better learning methods ok so in
particular the stuff that amet was
looking at there were a limited set of
parameters which were basically related
to the edge types in both entity types
that you had so he looked at some new
tasks one of them was basically
information management for scientists
and the language that he used was a
little bit more expressive the
representation that he that he learned
was a little more expressive so here's
sort of an example of what the sorts of
things his system learn so so
essentially what you're learning in each
of these cases is a similarity metric so
you can solve some particular class of
queries better alright so the similarity
metric is basically some strategy for
doing random walk on the graph and so
for me what you basically had is you had
a whole bunch of experts each of which
meant you were following doing a random
walk across edges of a certain type okay
so for example you might start with um a
word and let's say so here we're
starting off our query is the set of the
title of a paper the year you're going
to publish the paper maybe the authors
of the paper and I think there were some
entities like gene entities that were so
she with that paper so you start off
with this meta information and the goal
is to find the papers you'd like to cite
all right so there's lots of data you
can get for this you assume citations
are mostly right and you learn past like
this when it says start with our word
okay and then find a paper connected to
that word so we're starting off with the
initial uniform distribution over all
the words and then we're taking we're
taking a random walk from that to a set
of papers so I have a paper that's
linked to buy a bunch of the words in
the title then it will get more weight
than one that's just linked by one if I
have awarded the title like a Z then
it's going to be linked to a lot of
papers so its weight will get dispersed
whereas a rare word will get dispersed
less so going from word papers is kind
of like traditional I are in fact this
is one of the rules that gets learned is
basically sort of you know type the
title into a search engine and those are
things you might cite or in another or
you could basically go from there and
find things that site that paper and
then are cited by those papers okay so
that's sort of a co-citation rule so
Sony's system basically the structure
search and enumerates a large number of
paths of this sort and then it does a
linear combination of these things
it's basically finding waits for the
things that come out of each path so
each of these passes a random walk so
it's kind of smart about what it
generates and then additionally we're
going to put some weight on it that
makes the ensemble even smarter so
that's kind of where we we are and here
are a couple of other examples of things
so this is five papers cited in the last
two years this is one these are some
that are negatively waited so if you
just like look at recent papers then
generally you don't cite those as much
as other papers apparently okay so this
was knees system so I was talking about
this with Tom Mitchell and he suggested
the following idea so think about
applying this learning system to
basically predict relations inside the
null knowledge base okay so there's a
relation like like athlete plays in
league so this is in the ontology you
know you want to predict it and it may
be the case that there are chains of
relations that will get you there
reliably so for example if I know that
somebody is a athlete and they play for
this team the steelers ok and the
Steelers play in the NFL ok ah then uh
that chain may help you predict whether
or not leak please leak alright so the
question is can you use this same sort
of path learning to learn a similarity
metric which basically says I here's how
I I can infer you know the second
argument of athlete plays and lead from
the first argument so that's the task
we're trying to do basically it's
learning a binary relation in the
anthology so we try that actually works
pretty well and kind of interesting to
see the sort of things that it finds so
one is this athlete plays in the league
and you know the league is part of some
organization so let's see well as I
don't remember quite the semantics of
this but it's basically sort of like
this this this link right here ok with
nels actual concept names here's another
one which is kind of cute you start with
your at your athlete ok
so I start with an athlete and I go to a
concept that he's a member of so well
maybe that's just a node that indicates
the concept athlete in this graph and
then you find all things that are
instances of that so now I have the set
of all athletes basically uniformly
waited all right and now I want to find
out the sports those athletes play so at
this end of this chain I basically have
a distribution over sports that's waited
by the number of athletes that play that
sport so this is basically sort of a
prior distribution over possible fillers
so Sony's algorithm learns a lot of
rules that are kind of like this it
learns things that sort of give you
prior information or learn some things
that are like that looks like inference
rules it learns other things that look
like they're basically sort of trying to
get around holes in the knowledge base
so so here's one I want to find out the
the home stadium of a team's the way to
the seahawks play I can start with some
member of the Seahawks find out what
team they play for it now that really
should be back to the Seahawks okay but
it might not be because I may have to
you know duplicate concepts i may have
different versions of the same entity
that because of d duping and Nell is
imperfect okay and so then I can find
the home stadium of that team okay so
that's a pretty good inference rule if
you have some you know extra
unduplicated entities so it learns a lot
of kind of interesting things all right
and you know pra has been now for a
while sort of one of the components
Adele okay so that's where we are and
now i'm going to talk about sort of the
next stage so this is very recent work
it's a I guess sort of recently appeared
in a workshop at icml and there's like
longer virgins under review so this is
actually kind of cool stuff that I'm
actually kind of weren't interested in
this than I have been for a while about
anything for a while so when he was
around we were talking about a number of
possible extensions so some of them they
did he did for his thesis some of them
seem very hard to figure out how you
would do so one question is can you make
the rules at pra learns in this
knowledge completion setting recursive
okay so after a while we were thinking
about we came up with this idea maybe we
can use those ideas from pra in a more
powerful representation and sort of you
know jump by and get sort of a whole
bunch of these extensions at once so
just to give you a sort of a concrete
example again alright so for pra what it
learns these paths really look a lot
like logical inference rules okay which
is why I'm calling this inference in the
knowledge base okay so you might write
this rule that sort of takes you from an
athlete to a sport by saying well you
know if you're on the team you can tell
on the knowledge base that this athletes
on a team and that team plays that sport
then the athlete plays that support you
okay so that's how doubt that's that's a
rule okay and you know you're you're
asserting something new for you set of
rules based on things that you already
know in the knowledge base okay and you
know of course you're doing this for
lots of different relations so you do it
for athlete play sport you also do it
for a team play sport so might learn a
couple rules with that right and one of
these might call the other right so this
one calls this guy right you might
conclude the team plays a sport if you
know the athletes on that team in the
athlete place that's where all these
like relations are kind of by design
closely interconnected and redundant so
pra can learn this okay but it couldn't
learn something where these rules
actually get called you don't a mutually
recursive fashion so what you really
kind of like to do is have the rules
basically not be limited to talk about
the knowledge base but to basically test
see whether that fact is either known or
can be inferred okay which basically
means you need to have like sort of the
separate rules for you know team play
sport um set up here and for each
possible thing you need to have some
sort of ground out role that basically
says well it's already known in the
knowledge base and I'll conclude it to
be true okay so this now this is now a
mutually recursive program okay and you
know the question is how can we learn
that everyone kind of clear where we are
so far based I mean anyone okay so this
is a goal we want to learn something
usually recursive so so here's the idea
all right so I'm going to start with
the mutually recursive program so i
apologize i'm switching examples here
just to kind of reuse slides so this is
a different program that is basically
doing some kind of like similarity or
label propagation over web pages so a
page x is about topic z if you've
labeled it as about z or if it's similar
to y and y is about Z so that's kind of
a propagation step and I'll say that X
is similar to Y if there's a hyperlink
from X to Y or if there's a word in
common between these two that's
basically what these two rules are
saying okay and the way I've set this up
there's there's features associated with
these rules ok so this sort of has two
different features this has two
different features this has one feature
so the rule is always true and I've kind
of set up this way so when you get to
this the the head of the rule contains
the variable W which is the word and I
can tag the application of that rule
with the word that is being used ok so
there's a feature for this rule which is
just the word that's being used so so
what are these features about well the
features basically get put on two edges
of a proof graph alright so the proof
braff look kind of like this ok we start
with some goal alright so so up here in
the right hand corner we have a copy of
the World Wide Web ok it's got four
pages one about fashion one about sport
I'm sorry this is a subset ok and and
here are some words in each of these
things alright so to find out what this
page a is about I can do my proper I
don't have a ham labeling things so I
look at them to a propagation step it's
got to be similar to something or two
ways it can be similar we could say
that's linked to I guess this is B kb's
labeled fashion so this gives me one
solution fashion now we can sort of like
train through here it's linked by a word
let's say the common word sprinter so
sprinter is also in C so it gets me to
C&amp;amp;C looks like it's actually well I have
to propagate that to D&amp;amp;D is about sport
ok so there's another little chain that
kind of gets me to sport there are two
possible solutions ok to the search
space so um
we now have sort of a language for
representing recursion because we can
have a recursive prologue program and
the cool thing about it is the proof
space is a graph okay and it took me a
long time to realize that this is sort
of like a reasonable thing to think
about so this proof space is a graph and
since it's a graph I know something
about how to tune this exploration
process okay I could imagine just
basically doing the same personalized
page rank on the graph which gives me a
similarity metric between this query and
these two nodes right fashion and sport
so it's a similarity metric between
queries and possible answers to that
query all right and then given those
given that search process I can now
think about tuning that search process
to give me the results I want okay using
kind of standard techniques for for
learning and you know this graph here
I've sort of constructed so I've got
lots of features they've got lots of
edges for the notes okay so I kind of
have some ideas about how to do this all
right so a lot so again basically you've
got this probabilistic transition
through this graph okay it depends on
the features of the node which basically
depend on you know the rules and
sometimes the arguments of those rules
as they were applied okay so their
implicit reset transitions because I'm
doing personalized page rank because I
like doing that so that basically means
if I'm doing census or reset basically
it it gets hard to do walks that are
very long so it's much easier to take
the short path and to take this
relatively long path okay because a long
path basically have a lot more chances
to do reset so basically this is this is
supporting looking for answers that are
supported by a lot of proofs and in
particular a lot of short proofs so um
the semantics of this it turns out
there's a formalism called stochastic
logic programs that were similar but
didn't have the reset okay that were
explored you know 10 or 12 years ago um
so the semantics you know are basically
all this order to set up exactly the
same way and there's one thing about
this which is really cool so
ideally we'd like to take this small
recursive program and apply it to a
slightly larger subset of the web and
the one I showed you in that slide right
more than four nodes right so if this
proof tree basically will let you
recurse across the whole web so you can
propagate labels all across the web then
the proof true we're talking about is
really really big so if i use the
catholic logic programming for it
basically wouldn't work because building
this proof tree is like doing a
traversal of the whole web ok but if i
want to do random walk with reset it
turns out the weights in a random walk
with reset drop off very quickly they
drop off exponentially as you get
further and further away from your reset
node so that basically means that you
can get an approximation to the random
walk with reset by just sort of
incrementally expanding away from the
reset knowed it was a nice algorithm for
a very simple algorithm that that was
describing a stock's paper a few years
back ok it's called PageRank nimble and
they probe they proved that basically
the size of the sub graph you explore is
basically inverse of the error and the
and the reset parameter so as long as
you have some sort of lower bound on
reset you can get a relatively small
grounding ok so we kind of know how to
do I won't talk too much about the
learning algorithm because it's 406 on
Friday maybe you know think about the
weekend now but we're using basic kind
of off the shelf you know learning
algorithm for doing this and you know
the real key thing here is this fact
that the size of the structure that
you're producing right the ground
structure that you create that
corresponds to a to a query the thing
that you're doing inference over when
you're doing the learning is not that
large it doesn't depend on the size of
the database it just depends on these
two parameters ok so that's a big deal
because if you have in many other
probabilistic representations it turns
out the size the grounding is actually
very large so for example a Markov logic
networks you start out by building a
markov random field where the nodes are
all the possible atoms
in the logical theory okay and that's a
huge set okay so in this case since
we've got like you know similarity XY
that's basically not just all the nodes
of the web at all the pairwise nose in
the way though the number of nodes and
web squared okay so um here's a couple
of quick results just to give you an
idea how this works so I've suggested
it's going to be faster than something
like alchemy a Markov logic network
system first thing we do is like sort of
tried to verify that so we took a data
set that the alchemy people put together
and we took their mark off logic network
and we translated it using sort of a
mechanical translation to are like our
representation and so Markov logic
networks let you have non horn clauses I
can only do horn clauses so the things
that weren't horn I just threw away so
it gives us me this program ok I has one
got that now good ok and and here kind
of the numbers so here we're looking at
inference we're going from like a tiny
me database that has four citations ok
to a really really small database that
has eight citations I'm sorry this is
log so this is a you know 2 to the
fourth 16 22 the eighth 256 citations ok
and what you see here is basically you
know there are different learning
algorithms that are applied to the
underlying Markov logic network there's
also this lifted belief propagation
strategy where you try not to unroll
everything but in every one of these
cases what you essentially see is you
see that the side the time it takes to
do the inference grows pretty quickly
with the size of the the data set ok so
uh where is for our case that blue line
is what we've got all right so it's
independent of the database size you
know given that we've got alpha and
epsilon fixed ok and now we look at how
well at work so so this is before
learning we just put uniform weights on
all features ok and you know this is a
you see these are numbers are not too
bad not too great with learning we sort
of improve on most of these cases this
is the mark
autologic Network and this is all using
the same subset of rules the 21 rules
that we actually could use in our system
if we put in all of Pedro's rules then
their system gets a little bit better
but we're still doing a little bit a
little bit better than that so this is a
very encouraging result that this is
sort of our reasonable bias and it has
the computational properties we want so
there's also another really nice benefit
of this fact that the grounding process
is bounded in size okay um so I when
you're doing learning you basically get
a whole bunch of queries and for each
query there's a bunch of positive you
know correct and incorrect responses so
i want to say what are the things are
the same citation 120 there's some
things that are right or some things
that are wrong okay so operationally
what we do is we start here we do a
proof for that query all right and then
that's so that's grounded out to this
little graph so now we don't have like a
like a non first-order construct it's
just a graph with labeled edges and
nodes okay we're trying to tune the
weights on these edges so the cool thing
is that those graphs are all separate
right we could combine these things if
we want but we can also keep the graph
for each query as a separate thing the
only coupling between these graphs is
the parameters that appear on the
features that appear on the edges ok so
our algorithm is a little different
where you're optimizing the same metric
that URI and Lars Backstrom were
optimizing but our algorithm they used
like a Newton method and what we're
doing is we're doing stochastic gradient
descent so basically you compute like
the gradient with respect to a
particular graph ok you tweak the
parameters and then you go on to the
next graph ok so this is really nice
it's sort of a small memory type
operation because you're only doing one
graph at a time the only thing that they
share is the edge features and you can
also do these in parallel right so you
can have a different thread looking at
the inference process over each little
sub graph so there's some
synchronization because you're sharing
the edge features but you can still get
potentially a big win by parallelizing
so this is some other number so this is
basically
doing the learning and we're looking
here at sort of how much you speed up is
you add more threads okay so ideally if
you have 16 threads you get a 16 x speed
up we don't quite get that but we also
but we're doing pretty well ok so that's
the formalism and like 10 minutes ago or
something before you guys started
dropping off and falling to sleep we
were talking about why I wanted to do
that so what I wanted to do is I want to
get to a mutually recursive set of rules
right and try and learn this mutually
recursive set of rules ok so this is
basically learning how to do inference
and sort of a stronger sense rather than
just learning how to do one step of
inference with this stuff I've got I'm
learning how to do multiple steps of
inference at once all right so that's
really kind of the motivation behind us
so I don't have any way of doing
structural learning right now for this
language so what I did was I basically
used the Liao's pra system I just let it
run and so it learns like non recursive
rules I just basically take them and I
syntactically transform them so they're
recursive and they can call each other
all right and then I take that and I
learned train weights on the whole
program alright so that's the experiment
so a few details what I did actually was
um so the hypothesis is that this mutual
recursion is going to help for the moat
the for the highly connected entities
the things that are related to a lot of
things which are themselves related to a
lot of things so I'm to kind of get at a
sample that had that characteristic what
I did is I picked a couple of different
seed entities ok and then I did just
kind of a simple random walk with reset
personalized pagerank away from those ok
so personalized page rank will give you
things that are close to the sea and but
also give you things that are sort of
you know central ok things that are well
connected so it's kind of a mix between
page rank and and distance so i picked
the top few things in that random walk
all right so that's a subset of nell and
then I project
the knowledge base to those entities
okay so i did then a few different ways
and kind of here are the results so just
to kind of pick one of these out here so
if we take the top ten thousand things
closest to the concept baseball all
right the auc for the non recursive
rules is about 75 okay au you see for
the recursive rules is about 99 okay so
we're getting a huge lift here largely
because of extra recall we get because
we could do multiple rounds of inference
okay and this this sort of inference it
kind of the results vary across these
different samples somewhat but there's
always a big lift from from looking at
the recursion and learning learning over
cursive set of rules I'll just let
people stare at goes for a second I
really should have a better
visualization of this another thing
that's actually kind of interesting is
that these things also tend to presents
are very closely connected you know
statistically they tend to also be kind
of more common right so they're more
facts about the things that are kind of
closest to baseball's and things they're
kind of farther away so what you also
see typically as you see as you go down
this list the test sets get harder and
harder boy a splint a train and test is
the training set is everything up to
some particular iteration you know but
have been 713 and the test set are the
things they're learned later on in the
bootstrapping process okay so this is
the first way that we're now using joint
inference and now all right I'm going to
talk a little bit about a second way
that we're doing that okay so so Lisa
Couture is going to be at the faculty
summit I believe she's going to be
talking about this into some extent oh I
haven't actually checked with her so
maybe she's about how about some
different I don't know so just a little
bit more detail what Nell does is it
basically iterates over and over and
over okay in each iteration it basically
learns classifiers with each of these
different feature sets you know
including pra and basically does distant
training for that feature set use
the current knowledge base so it uses
the current knowledge base to provide
labels on whatever day that's got
sentences or semi structured pages or
what have you and then once you've got
those results of that learning it's a
classifier you can extend it you can get
new candidates based on you know the
unlabeled things that the classifier
fire is on so we get a whole bunch of
candidates that are from different
learning systems and then we do some
heuristics to find the best candidates
and those are we in our the system we
call it promotion right there promoted
into the next version of the knowledge
base and then you know we go around the
next loop the knowledge base is a little
bit bigger so at each iteration the
knowledge base grows and it's a little
bit better at recognizing things because
it learns a few extra patterns and you
know how to parse a few more web pages
and so on okay so what's the hard part
well we're using some fairly ad-hoc
techniques here to find the good
candidate beliefs it will be nice to do
this in a principled way so what's the
right algorithm for doing promotion okay
so uh what we're really trying to do
here is kind of deal with the fact that
there's lots of noise in the database
many different types of noise okay but
there's also lots of redundancy okay so
it seems that the way we should deal
with these many different types of noise
and exploit the redundancies do some
sort of joint reasoning so we should
pull information about things like Co
referent entities or possibly Co
referendum matey's we should enforce
mutual exclusion and other sorts of
launch logical relationships okay so we
should find something that sort of
jointly satisfies everything as well as
possible you know under the assumption
that there is going to be noise okay and
also wondering something we don't really
know what kind of noise or is we don't
know how much there is of each type so
just as an example of this here's a set
of extractions so there's there's one
here that's like way off it thinks this
entity is a bird rather than a country
which it really is ok if thinks these
things might be the same thing but it's
not sure it thinks that this is the
capital of that place okay and you know
it knows that things that have a capital
R countries it knows that countries and
birds are distal
sets those are mutually exclusive all
right so these are the facts we have and
we'd like to kind of get to something
like this all right so so Lisa good so
this is sometimes called like so in
Google they calling us a knowledge graph
so this is a this is certainly a graph
so for a long time Lisa Couture has been
looking at using the term knowledge
identification talk about the problem of
basically going from this sort of graph
which has many types of errors okay so
she's been using the term knowledge
identification so this this process of
you know sort of coming up with a clean
you know denoise version of a graph all
right so we started talking and we
decided to try using her knowledge graph
identification techniques for this
problem a couple of students Jay and and
you that are we really doing all the
work for this project let's be honest
okay so so how does she do this so again
she uses a joint reasoning in a
probabilistic logic so it's different
from the one I'm using and it's
interestingly kind of complementary so I
won't go too much into the details okay
you know essentially you ground things
out by instantiating your general rules
in all possible ways so so so unlike the
proper language this page rank version
of prologue the the grounding process
can be relatively expensive okay but one
nice thing is when you're done the
inference problem is convex so finally
the most probable explanation given your
current waits for all the rules is a
convex optimization problem so so how do
we turn this into a joint reasoning
problem in her framework so we basically
have a bunch of inference rules okay so
one basically says well if null was
going to promote it and maybe it's a
good idea to promote it so there's there
two types of promotions one is for
concepts and one is for relations so
this is basically true if Nell wanted to
promote the thing and basically this
sort of
that's a real relation that's a label
okay I here's another one that basically
says you know if anything is if there's
any candidate then go ahead and promote
it it's better to promote candidates and
not promote candidates okay these are
some rules which basically enforce
consistency about things where there's a
co reference assertion so if you think
anyone and entity to arco referent okay
then you should have the same labels for
example and they're also constraints
about that come from the ontology so for
example if arm l is a subset of p okay
you know athlete is a subset of person
and entity e is an inclass l so
somebody's a personnel on athlete then
they better be a person too so this sort
of enforces the subset relationship
these this subset this these enforce the
mutual exclusion relationships and and
so on so this these rules are actually
too expressive for my logic so the
problem is basically these rules aren't
horn right these mutual exclusion things
I don't I can't do those in my logic but
we can do them and lisas logic okay and
here's kind of the rough results so I
we've done some more experiments but
just to give you an indication there's a
set of facts from a year or so ago where
another group David load when a page 0
domingos to students and some of his
students looked at basically the same
sort of scenario doing this promotion
step within nel as a joint reasoning
process they were using markov logic
networks a ver generously gave us you
know their evaluation data so we could
we pretend you know they're kind of
precise predictions so we could
reproduce their results so I so are the
joint inference of runs in just a few
seconds the learning is a little bit
longer and so the baseline here is sort
of Nell's
learning method okay so here the AUC is
about point 7 60 ml ends do about point
nine nine and the PSL is about the same
okay and and this is actually sort of a
version of Nell's code that's um been
been tweaked for just sort of the test
set so the thresholds have been adjusted
for that but those 25k test sets so so
I'm going to go ahead and stop here and
maybe spend a couple minutes with
questions so basically you know the two
things I've talked about our it was sort
of an overview of this dowel system and
some of the key ideas behind it all
right and then you know an overview of
kind of these two lines of really recent
research so one is basically treating
promotion right sort of key step in boot
wrap bootstrap learning systems as an
inference process which isn't really
very interesting if you think about
bootstrapping in some very simple
concept comp space where you just have
one or two concepts but it's actually
quite interesting when you're talking
about bootstrapping and sort of like a
you know a non-trivial you know
significant ontology the other thing I
talked about was learning how to do
inference over a knowledge graph and
particular learning how to do this
multi-step possibly recursive inference
okay so so we've taken kind of past work
which is based on kind of random walk
with reset type procedures and learning
algorithms and extend to that the first
order logic which is actually a kind of
cool because we get some very nice
scalability properties for this it's a
very nice property with respect to how
you can paralyze that inference okay so
that's basically as far as I'm going to
go so thanks a lot for your attention
you can clap now and then you can ask
questions
Matt did you compare proper to the kiss
like the orange Sosa so we actually
haven't done that we could do the horn
subset and that's kind of on the list of
things to do right so the other thing
that it's kind of long list to do is to
see whether some of the ideas from
proper grounding a procedure can be
modified for psl right so you know
they've been kind of like you know
things that you know we've been kind of
plunging along with sort of slightly
independently and you know we've been
talking a lot about these things and you
know i i'd really like to sort of see
kind of that happen so but yeah so you
know one question is you know how much
are those you know handful of things and
how do they how much they matter you
know there's also some strategies that
I've also fought through so for example
we could do could basically do is set up
rules that basically say I'm going to
infer that there's a conflict if you see
this sort of situation okay and then
basically tell the system um alright you
know here's some positive examples
things you should infer and by the way
please don't in for any conflict so
every conflict is sort of a negative
example of something you should infer it
is like a post-hoc kind of yeah yeah
something like that so I mean to do that
they're like sort of some technical
things we'd have to do so like the
easiest way of like training something
around that would just to basically sort
of say well you know these rules that
allow me to infer conflicts the guy says
he doesn't want conflict so let's just
wait these rules down right then I'll
have no conflicts so it's not quite as
simple as just doing that but i think
that approach might be might be
applicable and then you know certainly
you know the question of well so we did
sorta would need to have some sort of
random walkie semantics you know for psl
if we're going to use the same kind of
grounding strategy right or you know
maybe we don't maybe it works okay
without that I'm thought it's not clear
right but will be good to do some sort
of experiments and kind of feel with the
space
is there any more questions yeah one of
that how you doing I'm sure it's built
in but since when do you do pretty much
every atom a unique identifier and then
oh yeah um so let's see okay um so short
answer long answer or so so in L
basically there's um sort of I typically
two versions of an entity one is
basically sort of the entity viewed as a
string okay and then there's the entity
as assigned to a type okay so you know
so if you look at something that's
ambiguous like Apple they'll be Apple
the company they'll be Apple the fruit
okay and then they'll be strings like
the string Apple right which can refer
to either of those right or maybe Apple
Inc which may be only refer to one of
those so that's the representational
scheme that Nell uses the way it gets
there is it sort of has a couple of
classifiers which input on that picture
right so there's one that basically
looks for things that should be Co
referent and aren't okay and then there
also some heuristic so if you have a
fact that's got a couple of you know
mutually exclusive high confidence
predictions that will consider splitting
that so so that's sort of the machinery
that's in Nell and so the results i gave
we're actually not just using those co
reference facts though actually was
actually using those and some additional
coreference facts from some coreference
machinery that that Lisa students put
together because they're like kind of
big coreference people too so that's
another discussion that you know Lisa
and I are going to have some time in the
future about sort of evaluating those
changes and sort of figuring out I mean
they clearly help with the promotion
stuff is it something we want to sort it
up in bring into sort of the main Nell
system around so yeah that's that's kind
of the shorts
longer story rich just curious about how
you do experiments with now so so let's
say you release something and then you
realize couple days later it's actually
creating a problem bad facts are getting
added to the system in Japan inference
you must have some sort of roll back
procedure we even go back before that
happy yeah right and that has actually
happened in the past so the way Nell
works basically is you know after each
round of bootstrapping you know the
knowledge base okay and all the you know
learn patterns and classifiers are
basically check pointed okay so we can
in fact roll back at any point and but
if you know that said it's really hard
to do that means those are typically you
sorry you can't hold back oh right right
right right right so that's a nice firm
well we do live breweries we take the
pages that come out and we cash them
okay and then whenever Yuri fetch the
same page you get it from the cash
rather than from the web which is not
ideal for temporal things right but that
sort of gives us some more
reproducibility and then most of the you
know the patterns and stuff are built
off a distilled version of the clue web
thing which is a static repository the
static your only sense if you would like
change the random season
a certain time how similar you would be
so we haven't done that for sort of the
large web repository okay I've done a
bunch of experiments like that with I
different versions a different version
that was basically sort of set up over a
biomedical ontology okay and it
certainly is quite sensitive to the
seeds in particular the quality of the
seeds okay so one of the limitations of
Mel is that there's some engineering
here right of the ontology to kind of
keep the system for breaking right of
the seeds because you're giving seeds
that you kind of know because your
assistant designer your smart graduate
student from CMU right you know you know
that these are going to be and be
unambiguous relative to the the sets of
things that you know they'll be frequent
enough to be informative right and you
know there's a big question is sort of
how robust is know when you get to a
different ontology but one of the
reasons why did the biomedical thing
would sort of like other designed
oncology's they're not a lot of
interesting ontologies that have these
sets of you know mutual exclusion on
things like that that can be used do you
ever get sort of compound interest
effects where if you make something just
one person
there are six months everything is much
much better you know is that thought
that there might be singularity coach
license if you achieve a certain
incremental improvement in the short
term or or is this just bad thing now
you gotta ask Tom whether that's true
and see what they hook a lot of women
say yes right yeah well so that's
concept drift right neck sort of like
clearly happens right and we've we've
had to like sort of like roll back and
there are lots of reasons for I mean why
don't you like run into like things that
are just like sort of spammy right I but
yeah they were like so like the so for
example the entity matching stuff was
trained on some subset of things and it
worked pretty well for a while then we
change the ontology and then it stopped
working right so L suddenly it's kind of
like sort of went out of control right I
mean there's some kind of like hacks
basically that we use to kind of keep
things from cascading in either
direction too fast so so so one of the
hex which we'd have to think about
whether we want to incorporate it and
lisas extension or not is that no single
component can introduce too many facts
of any type ok at once ok and that's
enforced even in kind of like ridiculous
ways so will I put in something that you
know introduces facts about the geo
locations of things will have a very
conservative matching algorithm and so
the first time around that'll like sort
of say ok well you know I know the
geographic locations of a hundred
thousand things and we'll be like okay
you have to put in 10,000 this iteration
10,000 apps iteration and so on so yeah
they're like geographic facts that are
sort of like waiting to get in the queue
as we iterate for example yeah I will
dream bodies
in other news freebase oh yeah yeah so
so i guess the goals are kind of the
same to build sort of a broad coverage
of knowledge base there are a lot of
technical differences so yah go to is
getting closer so y hago has a smaller
ontology and it's done by extracting
from a less diverse set of sources and
mainly Wikipedia um uh so freebase is
done primarily by merging structured
databases ok and the merging is like
sort of very kind of like systematic and
user guided so I mean I think they're
sort of hoping to get to the same place
but you know we're sort of trying to get
their kind of by different tools I mean
so part of why we're doing it this place
because we kind of like the idea of
pushing you know the frontier is a
semi-supervised learning and things like
that as well right so you know they're
diff different motivations for doing
these things but I mean there's a lot of
there's a lot of what we hired a postdoc
from the the Yago group for example and
you know we so there's there's a lot of
there's a lot of commonality between
those different projects
that's something well we Hampton Yago
we've done some kind of preliminary
experiments with freebase right so
they're the big thing is that there's
not as much ontological information so
we have to you know sort of infer things
like neutral exclusion constraints so
yeah but like doing freebase as sort of
an initial thing is also like a very
reasonable sort of thing to do we're
doing some experiments where we're like
burgeoning results from now on Iago and
freebase arm for application so we're
like you also using Nell to say dude
like distant training for extracting
relations from individual sentences and
there were you know there's no reason
not to supplement that data with other
sources as well do a question this
probably that's the move on it but given
a closed ontology is there any benefit
in knowing when to stop promoting facts
into the knowledge base for instance
country you now start getting it out
bitch yeah no no no that's actually a
great idea that's a great question right
right well so yeah so that's right it's
not never ever been learning that's true
well I mean so there are other parts of
Nell that say for example add new
concepts right or add things to the
ontology but yeah it's like I tease Tom
and say it's like you're making us a
selling point but never ending is a
synonym for slow right really we should
get to the end right but yeah so I'm
actually working with a student right
now and so there's Bob nadal v and so
she's working on basically learning in a
fixed oncology semi-supervised learning
in a fixed ontology where you know it's
a fixed but incomplete ontology okay so
take you know like it's a simple case is
you're doing semi-supervised learning
into 20 classes but no one's giving you
seeds for 10 of them you only have seeds
for 10 of the classes so you know as you
progress eventually you'll start running
out of countries right and then you'll
start grabbing other stuff right and
you know the questions can you recognize
you know when you know the country's
stopped and you've started moving into
sort of the next you know outlying you
know concept you know so maybe there are
things like US states which are in your
ontology and you could say well that's a
US state it's not a country right but
there might be other things that aren't
in your ontology like you know I don't
know provinces of ancient Rome right so
is Gul a country i right you know so
there may be things that sort of have
additional structure right yeah I'll how
the nails can deal with the facts that
depending on time what's of athlete is a
those 21 playing one team right next
season we might play another team yeah
the promotion here is complete to the
existing database yeah so right now now
like something is true then it that
basically semantically sort of means it
is or was true at some point in the past
right so it doesn't like consider time
and you know in the website version so
we have been doing a bunch of research
on how to capture time so partha
collector has been doing some stuff on
this is part of his research so so our
current approach for dealing with time
is to look at a large corpus that has
time-stamped objects like a newswire
corpus or something like that and have a
similar ontology it talks about when
things can sort of which things can
co-occur which things can't co-occur so
for example you know you can have a lot
of senators US senators at one time you
can only have one US president at one
time you can't be both the senator and
president at one time so if you look at
the distribution of when you see facts
that suggests you know Brock Obama is a
senator okay and when he's a president
ok and when George Bush's the president
you know right then you can get some
sort of information about when those if
you look at that jointly you get some
information as to when those transitions
happen right so that's kind of how we're
dealing with the time issue and there
are a lot of kind of interesting issues
right so how do you infer those
constraints for example
right but you know so far that stuff has
been rolled into the existing kind of
null system so that sorry for mr. for
the beginning but where does them
initial ontology come from or where is
that so how many concepts and relations
ah so it's less than a thousand but more
than 600 so I don't remember the exact
number right now it changes over time
because part of the development
processes pewno developers look at this
right you manually add it and every time
you add it you sort of stick a few
things in like how do you state it and
so on all right so yeah it's a the
ontology is a big deal right yeah but
you can just download it if you want
yeah yeah yes okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>