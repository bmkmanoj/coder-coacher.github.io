<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Photo Sequencing | Coder Coacher - Coaching Coders</title><meta content="Photo Sequencing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Photo Sequencing</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_NyGRHQFeF4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning it's my pleasure to invite
professor shy of Eden from Tel Aviv
University to give a talk here at MSR
shy did his PhD in the Hebrew University
in computer vision was a postdoc in
Microsoft Research Redmond and since
then been a researcher in Mitsubishi
Electronics research lab and senior
researcher at Adobe so shy thank you
thanks for the introduction actually
i'll introduce me to the grad program at
the Jews at Jerusalem so that's great
being here so I'm going to talk about a
paper we had several month ago with a
tally deckle and yell Moses at ECC V
2012 but I thought that before going
into this paper I'll just give a short
pointer to a paper that i actually have
with simon corpsman who's my grad
student is an intern now at MSR and it's
about template matching so i'll give a
five minute trailer to this work that
will be presented next week at cvpr and
then i'll go to the main talk on a photo
sequencing so first match is a paper
about a fast a fine template matching
it's a joint work with gillitzer and
Danny Weisman from the vitamin Institute
and what we're trying to solve there is
template matching so given a template
you want to detect its location in the
image and what you see in magenta is the
location we found out and in green it's
barely noticeable is the ground truth
location and you want to do it quickly
and efficiently and you want to have
some global guarantees on the accuracy
of the solution that you find in fact we
can do it even in a smaller template you
want to detect this template over there
and you see that we got almost perfect
result you can barely see the green
ground truth a rectangle
and we can even deal with something like
this we can detect the whisk of the cat
and detect the location in the image and
the code is up on the web you're more
than welcome to go download and play
with it we love to hear comments and
feedback and just a couple of slides on
how this works so we thought long and
hard on how to solve the problem and
this is the algorithm we came up with
all you have to do is sample the space
of all to the affine transformations
evaluate each one of them and return the
best and all the paper does really is
formally prove why this is the right
thing to do and a lot of experiments to
validate the assumptions that we're
making holding practice so the idea is
as follows let's say that this is the
transformation space assumed the x-axis
is this is a 1d case a is the
transformation space and the y-axis is
the error of each transformation okay so
when we say it step one take a sample of
the affine transformation we evaluate
the transformation at each of this
location these are the values that we
have and when we return the best result
we might return something like this
because this would be the sample
transformation with the lowest error now
the key observation that we make and
that's something that is well known is
that we assume that images are piecewise
smooth now what this means is that if
you take a transformation and slightly
perturb it the ER that you'll get will
change only slightly because the image
is piecewise smooth so most of the
pixels of the template will land on
pixels with similar values and only
around the edges there will be the
change this means that transformations
that are not on the net not on the grid
like this one which is the optimal one
cannot be too far away from the samples
that we do have so we have a way of
formally connecting the error that you
want to introduce which in there IA in
the template matching to the sampling
rate that you need to do in the to the
affine space and within a matter of
2 2 15 20 seconds you get the results
that I've shown you so here are a couple
of additional examples in this case the
templates are forty-five percent of the
original image size and you can see that
this is the result that we get this is
the ground truth so it's burning a you
can barely see any difference between
the two here are more examples template
sizes thirty-five percent and it goes
down to even lower than that now in all
these cases we did it this is a
controlled experiment because we took
the image extracted the template and
then try to find it back in the image
you might say there might be noise so
how will it work with real scenarios in
which you have a template in one image
and you want to match it to another
image so use the Mikolajczyk data set
and what we are showing here is this is
the template that we're using and this
is the location that we found in each of
the successive five images these are
different images you can relate this
template to each of the other images
through the homography is that they
supply and again you see that we are
doing a pretty good job this is the
worst case we're detecting it to the
affine transformation in this case there
is a projective transformation that you
need therefore we don't get the exact
match here is another example it's
fairly challenging there is a lot of
repetitive texture and still we do a
fairly good job except for the last case
here where we fail and it's mainly
because of the projective nature of the
transformation in this case we deal with
blur this is the original image these
are the five successive examples with
increasing amount of blur and we
correctly detect the template in each of
those this is the bark sequence it's a
zoom plus rotation and you see that we
get fairly good results except for this
one
but we can detect the template even in
these two cases and when we fail here
the failure means that we found this
region such that the absolute error of
this template is within a constant from
the true global position jpg artifacts
are no problem and we can we did the
same thing with the Zurich a building
data set again there is a template here
and we find it in another image of the
same building and there are many more
examples so again if you find this
interesting by all means go either to my
website or to simon's website and follow
the link to the paper and download the
code and play with their questions how
do you differentiate your eyelid with
Elijah we advise it for faces yeah so
Asaph assumes that you can detect and
described and describe the interest
point so in many of the examples I've
shown you like at the beginning it's not
clear you'll find interest points on
something like this so all the feature
based methods will not work in this case
on the other hand you might use our
method and use direct methods to improve
the result if you think you need a sub
pixel accuracy because our system is
pixel wise accuracy so in the paper we
give an analysis and comparison both to
a sift and to direct methods and show
the advantages of what we're done ok
good so now back to the topic of today's
talk and that's photo sequencing and
this is the problem that we want to
solve actually that's an illustration of
what we want to solve that's a picture
taking it in 2005 of the announcement of
the new pope and that are a picture
taken from the same location eight years
later when they announced the new pope
and
right so there is a new type of camera
you might call it a crowd cam and it's a
colocation of cameras in space and time
and the question is what do you do with
that and there are a couple of
interesting things to point out for the
first time you don't have a single
photographer determines what's the right
time to capture something everyone is
capturing the image whenever they want
and you hope that they will capture it
at roughly the same time when there is
an important event going on but there is
no guarantee and the second thing is
that currently these images do not
interact with each other we do know that
camera rays are very useful to do a
variety of things you can get a huge
panoramas high dynamic range and do
better tracking whatever but all the
cameras i'm familiar with assume there
is one person usually a PhD student or
his advisor who carefully calibrated and
synchronize the camry in order to
capture the high-quality image and the
question is can you work with such a
noisy data set so that's the goal of of
our work to make a baby step in in this
direction of analyzing dynamic content
from a collection of still images so
let's try and define it more formally
this is the park next to Tel Aviv
University and if you go do it on a on a
sunny day you can take a picture of the
boat on the river and other people will
take photos of the same event and before
you know it you have a folder full of
images taking it roughly at the same
time from roughly the same location and
the question is now what how are you
going to make sense of this information
so one obvious thing to try is try and
place the images one after the other and
play the video and if you place them in
a random order you get a very messy
result it's very difficult to understand
what's going on here right if you can't
infer that the course of each both you
don't exactly understand the dynamics of
the scene if you run our algorithm
instead you can see that how the green
boat
how the green boat moves how the the
white boat moves and so on and so forth
so our goal is to take a folder full of
images and try to infer the correct and
pearl order the images we're taking so
that's the formal definition given em
still images determine their temporal
order and just to give you a sense of
the dimensionality of the space given in
fact you have n factorial possible
permutations and if you have 15 images
like the example I've shown you now the
space is 10 to the 12 so you need to
pick the correct order out of 10 to the
12 possibilities so now that you know
what photo sequencing is let me explain
what it is not the first thing to notice
is that it's not video synchronization
and it's not video synchronization
because you have very discreet and small
number of samples from each camera so
you don't have the full video which is a
very dense sampling in time you just
have one or few images from each camera
and you need that information to recover
the relative temporal order between
images and cameras the second thing that
a photo sequencing is not is it's not
photo to reason because photo tourism
deals with the static aspect of the
problem so you want to recover the
camera position and the 3d structure of
the scene you don't treat the dynamic
content of the scene and the last thing
is there was a very nice paper by finger
at all from Georgia Tech back in cvpr
2007 what they did there they had images
of Atlanta captured over a period of
about a hundred years and they wanted to
find the correct order because they
didn't have the time step stand for each
of those images they used completely
different techniques and the setup was
different we are interested in images
that are captured in similar space and
time and want to organize them so we're
using completely different tools so the
assumption we are making is that the
images are taking from roughly the same
location at roughly the same time and we
want to organize them and the the way we
go about it we take a geometric approach
to solve the problem and we detect and
match feature points here
example of future points detected in one
of the images and then we use a standard
tools from a geometry to find a fan the
fundamental matrix in the static
features in the images and all the
outliers are considered a dynamic
features okay so we have the fundamental
matrix between every image and a
reference image and we have all the
dynamic features over there so let's
look at the dynamic features these are
the this is the tool that we are going
to solve the problem and let's look at
the dynamic features in this image you
see that in this particular image the
sift descriptor I think detected all the
points on the boat but it failed to
detect them on the boat up up there so
you see we're dealing with the noisy
process and for this particular feature
point it was detected in all these
images that you see the red arrow so in
a subset of the images this dynamic
feature was detected and matched for a
different dynamic feature like the one
marked with yellow over there you get a
different subset of images okay so the
question is how do you a grenade how do
you there are two things how do you take
advantage of the images that you have
the feature point how do you use that to
recover the temporal order of the images
in the greens in the yellow subset or
the red subset and then how do you grow
get everything together into a global
order of the images so the algorithm
consists of two steps the first one is
finding the partial order from each
dynamic feature so for the red a dynamic
feature you get a partial order like
this for the yellow you get some partial
order like that and once you have all
these partial orders you want to
aggregate everything together and get
one globally consistent temporal order
for all the images and I'll spend the
bulk of my talk on the first part and
then I'll mention how we solve the the
second part and the tool you're using
here is something called rank
aggregation I'll explain more about it
later questions
okay so we want to recover that import
order from a single feature set and here
is a plot to explain what's going on you
have the 3d point and we assume it's
moving along a straight line and as it's
moving along a straight line it is
captured by image 3 and then by image 2
and then my image 4 and then by image 1
so there is this implicit assumption
that the point moves along a straight
line and moreover it doesn't go back and
forth it's going in one direction we
don't care about the speed in which it
moves so one way to solve the problem is
to say well the order of the point in 3d
in a implies the temporal order so what
we can do is if we knew the positions of
all the cameras we can reconstruct the
line and then intersect each of the Rays
with the line get the position of the
point along the 3d line and solve the
problem and theory tells us that five
cameras are enough to recover the 3d
line and then you can recover the
position of the points and get the
temporal order we want to avoid that for
two reasons the first one is you make
strong assumptions about the data in the
sense that you need all the cameras to
be in the same coordinate system so you
need to run bundle adjustment which is a
step above the fundamental matrix
estimation that we are doing the second
thing is is a little more delicate we
actually don't care about the 3d
structure of the scene at this point all
we care about is the order of the images
and therefore when you're trying to do
this reconstruction you might be more
sensitive to errors and noise so we did
some a synthetic the synthetic
experiments to validate that but that's
not the point we really wanted to avoid
the 3d reconstruction because we just
care about the order of the temporal
order of the images so instead of
working in 3d what we want to do is work
in 2d and that's fairly simple all you
have to do is project this trajectory
line onto the image plane here and if
the 3d points are projected to points
along this line and you can do the
entire analysis in the image plane ok so
we are moving from 3d to 2d
so in order to do that we have to make
an assumption and we're making something
called the static image / assumption we
are assuming that there are two images
that are taken by the same camera and
this camera is static for that time
period okay so let's denote them i 1 and
i 2 and how is this going to help us
well let's see an illustration here so
this is the first image taken by the
reference camera and this is the dynamic
feature point that we detected and now
we take the second day image with the
same static camera and let's say the
boat moved here so now because we have
the static image / assumption we can
pass a line connecting these two points
and this is the projection of the 3d
trajectory of the boat onto the
reference image plane okay that's the
crucial thing we made our life very easy
because we assume we had one static
camera taking two images a different
time step it can move afterwards but
that's the Assumption we're making for
now so now that we have this line let's
see what happens when we take a
different image ik and we detect the
same dynamic feature on the boat because
we have the fundamental matrix
connecting ik and I one we can compute
that people align take the intersection
between the epipolar line and the
trajectory the projection of the
trajectory and get the position of this
dynamic feature has as it would have
been seen at time t k in the reference
camera system okay so now we see that
the temporal order of the images is i1
and i2 that the two images coming from
the reference camera and we see that ik
was taken before them and you can do
that for each of the other images and
you'll get a temporal or they're out of
the 2d analysis that you're doing in the
image plane okay so all these feature
points so this feature point was seen by
five images that's the temporal order we
get here
the first murder is just a salad Jason
is putting both of those in here
hypotheses we assume that we have two
images from the reference camera and we
assume we know the order of them get
children same camera yeah
the relative times participe and then
the other one obviously is that you're
not crossing the line of action so to
speak
in the air meter yeah yeah yeah so this
place to the assumption that images are
taking a trophy at the same time so a
point doesn't have to go back and forth
we assume it's going on in one direction
if it's oscillating then we have a
problem and I'll show an example later
on dealing with that one nice thing to
observe here and it goes to the fact
that we don't do 3d reconstruction I'm
talking all the time about points moving
along straight line in fact we know what
is the part of this boat you see it from
the trail it's leaving behind and you
see it's not a straight line but the
algorithm is robust enough and the
assumption because only care about is
the temporal order is robust enough to
give us the correct order even though
the objects do not move along a straight
line and another point that I would like
to make is that each feature point can
move along a different 3d line in a
different direction they don't have all
to move in the same direction that's not
necessary yeah it doesn't matter if you
cannot approximate its motion by a
straight line so depends how curve is
the is the actual trajectory as long as
the order of points do not flip when you
project it on the live in line you'll
get the correct order against those
friend is in some camera there's a line
of action
you go to the other side oh yeah yeah
like in my case they're going from left
to right it doesn't matter it doesn't
matter in the sense oh it would matter
for this future point here but if you
have so what will work if you had one
dynamic object moving in this direction
and another dynamic object moving in
this direction that will be fine looking
from both sides of the action line would
be a problem yeah we have example of
similar nature later in the presentation
I'll mentioned okay so there are a
couple of limitations with this approach
the first one is you have this reference
image which means that you need to match
all feature points in all images to the
reference image so it limits the amount
of images you can work with and the
second problem is that you're not making
use of all the information that you have
and I'll show an example in a minute so
here is an example of a parade in a in
Tel Aviv let's say this is your
reference image it would be extremely
difficult to match this image in this
image to the reference image the
background is way too different even
though the dynamic object that is the
focus of our work is is fairly prominent
so we would like to alleviate this
problem somehow the other problem is
that usually people take more than one
picture with their camera and we are not
taking advantage of the fact that we
know that important of images within a
camera we don't know the exact time step
but we know that image to was taken
after image 1 and before image 3 and so
so we want to make the approach more
scalable by removing the reference image
assumption and we want to make the
approach a more realistic by looking at
multiple images that you know the
temporal order in each of the cameras so
let's go back to our illustration we
don't have the 3d trajectory line
anymore and all we have are the
projection of this dynamic feature point
in the image plane ok so let's take this
is the reference image and we'll do it
for each of the images as the reference
image repeatedly
and you can map the the feature points
the points to epipolar lines so what you
have in a particular image is a point
and let's say three people are lines
coming from their remaining three
cameras right so here is another this is
a zooming justin on the image plane of a
particular camera this is the projection
of the dynamic future point and these
are the epipolar lines corresponding to
the projection of this dynamic point
from different time steps on the image
plane of this reference camera so if we
had the trajectory the projection of the
trajectory then the problem is solved
but we don't have so this might be the
projection of the trajectory this line
here or it can be this line here so how
are we going to solve this problem so
one thing to notice is that if you take
this line and perturb it slightly the
other of the points will not change so
the question is when will they change so
there are two types of cases in which it
will change the first one is very simple
if you pass the trajectory line must
pass through this point so we need to
establish another point and we'll take
the point to be the intersection of
these two lines for instance so you'll
notice that if you have a trajectory
line to the right of this line then the
order is green than red or red than
green depending on the direction in
which you're looking at and if this
black line crosses this critical line
then the order of points changes this is
green red and now it switches to red
then green so this is one type of
critical lines every time the trajectory
line crosses a critical line the order
of points along the trajectory changes
there is another instance and the second
type of critical lines is critical line
that is parallel to each of their people
a line but passing through the future
point here
so to see why this is true if you have
this as a line then you have blue and
then green and if you cross this
critical line you have green then blue
okay so taken together we've removed the
temporal assumption within don't have
the static image / assumption and
therefore increase dramatically the
number of possible permutations that is
a single dynamic feature can in aim in
deals in fact if you look at the image
plane it's now / all those critical
lines into regions and it's pretty
obvious to agree that within each region
you have just one order well actually
two up to the direction of of the points
which infers the order of the images so
here is a quiz let's say you have 15
images what's the maximum number of
permutations and let me remind you that
n factorial equals 10 to the 12 possible
permutations so if I'm giving you this
number how many different permutations
will you get from the analysis I've just
shown you is the question clear
the independent number of yeah
so let's see so the number of
permutations is two times the number of
regions because for each region it can
either go back or forward or backward
and the number of intersection points
for the first type of critical points is
n minus 1 choose 2 and for the paraline
parallel lines that the second type of
critical lines it's n minus 1 and if you
do the computation you end up having 210
possible permutations ok so that's the
in the case of N equals 15 that's the
maximum number of permutations in
practice you can do much better and this
is an example showing that what we see
here is a point this is the dynamic
feature point and you see are all their
people are lines for during from the
remaining images in the data set and
there are two cameras and we know the
order of each of the images in each
camera this is one particular order this
is the particular order of images from
the other camera so now that you
evaluate all these 210 and different
regions for each one you see if it
violates any of these assumptions and if
it does you throw it away you know it's
not a possible solution in practice we
get about four or five different
permutations for each dynamic feature so
we've replaced the static image /
assumption and we get a much much but we
still get a very small number of
permutations / dynamic feature okay so
to recap what I've shown you so far
we're using geometry and geometric
reasoning to infer the temporal order
for each dynamic feature you can either
do 3d reconstruction you can do the
solution based on the static image pair
or you can use this temporal constraints
that I've shown you a minute ago in any
case each dynamic feature will give you
one or a few number of temporal orders
on the image set the second question
that we need to solve is how to
aggregate all the information into one
consistently in global order of all the
images so the way to solve that is to
think of the
problem is a graph each node is an image
and let's say that this is the correct
in Perl order of the images then you'll
have the edges like this I I three goes
to I 12 45 and two now if you are
lacking there was one dynamic feature
that appeared in all images let's say
this red point and it there were no
errors will get there it will get that
and then it's just a pelagic assault on
the graph we get the solution it's
trivial in practice that's not what we
have what we have is a partial order
from the greenpoint another partial
order from the Blue Point and another
partial order from the red point now the
problem is that there might be earth and
the way errors are manifested in this
graph is that there might be a cycle if
you look at the edges you will see that
there is a cycle between 21 and four and
that's impossible okay so the question
is how do you aggregate the information
and resolve this conflict and the way to
solve it is for a tool called rank
aggregation and you define a metric on
the space of all possible permutation
and essentially you take two
permutations in the distance between
them is the number of press on which
they don't agree the order okay so we
are looking for a consensus full order
sigma that is as consistent as possible
with each of the partial order Sigma I
so each dynamic feature voted for one or
a few temporal order Sigma I and now
we're looking for one global temporal
order Sigma yeah this problem has been
investigated extensively in social
studies this is how you do voting and in
the context of computer science there
was a we are following a work by work at
all from 2001 and what they had is the
following problem they had a search
query problem you want to do you run a
query and you get results for multiple
search engines so now you have the pages
ranked by each search engine separately
and you want to aggregate all the
ranking into one global consistent
ranking and the solution is based on
markov chain solution
markov chain approximation the problem
in general is known to be np-hard so how
how is the solution going to work we
construct the transition matrix we we
build the graph and we construct the
edges such that the weight of this edge
is the probability that image one
appeared before image for you can simply
count the number of features that voted
I won before I four and that would be
the probability of that would be the
weight of the edge i J and now you can
look at the transition matrix that is
associated with this graph and you do a
Markov chain essentially you initialize
the state the probability that the last
state is each of the images to be a
uniform distribution so it's 1 over 5
and if you look at the eigen deleted
anger the leading I ghen vector of this
transition matrix m you'll get that it's
one on the last state and 0 everywhere
else imagine a random walk as you take
it to infinity with probability 1 it
will end with the the last position so
you can place image to their the last
image remove it and repeat so that's the
whole algorithm what happens in case you
have a conflict conflict meaning you
have a cycle in that case when you look
at the eigenvector it will not be all
zeros in just one it will be something
like this let's say there is a cycle so
all elements outside the cycle will be 0
but within the cycle there will be some
values and you're not guaranteed that
the correct image within the cycle will
be will have the highest probability
let's say that this image got sixty
eight percent probability so we'll
choose that to be the last image but it
might have been this one or this one and
that's the approximation nature of the
algorithm that's why we can guarantee
the globally optimal solution result so
the first question people ask us is why
bother with this approach to begin with
why can't you just use the clocks on the
filter on the cell phones and I thank
you for not asking it until now
so we ran an experiment we play the
stopwatch on the screen and asked all
the students in class to take a picture
when it reaches 10 seconds and again
when it reaches 20 seconds and send us
the images and this is the histogram of
the times that you get this is the time
offset and this is the number of people
that took an image at this time offset
so there is a total of forty images I
think here a couple of things to notice
here the range is about six seconds so
if you're talking about the dynamic
event six seconds can be depending on
the type of event quite a lot that's
problem number one problem number two is
there is no it's not a unimodal
distribution I think because there are
two types of the more I think two or
three types of carriers in Israel maybe
each group of phones is clustered around
the different peak moreover there are a
couple of outliers that I'm not showing
here that they had their camera phones
completely off seeing there are several
minutes off so given all these reasons I
think there is room for using a
vision-based technology to properly
synchronize images here are some results
here are nine images taken I think by
two cameras in this case we're using the
static image / assumption these are the
two images that are taken by the static
camera
and these are resulting random order
it's not very clear what this guy is
doing and that's not me by the way and
this is the result you get ok I'm
playing it back and forth so you can
clearly see that is skating from left to
right here is another example the same
set up in this case look at the little
girl on the slide you will see it's her
location that will give you an
indication of how well we're doing it ok
and again so we're recovering the
correct and power loader here and
another example this is a challenging
example because if if you look at the
image the top third is blue sky there
are no features there the bottom third
is reflections of the water so it's
difficult to get any feature points so
just getting the reporter lines the
static feature points and computing the
fundamental matrix is not reveal and
another problem is that the motion of
the girl is is almost parallel to Debbie
power lines which makes the intersection
that much more sensitive so I think a 3d
reconstruction here will not work very
well but in our case you see that we get
a very good result ok a couple of
experiments using using the data
courtesy of Park it out from an eccie
v10 we have here five cameras and we
have a number of images from each camera
so in this case we're using this
assumption we're not using the static
image / assumption the points were
manually
and marked in the in the images and
going back to your comment earlier
actually the guy is climbing up and down
but because we only assume that there is
the point the object is moving along a
single direction we we clip the sequence
only to the images going in one
direction otherwise it will be a messed
up and you can see that is constantly
climbing up okay and let me play that
again
good and another example from the same
paper there are 14 images for cameras
and we correctly recover the temporal
order here and the last example is from
a parade are three cameras for images
and two images respectively and this is
the result that you'll get and again
okay so let me summarize what we've done
I think it's a cool problem it's also it
focuses to think what are the
assumptions that you want to make and
play with geometry which I personally
very like very much also I haven't seen
much work on rank aggregation in
computer vision I think it's a very
powerful tool there are many instances
you can think of or you have ranks
between partial ranks and you want to
aggregate everything together in the
future I think the main problem we had
is the problem of matching detecting and
matching feature point is extremely
difficult it works well for the static
part so see store a shift will work fine
but for the dynamic feature points first
we couldn't get safe to detect feature
points on the dynamic object and even if
we manage to do that then the matching
would fail because the descriptor is is
not working you can't match the
descriptor of the same dynamic point
because especially on humans they change
their appearance too much for the
matching to work well so in this case
what we did we use the code by the adobe
group and non-rigid dense correspondence
and that computes a dense flow field
between two images and it seemed to work
better for our problem as a follow-up to
matching this will allow us scalability
ideally you want to work with hundreds
or thousands of images and if you have
automatic matching then you can scale
really well and are probably new
applications for instance separating the
dynamic from the static objects in the
scene to highlight if something really
changed on that's it thank you very much
oh yeah so if we should do a basically
find the majority vote that we know the
time as one way of progressing and
enough that you have one vote I'll give
an example suppose you have a scene
where a lot of objects are having a
periodical motion let's say between two
points and one or few objects that have
a linear motion around looking at that
you know that this is not repeatable so
we I'm not going into that in the talk
but in the paper we did some trying to
figure out the confidence that each
dynamic feature has so we might say this
dynamic feature appeared in more images
therefore we trust it more you can adult
all these priors into the voting process
so if you have a very dynamic feature
that you're very confident in then it
can have a higher vote and this will
completely dominate the solution and
then we'll get the trivial solution so
there is a way to introduce these priors
into into the the framework the rancor
gregation is agnostic to all this it
just takes the matrix that tells you the
probability of vibe occurring before jay
and gives up the solution we are not
using any time information from the
camera why isn't about go backwards I
guess not equally plausible yeah so in
this case we manually selected out of
the two possible solutions the solution
which day the boat is going forward but
you're completely right in the first
case when you had the static image /
assumption then because you know the
order of the two images from the static
camera it determines the sign bit of
time
you said you match the are you determine
the probability that one image happened
before the other based on the number of
match features right did you do any
other things like the quality of the
features or any like geometric soldiers
I don't think we did you can think of
the quality of dementia thing that would
be one option but in practice this that
was never really the problem we also did
synthetic experiments were used two or
three hundred cameras or images if you
will and we introduced errors and rank
aggregation gave a solution that was
like ninety-eight percent accurate the
real problem is getting the matching
that's that's the killer problem these
are these are relatively controlled
scenarios did you try it on you know in
the wild the Carnival is as wild as we
could put our ends on it's difficult
because the background changes
dramatically do you think that it's when
you look at the images then you see that
the background changed quite a bit and
there are lots of motion so just getting
the future points there is difficult we
try to find that sports scene like say
football we're like assuming that say
the team is moving forward so you have
your linear motion but you still have
the place with similar jerseys and stuff
I would assume that the matching yeah be
problematic I agree i wish we don't have
the data set for that if we can get
enough data we would love to try our
algorithm on on that
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>