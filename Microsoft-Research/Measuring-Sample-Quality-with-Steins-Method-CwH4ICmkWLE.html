<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Measuring Sample Quality with Stein's Method | Coder Coacher - Coaching Coders</title><meta content="Measuring Sample Quality with Stein's Method - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Measuring Sample Quality with Stein's Method</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CwH4ICmkWLE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
speaker is less Rumaki from Stanford
he'll be talking about measuring sample
quality with Stein's method thanks
Martin can everyone hear me okay so it's
available over 70 presenting some very
new work with my graduate student
Jackson gora and the motivation actually
comes from large-scale posterior
inference so the question is how do we
scale a Markov chain Monte Carlo to
massive data sets this is a motivating
question and of course the benefit of
using mcmc is that it approximates
intractable posterior expectations these
integrals on red here with
asymptotically exact sample estimates
but the problem is that every sample
point every X I that we draw actually
requires iterating through the entire
data set so if to run through my data
set each time I want to draw a new
sample and that can be very expensive if
I have a large data set so here's a
template solution that's been proposed
by a number of researchers now and the
idea is to approximate your standard
mcmc procedure with another procedure
that makes use of only a small subset of
data points that makes every sample draw
cheaper but it also introduces an
asymptotic bias so now your target
distribution is typically not going to
be stationary for your chain but of
course the reduced computational
overhead means that you can sample more
quickly so then potentially you can
reduce your Monte Carlo variance and so
the hope is that for a fixed amount of
sampling time this variance reduction
will outweigh the bias that's introduced
so that's the that's the basic idea and
this is I think a very appealing sort of
paradigm but it introduces some several
new challenges first how do we actually
compare and evaluate the samples that
are returned by these approximate mcmc
procedures and how do we actually select
a sampler given that some of them are
approximate and not targeting our true
posterior anymore how do we pick their
tuning parameters and how do we actually
go about quantifying this bias variance
trade-off the difficulty in all of these
cases that our standard evaluation
criteria like effective sample size or
trace plots or various variants
Diagnostics don't take into account this
asymptotic bias and hence because
they're assuming convergent to the
target distribution so this is what we'd
like to get around all right so here's
our challenge we like to develop a
measure a sample quality measure that's
suitable for comparing the qual
of any two samples that are
approximating a common target okay so
you're the targets that were considering
today continuous target distributions
I'll say their support is all of our to
the D but you can relax that to any
convex set and they have a density and
that density is going to be known for a
normalization constant and integration
the key assumption here is that
integration under P is intractable so
I'd like to compute integrals under P
but I don't know how to you so what
we're going to do well we're going to
use a weighted sample of a set of
distinct sample points x1 through xn and
a set of weights Q of X I that are
encoded in the probability mass function
Q so this little Q actually defines a
discrete distribution and we'll use this
discrete distribution approximations
these sums over I of QX I times H of X I
to approximate the integrals that we
care about under P okay and I should
point out that i'm not making any
particular assumption about the
provenance of our sample points so they
could be coming from a Markov chain it
could be biased it could be exact it
could even be generated
deterministically okay so our goal is to
quantify how well expectation under q
approximates expectation under p and
we'd like to do this in a manner that
first of all detects when we actually
have a sample sequence that is
converging to the target the text when
the sample sequence is not converging to
the target and is actually
computationally feasible so here's our
starting point I think a pretty natural
starting point is to consider the class
of integral probability metrics these
measure the maximum discrepancy between
the sample and the target over a class
of real valued test functions will call
that class H when H is sufficiently
large this convergence the convergence
of this I p.m. is integral probability
metric 20 actually implies that your
sample sequence is converging to your
target weekly alright so that's the
that's requirement to bouts the
detection of non convergence the problem
with this idea is that this definition
inherently involves integration under P
and that's exactly what we don't know
how to do so most of these I pm's can
actually be computed in practice but he
an idea what if we only considered
functions that we knew a priori so what
if we only consider functions that had
expectations that we knew ahead of time
under P for instance functions that we
knew have me and 0 under P if that were
true then this this I p.m. would only be
an optimization problem depending on the
expectation under Q and that's something
that we can potentially handle so this
is the basic idea and it turns out this
fall's exactly into the purview of
Stein's method okay and so we have a
couple of questions about this how do we
select this class of test functions will
the resulting discrepancy measure
actually satisfy our requirements will
detect convergence and on convergence
and steins method will help us answer
that so here's Stein's method in a
nutshell it's a method for
distributional convergence and it
proceeds for establishing distributional
convergence and it classically proceeds
in three steps so the first step is to
identify an operator will call it t and
the set will call it G of functions and
we want this is operated in the set to
satisfy the focus the following property
when we apply the operator to every
function gmr set and take an expectation
under P I want that at function to have
mean 0 ok so together this T in this g
or defining what will call the stein
discrepancy I'll go write that as SQ TG
and this is another I p.m. type measure
but because all these functions have
mean 0 under P it only depends
explicitly on expectations under q ok
that's step one step two is actually two
lower bound the stein discrepancy by
some reference I p.m. that we know and
love like the washer stain distance or
the total variation distance this can
actually be performed once in advance
for a large class of distributions it
says that the stein discrepancy
converges to 0 only if this other
measure that we trust converges to 0 and
this gives us requirement to protection
of non convergence step 3 is actually
two upper bound the stein discrepancy
under reasonable conditions to
demonstrate that you actually have
convergence when your truth sequence is
converging to the target when your
sample sequence converges to the target
ok let's dig into each of these steps
and a bit more deaf
first how do we pick this operator T
well a very convenient and general
approach is the generator method of barb
or the idea so you identify a Markov
process that has P as its stationary
distribution under mild conditions it's
infinitesimally then actually
immediately satisfies this condition
that it produces mean zero functions
under P which means that I can
essentially take the generator from any
from a Markov process and use that to
reduce my operator so let's give you a
quick example of that imagine that you
consider the overdamped lender longe of
in diffusion this exists for many
continuous target distributions it has
the following stochastic differential
equation that defines its progression
its generator is given here as AP and
this gives rise to a corresponding Stein
operator to move from this generator to
the stein operator I've just replaced
the gradient of view one half the
gradient of U with a function G so now I
have a an RG valued function G an
interesting property of the Seine
operator is that it only depends on the
ground entity through the gradient of
its log which is great because this
means that it's computable even if P
can't be normalized ok and on top of
this for a particular choice of Stein
set which will call the classical Stein
set this is a class that includes all
bounded functions with bounded gradient
and Lipschitz gradient we have this mean
zero property this is satisfying step
one of our of our Stein's method
procedure ok so this choice of operator
this land of n stein operator and this
classical stein set will give rise to
what will call the classical stein
discrepancy and now for the next step in
signs method we want to lower bound this
time discrepancy and it turns out that
in the univariate case this is a
well-studied quantity it's actually
known that for a great number of targets
convergence of the stein discrepancy 20
implies that the vajra steen distance
converges to 0 which means in particular
that you have can we conversions
these these functions I'm searching
trying to loosen a lower bounds I'm
sensing lower bounding the stein
discrepancy by the boxer Steen the
stands for all Q you think that you know
is zero so this the sensor itself is
fine as supreme over class of functions
yeah and those are the functions at will
after passing through the operator will
be mean zero right so that's a premium
is what's B it's being lower bounded by
the boxer stain distance okay so that's
great for the univariate case but it
turns out that many fewer distributions
have been analyzed in the multivariate
case in particular almost all the
literature's on the multivariate
Gaussian distribution here if you could
references on that to extend the reach
of that literature we salvage the
following result which applies to
classes of strongly concave
distributions including for instance
Beijing logistic regression with
Gaussian priors and it gives you a
result of the same sort of flavor that
is the stein discrepancy goes to 0 only
if the Vatra stay in distance does and i
just want to emphasize though these
conditions in this theorem are
sufficient but certainly not necessary
for getting the sort of result we're
hoping that this will provide a template
for other lower bounds for other classes
of distributions okay that's lower
bounding what about upper bounding you
can show their various ways to upper
bound this quantity one takeaway from
this proposition that we've established
is that if you're working with a
distribution that has a square a square
integral gradient of its log density
then the stein discrepancy converges
whenever the sensitive when your sample
sequence is converging in mean and
gradient of log P of X is converting and
square main and the gradient of log P of
X is converging in meeting ok but I want
to talk about a bit more about computing
the stein discrepancy so we have
something that potentially has good
convergence properties how do we
actually compute this in practice let's
take a look at the classical stein
discrepancy in particular its underlying
it is this optimization problem
and one nice thing about this
optimization problem is that the
objective only depends on the values of
gee that's those are your new test
functions and grad G at the end sample
points X I and that's because the
objective only depends on is an
expectation under Q your sample measure
the problem is that it's still an
infinite dimensional problem with an
infinitude of constraints to get around
this we'd like to be able to impose
these smoothness constraints the bounds
and the gradients and G and it's in this
Lipchitz condition at only a finite
collection of points so let's consider
that to do this we're actually going to
introduce another notion of a Stein set
moving beyond the class will sign set or
shall call the graph Stein set and it
will be defined for any graph which can
view as defined on the set of sample
points x1 to xn so the birds the vertex
set be here should be you should just
consider that to be x1 through xn and
essentially this definition just says
I'm imposing all the smoothness concerns
that I had before but only between pairs
in my vertex set so I have edges e and
I'm only can enforce smoothness between
pairs of points in that in that edge set
remarkably this a Stein discrepancy
based on this graph Stein set if you say
were to choose a complete graph that has
an edge between every pair of sample
points is actually equivalent to the
classical discrepancy in a very strong
sense meaning that you can upper and
lower bound this graph Stein discrepancy
in terms of the classical design
discrepancy times a constant and this
this proposition actually follows from
the Whitney Glaser extension theorem for
smooth functions the key implication
here is that this graph Stein
discrepancy actually inherits the
convergence properties of the classical
which means that for our purposes it's
as good as the classical Stein
discrepancy the downside of this
particular graph choice this complete
graph that connects every pair of sample
points is that it still introduces N
squared constraints and that's going to
be too many for large problem sizes so
can we find an equivalent Stein
discrepancy with only a linear number of
constraints define these we're actually
going to appeal to geometric spanners
so what's the geometric spanner let's
imagine that you have a dilation factor
it's at least as largest one then you
can define a graph a weighted graph that
places on every edge a weight that
equals the distance between the two
points and it for any pair of what for
any pair of vertices it has a path with
total weight no larger than T times X
minus T times a distance between those
two points so essentially dilates
distances by a factor of a most T but by
introducing this dilation factor you can
actually greatly reduce the number of
edges you need in your graph just like
in the last slide these Stein
discrepancies based on T spanners are
equivalent from this convergence point
of view to the complete graph Stein
discrepancies and in particular if you
choose a dilation factor of say two
equals two you can actually get away
with a linear number of edges and you
can compute that spanner in n log n time
ok so this gives rise to our recommended
algorithm we're going to compute a 2
spanner on the vertex set that just
consists of our sample points we're
going to use this greedy spanner
implementation which is very efficient
and then we're going to choose as our
norm a bit have a norm floating around
that's been floating around throughout
the talk we're going to choose the l1
norm as our norm this actually leads our
optimization problem to decouple into D
independent linear programs that can be
solved in parallel so we're going to
solve those deal and your program some
the results and that computes our
spanner stein discrepancy so we're
solving for these we're optimizing over
functions G but because now we only have
constraints at the sample points its
advice is just to optimize over the
values G at the same points use them in
advance and for any Q I want to say how
far away is this from p in terms of its
ability to approximate expectations oh
definitely so you can imagine like
flipping this and using it as a measure
for quality meal choosing a new sample I
definitely I've been thinking about that
as well yes I've reports a good MC MC
example yeah for it for instance yeah
so is that the four versus time distance
I remember that the stein discrepancies
and bandit right so the variation is
bounded by two but you're having smooth
functions chilly however that's right so
so what's the natural reference point so
when we compute a number for the
discipline Quincy what's the how do you
base a dumpling how can we calibrate
whether this is actually as a great
question so so far I've use this only a
comparison context where I want to
choose a parameter for a tuning
parameter for a sampler so generate
samples from different tuning parameter
values and I see which one's the best
and the question of calibration I think
is going to come out of imperial further
empirical studies we will learn what
good values good absolute values are but
so far I'm mainly focused on comparative
purposes yeah my questions okay let's
see some examples so let's start with a
very simple example just to illustrate
some of the properties of the
discrepancy so here I'm using a normal 0
1 target so p is normal 0 1 i'm drawing
samples either iid from normal 0 1 or
from a scaled student T distribution
that has matching meaning and variance
and I just want to compute the stein
discrepancy for increasing sequences of
points so i have endpoints from either
and i'm plotting how the steins
discrepancy falls off as increase the
number of samples in my crease the
number of samples drawn so the red curve
here refers to the gaussian samples the
teal curve refers to the student's t
sample and what we would expect is that
the stein discrepancy converges to 0 and
in particular this is a log of all plots
we respected to converting 20
essentially linearly as the sample size
grows and that's essentially what we see
i think the if you find the slope of
this it's converging at a rate of n to
the minus point 505 just about what
would you expect and on the other hand
we'd hope that this time discrepancy for
the students t actually this remains
bounded away from 0 because in fact is
not an accurate sample eventually for a
normal distribution target and that's is
roughly what we're seeing as well on top
of getting this value out you can
actually recover the function
gee that was optimal for that for your
Stein program and so I'm plotting those
functions here and I think a bit more
interpretable are the Associated test
functions which which can be gotten by
passing that function G through the
through the operator TP and these are
essentially the test functions that best
discriminate your sample from your
target they tell you like what function
in my class leads eq to be have a
greatest discrepancy with EP ok and then
plotting this on the right panel here
you can see in particular that the
student's t-test functions have a lot of
extra weight in the tails of the
distribution have largest they have the
largest magnitude values there ok
another example here I'm actually
comparing a few different discrepancies
so I mentioned that we had this
classical Stein discrepancy it turns out
that in the univariate case we can
compute that as well using a convex
quadratically constrained quadratic
program so it allows us to actually
directly compare it with this graph
Stein discrepancy that I introduced and
for simple distributions like normal and
uniform I can compute the boss Christine
distance as well so I'm actually
plotting all of those all those
discrepancies on these plots as the
number of sample points increases I'm
just drawing samples from the target
distribution here what we see in
particular is that okay the red and blue
curves are basically on top of each
other meaning that the graphs time
discrepancy is very closely
approximating the classical stein
discrepancy and the green curve is the
boss christine distance and we see that
even when there's a with even when
there's a magnitude gap we're actually
tracing out the rises and falls at the
boss Arsene curved pretty well so we're
reflecting the same sorts of
fluctuations that we see in bathurst in
distance as well ok potentially more
interesting example and this is based on
an approximate mcmc procedure called
stochastic gradient line demand dynamics
you can view it as an approximation to
land up in diffusion but it doesn't use
a metropolis hastings correction so this
means that the stationary distribution
will deviate from the target and in DB
it's increasingly as a particular
parameter the step size parameter
epsilon grows on the other hand if
epsilon is too small you get very slow
mixing
this chain and so it's actually pretty
critical to choose a good value of
epsilon to get good posterior inferences
so we want to use the stein discrepancy
to choose a parameter epsilon and I'm
actually comparing what would happen
with using the stein discrepancy to a
more standard diagnostic effective
sample size which is a common measure of
autocorrelation if you use effective
sample size to pick your parameter it's
not taking into account your
distributional bias as epsilon grows so
essentially it just chooses the largest
parameter it can so it just says pick
the biggest step size and I want to
maximize this so it says pick the
biggest eps eyes you can if you use the
stein diagnostic it favors an epsilon
that's somewhere more intermediate in
this in this grid region that i've
explored and here's what here's what
characteristic samples look like from
different step size choices so the step
size chosen by ESS is on the right and
you see that the samples that you
obtained are very over dispersed with
respect to the actual posterior
distribution here which is I've drawn
the equitability contours eques density
contours for reference meanwhile if you
pick an epsilon that's very very small
you have very slow mixing and you're
very under dispersed and the step size
parameter chosen biased the stein
diagnostic was this one and this is what
a sample looks like it looks much closer
to the posterior that you're trying to
approximate okay you have to rerun it is
every one of those that's right that's
right bridge step size I really have put
into step sizes um I'm not sure what you
like with all of this emphasizes the
proceeds oh you could do that you
certainly could do that you could do it
adaptive way yeah what's talk more about
that so okay so I mentioned this bias
variance trade-off so here's just a
here's another method approximate random
walk with topless Hastings it also
approximates an mcmc procedure designed
risk and it's a design for scalability
it has a tolerance primer epsilon as
well it's bigger you're actually
computing fewer likelihoods fewer data
point likelihoods you
more rapid sampling and more rapid
variance reduction if it's smaller you
get a closer approximation to the true
metropolis tasting step so you actually
have less flyest in your stationary
distribution and the question is can we
quantify this biosphere inch trade off
so I do this in a beige in logistic
regression setting with a cancer patient
data set this nodal data set and the top
left plot you can see approximate this
approximate method when teal and the
exact method in red and roughly what we
see is as the number of the likelihood
valuations goes up well originally the
approximate that is dominating it's
getting a better approximation because
as a lower variance as lower variance
from drawing more samples but eventually
their exact method over takes it and all
these other plots are generated to give
you some external evidence to generate
these plots I simulated a very very long
chain from mala from the metropolis
adjusted land of an algorithm and I use
that as a surrogate for ground truth and
that and then I was able to compute
various statistics which show comparable
sorts of behavior in terms of the
bias-variance trade-off but in each of
these cases I needed to draw an external
circuit chain and that's what you don't
need to do with a sign discrepancy
measuring ok see I'm learning very long
time so this is then this will be my
last slide there's nothing else oh so
far I've only focused on random sample
points but in fact you could use the
status crepyn see to evaluate
deterministic sample points as well so
here I'm actually comparing convergence
rates of the stein discrepancy for
different deterministic sample sequences
in particular Sobel sequence and a
colonel hurting sequence and comparing
that to what you'd get from iid sampling
and what you can see here are the
estimated rates of convergence the rates
for iid sampling and Sobel sequences
basically accord with what we'd expect
from the literature Sobel's roughly one
on n or log n on n iid is one in root n
the hurting performances actually
outpaces its best-known bound so it's
essentially curving it converging at one
on n rate its best-known bound as 1 over
root n for this particular class this
Hilbert space that we're operating on
which is s there might be an opportunity
for sharper analysis ok so I think there
are lots of opportunity for future work
I'll just put the slide up there and
then let you ask some questions
so we don't run out of time yeah sure go
ahead comments this also might be useful
in helping us how thin an mcmc algorithm
because if you have an mcmc sample you
could have many thin versions and you
can see how much the discrepancy changes
if it doesn't then you stick with one
particular version of thinning without
major looking for that you also use a
real weight your example points yes yeah
I guess you could potentially use it to
calibrate variational base algorithms I
could have a new a good sample for the
variational days and then adjust the
variance but that would be your sort of
epsilon would be some sort of inflation
too because the variational placements
not underestimate the variance I'm gonna
be at you're saying yeah I wonder so
certainly certain the framework of
drilled so far is for discrete sample
measures but I imagine you might be
something more direct with a variational
approximation as well that is let Q be a
continuous distribution and a lot would
have to change the algorithm certainly
have to change but the idea in principle
should be able to should apply so if you
to see Stein functions how do you know
it's sort of picking up important
aspects of deviation from posterior at
this so this is what this is where the
lower bound comes in you can guarantee
that you're only going to have this
function converging 20 if or whether
it's never it's not going to converge to
0 if in fact that your sample sequence
is not conversion to the true posterior
then that's that's the property that you
want so I yeah anyway related or could
it be used for exact sampling for
continuous
so know how to use it for this so
certainly you could imagine using it as
was suggested to draw samples that is
try to pick sample points that are
minimizing this discrepancy measure that
would be that actually be deterministic
procedure but it could still lead to a
good approximation I don't know how you
would use it for exact sampling okay so
let's think Lester again each year
microsoft research helps hundreds of
influential speakers from around the
world including leading scientists
renowned experts in technology book
authors and leading academics and makes
videos of these lectures freely
available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>