<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Dual-Tree k-means with Bounded Single-Iteration Runtime | Coder Coacher - Coaching Coders</title><meta content="Fast Dual-Tree k-means with Bounded Single-Iteration Runtime - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Dual-Tree k-means with Bounded Single-Iteration Runtime</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QciM7aHCwWI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
from you all right so uh Ryan's visiting
is from Georgia Tech he foresees he
worked on a lot of tree related
algorithms which is going to talk about
but he also spent a great deal of time
creating ml pack which is a significant
open-source machine learning system okay
thanks John for the introduction you're
right my name is actually Ryan not
visited but I will respond to both okay
so before I get started I do want to say
that this work has really only come
together in the past handful of weeks so
you are the first live audience to see
it other than my cats and they weren't
very interested so hopefully you guys
are more interested so let's talk about
k-means so k-means clustering i would i
would wager a bet that everyone here
knows it it's probably the most widely
used and most popular clustering
algorithm out there for handful of
reasons the first is that it's simple
the second is that it's probably one of
the first clustering algorithms taught
and most introductory machine learning
courses those two things are related the
third is it's reasonably fast the fourth
is that it only has one parameter which
is K the number of clusters so just as
some quick review how does this
algorithm work we get our K initial
centroids and we get our data set s we
assign every point in the data set to
its closest centroid then we recalculate
the centroids with the new assignments
so the two barely visible centroids here
in red and blue I've colored along with
a bunch of points so your red points are
closer to the red cluster and your blue
points are closer to the blue cluster
then when we recalculate the centroids
after the assignment step the centroids
move and you do this over and over again
until either it converges for whatever
your measure of convergence is or until
you get tired of it and control see it
or until it goes to the maximum number
of iterations whatever what have you
okay so now the simplest implementation
of this algorithm it takes oh of KN time
per iteration because we iterate over
every point then we iterate over every
centroid great and so for the rest of
the talk let's just take n is equal to
the number of points in our data set so
the next question because this is a
machine learning talk we're going to
talk about how to scale k-means
everybody talks about scaling data sets
that is where the term big data does
come
after all so people talking about
scaling k-means usually talk about
scaling n the size of the data set but
it's less often that people talk about
scaling k the number of centroids but it
is relevant for instance if i'm coming
up with a code book for vector
quantization or local coordinate coding
or something like this I may do k-means
clustering with K in the thousands or
tens of thousands or maybe even more if
I'm clustering large-scale astronomical
data I may have K in the thousands also
so when I say ok let's let's say okk is
large and n is large to what are we
going to do we have a number of
approaches the first is we can sample we
can just pick some of the points out of
our data set of size n this doesn't
really work too well it's known to work
worse and worse as K gets larger we also
have approximation approaches I'm going
to set these aside because I'm more
interested in the exact solution today
ok i'll come back to approximation later
we also can select our initial centroids
in a more smart matter i'm also going to
set this aside because this is something
that we can do on top of our k-means
algorithm my interest here is
accelerating the individual iterations
of finding the assignments of every
point and then recalculate in the
centroids so we have a couple algorithms
that do this now first as a blacklist
algorithm which builds a tree on the
data single tree and then iterates over
all of the centroids rules them out for
certain notes then we also have two
other algorithms elkin and hammer Lee's
algorithm which iterate over the data
set sequentially and maintain balance in
the distance between points and
centroids in order to try and prevent
the computation of distances between
points and centroids and so these tend
to be pretty fast but they're not really
tailored to the large K case I should
leave the cap off for convenience so how
do these algorithms actually scale let's
just get it out in a nice table here the
naive algorithm I already told you it
scales as oh of KN per iteration the
memory usage is o of n plus K we need to
store the data set we need to store the
centroids the blacklist algorithm
assuming we're using KD trees which is
what was done in the original papers
where this was described we have an O of
n log n set up cost before we start we
build a tree on the points every
iteration in the worst case can take ok
n log n in case we can prune absolutely
nothing this is
actually how that it tends to run it
tends to be pretty fast and then for
memory memory usage we store the tree
and login and we store the centroids k
elkins algorithm there's no setup costs
but during the iteration we calculate
the between centroid distances that
takes k squared time and then in the
worst case we calculate every between
point and centroid distance at so of KN
time and the memory usage is the same
because we maintain all of these bounds
hammer Lee's algorithm does basically
the same thing but it maintains fewer
bounce in memory so we get a better
memory usage okay and so now there's one
where the line in this table which is
what I'm here to talk about today this
algorithm called dual tree where our
setup cost is takes n log n time our
worst-case runtime bound is okay log k
plus and asterisk because it depends on
some other things which I'll get to
later and then the memory usage is o of
n plus K ok so this algorithm is
empirically several times faster than
other algorithms and if several times
isn't enough for you then all you need
to do is keep making N and K even larger
because it scales better than the other
algorithms ok so I called this the dual
tree algorithm so this means that I now
need to explain what dual tree
algorithms are so I'm going to discuss
the basic idea behind dual traveling
sort of they look like and then I'm
going to formalize it so your typical
dual tree algorithm we build two trees
the query tree T sub Q and the reference
tree T sub R and the query tree let's
consider for for one instance task of
nearest neighbor search the query tree
will hold those points for which we want
the nearest neighbor and the nearest
neighbor will come from the reference
set ok so we have query points we have
reference points query tree reference
tree once we build these trees we're
going to traverse them simultaneously
and as we do this when we visit a pair
of query node and reference node we're
going to see if we can prune it and if
so we prune that whole sub tree we don't
go any deeper otherwise we perform some
base case computation between points
held in those notes ok why do people
care because these algorithms tend to be
really quite fast and we do have many of
them so we can solve lots of problems we
can do nearest neighbor search that's
one example I just mentioned we can do
kernel density estimation and Colonel
conditional density estimation we can do
n body force calculations we can do
endpoint correlation fun
estimates i can calculate minimum
spanning trees i can do max colonel
search i can actually even do a
proximate matrix multiplication but I
point that out not because that's I
don't think that is actually interesting
in practice I think it's interesting as
a thought experiment so but you can do
it is my point so we have all of these
and if you go read the papers all of the
algorithmic sections in them they look
very different so then the question
which I'm going to ask and then of
course answer is can we generalize and
of course the answer is yes we can come
up with a convenient and modular
abstraction through which we can
understand dual tree algorithms but
first I need to bring in some notation
so the first thing is a space tree a
note this is not a space partitioning
tree because I allow the nodes to
potentially overlap so you throw that
out we got space trees interesting
mental image so your tree is a an
undirected acyclic rooted simple graph
with a handful of properties first every
node holds a number of points
potentially zero and is connected to a
parent and a number of child nodes
potentially 0 there's one note in every
tree that doesn't have a parent that of
course is the root and every point in
the data set is contained in at least
one node of the tree maybe more but at
least one then the last bit every node
corresponds to some convex subset of the
input space that the points lie in that
contains all of the points as well as
the convex subsets represented by each
child so another way to say this is
every node has some some bounding shape
that contains all descendant points okay
so here here's a abstract representation
of a tree this tree happens to be binary
it could be a KD tree but I point this
out because I want to make one
distinction here which will be important
for the rest of the talk each point can
hold nodes each point can also have
descendant nodes these two things are
different so n 13 here might hold a
point or two but that may not be the
same as what n four and five and six and
then seven hold so when I say descendant
points I mean the points say the
descendant points of n 13 points held on
all of this this subtree the point L&amp;amp;L
13 and 13 are the points held only in
this node and forth yeah
the space covered by n 13 is this always
a superset of all of this yeah by
definition yep I am going to make one
restriction for today that makes things
a lot simpler it is i can read arrive
all the work i'm going to do without
this assumption but it makes life a lot
easier if i hold points in some node
those points also have to be held in at
least one child of that note okay so if
i have a point that appears it will also
appear in lower levels the cover tree
satisfies this property and KD trees in
general which only hold points in the
leaves or any other type of tree that
does that same type of thing will also
satisfy this property okay so the next
definition I need to introduce and I
have to apologize for this one I promise
this is the only slide that is a wall of
text we need to introduce and formalize
our traversal so our pruning dual tree
traversal is some process where we have
two trees we have a query trini we have
a reference tree as earlier we're going
to visit combinations of nodes a query
note in a reference node and we're going
to visit these combinations each no more
than once when we visit a combination
we're going to perform some computation
to assign a score to it i'm going to
call this a score function whatever that
function is will it will delay that to
later if the scores above some bounder
infinity we prune the combination we
don't go into any two Senate
combinations otherwise we're going to
perform a computation between each point
in the query node at each point in the
reference node now note this is where
the distinction comes in I didn't say
perform a computation between each
descendant point this is perform a
computation between each point so again
if I have a tree which only holds leaves
at the bottom level I will never call
base case until my combinations are both
leaves okay so if we don't proven any
nodes during the whole thing then we're
going to perform our base case once
between every point in the query tree
and every point in the reference tree
okay so this is our abstraction for a
dual tree traversal I won't visit this
again I won't really talk about what
these look like we're just going to
assume that we have this thing okay what
does this get us we have a type of tree
there are tons of types of trees we take
one we combine it with a type of
traversal there are fewer types of
traversals but you could have many types
of traversals we could do breadth-first
depth-first some combination thereof
then we combine it with our problem
specific base case or point to point
function and our score no it's a node
function what do we get we gotta do a
tree algorithm okay what else do we get
we get really easy to understand a
beautiful algorithms okay and so this is
this is opinion but but I think it's
right and I'm hoping to convince you of
this by the end of the talk more
importantly say I come up with a new
type of tree and this tree performs
better for some type of data that
propagates I can just compare that with
the traversal and all of the problem
specific functions that I already have
and now I can say oh my my new type of
tree I can do all of those dual tree
algorithm that can solve all problems
that i can with dual travels with it it
eases the development of new algorithms
and i'm going to demonstrate this
because when i come up with my dual tree
k-means algorithm we're just going to
come up with these two things plus a
little more but that will be the dual
tree part of it and when I sit down and
I i write this in code this is a nice
software of distraction that we can use
okay so implementation is the guy who
writes the tree doesn't even need to
know the guy who wrote to the base case
and scored functions it's probably
helpful and people should know each
other in the same project but you don't
have to so this is very nice okay so
let's take a look at a quick example of
range search we're in for every query
point we want to find every reference
point which lies within a range
parameterised the lower side of the
range as L on the upper side of the
range of you okay so the base case is
pretty simple we compare a query point a
reference point we could actually write
this on one line if we wanted but I
haven't for whatever reason if the
distance between the points is within
the range add the reference point to the
list of results for the query point the
score function is also pretty
straightforward if the range of possible
distances between the nodes so the
minimum possible distance between
descendants and the maximum possible
distance between the Senate so so in
this case here is the minimum distance
and the maximum distance would just be a
little wider add the radio or the
diameters of the notes if that range
doesn't overlap the target range then no
possible descendant combinations can
even lie within the range we can prove
okay and so will return infinity like
we're supposed to otherwise we'll return
0 and you say to yourself wait why are
we not just returning true or false for
when we should prove okay the reason is
recursion order if I'm doing something
like say nearest neighbor search I want
to go to the best combination first so
this is why we return a number which
tells a traversal which may or may not
act on it depending on a traversal which
way it should go okay but in the case of
range search a recursion order actually
doesn't matter so we return 0 guess is
this half so basically quick sorbitol
reversing on one side yeah that's
helpful yes exactly so for the new yeah
exactly this is actually sort of sort of
what that does so for nearest neighbor
search I returned as my score the
minimum distance between the nodes which
is exactly what hap sort I believe sorts
on and then we recurse based on that
priority ok so we've we've captured that
that okay so before I move on I kind of
want to make sure we're all clear on my
dual tree algorithm abstraction so any
questions I'm gonna drink some water
okay cool then let's move on dual tree
k-means so I want to develop an
algorithm where we can do an exact
iteration of the algorithm I showed
earlier is called Lloyd iteration as
what I'll call it we want to do that
exactly and we'll incorporate a handful
of pruning strategies first we will
build our query tree on all the points
in the data set and our reference tree
on the centroids ok so for every query
point we're searching for the nearest
centrally looks like nearest neighbor
search we'll come back to that in a
minute okay so that said we'll use four
strategies for pruning and reducing our
work when we were cursed down a
particular branch of the tree that's
built on the points we should terminate
early if we can determine that only one
centroid can possibly own all of its
descendants when we encounter a
particular node combination we should
prune if we can show that no possible
descendant centroid can own any
descendant query point we soon visit
nodes are points when we can say that
between iterations of this algorithm the
owner can't possibly have changed and
when possible we should use some
bounding iteration from previous
information so if I know that the
distance between a point and a centroid
is this far in one iteration then
I should try and reuse this information
however I can instead of throwing it
away next iteration own what i mean by
yeah yeah by own so when a centroid owns
a point it means that the centroid is
the closest clusters to that point okay
so when a centroid owns a node then
there's no closer centrally to any
dissent a point of the note might have
changed yeah typically you might expect
it not to have changed in a completely
radical manner so yes sweet moves to a
point that sort of close by right and
that's the using bounding iteration from
previous city or bounding information
yeah yeah yeah so we'll loosen the bound
as much as we need in order to keep the
workflow okay so a first attempt i
mentioned i come back to nearest
neighbor search this is this is a pretty
simple idea here's how our k-means
algorithm will work first we'll build a
tree on the point i'll call that T and
then each iteration will look like this
first we'll build a tree on the centroid
and then we'll use our black box of dual
tree nearest neighbor search with the
tree on the points as a query tree and
the tree on the centroids as a reference
tree and what this gets us this gets us
a bunch of a bunch of cluster
assignments for each point and we can
then iterate over all of those and build
our new centralized okay and so this is
cool it gives us actually some some nice
theory but the talk would be too short
if I stopped here so let's complexities
at first we'll build a tree on the on
the points this is where n log n set up
cost will come from and then during each
iteration will build a tree on the
centroids and then if it's not the first
iteration if we already have some
bounding information we're going to go
through the tree and we're going to mark
those nodes which we don't need to visit
in those points which we know the owner
can't change okay and will also update
our bounding and information well loosen
it as necessary okay then what we'll
have is we'll have a tree with lots of
nodes we don't need to visit so will
coalesce it will throw away everything
we don't need to visit make our tree
much smaller then we'll run our dual
tree caning
algorithm which I haven't yet described
on the coal ash tree of points and the
centroid treat once that's done we'll
expand our tree back to how it was
before and then will I will explain it a
little bit later so maybe you can hang
on but basically I'm going to throw away
the parts of the tree that I don't need
to visit which I've determined to be
statically print this will make more
sense than a couple of seconds a couple
of minutes maybe then lastly we're going
to extract the centroids from the tree
using the information we've gathered
yeah yeah sure i can draw an example on
the board i'm developing this algorithm
in a tree independent way so i'm not
going to restrict how we need to build
this tree but let's say i have a couple
of points here if I'm building say a KD
tree what i will do is I'll pick a
dimension so let's pick this dimension
we have two dimensions and I got to find
say the mean in this dimension which I
think will look something like this
maybe I got to split the points along
the meat ok so I get two nodes that look
like this and then my root node has
these to as children or actually this
bounding box can be a lot tighter so
yeah well it's not erasing that's ok and
then I might at the next split I'll
split on the other dimension so I'll
split each of these notes so I'll for
this node the centroid will be I hope
I'm getting this reasonably right but
we'll split by something like this and
one node will hold only this and the
other child will hold this and then I
can do the same thing here and I'll get
these two children does this make sense
reasonably yeah basically I'm organizing
my points hierarchically somehow yes
yeah there are many other types of trees
you've got the the our tree and archery
has like a billion variant vantage point
trees and metric trees random projection
trees these ol should fall into the
definition of space tree
a duel tree nearest neighbor search
actually this is an example of a duel
tree algorithm so once the what I to get
the duel tree algorithm I take my tree I
can bind it with the dual tree traversal
let's say we do dual depth-first okay so
I visit a combination of query note and
reference node and if I don't prune it
then say each of these has two children
what I'm going to do is I'm going to
recurse like this i'm going to go left
left so query left reference left query
left reference write a query right
reference left and a query right
reference right and so i'll repeat this
procedure so it ends up being
depth-first search but a step first
search in two trees simultaneously so it
looks a little more complex I could also
do breadth-first search so I guess I
didn't actually talk about the breadth
first or the depth first portion there I
recurse into all four combination
children and then I recurse down in the
first that makes it depth first but if I
had just gone across all four and then
made my next level and gone across all
those combinations does that make a
reasonable amount of sense yeah okay
there are some complex traversals out
there but again all we need is that we
visit combinations yeah sure yes so let
me let me run back to this here so in
this particular yeah this is range
search yeah so in range search basically
what I'm doing is I'm building a tree on
it and I'm saying when the nodes are
sufficiently far apart throw away the
notes because I can't possibly be in the
range we're looking for so instead of
having to compare every possible pair of
points to see what our range search
results are we can actually prune away a
lot of our work often at a high level
so to where those points live in space
yeah yeah okay should I keep on moving
forward sure if you query for each
individual point individually in the
best possible tree is longer than make
time but if you do a dual treat for all
the query points at the same time as the
reference points you can be close to
time for the point yeah I'll pull out
this result later when i when i get to
the theory section okay anyway we were
here okay so the last thing I said was
well once our dual tree algorithm is
done we will extract the centroids from
it and then we'll fill this vector M
with how much each centroid is moved and
then will iterate okay so now I gotta
fill in the pieces here so first let's
actually develop a contract of what our
dual tree out of them will give us when
it's done so at the end of the running
of the dual tree algorithm will get
these things so as input we pass our
query tree which is built on the points
to our dual tree algorithm we pass our
reference tree which is built on the
centroids okay and we'll get the
following things first every query note
every note of points it may indicate
whether or not it has a single distinct
owner okay and I'm the notation i'll use
sort of is related to the way i
implemented it we can call this owner
function and we can figure out if a node
has a valid owner and then what that
owner is every query node will also hold
an upper bound on the distance between
any descendant point and its nearest
cluster centroid ok so for forever you
descend a point they'll have some upper
bound will also have a lower bound on
the distance between any descendant
point and it's and any cluster centroid
that was pruned okay and these two
bounds or generalization of the probably
easier to understand single point bounds
so for every every point which is a
descendant of a or or which does not
have any ancestors that are owned so so
the ancestors were never pruned at some
point it will have some specific owner
see of P sub Q also have an upper bound
on the distance between that point and
its owner and also have a lower bound on
the distance between the point and its
second closest
and these are the things that I'll use
to statically prune things in the tree
but what I'm going to assume for now is
that I'm going to have some dual tree
algorithm and at the end of it I will
get these things this will allow me to
flesh out the other parts of the
algorithm okay yeah because I'm
interested in the results for the query
points yeah the reference treat is
what's built on the centroids I don't
right now I'm not looking for
information for them so I'm going to
cash results for the query points and
the query nodes okay so let's start with
how I extract the centroids from the
tree this is a recursive procedure so
given any particular node in the query
tree after my dual tree algorithm if
that node is owned we know it only has
one specific owner so add the
contribution of that node which will be
the centroid times the number of
descendants to the to the centroid and
don't recurse if the node isn't owned
and it's not a leaf then we can recur
Stu a deeper level and if the node isn't
owned and is a leaf then we add the
contribution of each point because we
will have the owner of each point okay
and so this this is a made a lot easier
by the assumption i made earlier which
is that every point is held in
subsequent levels this is uh yeah all
the descendant points yes yes yeah so if
I know that when I'm building my
centroids then I don't need to recurse
any deeper maybe I've cashed the
centroid of that node actually I should
have cash the centroid of the node when
I built the tree so I can just add that
to my centroid and then multiply that by
the number of decides yeah so yeah you
need your notes need to mother said the
centroid of the descendant point yes and
you can catch this when you build the
tree for no real significant extra cost
ok so I'm going to assume that we've
done it so now that we have our
recursive procedure the actual procedure
of extracting the centroids is pretty
straightforward we fill our matrix of
centroids with zeros and then we fill
some counts with zeros and when we when
we update our centroids well so
a keep track of the number of points
that are owned by its centroid so then
we get our unnormalized centroids by
callings procedure recursively on the
root of the tree and then we normalize
each cluster by dividing it by the
number of points held in that cluster
okay and then we return or centroids and
we have our centroids ok so now updating
the tree once we've extracted our
centroids maybe we've come back around
we started the next iteration and our
goal is to prune away work that we can
determine that we don't need to do ok so
we'll do a handful of things first those
upper bounds for each node which
represent the maximum distance between
any dissent point in its nearest
centroid we need to loosen those by
however much the the centroids have
moved because remember we have this M
vector how much centroids have moved ok
and we also need to loosen those for
every point we also need to loosen the
lower bounding to decrease them again
because the centroids could have moved
between iterations ok then we want to
determine nodes whose owner can't have
can't change during the next iteration
those nodes will mark them as statically
print we won't do we won't even consider
them we also want to determine points
whose owner can't change and we'll mark
those points as statically print ok so
now the static printing process suppose
yeah yes yeah that'll be what it ends up
being it's there are some if statements
in there but that basically what it is
suppose some point P sub Q have some
owner C sub I ok so here's my point P
sub Q and the owner must lie inside this
upper bound ball that's what I know
because my this this U of P sub Q is an
upper bound on the distance wing p sub 2
and C sub I somewhere in here i also
have a lower bound on the second closest
cluster distance so i know that no
centroids may lie in here and the next
closest century has to lie out here
somewhere okay so that's what i learned
last iteration now i can adjust these if
i increase the upper bound by how much
the cluster that it owns this has moved
because i have that information and then
i can also decrease the the lower bound
on the second nearest clustered by the
maximum movement of the
a cluster okay and so in this particular
case the upper bound ball is still
contained entirely within the lower
bound ball so we know that only one
centrally can exist in here thus we can
mark this point statically pruned its
owner cannot change and so what the
actual computation that we do here is if
the upper bound plus its adjustment is
less than a lower bound minus its
adjustment we can prove okay we have
another strategy for pruning but before
that here's a quick example of what
happens if the if the lower bound is now
too small now we can have a central that
lives in the the distance between the
two balls here and we no longer know
that only one can own it okay so in that
case we can't prove we have one more
strategy in this case take our point and
take our centroid in this case I've
drawn the centroid because it makes
drawing this figure a lot easier and we
also have our upper bound so we know
that the centroid must live within this
ball though we know that also that it
lives right here okay now suppose that I
knew the distance between the centroid C
sub I and its nearest other centroid
okay if I knew that and then once I
adjusted this upper bound so I know that
C sub I must lie in this ball if I draw
this ball here because the distance
between the closest centroid C sub I and
its nearest neighbor is at least this
much there can be no other central that
lies inside the dotted ball here and
because as completely encapsulation
capsule eights the ball with radius
adjusted upper bound then the points
owner can't have possibly changed okay
so mathematically this is if the
adjusted upper bound is less than two
times the distance between the centroid
or the owning centroid and its nearest
other central ok so somewhere during
this update tree process i'm going to
figure out every centroids nearest
neighbor okay so that's a black box
nearest neighbor algorithm that we can
use to do that okay if the distance is
not large enough then we could have a
centrally that falls into here and we
don't know that it can be so we can't do
that okay so here are two strategies for
pruning points these actually happen to
generalize two nodes too so if I have a
node that has a valid owner and then the
adjusted upper bound for that node is
less than the adjusted lower bound for
that note
we can again prune and then the other
rule works basically the same if if the
node has a valid owner and the adjusted
upper bound is less than two times the
distance between the owning centroid and
the its nearest neighbor or its nearest
central neighbor then we can statically
bring the point yeah yeah I am
interested in coming up with better
trees but that's not the subject of this
talk so in this talk I'm basically
coming up with an algorithm that so that
I can accelerate a single iteration of
the k-means algorithm ok so I'm showing
how we can use dual tree algorithms to
do this in a reasonably easy fashion I
have one or two more premium rules the
first is if every descendant point of a
query note is statically print we can
print it there's no work to do there and
if all children of a node are statically
prune we can prune it too ok because the
children roon there's no reason to go
there so I'm not actually going to show
the update tree function because it's
quite complex so I'm just going to write
on the side that what we get out of this
is correct and you'll have to trust me
past there I would say you could
reference the paper but because this is
like I haven't written the paper yet
this is like brand spanking new great so
now we have our static pruning rules
what we get is a say our tree in this
case I've colored the red notes we can
take those as to be statically prune and
the blue nodes haven't been statically
prune ok so coalescing our tree we
remember we don't need to visit the
statically prune stuff so we can just
throw those away and now because I've
assumed that every point here is going
to be held in some child some descendant
here then we can actually coalesce these
upwards oh there it is yeah so now we
get a tree which is much smaller yeah
mean that you could be my definition you
can prune all the points in that yes the
actual implementation of it I try to
prune the whole note at once if I can't
prune the whole note at once and I got
to go through each point and try to
prune all the points but it's possible
that the node bounds weren't tight
enough to prune the whole node as a
whole but I was still able to prune
every point in which case I'd then have
to go back and say okay now we now we
can prune the node yeah okay so we now
get our coalesced tree in practice this
actually tends to be a lot smaller
especially as we get close to
convergence of k-means okay so now the
last piece that we need to fill in is
whether our base case and score
functions that actually are or dual tree
house and we already a strat established
the contract of what the dual tree out
of them will give us now let's see how
we get that so our base case gets called
with two points the query point and then
some reference centroid and our goal is
to find out does this point on the
centroid okay but we'll call this many
times many different points in many
different centroids okay so if the point
is statically prune we can return we
already know it has an owner and we've
already adjusted the upper and lower
bounds on it so we do any work if the
distance between the point and the
centroid is less than or upper bound
then this centroid is a new it's the new
closest centroid candidate so we take as
our lower bound we take our old upper
bound and as our upper bound we take the
distance between the point in the
centroid and then we update or our
current estimate of what the owner will
be okay if the distance is greater than
the upper bound but less than the lower
bound then we just update the lower
bound okay and this is our base case and
it's not too hard to sit here and think
if I called the base case with every
point and every centroid then i would
get valid lower up the lower bounds
valid upper bounds and valid owners for
every point okay yeah these are being
separate event
yes each point has an upper and lower
bound and each node also has an upper
and lower bound and the upper and lower
bounds for a node would just be the
largest upper bound for a descendant
point or the smallest or one for a
certain point yeah but they're not
constructed like that David David me
that's what they represent basically no
I'm asking is is that going to be a
consistency that will actually be
present in the numbers or yes yes the
the upper bound for a node is a valid
upper bound for every point is that what
you said need not be equal to it could
be a little bit looser yes okay that I
answer that all right did you the two
different pruning rules yeah the way
that so shear only move on from the base
case here actually not I want to do that
yet the way that I will find the upper
bound will be the maximum distance
between two nodes this could be greater
than the maximum distance between any
particular descendant combination so the
bound could be loose but it will still
work for pruning ok so if we've all
reasonably convinced herself that this
base case is right which hopefully we
have yeah that's right the nodes have to
abstract the points beneath them yeah
yeah yeah I'm not updating the point
bounds until I get to somewhere in the
tree where I'm actually holding points
ok so if my tree only has points at the
lowest level then I won't actually call
this till I get to the bottom and
hopefully I pruned away most of my work
by then based on the the node bounce ok
so the last thing the last piece is the
score function which is a fairly complex
so I apologize for that as input we have
a node on the query note on the query
points and we have a note on the
reference centroids ok so the first
thing we're going to do if the node is
pruned we return infinity it's print we
don't need to do any work we're done ok
next if we haven't yet visited this node
then the parent could have a valid lower
bound the parent may have pruned
something and the lower bound is only a
lower bound and nodes that have been
pruned so we need to take that parents
lower bound if we haven't yet visited
this query node at any time during this
traversal ok then uh-huh let's ignore
the first line here let's take this
piece by piece but I have to put this
here for scoping reasons if the minimum
distance between the query node and the
reference node so that's the minimum
possible distance between any descendant
point and any descendant centroid if
that's greater than the upper bound then
I already know that there must exist a
better centroid for every descent a
query point therefore I can mark the
reference notice prune ok and because
it's prune if that minimum distance is
now less than a lower bound which is
lower bound on prune nodes I can update
the lower bound ok now suppose that this
isn't true suppose that I can't prune
the node ok i'm going to pick a centroid
out of the reference node just however i
do this doesn't map i'm going to pick
one and then i'm going to take the
maximum distance between the centroid
that I've picked and the query node so
this is a maximum possible distance
between any descendant point in the
query node and any in this centrally
that I picked if that's less than the
upper bound then I know that this C sub
D the centroid I've picked is going to
be closer to all of the descendant
points so i can update the upper bound
and i can take this as the tentative
owner ok now the last bit if I have been
counting what I've pruned and I notice
that all the clusters except one or
pruned then we can say oh that that
owner here which I haven't necessarily
marked this as a valid
I'm just holding C sub D here that owner
now must be the valid owner of all of
the descendant points of the query note
okay so i can say i can mark and sub Q
as a valid owner and then I can return
infinity because I don't need to go any
deeper okay and then the last little bit
we need to return infinity if we marked
our note is pruned up here and otherwise
we'll return a score for recursion
priority okay and you can do a
correctness proof on this I don't really
have time here but basically the way it
works is we show that base case is
correct if we don't prune anything and
then we show that we don't prune
anything that we shouldn't have proved
is the basic strategy here okay so that
said no questions let's move on to the
results I'll start with the theoretical
results so remember that this algorithm
I've come up with is independent of the
type of tree used so I can say well
let's use the cover tree and then let's
use the traversal that goes with the
cover tree okay and when I've said this
this actually brings in some interesting
theory so the cover tree depends on the
expansion constant which is sort of a
measure of intrinsic dimensionality okay
so this is the measure I was talking
about when there was an asterisk in my
earlier bound okay so i assume that my
data set is well behaved and i therefore
assume that C doesn't doesn't scale
within okay so I'm going to make that
assumption for this these theoretical
results and what do I get each node has
up to see to the fourth children oh yeah
I didn't say yeah yeah okay yeah so the
actual definition here is given some
data set the expansion constant is a
smallest see greater than or equal to 2
such that for every point in the data
set I'm going to take a ball centered at
any point of whatever radius and then
I'm going to double the size of that
ball the number of points contained in a
ball of twice the radius can be no more
than C times the number of points in the
original ball let's somewhat related to
the doubling dimension okay so when we
assume that this is well-behaved yes for
all delta 2 i'm sorry i've dropped the
ball here yes so for every point in the
data set and every possible radius
that's greater than 0 okay so assuming
that we get that each node in a cover
tree has depth
of ofc squared log n we also get bounded
tree sites which is nice every cover
tree has no more than 0 of n nodes and
then we get a bounded construction time
of CC to the 6 n log N and this isn't my
work this is John and friends from some
years back but i will now pull in some
of my own recent work some co-authors
and i recently showed that if i have a
general duel tree algorithm so just a
duel tree out of them where i use a
cover tree and the traversal and then
whatever base case and score then i get
0 of n time for a query set of size 0 of
n plus lots of little factors I'm not
going to go into the little factors so
my proof strategy to come up my result
will be first fill in the easy little
factors ok that goes pretty
straightforward then I'm going to say
our base case in score look really
similar to duel tree nearest neighbor
search we're going to be able to show
that our pruning is at least as tight
sat ok then we can use the results from
dual tree nearest neighbor search which
are the expansion constant of the
reference set to the ninth times n plus
theta where theta is a measure of how
good our tree is which generally will
scale sub linearly or linearly with with
the tree side or with the size of the
data set ok so we can just use that
result and what we end up with is that
our dual tree traversal not the whole
algorithm but just a dual tree traversal
part of it it takes the expansion
constant of the centroids to the ninth
times n plus the imbalance of the tree
ok and if I've already assumed that the
expansion constant of my data is
well-behaved it's not really a stretch
to say that the expansion constant of my
centroids are well-behaved because they
also arise in some way from the data set
yes sublinear or linear in n this is I
don't have the tightest bound I can show
right now so this is a real recent work
and I'm hoping to tighten this but the
tightest bound I can show is n times the
aspect ratio but empirically what I see
when the tree is well-behaved then I get
sub linear linear scaling in the size of
the data set
what I mean is that the cover tree is a
it's a level tree so when I build the
cover tree I have a number of levels
okay and so this is like say level zero
this is level one and then this is level
two ok so what the imbalance actually is
is it's a count of the number of missing
level so here I've drawn a missing level
this node has a child that's two levels
lower ok so my imbalance is actually
just a count of missing levels and the
furthest pairwise distance in the data
set x divided by the nearest pair wise
distance ok this is the tightest bound
that I have but I think you can prove
tighter I haven't gotten there yet ok so
what I'm going to say for now with not
all that much support other than
empirical support is that theta is
reasonably well behaved ok I want
something better than this for the paper
I'll get there now we can do our run
time analysis and remember our update
tree step we assume that we were we had
the nearest neighbors of every every
every centroid we can just plug in the
nearest neighbor runtime result here so
we add up the time for all of our pieces
and we get that the whole iteration
takes k log k plus the expansion
constant of decentralized times n plus
theta that's how long a single iteration
takes so now the asterisks are mostly
removed and this is about the scaling
that we get per iteration worst case for
memory analysis our coverage we take so
ven space and storing the centroids take
soph k space and we won't use more than
0 vendo memory during the traversal so
we get 0 of n plus K memory usage for
cover trees for other types of trees
like KD trees will see n log n plus K
usage ok now let's get on to the actual
experiments yeah expecting these see
constants to be completely independent
of k and
all of these problem that is the
assumption for the cover tree bounce yet
reasonably yes so if some other large
exponents yeah so in practice so in
practice these bounds so let's go back
to this here if I actually calculated
this number if I calculated the
expansion constant of the centroids this
number would probably be huge the
assumption is that seed doesn't scale
with n so that as I make and larger than
the scaling characteristics so like I
can't use this bound as just calculated
so but if you're not mother sent drugs
and probably you also want it to be
independent of care right because you
want to scale to large scale that's
right and so the all of the other
algorithms have scaling that of K times
n this is 0 of K log k plus n so in that
sense we've improved as long as we're
making the assumption that the data is
well-behaved which for a reasonable
number of real-world datasets it is okay
so moving on from the assumptions let's
get it down to the actual the actual
results so I've implemented everything
here in ml pack we can run it pretty
easily just from the command line we
specify the data set the number of
clusters the initial centroids we want
to start from the same place and then
the algorithm we want so dual tree
blacklist I'll canned ham early and
naive in some cases I've actually done a
better job of implementation than the
reference implementation so with
blacklist algorithm actually prunes
tighter than what I would download off
the author's website I picked a handful
of data sets the scaling from 40,000
points 26 million and a handful of
dimensions they come from a variety of
settings for initializing my clusters
I'm going to use the refine start
algorithm but when N and K gets very
large this takes an extremely long time
so in those cases I've just used random
initialization okay so what I'm going to
show now is a lot of numbers and what
the numbers are are the average number
of distance calculations per iteration
of the algorithm so I run the algorithm
I count all the times that I called my
distance Oracle or whatever and i divide
it by the number of iterations and I get
lots of numbers okay so what I can see
and I've organized these data sets so
that the small ones are at the top and
we get down to the large six billion one
at the bottom what I can see is that for
small data
sets the overhead of trees doesn't pay
off elkin and hammer Lee's algorithm
just dominate but as the data sets start
getting larger and as K starts getting
larger the dual tree algorithm does less
and less work and then eventually if we
get large enough elkins algorithm runs
out of memory because it uses k squared
+ KN memory and black post algorithm
actually runs out of memory to for my
largest implementation now you call you
could call this a cop-out if i did buy a
computer with more memory yeah i could
get it to run but the scaling that I'm
seeing I would expect it to be way
slower anyway so these aren't ya ya ya
and so I have runtime numbers next ok
also i forgot to say for these
simulations i've used the KD tree not
the cover tree because i have a bug in
my coverage reconstruction code that i
haven't worked out yet but i will get
numbers for that at some point i expect
them to be reasonably similar they
should exhibit the same characteristics
so runtime as you were saying is a more
interesting measure and I do see that
yes because trees are jumping around in
memory whereas elkin a hammer these
algorithms are going sequentially they
have cash benefits there we don't we
don't get that so the tree algorithms
are somewhat slower but at some point
again the overhead no longer matters and
we get an algorithm that works better in
our desired region of interest and the
dash is here I just actually haven't
bothered to run the naive algorithm
because I don't want to wait that long
okay so that said let's make some
conclusions about dual tree k-means
first we get exact results as a not an
approximate algorithm we're doing you
know exact same thing in the worst case
if we make some of sometimes about our
data then we get 0 of n plus K log K
about time when we use cover trees takes
more no more than 200 of n plus K memory
when we use cover trees and importantly
we can use it with any type of tree and
traversal right it's modular all we have
is a base case and score function we
specify our type of tree ok it will work
best our pruning rules our tightest for
traversals that our breadth first in the
centroids ok that's how these rules
happen to work lastly it's empirically
the best algorithm for large enough n
and large enough k ok so now a couple of
words quickly on where we can go from
here first Colonel ization people say
well camions isn't interesting I want to
do Colonel k means well but we can't
some trees including the cover tree and
metric trees they just need a metric
okay well here's a metric given some
kernel and that I can evaluate here's a
metric here's the distance between two
points and kernel space so I can just
plug this in and now I have a tree
that's built implicitly in kernel space
this is great now I can adapt my
algorithm reasonably and do colonel
k-means okay at first glance which is
all that this idea has had so far now
every distance calculation between a
point and essentially takes oh then time
because I don't know exactly where that
centroid is in kernel space and I can
only represent it as a sum of the inner
or the the Colonel's between the query
point and all of the points that are
owned by that central okay so what we
end up with and is scaling us more like
N squared plus K n log KN but we can
compare this with Oh of K n squared
which is the naive case okay in addition
Colonel K means we can see this and we
can easily generalize to other spectral
clustering techniques so there are need
directions to go there okay now the next
question which I said I put off for
later if we're doing you know why are we
doing exact nearest neighbor maybe we
want to do approximation well
approximation with trees is actually
easy remember that our bounding rule
looked kind of like this take the
minimum distance if it's greater than
the upper bound proof we can adjust this
take the minimum distance that's greater
than the upper bound minus epsilon for
some epsilon then we can prune this is
absolute value approximation we can we
can also use some other notions of
approximation okay we can also apply
this idea during the static pruning
process and we can save some work we can
make it even faster ok the next idea and
probably the most interesting idea in my
opinion is parallelism so it's really
kind of unfortunate that this is all I
have to write on the slide about it
that's why I've included a picture of a
shark with a laser on its head so
parallelism remember we have trees we
have traversals and we have our base
case ins pruning rule ok our tree and
our base case improving rule can be
completely independent of the traversal
and the traversal can be the part where
we have the parallelism so I can have my
massively distributed parallel reckon at
my trip pair a traversal that runs on
GPUs
and the guy who wrote the tree and the
guy who wrote the algorithm doesn't have
to worry about it right so this is
actually what I will be going home to do
this summer so wait on more results
sharks with lasers will be coming okay
so let's take a kind of a look at the
landscape of what we have as far as dual
tree algorithms we got trees traversals
and problems we can solve so here's kind
of a subset we got a bunch of types of
trees are going to handful of traversal
and we have some problems we can solve
okay so I've shown new tighter bound for
nearest neighbor search I've shown with
a co-author that we can solve the
problem with Max Colonel search with the
dual tree algorithm I have a faster KD
tree traversal I just showed you k means
that can be generalized to spectral
clustering and I just talked about a
parallel and distributed traversal i
also have a better idea for a
high-dimensional tree which will allow
us to do a proximate colonel pca and you
can even train hmm and with a dual tree
algorithm approximately okay so now we
have all of this stuff how do we choose
how to put it together right so if I
pick my problem then the best tree and
the best traversal for that particular
dataset picking this might actually be
hard so we could just you know pull the
lever and use the cherry tree but this
may not actually be the best thing to do
so maybe a better solution is a black
box okay our user can pass in their data
set the problem that they want to solve
and maybe whatever parameters go with
that data set and then inside the black
box we can take a look at the properties
of the data set and we can make a smart
decision about which tree we're going to
use which traversal we're going to use
and maybe even which set of pruning
rules we're going to use okay and maybe
the user can override these with their
preferences and we get results okay this
is important because people at the
higher level from what I'm doing can
just use this stuff they don't need to
know about trees okay and we can even go
a step further than that and say our
black box doesn't just need to be around
dual tree algorithms it can actually
just be around k memes right it's a
black box for k-means decide which of
these five algorithms are going to use
and if we're going to use trees then
recursively call the black box for trees
okay so some conclusions of the talk
here first dual tree algorithms are fast
and in some cases we get compelling
theoretical guarantees second we can
solve a surprisingly large number of
problems with dual tree algorithms third
there's no good k-means our than for
large and large K or there was no good
k-means algorithm because we can develop
a dual tree algorithm for that case
which has a stir
runtime bounds and oh of n plus K memory
usage right it performs pretty
compellingly when we use in its designed
operating region it is not make your
datasets larger you can use it go
download ml pack on github and just use
it out of the box as easy as I showed
and improvements and parallelism are
coming and it won't just be 2k means
it'll be two dual tree algorithms as a
class okay so this concludes my talk any
questions I hope I didn't talk too long
suppose you want to just a multi-class
classification you want to very quickly
okay then you have a large number of
different labels potential labels yeah
and now the way that you want to
actually make splits the notes is much
less clear because it's some sort of
hoarding problem at the nodes right
right so so this is a rather different
kind of tree from what you talked about
but I'm wondering if you've thought
about this and what not it could
transfer any of these things get
transferred a little bit suppose that I
have actually let's use k-means as a
classifier right so basically the the
label that i choose is the label of the
nearest centroid or just the nearest
central in years cluster whatever that
is okay in this case my decision surface
actually makes sense in some space and I
can build a tree as I did here on the
centroids so I can quickly throw away
stuff that's very far away okay so maybe
I can think about this more for
generalized what models in some other
space does this that I'm postulating
this isn't an exact answer were you you
run your k means algorithm and that
gives you some splits effectively and
you use that to define the tree and what
you're going to learn the mud learn that
or splits and use that wasn't exactly
what I meant but I don't see why you
couldn't do that to kind of come up with
a tree where basically you say if I so
it so I have some very high level
classifier or something like this and if
it classifies left then I get a bunch of
lower grain classifiers on the left and
if I classifies right I get a bunch of
lower gain classifiers on the right and
you do this hierarchically and
iteratively I don't see why it couldn't
be done but I don't have an exact
solution for you here now whoops yeah no
I haven't thought about that I know that
the guys who did the blacklist algorithm
it did then do some further thought on
an algorithm called X means which is I
believe it chooses k based on the EIC
after it runs every iteration I have not
thought about going that direction my
interest here was can I do k-means
faster and turns out I can't so we
should let itself naturally do two
returns I've got a hierarchy so I
thought about agglomerative clustering
for a while single linkage clustering
actually including in space say turns
out to be the minimum spanning tree you
can actually do that fast with trees so
somebody already play another flag I
can't do that i sat next to him though
that counts so you can do that I think
it is possible to extend to other types
of linkages like complete linkage but I
think you can only do that approximately
be matching is another example of a
similar type of approach you could
possibly use hierarchically but again
that I think you can do with trees I
think you can only do it approximately
yeah
it depends on what you're doing say I'm
training a a codebook for vector
quantization this is a the most
compelling example I can find people
actually do this the vector quantization
algorithm as it was originally given
like 40 years ago is something that's
very similar to K mean so people still
use that hand k-means as just black box
I want four thousand clusters will hope
there'll be a decent representation is
that reasonable I've heard of examples
with people trying to do a hundred
thousand things for like search I'm like
that seems to be yeah actually in work
situations where you need to have a
large amount representation power and
you have a large amount of data so
someone say the least well motivated
well then it would actually be helpful I
think it's less clear yeah the biggest
applications i could find were in a
thousands or tens of thousands you say
people are doing hundreds of thousands
maybe i'll start saying that
okay well thanks for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>