<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Session 1B: Life Sciences | Coder Coacher - Coaching Coders</title><meta content="Session 1B: Life Sciences - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Session 1B: Life Sciences</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v7KZNhr-UN8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hello everyone my name is Radha to Doran
and I will present you the a brain
project which has a goal to understand
the impact of genetic variability on the
brain in order to to do this we have
developed an application that joins
genetic analysis with narrow imaging
data analysis and the goal for this is
to offer to medical doctors to
biologists to viewing fermentations the
the necessary information that will
explain the variability that exists
between individuals and more than this
it will be able to early predict some
diseases we will talk later about this
and in order to perform this joint
analysis that we will see that it's
quite computational our approach is to
use the cloud the a short cloud so what
exactly do we mean with the joint
analysis so far in the context that we
are working the medical doctors for
example have three possibilities treat
type of studies there is the clinical
behavior where they supervise the the
patient there is the genetic information
analysis and the MRI magnetic resolution
images more than this two out of three
the one in blue can be even today
consider joint analysis so we can make
correlations and let me exemplify what I
mean with this for example clinical
behavior with genetic information think
about the patient that might or might
not have the down syndrome now if a
medical doctor supervises this patient
in a clinical trial he might have hints
that this patient might have this
disease and he will know we're in the
gene to look to confirm or inform if the
patient has this disease and then is the
same for clinical behavior with MRI
images think about a patient that
have dementia for example but now if we
supervise it in our clinical behavior
this will give us hints that something
is wrong with this patient it must it
might have a disease so the medical
doctors will look in the brain for
certain regions that might be a trophy
ated and vice versa if we see in the
brain some regions that are a trophy
ated well then we have hints that this
patient might behave in certain
parameters in the clinical trial but but
this link between MRI images and genetic
information is not fully understood
today we cannot do the same type of
joint analysis as for the others too and
this is what we want to do we want to
fill this gap and we want to offer to
medical doctors to biology's the
information to perform such joint
analysis here why why do we want to do
this the reason is that there are
several brain diseases that have genetic
origins and examples where you can see
here like Huntington's like oh it is
like dementia and by filling that gap by
being able to perform this joint
analysis we will gain the following we
will be able to understand better how
these diseases manifest itself it will
be possible to early detect them and
even to predict some responses to treat
months the only thing is that to perform
this joint analysis it takes a lot of
computation it it requires a lot of
computation how do we perform this joint
analysis well we perform it by finding
the associations between brain images
and jeans and in order to find these
significant associations let's start by
presenting how the data look like so you
can imagine the two sets of data log
like two big matrices and for the
in images so for this mattress on each
line we have an image this image is
divided in regions these regions are
called voxel volumetric picture elements
or 3d pixel basically and in the trials
that we have today we have like 50
thousands or several thousands of such
regions but with the upcoming MRI
machines that are high resolution they
will reach even millions of such boxes
for the genetic data we already have
hundreds of thousands of such parts that
are called snips SNP snip and we have
this for all the patients that are in a
clinical trial like 2000 and the thing
that we have to do is to take each voxel
with all the genes and to perform the
test to see if there is correlation
there is a significant link or not now
even worse than this so we have a
multiplication let's say all we told
it's actually more than a multiplication
more than this because we are doing
statistics we have to repeat this test
several times so we have all the voxels
with all the genes repeated several
times and after we perform this study we
expect to obtain something like this
which is for example an image with
certain regions in the brain that are
highlighted and these regions are those
locations those fox cells that were
found to have a significant correlation
with a certain snip or gene and this
image actually was obtained with with
our tools this is a more formal
description of what we want to do so we
want to find the relation between genes
and phenotypes by minimizing the false
positive detection and false negative in
order to do this we have to perform
permutations to have more samples in the
statistics three
increase the confidence level on the
results that we obtain so how can we
perform this was a parallel job because
it will be as we will see in the next
slide we cannot talk of performing this
on a single machine on or even off on a
small number of machines well we have
chosen the MapReduce parody to perform
the computation in parallel and we have
here two exemplification on how we can
divide the computation the first one in
the right as I said previously we have
to perform several permutations in and
in each permutation we have to test
these associations so unnatural way is
assigned for example to each map or a
certain permutation and well if we have
ten nappers ten permutation 1001
thousand per mutation and if we do this
by increasing the number of permutations
we increase the number of intermediate
data that we must reduce in the reduced
step the thing regarding this is the
fact that if there is to consider all
the data the workload data mapper has to
perform is very big so we had to go to a
different one with a different method
which is to split the data to split the
computation with respect to the data so
instead of sending all the metrics from
genetic data or brain image we send just
the sub matrix and we assign this to
each mapper and in this way we create
more map jobs and well again then we
have to reduce them in order for both of
the case we have to keep only the most
significant links that were found
between certain genes and certain
locations so far I was mentioning that
to perform such a study is challenging
and let me show you what I am referring
to challenging so challenging it's
challenging from the point of view of
data
why because after we perform the
correlation so we have the brain images
we have the genes the result will be a
matrix that has dimension the number of
voxels which we can see here and the
number of snips which we can see here
this matrix is of doubles and if there
is like for the big test that will be
relevant from the medical point of view
we will have to do 10,000 permutations
which will which means that after the
map phase it results that a total of
almost 2 petabytes of data that we need
to store and then reduce the good thing
is that not all this data is useful only
like five percent and by useful I'm
referring that only five percent of this
data is not zero sitting zero meaning
there is no correlation because between
most of the genes and most of brain
location there is no correlation so it's
normal but still we will have one almost
100 terabytes of data to store and to
reduce which is challenging but it is
from my point of view from the
computation point of view we have again
number of permutation number of voxels a
number of snips which results that we
have 2.5 10 to power power 14
associations test between them to
perform now if we just put the initial
algorithm which is the univariate matter
directly into code no tuning no nothing
it works on a normal machine with
approximately 10 to power for
associations per second our colleagues
the binary form editions were able to
tune quite seriously this algorithm it
works 100 times faster and with this I
proposed to to perform an estimation or
rough estimation on the time spent to to
perform this and it's here but we
aware that here we don't consider how we
store the data the time to breathe and
write the data we just take the number
of Association / this value and it
results that if we use a single machine
it will take like five years just to
perform one simulation so we have to or
we should go to the cloud we should
perform this at large scale and i have
put for the estimation that we will do i
have put this limit 350 course because
we've been told that this should be the
limit in which we should stay with the
deployment yes but so the estimations
are done with with this value so the
first thing is seeing how fast the
algorithm works on Asia it works very
well so if there is to perform the same
estimation of the time but again not
taking the time to transfer the data and
all these we still are able to reduce
from years from five years two days now
even if with the variability that exists
in the cloud with the delays with the
latency with data transfers and all
these we get let's say a double in the
time span with respect to our estimation
it's still days which is acceptable to
perform such a study and for the storage
you hold how to store although all that
data as I will show you in the next
slide we propose to store the data in
the local storage that exists in each
VMs so taking this into consideration
this is approximately the size the local
storage that would be available for one
core multiplied with the course 87 it
results that the storage capacity is
about 87 terabytes which is about the
five percent thresholded of useful data
so things look quite good for the cloud
from these estimations which encourages
us to
go further so this this is the
infrastructure the architecture of the
tools that we have developed developed
for Asia in order to execute this
application and this will be presented
in more details next week the CC grid
conference so the first thing as i said
we propose to store the data in the
computation nodes and why we want to do
this because we will have data locality
the computation and the data will be in
the same node so it will be much faster
we will have higher performance and to
do this we propose to have a distributed
system that is deployed in the
computation nodes in the cloud that will
gather all these local storages and it
will expose it as a uniform big file
system to the application so all the
application in the machines which can be
my port reducer or whatever will have
the same view of a big file system and
it will be very easy to pass data from
one machine to another in addition to
this we also develop MapReduce framework
that uses Thomas blobs for storing the
data and asier queues for scheduling the
messages the map message is the reduced
messages and all these so to show you
how our idea works this is the first set
of charts in which we compare Thomas
blobs with two ways of using Azure blobs
well it's quite two different storages
but this is the thing with which we can
compare in the cloud so the two ways of
using Azure well it's the regular way
and the other way is to set that all the
demands all all the right request will
be processed processed by Asia blobs in
parallel and we have set this to eight
threads that that is actually the
maximum limit and even so having the
data in the computation so having
locality and low latency it
improves quite a lot the throughput we
have represented here the average
throughput that the client achieves in a
machine the next set of chart consider
tried two different scenarios the
aggregated throughput of all the
machines in in the system again we have
compared Thomas blobs with Asia and well
as we expected we have higher
performances and well one of the reasons
for this is again that having all the
storage and the computation isolated
it's much better then if we use Asia for
such computation because Asia is a
public cloud multiple clients excessive
access the storage we have HTTP transfer
of data the last set of charts that I
will present is actually executing the a
brain application as a MapReduce process
on the cloud so the first platform that
we have used is our platform that we
have developed and the other one is a
MapReduce platform that uses a juror
blobs storage and we have two different
scenarios the scenario in the left
considers that we increase the number of
mappers and for this we have used the
parallelization with respect to
permutation so having more diapers more
permutation more intermediate data that
will have to be reduced by the by the
reduce phase and in the one in the right
we consider more genetic data and more
neuroimaging data that we take into
consideration but again increasing the
size of the initial data also increases
the size of the intermediate data and
hence how much data will have to be
reduced and we can see that with our
tools we are able to have a shorter time
span of executing the application which
actually encourages us for the big
experiments that we plan to do
in the future so to sum up our
experience in the obtained project so
far for Asia we have developed some
tools and these tools have scaled very
good up to the 350 course limit so far
the longest in terms of time span
experiments that we have performed were
up to two days but we are very close to
to start the big ones that we saw yes we
expect that they will take like 10 days
for this it will be very important we
still didn't choose the vm that will
best fit because on one hand is good for
us to have as many VMS as possible which
would suggest taking the small VMS but
on the other hand having the big VMs we
have more memory per core and this will
actually increase the performance so
it's it's a very important factor just
this is the only reason why I have
mentioned it here and all the
computation that we have performed for
testing for validating for the
experimentation is 60,000 hours of
computation on Asia this is one course
running for one hour and I have repeat
here an image that resulted from our
tools with some regions in the brain
that are highlighted thank you
and sorry if you want well I can try
something to to show you a demo on the
cloud well some parts but less time when
I try it it was on 29 or februari when a
shark rush so if it happens again if it
doesn't work then it's the edema curse
oh yes let's see if okay so first we
have I don't know if you can see we have
a deployment which is in north central
us on 350 course as I said I will show
you in two steps in order to to go very
fast we have a front end from which we
launched the computation we have several
parameters like how for example here we
we split the computation respect to the
data so in how much parts chunks we will
split the MRI images or the snips
threshold values and things like this so
well let's start it and this is the
number of subjects that we have well if
in case we will have more subject we can
change it but otherwise we should put it
here this is a very very small
computation as we will see we take just
4,000 sub voxels so let's launch it this
will actually start all the computation
in background meanwhile before the
presentation I have done another
simulation in which well some images
resulted so we have here such an example
with regions in the brain as I hope you
can see that they have two colors
because actually depending on how
strength the link between the two is we
have different colors were eyeing from
red which is very weak
two white a powerful white which is a
strong and after all the computation we
we get images like this and what we can
slide show and of course for the billing
fermentations it will be possible to
download all the images and meanwhile ah
it's not skin the computation is
performing so even for that small one we
can see the number of associations that
we need to perform and this is actually
multiply with 1000 it's I'm sorry it's
two four six eight digits plus another
three here number of associations okay
oh my name is Frank olkin I'm from
Berkeley here um so when you did the
computational complexity at the
beginning of the talk yes you talked you
you you multiply the number of voxels
and the number of snips that I
understood but you had this factor of
10,000 for the number of permutations
and I don't understand where that comes
from or why why you have to consider a
firm yeah did you ask wine perhaps yes
of course in order to perform this test
these we are performing actually some
statistics and we have some randomness
that we insert whether or not there is a
computation so it's not just taking the
value for example of the two that is in
the MRI image with the genetic
information and multiply it for example
we have to perform statistics and in
order to do this we have to control
there is a model that the binding for
Marty shins are using in which they are
trying to control with this simulation
how what's the error rate and what's the
chance of having a false detection and
for this they have to imagine that they
have more data and more patient and
since it's very expensive to to pay
people to to come to perform such
studies and more than this after after
we we have for example an image of of
the brain of a patient there is a lot of
pre-processing of this data because all
these two thousand images of patients
are taken in various places in the earth
so all all these MRI machines are
actually generating quite different
images and the patient will move or
don't that exactly in the same positions
so there is a lot of pre-processing that
is done just to calibrate the image
to try to translate the people so like
all of them would stay in the same place
same positions so because of this work
that must be done and because we would
need more and more information we have
to consider that we have more
information and for this we perform the
double and this is the reason why we
perform the permutations actually for
example if we consider here so we have
several patients like this with the
values for each box cells and we take
this this is like a set of subjects by
performing a permutation so for example
changing this we break all the
dependencies that exist between them and
it's like having a new set of data and
this is why we have to repeat this
permutation several times and this is
why we have 10,000 because this is what
the buyer for morticians were saying
that this is relevant for them actually
if we have 100 permutation this is good
for them just to know if they are going
in the good direction or not 1000 is
good for them to actually draw some
conclusions but 10,000 I code them this
is the value that needs to be put there
in order to publish something no
question Joe so I was curious about the
the sniped data so you it seem like
you're primarily interested in protein
expression so you're only looking at
snips and coding sections of DNA are you
looking at all snips from what I
understood from the binding for the
auditions because I'm more oriented
towards I just anything they don't take
all all the music just me I think they
probably already did was that you know
that the way it gets it translated from
RNA to protein is there's there
free nucleotides and they're called
codons and the third one generally does
not make a difference in terms of the
protein you end up with and so I assume
that they sort of filtered out so they
didn't count snips which were not
materias yes yes exactly thank you
genomics yogurt some on USC I might have
missed this in the talk but do you roll
your own MapReduce framework based on
the autonomous blob or did you use
something off the shelf we we have
developed our oh sorry our own because
well once we had the storage and well we
are relying a lot of on a sure queues
and of the properties it was not very
challenging so it's a let's say it's
more like a prototype it's a simple
implementation that works for have you
had a look at Daytona for Microsoft
Research daddy traitor man yes I think I
have just the download it like let's be
honest one month ago but I was in
different places I was traveling and I
didn't have time to actually launch it
and I'm also going next week there is a
I think no not the next week two weeks
from now on there is a seminar about
Hadoop on Asia and we also want to take
a look on that but actually I think at
some point even if we have multiple
possibilities it will be good to compare
them but we will I think still use our
homemade MapReduce because actually we
we develop more than just MapReduce
because well I didn't insist on this due
to time limitations but if you look here
i have put just one reducer because
actually for boeing for Marty shins they
need one result at the end so they need
to filter all the data now if we use a
traditional MapReduce like in Hadoop if
we put 10 reducer or 100 reducers
because of course we have to perform
this computation in parallel this is why
we have went to the cloud now if we have
100 reducers we
100 final results we don't have a unique
final results which is interested for
them and this is another word that we
have done also based on this is to
propose an iterative reduce so we have a
normal map like in map and an iterative
reduce that keeps creating jobs and
considers these results as intermediate
results until we have the unique final
results which I think it's more hard to
to do this on Hadoop but yes yes but
still at some point you need to have
like a final combiner aggregator that
will take all the results in because
once we put this online the our
colleagues that are located in the
different part of France will actually
just want to use that simple interface
and to retrieve a result so at some
point somewhere must reduce all the
results towards only one when they built
their own MapReduce at the time they
were doing it and they started a year
ago on this we didn't have in Microsoft
anything else we didn't have a dupe up
Daytona was not available triad was
around but it wasn't up on Azure and so
they about five or six other projects
that are represented here built their
own so that's common also on the 350
course I said I would explain that these
guys got limited to that because the
azure business desk I was working with
them on getting this said they had a
paying customer that needed quite a few
more cores and at that point science was
not considered a paying customer unlike
got
is
but we are respecting the email we are
keeping in that Linney ok I think it's
time for the next presentation thank you
very much and now we're going to
genomics this presentation is by the
University of North Carolina Charlotte
Asian the body is a grad student working
with Shan Shan memories and the terror
of his presentation is very large operon
predictions via comparative genomics I
am a boy from a Titian as so I was not I
was a computer scientist now in the
bottom fermentation three years ago I
changed to bind to the field of
bioinformatics so the thing that I'm
going to present to you now is another
example of using cloud computing to
solve a simple very simple biological
problem and it's in the continuum of the
keynote that we had this morning as
Joseph was presenting and in his
research they were they were creating
input data computationally and then
analyzing it and then getting results
but there is another aspects of biology
and bioinformatics that you have input
devices biological experiments that
create massive amount of data that you
have to be able to analyze and work on
it for instance this machine on the
right or left it's a sequencer it
creates about 30 to 60 gigabytes of data
per per day so if it runs the whole year
it creates about 10 terabytes of data
and it's only one machine and we have
four of them in our lab so it's a lot of
data and it's mostly text based and it's
sequencing-based but before I get you to
the field that I'm doing the the field
of research that we are doing I'm going
to take you to the life of DOMA the
domains of life there are three domains
of life in nature and one of them is
eukaryotic which is awesome vegetables
and everything that has more than
oneself and they have other stuffs
and then there is bacteria and archaea
they call per carry otics-rio and
different size and they are very
abundant in life there are for instance
if a human body has 10 to the power of
13 cells on a body but you have 10 to
the power of 14 10 times more bacteria
living in your luck in your body so and
inference you have about a thousand
different species of bacteria on your
skin or in your gut so it's it's very
abundant and it does all the thing that
happened almost most of the things that
happens in in the life and very
important that's why and and the feature
if they have is didn't the picture down
there you see that the DNA is not
encapsulated in a nucleus and everything
every single thing that happens in in
this all happens in the cell itself out
in contrast to you Curtis as US but it's
not that oh it's not only important in
life but we have more research going on
on bacterial genomics that are going on
on eukaryotic as you see as of last
night there are 2874 genome sequence in
a bacterial genome but only 173 in other
than that and there are about eight
thousand that are going to be finished
this year at the end of this year when I
started my research January of this year
that's that number two thousand eight
hundred it was our 1500 only so in five
months we see that explosion of the data
here so as I say everything happens in
the cell the thing that we are concerned
about is the translation of DNA sequence
two proteins it happens through a
intermediate molecule called mrna the
gene on the DNA transcribed to the mRNAs
and then translated later on in the same
environment in prokaryotic air to to the
proteins but it happens in eukaryotic
and prokaryotic but there is one
featuring probiotics which is very
important it is that your transcript
your mr a molecule could end up in two
different proteins and this structure is
called an offer on so uh this is
important because when you have such a
transcript you can say all right all of
these proteins that are are being
transcribed and translated together
might be working in the same function
also they the transcription of the gene
is like the transcription of genome is
easier you know that okay it start a
starter transcription five genes and end
up transcription you have to know the
all prong and there is a regulation that
works on the whole opera the binding
sites and transcription factors that
increase or decrease the translation of
the genes work on the whole set so it's
very important it's very important to be
able to answer the simple question of
given a pair of jeans are they in an
operon or not seems like a select but
the thing that we are doing we are doing
prediction based on existing genome and
ink and it's of its identification and
biology's tend to later on Google and
experimental you're verifying it but
when you're doing computation we are
looking at some features that could be
fed to the learning machine algorithms
the features that are working on or for
instance the distance between the two
genes tend to be shorter when they are
in awe prime also you're less likely to
find transcription factors between those
two genes also because those two genes
work together they are evolutionarily
conserved the meaning these two genes if
they happen together in one genome they
should be in other genomes together and
then our mutual expression levels and
functional similarities of these two or
obvious as us as I said when I started
we have 1500 published genome
they tend to change the very front
slides from 150 genes to about 9,000
jeans and you multiply it by a thousand
it would be number of the size of the
genome and this is all this thing that
we have from these genomes is their
genome and their predicted gene in some
more well studied genome you have other
information for instance one of the two
most vile studied genomes of all time
are equal I and B subtilis which all of
both of them has about four thousand
genes or you know that about thirty
eight percent of the gene pairs in
e.coli or in operant for beats that was
you know only about nine percent are in
an O club in an opera it doesn't mean
that the rest sixty-two percent or not
in Opera this is the the more thing that
we have in biology this is missing
information you don't know anything
about the rest of sixty-two percent
because no scientist went around and
checked those sixty-two percent so this
is this thing that we got to take under
consideration in our in our research
back to the features I don't have the
time to go in detail are for all of them
I go into one which is the most
computationally intensive one that we
did gene care neighborhood conservation
uh first thing when you have a pair of
jean you want to see how they change you
know they could get closer or later in
as in genome to your genome tree or they
could be another Genie inserted in the
middle or you could lose one of the
genes in some other genome as in genome
five or or it could have a bunch of
genes inserted between them oh it's it's
easy when you look at it like this but
it's not as easy as it seems because
first of all you don't know with gene in
genome to corresponds to genome to gene
alpha in genome one this is called
ornithologist genius you have to be able
to find the autologous gene of gene
alpha in other genomes and the rest of
them
so are in order to find this a score you
have to do you have to find their target
genes by running a voice blast as
mentioned earlier today a couple of
times and then process the blast results
and then calculate your scores first of
all blast is not a simple algorithm it's
a string search algorithm which finds a
query sequence in a database which is
like is this like this no it's not like
that because it's search for inexact
sequences and gapped sequences and it
takes a lot of time so we have 1500
genomes and we have to do about 2.2
million blass runs each of which take
two minutes and it adds up to eight
years which is the bottleneck that since
such kind of research has never been
done before so we have an ahjumma we had
the privilege of having Anna drunk loud
with 300 notes of small flies are one
core two years of memory and too many
terabytes of data we is estimated that
okay we run this we get this much data
at but at the end we did some
compression we didn't get this much data
at all but we ran the space the HPC
scheduler the HPCA scheduler can do like
a service-oriented applications or
message passing interface and standards
but the thing that we're gonna do is run
a lot of blasts and then run a lot of
other computational stuff on top of it
so we use a simple parametric sweep
application our setup is as easy as
having therefore our setup is easy as
having like 298 compute nodes 1 fronted
nodes and one a head node and then store
every single data that like input data
intermediate data the blast results the
score values and probability values that
we get in the middle on this under Zuhra
storage and a sample run after we deploy
the code is that every single working
node will download the required stuff
that it needs run the code that it needs
to do and put it back and check for the
logs for instant in the case of running
the blast running one genome against
another it will download the two genomes
that it needs to work on from this
storage account and then run the blast
and at last upload the blast results
back compress and store back in the
storage account the run time that we got
the average runtime of blast this step
which was the most time consuming was
about one point 58 minutes 1 minutes and
58 seconds and that ends up to be at the
end about a couple of weeks that
regardless of unavailability of some
nodes and stuff and at the end we had
seven point five terabytes of blast
results oh and the rest of the rest were
small tables which took about twenty
five gigabytes and our initial databases
that like our 1500 genomes were 11
gigabytes so the important thing is this
was done but less than ten thousand
dollars if you although we had the
privilege and we had this and we didn't
pay for it but if you put money on it
it's done for less than ten thousand
dollars and it's a one-time experiment
and when you run the blast you have the
results there and you don't need to do
it again if you want to add one single
genome you just run like run that single
genome against the rest of the genomes
and
you don't worry about doing the
remaining stuff again and those results
can be used later on for any other
research and could be shared with others
uh so our the importance of such thing
that first of all we are I'm going back
to my first slide background at least
that I'm going to emphasize that first
of all there isn't we are in error that
that we can do science in a way that we
could not be able to do before there
were lots of data sitting on the web
that nobody was using it and there are
lots of data being produced that nobody
was being able to process them but now
we and if it was very costly to have and
maintain your clusters and uh and
transferring data was was was very
troublesome but now we could we could
use this power that we have to do
computationally intensive research and
also the prediction power of us the
convent confidence level of our research
could be increased because uh before
that like the research before this when
they did it they had like a handful
number of genomes to comparison like the
most the most number of ginos I have
seemed to be used in an analysis was
about 120 now as you see we have about
2,800 genomes ready and it's going to be
about eight thousand thousand genomes
and it could be used very easily it
could scale up and uh as as the future
data will be available very fast so I'm
going to finish now and I'll thank you
and I'll take questions
a job I saw that you are also using
Python for for this I think it's very
common for bioinformaticians did you had
problems I mean was it hard to put and
have Python in the machines because I
know we had some problems with that well
no there isn't ah you can have your set
up scripts for your computing nodes and
it's uh when this deployment is launched
it has Python installed on it it was
hard to do it like it took me a couple
of weeks to figure out how to do that
but it was like done now when I every
time I deploy I have my Python installed
plus every single package that I have
there and you are also using for this I
guess some scientific libraries like
numpy oh yeah yeah yeah they are darling
install already like one way that I use
I've found it easier to do that is that
I create a local copy of Python with
every single library that I need and
then compress it deployed with the a
result with my deployment and in my
setup script I'll unzip it and have it
there and put the pass variables and
stuff yeah that this is the easiest way
to do it because installing packages on
Python on the cloud is terrible so hard
so I was sort of curious so are you
mentioned that after you ran your
various experiments and then you have
these results from blast doing you know
your sequencing and identifying
potential positions um then you're
saying that you know this is something
you'd like to save and maybe you would
use again or others who use again if you
put yourself in the shoes of another
researcher you say okay fine these data
have been posted what would you want to
be present so that you could really make
use of the resort casino blast has many
parameterizations to it yeah
so of course um that's an issue too but
the main results that i would say could
be very useful for other scientists is
that i didn't mention when you run blast
you have to run something called ortho
mcl which runs an ontology finding
autologous finding algorithm and then
the Dozen and mcl to cluster them that
resolve is in a standard so you can say
all right for all these genomes that you
have you have these are the autologous
jeans because uh some others have have a
trouble finding the autologous gina and
jeans there another question Dennis so
this is very good I take my hat off to
you for having to learn how to program
the cloud as the especially the
Microsoft cloud as it was evolving and
bi was charged so follow up on Joseph's
question I'm wondering is there once you
do have this data in the cloud and
you've identified the parts that people
could possibly reuse how do we get the
bioinformatics community to to discover
that what can we find mechanisms to to
index this stuff to make it available
well I haven't thought about that
actually but the data that I produced is
now being used by one other lab and
they're creating a visualization
software based on ortho legis jeans
that's all they needed and so your x6
they're accessing it directly to like
they have my old credentials to just
access directly to the cloud and get the
autologous information alone as I add
stuff there they'll be they'll they'll
have access to that but how to share it
well yeah
any other question thank you so the last
presentation of this session is going to
be Simon Woodman from Newcastle
University and he works with jazakallah
google hidden Paul Watson and
presentation is fast explosion of the
quazel model space with isa and central
thank you very much and so yes as one
said on I'm sorry Woodman and it seems
we were to a theme today about I this or
computer scientists working with real
scientists if you like and having to put
themselves in their shoes and unwise
versus the last last week said so i
should say that I'm a computer scientist
working collaboration people at the
northern institute of cancer research
and as part of the Venus II project
other people talking about so the
problem that some that we're trying to
solve is that given a particular a
particular chemical compound drug if you
like what are the properties of that
compound and and there are there are
typically sort of two ways to two ways
to think about it one is it we can
perform experiments figure out toxicity
toxicity and things like that but there
are ethical legal issues around that and
the little white fluffy bunny at the
bottom probably doesn't like it very
much not to mention it's time consuming
and so as an alternative and there's a
whole bunch of data that that currently
exists that we can we can use as a
potential alternative to to some
experiments so if we look at the camel
database this has been built up from
published data where people have done
experiments in labs they publish the
results in papers and then in a lot of
cases PhD students have been appealing
paid or delivery of cherished ookie
rekey that data into the Kemble database
and so that they type the type of data
it contains there about 600,000
compounds in there is some identify for
the compound
usually in a string format and then some
measure of activity in the body and so
what we're interested in is a process
called q sr and the theory behind Coosa
is that a particular biological activity
is a function of some properties of
their compound and there are there are
lots of different ways of computing the
different quantifiable properties of a
compound and they're called predicted so
there are things like the solubility the
shape of it the number of particular
types of Adam things like that so it's
what we're interested in doing is
building mathematical models of these of
these descriptors that we can we can
generate so the way that we the way that
we approach this is that first of all we
we we have the data that comes from
Kemble there's a north of dayton Kemble
first of all we trim it because the
certain certain experience of don't
contain enough data points for example
then we it's a classic or aggression
exercise we split it between a test and
a training set we calculate the
quantifiable descriptors and I think
we're currently calculating about 100 of
them then we run a process of feature
selection to knock out the descriptors
that are too closely correlated and that
gives us a set of a set of sets of
descriptors finally we we actually build
build the models and then we look at the
properties of the model and if they're
good we keep them if they're not we
throwing away however there are multiple
different ways of calculating each of
the boxes and that last that last slide
so for instance if you just think about
partitioning the training and test data
you might do a random split you might do
a t20 split there are other ways of
otherwise splitting it then when we're
calculating these descriptors of a
compound there are different tool kits
that we can use to calculate them
so the city called cdk which is written
in java the something called CDL which
directly c++ and there that's those both
open-source ones whereas there are
proprietary ones as well the same for
selection algorithms in three genetic
algorithms random selections and and so
this process that we do branches at each
level as shown there so so for one input
input data set we partition in multiple
ways we then for each those partitions
we calculate a bunch of sets of
descriptors we then on each set run the
the selection algorithms and then
finally we build various different types
of model including linear regression
neural net and in things so so we start
off with one data set but we end up with
about 80 odd models / / data set to
manage this whole process we use
something called East and Central which
is really a cloud platform for for data
analysis it's not specific to to cuse
are a drug discovery it's it's quite
generic in this case we run it on as
your although it can run on ec2 or
on-premise and it gives us data
management and it also gives us workflow
workflow functionality and those
workflows can be composed of blocks
that's written in Java our octave
JavaScript and Python and in different
languages and they can all be combined
into the same workflow it also gives us
a versioning and a provenance provenance
history of what's happened to uptick for
objects I've talked about a bit later on
and with this within this particular
project and say we're running it on as
your and so we run a sound central in a
vm roll any sound Central's it's a web
application and it talks to a database
in the back end and those are those
within as your vm rolls we actually use
a single
a single VM application so that we we
avoid the load balancer and as your
books that that causes problems so
actually to actually perform the work we
then have a whole bunch of worker roles
which run a workflow engine and they're
connected to thuy science central
through a JMS queue we looked at using
the native is your queue but in this
case the the queue isn't ER is not a
bottleneck and we wanted to keep the
portability free science central so so
realizing this this process we end up
with a whole whole bunch of different
workflows and one of the keys and the
the way that we we get the branch and
behavior is that a workflow when it gets
to a particular point schedules other
workflows to be run so probably can't
see it but this this top one is this a
top-level workflow and starts off
calculating descriptors and then it
schedules the feature selection and it
will run a number of workflows based on
the the number of results from the
descriptor calculator stage and we only
we don't know that until runtime within
each within each worker role we have a
excuse me
with each worker role we we run a
workflow engine and because we care
about performance one of the things that
we we do is is when the worker all
starts up it installs a java runtime
downloads the workflow engine installs
it and then executes the engine that
will go to the JMS queue and get jobs
get jobs from it one of the first things
it does is it looks at the the type of
that job is it in our job is it a Java
job is an octave job so and so forth and
does that work role have the the runtime
for that for that job already installed
if it doesn't it goes and deploys it
goes and gets it deploys it if it's if
it's already there it just skips that it
skips that step then it gets the data
necessary it executes the job it puts
the push the data into into blobstore
and the whole process starts again we
found that just having a single message
queue gives us nice failure semantics
for when when roles go down or when we
want so we want to grow or shrink the
shrink the pool size um and it it works
very well so the results this work are
we've built around 250,000 models and
using different and different
technologies and that's come from about
coming up to four hundred sixty thousand
work for executions and four and half a
million actual service calls and we're
in the process of building an
application called kezar Explorer will
let end-users look at those models
search based on on properties for
instance what what activity they're
they're targeting and also get
predictions from those models in terms
of it as a performance evaluation we
actually get very good scalability up to
200 nodes and so when we have 200 nodes
we get a speed-up of eighty-eight
percent and compared to sort of an
either ideal speed up of a hundred
percent and that means that to run in
this case our test data set it costs $51
and takes just over an hour and that's
running 200 200 nodes + + 2 v.m nodes
for the for the server we compare that
to some previous work that we did where
we had we start off with a legacy system
called the discovery bus and this this
grass looking at the throughput in terms
of number of models that are generated
per minute this bottom line was the
previous work and we tried going up to
80 as your load but to actually topped
out around 35 ish i think and the the
top line is the is the performance we're
getting now this graph only goes up to
100 i'm not sure where the missing data
point is it should go up to 200 and it
continues so approximately just of a
slow curve so thinking about so the
applicable ax t of clouds for for this
problem it's actually it's a very
applicable problem because it's bursty
so Kemble periodically updates the
amount of data in its database and
there's typically about ten percent new
data that we can use so we only have to
do a sort of do a large run once and
then perhaps twice a year when Kemble do
updates then we've got another ten
percent to run it also means that when
people are getting predictions we can we
can grow and shrink as to to service
users demand one of the one of the other
things that are interested in is if
people want to supply new modeling
methods then how do we integrate those
into the system so it's we've got four
at the moment but a lot of times when
whence the scientist give but give us
him the talk people ask well well why
don't you do insert my favorite modeling
technique here so we're looking at ways
to to add a new modeling method to the
system and one of the things that we've
learned about what we're doing this and
about getting good performance after
cloud is it really depends how chatter
your problem is and how much data you
have to download her job I don't think
I'll come as much surprised to lock
people in the room
and there are two easy ways or quick
wins if you like certainly for us was
the only deploy dependency wants so
we've got 460,000 workflows if we're if
we're installing all the dependencies
every time then not only a wee wee using
an awful lot more bandwidth than we need
to take time to install these things the
other is to it to avoid storage
bottlenecks so so our current
scalability is limited by accessing the
database and we've pushed as much as
possible into Azure blob store but we're
still limited by updating estate in the
database so with them with doing drug
discovery and so the performance is
great but really we need to capture both
the data that's the output and the
process that we've gone through and this
comes down to release that the
provenance of data and this is an active
sort of research topic of hours so we're
interested in things like sort of a
particular model how is it generated
which which set of descriptors were use
and we're actually we need that data to
be able to get predictions as well how
reproducible are these results and that
actually opens up a completely different
Avenue research about how do you say
whether two things are equivalent do you
do is add a bit of bit wise comparison
but if you've got two models that have
got some some non determinism in you
won't get exactly the same thing out
we're then also interesting that or for
instance if a book is manifested in in a
particular part of the system which
models are the ones affected and how do
we regenerate those so what we've done
is we've built a provenance server based
on the neo4j graph database it's thing
about provenance is it's really just a
big graph is it's talking about things
like sort of this this particular
service we was used to have this input
day to produce that output date which
then fed into the next one and so on and
so forth and we found neo4j is a good
way of storing it querying it and it has
has quite powerful traversal mechanisms
for the graph
to do to build it we've built on on top
something called OPM which the open
provenance a forgotten what the M stands
for model thank you and so we built a a
library that wraps neo4j in terms of
certain lo p.m. interface and they're
starting to look at these of the high
availability options so it's our
provenance so a graph looks looked a
little bit like this so for a particular
model that's been generated with a
neural net model it was generated by a
particular model builder workflow which
used a descriptor values file one of the
one of the nice things about OPM is that
you can have different and different
views on the provenance so so we've got
this high level view of it came from
this workflow but we can also have a
sort of an inspection on the workflow
that says that it came from from an
export files blocking the workflow which
came from a neural net build or
description file and so on so forth and
one of the things that we're also doing
moment is looking at how to apply
security to this provenance graph so so
in a multi-tenant system who should be
able to see what and it gets interesting
if I shared some data with someone and
they processed it in a particular way
what should I be able to see with the
with the provenance caption in our
system it's it's not just one workflow
that has resulted in in a particular
data file there are multiple workflows
that have been linked back to some some
source data and so we have to deal with
the fact that a workflow has an output
and it has intermediate data and how do
we how do we chained them together one
of the things we're interested in is if
we want to regenerate a particular piece
of data but updating version numbers how
do we do that so how do we go from a
provenance trace to something that's
actually executable in the in the
presence of multiple workplace who have
been linked together and so we have an
algorithm that given us at the left-hand
side of to work flows with some
some data that's been generated by one
used by the other we then run an
algorithm on the graph that removes the
the unnecessary intermediate date of
these TD transient data nodes and we end
up with an executable workflow that just
contains the processing blocks and not
the the important export blocks and
that's actually that's next cue to a
workflow in the system so how does that
tie into to the drug discovery will when
someone adds a new a new type of model
builder we're able to mine the
provenance and generate a virtual
workflow that uses the the descriptors
that have been generated as part of
previous experiments and then creates
models for those for those descriptors
using the new modeling method I say
that's that's working working progress
so looking at what are the things we
we're going to trying to do in the
future is first of all sort of scaling
past 200 nodes we've currently got some
database bottlenecks um but certainly
looking at sequel as your to see whether
run whether that helps who's talking
someone else hear about some other other
data access possibilities and and one of
the reasons we stopped at 201 was a time
constraint but also we'd reached a level
of scalable scalability was acceptable
to the scientists we were working with
they weren't screaming for it to it for
it to run any faster so it was a kind of
natural place to stop we're also looking
at ways of visualizing that provenance
and how it's how it's secured who should
be able to see what and then trying to
take a step back and look at sort of the
the meta questions around the cuse our
process so for example which sets of
descriptors produce the best models are
certain data sets just better at
modeling than others and that comes from
from modern from mining the provenance
data
so finally thanks very much for
listening happy to take any questions
and everybody's rants is raising your
hand so a lot of questions is it
possible to use the East science
workflow engine purrs standard
standalone engine or just with the
velocity project and generic worker
night now it's a everyone stand alone
fine even stand line to run on as you it
will run on ec2 or to run on my laptop
so happy to have a chat afterwards and
talk about it well the follow-up
question is is it harder is it possible
to put whatever code you want for
example if I want to put my mapper and
my reducer and to create the MapReduce
workflow with your tools can I'd like
just to meet the the job so it's fairly
straightforward assuming that the model
is sort of file in file out and we have
some tools to help with the process and
some examples and things like that but
as long as it's as long as the codes
either in Java octave are then it's very
straightforward so I'm a knitted
progress since discovery bus okay but
have you thought given any any any
you're in Venus II project where there
are many applications yep have you
thought about how you're ihsan central
services are they generic for those
applications have you explored that q
sirve is one application but it's only
one application there may be a very
specialized one I wondered if you'd
looked at the the other types of
applications so um we haven't looked at
it from a from a venus ce point of view
and what we
so what we find is that we have a bunch
of services that are very generic across
any domain but we have a lot of services
I was thinking about the workflow
components so that they're typically
blocks that deal with matrices of data
so that either transpose it select
columns generals or data cleaning that
kind of thing we see a pattern emerging
and we're working with people who do
analysis of physical activity data and
we're seeing a pattern where they want
applications that upload and data entry
South Central and then run various
workflows on it that have been
pre-approved if you like so for instance
a workflow to clean the data and then a
workflow to extract certain certain
features outfit so we're seeing a common
workload pattern across say a project
was doing physical activity recognition
project that's doing spectral analysis
and the cuse our project so it's a
different types of database in the
workload buttons a comment and questions
I don't know how central drug discovery
yes as an application for you but over
here in the US most of drug discovery
work is done by the in the private
sector when one of the works that we did
with eli lilly big pharmaceutical giant
here they were looking at applications
of provenance and one of the interesting
users they had was in terms of patent
filing so if they need to really keep
track of their drug discovery process so
when they file a patent they can go back
and prove that they actually made an
invention at a certain point in time so
yeah that's one aspect that might be
interesting for you question I had is
hot given that Ming drug this way is one
of the applications how much is hosting
intellectual property on the cloud an
issue in terms of security and all that
it's a big issue and and I think one of
the one of the things that we we've
tried to do is is have multiple ways of
hosting hosting this so if you want
hosted on premise then you can
do if you want to go to the cloud then
you can do we've looked at working with
the NHS in the UK actually on a neuro
imaging project and they they're very
strict so so it's no no data is leaving
our firewall and I noticed on on your
slides earlier you had the the newer
images with and the skill still on and
in the UK that'd be a big deal because
it's is clusters patient identifiable
information because you can really
reconstruct the face
I sure and so we aren't we're seeing
that in this case the the the start the
input data is open source so it's not an
issue but there are plenty of cases
where where it is so I had them also a
question about provenance so on one of
the ways that provenances is actually
also being discussed is there's a lot of
concern that in science communities the
way credit is allocated whoever makes
that publication a top journal like
science or nature but the ironic part
about it is that some may have been have
made a very important contribution to a
key data set for example or key tool
that made it so it was relatively easy
to make that publication in science or
nature so one of the ways provenance is
being talked about is to track who made
what contribution Lynn to better
attribute what it is that you know the
net to the contributions are and and
this would then lend itself more towards
sharing towards reuse of the like and I
don't know to what excites for the
background but I don't know to what
extent this kind of use cases also
within the scope of what you're
considering for Providence so we're
considering it and one of the one of the
things that we were thinking about is if
a date set is is published how can we
host that data set and but also open up
to put a closed group of people for
instance and June reviewers the the
provenance of that of that data set and
I'll allow them a potentially sort of
sandboxed area to then play with that
day to rerun the analysis and really
generate a little bit more confidence in
the in the data that's being published
so I think it's an active area and there
is certainly a lot of exciting
challenges just a quick one one block
per node one work like that night one
workflow / no fair now sorry but but sir
but none of those blocks do sort of
parallel algorithms and not now they're
all that they could do but and we
wouldn't do anything special it would
still be one it
could run a single note hey John
just a small one about the scaling up
you mentioned you only got you did
manage to get eighty eighty eight
percent which isn't great but you fully
understand where the twelve percent went
you said the database is the bottom
there and so there's data transfer and
to get the data into into each
particular node then getting the data
out again there's some updating the the
state of the workflow on the main
service so we've removed most of the
notification messages if you like but
there's still a message to say I'm
starting this workflow and allows us to
do sort of recovery if it doesn't finish
and things like that okay so a hundred
percent was an ideal low en yeah it's
yeah yeah you're very close to the
maximum you can achieve and then so
where is your database if you're not
using the sequel it's in there VN in as
you have no more questions we have a
break and then at 4pm we will continue
the activities the next two sessions
will be interactive services in the main
theater and environmental applications
in this room and both begin at 4pm thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>