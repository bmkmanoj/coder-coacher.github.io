<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lab Tutorial: Multi-Objective Decision Making | Coder Coacher - Coaching Coders</title><meta content="Lab Tutorial: Multi-Objective Decision Making - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lab Tutorial: Multi-Objective Decision Making</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_zJ_cbg3TzY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay could afternoon everyone and
welcome to the lab tutorial and it's a
very great pleasure to have Shimon white
sent from Oxford she wanted his PhD at
UT Austin and then spent a few years at
the University of Amsterdam I think he's
done lots of exciting work in machine
learning and artificial intelligence
from theory to building telepresence
robots and he's here today to tell us
about multi objective decision making
which is something that my wife and
daughter said I could really do with
learning about it thank you Nick for the
introduction and thanks also to Katja
for the invitation to be here it's a
pleasure to be here today and to have a
chance to tell you a little bit about
one topic I've been working on for the
past few years which is multi objective
decision making let me start by
acknowledging my collaborators this is
de rigueur lawyers who was a PhD student
of mine when I was still in Amsterdam
and is currently a postdoc of mine in
Oxford and that's frontally who's a
longtime collaborator of mine is
currently at the University of Liverpool
and some of the research I'm gonna tell
you about today was funded by a grant
from the Dutch National Science
Foundation so what I'm hoping to do in
this talk is to start by really
explicitly motivating multi objective
decision making because unlike some
other research topics the motivation is
actually not self evident so I want to
really convince you that it makes sense
to reason about multi objective models
and to develop multi objective
algorithms then on the basis of that I'm
going to propose a problem taxonomy that
sketches a few different kinds of multi
objective problems and describe to what
the appropriate solution concept is for
each of those problems then I'll talk
about one particular algorithm for
solving multi objective problems that we
developed which is called optimistic
linear support and if I haven't run out
of time I'll talk about an application
of that method to a particular multi
objective problem that involves
coordinating the behavior of multiple
agents okay so let's begin with the
motivation there are many many
real-world problems that are naturally
characterized as having multiple
objectives for example if you need
medical treatment you might want to
maximize the chance of being cured but
also minimize the chance of having side
Peck's are dying if you're trying to
build an intelligence system that will
coordinate traffic in an urban area you
may want to minimize latency maximize
throughput respect some kind of fairness
principle for all the drivers minimize
environmental impact and so on here's
another example this is a benchmark
problem that we use in a lot of our
experiments is about mining commodities
imagine you own a mining company so
these these red squares are all the
mines that you own in the mountains and
in all of the villages those circles
they're a bunch of workers and every day
you have to decide which workers to send
to which mines and when the workers go
to a mine they can mine a certain amount
of gold and a certain amount of silver
and these are two different objectives
that you would like to maximize okay so
we can see that a lot of problems are
can be naturally characterized as multi
objective but that actually doesn't
necessarily imply that we need special
multi objective models that we need new
algorithms for dealing with situations
that are that are naturally thought of
in a multi multi objective way so I want
to try to convince you that in fact we
do need those models I want to start
with something called a Sutton's reward
hypothesis so some people actually don't
believe that we need multi objective
models some people explicitly argue
against having any kind of multi
objective models or algorithms and one
of these people is rich Sutton who quite
literally wrote the book on
reinforcement learning
so he's formulated what he calls the
reward hypothesis which says all of what
we mean by goals and purposes can be
well thought of as maximization of the
expected value of the human of some of a
received scalar signal the reward so
here I've just tried to sort of
formalize what what Sutton is saying in
the reward hypothesis so we have here
this the capital Pi this is a set of
policies which we can just think of as a
set of possible solutions to our
optimization problem and this function V
is a value function it maps each one of
these policies to some scalar some
scalar value that says how good that
policy is and as Sutton is saying the
value of any given policy can be thought
of as the expectation of some summation
of some reward signal and to solve the
problem we want to find the best policy
and the best policies of London which
will maximize this scalar award
waited over what so you're saying it's
my simple as most that much to you so
they do waited some that's your these
yes so that's that's a scalar is a ssin
of the problem which are I'll talk about
in detail in a couple slides so I think
that will become very clear so but but
what Sutton is starting from the
position that we don't have these
multiple objectives he's saying okay
every policy can be mapped to some
scalar value so we don't have multiple
objectives yet so there are several
things that I could object to in the
reward hypothesis I could say hey
sometimes I might want to be risk
sensitive and not maximize expected
value or might say I might have a
problem where the rewards don't
decompose as a sum a non additive
problem but for now I'm just gonna focus
on this one aspect of the word
hypothesis which is that the the
assertion that this signal is this
scalar okay so in order to motivate
multi objective methods we're going to
have to argue against the reward
hypothesis and there are two two parts
to this argument what I call the weak
argument and the strong argument and the
weak argument is the argument that I was
already implicitly making at the
beginning of the talk when I showed you
these examples the weak argument says
simply there are many real-world
problems that can be naturally thought
as having multiple objectives so rather
than mapping these policies to scalers
we map them to vectors so n here is then
the number of objectives and so every
policy has some value in each objective
and I'm using bold here to indicate
vectors so V is now effective the reason
this is called the weak argument is
because the devil's advocate could come
along and say hey why don't we just
scalar eyes this multi objective problem
and what do we mean by scalar eyes we
use the scalar ization function that
projects the multi objective value onto
a scalar so here we have on the left
hand side this V is not bold so we have
again a scalar value and this scalar
value is determined according to some
weights which are the weights of the
scalar is a ssin function that converted
the multi objective value back to a
scalar one so f is our scalar ization
function it's a function of the multi
objective value and some weights that
parameterize the function so the most
intuitive case which I will talk about a
lot today is the
your case in which case the scalar is a
ssin function simply computes an inner
product between the multi objective
value and some weight vector so what
this weight vector is doing is
specifying an a priori prioritization of
the objectives and the the devil's
advocate objection to the weak arguments
to say if I just perform this scalar
ization I have a single objective
problem again and I can just apply all
my single objective methods off the
shelf without having to do any new
research without having to develop any
new methods so to respond to the devil's
advocates the devil advocates objection
we need to make a strong argument so the
weak argument is necessary but not
sufficient and a strong argument says
this a priori scalar is a ssin is
sometimes impossible infeasible or
undesirable and in such cases it's not
sufficient to take a single objective
method which will give us one policy as
a solution instead what we need to do is
produce a coverage set of all undaunted
solutions all solutions that could
possibly be optimal when later we
perform the scalar is a ssin that now we
are unable or unwilling to do okay so
why would we be unable or unwilling to
two scalar eyes a priori there are three
different scenarios that that sort of
capture the reasons the first scenario
is what we call the unknown weight
scenario so in this scenario we simply
do not know the weights of a scalar is a
ssin function at the moment at which we
have to do the planning at the moment at
which we have to select do all the
computational work to figure out what
policies are good so the idea here is
the weights will be known later at the
moment at which we have to execute the
policy but they're not known at the
moment at which we have to do the
planning so that's what this figure sort
of illustrates we have a multi objective
problem we feed it as input to our
planning algorithm which produces not
just a single answer but a coverage set
the set of all possible undaunted
solutions then later the weights become
revealed to us and in the selection
phase we use those weights to scalar
eyes the problem and select a single
policy from this covered set which we
then execute yes yes so you can think of
it that way so at the moment at which we
we find out the weights it's too late to
then do all the planning
satisfy their planning algorithm we need
took half a second to execute you might
think too much I've done the planning
algorithm with the old single objective
yeah absolutely what you should it takes
ages in comparison to the amount of time
between when the weights are revealed
and when you have to act can you make
something yes so figuring out the figure
out the best way to send all the workers
to the different minds may be very
computationally expensive so that's a
planning problem which may require a lot
of resources and doing that at the last
minute when the weights are revealed may
not be feasible so that the think of the
price of gold and silver are changing
throughout time just according to the
market the later that the the later that
you make the decision about when to
about how to prioritize gold and silver
the later you make that decision the
better off you'll be because the the
closer the decision will be to what the
market price will actually be when you
sell gold and silver oh sure yeah yeah
so the cover set might have other uses
besides just selecting a policy to
execute yes yes agree okay so another
scenario is what we call decision
support scenario in this scenario it's
not so much that the weights are unknown
it's that the weights can't really ever
be quantified in the first place so the
weights if they exist they exist only
implicitly inside the brain of some user
so so the user cannot really
meaningfully quantify the weights but
the user can express preferences so for
example in this medical treatment
scenario if I said okay how much more
important is it to you to avoid side
effects than to be cured of the disease
is it 2 times as important three times
as important to point four times it's
hard to meaningfully quantify that
distinction but if I say okay here's one
one treatment option with a certain
profile of risks and here
another treatment option with a
different profile of risks you can
probably figure out which one is right
for you so in the decision support
scenario everything proceeds exactly as
in the unknown weight scenario we have
this multi objective decision problem we
compute this coverage set but then in
the selection phase instead of the
weights being revealed we just show the
coverage set to the user and the user
selects whichever policy he wants
according to his idiosyncratic
preferences which implicitly correspond
to weights that are somewhere in the
yeah that's what I mean when I say the
weights may exist implicitly in the
user's brain like implicitly the user is
using a scalar is a tional function
which has some weight there are two
issues one issue is is the scalars a
function linear or not in both of those
cases multi objective reasoning can make
sense but the nature of that multi
objective reasoning will be different
and I'll talk more about that later but
do you want to be multi objective at all
I think depends on whether you either
any of these three criteria so only an
unknown weight scenario do we say the
weights actually we cannot use the
weights when we have to plan in the in
this one when it's infeasible that's
decision support you know in principle
these weights do exist and it's not
there there is known during execution as
they are during planning but it's just
hard to really put a number on them are
you going to extract that number from
the users brain it's easier to have to
show them options and have them choose
those options than to ask them to pick
to pick weights of a scalar is a ssin
function and then the third scenario
where it's just infeasible to scalar
eyes is what we call the known weight
scenario I'm not really going to talk
about this today but I just want to
mention it for completeness there are
some scenarios where you actually do
know the weights and the planning phase
and you could scalar eyes but that
scalar is a shin yields a problem was
actually much harder to solve and there
there are some reasons for the
which I which I won't really go into but
just for example if you scaler eyes
according to a nonlinear function then
you might no longer have additive
properties so you may not only be able
to decompose this value as a sum the way
Sutton claimed in the reward hypothesis
in which case whole families of methods
based on things like the bellman
equation would not be applicable so may
actually be easier to just solve the
problem in a multi objective way even
though you could scalar eyes so those
are the three scenarios so this is a
summary of the motivation basically
multi objective methods are useful
because many problems are naturally
characterized by multiple objectives
that's the weak argument and cannot be
easily scale Erised a priori that's a
stormer okay so hopefully I've convinced
you that at least in some cases it makes
sense to think in this multi objective
way so now what I'm going to do is
propose a taxonomy of multi objective
problems and talk about what what
constitutes a solution to each of those
problems okay so if you look at the
multi objective literature there are
kind of two different proaches to coming
up with solution concepts the first one
is what I call the axiomatic approach
and in this approach we just define the
optimal solution said to be the Pareto
front and the Pareto front is the
concept that's probably familiar to most
of you in the utility be other approach
which we call the utility based approach
we try to actually reason from from more
basic principles about what the solution
concept should be so the reasoning is as
follows during this execution phase at
the end of the day we can only actually
execute one policy so if that if we
select that policy in a rational way
then that policy will be optimal with
respect to some scalar utility it will
be the policy which maximizes scalar
utility according to some weighting of a
scalar ization function and this w may
be hidden or implicit because it might
be in the users brain but the idea is
that it exists in some platonic sense
out there it might or might not be
linear I'm not I'm not committing to the
linear assumption yet now I'm speaking
generally so this is the these are just
W just parameterize is some arbitrary
scalar is a ssin function f ok so given
given that the policy that we select is
optimal with respect to some W
then the goal in the planning phase is
to find a set of policies that contain
an optimal solution for each possible W
so we don't know what W is when we have
to plan so we want to cover all our
bases by having a policy that's optimal
for all W so if W is unknown then in
general this the size of this set will
be greater than one so what we want is a
set that doesn't contain policies that
are suboptimal for all W because even
though we don't know W yet we are we do
know that we don't want those policies
we can already rule them out so multi
objective planning can be thought of as
a pruning process we prune away the pot
of the policies that we know won't be
useful even though we don't know W yet
okay so what exactly does this set
consists of that we can deduce from
properties of the of the optimal
solution set and this scalar is a ssin
function so for example if we say the
scalar ization function is linear then
we'll end up with a different solution
set than if it's nonlinear so that's
what I'm going to that's what the
taxonomy will will illustrate so let's
start with some definitions let's
formalize this idea that we want to
prune away the policies that will never
be useful okay so an undated set it's a
subset of all the possible policies for
which there exists some W for which the
scalar i's value is maximal so it's a
set of all policies such that it's in
one of the possible policies and there
exists some weight such that for all
other policies PI Prime the scale Erised
value with respect to that weight of the
policy PI is greater than or equal to
that of PI prime so there exists some W
that sort of justifies the existence of
this policy because at that W that
policy's optimal okay but it turns out
this is slightly overkill because this
this unde Amin a set can include Paul
some policies that we don't actually
need to keep so what we actually are
interested in is the coverage set the
coverage set is a subset of the
undaunted set so set for every W there
exists a policy in the coverage set that
maximizes the scalar value so it's a
subset of the undaunted set and for
every weight we have some policy that's
in the coverage set and which is optimal
for that weight okay
here's a simple example that just
illustrates the distinction between
undaunted set and the covered set so
here we have normally this this weight
vector is going to have continuous
values so we're gonna have a continuous
weight space but in this very simple
example we just have a binary weight
space there's just two possible values
that W could come could take on now I'm
not saying they're only two objectives
I'm not saying anything about how many
objectives there are I'm just saying
this multi objective problem is scale
Erised with respect to a scalar is a
ssin function that's parametrized by W
and W is binary okay so there are two
possible scalar is a shion's and now we
have four different policies and for
each policy we can see what the value of
that policy that the scale Erised value
that policy is under each of the two
possible scalars Asian's okay so now we
can easily see that the unde AMA native
set in this example is policies one two
and three because for each of these
policies there's one scalar is a ssin
for which that policy is optimal right
however we can construct the coverage
set that uses only two of those three
policies either 1 &amp;amp; 2 or 2 &amp;amp; 3 because
there's only two possible scalar is
Asian's and we can cover both of them
using only two policies ok so what we're
trying to find then is the coverage set
once we found this coverage set then in
the execution phase we're going to use
that coverage set to figure out which
policy to actually to actually actually
execute in the unknown weight scenario
the way to reveal to us and then we can
just select the maximizing policy by
performing this Arg max we just iterate
over the policies and our coverage set
and find the one that has the largest
value scale Erised according to the
weights which are now no no no it does
not need to be finite so I'm gonna
discuss scenarios where the where we
consider both deterministic policies or
stochastic policies and if it's
stochastic policies then it's clearly
there's an infinite number of possible
policies
where your cursor is now if you just
replace hi in CS and with PI in all
policies then that's the original
problem spec absolutely well it just has
to be smaller that's the point is we
have we have like a partial planning
process we want to do as much as we can
before we know the weights and then
afterwards we'll do whatever we couldn't
do beforehand so the more policies we
prune away the more work we save during
the execution phase absolutely yeah yeah
definitely
the smaller the better and of course you
can construct worst-case scenarios where
the coverage set contains every possible
policy and then the multi objective
planning offers no leverage at all okay
so in decision support then we just show
the covered set to the user in the user
selected policy okay
so here's the problem taxonomy we're
going to look at two different
dimensions so one is is this scalar is a
ssin function linear or does it Oh BAE
only a weaker assumption that's
monotonically increasing and do we
insist upon having deterministic
policies or do we allow stochastic
policies so for each of these four
scenarios I'm going to describe what the
appropriate solution concept is so let's
start with the top left let's consider
for now only deterministic policies and
let's assume the scalar is a ssin
function is linear so as I already
mentioned if the scalar is a ssin
function is linear it just computes the
inner product between some weight vector
and the multi objective value which case
each weight is quantifying the
importance of one of the objectives for
mathematical convenience we typically
assume that that this is actually a
convex combination so that all the
weights are greater than or equal to
zero and that they sum to one linear
scale ization functions occur a lot in
practice and they're very intuitive
particularly in any situation where
utility at the end of the day translates
to money then a linear scalar ization
function makes a lot of sense because
those weights just correspond to the
cost per unit of each objective so for
example the cost of gold and silver
would be then the weights of of the two
objectives
okay so if we assume a linear scale
ization function then the the analog to
this unde Amin ated set is the convex
hull so the convex hull is the sub set
of policies for which there exist at W
that maximizes the linear scale Erised
value so this definition here is exactly
the same as the definition I showed you
earlier for the undaunted set except now
I've explicitly computed the scalar i's
value using this inner product because
the scalar is a ssin functions linear
right and the analogue to the coverage
set is the convex coverage set which
again this is the same definition as
before but now we compare policies using
this inner product okay so we can
visualize this in two ways as erequest
we can visualize this in two ways by
looking at objective space in weight
space these figures are very important
for understanding everything that comes
later so I will do my best to make this
clear so on the left hand side we have
objective space so what we have a
problem here with two objectives
objectives zero and objective one and
each of these points corresponds to a
different policy in the location at that
point tells you the multi objective
value of that point so here we have a
point that this is a better example this
point gets a value of 2 and objective 0
and a value of 1 in objective 1 ok so
the convex hull consists of the red and
blue points here together and I've drawn
this line connecting them just to give
some intuition as to why this is called
the convex hull and however the blue
point is on the convex hull but not in
the convex coverage set and we can see
why it's not in the comics covered set
by taking a look at weight space so in
weight space on the x axis here we have
the value of one of the weights so
remember because we so we have two
objectives so the scalar i's value is
this this inner product and because we
assume that that it was a convex
combination we know that W 0 is equal to
1 minus W 1 so I can completely describe
the weight space in this simple example
by just specifying the value of W 1 ok
now each of these points in objective
space corresponds to a vector in weight
space because each of those points has a
multi objective value so that's that's
its value at the endpoints this is the
value if I only care about objective one
this is the value if I only care about
objective zero
and the scale Erised value will change
according to this line as I change where
I am in weight space and because the
scalar is a ssin function is linear it
changes in a linear way yielding these
vectors okay so what is the convex
covered set the convex coverage set or
all the policies whose vectors
contributes to this undaunted upper
surface and if you have any experience
with planning and partially observable
Markov decision processes this will be
extremely familiar to you
okay now the blue line does participate
in that upper surface but we can still
represent that whole upper surface
without that blue line because it only
contributes at this one point at which
other policies are also optimal so this
illustrates why this blue point is in
the convex hull but not in the comics
covered set okay so that was the top
left corner because we have a linear
scale ization function and for now we're
considering only deterministic policies
our solution concept consists of a
convex coverage set of the termina stick
policies the covering set right under
this last slide the thing you didn't
tell us but I think you're saying is
that those guys at the top a kind of
small subset of the points that are
going to characterize you're giving us
some reason to believe that the set is
small we're going to go you just gave us
a definition which could have been the
whole set for we knew it can in the
worst case be the whole set yes okay
through these pictures you're
encouraging us to believe that for this
linear thing in practice in practice
that's absolutely the case but yeah I
mean you can easily construct an example
where each of these vectors participates
in a supper surface and then you just
can't do anything
so yeah exactly thank you for helping me
to answer that because okay so what if
I'm not prepared to assume the scale
relation function is linear then we can
make a weaker assumption which is that
the scalar is a ssin function is
monotonically increasing so the Scottish
function is monotonically increasing if
changing a policy such that its value
increases in one or more objectives
without decreasing in any other
objectives also increases the scalar i's
value so in other words all else being
equal I'd rather have more of any given
objective and I would argue this as a
minimal assumption because if we don't
satisfy this assumption then we don't
have the basic notion that that an
objective is something that's good
meeting an objective is something it
could the value is no longer inherently
positive so I think this is a minimal
assumption if we make this minimal
assumption then we know we can ignore
policies that are Pareto dominated and a
preto dominated policy so a policy PI
preto dominates another policy PI prime
if its value is at least as high in all
objectives and strictly higher at least
one objective and this is a concept
that's probably familiar to many of you
now policies preto optimal if no policy
Pareto dominates it okay so the analog
to the unde Amin ated set in the case
where we have these monotonically
increasing scalar ization functions that
are not necessarily linear is the Pareto
front the Pareto front is just a set of
all policies that are not Pareto
dominated and again this might be
slightly overkill so we can define the
Pareto coverage set it's a subset of the
Pareto front subset for every policy PI
prime it contains a policy that either
dominates PI primer has equal value to
it okay so let's go back to our
visualization it's exactly the same
visualization as before but now I've
added a new point here this black point
or so I've changed that point from gray
to black because this point while not in
the convex hull it is on the Pareto
front we can see that it's in the Pareto
front by looking at the region that's to
the up and right of it it's empty there
are no dots there so there's no policy
that Pareto dominates this policy
therefore it's Pareto optimal and in the
Pareto front and if we look at weight
space so this black line corresponds to
this black point it's on the Pareto
front we can see that because even
though this black line doesn't
tribute to this unde Amin ated upper
surface there is no single other line
which is always above it so Pareto
dominance is a pairwise notion of
dominance that means you need to find a
single other policy which dominates it
and that's not possible for this black
line but it is dominated by a
combination of other lines and that's
why it's not on the convex hull okay
right so what we've concluded then for
this bottom left corner is it if our
scale is a SHhhh infection is only
monotonically increasing we want instead
of this convex covered set we want this
Pareto covered set okay so let's now
relax the assumption that we have
deterministic policies and let's
consider stochastic policies so what is
the what is the deal with deterministic
versus stochastic policies in multi
objective decision making so in most
settings stochastic policies are fine
however there are some settings sorry
okay mr. gesek policy is one that that
involves some randomness that selects
actions probabilistically exactly and
that you're hitting on exactly why they
might not be desirable in some settings
so if you're making a control policy for
a robot you might not mind if its
behavior is stochastic but if you go to
the doctor and the doctor says I've got
a great treatment plan for you the first
thing we're going to do is flip the coin
you might not feel comfortable with that
and there might even be ethical reasons
why the doctor wouldn't shouldn't
propose such a solution so you might or
might not agree with that maybe you
would be fine with flipping the coin
yourself but the point is there are some
settings in which we at least some
people don't want stochastic policies
now in most settings it's re in many
settings from a decision-making
perspective requiring deterministic
policies is not a restrictive assumption
so for example in the Markov decision
process it's well known that there
always exists at least one deterministic
optimal policy so you can restrict your
attention to term asset policies without
sacrificing value and for multi
objective extensions
of such settings so for example a multi
objective Markov decision process if I
use a linear scale or zation function
then I have a nice result that that I
can again restrict my attention to
deterministic policies so the result
says that the convex coverage set
applied to the space of deterministic
policies is the same as a convex
coverage set applied to the set of all
policies so looking at only
deterministic policies is a lossless
restriction I'm thinking about it might
actually really occur in medical doctor
might say to you this is medical trial
that I can put you in for which sounds
promising but if you go for this a 50%
chance you'll get put in the control
group the doctor might indeed toss a
coin so the key issue the key issue
there is whether we consider that so
cast stochasticity part of the
environment or part of the policy so is
that is that a coin flip which is under
your control I mean you try to rephrase
that do you get to choose to flip the
coin obviously don't get to choose the
outcome of the coin do you get to choose
to flip the coin or is that something
that that happens externally to your to
your policy so even if we restrict our
attention to deterministic policies that
deterministic policy may be applied in a
stochastic world the world may respond
stochastic lis to our deterministic
actions like I decided to enter the
trial and then the world flipped the
coin to say whether I was lucky or not
but the point is in some settings so I
here I said in many settings
deterministic policies are not
restrictive but there are settings in
which it is in which case if you have
stochasticity under your control you can
choose to flip coins then not doing so
will actually hurt you there are some
such settings okay so here I'm saying
Indy go ahead
the two main sources of benefit to
stochasticity our adversarial settings
if the other agent is against you then
you want to an element of surprise it's
like in a game theoretic setting and the
other one is partial observability so if
you don't reason about partial
observability in an optimal way then
having a stochastic behavior can help
you sort of prevent getting stuck in a
rut okay
so within the settings where
deterministic policies are not
restrictive then we can extend than in
the multi objective extensions to those
problems it's also not restrictive to
look at deterministic policies if we
have a linear scale ization function
okay so what that means is when we have
stochastic policies yeah and the linear
scale ization function we can still have
as a solution concept of convex coverage
set of deterministic policies because we
know we can just ignore the stochastic
policies and there's no loss there okay
what about if we have stochastic
policies and a monotonically increasing
secularization function so because we
have a nonlinear scalar ization function
stochastic policies may actually be
preferable so what can we do about that
we can reason about what we call mixture
policies and what is a mixture policy it
just assigns it takes a set of n
deterministic policies and it assigns a
probability to each one and then when
the policy is executed we just sample
from the resulting probability
distribution and execute the
corresponding deterministic policy so
this mixture policy has it has value
that's a convex combination of the value
of the Constituent policies so right so
if we look at our figure again this red
line that connects the dots on the on
the convex hull is no longer just for
illustrative purposes this line now
corresponds to the value of all the
different mixture policies that I can
construct from the deterministic
policies in my convex coverage set okay
so what are we going to do now that we
have a nonlinear scale ization function
and we're allowed to consider stochastic
policies it turns out that a Pareto
coverage set that's what we want for our
non linear scale is a
function can be constructed by mixing
policies on the convex coverage set of
deterministic policies and you can see
that by saying okay if we just construct
this convex coverage set what are we
missing this convex coverage set of
deterministic policies what are we
missing we're missing the stuff that's
in the Pareto coverage set but not in
the convex covered set like this black
guy this black dot right this was this
guy was on the Pareto front but not on
the convex hull and if you look you can
see this point is clearly dominated by
points on this red line that are in this
this this rectangle so we know we can
find mixture policies here that dominate
this thing that we left out so we can
get away with leaving it out so even
though we have a nonlinear scale ization
function all we need to compute is this
convex covered set of deterministic
policies so if we look at our taxonomy
we see something a little bit surprising
three of the four boxes here have the
same solution concept the convex covered
set of deterministic policies even if we
aren't willing to assume that our scalar
is a ssin function is linear we can
still get away with this convex covered
set of deterministic policies unless we
have this one combination if we insist
upon deterministic policies and we have
a scalar zation function that we don't
know to be linear then we need to
actually compute this Pareto covered set
in all other cases the comics coverage
set is enough so it's kind of a
take-home message of the of the solution
concepts unless the scalar is a ssin
function is both nonlinear and we
require deterministic policies the
convex coverage set of deterministic
policies so you need to find something
to find in a mixture policy that yeah
yeah exactly so that's okay yes the
villages are going to dig into mine so I
once you send them out they've gone all
right so if you you're doing that every
day for a year then you could flip
points but if you just think it for one
day then you have to choose what make
one choice
so detail that I didn't really go into
is that when we move from deterministic
to stochastic policies we have to think
a little bit more about how we define
optimality and so here we're defining
optimality as maximizing expected value
and in some cases so the another
objection and could give a whole lot of
talk about this another objection to the
reward hypothesis is sometimes we might
not care about expected value we might
want to be risk sensitive or we might
say we have something processed which
isn't repeated many times so the
expectation is not of interest all of
those are valid objections but here
we're assuming we care about the
expected value of the stochastic policy
okay so how am i doing okay okay so that
was some sort of a whirlwind tour of
multi objective decision making now what
I'm going to do is tell you a little bit
about a specific algorithm that we've
developed and it probably won't be a
surprise to you at this point that this
algorithm computes the convex covered
set of deterministic policies because
the the point of the taxonomy was to
motivate why this this is really the
thing that we should be computing in
most situations okay so let me try to
before I tell you the details of the
algorithm let me try to put it in
context a little bit this is kind of two
families of multi objective methods what
we call inner loop and outer loop
methods so this this box here is a sort
of simple illustration of how a single
objective method works so the single
objective decision-making method it's
trying to maximize value so somewhere in
this method there's going to be some
kind of maximization performed to define
the best policy okay so what an inner
loop method does is it takes that single
objective method and replaces this max
operator with some other operator that's
like a multi objective analog to
maximization and basically that multi
objective analog is pruning so you can
think of maximization is an extreme form
of pruning you prune away everything
except the one best one and here in the
multi objective setting we prune away
only the things that we know to be
suboptimal before we know the weights so
pruning is the multi objective analog to
maximization so we take some single
objective method we'll get rid of that
max so we do some
instead we have a multi objective method
we call this an inner loop method
because the multi objective NISS of the
method is inside deep inside some inner
loop of the method okay well we can also
do is take an outer loop approach where
we say okay we just take our original
single objective method and we don't
alter it at all but we just wrap put a
wrapper around it we just call it as a
subroutine inside some larger method
which figures out in a clever way a
series of single objective methods to
solve such that the that the solutions
to those single objective problems
constitute the solution to the multi
objective problem so we sort of
incrementally construct the convex
coverage set by solving a series of
single objective problems so optimistic
linear support is a method that we've
developed which is of this latter form
so it's an outer loop method and this is
based on it's an adaptation and
improvement to a method called linear
support that was developed by Chang in
the 80s for solving partial observable
Markov decision processes and you might
remember I mentioned before that this
undaunted upper surface this is a
concept which comes up in in pommes DPS
all the time and so there's a close
relationship between pommes DP methods
and multi objective methods so we've
sort of leveraging ideas from that
literature but linear support was never
a practical method for pommes DPS but it
turns out to be a very good starting
point for multi objective methods okay
so what does optimistic linear support
do it solves scale Erised instances for
specific weights so we're going to
select we don't know what what the true
W is but we're going to select a bunch
of different W's and for each one we're
going to solve the corresponding scalar
ice problem and we're going to use those
solutions to construct the convex
covered set and we're gonna this covered
set is going to cover this whole
continuous weight space but it's going
to terminate after checking only a
finite number of weights I'm just gonna
it's gonna if we wanted to compute the
exact convex covered set okay so this
the the next series of slides
illustrates how how optimistic linear
support works so we start at the extrema
of the weight space we say okay suppose
I put all the weight on one objective
that yields a scalar i's single
objective problem where i only care
about that objective let me feed that
to my single objective solver it will
give me a policy which is optimal for
that wait okay so this is the best
possible value can get for this wait but
that policy that it returned has a value
for all weights and that's what's
indicated by this vector so this vector
is the all the different possible scale
Erised values of the policy we received
which is optimal for the weight at which
we perform the scalar is Asian so what I
need is a single objective method which
returns not just that the policy that's
optimal for that for the for that weight
but returns the multi objective value of
that policy so it tells me what is the
value of this optimal policy with
respect to each objective if I know that
then computing this vector is trivial
because it's just computing that inner
it's just computing that inner product
for whatever weight vector you want but
I do need to know the multi objective
value okay so now I'm going to do the
same thing for the other extremum that
yields another vector and then where
these two vectors meet this is what's
called a corner point and Chang had a
result which said that the the place at
which the current solution can be
maximally improved will always be at a
corner point so we're gonna select this
corner point and we're gonna scale ur
eyes according to that weight we're
gonna feed that to our sing objective
solver we get yet another vector and
adding that vector creates two more
corner points I'm going to test those
corner points and when we when we test
those corner points we won't get any new
vectors when we test though those corner
points we'll find out that oh the
vectors we already had were already
optimal for those corner voids so now
we're done no no new corner weights were
created because no new vectors were
added so we're done we've tested only
five points and we have now this
undaunted upper surface corresponds to
the convex coverage set give us a policy
that's optimal for any point in this
continuous weight space in this example
it was five but it will always terminate
after a finite number
the comics coverage set is a finite size
okay yes yes because yes that's correct
okay so the reason we call this
optimistic linear support is because we
take all the corner weights that we've
generated but not yet tested and we put
them in a priority queue and we the next
corner wait we we process is always at
the head of this priority queue and the
priority is based on what we call
maximum possible improvement so let's
take a look at this point here let's
take a look at this corner point so we
haven't tested this corner point yet so
we don't know what the vector
corresponding to the optimal policy for
this wait we don't know what it's going
to look like but we can put an upper
bound on it we can say okay whatever
this vector is it had better obey the
constraints imposed by the points we've
already tested so that's what's shown by
this dotted line this dotted line is the
most optimistic scenario for what this
unknown vector could look like this
vector has to pass under the points that
we've already tested at the weights at
which we tested them right because when
we tested this weight we found the
optimal policy for that weight so it
cannot be possible that then when we're
testing another weight we get a policy
which is better according to that
previous weight so each of the weights
we've already tested generates a
constraint and we can compute this this
dotted line this this optimistic vector
by solving a linear program and the
difference between the value of this
vector and the corner point at the given
weight that's this Delta this is the
maximum possible improvement this is the
most that our solution could improve at
the current weight by processing that
weight by solving that weight so that's
our priority is this this Delta and the
nice thing about using this maximum
possible improvement as the priority is
it naturally gives us a way to generate
approximate solutions and turn our
algorithm into an anytime algorithm so I
can for example say I can for example
say I'll settle not for this for the
convex coverage set but for an epsilon
convex coverage set and then how do I
know when I have an epsilon convex
covered set I just
stop when the thing at the head of the
priority queue has a delta less than
Epsilon because I know processing that
corner wait cannot yield an improvement
greater than epsilon so I'm already
within epsilon of optimal and everything
else when the queue is gonna be even
smaller or I can just run until time
runs out and then I can say okay how
good is the solution that I had I know
that it's an epsilon CCS where epsilon
is the delta corresponding to whatever's
at the head of the priority queue yeah
question party other than the vertical
distance she would annasmith the
Superdome on surface release time this
volume corresponds to the total amount
of sub optimality of the current
solution the maximum possible
improvement corresponds to the sub
optimality at this point they're not the
same they're definitely not the same I
think the the priority queue would order
them differently if you use that
criterion would that be better or worse
I don't know if the top my head and how
we need to think about it let's say
that's an interesting idea how maybe we
can talk about that offline okay all
right so what we can also do is say
maybe we have a problem which is too
difficult for which it is too difficult
to solve even a single scalar eyes
instance so I'm solving multi objective
Markov decision processes the Markov
decision process has a million states I
can't afford to come up with an optimal
solution to even one scalar eyes
instance so what I want instead is to
use in an approximate solver instead
suppose that approximate solver has
bounded error so I I feed my scalar eyes
instance to that single objective solver
it gives me an epsilon optimal solution
I can just fold that epsilon into my
decision-making on them at the multi
objective level so when I compute this
maximum possible improvement I consider
not just the distance between the vector
between the current point and this
optimistic vector but I raised the roof
by an amount epsilon corresponding to
the fact that the values I've computed
may actually underestimate the true
optimal value by an amount Epsilon and I
know it won't be more than epsilon
because I'm using an epsilon optimal
single objective solver okay so that's
optimistic linear support the the main
advantages of it are that I can apply
this to any multi objective problem this
is it's not really an algorithm it's a
generic framework that can be applied to
any multi objective problem because it's
really just a wrapper around the single
objective method so I can apply it to
any multi objective problem for which I
have a suitable single objective solver
what is this thing a suitable single
objective solver well this was already
discussed in in answer to a question so
basically it's a solver which gives me
not just the optimal policy but which
tells me the multi objective value of
that policy so that I can reason about
those vectors this is not a huge
constraint in practice another nice
property is that it inherits the quality
guarantees of the subroutine so as I
mentioned before if I have this this
epsilon optimal single objective method
I can take that bound on the single
objective method and push it through the
multi objective method to get a multi
objective method with bounded error for
small and medium numbers of objectives
this is way faster than an outer loop
approach for large numbers of objectives
the number of corner points become that
a generator becomes prohibitive and
inner loop methods are faster now there
are certainly real-world problems where
you might want to model them as having
thousands of objectives like that
traffic problem you might want to say
every every driver is an objective in
which case you would definitely not want
to use OLS or any outer loop approach
but most of the problems you come across
in the literature and I think many a
problems in practice actually have only
a few two three four objectives this is
sort of natural resolution at which
people think about multi objective
problems and for that whole family I
think an outer loop approach like OLS is
the way to go okay so the last part of
my talk is an application of this
about an application of OLS to a
particular problem I think I'm gonna
skip some details here because I'm is
that including questions sorry okay I'll
try to wrap up in five minutes and then
we can you can ask them any questions
you like so just applying OLS to this
particular problem of multi objective
coordination okay so what is multi
objective coordination we have a bunch
of agents they're on the same team but
they need to figure out how to
coordinate the behavior so as to
maximize their shared payoff so we have
a but we have n agents and we have a
joint action space so each agent can can
select some action and we assume that
this utility this that they're trying to
maximize is the sum of some local
payoffs so the total payoff of the team
is the sum of some local payoffs where
each local payoff depends on the action
of only a subset of the agents so in
other words is a graphical structure
here that can be exploited so this can
be this can be captured in for example a
factor graph here we have three agents
and we have two payoff functions say
this this payoff function depends only
on agents 1 &amp;amp; 2 this one depends on
agents 2 &amp;amp; 3 ok so as you may or may not
know there there are many algorithms
that have been developed to solve this
kind of problem they're there they these
methods were originally developed in a
probabilistic setting but they can be
applied to this optimization setting
very easily one of them is called
variable elimination the main idea is
that for each agent we're going to
compute the value of its best response
to all of the local joint actions of its
neighbors and in doing so we can
construct a new local payoff function
that eliminates the agent from the graph
this gives us computational complexity
only in what's called the induced width
of the graph rather than an N so rather
than having to iterate over this
exponentially large action space we can
much faster find the maximizing joint
action ok if we take this problem to a
multi objective setting we have exactly
the same thing as before but now these
payoff vectors are vector valued so you
have a total team payoff that's a vector
multi objective payoff and it's the sum
of the multi objective local payoff
functions so here we have these local
payoff functions and if you know
anything about game theory this might
look to you like a normal
like a normal form game but it's
actually a little bit different so it's
not the case that four is the payoff for
one agent and one is the payoff for the
other agent because they're on the same
team this is the shared but multi
objective payoff of the team so they
have the same interest in mind but those
interests are multi objective and so
this is the value of one objective and
the value in the other agenda okay so
what we could do is we could take an
inner loop approach we could say if we
think this variable elimination
algorithm that I very quickly sketched
we could replace maximization with a
pruning operation so we've actually done
this with so some people did this before
us in the for it to compute Pareto
coverage sets this is the work of Rulon
and LaRosa we took that work further for
computing convex coverage sets this is
an inner loop approach what we can also
do is take an outer loop approach so we
can just take this optimistic linear
support framework I described earlier
just stick variable elimination in as
the as the single objective solver and
we're done this is an algorithm we call
variable elimination linear support so
this graph shows some results that we
obtained using these methods so on the
left hand side we have a some randomly
generated problems so we have an
algorithm for generating random graphs
and then we solve those graphs and then
this is on the right hand side we have
the commodities mining problem that I
showed you at the beginning this is a
two objective problem okay so on the
x-axis we have number of agents and on
the y-axis we have the runtime of the
algorithm and notice the log scale here
on the y-axis so the first thing that
these results show is that this red line
see move although it starts out slower
then then this brown line which is P
move so that's computing the Pareto
coverage set even though it starts out
slower it it's growth is a lot more
gradual so as we get to larger numbers
of agents computing the convex covered
set is key to keeping the problem
scalable to keeping it tractable and
what we can also see is that if we take
an inner loop approach using veils
that's this black line we we get an
algorithm that runs much faster this is
like an order of magnitude or almost an
order of magnitude due to the log scale
and what we also see is that if we
tolerate even a very small epsilon that
also
yields a big speed-up so by taking in by
computing the convex covered set taking
in outer loop approach and tolerating a
small epsilon we can greatly improve the
runtime of this algorithm solving these
multi-objective coordination problems we
see a similar result here in the mining
commodities problem here we have we
don't have a log scale on the y axis so
we weren't even able to run P move on
this problem and see move scales very
poorly this is our outer loop approach
computing the the true convex covered
set and this is it computing epsilon
coverage sets with different values of
epsilon so there are huge speed ups here
due to taking an outer loop approach and
and tolerating some some small error so
this is my last slide and in this slide
I just want to mention a few of the
other directions so what I've told you
so far is actually just the tip of the
iceberg
we've done quite a bit of other things
related to this in our own research so
variable elimination is a
computationally efficient algorithm but
it's not memory efficient but we can
take tree search algorithms that are
memory efficient and use them instead of
variable imitation in this linear
support framework and because linear
support just solves a sequence of
problems it's memory footprint is
actually only as big as the memory
footprint of the single objective solver
so we can directly obtain a memory
efficient algorithm using this framework
also I think I'll skip this so we've
also successfully applied this framework
to sequential decision problems multiple
Markov decision processes and partially
observable Markov decision processes and
in doing so the sort of key the the key
to a scalability the key to be able to
being able to solve these problems in a
tractable way something that we call hot
starting so everything that I showed you
here when we solve a particular when we
solve this the scalar ice problem for a
particular weight we solve it from
scratch but that's actually not very
smart because in this linear support
framework chances are we've already
solved a single objective problem for a
similar weight we have no guarantee that
that the solution will be the same but
it but if we hot start the solution for
the new weight with the solution to the
old way we can solve the new
for the new weight much more quickly and
this yields even bigger speed ups which
is the key to success in sequential
decision-making setting so I think I've
talked long enough thank you very much
for listening I'd be happy to answer any
more questions you have production style
in Earth's rather than a maximization
thing I mean it's no problem you can
take your cost and multiply them by
negative one and and then maximize its
it's it's no problem you know search
engines work fast because they do some
pre computation I'm sure I shouldn't do
this but the message I'm getting is is a
type of pre-computation I can do to
speed up some optimization problems
yeah yeah many things that is very more
than that I mean I think I think there's
a totally fair characterization I think
of it as I think of it it as partial
planning you you want to do some
planning but you don't have you don't
really know the whole planning problem
some key information about the planning
problem is not available to you so
you're just gonna do as much as you can
in advance so that later when that
information becomes available implicitly
or explicitly then you're you're better
off that second stage is sped up by the
by the first stage just carrying
the a single lung of a single objective
in this outer loop thing you described
like you make multiple runs of the sort
of standard output
there's each one take a week or you know
they're very quick then you could just
run it when you know the weights back to
the question about whether you really
need to state this so slow then really
overworked so I find it difficult to
give a general answer there because we
have problems of all sizes so in our
experiments we of course restrict our
attention to two problems where the
single objective solution is not
extremely expensive because we want the
whole experiment to run in a week and
not a year but that's that's sort of
from the perspective of a scientist we
want that to be small so we can do our
analysis in the real world of course you
have problems of all sizes I think I
think actually you answered your own
question because you know if it's very
expensive then this is worth doing if
it's not expensive then it's not worth
doing if you have a single objective
planner that runs in real time sorry
well so there are it's easy to come up
with applications for a decision support
because decision support simply requires
you to have a human in the loop these
these cost issues these relative cost
issues don't come up as directly so all
I need is some human who for whom it is
difficult to quantify the trade-offs
between the preferences and then I want
to take this multi objective approach
and show them the the convex covered set
yeah absolutely so I mean we have a
robotics project where we're we're doing
this now where we're trying to get the
robot to behave in a socially
intelligent way we have different ways
of learning cost functions from data we
need to integrate these cost functions
so that at the end of the day the robot
has one policy that to which according
to which it behaves how are we going to
integrate these cost functions there are
a lot of idiosyncratic issues that we
can't all capture in the planning
problem and solve algorithmically so
what we want to do is say okay let's
compute the coverage set take each of
these cost functions separately compute
the coverage set and let's show it to
the user and in this case the user is
not you know the person interact
with a robot the user is an engineer on
our team who's going to look at the
resulting behavior and say yeah this is
the one that's about right this is the
policy that that that gives the right
balance between you know the proxemic
distance you stand from someone and you
know whatever the other the other
objectives are so yeah I think that
definitely comes up in practice okay
super well we have drinks and time for
further questions officer let's wrap it
up in here and move outside thanks again
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>