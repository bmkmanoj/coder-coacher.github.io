<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Saturday Morning Session 1 | Coder Coacher - Coaching Coders</title><meta content="Saturday Morning Session 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Saturday Morning Session 1</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EjRjYldEi1U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so the the next and last talk in
this session music paper presentation on
fourier sliced super resolution in one
op de camaron by fernando pérez
alejandro perez manuel rodriguez eduardo
magdaleno and you know Fernando is is
presented first of all thank you to the
organizers and all of you for being here
it will be a difficult task to give this
presentation after Kurt presentation
being after the leading company in the
penalty field but i'll of course try to
do my best ok so my presentation is four
years a live super resolution in turn
optic cameras I am Fernando Paris from
the University of La Laguna in Spain and
I could have said penalty camera records
the four-dimensional light field of a
scene it uses a microlens array between
the lens of the camera and the image
sensor it has a long history starting
from the early works of fives add until
some new advances by running G or taller
George F so what's the interest
implements the cameras they offer new
capabilities of a regular cameras they
offer refocusing 3d estimation only
focus images and former properties so if
you need a a penalty camera you can buy
a light row camera from the laser
company but there are more we have a
german company rate tricks that also
bills not the camera son of course open
optic camera in university in our
university the DECA farias camera so
this paper we will deal with
who seen plenty cameras allow refocusing
images after the shot us as you may know
we have two examples there in the on the
Left we have imaging focus on the front
on the right we have the same image
focused on the back so what's the
problem we are addressing to herein in
this paper refocus images can be
computed with several algorithms they
can be focused at any depth but the
refocus images have a small resolution
that can be less than 1% of the camera
sensor so a solution to this problem
would be to increase the solution using
super resolution techniques there are
several fast algorithms available but
those images in those algorithms can
only be generated for a small set of
thefts so our contribution is a unified
framework for several standard and super
resolution algorithms that include some
well-known refocusing algorithms like
fully realized photography or focus
penalty rendering and the Dallas to
obtain super-resolution using standard
resolution algorithms and remove some
depth constraints from several
superresolution algorithms so how can we
compute a refocus photograph planetic
camera captures the radians traveling
from a position on the lens plane to the
oxygen you on the lens plane to a
position X on the sensor plane we may
see this in the diagram we see on the
slide on the on the left where we have
the x-axis that corresponds to the
sensor and the you axis that corresponds
to the lens as we see in the excess in
the x-axis the recent the resolution is
equal to the number of microlenses and
this is the useful resolution that we
obtain when we focusing
now how can we compute a refund refocus
photograph well all that we have to do
is to use the photography operator that
tell us that a photograph is just a line
integral in the delightful just aligning
to relief you were using bidimensional
light-filled we are using here we
dimensional light field because it's
easier to see all the results but in
fact we are working with a four
dimensional light field and so we have a
plane internet so all that we have to do
is to perform this line integral over
the light field so how can we do that we
have celery solution methods and the
easiest way to do is just okay if we had
to compute a photo feel better just
interpolate and soon and if we do that
we interpolate missing points and soon
well we obtain is a photograph whose
resolution is equal to the number of
microlenses a much better approach is to
do it in the frequency domain that is a
very important result from Brennan G
that says that if you want to compute a
photograph it's easier to do it in the
frequency domain so all that we have to
do is take the light-filled go to the
frequency domain distract a slice from
the light field and just before the
inverse Fourier transform ya as we see
here we have in the x-axis the spatial
resolution goes goes from n x 2 minus NX
and in the formula there we have
frequencies going also from minus NX to
annex studies we have the same number of
points as the same number of frequencies
that it should be ok we have several
methods here for yourself photography
from Renan G or the discrete focal
struck transform that has the advantage
that is an analytical transform
well what are the properties of these
methods the main difference between
spatial and frequency methods are used
their computational complexity as you
can see on the slide the computational
complexity of frequency metal methods is
much lower than the computational
complexity of questionable methods he
it's lower by nearly two orders of
magnitude ok let's see now some super
resolution methods the simplest way to
perform super solution is again just
interpolate and soon and if we do that
we can see well to what can we obtain we
have in diameter and I gram of the
slider three-time Super solution and
just what we have to do is to know what
to submit the missing points and just to
perform the zoom over the over the lines
another approach to do this is the focus
panopticon during by total George F and
what he does in this method is to tile
the central part of each micro lens in
the spatial domain so we have our
original light field we perform tiling
the central part of which microns and as
we see we obtain a three-time in this
case super rizal image much better
approaches to not only child but to
average all the values and we obtain
also in this case a three times of rizal
photogra and the idea here for us is
that this kind of method what is trying
to do is to solve the resolution problem
by treating it as missing data problem
and we can interpret what this method is
doing by saying that it obtains
resolution by solving a missing that
problem with constant depth lambertian
prior the theorem we saw in the previous
ladies is exactly equivalent to the
diagram you see on the right what we are
doing is just to perform the average
over the red lines we see there if we
have a lambertian an emerging prior alum
better prices that those lines are
constant so what we're doing is to
estimate the call the common value over
this line and we perform adjust a
maximum likelihood estimation of this
common valley with this interpretation
we have two methods methods the
plaintiff focused relative rendering we
saw before and we have another one the
super resolve the script focused are
transformed now what are the properties
of these super solution methods the main
difference is the computational
complexity the space the simple special
method is the slow and the average
methods are very fast they are well it
seems that they are not too fast because
this their computational complexities
and to the fourth but the size of the
sensor is also and to the form so
average methods are very fast but they
are constrained their resolution
increment is constrained by the tile
size or the number of microlenses and
also the number of different depth is
also constrained by the number of
microlenses so in order to see that
they're indeed very fast we can see here
an example with our camera of computing
at 24 frames per second both deaths
estimation and an all in photos image
with this kind of algorithm okay so
what is our contribution as we said as I
said before the problem or problem is
that the number of super resolve fuck is
photographed since below and the basic
idea is that we want to chain the best
of both worlds super resolution with
special methods but we also want to
obtain a high number of photographs with
Senate resolution methods another
intuition is to mix both worlds to mix
the spatial on frequency methods how can
we do that well in fact it's very easy
we just get our original light field we
extend the light filled with zero
microlenses implicitly so that the sun's
over the the lines is not changed and we
just perform a furious life photography
technique and if we do that formerly
what we are doing is mixing a line
integration of spatial domain with a
frequency representation of the Delta
function and what we obtain is what we
call the super result warriors life
photography that is similar to the
standard fully dressed life photography
Byron Angie but with difference it says
that to obtain an MX by ma image what
you have to do is ok go to the forest
life photography technique by Aaron
Angie and just increase the number of
special and frequency values from NX the
number of microlenses to MX the desire
resolution and just increase the number
of frequency turns from an ex to MX so
that the number of special values and
frequency one frequency terms are the
same just as before but now it's not
they are not equal to the number of
microlenses they are equal to MX studies
selectable parameter this is some kind
of surprising fact because it is a very
easy extension of a very well known
algorithm that is the four years
lithography by renergie and with this
simple change it gets super resolution
well we obtained is a selectable image
size and refocusing plain MX is now the
image size and their focus on plane is a
function of alpha and while we obtained
here is that it formulates a
generalization of all the particular
cases I've talked before blend the focus
of linearity rendering super resolve
discreet focal struck transform for
yourself photography and the discrete
vocalist actress also in this formula we
have a unification of several methods
both standard methods and super
resolution method now what is the
computational complexity of this method
we may compare it with a simple special
super solution that we saw in the first
play that is unconstrained and what we
see is that the computational complexity
of this super result for a slice
technique is lower that the simple
special method just as it happens in the
standard case okay so we have here some
results and the results and what we show
in this result is the on the right on
the left sorry a standard resolution
image and in the center we have a 3 x of
resolution with the Fourier a slight
photography technique by r NM g that we
have extended and on the right we have
the same with the other frequency method
that is too
discrete focal stack transform this is a
synthetic light field we have here a
plane optic camera well when I was this
is the example that is in the in the
paper I was afraid that it it was
difficult to see the difference so i
made a soon for the conference and you
can see a detail and on the left we we
see the standard resolution image and
what we obtain with this frequency
extension of the four years life
photography technique oh that was a
productive 1.0 camera the technique
works also with have been up to 2.0
camera this is an image we saw from
toddler yesterday that is in his light
field repository in the internet and we
have again an ascender resolution image
and our super resolution extension with
frequency methods so we said that the
super-resolution methods were very fast
but had some constraints for example the
super result discrete Fourier transform
generates super result images of nearly
and to the fourth pixels there's a
machine for there no end to the four
pixels so if we need less resolution
that's about the size of the sensor if
we need the less resolution the cost of
decimating this image if we do it in the
frequency domain is about n to the
fourth log N and with this computational
cost we can obtain more photographs with
the same computational cost using this
fully realized super-resolution
extension that will be some here
well we also remove the depth
constraints of this these methods we see
here a detail of the Civil image we saw
before we now obtain a lot of refocus
images on several planes okay maybe we
can see better here now that refocusing
proceeds from the front to the seagull
now and now it will go back to the back
of the image and see that it proves it
proceeds very smoothly because we obtain
a lot of planes okay this kind of super
solution methods have a problem they
generate our call plane optic artifacts
in out-of-focus areas of the of the
image we can interpret this as something
that happens because the prior that we
said before we we're applying fails in
that in that case so I como solution to
this problem is to generate a Nolan
focus image using an estimation of that
time we can see this kind of solution in
in the slide note that to generate a
Nolan focus image is much easier well
not much easier but it's an easier
problem than to generate to estimate
depth there are some errors on the
somatic death see the sky for example
exact the depth of the sky is wrong it
should be white it's wrong because there
is no texture on the sky so the
algorithm is an algorithm gets doesn't
get good good results but anyway the sky
is correcting the autofocus image and
note that in the early for goose image
we have focus both the front of the
image the seagull
and back so as conclusions we have
presented a super career slide
super-resolution algorithm that unifies
several algorithms that are that have
been developed in the last year's and
also allows to obtain super solution
using a standard solution techniques
specifically we can obtain
super-resolution using renergie fully
realized technique just by easily
extended it and that removes some depth
and resolution constraints from super
resolve algorithms so what will be our
future extensions we are planning to
pour these algorithms to GPU and fpga
for real-time processing as we did with
we saw in the demo before and we are
planning also to remove these plan optic
artifacts that are common tools
resolution and methods by using some
more complex priors more complex are
like peace piecewise lambertian priors
then we have used here in this line so
this finish means my presentation and I
will be happy to answer any questions
so we have plenty of time for questions
so sorry three yeah yeah yeah
theoretically we can go as far as the
size of the sensor you see celery
solution methods the resolution is and
square and the sensor the size of the
sensor is n to the fourth so we can go
theoretically from and we can increase
this by two orders of magnitude but this
is only theoretically because easily we
get a lot of cleanup plenoptic artifacts
if we go below far far away from 5 times
7 times 9 times we we get a lot of
lunatic artifacts and we have to use
these deaths estimation technique the
problem is that deaths estimation
techniques when we have a big image they
have a lot of computational complexity
so theoretically we can go very far yes
order there is a there is a noise limit
but the main problem is is not noise the
main problem is that this that appear
the kind of penalty artifacts because
our prior assumes that all the elements
are at the same depth so if we go very
far from this it began to appear some
some artifacts this is not a problem of
all this is not only a problem of our
technique all super resolution
algorithms get this problem so in order
to solve it we have to estimate depth
and this is a very costly thing and it's
also an under constraint problem not a
difficult problem
well I could blur property if I wanted
to generate the fuck used parts and
unfocused part I could do that in our
example here everything is in focus but
as you say I could as I have that i
could try to defocus part of the images
depending on depth in order to simulate
focused part of the image now focuses
and out-of-focus parts of the images but
we we don't do that in our now we got we
just get this kind of all in focus image
know that we when we get the only focus
image and estimated that what we are
really doing more or less is to
reconstruct all the light thing it is
more or less in the lambertian case it
is more or less equivalent to
reconstructing all the light field so if
you have all the light field you can do
okay you can focus some part d focus all
our parts and so you get the whole life
field this is also interesting because
this is the thing we are working in the
question that you get the light field
again it's important because there is no
at least I don't know no results about
it if light Phillies invertible from
this kind of images that is if you have
a focal stack or some imita focus had
several parts can you recover the light
view and this is something we're working
on
whenever those artifacts will mention
there are those the optic artifacts
we've got a nice interesting a good
result there is a live aquarium by this
which we were presenting of the posters
fixing is like this type of party party
without
using Korea methods the other point was
super-resolution there are different
terms of super-resolution there's
misunderstanding some things we do not
go suppose you call superocean others
so terminologies mixed a little bit but
we made it similar to this in the
spatial domain we receive
we get similar results only the reason
we are not doing career wise approach is
that it doesn't burn to the gpo easily
it's difficult and by using GPU we are
getting 33 times faster that's why space
spatial methods are better faster than
Perez Weiss medic in our experience you
may want to comment on this and my last
point is supervising gives you four
times better resolution than this i hope
to be
the dieting after four times okay of
course I will go to the poster session
because taller is one of the leading
experts in in the field yes okay special
methods have the advantage of being nice
report to the GPU in fact the demo I you
so there was with working with a light
filled with four GPUs are working in
parallel and yes they it also depends on
their resolution because at least in the
formal way well it depends on which
which kind of metal you do you use
because let me
okay if you use the simple special
approach if for if you get a if you want
a very big image at least in the
theoretical in a theoretical way this
kind of method is better it generates
more with more focused photograph is you
want a lot of refocus photographs at
least formally it it should be better
okay other if you only want one specific
photograph or maybe two three four I
agree that probably is it is likely that
spatial methods are available if when a
lot of photographs pizza and photos of
her big size at least theoretically this
kind of method is is better this with
respect to the simple special approach
the the focus technique of
let me see
this kind of metal that is that was
available developed by solar this is yes
very very fast and we have used a
similar method that doesn't perform
tightly because in tiling we we get a
problem that the size of the image is
dependent only on depth we have
developed a similar method that is here
and these are very very fast methods but
the problem is that number that the
number of planes is also is limited
because as you are doing tiles you can
get no more than the size of microwaves
in flames so if you want more planes you
have to interpolate and you interpolate
and you want a lot of planes in terms of
computation theoretical traditional
complexities this for your kind of
processing should be better okay I think
we that's all the time we have are you
under 7 13 ok let's thank for under one
more time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>