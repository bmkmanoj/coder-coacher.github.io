<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Verifying Data Parallel Programs Using Sequential Program Verification Technology | Coder Coacher - Coaching Coders</title><meta content="Verifying Data Parallel Programs Using Sequential Program Verification Technology - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Verifying Data Parallel Programs Using Sequential Program Verification Technology</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fDeYP0syWFs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's my great pleasure to welcome
Alistair Donaldson again to MSR Alistair
is a professor I don't know if that
that's not the title actually like the
lecturer lecturer in the Department of
computing at Imperial College London he
after after after after doing his
postdoc at Oxford University he spent a
few months as a visiting researcher at
Microsoft and he started the project
called GPU verify for verification of
GPU kernels back at that time in
collaboration with people here and since
then he has made a lot of progress on
that project along with colleagues at
Imperial and he's going to tell us about
some of that work today yeah thanks chas
so given it it's such a small audience
let's make it really in formalin I just
tell you about what we've been doing
please ask as many questions and yeah we
can we don't have to get through the
whole deck of slides if you have more
questions again so this is joint work
with shaz initially just shares when we
were collaborators here at microsoft and
then since then I recruited various PhD
students and postdocs none of whom have
GPU verify as their principal thing but
all of whom have done something some
hacking on it there's this long list of
contributors and the project is
supported by the the car project which
is it which is an eu-funded project on
correct and efficient accelerator
program in which I'm coordinating at
Imperial and the project's kind of split
between optimizations performance
optimizations and correctness checking
and we're focusing on the correct is
checking side of things so these are the
guys who contributed to the project I
have 33 post logs your rune Adam who's a
bit shy as you can see and John and then
some peace these students Paul and
Nathan who've worked a lot on the
project and Dan who just started
recently and a new student pantazis
who's starting in July and I'm actually
looking for one more PhD student so if
you know any bright young candidates who
would like to live in London please send
them my way and generally the aim of the
research in our group the multi core
programming group is to design automated
techniques to help people write correct
and efficient parallel software so I'm
particularly interested in concurrent
programming partly because i think is
cool to try to get things to go faster
but partly because
the correctness challenges it raises and
actually I think they're from a
correctness and verification point of
view I think it is very hard to verify
general properties of arbitary
sequential programs I think if you take
a parallel program and you look for
things like data races or deadlock
freedom they can be easier perhaps to
give people useful tools to help them
with rather than trying to try to look
look at the verification of more general
properties okay so I'm going to tell you
about our work on verification of GPU
kernels which i think is an application
of the idea that's becoming quite
popular these days of trying to analyze
concurrent programs by somehow
converting the problem to a sequential
program analysis task so first of all
let me tell you a bit if I were a GPU is
GPU is a graphics processing unit and
I'm going to give a schematic overview
of what a typical GPU looks like nothing
that I say here is going to be
completely true of all GPUs so if you
know about GPUs then you don't you know
that it's not strictly true but a GPU
generally consists of a number of
processing elements which you might like
to think of as cause other than
typically a bit simpler than CPU cores
and every processing element has a small
amount of memory that it has exclusive
access to then there are a number of
these processing elements on the GPU and
the processing elements are arranged
into groups such that every group has a
portion of memory that share them among
all the processing elements in the group
so these guys can communicate with each
other through this group shared memory
but they can't communicate with those
guys through each other's group shared
memory and then in addition there's a
pool of global memory which all the
processing elements can share and to
some extent processing elements in
different groups can communicate through
this global memory however in typical
GPU designs there is a mechanism for
processing elements in the same group to
synchronize with one another but no
mechanism for processing I'm is
indifferent rupes to synchronize with
one another so this global memory is not
really used for inter processing element
communication is more used to actually
get data from the host device and give
data back to the host device make sense
yeah you GPUs typically provide until
now interlocked operations like such as
like compare and swap those things well
yeah they do yeah and that's actually
something which we want to look at neck
that's quite challenging from a
verification perspective but the one
problem is that there's not really a
consensus among which operations the
various families of GPUs provide for the
opencl spec for instance has a bunch of
atomic operations that it specifies okay
so GPU accelerated system would
typically consist of a host computer
like a multi-core pc here i'm showing
you may be an eight core pc and a
plug-in card and what happens is the
host is responsible for copying both
data and code into the global memory of
the GPU and the code is a function
called a kernel function and this has
nothing to do with OS kernel so
sometimes get people asking me questions
about so it's a cui different meaning of
the word colonel or the same ultimate
meaning but a different specific meaning
and the host then says to the GPU go
invoke the kernel and what this does is
it lights up all of these processing
elements and what they do is they copy
data from global memory into group
shared memory from group shared memory
into private memory they crunch through
it eventually copy it back to global
memory when they're done the the host is
interrupted and it can copy back the
results for further processing so in a
typical GPU accelerated application you
might just have some preparation code on
the host one kernel implication and then
some processing code or you might have a
sequence of kernels in a pipeline or you
might have something like a loop with a
kernel invocation inside it if you're
doing an iterative algorithm where you
have a number of time steps a common
thing would be to have a timing loop on
the host and then for every time step
you do some calculation and then do
again and again and again okay so a
serious problem when programming GPUs is
the problem of data races which are
well-known from regular concurrent
programming so a data race occurs when
we have got to processing elements or
threads running on to processing
elements in the same group that access a
location in group shared memory and at
least one of these accesses is a right
and this is got an intra group data race
and we can also have an intergroup data
race where we have threads running on
processing elements in different groups
and they access a global memory location
and at least one access is a right and
there's no synchronization operation
separating them this is an intergroup
data race and we
also have an intra group data race on
global memory which I'm not showing you
in this diagram so data races lead to
all kinds of problems and I think
something that's interesting in the GPU
context so I think it's very well known
the problems with the data races bring
problems of non-determinism mainly but
in GPU Chronos this I think a worse
problem which is that actually you may
have device determinism so on a
particular GPU architecture GPUs if you
know a bit about how they work you know
they're kind of deterministic so
actually threads don't get scheduled by
an OS and you don't have preemptions and
that kind of thing threads get scheduled
by a driver and they get scheduled in a
very deterministic way on a given GPU so
may well be that you've got to Colonel
that could exhibit a race but never does
on NVIDIA architecture X and then if you
report that Colonel to another
architecture you may then discover
there's a problem and it and there are
programming models such as opencl which
aim to be portable so that actually
kernels get compiled at runtime for
whatever architecture is available so in
that setting you don't necessarily know
what your customer is going to be
running your kernel on so if you've
tested on a variety of architectures and
discovered no data races it may be that
you cannot discover data races by
testing alone and yet on some
architectures there would be data races
so and another thing to point out is
that data races in GPU kernels are
almost always accidental and unwanted so
we don't have the the case in systems
code where there are deliberate benign
data races what I've seen our benign
data races where for instance many
threads right the same value to a
location that that happens you sometimes
have data races where for strange but
actually good reasons a thread is going
to write something but it's guaranteed
to write what's already there and
another friend might be reading in which
case that doesn't matter but I haven't
seen examples where we have got for
instance synchronization primitives
being implemented by busy weight loops
and GPU kernels there's not something
that would be very efficient and it's
not something we would be portable
across architectures either so what
we've been doing in our work is looking
at data race analysis for GPU kernels
but let me tell you first of all how you
would avoid data races in a GPU kernel
and also show you an example colonel so
this is a little cone or written in the
opencl language we have a regular c
function which we prefix with the
keyword colonel to say this is an entry
point to the current
this is where threads commence execution
and the Colonel's going to declare that
it takes an array of integers as an
argument and then this array the
contents are going to reside in local
memory which is in opencl what group
shared memory is called so local means
group shared and then also this kernels
going to take an int offset as another
argument and what the colonel is going
to do is it's going to add every thread
is going to write to its threadid what
it's going to write is the what what is
already at is thready plus what's at its
neighbors thread Eddie so it's going to
write a Ted plus a 2 plus offset ok so
spot the data race I'll grab my coffee
what you think yeah reading to someone
else's too exactly right so if offset
was one for instance and Ted was zero
then thread 0 will be writing to 0 and
reading from one potentially in parallel
with red one writing to 1 so this would
be a read/write database ok and we can
avoid this database by using a barrier
so we can for instance read a tip plus
offset into a temporary variable then we
can do our right using temp instead of a
2 plus offset and then we can have a
barrier synchronization statement in
between these statements and what
barrier says is that every thread
running the current almost get to the
barrier before any thread leaves the
barrier and furthermore that all loads
and stores for memory would have
completed before any thread leaves the
barrier now actually like I mentioned
briefly earlier it's only possible for
threads within the same group to
synchronize with each other so a barrier
is a synchronization operation between
threads in the same group for the rest
of the talk I'm just going to assume all
threads are in the same group for
explanation purposes although in the
tool and theory we deal with the general
case ok any questions at this point
about GPUs kernels and suchlike oh yeah
so it stops the accesses from being
concurrent ok so there's been a lot of
interest over the last few years in
verification and analysis for GPU
Chronos the leaders in this area where
the group of Ganesh Gopalakrishna at the
University of Utah who
have a talker pug for analyzing cuter
kernels which was published at FSC a few
years ago and there these days more
focused on a tool called giclee which
uses dynamic symbolic execution it's
based on the CLE execution engine
developed a Stanford an hour at Imperial
College and an interesting thing they've
done recently is extended the GP tool to
handle atomic operations which is very
nice piece of work the university of
twente her collaborators on our car
project are looking at using separation
logic with permissions to approve data
race freedom of GPU kernels the idea is
that you have a permission logic and you
prove a colonel by showing that a thread
can only write summer has write
permission so it's a nice application of
separation logic there's a nice paper
from East up a couple of years about
about the samt model that's used for
CUDA curdles and yet another partner in
our car project to looking at doing
symbolic execution of GPU kernels and
finally aside from our work there was a
paper about using test amplification a
pldi last year the idea of this work is
that you actually dynamically run a
kernel and check for one trace whether
the bday two races and then you use some
static analysis to try to discover
whether that trace was in any way
influenced by inputs to the colonel and
if it wasn't you can conclude that the
conus free from data races yet actually
run the girl or the never city later I
believe they did it with a simulator
because it's quite difficult with it
it's very difficult to do logging on a
GPU yeah okay yeah we published our work
at oops the last year that was the main
paper about GPU verify and a more
technical paper about some of the recent
developments in the toilet ESOP this
year so GPU verify is a tool for
verifying database free in which have
described you and another property
barrier divergence freedom which I'll
talk about briefly later for opencl and
cuda kernels so CUDA is a GPU
programming model from Nvidia who the
market leader in GPU devices and opencl
is a more general programming model
that's been put together by the Khronos
group a consortium consisting of a bunch
of partners including I would say pretty
much every major player apart from
Microsoft I think it's fair to say and
so Microsoft have C++ am p which is
another accelerated massive parallelism
which is different again and yeah we
decided to focus on both of
perl programming models because they're
very similar we'd rather just focus on
opencl i suppose for simplicity but CUDA
is more widely used still hopefully that
won't be the case in it in a year or so
but it is in the moment so before I go
into the details i'm going to give you a
demo of GP verify to give you a feeling
of what it does please just interrupt me
if you have any questions so i'm going
to write a little kernel to perform a
reduction operation well this is going
to do is it's going to take an array of
int in local memory and i am going to
have every thread i'm going to have the
threads some their neighbors elements
using doing a tree reduction so a thread
will some a neighbor if there are n
threads and over two places away and
then n over four places away and then
over eight places away and in over 16
places away and every iteration of the
reduction loop Hathor threads will drop
out to the computation this is a common
thing to do in a GPU connor to co2
callate results so I'm going to say 4 in
D equals n over 2 where n is the number
of threads while d is greater than 0 i'm
going to divide d by 2 by shifting it
right by 1 i'm going to say if my ID is
less than D so if I'm still active then
a at my ID is incremented by a at my ID
plus my neighbor d places aways ID and
I'm going to put in some defines here so
there's no did actually an open CL I'm
going to find it to be get local id0
this is a built-in function that gets a
threads local ID in the zeroth dimension
so these kernels could be
multi-dimensional I'm not going to go
into the details of that here let's
assume the connoisseur one-dimensional
and n is going to be the number of
threads and dimension zero all right so
assuming i have not made any syntax
errors this should the tool should do
something on this so first of all the
trouble complaining it will say that the
work group size must be specified
using local sighs so what we're not
trying to do in this work is
parametrized verification we're not
trying to prove that these kernels are
correct for any number of threads that
of course would be a nice thing to do
but kernels are not usually correct for
any number of threads they usually
correct correct for say every number of
heads as a power of 2 or every number of
threads with some property and second
the theorem provers that underlie our
work don't do very well with nonlinear
arithmetic and we very commonly do
something like multiplier varial by a
variable by the number of threads so if
the number threads is a constant that's
okay but it's not constant that'll be
very hard to reason about so GPU verify
local size so let's try it with a
thousand of 24 threads and simile you
have to say how many groups there i'm
just going to siddur consider one group
here so tool think for a minute I tend
to find that when I first run a c-sharp
application it takes a while ok so it's
reported a couple of possible data races
so it's saying that a kernel at CL there
is a possible read/write race on the
array a a byte offset for so we see a
cast to a character pointer and then by
offset for and this is 99 column 25 by
thread 0 of group 0 on line 9 column 15a
right by thread one of group 0 so if I
go back to the example then line column
25 i think is this read here and column
15 is this right here ok and you can see
that because i have not got any barrier
synchronization there's actually nothing
to stop one thread skipping ahead to a
further loop iteration and interfering
with another thread in a previous loop
iteration so to eliminate this problem I
can put a barrier synchronization in
here so bury it and then I give a flag
to say this clk local mem fence this is
a way of saying I want to do a barrier
on local memory so because this array is
a local pointer then that's the right
thing to do ok so just look good yeah
really
okay so now what the tool is going to
say is that one error so line ten column
13 barrier may be reached by non-uniform
control flow so there's barrier
statement here made a deliberate error
and why i did was i enclose the barrier
slim and inside this conditional which
means that some threads will reach the
barrier but not all threads and this is
illegal in the open sea isle programming
model if you have a barrier and
conditional code either all threads are
no threads must reach the barrier in
other words the condition guard in the
barrier must be uniform ok so the tool
is detected barrier divergence here when
I say detected and running the tool in
verify mode so it's not actually
detecting anything it's not managing to
prove the colonel so we can run the tool
in a bounded model checking mode where
it will unwind the program so if I fix
this problem then what we should see now
is some success so too will tell us that
there are no data races within work
groups no data races between work groups
no Barry divergence no assertion
failures because you can write your own
assertions in in GPU verify but of
course no warranty provided because this
is a research prototype ok I wonder if
is so that's it is intended to be send
yeah we're trying to do sign
verification but there are various ways
in which we're not signed for instance
will make the pragmatic assumption that
the pointer parameters to a kernel
pointed pointer disjoint arrays that I
actually have a feeling that that may be
required in the spec I should look that
up but even if it's not required that's
what people do I mean and if we didn't
make that assumption we would just
report it erases everywhere so that's
one thing we don't do bands checking so
as possible you could have a buffer
followed by another buffer and then we
could and then you could overflow the
bounds of that buffer and have a data
race as a result whereas we would see
rights different arrays and we would say
that they were raised free so there are
various ways in which you know where
sound modular a whole bunch of providers
okay and yeah we can write some some
assertion so I might see here assert
is power to D and I could say
X as a power of two if X bitwise handed
with X minus 1 equals 0 and X is not
equal to 0 so a power of 2 is a binary
number one bit set so you can test that
by saying if the if you just subtract
one you get all the lower bit set if you
add them together you should get zero
but zero also satisfies that property so
0 is not a power of 2 ok so now I could
run GP verify on this and I you might
notice that made a slight tweak to the
color to make this not not hold there is
a loop there yeah but we have to have
looping barriers to do all that proof
right yeah yeah ok so assertion might
not hold for thread 544 group 0 you
might think hang on a minute is that a
problem specific to thread 544 so we
could say hear something like assume
that it is less than 10 for instance
which is just something you could do as
a programmer to to rein in the verifier
a bit and now ok it's actually
complaining I about thread 8 so she's a
constraint solver behind the scenes and
we just told the constraints over I want
you to find me a thread less than 10
obviously you have to be very careful
because if we do something stupid like
that then the kernel of course will
become correct all of a sudden ok but
anyway what I wanted to show you was
that now we could run the tool in bug
finding mode because as Chad said we
need loop invariants to prove
correctness of these kernels and it may
be that the car is correct but we didn't
find a loop invariant Lily the right and
Lupin being at the sea level yeah see
you live alone ah so i could say fine
bugs and then loop unwind equals four
for instance and oh wow ok let's not do
that
right now if I said find bugs and I have
1024 threads on the command line how
many chose to use in the bathroom yeah
so it so I say it ties in 2024 threads
but rule actually is going to only ever
consider two threads which I'll come to
in a minute and when I say and to make
that sound it uses some abstraction so
when I say find bugs actually it's gonna
be finding bugs not employing any kind
of loop invariant abstraction but it
will be employing some abstraction still
so the bugs could still be false
positives due to that abstraction no
it's not let me get on to that that's
the next part of the talk to ya okay
well I anyway I would have been able to
show you some things a bit wrong no but
are going to show you that actually
because I said greater than or equal to
0 this really is a bug and we'd find it
if we own mind enough and if we but why
can't show you is that if we just say
greater than 0 then the tool should be
able to verify this our man something is
seriously wrong ok I've been working on
this adding new features so never do
that before I talk oh right right yeah
wouldn't know but this I've now fixed
the bug and
so now it should be inducted friends
no but you have to since up live
illuminated right nobody should get this
should you think of that as little that
should be inducted right because you can
take a part 2/2 to Turner zero or
another power to that should work yeah
can you can you get out the actual
variable values 1introduction fails so
no we yes in principle because he just
yeah yes in principle but no not right
now okay I'm not gonna try and debug
this my net but as kind of retreat in
case yeah alright so let me tell you now
about the verification strategy behind
the tool so the first thing is what
we've been trying to do this project is
actually exploit the simplicity of the
GPU programming model to come up with a
very efficient verification method so
the first thing we explore is the fact
that data races always occur between a
pair of barriers so Barry 01 and Barry
02 all right so we have a barrier-free
region of code and we can have a race
between statements of different threads
in this region but we can't have a data
race between say a thread executing
something up there and a thread
executing something in there because
they can't be at these places I'm
separating so this immediately makes the
program analysis problem easier we can
restrict attention to barrier-free
regions of code this is something that
the pug approach also exploits okay the
next thing to observe which is something
that was kind of new to me but I've
since discovered is a fairly well-known
thing is that when you're doing data
race analysis you can often proven the
number of schedules you consider if
you're guaranteed to abort on a data
race so actually between a pair of
barriers a and B we can pull the
following trick we can run thread 0 all
the way from A to B and log all of the
accesses that it makes then run thread 1
all the way from A to B and log all its
axis is and also check all of its axis
is against those of thread 0 and if we
find that there's a problem between
these we abort immediately otherwise you
run thread to all the way from A to B
log all of its accesses and check them
against thread 0 and 1 and abort if we
find a race and we keep doing this until
we finally run the final thread from A
to B I guess we don't need to log all of
its accesses but we do need to check
them against those of all the other
threads and abort if we find a race so
the thing to observe here is that if you
think about it a data race always occurs
between some pair of threads so with
this schedule if there could be a date
between some pair of threads between
these two barriers then this shade you
will find the race because of the
logging and checking and if there's no
race then actually this schedule will
lead to precisely the same state here as
any other schedule would have if you
think about it you might think well
that's not true because the different
interleaving my resulted in different
interactions between the threads lead
into a different state but the threads
can only interact by racing with each
other and if they race we abort does
anyone disbelieve that or want me to
clarify it further i think is between
barriers no one can read anything that's
been around since the barrier yeah yeah
and woo is that exactly except well they
can but in that case we're going to
abort yeah right so I mean so you don't
even have to know the order of reason
rice in the fridge when you check the
locks right you just say did anyone
really some yeah that's right we can
record these assets not security yeah
okay so it's a pretty straightforward
trick and this immediately reduces us to
the problem of sequential program
verification now we can rewrite a GPU
kernel as a sequential program where we
serialize the threads in this schedge or
in fact any other schedule we choose we
could take a round-robin schedule means
you have a very simple sequential azizam
yeah posed to the brother more concise
case if you about the first yes that's
right yeah ok so this avoids reason
about interleaving which is good but we
can actually do better and we can
observe that data races occur between
just pairs of threads so what we do is
we can pick an arbitrary power of
threads I and J inside this region sorry
inside the range of threads and now we
can consider barriers a and B and we can
consider running thread I from A to B
and logging all of its accesses and then
running thread j from A to B and
checking all of its accesses now because
I enjoy yer arbitrary and I'm not by the
way talking about choosing a specific
pair like thread 1 and 2 i'm talking
about taking considering every possible
pair if we can show for every possible
pair that between these barriers they
can trace then there can be no race
between any threads for the barrier
region alright so in some feedback on
our work we've had comments that this is
quite similar to ideas in protocol
verification where you pick a pair of
of processes and you make the other
process abstract I think that there are
similarities but the key difference here
is that we actually have to consider all
of the pairs because these kernels are
not symmetric even though the threads
run the same program they don't have to
do the same thing they can follow
different control flow yes symbolic
plastic yes symbolic constant sorrow the
solar well because they're all possible
values yeah exactly so if a day to race
exists then some choice if I NJ will
expose it and if we can show for all I
and J that that this little program
pregnant is free from data races then we
know that there can be no data races if
we did have all the threads executing
any of you if you have it it is it can
be a false one simply because you
started it in that regional state so
that's that yeah if this was the
beginning of the colonel you had a
precondition on the entry state then up
to the first barrier we could be precise
all right but this brings me on to my
next slide which is is this actually a
same thing to do at all so say we had
Barry a barrier be we pull this trick of
running thread I and then thread J and
checking for races between them and then
we have barrier be two barriers see and
we pull the same trick again well if you
think about it this is not a sign thing
to do because it would appear that the
other thread just don't exist we would
see the world changing for these two
threads but then the point of a barrier
synchronization is now we can see what
the other threads did and we can safely
read it if we don't model those other
threads and we just continue then we are
you know you might have an array this
all zeros and threads are going to set
it to one so it should be all ones after
when we reach the barrier we're going to
see me one in two places and carry on
and then the analysis is going to be
nonsensical so this is not sound on its
own to make it sound we have to make the
shared state it's somehow abstract to
model possible effects of all the other
threads so there are two things we've
explored here in our initial work and
something more sophisticated recently
the simplest idea is to make the shared
state completely arbitrary there are two
ways of doing that one is you can have a
the shared state every time you reach a
barrier another thing you can do is you
can actually just remove the shared
state completely and treat or reads as
non deterministic reads so if you read
into a variable you just havoc the
variable and when you write to the
shades that you simply you log where the
right would have gone for race checking
purposes but you just
remove the actual right statement so we
have both options the second option has
the advantage of bullets avoids the need
to generate arrays for the theorem
provers recent back which can can lead
to more to better efficiency but it has
some disadvantages as well which I which
I won't go into right now but I can tell
you about them later if you're
interested alright so the GPU
verification strategy is to exploit that
any schedule will do trick and the two
threads will do trick with some
abstraction to me the whole thing sound
and also with predicated execution which
I will come to if i have time in a
little while to turn a massively
parallel colonel k so a program that we
want to consider for thousands of
threads potentially into a sequential
program p that is linear in the size of
k the text size of k such that if p can
be proven correct by which i mean that
no assertions can fail MP and I mean
partially correct here not totally
correct then k is free from data races
and barrier divergence this is the the
meta theory behind our approach but if
you're going to consider only do threads
then you don't really need predicated
execution rate because you can just copy
the colonel twice not they step into
procedures and i kind of thing oh yeah
it will lose their exactly yeah so
without coming to predicates get
execution shortly and we can describe
things am i doing for time I don't have
a watch on me oh oh they're okay cool
right okay so let me tell you briefly
about the the toolchain architecture
what we do is we take an open sale or a
CUDA kernel and in future we would like
to consider C++ amp curls but there been
more challenging because C++ amp has
this nice thing of being a single source
solution where you write a C++ program
and there are special use of templates
to describe that you want some piece of
code to be accelerated which is a bit
like saying it should be run as a GPU
colonel other amp is in principle more
general than that this makes it quite
quite difficult for an academic tool to
pies okay so these are easier targets we
use the clang and LLVM compiler
framework in particular the Klang front
end to turn a colonel written in one of
these programming models into a bougie
program sequential booby program so we
what we actually do is we we parse the
colonel and turn it into
one kind of boogie program and then we
do have this kernel transformation
engine that applies all our tricks to
produce a sequential program to be
verified and then we give this to the
Boogie verification engine developed
here at Microsoft Research and boobie
uses an smt soul but principally the z3
smt sober although we're looking right
now at support for the CVC force over as
an alternative which has a more
industry-friendly license and then for
verification to work we have to generate
candidate loop invariance and procedure
pre and post conditions although GPU
Curthose don't allow recursion currently
and they're not that big the programs
tend to be hundreds tens or hundreds or
maybe a thousand lines of code rather
than tens of thousands of lines of code
so we have not yet found a case where
it's it's we find it's always better to
do full inlining than to try to actually
infer contracts sometimes we can infer
contracts but actually it's more
expensive to do the inference than to
just in line everything and often the
imprints will fail so really our efforts
been on candidate loop invariants and a
good thing about this setup is that
clang is extremely widely used them is
being used by almost everybody these
days Bujji is very widely used in the
verification community and z3 esposas
even more about lose because i would say
that every verification tool at the
moment seems to use xiii and then i
think there are a whole bunch of other
users as well so these things are being
improved all the time other people fix
bugs in them and the only magic of our
approach where we have to actually be
really careful we don't introduce
unsoundness is in this component here so
we compare all of our brainpower into
making this correct and rely on that the
things being as correct as can be
expected from complicated mr. software
so yeah the scientists of our approach
is much easier to argue that it would be
have we built some complete verifier
four Colonels but we actually did the
verification condition generation in
some smart way ourselves okay so now I
wants to show you an example of how we
take a kernel it doesn't have any loops
or conditionals and do this two threaded
transformation I'll go through this
reasonably quickly because it's fairly
straightforward so this is an open CL
kernel and what we do is we generate a
sequential program i'm going to show you
in c form here although it would really
be a bougie program void foo and we have
a precondition we introduced to
valla Constance Ted dollar one and tib
dollar too we have a precondition saying
that they are in the range of thread 80s
there between 0 and n and then they're
different from one another so this is
our way of considering two arbitrary
distinct threads we have these symbolic
constants tip dollar one and two dollar
two and then what follows dollar one and
dollar two are going to be used to
indicate that the version of a variable
for the first thread or the second
thread and just to be clear I'm not
talking very much not talking about
threads 1 and 2 i'm talking about the
first and second of the two arbitrary
threads under consideration okay what we
do is we take the parameter idx and we
say that every that each thread has its
own copy of this parameter and we had a
precondition saying that these copies
are initially equal because the colonel
gets invoked with parameters been passed
by value and every thread receives the
same value for parameters so the threads
could in principle change these
parameters later which is why they need
their own copy but initially the values
will be the same and we just remove this
array a because i'm going to show you
this abstraction which we call the
adversarial abstraction where the shared
state just disappears completely then X
becomes X for thread 1 x4 thread 2 y
becomes Y for 31 y four three two and
now this read from a tit + index into X
this turns into the following first of
all we log that a read has occurred from
a and we log that thread one was the
reader just thread one we're not going
to consider thread to reading only to
everyone so Ted dollar 1 plus idx dollar
one that's the offset for the read and
then we check that thread to reading
from a at this offset is okay with
respect to any prior reads or writes
that have happened which in this case
would just be that read okay but this is
the general trans translation so we do
logging for thread 1 and checking for
thread 2 and then to reflect the fact
that X will it be modified by reading
from the shared state we havoc X for
both of the threads so both threads are
doing the read but we're logging it for
the first thread in checking it for the
second thread but we have to reflect the
fact that the rehab for both of them so
we have a both copies of X and this
model is the fact that a could have been
changing arbitrarily by other threads
that may have even been having data
races with each other
then we do the simple way so log recheck
read havoc way and then slightly more
interestingly when we write into a it
did we write the value X plus y then
what we do is we log a right by thread
one at index dollar one so at thread
one's ad then we check a right at index
kids dollar 24 thread two and then
there's actually nothing more to do so
for the reed case we have two havoc the
receiving variable but for the right
case we don't have to do anything to
reflect the effect of the right because
we have actually removed the shared
state completely so it's like the right
is disappearing into the void and in
return reads anything arrives alright so
now let me explain briefly the race
checking instrumentation we use this is
a bit of it over an implementation
detail we could have done something
different here but this we found to work
very well so for every array parameter
we introduce a bunch of global variables
we have a variable read as a card for
the array which is a boolean and a
variable right has occurred which is a
boolean and the idea is that this
boolean will be false if we are
currently not considering any read being
in flight for this array and it'll be
true if we are considering some read and
then we introduce a variable real offset
a and available right offset a which are
integers so the idea is that if Reid has
occurred is true then read offset says
the offset corresponding to the read if
readers occurred is false then the value
of read offset is irrelevant so this
allows us to track either 0 reads or one
read but no more than that does this
make sense so far and i'll show you how
we're gonna use it in a minute and then
we introduce for procedures a procedure
called log read a which takes an offset
log right a which takes an offset check
read a and check right eight each of
which take an offset and the idea is
that the the log procedures will be
invoked with respect to the first thread
and the check procedures with respect to
the second thread so what we're going to
do is we're going to consider just the
first third logging and just the second
thread checking but because we're going
to consider all possible pairs of
threads this is ok so we're exploiting
symmetry here ok and we get rid of the
array parameter alright
yeah okay yeah you're one step ahead all
right so I think I've explained this yep
this is for undergraduate at summer
school so I stepping through a bit more
slowly but I think we can skip on okay
so log read takes in an offset and there
may be immediate than you might think of
would be to say we say that a read has
occurred and it occurred to offset but
this wouldn't be good enough because
this would mean that we would only be
logging the latest read and we want to
be able to check for races against any
prior read that happened before the last
barrier so what we do is we wrap it in
an F star so the theorem prover that the
verifier should consider that the
program does log this read or then it
just continues to log whatever it was
logging if anything all right so yes
star is an expression that evaluates non
deterministically so we either log this
read in which case we dis occurred a and
redox a are overwritten or we leave them
alone in which case they do whatever
they were already doing and log right is
exactly the same now check read is very
simple we just assert that if a right
has occurred to a by the other thread
then the offset written to by the other
thread must not be the same as this
offset but I'm checking and that's all
we have to do and yep this is where just
said and check recheck right is slightly
more sophisticated because the right can
race with either a right or a read so we
check that if a right has occurred then
they offset written to must be different
from the offset that I'm writing and if
a reader's occurred the offset read from
must be different from the offset that
I'm writing okay and then finally we
have a precondition on the whole kernel
saying that there initially is no read
and no right on the array a and we do
this for every array in principle we
could we could just have a single
boolean and a single offset and we could
store for a reed which array it was from
which offset okay and that's actually
why I tried first and shaz suggested
splitting up into multiple arrays
because I think this was given the
theorem proved a really hard time and
also it means that when you go well if
you know about theorem proving based
verification you have these modify sets
for loops and it means that if a loop
does a read that read is going to kill
this logging stuff for every array in
the kernel you have to have an invariant
recovering where reads could have
happened for you know all the arrays
even if they weren't touched by that
loop so splitting things up here has
some advantages and then barrier is
quite nice to implement well the first
way I thought about doing it was that
barrier should set read as occurred and
writers are going to false by assigning
to them and then I realized that this
was really inconvenient because you
might have a loop that does not read or
write a particular array but it's got a
barrier in it and then because we were
assigning to these variables they were
in the loop modified set and then we had
have invariant saying that you know
they've got the same value they had
before and oh yeah we had to have
invariance about these variables whereas
if we just assume that they are false
this does the trick ok so the intuition
behind this is that if you remember that
the instrumentation there was always a
possible non determinacy choice not to
track a read not track a right so
there's always one path that's very lazy
it doesn't do anything it just goes no
no no no no no no no no no no no no and
then there's this tree of other parts
that do track some reeds and some rights
against each other and there are the
important parts that actually find data
races but when you hit a barrier you
basically say assume that we did the
lazy thing so all the paths that did
track region rights they get sniped and
we only have this path that we can do
race logging and checking a fresh from
there ok any questions to buy this I
mean it's a kind of low level but I find
it quite interesting find this assuming
business quite interesting yep well this
analysis on so if you have a single
thread which
but really on the right in the center at
each the realest real analyze the
difference upset yeah this analysis say
there is a database insider threat it's
sequential you mean like so we have an
array a and we do something like a read
so x equals a of some offset society of
Ted saya and then a right did you say
like eight it equals x plus one for
example yes yeah so I will you consider
has better ways no because there is a
race checking performed within a thread
so it's we're not going to check likes a
thread six we're not going to check just
thread six is Ryu conflict with thread
six is right but but for a threat nine
we will considered as thread sixes read
config conflict with red nines right but
they won't complain because they use tid
which is different for every thread but
as I see the transformation you should
before yeah so the first one will
generate a read so we do like log read
ted one yeah and check read ted to
that's what we would generate Oh
basically chefs okay and then we would
do is log right Ted one check right on
to yep when you log right kid oh you
never checked in no we only have a
checked it to ya sure the first thread
of the logger and the second towards the
check of this so what basically we're
going to consider that log read and that
check right we will consider we will
look for conflicts between those and for
conflicts between those okay yeah that
that case
and actually that case there will check
for a right comforting to the right in
both cases you can see it still 1 into 2
which will be different so good
questions here there is between
difference putting different threads
that's right okay now let me tell you
how we had a loops and conditionals so
we use predicated execution the idea
here is that we flatten the colonel so
that all threads execute the same
sequence of statements we more or less
eliminate any conditional code we can't
eliminate loops those who have to do
something a little bit sophisticated for
loops so let me explain first of all
independently from GPU girl I predicated
execution works just consider this
snippet of code if x is less than 100
increment X otherwise income and why we
can make this predicated by introducing
two fresh booleans P and Q and we can
say that p is assigned to the boolean x
less than 100 + q assigned to the
negation of that not x less than 100 and
then we can have if this programs being
executed then both of these things will
be executed but in predicated form so we
will say that x becomes equal to x + 1
if p holds otherwise it just gets
assigned back to X so effectively this
is a no-op if p is false and then we can
execute this statement so Y gets
assigned y plus 1 if Q is true otherwise
it gets reassigned why okay so this is a
something that has been used it's used
sometimes and compilers if you have got
say a processor where branches are quite
expensive you might have a little bit of
conditional good like a simple if then
else but you don't have many statements
in either the then branch of the else
branch it might be more efficient to
flatten the whole thing using predicates
than to actually have a branch and
invoke the expensive Africa just to a
question about the cell violent imagery
how would I compiler translate p ? x + 1
colon eggs it would you use a branch
well it would use a special instruction
a special select instruction yeah so
this is going to work at the
architecture supported that kind of
destruction it's a very common thing to
you in two if you want to vectorize code
automatically you first can eliminate
these conditioners and then vector
architectures tend to have a select
operation that can crunch through things
like this so you may kalakaar a vector
of boolean's and you've got a vector
select that takes a vector bullion's and
then a vector of then values and a
vector of else values yeah but it
doesn't work if you've got really deeply
nested stuff because if you flatten it
all then you've got selects with selects
with selects with selects with its
Alexis item but it's fine for a theorem
prover okay so what we do is we apply
predication so that every execution
point there's a predicate determining
whether each of the two threads is
enabled and then we parameterize the log
and check procedures with a predicate
recording whether the threads are
actually actually logging or actually
checking or really they're not supposed
to be there because they didn't get into
that part of the code so yeah like X
becomes equal to e we turn now into for
each of the threads this is if we've got
a predicate p this we're translating
with respect to a protocol p we do the
the Select thing so we say for the first
thread X gets a dollar one by which I
mean the expression e dollar eyes so I'm
going to take all the local variables
and turn them into them to their dollar
one form ok so if P holds then e 1
otherwise X cuts left alone and then an
array read becomes we log the read for
thread one but now we pass in p12 to
record where the P was really alive or
not and the same for checking for thread
2 and we'd do the have a king but
neither have the King has to become
predicated so we don't necessarily have
a KX we only have a kext if we act are
enabled so we now have to do something
like X 1 becomes equal to if P then
arbitrary otherwise x1 in fact like we
do something a bit different from that
in bougie because you can't have star in
buki and a right is almost the same as
before but we pass in these predicates
all right now I think I have some slides
about loops
coming up yeah that the if-then-else is
where the predicate scum from so if
we've got if II do s otherwise duty what
we do is we introduce a new predicate q
Soapy's our current predicate so if we
say q4 thread one is p for thread 1 and
e 1 so we take the existing predator and
we strengthen it with the conditional
and then we have another predicate r
which is the incoming predicate
strengthened with the negation of the
conditioner and in case you were
wondering before by the way why didn't
why it had P and Q I didn't just use P
and not-p maybe this will if you were
thinking that maybe this will answer
your thought because Q and I are not
going to be necessarily in the negation
of one another because they're both they
both involve p right / subtlety okay and
then we translate the then side with
respect to Q and then we translate the
else sigh gratis para are okay and then
the loop cases I think it's the most
interesting what we do here is we we
can't eliminate the loop rope but what
we do is we force both threads to keep
executing the loop until they're both
done with the loop so if neither of them
want to enter the loop would you skip
the loop but if what are the monster
enter the loop and one doesn't they both
enter the loop and they both xq the loop
but the one that didn't really want to
go in the loop just does nothing until
that first thread is finished and then
they both leave ok so we turn this into
the following we evaluate the loop guide
into a predicate Q so it's the loop
guide strengthened by the incoming
predicate and then we loop while I the
q1 is true or q2 is true so we loop
while at least one of the threads is
enabled we translate the body of the
loop with respect to Q so this means
that a thread when we translate s the
body with Q this will make sure that if
a thread was not enabled it hits Q was
false it won't do anything and then we
are we update Q so we say that Q becomes
its old value strengthened by the loop
guard which may have changed probably
will have changed according to the new
body okay
we do yeah yeah that's right and that's
why you saw paper was about that was
actually in my opinion one of the
biggest challenges of this project so we
had all this figured out Morris when I
was a Microsoft and then we started to
build a front end based on clangs AST at
the structured level because that was
the easiest thing to do in our minds and
I worked okay but it meant that we
couldn't handle kernels that did switch
statements or kernels with breaks and
continues they or they were hard to
handle so yeah so we thought about the
go to the but then we thought how do we
do this predicated execution at the
unstructured programmer but because here
it's very nice because of the
hierarchical structure you have a
predicate you can descend and ascend in
any way she's had a very smart idea and
we worked out the details soon ya
understand the purpose of this is
predications a lot of it seems like what
good you had already do for verification
in their engine so is it because of the
loops that that you have to reduce this
explicitly that you can't just let
what what we're trying to do is just so
we're trying to give boogie a program
whose correctness implies race freedom
of the GPU kernel and this moment we're
just trying to construct that program
once we have that program bougie then
has to do as usual thing on the program
is to do verification condition
generation you argued previously that we
could just run thread 1 followed by
thrilled yeah yeah and that was going to
be our schedule in fact you have sort of
oh well I told you that you could do
that schedule or a smidgen you could do
a round robin schedule where you run
like for everyone might accept their
tunics acceptable mixes that protein
mixes that means owning that one right
so the question is why in fact you not
just do the schedule where thread one
execute all the way to the very random
yeah bruh to and I think the answer is
what he said which is that it's the
loops because you want to go to join t
invariant for those two threads out the
loop heads right what I would say it's
like more fundamental that but like
forgetting berries from it how do you
even write down that program the where
say you've got say you've got a barrier
in a loop for instance and I didn't want
someone if you I would argue that if you
did not have any illusions
program ray yeah it was a straight line
code then what you could do we just
replicate the code from one barrier to
another twice one for Ted 1 152 for the
barriers and nested inside conditionals
well oh if you have no conditionals then
yes you can do exactly the replication
thing if you've got conditionals like a
nest of conditionals with barriers here
and barriers there how'd you get these
barrier regions barrier to bury regions
is it it becomes tricky and also what
you do if you've got procedures and you
go you've got like some code you go in a
procedure then there's a barrier then
you have to basically expand everything
I know I said we do do fill a night and
currently but we don't want to be
limited to that we would like to be able
to do a modular analysis yeah yeah I
mean I guess if you know where the
barriers are and you can go from under
the next yeah okay so yeah okay so yeah
I think it's a really good question and
that was actually what I very first
thought of but the problem I found was
that this notion of going from one
barrier to the next becomes quite
complicated with conditionals the next
barrier might be the same barrier
haven't gone round two iterations of a
nested loop or something so I couldn't
work out how you could write out this
this program but that it's starting to
expand all the possibilities you know
you might be able to kind of expand all
the possible resolutions of the
conditionals but then I think the
program would grow very large to control
the cases well you have to consider all
the very very real cars yeah yeah
and this is a little odd right this is
any industry and and I think that the
thing that this brings which we I hope I
hope people were right more complex GPU
kernels in future thus verification
become important more important and thus
modular verification will become
important and then I think a strength of
this predicated approach is it means
that actually both threads appear to be
stepping into a procedure at the same
time and then you really can verify
procedures in isolation and use
specifications solution okay so you
haven't shown us the predicated barrier
yeah that's sure that's very with the
anakim chance okay you haven't seen
enough slides ya know quite so I'm
running over time a bit nicer I'll go in
a few more minutes in this all right
because there are lots of questions in
the middle you can continue a cool-down
45 okay if you need to go I won't be
offended okay so yeah barrier we turn
into barrier giving it both the
predicate predicate weren't pretty good
too all right and now so the log reading
log right they get modified in the
obvious way so we now have an enabled
parameter that says it's a threat
enabled and now if there's not enabled
we do not log its access but if it is
enabled we may login access so this is
like before but just with this
predication okay and yeah the same for
that the checks are predicated so check
right says if I'm enabled if I'm
disabled there's nothing to Jack I'm not
really here I'm not really going to race
but if I'm enabled then do the usual
check ok but no this was just this
assertion that said we haven't blown
yeah anything
right so now sorry so let me show you
yet so the barrier takes these enabled
parameters now this is how we check
barrier divergence and actually
predicated execution actually that's why
we first impregnated execution we wanted
a way to check barrier divergence
remember we were like talk to all these
people about wall what is this buried
averages problem we finally nailed down
and that was the initial source of
predicates predicated execution but then
it had various other benefits which I
know I think of what very different very
so it's slightly more subtle than that
if they wind up at different bear
instructions that is very divergence
however if they get to the same barrier
but they've executed different numbers
of loop iterations that's Barry
divergence so for instance if you've got
an outer loop and an inner loop and a
barrier inside the inner loop is not
permissible for one thread to go to
basically go how to loop in a loop in a
loop in a loop in a loop and loop and
the other thread to go out to loop outer
loop outer loop a loop in a loop in loop
you know and hit the barrier the same
number of times that's no good they
actually have to take the same path
through the loops hardly open CL spec
yeah and the reason for it is that it's
very difficult to actually compile a
barrier operation in the way you could
in something like open MP or MPI you
kind of processes hit different barriers
and you can implement barrier
synchronization that way in a GPU
Colonel it would be very difficult to do
that due to the the way these threads
actually work so the way we implement
barrier is we assert that the threads
are uniformly enabled either they're
both disabled or they're both enabled
but if one is enabled and the other is
disabled this means that they've reached
the barrier they've either reach differ
one of them is it basically is one of
them is not there either because he
would be going to a different barrier or
not going to any barrier ever or would
be say in a like out of sync with
respect to the loops they're executing
so this precisely captures the
requirements for Barry divergence
checking if I mean I sometimes give talk
by really explain that carefully with
some examples which i haven't done here
and then if if the first threads not
enabled if either of them is not enabled
because they then they're both not
enabled to be returned because the
barrier is not really been hit otherwise
we do the assuming thing as before
you want to say if not enabled one or oh
I see because that everyone is equal to
might be clear it might be clearer not
to do that optimization but it so make
sense the definition of our divergence
for non structure
okay yeah all right well that I mean no
I just had a slight here where if there
was time I would talk about some of
these things I don't have slides on them
so the like the we spent a lot of time
working on invariant inference which I
find very interesting we use who do you
need to do that and and now i'm working
on and I'm I've been talking to a castle
are quite a bit by this is some ideas
for trying to optimize Houdini to be a
bit smarter in how it considers
candidates and for GPU verify my ideas
work really well that's where they came
from and cash gave me some obfuscate the
bougie programs and they might ease
didn't work well on them for and I'm
hoping to talk to share about that of
this next couple of days so yeah there
were a lot of really interesting
practical issues in building on the
Alabama compare different but the
principal one was this predicated
execution for unstructured control flow
graphs that was very challenging and
interesting and then doing source level
error reporting actually was very
interesting because these we don't
report some assertion might fail we
actually want to report there might be a
date race between a pair of statements
and the problem we have is that what for
one of the statements things are fine
that's the statement that was reached
second and then there is an actual
statement we know that that statement is
the potential culprit but then that
statement will be interfering with
something there may be another statement
or it may be something that came from
abstracting a looper abstracting a
procedure and what we want to say to the
user is we want to give them a
best-effort guess at the program
statement that caused that problem so
what we actually do is we carry source
location information around in lucan
Darian's so we wither with a read or
write we log the offset we also log the
line number and then we have loop
invariant that say things like if
there's a read it's from an offset
satisfying this pattern and it's from
one of those line numbers I seriously I
know it sounds crazy but and that means
that when we get a model from XIII we
can ask which line number the the the
first era came form and that allows us
to give er a potential error I mean
the way you have encoded the lien or the
right logging is using this
non-deterministic choice yeah so just
following that control information which
ranch was taking will that give you know
because say you've gotta read this
inside a loop then have licking the loop
modified variables can just set can just
say a redid occur in it occurred from
location 5 billion and that may just be
a false positive or maybe that there is
a right in the loop that could write to
some to at that location so what we have
to do is additionally but what we do we
don't have to do this in fact Matthew
Park concern I told him and he came up
with a smart idea but what we do
currently is we actually carry around
source location information in those
tracking variables then we have at the
source line and then we have a global
invariant saying that the source
location variables can only be one of
the possible locations they could be and
then we have more smarter in various
they try to infer bounds on those line
numbers Matthews idea of smita right
basically you know a certain check
failed you know so you know Amelia which
arrays the problem so now what you can
do is you can eliminate all of the other
all of the other checks and you can like
all the logs and other arrays now you've
got a bunch of logs on the array in
question split them into two sets
disable them half of them here half of
them there and run the verifier on both
at least one will fail as soon as one
fails kill the other one and i divide
those into two and keep going you
basically binary search until one log
remains and one check remains and that
would avoid all this invariant stuff
because it's very expensive to carry
this stuff around any votes I get it is
it Delta debugging yeah take a take away
keep taking away okay yeah yeah anyways
beside you got hard to implement might
be yeah there might be a better way to
do that i don't i don't know but um bad
you know out of the very sources of
imprecision now what's most commonly the
reason why you fail to raise freedom is
it because you sort of had withdrawal
the reads from the array yes not having
a strong enough loop in Barry and saying
that way ok so it's so is it because
you're at structuring allowed a loop
invariant that would prove it or is it
because by abstracting so much you know
by turning all the rates of the resin to
happen yes you know you just wanna be
possible it's almost always that there
are some examples where our upfront
abstraction makes it actually impossible
to verify the colonel no matter what
invariants we didn't got but that's
every year and there's an important
class of curls which gives a paper under
review about dealing with using this
barrier invariance technique for more
precise abstractions but in the for the
most part kernels don't fall into that
category and those kernels it really
comes down to finding loop invariants
the characterize the access patterns of
of arrays and we have a template based
approach for doing that and template
beta bridges have advantages but their
main disadvantages that if the access
pad Falls I said the template or is
obscured by by syntax that you know then
then we then we die it's hard because
you have access to the service for
yes Owen straight on me or maybe yeah i
mean i can show you some examples we
have a bunch of kisses and so the things
i would like to explore our be more
aggressive with the invariant inference
and using this technique they alluded to
about optimizing Houdini to make Houdini
be able to take larger candidate sets
because the more candidates the more
chance you have second thing is we're
looking at a daikon like technique for
actually doing some dynamic and Varian
generation to give Houdini candidates
that's probably how we're going to try
it and the third thing what we have a
grant-funded in which we said we would
export interpolation so what just a
simulation yeah actually simulated
destroying you said you loved this
tremendous juvie yeah we either we just
need to run the boogie program if that's
possible okay using that this is too
like I remember the name is someone is
rayna boogie in charge yeah so we might
use that or we might work on an
interpreter the llvm bit code so that we
can run our Colonels and this is queasy
el tule Imperial that is dynamic
symbolic execution of opencl kernels the
part the main problem is once we find
these invariants he's likely invariance
how do we map them back to the actual
boogie variables that's the kind of
practical problem that command that can
be really tricky if you're dealing with
yeah actually the dude on the larger on
the very condition condition
okay we are the the problem with but the
daikon start approach is also a template
based approach it has the potential to
find things that are syntactically
obfuscating but occur dynamically so you
can see that they happen but still you
have to have your predefined things
you're looking for so I really like
despite haven't seen a number of talks
about interpolation I still don't get
the general thing but I'll I understand
things up interpolation you may be able
to discover things that are problem
specific it was just about give me my
diners you can discover things you can
actually prove that's true to say they
rely on some non linear okay so so we're
really trying to actually push this
technique to people in industry and I
really think they're probably they're
more interesting bug finding and
suchlike so we might use the daikon
thing to take invariance and trust them
you know just as I mean so I think that
you know your care of reviewers will cry
if they hear you say that bad all the
damn thing once you find enough but
instead so listen another from so often
you have a loop that's going to do an
extortionate lee large number of
iterations and it's correct and then you
have another loop this bad and you're
never going to get past that with
unwinding so you may be able to get a
hint at a problem by a failed proof
attempt I have some ideas about trying
to trying to under a proximate loops
yeah yeah so there's a nice paper by
gear rising back and others at cav but
was not loop acceleration is loop under
approximation which something
yes so I look at that and I think like
that was formulating some some things
I've been thinking about which is why it
would be cool to have over level right
okay so what's the other people could
benefit so that everybody is your
benefit yeah that's right yeah I think
that there are many advantages to what
you're proposing because if nobody is
really interested in all these templates
and invariance that the lowest level
that you can do it the less engineering
would have to do trailer and your career
shares information glad you so like I
said my plan for practical deployment of
GPU verify is as follows there are three
versions of the tool is one version
which is eager to find bugs more version
is eager to verify and one person is
neutral so by eager to find bugs I mean
you do like unrolling for instance but
you get to verify I mean for instance
you turn off race checking but keep on
race logging so that you can quickly
find your best in there in with Houdini
whether the expensive actually
performing the race check and once you
find the best in very you then see if
it's good enough to do the race check
and then in the middle you've got the
one that just does everything at once
and it may it may quickly discover that
proof won't work but doesn't find a bug
so the idea is if the bugfinder finds a
problem you kill the others and say
there's a problem if the verify approves
things you say good and if either of the
two verifier approaches fail you just
ignore that and after 90 seconds you say
no problems were found so that's my kind
of that's my the way I envisage people
getting use from the tool I don't really
think it's going to be very useful if is
I mean despite our efforts within
varying inference there's a very high
chance that the tool will report false
positives
trying to take a hit incomplete crews
and sort of use those as a way of google
triaging error reports yes I think which
ones are most likely
also the thing that you're doing the
joint invariants for the two threads
loops yep is actually it seems pretty
closely related to this differential
assertion checking pretty okay yeah
because he's essentially doing that as
well he has a construction that's
essentially giving you your boogie ty
boogie transition that's essentially
given you joint invariants for four
loops and for them okay so you got to
like two programs it's like your
concerns and interests and say two
versions of the same for a seat right
and what he wants to know it's starting
to say one is safer than the other you
know the one if one gives the second
version if the second version crashes on
a good name put then the first version
crashes on that on that input and so
that's what he needs by differential
assertion jacket yeah so but in practice
what that means that you're looking for
jointed variants for the loop so it's
like running you know the the loops and
the two versions okay lockstep it may
give something's dead okay definitely
talking about okay thanks for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>