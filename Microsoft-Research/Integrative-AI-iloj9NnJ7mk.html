<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Integrative AI | Coder Coacher - Coaching Coders</title><meta content="Integrative AI - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Integrative AI</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iloj9NnJ7mk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay well welcome to the session on
integrative artificial intelligence
interview today I over the last 25 years
or so we've made in our field
significant advances on imprints and
decision-making perception she learning
speech recognition natural language
processing even dialogue systems getting
more fluid and flexible and at the same
time much has been said about the
certificate trends in AI vision pulling
apart from main AI conferences ACL being
a separate meeting with natural language
processing the speech the speech
conferences sig dial dialogue meetings
and so on these advances that i
mentioned have come at the cost of the
separation into these separate societies
and communities at the same time i have
typically celebrated the incredible work
going on along these different
dimensions of competency from the point
of view of the founders of AI so there's
been an interesting attention building
especially in people that want to build
end-to-end systems that accrue or
accrete multiple competencies of the
form we see for example in human beings
listening understanding reflecting
planning into larger intelligences of
the form closer to the type of systems
that the founders of AI were thinking
about in their initial proposals and
reflections they really interesting
tantalizing promise of weaving these
competencies together but there's lots
of questions about how to do that well
what the outcomes might be and the
promise of this path as one approach to
reaching surprises unexpected new kinds
of experiences and abilities we have
some really exciting work going on at
Microsoft Research these kinds of
systems we also see this trend among
work in the field among our colleagues
some of whom were with us today in the
audience and we'll be presenting and
it's really interesting to see these
different kinds of approaches going on
yesterday I in the early morning
sessions we saw approaches to
integration for bringing language and
vision together for example more deeply
and richly and fine in fine grain
representations that's one approach we
could also imagine approaches that bring
together separate modules that you might
say play a symphony of competency
together what is properly coordinated
and we'll hear more about that today as
well so we have three fabulous speakers
today fabulous researchers leaders in
their field all doing what i would call
core interview today I the science and
the engineering of that kind of work
danbo who's is a researcher in the
adaptive systems and integration group
at microsoft research he's been seeking
to develop systems that embed
interaction and computation deeply into
the flow of everyday tasks and it's both
collaborations his work over the last
few years has focused on computational
models for multiparty engagement
turn-taking interaction planning and
court challenges to support this kind of
thing in inference learning and decision
making prior to joining microsoft dan
obtained his PhD degree from Carnegie
Mellon University where it wasn't a
surprise he investigated very similar
problems of dialogue management and
error handling all most to those two
dimensions of analysis are very
important in his in his current work
also heralding more recently from CMU
like now is been well avello so who's
the herb Simon University professor in
computer science at Carnegie Mellon
manuela is is has been leading bunches
of work in AI and the subfield of
robotics she's passionate about
autonomous agents that collaborate
observe reason act and learn she's have
been pioneering work in robotics people
probably know for working robocup
in work on collaborative robots robots
to collaborate with people co bots as
the word she has used a coined for that
she's an i triple e fellow triple-a as
fellow triple AI fellow and i had the
pleasure working with her as the past
president of triple AI and she doesn't
of course she's well known for her work
in in Robocop as being one of them the i
would say to you know leading spirit
behind robocup Larry's it Nick you
people here yesterday saw some of his
presentations he's a principal
researcher in the interactive visual
media group at microsoft research he's
interested in a broad range of topics
related to visual object recognition
language and artificial intelligence
he's been an inspirational leader with I
think your recently awe-inspiring
efforts and directions intimate
understanding the grounding of imagery
with language and I think people here
are familiar with his recent work in
abstract scenes the common objects in
context the cocoa a challenge problem
and workload effort and image captioning
work which stemmed from that effort to
put together the cocoa data set before
he he did somewhere that work he did
some several well known earlier
technologies like photo DNA before
joining MSR he received his PhD also I
didn't recognize this when I first
looked at these BIOS also from CMU I
guess is to CMU thing is this trend at
MSR it really holds that well to start
hiring mostly from Stanford in future
years up sorry all your schools with
that let me let me have Dan come up come
on Dan thanks all right thanks yeah so
I'm a researcher here ended up this is
the interaction on group here in MSR
Redmond and as Eric mentioned my work is
on physically situated language
interaction basically the central idea
that drives its research agenda is
getting machines to kind of look more in
their surroundings into physical space
understand how people behave and drive
interactions with spoken language off of
that understanding so over time we've
done work with you know virtual avatars
that
book shuttles on campus and robots that
give directions to people's offices and
conference rooms and games that engage
multiple participants in educational
games and so on and so forth that's kind
of the class of systems we've been
looking at but the point that I'm going
to try to drive home today is that this
whole space of physically serrated
language interaction I think can
constitute is like a prime example of
what we we might mean by this term of
integrate integrity of AI because you
have to bring together components and
comparin sees from different realms of
artificial intelligence like vision and
speech and planning and embodied control
and everything has to work together such
that the whole that you create is larger
basically than the sum of the parts so
in today's talk what I want to do is
give you one small sample of research in
this space of recent work we've done in
this space that hopefully drives this
point home and then I'm gonna actually
switch gears and pivot and talk a bit
more about some of the challenges that
we've encountered over time in this
space that I think generalize that are
broader than just about situated
language that are about more how do we
construct disintegrative systems so
let's start with language well when
people whoops this is not an sting of
course
so when people come together to do
situated language they need to manage in
parallel and resolve a bunch of
different problems and often we do this
without consciously thinking about it a
first problem we have to resolve is make
sure we have an open communication
channel and this gets regulated in a
sort of mixed initiative fairly complex
process we call engagement where we
negotiate how we initiate how we
maintain and how we get away from
interactions it involves a lot of verbal
and nonverbal cues a lot of things that
we implicitly pay attention to and we
coordinate with each other to solve that
once we have this problem so we need to
bounce signals to each other and that
also is coordinating we don't all talk
at the same time there's this process of
turn taking in conversations there's
again this sort of locally coordinated
mix initiative process between people
that again leverages nonverbal signals
and various kinds of cues only on top of
that we start recognizing intentions and
decoding the meaning behind these
signals that's where speech recognition
language understanding these various
layers come in and then on top of that
you have to understand that in the
context of the entire interaction
integrate that with the discourse
history and plan the conversation
forward so this is almost like a minimal
stack of communicative competencies if
you want there's there's different ways
to slice and dice this space but that's
one way to think about it and in the
work that we've been doing is we're
looking at basically how do we integrate
this with a situational context how do
we construct models for these processes
that leverage information from the
surrounding environment from what
happens around us the who the what and
the why of the physical situation so
today what I want to do is give you one
small example of such a piece of work
and in particular I'm going to focus on
one piece of situational context which
is a participants attention where
someone is attending while we are
speaking and I'm going to show how that
relates to turn taking to a model of
turn taking this work is inspired by
observations made by Goodwin in the 80s
on the relationship between these
fluences and attention this is what by
the way with that in turn Joe you at CMU
that was an intern with us last year and
so the starting observation is that we
all know that conversational speech is
kind of discipline there's all these
hesitations and arms and us and everyone
that does speech recognition knows that
if you look at transcripts they look
kind of weird they look like this
it will be like anyway oh we went I went
to bed or Brian you're going to have you
kids'll have to go or I come in I no
sooner sit down on the couch right
there's all these false starts and
restarts and hesitations and Goodwin's
inside was to look at this in
relationship to what the listener is
doing and in particularly relationship
to what the listeners attention is doing
and so here I have that represented with
a red line is when the listener is not
looking at the speaker and the black
line is the portion of mutual gaze when
the two of them are looking at each
other and so if you look across all of
these it's kind of interesting because
there's this pattern that emerges where
the last restart the point from where
the utterance becomes grammatical
coincides with a point of mutual gaze
and so that's kind of interesting he has
a number of other examples where the
pattern works in slightly different ways
for instance here with a dotted will can
see this varial the dotted black line
represents a portion where the listeners
gate starts to move towards the speaker
and then the circle is where gazes
reestablish mutual gazes be established
so again we have this entire grammatical
portion of the sentence that coincides
with a portion with someone is driving
their attention back and so with these
observations they serve there's a
hypothesis that start forming that maybe
these differences are not just errors in
production maybe they actually have to
do something with regulating attention
and with the ensuring mutual ground I'm
going to you know generate the
grammatical sentence while you're
looking at me and maybe the fact that
I'm producing disfluencies is going to
make you turn back towards me now in
contrast to all of this if you look so
there's this very high in some sense
coordination between people that happens
across these channels now in contrast to
this if you look at what we do in
typical speech interfaces today well in
mobile phones we have this push-to-talk
systems where you push a button and we
talk and that makes a lot of sense for
those interfaces for the form factor but
when you don't have that affordance in
general in dialogue system for a long
time there has been this volleyball
assumption of you speak than I speak
kind of of turn-taking and that leads to
interaction breakdowns often and the
interaction between breakdowns become
even more prominent if you go into
multi-party settings or if you try and
do language in the open world like with
this robot trying to give direction
there's multiple people around so I
wanna show you one of the
examples of two here trying to interact
in the video you'll cease from the
robots viewpoint so she was trying to
interact with the robot and try to get
information about the conference which
is actually rushing towards so you see
what happens say that again I'm looking
for my meeting you said Matt McKinley
right sorry that was Alberto back li you
wanted correct well yes I'll see you
later so apart from the speech
recognition errors and those problems
the robot is completely oblivious to the
fact that she's looking in her cell
phone at this point trying to find the
room number she's trying to get to and
continues kind of contributing in the
conversation trying to get her to say
something and so with this and looking
thinking back of good news work but we
thought hey can we build a model where
we coordinate speech production
incrementally in a better fashion with
the attention of the participants in the
sea so we developed this model where the
basic ideas is a balance between
attentional demands and attentional
supply so on one hand we have
attentional demands which are defined on
each phrase and which specify where do
we expect the listeners attention to be
at the onset of the phrase when we're
about to start producing the phrase and
throughout the phrase and when I have
different expectations at different
parts of the dialogue on different
astron seas for instance in most cases
if ask your question maybe I respect you
look at me but if I tell you about going
that way well maybe it's okay that you
look where I'm pointing right so we
define these attentional demands and
there's interesting questions about how
do you define those and then on the
other hand we're measuring the
attentional supply we're measuring what
really happens with a person's attention
this is based on in France models that
leverage deeper down machine learn
models that track visual focus of
attention based on features from active
appearance models and head post trackers
and so on but the basic idea is you have
these models and when you have a
mismatch between what you're expecting
and what you're seeing instead of just
producing blurting something out you use
this coordinate if policy where the
robot instead of saying for instance to
go to 38
to take a left take a right it
introduces these hesitations on the way
like you'll start with a pause if he
doesn't have a tension then also excuse
me there's still no attentional wait
some more then maybe we'll launch the
first part of the utterance but not
complete it still trying to grab your
attention eventually my get to launch
the actual utterance and at any point in
time if the attention returns it
launches the phrase right so we have
this coordination policy that induces
these disfluencies to try to match the
attentional supply that that we observe
in the scene and this is done phrase by
phrase so then we move to the next
phrase so we implemented this in the
robot and to give you a sense of sort of
how the behavior looks like from the
participants viewpoint here's a quick
demo video that Eric and I shot yes
could you tell me the room number or
person you're looking for horny to get
to 4,800 I think it's me
excuse me 40 50
you loved it you loved it all the way
I'm looking for 4300
4300 is just down that hallway and will
be the first group on your left by the
way would you mind swiping your badge on
the reader below so I know who I've been
talking with so now here's from the
robot viewpoint some of the sensing and
computation that happens the code blue
greenish line that the nose participant
intention is the inferred model of
whether my attentions on the system
which is high or away from the robot
which is low these are my address is in
blue and this is a whole phrase that
robot is trying to produce that's being
split and robot injects these expenses
on the fly as it observes that my
attention is drifting away
excuse me
go along this go along that hallway
4300 is just down the hallway it will be
the first room on you alright so now
this is a demo video that they
recognized yet right but here's a few
sample natural interaction so the robot
is deployed in front of the elevator
Bank in our building and people randomly
sort of go and interact with it some of
them just to have fun some of them to
get actual directions so here's from a
few sample collected from natural
interactions at the top here you see
what the robot is trying to say with a
decomposition into sub phrases marv I've
these vertical bars in the orange
regions are things that are injected on
the fly you know in the production
incrementally in order to coordinate
with the attention of the speaker of the
participant listener so if you pay
attention to how this correlate where
people's attentions drift off you will
get a sense of that coordination take
the elevator down to the second floor
turn left as you walk out of the
elevator
excuse me 28 no no we'll be on that side
of the building excuse me
good night
can I help you find something
morning do you need directions
excuse me do you need directions don't
forget to 22 all right so let's dive
convince you that we solve this problem
there's lots of failures so you know
these are the good cases but inference
making inferences about the physical
world in open war setting is very
difficult and so there's a lot of
robustness that needs to be created so
I'll show you just one example where the
world kind of gets crazy participants
you know get close get far system
incorrectly infers that they're not
attending and because of that keeps
injecting these interjections even
though they are actually paying
attention excuse me sorry you wanted
3866 correct that's right
excuse me
tn23 866 go to the end of that hallway
go to the end of that hallway keep on
walking to the end of the corridor keep
on walking to the end of the coroner
journal oops well guess I'll see you
later we have a special disengagement
model there where the robot expresses
disappointment when things don't go that
well but hopefully this gives you a
sense I mean there's a lot of work to do
but it gives you a sense of the kind of
coordination and fluidity you might be
able to accomplish once you start
reasoning about these different
dimensions like speech and vision
happens in the scene with people's
attention together this is but one
example in this space and the same kind
of you know benefit in some sense can be
obtained from this close integration in
any of these levels throughout the
entire communicative stack and so what
there's a lot of promise i think in here
in terms of doing multimodal sensing a
multi-model integration over the years
that we try to develop these kind of
integrative systems that build that that
bring all these things together we're
also running into a number of challenges
and I want to switch gears now and talk
a little bit about some of those as I
think some of them generalized to a
broader class of systems they're not
necessarily about language now there's
different kinds of challenges some of
them are for instance technical like how
do i do multimodal integration with
structure data of different times of
different timescales what I want to
focus on today is actually more of a
almost like an more of an engineering or
science of building integrative systems
kind of problems so here's what I mean
by that if you look at some of these
challenges I think part of them have to
do with the fact that these systems are
just purely complex like they have many
boxes many components that need to be
connected together and talk to each
other if you list all the different
kinds of models that are in you know
that robot system that you just saw
there's all sorts of things from sensing
components to inference models to
planning components to embody control
components rendering so in some sense
we're facing a lot of sort of software
engineering challenges in a traditional
sense but I think it goes beyond that
one of the things that I think is
interesting
to do with the programming models for
coordinated computation somehow we
develop each of these boxes kind of
independently maybe like someone
develops the speech recognizer someone
does the vision system or parts of the
vision system but they somehow need to
be coordinated in how they operate and
there are some programming models for
coordinating computation but I think
generally that's an interesting area
also for the future and generally tools
for inspecting and visualizing and
understanding what happens in the
systems I think it gets more interesting
once you start thinking about the need
of these systems to act in real time at
low latency and under uncertainty and so
one question that comes into my mind
here is do we maybe need to think a bit
more about our programming languages do
we need to evolve our programming
languages a bit so two constructs that
I've encountered over and over again in
these systems are time and uncertainty
and I feel like neither of these are
kind of first order citizens in our
programming languages we always have to
go implement on top of whatever
programming languages were in support
for dealing with time and support for
dealing with uncertainty so I'll make an
argument I don't know that that maybe we
want this to be first order citizens
just like I say double F I want to be
able to say stream double F and I want
to do anything I can do to a double to F
but i also want to get automatic
persistence overtime of f i want to be
able to do historical access or sampling
over it or compute some operators and
transforms over that and i want that to
be baked in deep down almost in the
programming language uncertainty also is
the first order citizen i think
everything a lot of things about AI
today are about you know managing
uncertainty and you know representing
and doing inference and belief updates
and I wonder if we as a community as any
community should interface more and talk
more to the programming languages people
and see if there's something to be
explored there another I think point
that's really interesting is thinking
about how we integrate what for a lack
of a better name i don't know i'm still
looking for a better name is what i will
call like human authored and machine
authored components what do i mean by
that if you look in these systems like
if you start doing the diagram for one
of these robots what you see is there's
all these boxes but some of them are
like things that are kind of traditional
software boxes where someone writes
functions over simple inputs and produce
some output send their determination
can their unit testable and pepper
throughout all of that we have various
kinds of machine learning models and
data-driven models that might have large
dimensionality inputs that are not unit
testable in a strict sense there maybe
you might've able to evaluate stochastic
in a sarcastic manner their accuracy but
it's interesting of how do you do the
coupling between the systems and I think
coupling this raises interesting
questions one set of questions I think
has to do with the engineering of this
integrated system so let me let me try
and make this concrete and give you an
example about this idea of what does it
mean to learn in a connected system so a
lot of machine learning algorithms focus
on in one of these boxes can I get
performance up can I make this be thirty
percent better or forty percent better
in accuracy or something like that but
what often happens in the systems is
that these things are coupled and so for
instance in the attentional models that
I just described the attention model
that figures out where is the personal
attention leverage is a lower level
model that tracks that attention
geometrically like is it front left
right up down which in turn leverages
scores from a face confidence model that
in turn leverages features from a face
tracker that I don't even know what
models ran inside of it because it's a
black box and so if someone changes
something down the stream that affects
the entire stream and we don't
understand well how that all plays out
how to validate that how to make any
sort of guarantees when we architect
these systems so again just like I said
for programming languages I'm wondering
if there's kind of new frontiers here
for machine learning at the intersection
of machine learning and software
engineering what does it mean to
engineer these kinds of systems where I
want to be able to plug in various kinds
of inference models that are trained by
different people at different times in
different ways and make them all somehow
sing together another challenge I think
that with respect to learning in these
systems is the interactive setting in
which these systems often live
oftentimes machine learning is done in
batch like we have this data set we keep
getting performance higher and higher
but what happens in the systems is you
will train a model you'll put it in that
will affect the way the systems behave
which affects the world that the system
interacts with and all of a sudden now
your input distribution has shifted so
you're no longer under the right
conditions another dimension that I
is interesting to think about is the
idea of metal reasoning and system-level
self optimization how do these systems
do can is there any way for us to do
some sort of blame analysis or diagnosis
when we have these complex systems I
know at the end of the day something
failed the person ran away and they were
frustrated or something how do I figure
out what in the chain work correctly and
what didn't especially when some of
these components are uncertain or
stochastic in nature right and then
there's interesting questions I think
about self optimization and and
self-monitoring like we did some work on
on reasoning about the system's own
inferential delays in this pipeline
because latency is important for
turn-taking so you want a reason how
long will it take me to run this
pipeline in order to decide which action
you want to take so I think I'll close
it there the last thought I want to
leave you with is that I do think there
are interesting challenges in between
these areas of machine learning or AI
software engineering programming
languages and distributed systems and I
think as a community will be interesting
to reach out to the other communities I
hope we'll hear more in the panel and
just generally more discussion on this
thanks very much I mentioned earlier
that there will be a panel discussion on
integrative AI that will have been right
after the break following this more
plenary oriented session with with with
talks and now we have a few minutes for
Q&amp;amp;A with Dan we'll do it we'll do
questions and answers after each talk
and so any hands up and we have some
runners in the room and over back over
here wait wait wait it's okay these
fellows want to really give you their
microphone yeah I'm the one thing I
noticed in your videos was that
somebody's people are very rude to the
robot and you'd expect him to be and
they wouldn't act like it that way if
they were getting directions from from a
human and so the in a sense the system
is dealing with the artificiality that
that is nearly inherent in that
human-robot interaction so I'm wondering
it sounds like a nearly unsolvable
problem right as long as people are
confronted with a robot they'll treat it
like a robot and be particularly route
in terms of diverting their attention
away etc I almost feel like I should let
every counselor description but I'll say
I'll say do you think student he can
chime in because he's working with me on
some of these problems but so one thing
I will say is when I observed you know
people interacting with robot it's
actually interesting to see the
variability of sort of attitudes and
approaches with which people come to the
robot in some of these videos indeed
maybe people are you know put off or the
route or the curt we've also seen cases
where people are literally differential
they won't leave there because the robot
doesn't has a bug doesn't say bye and
they don't want to leave until the robot
says bye so we've noticed also some of
these and I think what's interesting is
people's attitudes I think might also
change and their their ability to kind
of but they put themselves into into the
into the place where they interact with
this naturally I think depends also on
the failures of the robot there are
sometimes moments of magic created when
everything lines up and everything works
perfectly where people are in this like
oh my god you know and then but the
problem is it often fails like it really
like you know more than fifty percent of
the time there's a problem one sort or
another my sense is as we sort of reduce
the number of errors you know it will
become also more fluid so I don't think
it's hopeless so and i will i will like
then when our eyes met I could tell you
the same example we first had fielded
the attractions robot and we're looking
at videos and the system discloses that
video and people so they understand that
and so on for privacy but it and they
can remove their video by sending an
email and so on but we were looking at
the videos and we noticed as a
professional woman who wasn't from our
building was dressed very much like she
was a visitor and kind of nervous and
had her phone out and she was struggling
to find the room and she gave the room
to the robot it was the wrong room it
which didn't exist when robot was
struggling japonica's oh so sorry I'm so
sorry like looking very like sorrowful
and gave the robot the actual correct
room and the robot gave her the great
answer and she ran off could she was
very rushed she came running back and
said thanks
they know it's just really impressive
how people personify an Ambu humanist
and even you can you see in her mind her
if her mom maybe even just a kid saying
you always thank people may help you
running back to say thank you to the
robot what time by the way I don't make
on this work Dan presented we had a
great time working with Xiao Yue from
CMU on this and in turn we double in
size every year by our intern load
coming in across Microsoft campuses we
really adore and have long-lasting
relationships with the interns from all
the departments of all your departments
so any other questions for Dan um so Dan
I really like the talk thank you well I
think both parts of the talk with great
i wanted to ask a question about the
attention and turn taking mm-hmm so you
have these nice so it's great to see
you're working on kind of how production
connect so how do you kind of
systematically or generate disfluencies
at relevant places and you have nice
examples where disfluencies related to
the coordination of attention but there
can be other sources of disfluencies or
things like you know lexical access
problem in the speaker and so i'm
wondering like do you feel like you have
a grip on how to connect potentially
using data the places where it makes
sense to generate disfluencies based on
you know potentially coordinating
attention or other kinds of factors that
are at play and these interactions the
short answer is no the the longer answer
is i do think i mean i completely agree
with you not all these fluences are
about coordinating attention you know
and it's not clear how many of them even
are i think one thing that's interesting
is you know these observations from
Goodwin was were based on certain you
know probably very small kind of samples
it's interesting to think I feel like we
should do more of observing with today's
technology human human interactions and
trying to get to better understand
exactly this you know these things that
you're describing you know how much
comes from where and what's a model that
kind of unifies these different these
different branches there's also
connections between this I feel that
also we really don't understand
all and we're highlighting when we build
this stuff for instance between how this
and gesture works right like if I how do
i generally if I'm pointing there can I
destroy CB even generate like you know
so there's I think the story gets
composed with extra dimensions and we
haven't explored those but I think yeah
certainly interesting okay one last
question up again yeah so uh you might
be just as ready i didn't i didn't hear
it but does the system have any
understanding of the reasons why the
interlocutors attention might be
elsewhere you know either he or she is
looking for information or talking to
the companion or engage in other
activities say driving for example so it
depends on how you mean reasons so we
have a set of targets defined in the
environment so the system understands
that for instance if someone's looking
there when i'm pointing they're like it
has that level of understanding that
that's an okay thing or that's an
expected target you know but this is all
pretty fine for this domain like it's
not a more general general model of you
know the system also has an
understanding on in multiparty settings
when they're multiple people around of
who's engaged with it and who's engaged
you know we're not leveraging that right
now but definitely that should also be
leveraged like you know do i understand
that hey your attention is on the other
person cuz you're making a joke or
because you know you're actually asking
them which room you need to go to all of
these distinctions we need to be in
right now it's in some sense quite
simplistic it's just the set of targets
that are semantically defined
corresponding to this domain okay thank
you very much damn next Larry come on up
and you worry about LA right before and
just you should just dive right in all
right so um so today what I'm going to
be talking about it so I'm going to
continue the same with an integrative AI
but I'm me taking a bit of a different
angle I'm like me talking about robots
at all or anything like that because I
know you think but I haven't worked much
in my obaachan we're not going to talk
about robots but way I'm going to do is
talk about how we integrate vision
language and common sense and I want to
begin is to discuss what does it mean to
understand and understand is like this
it's a really loaded term I think an AI
and I think there's been a lot of
controversy about what
it mean to truly understand something
and if we have an algorithm and we feel
like it doesn't deeply understand the
problem people always go up now that's
that's that's not true AI that doesn't
really understand what's going on in a
deep level like a human does so let's
just have a couple examples of
understanding so does anybody in this
room believe that a computer can not
understand the concept of red cannot
understand the concept of red say you're
wrong I mean you take it you take a
computer it knows Alex contenidos read
write and then you can stick a camera on
it and it won't learn that when the red
butter is up there is fire that this
means red and that is the same type of
deep understanding that we as humans
have right so we can understand give it
to us give it to us I know you can you
make arguments about red wine or red
faced you know but I'm talking about the
concept of just the color red you know
we understand that I think we can give
it two computers that they understand
the bread we're going to bait it later
maybe in the panel section what about
something like this what it is a sheep
fluffy if you went to yesterday's
sessions you can see that well if you
want to learn common sense knowledge you
go out onto the web we could learn that
the word fluffy co-occurs with the word
cheap a lot right and you can say well
yeah she / fluffy because that's what
I've seen odd but do you understand
really what fluffy means you can go one
step further you can give it an image to
and say this is a cheap and it's fluffy
and you can give this to a computer
vision algorithms say oh I understand
what the vigil incarnation of fluffy is
but then you showed another image like
this it is good say huh look it's like a
tree you know so if it you see the
computer it doesn't have a deep enough
understanding with the true meaning of
fluffiest so it really need is robots
going around a little touch sensors on
them to understand what it means to
actually be fluffy if you really want a
deep understanding of fluffiness but
it's not true that you can just take
your robot put it into the world with
every type of sensor you could imagine
and it will understand deeply what
everything means there's other concepts
i believe which you're not as easily
understood such as angry you know I you
know if you want to have a first-person
interpretation of angry hopefully our
computers no time soon really understand
angry from a first-person interpretation
unless we're going to be an
trouble we don't want angry computers
running around and being mean to us all
right so it's always like different
types of levels of understanding and how
we can understand and the more they
integrate multiple signals the better
maybe we can understand now what I want
to do is I want to do a little case
study and I want to talk about
understanding specifically in the realm
of an image captioning I think is
interesting for two reasons one is that
yesterday you guys heard a lot about
image captioning so I think we're kind
of all on the same page where image
captioning is now the second thing is I
think image captioning really captures
why you know it captures the imagination
because if you can describe it an image
that means that you really i mean we
interpret that as really being able to
stand with that images about we
understand that we understand what the
humans are that have to understand that
interpretation we must understand what
deeply what's in that image you must
understand the scenario that's going
around and it really just you know in
theory if we really did do image
description correctly we would need this
very deep understanding so if we do
image captioning the standard pipeline
is just you have vision extract up some
visual features you have some sort of
representation and then you spit out the
sentence and a good inclination of this
is something that oreo van y'all did
where you have an image you have some
deep learning algorithm like a CNN and
then you throw in an RNN like an lstm
like this and it just spits out a
sentence and if you take an example so
there's a lot of groups that all showed
really great results simultaneously this
is a result from the MSR paper and you
look at this caption a man standing next
to a fire hydrant in front of a brick
building and if you look at this caption
and you look at that image it's amazing
I mean you understood that this is a man
he's standing he's next to a fire
hydrant he is in front of a brick
building I from a computer vision
perspective where you're going out
you're a little expectation is somewhere
near your feet and it does that well you
just blown away and I think there were
graduate students jumping up and down
across the u.s. when you're first
running their algorithms you know on is
this captioning data and they're
actually producing something reasonable
so why is this why were they doing so
well with did they have a deep
understanding well there was a little
bit of a red flag I think in some of
these results which kind of went
unnoticed initially which is that
depending on the algorithm
thirty-five to eighty-five percent of
regenerated generated captions reward
for word the same as a caption in the
training data what does this mean you
that the exactly the captain's exactly
the same as that you negated does it
mean that we're really understanding
these captions if all we're doing is
borrowing captions from the training
data and slapping them onto the images
in the test data so in order to test
this theory what we did is we said let's
just train a nearest neighbor classifier
so we have a test image it comes in we
find a bunch of training images which
look very similar to it in some visual
space as trained to be you know match
images which are semantically similar we
then take those images and we take all
the training captions that come along
with them we find which of those
captions is closest in blue score cider
score which is most similar to the other
sentences in that list and we basically
pick that sentence out and that's our
caption right the very simple baseline
all you need to do is match images to
images how well does this do well here's
some qualitative examples we have a
black-and-white cat sitting in a
bathroom sink it's pretty impressive
it's even more impressive that there's
an image in the training example which
looks exactly like that and then you
have to you know zebras in a giraffe in
a field not perfect but not bad and if
you look at automatic evaluation metrics
and you look at where nearest neighbor
is it's near the top its middle the top
depending on which evaluation metric are
using and this is better than many of
these you know deep learning algorithms
and it makes you question how much of
these are n ends are doing and how much
more work way to do in the future so why
does nearest neighbor work this is I
think counterintuitive to a lot of our
beliefs there's it you think about the
space of images its massive it's huge
how could nearest neighbor possibly work
in generating captions well in order to
understand if you didn't understand a
data set up a little bit so we have the
all these results of basically on the ms
coco data set which has which was
originally built for segmentation and
detection so to any different object
categories has 160 thousand images a lot
of segmentations and we talked about
this yesterday
but this data set is a part of a large
collaboration amongst many many people a
lot of the credit goes to some EU Lynn
who is the student behind this he's
really done just a huge amount of work
on it Peter dollar I'm gonna our
collaborator on this he's done a lot of
work as well so let's go back and let's
look at one of these fantastic captions
a man riding a wave on a surfboard in
the water right just nailed it perfect
awesome like it's the water is riding is
on the wave you know the level
understandings got to be there but now
let's look at the training data and
let's look at the images which lie close
to that image you know that caption
works pretty low for pretty much all of
those as well now let's look at all of
the training images in ms coco this is a
video of every single image in the
training data set ordered by semantic
similarity and what you'll notice is
that for many many images there's a lot
of other images which look very very
similar so i can make up to go along its
piece on along a road a watchtower a
watchtower with a brick building a white
bathroom with a white toilet and white
sink a living room we should is messy
two laptops on top of a table you know
etc right so you can see how this could
work now if you were skeptical and you
know as computer science researchers we
generally are you could say well this is
just a 80 fact of the data set you can
just collect this data set wrong you
have too many images which are similar
we specifically set out and creating
this data set to have diverse imagez to
have images which did have a lot of
contextual information which did have a
lot of objects in them which did work
which weren't simple and easy images but
it turns out that this property of the
Microsoft Coco data set is not just a
property of the Microsoft Coco data set
it's actually a property of our real
world so there's anybody seen this video
online anybody in computer of it is this
this ism you should go watch the full
video it's really awesome so this is but
they have a bunch of videos and
basically make up words to describe
different concepts in this concept the
word is at VMware dahlin you should
search for that and in the concept is
the first
tration of photographing something
amazing when thousands of identical
photos already exist right you think you
were cool when you took that photo
you're not somebody else is already
taken that photo you're not as cool as
you think you are we as humans we think
too much alike and nothing shines more
of a light on that then when you go to
Flickr and you look at the images and
you're like yeah we are all very similar
in the video that I showed you earlier
of the cocoa data set at the exact same
video with random Flickr images and the
same property holds you look at it you
look forward to both on youtube so you
can go check it out on youtube all right
so if you're thinking about it from a
product standpoint let's say your
Microsoft of the world or Google or
Facebook of the world this is fine you
get 2 billion images uploaded a day we
could actually caption them all using
these kind of newspapers approach if
they work they work who cares right but
as AI researchers we're kind of like a I
actually do want to deeply understand
things I don't want it I don't know
cheat cuz you know I don't my my fellow
researchers to say Larry all you do is
cheat and you you know and even
publishing nearest ain't pure stuff we
put it out on archive but we wouldn't
expect it to actually be accepted into a
real conference but yesterday we did
since we see some works which actually
tried to go into a deeper understanding
we had the MSR paper and then also rich
zemel has a really nice paper where they
not only just look at the entire image
but they also try to look at sub regions
of the image and see where is the teddy
bear where is the person you know where
this these specific objects are what I
want to talk about is another piece of
work that i did with shen li chen who's
a student from CMU of abba knobs the
group does he's just a fantastic student
and a project that we called mind's eye
now if you think if you hear a written
description or you read a written
description you don't just read it you
don't just understand the words you
actually tried to picture what's going
on when you read that description as
your understanding it and this this
discrete this visualization of the
description in your head it's actually
critical to understand the words as
you're reading them so for instance you
could read a sentence like a girl and a
boy picture a girl boy not might picture
them knocking on the door and they get
down the tower and then they
knock down at our right so we all have
might have different visualizations but
you know we all visualize something now
when you think of this what this does is
it takes not just this one way
interpretation but you have to go both
ways back and forth from vision
representation language and then back
the way we did this if we took your kind
of generic r NN model which you have
visual features coming in you have a
hidden state which is the S the white
area and then the green which is like
the output of the words we stuck another
layer on top of this where we took the
words and from the words tried to
reconstruct the visual features again so
one way to think of this is is this as
an auto encoder and which is sentences
in between so you try to reconstruct the
visual features using the sentence and
what's cool about this is you can
actually kind of tap in to the hidden
states and see what the network is
thinking as it's reading a sentence so
if you look at this example mike is
holding a baseball bat and as soon as
you say baseball the baseball glove
lights up the bat lights up the ball
lights up the baseball cap lights up so
you can see as you say baseball all this
things light up just as any human it
would light up because you'd picture an
entire scene in your head here's another
example when you say hot the hot air
balloon lights up hot dog lights up and
then when you say hot dog hot dog lights
up the hot air balloon turns off dog
never lights up and when you say hot dog
also the hamburger lights up because a
lot of times hamburgers and hotdogs
co-occur the grill the table etc light
up as well so now I want to talk about
limitations a little bit so here we have
a caption Aereo an image a crazy zebra
climbing a giraffe to get a better view
now if you think about and this is a
great caption right and you think about
how if you could generate this caption
to be amazing but what would you need to
do to do it right so if you let's say
you want to figure out that you want a
climbing zebra detector you can have a
really hard time from first off there
exist two images on the web of zebras
climbing this is one of them and if
you've been paying attention to the deep
learning that's not enough training data
and and if you do language modeling you
think about it if you say zebra climbing
the language while is going to be
screaming at you do not say this do not
say this do not say this I've read the
entire web and nobody's ever ever said
zebra climbing you should not say this
and it won't let you say it
right so we have these problems is this
the limits to the vision and a language
models that we have today here's another
example a man is rescued from his truck
that is hanging dangerously from a
bridge this image is awesome you look at
this image it's a single moment and but
if I asked each and every one of you to
describe 23 our sequence which
surrounded this image you would probably
all have a very similar story the guy
drove across the bridge you crashed you
ran off he was fell off the bridge he
was really scared so people ran up to
him they were at lower the rope they you
know pulled him back up everybody
cheered and you know it was a happy day
after all for everyone right but do you
even have that of a deep of a level of
understanding you need to know so much
about the world you need to have a lot
of common sense knowledge to understand
that that truck could have fallen in any
moment and killed the guy that's why
it's dangerous that because it was
dangerous and because he actually is
being hauled up right now he's being
rescued a word like rescued is really
hard to detect and really hard to
understand and here's another example he
probably most of you have seen this
photo since this is some this photo is
just by itself is is just it's it's a
really it really captures you you know
it's it's really impactful photo just by
itself but if you understand the story
behind this photo and what it represents
it takes on a whole new meaning if you
understood that this is representing the
poverty during the Great Depression and
this is really indicative of basically
the whole era in the United States
history this photo has a much bigger
meaning but you cannot understand that
you cannot grasp that without this
additional knowledge it's not common
sense knowledge but its knowledge that
we have so we really shouldn't be
thinking of this problem as vision
representation language we really need
to be thinking of this problem as vision
language common sense knowledge
representation there's probably even
more I'm a vision guy so people might
want to do touch your you know smell
hearing all the other things but you
know you can add them in but it needs to
be holistic you need to be considering
all of these elements but this is I
think where we commonly get stuck
because once you start thinking we need
and since we need vision we need
language this is so challenging because
we are you know historically many of us
you know have been kind of silent we've
you have been computer vision
researchers or NLP researchers a machine
learning or you know whatever
researchers and it's like how we even
begin working on this how do we begin
taking a more holistic approach and what
I want to do is just talk about two
projects which I think begin to kind of
pull us in that direction so the first
thing I want to talk about is abstract
scenes so this is just a whole body of
work it's done most of it with Debbie
parikh a longtime collaborator of mine
her students a bunch of students from
CMU and the premise of the work is the
follows if we really want to understand
you know common sense knowledge about
the world semantics do we really need to
understand things at the pixel level you
know you think about semantics you think
about words you think about common sense
knowledge you don't need to understand
it at the pixel level you can just
understand it at a more abstract level
you could understand it you know
basically from cartoons how many of you
understand that people get eaten by
bears because you've seen somebody get
eaten by a bear very few of you will
hopefully very few of you right however
maybe you've seen a cartoon of it or
maybe you visualize it in your head or
maybe you know you've seen other things
being eaten by bears like salmon and
extrapolated from that who knows but if
you take cartoons and let's say you have
a very diverse set of cartoons where you
have whole sorts of different people
different ages different poses you have
all sorts of different things that they
can play with and all sorts of animals
that they can interact with and then you
go to mechanical turk mechanical turks
are awesome my favorite people in the
world they come out and you say
drag-and-drop interface and they
basically create scenes like these and
you just look at diversity some of them
are funny some of them are boring but
they really just generally just depict
everyday world scenes and from these you
can induce a lot of knowledge about our
world and there's a lot of really cool
things you can do with abstract scenes
you can't do with real images for
instance you can have a mechanical
tricker create a scene like this one
here you have another mechanical
creature come in and described the scene
and then from that description you give
back Mechanical Turk and you say
generate another scene which depicts
this description so you can again in it
description like Mike fights off a bear
by giving them a hot dog while Jenny
runs away and then you get six different
mechanical triggers do you create
depictions of that scene and you notice
a diversity yet how the similar they are
you get to learn what it means to give
away usually you're not happy about
giving away something you learn what it
means to get you know run away you learn
what you know bear is in all these 20
different things from this diversity so
you can learn things that are kind of
subtle that are harder to learn
otherwise like what's the difference
between running after something and
running to something probably most of
you haven't really thought about you
just kind of take it for granted but you
know running after something means that
you're running the other person is
running as well when you run choose
somebody they're probably just standing
there and then there's other things that
we don't necessarily think of as visual
but have visual interpretations like
want and watch it turns out that if you
want something you're basically looking
at it and if you watch something you're
also looking at it so visually want and
watch are actually very similar even
laughing at is very similar because
usually when you're laughing at
something you are also watching it
alright so the final thing I want to
talk about is visual question answering
so what I just talked about was one way
of gathering common sense knowledge or
one way of gathering knowledge about the
world but one thing if you really want
to drive the community towards more
holistic problems towards integrative AI
we need to have challenges which are
integrative which do combine multiple
modalities which do require a very
diverse set of knowledge and I think
this is what has me excited about visual
question answering and this work again
is done with debbie pura Cubana and many
other students at virginia tech it's a
visual question answering what you do is
you have an image you have some
questions and the goal is to answer the
questions the questions are open-ended
we which mechanical turk and said stump
a smart robot give us a question that
can stump a smart robot and then we went
back in this it collected answers it's
open-ended answers open-ended questions
so it's very diverse so and then the
interesting thing is it doesn't just
require knowledge about the images or
the language you also need to
acknowledge about common sense
information like does this person have
twenty-twenty vision what is having 2020
vision mean it means you have to have
glasses you don't need glasses right or
is this a vegetarian pizza or the one on
the floor which which I like was we had
we labeled abstract scenes as well which
is is this person expecting company
that's an interesting question is this
person expecting company well there's
two wine we melt if you can see it but
there's two wine glasses on the picnic
lot so obviously it's expecting company
or he likes drinking a lot it's hard to
tell alright and if you look at the
diversity of questions that people ask
it's amazing so this is the an
illustration of the first three or four
words of all the questions in our entire
data set so if you read it from inside
out so you can say like is this a how
many people are does the what color is
what is the man and there's a huge
diversity in the type of questions and
the typical answers each of these have
is diverse as well you know can have a
lot of yes/no questions you can have
counting questions you can have color
questions you can have what sport is
this you can have arbitrarily complex
questions surprisingly yes no question
is have a tendency to be harder than the
other ones and this data set if we're
probably going to release our first beta
we have a kind of alpha kind of taste or
release out there right now if you want
to check it out we're going to be
releasing a beta release which will have
120,000 images with 360 thousand
questions and 3.6 million answers it's
going to be a huge data set and that's
going to be released in the next couple
weeks and then we can have a full
release coming out this fall with all of
the which is going to be on all the
Microsoft Coco images plus another set
of random Flickr images for greater
diversity plus fifty thousand abstract
scenes so it should be a really large
hopefully really interesting data set
and it really allows us to get at vision
language common sense together it's in
it it's something that can be easily
evaluated which doesn't have some of the
problems that we had with the image
captioning task we don't even know how
to evaluate the task so we're really
excited about this so in conclusion I
think it's really you know we start
thinking about this integrative a I you
know it's a challenge as researchers not
to only think of how do we create
systems which can do integrative AI but
how do we pose challenges which do all
of this how do you create data sets from
which you can learn from the
it's like though if you look at it
holistically it's an amazing rich and
amazingly interesting space Thanks okay
any questions for Larry maybe I'll start
with one you ask a large population of
individual to think about stomping a
smart robot and depending on who people
are and their backgrounds or even my
stochastics have different sense for
what that means how would the is are you
thinking of characterizing or clustering
those kinds of QA challenges by notions
of difficulty with regards to some
standard or some information theoretic
measure so what we've done is we've done
some clustering on the question typing
in cluster based on the first few words
to the question and that does actually
kind of cluster similar ones together
now one interesting question is how do
you know whether common sense knowledge
is required to answer the question you
know you say how many chairs are there
in the room that doesn't really require
common sense knowledge right where the
you know the 2020 vision one does now
what is what I have we haven't been able
to quite figure out yet is how do you
tease apart those questions how do you
measure how much common sense or how
much knowledge is needed to answer these
and that way we could actually split
them up by difficulty anybody has AI any
ideas let us know and I think it'd be
great to do a mechanical turk task could
do that but we haven't quite figured out
the right way yet okay here again Hi
Larry this really great talk wonderful
stuff I was wondering if you thought
about using abstract images to try to
build up models of the spatial semantics
of prepositions I mean there's a lot of
work on cross-linguistic stuff and
learning them but being able to actually
have participants generate their own
scenarios which none of the existing
methods do right now might actually be
much more revealing so what we've done
so we've done various explorations in
this and what we've explored is what we
call relations so that actually includes
verbs prepositions like next to running
after we kind of group them all together
you know NLP people might yell at us but
you know basically it's a relationship
of one object to the other and we've
done we've done a some trying to detect
what relationship exists we also have
another paper where
we can get sentences that's describing a
scene and we try to render a scene
depicting the sentences and for that you
need to understand these relationships
so we actually the models that we that
model these relationships is still a
fairly naive they're just Gaussian
mixture models of the spatial locations
of these objects as well as their
attributes like is a person running
which direction are they facing you know
because gaze which directionally looking
is very important for many of these
things so we've done that now where I'd
like to go is more into the temporal
we've done initial studies in tempura
we've you know had asked her curves
actually create sequences of scenes but
we're just I just scratching the surface
you know and then and if you just ask
turquoise to do this what is incredibly
rich space but yeah we really have just
scratched the surface of it I think it's
a lot more you could do ok yeah this is
just a query from an academic interest
point of view do we have a formal
definition of what is common sense even
from a visual perception or vision point
of view and how much of that overlaps
knowledge it's a good question because
what is common common sense is common
knowledge you know and i think i think
was Lucy who showed yesterday which said
you know common knowledge knowledge that
is common sense is knowledge that I I
assume that you have and that you assume
I have right but that can change you
know culturally common sense knowledge
of berries right an age that various as
well in order to communicate we need to
understand what our common knowledge
base is right and you guys even as
researchers we are common sense
knowledge relative to the two of us is
also very so I think the term common
sense is not something that's global
it's actually you have to define your
population and then given your
population then you can then define what
the common sense knowledge is right and
is it this big gray area between that
and you know just generic knowledge
sometimes we say that common sense is
this incredibly a copious massive amount
of information about the world that's
not in the dictionary anywhere just
assumed Candice Candice I had a question
over here you see her
but we did for the TV audience though
they need to hear you so I noticed that
among your missing words for questions
was why does it occur in your data set
very much it's the hardest of the why of
the questions to deal with why you know
we did bias the data set towards
questions that could be answered with
just a few words to make the evaluation
easier and when you get to why why is
usually a long answer and the most
annoying one for three-year-olds and
dealing with them no I agree with you
there's not as much why so that that is
a good question and since it is a longer
to know since the answers are generally
more longer maybe that's a good area for
future research and I think for
mechanical turk risk we could even prime
them to say ask question that's a why
question imagine your three-year-old you
want to ask a question about this image
and you can do that that's a good that's
a good idea i like that so so beyond tau
will probably all here too because of
why and will end with that thanks Larry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>