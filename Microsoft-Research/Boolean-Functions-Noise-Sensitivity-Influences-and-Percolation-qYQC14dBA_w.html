<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Boolean Functions, Noise Sensitivity, Influences and Percolation | Coder Coacher - Coaching Coders</title><meta content="Boolean Functions, Noise Sensitivity, Influences and Percolation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Boolean Functions, Noise Sensitivity, Influences and Percolation</b></h2><h5 class="post__date">2016-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qYQC14dBA_w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a great pleasure to introduce Jeff
steiff and i would like to start by
saying that he's the bing-bong lecturer
this year so every year we have a one
distinguished mathematician probable
list he's been bomb lecturer let me say
a few words about William Birnbaum some
of you know who he was but some of you
may be attending this conference for the
first time William being bound left
almost the whole 20th century from 1903
to 2000 it gives me a special pleasure
to say a few words about him because he
was born in Poland and he got his PhD in
both which was in Poland before the war
and that's the place where much of
pre-war published mathematics was done
some of the best-known names associated
with with love webinar olam cuts and
beam bomb well I will not go into
detailed history but in 1939 he came to
the University of Washington and he
stayed at the University of Washington
until the end of his life of course at
the end he was retired he is probably
best known as a statistician he was one
of the editors of the annals of
mathematical statistics this was the
journal that preceded separate channels
of annals of statistics and also
probability he was also the president of
the IMS but he also made a very
important contribution to mathematics
nowadays many people know about or llege
spaces but some people started calling
calling them bin bound all these spaces
because bin bomb and/or llege
co-authored the the first original paper
and always kept working on these spaces
and be bound moved to different problems
and he made them mostly
his contributions to statistics anyhow
so it's a great pleasure to have a
distinguished speaker today and we'll
talk about boolean functions noise
sensitivity influences and percolation
thanks a lot Chris so first I want to
thank the organizers for inviting me to
give this talk I'm visiting here
Microsoft for a year and having great
pleasure and joy myself talking the
theory group and all the visitors and
it's a great opportunity I appreciate
being here so I'm going to talk about
the following topics it's going to be
very much of an overview lecture so feel
free to interrupt me anytime you want
with comments or questions so these are
the four concepts I'll be discussing
boolean functions noise sensitivity
influences and how they arise in a
particular model called percolation and
I think I saw said all his pictures we
do to someone else so these pictures
also borrowed from elsewhere okay so
there's some lecture notes that
Christoph garbhán and I wrote called
noise sensitivity of boolean functions
in percolation and they're already
published but we're in the process of
putting them extending them in writing a
book so if anyone were to look at the
lecture notes and have any comments we'd
be a very we'd welcome any types of
comments for their coming book and
today's lecture will be a brief survey
of some of some of the topics covered in
these lecture notes okay so the first
thing we're going to talk about or
bullying functions and noise sensitivity
and the basic setup for noise
sensitivity is quite Elementary we have
n random variables x1 through xn there
iid plus or minus 1 each with
probability half so we just have n coin
flips and we denote the vector a vector
x x and then we have a function f from
minus 11 to the end this is sequences of
length and of
and minus ones the sequence length ends
of plus and minus ones into plus or
minus one and this is called the boolean
function it just has two possible
outputs and so in some sense this
function will be turn it will the
function will be a function of these and
inputs X 1 through X n so this is what a
boolean function is and the next notion
we need to do to introduce noise
sensitivity is we have to talk about a
small perturbation of X so X is denoted
there and X epsilon is here and this is
denote this is again a vector of length
n X 1 epsilon through xn epsilon which
we think of as a small perturbation of X
and the way it's going to be perturbed
and is a very simple way you go to each
of these n bits and independently with
probability epsilon i take that bit and
i throw it away and i replace it by a
new plus or minus one each with
probability a half completely
independently so for each of these is
erased with probability a very small
probability epsilon and replaced by a
new coin flip so that's or perturbation
and then the basic question is if we
look at and of course note that X
epsilon this but this perturbed thing is
also of course because it's everything
is independent in symmetric this is
again a sequence of n coin flips they're
all it's still I idea half a half and
the basic question of noise sensitivity
is if we look at f of X in f of X
epsilon of F of the small perturbation
are these things going to be close to
independent or are they going to be
highly correlated now obviously if
epsilon is extremely small then X
epsilon and X will be the same with very
high probability you wouldn't have
perturbed it and these f of X and f of X
epsilon will be more or less the same
value almost with probably close to one
and so we have it highly correlated so
what we should think instead is epsilon
is very small but fixed and this
function f will be a function of very
many variables and some complicated
function of these and variables
and then it's not quite clear if things
will be independent or not and this is
the first definition this is a
definition by benjamin ike alliance rum
who introduced this concept we say a
sequence of boolean functions f n which
will be mapping as above sequences of
length n to plus or minus 1 we call it
noise-sensitive if if you fix any if you
take any fixed epsilon bigger than 0 and
we look at the correlation of the
covariance between f n of x and f n of X
epsilon this is the covariance that this
goes to zero now what this means FN is
just taking two values so being
uncorrelated is the same as being
independent so this simply says that if
we look at f of X in f of X epsilon
these are asymptotically independent
they become independent as n goes to
infinity if you fix if epsilon is fixed
ok so does that interrupt me if there
any comments or questions ok so let's
take three quick examples that are easy
the first one is the simplest boolean
function in the world is called The
Dictator function FN of x 1 through xn
is simply X 1 it's just the first bit so
this of course doesn't really depend
upon n and it's not so hard to see if
you do a small perturbation of this most
likely things are not going to change
and this will not this will not be noise
sensitive because the function doesn't
really even depend upon n an example of
a noise sensitive function is the power
D function these bits are plus or minus
1 so i can multiply them out and if i
take the product of them we call this
the parity function this one is another
boolean function it turns out it is
noise sensitive because basically if
epsilon is fixed in n is enormously big
with very high probability you'll be
you'll be resampling one of the bits
with very large probability and once
it's resampled is just as likely to be
plus or minus 1 because is likely to be
plus or minus 1 and so the things become
completely uncorrelated and
and independent an example where things
are maybe not so clear at first is
something which is called the majority
function so in this function and the
number of variables has to be odd and
what the function simply does it looks
at the end bits and it looks if there's
a majority of one or majority of minus
ones if it's a majority of one's the
function outputs one if it's a majority
of minus once it outputs minus one one
way to do that is you can just sum up
the bits there plus or minus one and you
take the sine of the sum so this gives
you the majority function and question
is whether this is noise sensitive or
not is not as perhaps obvious as the
other ones but it turns out it's this
one is not noise sensitive so if you fix
your epsilon very small imagine it on an
election with Democrats and Republicans
and with everything being iid a half a
half and you take and you see that the
democrats won and then you change a very
small percentage of the people's votes
the Democrats will still be the winners
so this is not not noise sensitive ok so
our main example of interest they'll be
of interest to the talk is is
percolation theory so in percolation
theory what we do is we have a is an r
by r square a piece of the hexagonal
lattice exactly like this and each of
those hexagons is painted black or white
each with probability a half
independently and I think of those is my
input big bits there my now I have about
art since it's r by r i have about r
squared of these plus or minus 1 think
of black is one and white is minus 1 and
now if i can define a fairly a boolean
function which describes the percolation
picture what i'm going to do is I'm
going to ask if there's a crossing from
the left side to the right consisting
only of black hexagons so in this
particular realization there is a black
path from the left to the right side
going as I just showed you this the
probability of this is about a half and
so we define a boolean function one if
there's a
to write black crossing and minus one if
if it's not so this function simply
tells you if this is right crossing and
you can ask if this is noise sensitive
or percolation crossings noise sensitive
so once so the way you think about this
is on the left side we have our original
percolation configuration Omega and
let's tell you let's assume that there
is a left 21 I've made black turn into
red let's assume there is a
left-to-right crossing and now i'm going
to apply this so called epsilon noise so
each of these hex very only epsilon
portion of these hexagons are going to
be reflect to determine their value and
the question is is there still a
crossing from left to right given the
fact that it's a crossing before I did
this noise is that how much information
does that tell you that they'll be a
crossing afterwards noise sensitivity
means basically there's no information
being transferred and so that's what the
question of noise sensitivity turns out
to be in here and the theorem concerning
noise sensitivity was proven by
benjamini hawaiians rom that the
percolation crossings are noise
sensitive so that if you have a crossing
initially you do this very small
perturbation you get no information
whatsoever well they'll be a crossing
after the perturbation so we call this
noise sensitive it's sensitive to to a
small amount of noise so now this is if
epsilon is completely fixed the theorem
to the the statement is for every fixed
epsilon these become asymptotically
uncorrelated now it's clear is epsilon
gets smaller the perturbation is closer
to the original configuration and so
it's going to be harder for things to
become independent but still we can ask
what happen if we let this epsilon not
be fixed but go to 0 with our at some
rate now of course if epsilon R decrease
this is 0-2 quickly you'll never get the
independence of the the crossing before
and after if epsilon is too small the
perturbation won't change anything but
one can still ask how if a 10 go to 0
and what kind of rate and in the initial
initial pay
or where they prove noise sensitivity
they proved something stronger they said
yet we can even let the epsilon or go to
zero as long as it doesn't go too
quickly it has to be bigger than or
equal to some specified sufficiently
large constant divided by log R so as
long as it doesn't go quicker than 1
over log are essentially you still get
this asymptotic independence of the
percolation from before and after and
the next question one can ask is what
happens if the amount of noise so this
just says you can go logarithmic lee but
one can ask 10 you send epsilon 2 are to
go 20 even quicker like an inverse power
so what happens if the amount of noise
epsilonr decreases to zero as a power or
mean an inverse power of our like 1 over
R to the one-half could could you still
have these things being asymptotically
uncorrelated okay so that we'll get that
this will be the motivating motivating
question which we'll come back to in a
few few minutes so now i'm going to
introduce a couple other concepts that
arise in boolean functions and arises in
this area that turns out to be very key
to the study of all these things and
this key player is two things which is
called pivot ality and influences so now
we go back to the general context of a
boolean function and I want to talk
about the nut the idea that a particular
bit I so I is going to be one of her a
bit the fifth bit of the tenth bit I
want to talk about the ice bit being
pivotal and so for boolean function then
a boolean function f the event that I is
pivotal this is defined as follows it's
the event and this it turns out to be an
event which is measurable with respect
to the other variables it's the event
that if you were to change the ice bit
the outcome of the function would change
so in other words take a realization and
look at what if F is a 1 or minus 1 now
go to the ice pit and change it maybe f
changed maybe f didn't change if F
change
then I say I was pivotal basically to
its pivotal for the event whether F is
going to be plus or minus one so that's
the notion of pivotal in the notion of
influence the influence of the ice pit
which we denote I sub I of F the big I
is for influence the little I is that
we're talking about the ice bit and FS
or boolean function the influence of the
eye I fit is exactly the probability
that I is is pivotal the problem the
probability that this occurs at this
particular I fit is is turned out to be
very important in this realization that
by changing it you change the outcome f
so let me go back to our three boolean
functions dictator the parity function
the majority function and just check
what that is there just so that just so
that the concepts clear so for the
simplest one for the dictator function f
FN of x 1 through xn is just the first
bit obviously the first bit has
influenced one because it's always
pivotal if you change the first bit the
outcome always changes and the other
bits of course have influence 0 because
those functions of quartz don't even
those variables don't even come into F
for the parity function that's also very
simple all the variables have influenced
one because if you take any every any
bit is always pivotal if I change any
bit the the function change gets
multiplied we'll just we'll just change
because you're taking a product of plus
and minus ones so trivially all the
influences are one for the majority
function it's not a slightly more
interesting a lot not so hard to see
that the influences of course the
influence of all the variables are the
same because all the various variables
are playing the same role and the
influences are one over N to the one
half approximately and this is basically
because if you look at the first bit
what does it mean for the first fit to
be pivotal when is it going to be the
case that the first bit by becoming can
change the outcome the first bit can
only change the outcome if there's a tie
among all of the other bits and if you
have n minus other bits and you want to
tie between plus and minus ones this is
basically like a random walk being back
at the origin at time n in this the
kays like a constant over n to the one
half power so in this case all the
influences are one over N to the one
half let me mention although it's not so
related the noise sensitivity it's sort
of an interesting theorem if you find
the notion of influence interesting so I
want to mention an interesting theorem
in the area and the theorem is an answer
to the following question how small can
all the influences be now of course if
you take F to be the constant function
all the influences are zero so this is
uninteresting so we have to we have to
avoid the generate thing so let's stick
to functions F with the probability that
F is one is say somewhere between a
quarter and three quarters so it's non
degenerate and if we stick to such f you
can ask how small can you make the
largest influence is there any guarantee
that that there'll be some variable of
reasonably large influence and and if so
how large so if we looked at the
majority functions this the majority
function so show that these Maxum
influence can be as small as 1 over N to
the one half and the question is could
it get even smaller than 1 over N to the
one half and the answer is it turns out
it can be a lot smaller than 1 over N to
the one half power so here is a
particular boolean function so if you
take the n variables X 1 through n xn
and we're going to partition it into
disjoint blocks first block second block
third block etc and the length of each
of these blocks is going to be a sub n
which is going to be log base two of n
minus log base 2 log base two of n if
you if you define the bullying that so
we partition it that way and then we say
that the function f is one if at least
one of these blocks is all one so if you
can find one of these blocks that are
all one the function is one if there's
no such block the function is minus one
ok this is a boolean function it turns
out it's non degenerate the probability
that F is is one is somewhere between
well but a a quarter and it's between a
quarter and three
it's non degenerate so it's easy to
check that the functions are non
degenerate and it's also easy to check
that these influence or have become our
log n over N so these influences are
much smaller than 1 over N to the one
half and the majority function its log n
over n which is much smaller and it
turns out there's a theorem by conchal I
and lineal that says this is the best
possible so if you take any non
degenerate boolean function there's you
always can find at least one of the
variables whose influences at least a
constant log and over n so the answer to
the question how small can the
influences be they can get a small log n
over N but they can't get any smaller
what's that I'm such even the human
constant I don't know no Jeffrey says no
we don't know we don't know governor of
202 what's that there's a gap of
interval all too well no it's going to
be doing this in the optimal concert a
multiple you've got a load to people the
best human ah ok ok but the best example
is something like right the best example
is gorgeous
okay so this so we originally talked
about noise sensitivity and then we
moved into influences so what is the
real let so I should say that so this
this argument uses it uses for analysis
and hyper contractility of certain opera
aerators a so-called benami Beckner
gross inequality but I'm not going to be
going into this is more of an overview
but these are the types of things that
come in so why are the influences
relevant to noise sensitivity if you're
only interested in my Sosa tivity why do
you care about the influences and the
one of the fundamental theorems in the
area was originally done in the Benjamin
tickle I strong paper which says that if
you take any sequence of boolean
functions and you look at the sum of the
squared influences you square the
inclusive sum them up if this goes to 0
then the sequence is noise sensitive so
in this sense the influences give you
the influences give you information on
whether your noise sensitive so this
condition is certainly not sufficient is
certainly not necessary because if you
take the parity function the parity
function is trivially it's very noice n
sitive but it certainly doesn't satisfy
that because all of the influences are
one and so that some is in fact n which
is going to infinity so it's not
necessary but it turns out to be
necessary for a very large class of
functions the kind of the condition is
necessary for monotone functions this
means increasing so boolean functions
monotone if whenever you change some of
the bits from minus 1 to 1 the output of
the function can only increase and it
turns out for these this this condition
is necessary and so this gives you a
sufficient and necessary and sufficient
condition for noise sensitivity if you
take the majority functions I told you
that this was not noise sensitive what
happens when you plug this into the
formula so the influences who is 1 over
square root of n you square it you get 1
over n you sumbit you get one so it
becomes that some is of order one for
the majority so it just barely it just
barely misses satisfying this condition
and it turns out that the majority
function is in
sequence in many respects in for boolean
functions this serums also proved using
for analysis hyper contractivity that an
inequality or generalization of
inequality due to tell hologram okay so
this is if we take this general theorem
okay so now we want to get back to
percolation crossings so I told you
percolation crossings or noise sensitive
so how do we get the noise sensitivity
of percolation crossing from this so I
told you percolation crossings are noise
sensitive and although it was not done
in the in this way in the original paper
I want to explain to you basically why
how it would follow from this theorem so
assuming the serum how do we get the
noise sensitivity we have to somehow
compute the influences for the
particular case of percolation so so at
this point we now have the event of you
have a square in r by r square the event
is whether there's a left-to-right black
crossing and i want to have some way of
computing with the influences and this
and the answer to this is going to bring
us into what are called critical
critical exponents in percolation so let
me describe what these are so first of
all I only describe percolation on an
hour-by-hour box but normally do we we
do actually do percolation on an
infinite lattice so imagine you do
percolation on an infinite hexagonal
lattice now we've always been taking p
as a half the probability of a black but
you could take the probability of a
black to be anything so imagine you take
the property of a black black to BP and
white one minus P independently and I
want to ask is there an infinite black
component so I do this on entire
infinite hexagonal lattice is there an
infinite black component and the answer
is it depends upon p and there's a
critical value and it was essentially
shown in 1960 by harris that when p is a
half there's no infinite black component
and it was proven 20 years later by
testin that if p is slightly bigger than
a half if he's bigger than a half then
in fact you suddenly get this infinite
black component and so we say the
critical value
for percolation is a half and at this
critical value of a half there's no
infinite component okay now we look at
this picture we look at our infinite
lattice p is a half and I want to look
at the event that there's a open path a
black path from the origin to distance
our way since the probability of having
an infinite black path is 0 we know that
this probability goes to 0 and the
question is how fast does it go and how
fast this was go and buy lawless ramen
Werner 2002 says the probability of this
event decays like L alpha one of our is
just the definition of this it decays
like our to the minus 5 48 essentially
plus little o of 1 don't worry about
that so decays like 1 over R to the 548
and we call 540 asa critical exponent so
let me just mention that this there's a
critical value which is P as a half is
the so-called critical value that
determines if there's an infinite
cluster and there's this critical
exponent 548 there's a big difference
between them so people like to say that
the 540 ace is universal and they like
to say the P is a half is not universal
what this means is if you this this a
half it happened to be the right thing
for this model but if you change the
model a little bit and did something
else you you could get a different
critical value for P however if you took
a different model and looked at the
critical percolation in this if you
looked at the the percolation picture at
the new critical value it looked at this
event again it's it's believed that this
should still DK like 1 over R to the 548
so five four days should be the number
that comes up for all these models but
the one half was very special to this
and that's why they call that universal
okay ah now there's another critical
exponent that's going to be relevant for
the influences and this is called the
forearm exponent so now I want to look
at the following event that that from
the origin there's two black paths going
out to white pads going out and
this clockwise order black white black
white again of course this is even more
unlikely than the other so that
probability this will also go to zero
and the question is how fast does it go
and smirnoff and werner show that the
probability of this event decays like 1
over r to a different power and this
other power is 5 force so we say that 5
force is the critical exponent for the
forearm events because there's four arms
in this picture ok now this is exactly
the right picture to capture the notion
of influence so what is the probability
of a hexagon being pivotal for
percolation so now we're back into this
crossing picture we're looking for a
blue now path from left to right and we
want to know when is it the case that a
particular forget the D here that a
particular hexagon X is pivotal and so
what happened what has to happen if X is
pivotal it means if X is on if it's blue
there's a left-to-right crossing and if
it's red there's no crossing well so if
X is on that means there has to be some
blue crossing from left to right and
this blue crossing has to cross through
X if the blue crossing didn't cross
through X then by turning X off you
wouldn't have gotten rid of the blue
crossing so 4x to be pivotal this blue
crossing has to go through X and in
addition there has to be no path blue
path from this part of the path to this
part of the path because if there are
paths going like this there you could
get from here to there avoiding X and X
would not be pivotal so in order there
be no blue pass from here to here there
has to be a red red is now the other
caller rather than black and white we're
blue and red no there has to be a red
path going up there in a red path going
down there so that's that's what the
picture is for to be a be pivotal and of
course if as well as you're not near the
boundary this is exactly what we had on
the previous slide the four RM exponent
with these four arms going out and so
with so the probability of being pivotal
is as long as you're away from the
boundary
it's atmos by the critical exponent on
the previous slide 1 over R to the five
force and this yields the sum and what
is this due to the sum of the squared
influences well there are r squared many
hexagons each of the influence is one
over R to the five for sy square to the
one over R to the five or so I get 1
over R to the 5 halves I get our squared
times that and that goes to 0 and so
other than dealing with that you have to
deal with the boundaries not very
difficult you have to deal with boundary
issues because this is really the only
the appropriate picture of your way from
the boundary but this this is basically
tells you that the sum of the squared
influence code is 20 and you get noise
sensitivity okay so now if we go to
quantitative noise sensitivity this is a
question this can you take the epsilon
or is going to 0 so recall that Benjamin
in clay Tron as I told you said that
port percolation crossings d quarrel a
under noise even when the epsilon RS go
to 0 as long as they don't go too
quickly if they're bigger than constant
over log or for sufficiently large
constancy and might we believe that
percolation crossings can d correlate
and when I you I under D correlate can
mean lots of things it can mean not
completely correlate can mean partially
independent or completely independent
from media correlate means essentially
completely independent so that they
become asymptotically independent not
that they're just not completely
correlated but they're asymptotically
independent so might we believe that
percolation crossings can decor late
even if epsilon R is 1 over R the Alpha
and if so what would be the largest
alpha we could use if we take alpha too
big the probability the noise becomes so
small that things won't change and you
can't possibly do correlate and so this
is a well-defined question how big can
we get the how big can get the alpha and
basically allow the talk will be trying
to convince you were telling you what's
known and how to get with the app via
alphas okay well now you know the Alpha
so the uris thick is yes i love this
year this is the answer there maybe
we'll see the heuristic on the next
slide so yes might we might believe that
cautions d correlate and and get in a
guess
that the largest alpha is three quarters
and what I want to do in the next slide
is explained to you why you might
believe the best exponent is 3 quarters
ok so I'm just I just wrote repeated
that here so there we have it here in a
heuristic for the noise sensitive you
exponent for percolation Mike we believe
we decor that we D correlate even if we
go like 1 over power yes and the largest
alpha is three-quarters ok so in the
body bot bottom the slide I hope to
convince you you're ristic alee that
three-quarters is the answer ok so
recall the four so we already said this
before the forearm exponent says that
the probability of a hexagon being
pivotal is about 1 over R to the five
fours so that's the probability of a
hexagon being pivotal so the expected if
we look at now the total number that's
the probability of one pick a hexagon
being pivotal if we look at all the
pivotal's and look at the expected
number of pivotal's how many pivot
what's the expected number of pivotal's
well we have R squared many hexagons
each is pivotal with about that
probability so the expected number of
pivotal hexagons is about or to the
three quarters which is then three
quarters above therefore imagine you now
the noise is epsilon R the probability
to which your resampling is 1 over R the
alpha what would be the expected number
of pivotal hexagons a tree resample we
have this random set of pivotal's what's
the expected number that we resample
well it would be exactly the number of
the expected number of pivotal hexagons
times epsilon R so this would be just or
to the three quarters minus epsilon so
if alphas bigger than three quarters
then the expected number of pivotal's
that we resample is going to 0 so if
alpha is being in three quarters that
means we don't resample we don't we
don't actually resample a pivotal and
things don't change ok now you might
argue right away on that you hold it you
know it's possible I hit I I don't have
to touch a pivotal to change things
maybe I change this one which was in
pivotal and this one which is pivotal I
change them both of course I could
change things so
it's not that certainly not the case you
know the fact I didn't hit pivotal's
says I don't says I don't same things
but it's a heuristic and in fact making
this heuristic rigorous is actually
quite easy so this can be made rigorous
easily in a five minute argument if
alpha is less than three quarters then
the number of hex pivotal hexagons that
we resample is now part of the true
force minus alpha which is now very big
so that means that we're very likely to
hit a pivotal now and if we hit a
pivotal well things change and things
should all get mixed up you've lost all
your information so so that's the
heuristic of the three quarters now this
part is much much harder to make
rigorous but it's a heuristic where the
three quarters come from you if you hit
a pivotal things should change and then
you may be everything got lost in the
shuffle and you have no information
about but and so what you knew
originally about the pics that got
across and gives you no information
afterwards okay so uh that's okay so
that now explains the heuristic for
three quarters so now let me tell you a
two of the two different approaches that
we use to get partial results the first
one didn't give the three quarters but
it was it was the first argument that
gave that allowed to allow epsilon or to
go like 1 over R to some power now the
these these this this approach and then
approach i'm going to describe after
they're very different they're also very
different than the original Benjamin E
coli thromb argument that said if the
noise is bigger than constant over log
or things go to 0 so the although i
should say and i'm going to be getting
into the cell i'll explain very simply
some of the fourier analysis later the
all the three approaches used for
analysis but beyond that common
component the arguments are the
arguments are quite different and on
this slide i'm going to describe the
second approach which is very related to
computer science theoretical computer
science you could say so now you have to
sort of think like there
medical computer scientists if you know
how they think so so we have a boolean
function I have to take I want to
consider randomized algorithm so before
reading the slide let me tell you in
words what you should imagine a
randomized algorithm is let's say you
have a function a boolean function of n
variables and you know what the boolean
function is maybe it's a majority maybe
it's something else and I asked you to
compute F of x1 through xn well you
could do it you know the function f but
unfortunately you can't see the bits x1
through xn or covered you don't know
what they are so what you can do is you
can ask for me some of the values of x1
through xn you might say Jeff told me
what the third bid is please and then I
saw that was a one and then you look at
that and say oh that's the one then
please tell me what the tenth bit is
tenth bit is and maybe I'll say that's
minus 1 and then depending on as you get
more and more information you're let you
keep on asking about new bits but each
new bit you ask about which that you
choose will depend on what you've seen
up into that point in time and you want
to now at some point presumably before
you answered asked found out about every
bit you might say don't tell me anything
else I already know what the outcome is
for example in a percolation picture if
you already found out those a
left-to-right black crossing even though
you didn't know some of the other
hexagons you wouldn't need to get any
more information and a rhett and what
you want to do is you want to do this
type of thing ask me these questions
with a asking with trying to ask as few
questions as possible okay so now the
given that it should be easy understand
this a randomized algorithm a4f examines
the bits one by one where the choice of
the next bit exam and may depend on the
values of the bits examined so far now
it's also allowed to be random which bit
you choose next is a lot of depend on
what you've seen and some exterior
randomness it even can even the first
fit is random so the algorithm might
pick a bit uniformly at random or
according to some other distribution
that's allowed and that's what we call a
randomized algorithm and the algorithm
stops as soon as you know what the
output of F is and now I said you want
to ask as few questions as possible
and so this is one way of describing
this is the following when when the game
is over when and you know what the
output F is there are certain bits which
you've asked me about and we let script
is let g BJ be this random set of bets
the random set of bits that are examined
by the algorithm and you define so so
that's a random set and I define the
reveal meant of a to be the following I
call it Delta sabe it's sort of
represents the degree to which my
algorithm a I call a holiday reveals the
bits what i can do is for every one of
these n bits i can say what's the
probability that that bit was ever
looked at by this randomized algorithm
that's the probably that I is in j so if
you fix jay is random but if i fix a bit
i I can talk about that probably that I
is in Jane other words the probably that
you asked what I is and I want to take
the maximum that soup for the maximum
over all the different bits and that's
called the reveal meant of the algorithm
and with oded one version of the one
theorems we have is that if you have a
sequence of wool that led FNB a sequence
of boolean functions with a n being a
randomized algorithm for FN and the
first theorem says if the if these
algorithms are such that the reveal
mints go to 0 so for very large n for
any fixed bit it's very unlikely you'll
ask what the value is then it turns out
the sequences noise-sensitive i'll
explain concretely how you do this in
percolation and this also turns out to
give you a quantitative version which
which which says the following if this
the reveal mints go down according to
some inverse power then so if its most
see over N to the Alpha for some alpha
then if you take any beta less than
alpha over 2 then you end up getting the
noise sensitivity you want as long as
the noise the noise is one we take the
noise that be 1 over N to the beta
so what we're asking is is FN of x + FN
of x perturbed the noise version with
this very small amount of noise one over
N to the beta this thing says that these
become asymptotically independent italy
whenever beta is smaller than alpha over
2 if you have a bound on this on the
resilient ok so how do you what does is
give you for what does this give you
four percolation so I have to explain to
explain percolation I have to tell you
what this interfaces I also
unfortunately have to change white to
black and black to white now so now
we're going to look at left
unfortunately i got these pictures from
someone else have no idea how to change
the pictures so all right rotating
doesn't help i anyway so I now we want
to is there a left-to-right crossing or
not and this is a way this is called the
the interface between the two this is
often when you look at SLE 64 critical
percolation there's a famous picture of
Oded and this is what were is going to
be described here so here's how you
determine if there's a left-to-right
crossing of white I start a picture this
red path here and I continue the path
always keeping whites on the right and
reds on the left so the path go blacks
on the left so here it comes this way to
be white then I want a black on the Left
comes down here this defines the path
what this may this is a well def this
defines the path precisely it comes
around and goes around it always keeps
the whites on the right comes here
bounces then stays here then it comes
back here and goes up there okay keeps
whites on the right blocks on left now
this red path tells you if there's a
left-to-right crossing because start
this path it's going to be bouncing off
this side a while on this side a while
eventually it's going to hit the top or
the left side if it hits the top that
says there's no left-to-right crossing
because on the left side of this red
path is this black vertical path that
will stop the white path conversely if
this red path the interface hit the left
side before the right side you could
look at exact
what's above the the interface and that
would be your white path so looking at
this red path tells you that there's a
crossing ok now the algorithm is the
following the algorithm starts here now
you might not like this because this
that means I always examine this bit
with probably once the reveal mint won't
go to zero but let's not worry about
that so the bit the algorithm simply
asks about bits it needs in order to be
defined so it says what are you it's
white now it goes this way the path and
so it asks this bit because this is the
next one it needs to know that's black
the path goes up here then it needs to
ask this one so basically this path is
going to be in the end asking these bits
on the two sides of the path but no
others so that's how the algorithm works
now if we take a now let's ignore the
boundary this doesn't really have to
modify this i'm not going explain how to
modify it but if you'll tell you why it
works if you're away from the boundary
if you're in that middle of the heck if
you're in the middle here somewhere and
I ask what's the probability that this
particular hexagon is being looked at by
the algorithm that's what we need to
know what's the probability hexagons
being looked at by the algorithm well to
be looked at you're only looking at the
ones that are Jason to the interface so
what has to happen is if you take a
point in this interface you'll see you
have a black path to the boundary and a
white path to the boundary so hexagon
near the center of the of the picture is
examined only if there is both a white
and a black path coming out of it that's
a critical excellent that I have not
described for you that's called the two
RM exponent and that's also critical
exponent which is known in a decays like
1 over R to the one quarter so this says
points near not near the boundary the
property of them being revealed is at
most one over R to the quarter you can
not do some extra randomization to get
rid of the problems on the boundary so
hence and therefore by the previous
theorem you end up getting d correlation
if epsilon R is larger as long as
epsilon R is larger than 1 over R the
one eighth so it gives an exact route
that you can let the epsilon or go decay
is a power
not of this power you can still get
noise noise sensitivity the D
correlation but but of course the one
had the one eighth this is often as the
factor of six often the three fourths
conjecture yeah example of a nice
monotone function there is no sensitive
but has no I'll go yeah yeah you do GN p
@ p is a half and do click campaign
containment at the right value of click
to log ask if you have a to log n click
it turns out it's noise sensitive and
there's no algorithm yeah yeah it's
about to yeah the site you want you have
to choose the size to get it non
degenerate it's about to log and then
only it only becomes a non-degenerate
function at certain values event but
there is an example and it's known that
there's no algorithms it's it yeah it's
it's it's known so that that's that's
the best example I know for the answer
to your question I can also say you
might say okay you're off by a six but
can you find a better algorithm this is
one algorithm and the answer is maybe
you could find better algorithms like we
don't know if you can find better odds
an interesting question but nonetheless
there are theorems that say maybe
they're better algorithms but they can't
be that much better and their bounds on
how good an algorithm can be and and
it's knowing that you can't possibly get
up to the three quarters through this
via this method okay so so the fort know
what time do you know time I started at
ten more minutes okay so okay so the way
this is all done is the the Fourier set
up to eat so the you have the following
thing if you take the set of functions
now that's 0 1 to the end it should be
plus minus 1 to the N int or if we look
at all functions not just boolean this
is of course a 2 to the N dimensional
vector space because the domain has 2 to
the N elements and there's a very very
nice orthogonal basis for this vector
space which are these sets these sets
called Chi sub s where s is a subset of
wat 1 through n if s is the empty set
chi empty set
just taken to be the constant function
one and otherwise KY s it's the
following but Bernoulli boolean function
kaya x1 to xn is you simply take the
bits sitting inside of s and you
multiply them that's Kaissa best if you
know about group theory these are the
characters for the group zima to to the
end but you don't need it in this
context everything can sort of stay
combinatorial you know I've so we're
deal with that okay so that those are
the characters now those those those are
our basis elements are orthogonal and
therefore if I give you any function f
you can express you can simply write it
out in this a fog inul basis I can sum
it up overall as containing 1 through n
F hat of s Chi of s and F hat of s is f
hat of s is called the Fourier
coefficient of s and what's elementary
to check a very elementary to check is
that this that this correlation here
which is what we're interested in that
this is equal to that this can be
expressed in the following very simple
way in terms of these Fourier
coefficients now since my F is mapping
into plus or minus 1 is the L it has l2
norm is one that means if I sum up the
squares these Fourier coefficient just
by Pythagoras theorem they add up to 1
but what's interesting is what happened
but you end up getting that this
correlation is given by the sum K equals
one of the N 1 minus epsilon to the K
times this thing now if i sum up f hat
of s squared over all the asses we get
one but here I'm just going to sum them
up over the s's of size K and this is
sort of we call this the weight at size
at size K and in order list to be small
i mean what epsilon small but if i take
1 minus epsilon to a big power becomes
small and so what you basically want
noise sensitivity corresponds to the
Fourier weights these things here being
concentrated on large values of s that
most of the weight should I be on large
values of s that's what this formula
tells us
so let me let me give you an interesting
relationship between the spectral sample
and something else so it's not necessary
but it is it's not it's nice to have the
following picture there's something
called the spectral sample given a
boolean function f its spectral sample
which we call script s sub F this is a
random set its random set of 1 through n
and it's defined distributional e as
follows the probability that this random
set is equal to s is simply the Fourier
coefficient of s squared these all add
up to 1 so this gives us a probability
distribution on the subsets and then if
in terms of this random set this coral
this you can rewrite this correlation
which you want to go to 0 very nicely in
terms of this this s sub F that it's
basically it does the expected value 1
minus epsilon raised to the size of this
random set and so noise sensitivity is
basically equivalent to the cardinality
of this set going off to infinity now so
two or so to understand noise
sensitivity what one has to understand
is the structure of the cardinality of
the set and this set is quite comp
there's a very complicated random set
but this tells us that's what we want to
understand so quantitative noise
sensitivity can be obtained by
understanding the typical behavior of
this random set now what's not so hard
to get hard to get ahold of is the
expected value of that but the hard
thing is to show yet that the expected
value of this random variable is
actually telling you the typical
situation so there turns out to be an
interesting relation between we have two
different random sets for boolean
functions we have the spectral sample
that's one random set and we have the
the pivotal set that's the pivotal set
is the set of points which are pivotal
for the boolean function that's another
random set and it turns out that there's
a very interesting relationship between
them now they're not defined on the same
space so you can't ask if they're
independent or not but that you can ask
how their distribution Lee related and
there's
amazing things the probability that a
given bit belongs to these two random
sets are the same and therefore the
expected size of these two things turn
out to be the same and it also turns out
these two random sets have the same
two-dimensional marginals now this turns
out to be very useful because too you
can sometimes obtain weak results for
the spectrum you can you get what you
can do this tells you that second moment
methods turn out to be very useful
because seconds if you apply second
moment methods that to this spectral set
you can instead transfer it over to the
set of pivotal points which turns out to
be a little bit easier to handle and
this thing allows you to transform sense
sort of second moment arguments but
unfortunately they don't have the same
distributions in general just these two
dimensionals so this can't carry you all
the way as you want so what's happening
with percolation so the expected size of
these two things basically if you look
at the slides I told you before and put
them together the total influences are
the three quarters but what you need to
show is that the typical behavior of the
spectral set and and and it is very long
paper and acted by garbhán patayan
Shrum this is just a corollary of what
they did they did a lot but just as one
thing which is relevant to the talk is
the main point of the talk is that the
typical behavior of the spectral sample
is art of three quarters so not only the
mean it's typical behavior and this was
a very difficult project and the fact
that this is the typical behavior this
end in the end yields the conjectured
noise sensitivity so the the that's what
you need to do in so the point is even
though one is really only interested in
the cardinality of this their method
here you didn't just look at the
cardinality it looked at this random set
as a random subset and try to analyze in
some way and it turned out that it was
it has some relationships to fractal
percolation and they were mad at man
it'll to get enough information on it
too to prove this result
okay skip that part and okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>