<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Making the Most of Your Samples | Coder Coacher - Coaching Coders</title><meta content="Making the Most of Your Samples - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Making the Most of Your Samples</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9oSXuxWFnsY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello everyone oh it's my great pleasure
to welcome Zi long back he has been an
intern here before twice actually and he
got his PhD from upenn I spent a year at
Stanford has post talk and is now a
professor at the University of Hong Kong
and he's going to tell us how to make
the most of her sandals which i think is
really important so okay uh thanks for
the introduction and thanks for coming
so this is a joint work with each I and
team and it's a paper that's in coming
easy so so we're going to talk about
oxygens and talk about revenue
maximization in auctions so basically
auction is about allocating a bunch of
resources to agents in some specific way
following some protocols to satisfy
optimize certain objective and the two
most study and natural objectives in
auction are social welfare namely kind
of the overall happiness of all the
agents and the second one is just
revenue maximization like making more
money out of this sale of items so you
can find tons of real life applications
where revenue is a natural objective say
at auctions and Microsoft Bing or Google
or those auctions on eBay and so on so
roughly speaking there are two ways to
get good revenue out of selling items
right one is from the competition of
these agents potential agents potential
buyers if there are kind of value for
the items are somewhat close to each
other the competition is high then just
let them compete with each other and
like use the second price also will give
you good revenue but sometimes maybe
only one agent has high value for the
item in which case there's not no sir
not enough competition and in which case
are the usual practice is to use
resources to get good revenue just
and the good things about using research
prices is they are simple and natural
and also it's simple for the auctioneer
to implement the auction and also easy
for the buyer to figure out what's going
on and what they should behave right and
also it's kind of optimal in many
settings so a classical result by mines
and back in the 1980s shows that a
victory auction or the second price
auction combined with the reserve price
is actually optimal are for selling a
single item assuming these agents values
are drawn I ID from some public known
distribution so since using research
prices to maximize revenue is natural
and sometimes optimal so a very
important question is how to set reserve
prices so here's sort of the very basics
of how to set reserve prices so consider
the following very the most basic
scenario we have one item for sale only
one potential buyer and his value is
drawn from some publicly known
distribution denoted as f so arguably
the most simple setting you can consider
in terms of revenue maximization and
given any price P we are going to let Q
of P to denote the quantile and in this
case just 1 minus the CDF of P this is
the sale probability r and then the
revenue of a southern quan how is just
the revenue we can get by setting the
price to be the value at their quand
house so its V of Q times Q okay so
we're going to adapt some adopt some
standard assumptions from economic that
the distribution satisfy some regularity
condition for now just imagine that as
having this revenue curve in terms of
the function of quantity be a concave
function and then now this dysfunction
is concave well we can easily find the
Maximizer of this function which would
correspond daquan how and the
corresponding price are that we should
use if once you maximize revenue
okay so here's all very nice and simple
but that leaves a very important
question where do this patient prior
come from so in the literature usually
we just assume that magically we have
some closed form a like description of
this prior and then suppose we have
those closed form descriptions then we
just like do the magic and come up with
this optimal prices right so but in
practice this prior do not come from
free so where do they come from one
obvious answer which is a valid answer
is from the data of past use of behavior
say at auction we have tons of past
behavior and our people submit bids in
the past and if you kind of assume they
are kind of behaving this same and
having the same distribution across
different time span across different day
then we have certain knowledge about
this prior distribution okay however
it's not clear like even this past user
behavior it's not clear using this
optimal reserve like from some kind of
empirical distribution from this data
it's actually approximately optimal for
the qu prior okay and also this past
data do not give you some any kind of a
closed-form description no one's going
to come tell you that ok this is a
uniform between zero one or some
something like that ok they only give
you some samples from the prior even in
the most optimistic a case so it seems
that a more practical model would be
like we are given some ID samples from
the prior and now I want to choose a
reserve price based on these samples and
again we want to approximately maximize
the revenue ok so this is the model that
we are going to focus on in this talk
okay so more formally a.m pricing
algorithm will take em ID samples from
the prior as input and output some
resort price and then we will consider
its approximation ratio as the expected
revenue we can get from this algorithm
over the random realization of these
samples and compare this with the
optimal revenue you can get in hindsight
choosing the absolute optimal reserve
price and this is the approximation
ratio of the algorithm and we want to
make this as large as possible obviously
and there are multiple ways we can ask
this question for example given a fixed
number of sample what pricing algorithm
give the best approximation ratio okay
so in some sense how this is being done
in practice is that we take the samples
and we pretend that the underlying
distribution is just a uniform over the
samples and then based on this empirical
distribution we try to choose the best
possible price and maybe some hack on
the empirical distribution as well but
that's essentially what what's being
done but it's that the best thing to do
and more importantly if this number of
samples are small and you don't get you
form such an empirical distribution and
good approximation of the prior how how
are we going to good get approximation
ratio and secondly we can also ask this
question in terms of kind of a sample
complexity style so given the target
approximation ratio how many samho does
this optimal algorithm need in order to
achieve that target approximation ratio
okay and surprisingly even these very
basic questions and for the very basic
setting of a single agent a single item
largely we don't know the right answer
okay we have some kind of upper and
lower bound but there's pretty large gap
between them and now we don't know the
right answer yeah we have a prior or no
you should know by now just wait so we
have a prior but it's not given to the
algorithm in closed form we can only
access the prior through this ID sample
okay so we will assume the existence of
a prior
but we do not know where this other than
those samples we get okay okay oh we we
don't know that it's Turkish anniversary
how do you know the distribution yeah
then expire son yeah its response yeah
yeah it's just myosin but like okay then
I guess if you can for example think was
just one sample that's right like this
isn't downloading peer what the right
now bottom is adjusting so um there's
some result for that Senator a surprise
what else to do yeah so actually what is
this better than whether you have mystic
one but i suppose is a randomized and up
for grabs on distribution you can do
better than setting that no but that's
all I'm asking then all you seen you
assumes of the water distribution what
I'm saying is it I don't achieve you do
nothing then you said that what our
sample you have a surprise message
usually using a physical Iranian for
that regular it is necessary yeah you
know that Sigma you should assume
something that you know water
distribution is so certain now okay okay
yeah there's some restrictions some
regularity assumptions on the
distribution that's needed so that's for
sure but but other than that we don't
know anything about a distribution so we
know that for example we shouldn't be
like targeting a absolute sale
probability and all the revenue just
come from that small probability like
for that basically you so here's a kind
of potential bad distribution right so
if you have property one over age the
value is aged for some very large age
and then otherwise the value is just
zero then you cannot expect you have
kind of anything weird right so some
kind of regularity kind of small here
assumption is needed better than that we
don't have any knowledge of the
distribution okay okay so on so again we
can you asked the question in the
following ways and in different regimes
so for the sample complexity obviously
we will ask in the asymptotic regime
assuming we have access to many many
samples and all we want to know is how
many samples sufficient and necessary to
get one man accepts an approximation and
in terms of the algorithm question we
are going to ask
this question in the fuel sample region
because if we have many sample in some
sense the algorithms clear right you
kind of want you use those samples and
form some empirical distribution and at
least when the number of sample go to
infinity that's the right answer but
with only a few sample now what's the
right algorithm that's not clear so
that's the also the question that when
you ask and now we get you the
regularity assumption that we are making
about the algorithm so there are two
common assumption or in the literature
one is called a kind of regular
distribution there's some technical
definition saying that the virtual value
defined in this way is non-decreasing as
a function of the value of V okay but
really it's just saying that the revenue
curve kind of this RFQ on the quantum
space is a concave function okay and
this more restrictive assumption call
mhr so the prior has mountain has array
again there's a 1000 consumption like
this guy is non-decreasing but for now
let's assume that it's just saying that
the revenue curve is strictly concave in
some sense we will get to the point like
in what sense is strictly concave later
in the top which will value so is the
real you should be minus so it kind of
comes from the dis- analysis on revenue
maximization it's kind of how much
revenue you can get if you sell the item
to your agent with this value basically
but like roughly speaking yeah you're
using second prize yeah okay sorry
intrusive again expedient female
draw from this region expected payment
of any n is like this expected payment
is expected virtual surface-based
payment is some check it out it's not
something directly given to you so
instead of counting payments so another
way of Coptic revenue which is counting
virtual value for each value poi to the
distribution so they are all these will
get recognized so the act of expanding
value of x men since u is equal to
magnitude of it okay okay so basically
we're going to do some combination of
these two like a few like a few sample
and this is in critical regime and
regular and MH our regime okay okay so
we're going to go over us what we know
about this problem across these
different regimes and kind of a go over
what our results are and then depending
on how much time I have I'll go over
some of the proof sketches so our first
of all let's look at the asymptotic
regime so again we have many samples and
we want to know how many Sampo is
sufficient and necessary in order to get
1 minus epson approximation so this has
been studied before so we know that okay
so our Demonata at all proposed this
empirical reserve algorithm which is
what you would expect so given MGM host
v12 vm so I times VI is basically the
revenue could get assuming like the
underlying distribution is just a
uniform over v1 to vm okay so it should
just take the art max of this except
there's one caveat so it's called the
Alpha guarded empirical reserve because
it will rule the Alpha fraction largest
fraction of the value okay so in terms
of a kind of a machine learning kind of
perspective it's try to prevent
overfitting because I'm if the art max
is actually one of those VI that are in
the Alpha largest fraction then
basically your revenue is coming from
very small fashion of the sample right
so maybe you have overfitting to that
very small fraction of the sample which
may not be a good thing okay so
basically you choose alpha to be
something like epsilon and then
you use this algorithm and what they
show is that the absolute guarded
empirical reserve with one over epsilon
cube samples is sufficient to get 1
minus absent approximation for regular
distributions okay at yep if I do so
many samples right I mean look at the
empirical distribution okay how far will
it be away from well let's say the
protonation distance from the wheel this
we share because it's because in
something wanting to do is I just
learned the real distribution right yeah
but there's no comparison but we need to
make sure PP careful 55 the distance if
you take total variance then the photo
variance could be just one right because
it's a discrete distribution and so on
so let me yeah yeah so um so in some
sense elisa for the purpose of getting
revenue it's close enough obviously and
other than proto various distribution
i'm not sure will be a good measure to
measure distance between a discrete
distribution and the potentially a
become continuous underlying
distribution right so I guess I what I
want to say is that I don't have a good
measure of distance of the top of my
head for measuring like the empirical
distribution and shoot underlying
distribution for this case but yeah but
at least they show that it's sufficient
for like getting revenue okay and for
the more restricted mhr distributions
they show that one over epsilon square
samples is enough and also you don't
need those to put the Alpha guarded
version you just like just use them
empirical optimal price and then that's
what my settlin approximation okay and
yeah sure this goes against what we just
said right listen is this makes logical
sense to go yeah it makes logical sense
you guard if it could be possible that
like one of these guys that are one of
this very high value or the art max
right but our mhm obesity is a stronger
small hill distribution and then it's
not likely those those will fool you out
with okay
okay more recently there's a paper by
arco where roughgarden that study the
essentially the same setting but in a
more complicated scenario where we also
have a single item for sale but we have
k bidders instead of on one bidder and
these capers are prior distribution may
not be identical okay and their main
result is that the symbol complexity has
to be polynomial in the number of
bidders k okay and so so in terms of the
dependency or absent or they show is
that it's something like 1 over epsilon
square root of epsilon so pretty far
from those upper bound are from the
previous paper okay so what do we get
our in this work yep yeah for for each
district so in the model basically in
each round you get one sample for each
buyer and they're saying that the number
of rounds you need is need to be
polynomially in k okay so you need more
fine-grain for our knowledge about the
distribution if you have none identical
bidders so on so on de on the
algorithmic side we give a improve mhm
our upper bound we show that are using
the same algorithm are actually one over
epsilon to the 1.5 is enough to get 1
minus epsilon approximation ratio and we
also show a matching lower bound
actually we give a lower bound framework
and we show that one over epsilon to the
1.5 is the right answer for mhr
distributions and one over epsilon cube
which is achieved by this previous
analysis is also tied and this is
borrowing some techniques from
information theory and our differential
privacy and i would like to kind of say
a few more words about this results for
mhr result mhr regime because it's kind
of surprising at least to me um so first
for when we first get this result we
think something is wrong with our
analysis and each I was saying like
three have should not be the right
answer for anything especially in terms
of like sample complexity it's very rare
you see something like that and also are
noticeably this is fewer sample then
what you will need in order to estimate
the optimal revenue up to 1 minus x n
factor so even if I tell you what is the
optimal restored price you basically
need one over epsilon square samples to
estimate the optimal revenue up to your
1 plus minus epsilon factor right
basically once you ask them in a sale
probability and you want to estimate
that to kill 1 minus epsilon factor you
need one of rap's and square samples
okay so in this sense using this man
example we don't quite know what the
optimal revenue we K we get but we know
what is a good price okay so kind of the
optimization version is easier than just
estimate estimating the objective and we
will see some intuition why this is
possible now for now let me also go over
some some of the results for the single
example regime so this is a classic
paper or by below and clamor and they
basically shows that if you have one
sample and you use that sample as your
reserve price then this is one half
approximation assuming the distribution
is regular and in for the more
restricted mhr version basically no
results is known prior to our work or
people just plug in this one half so
what we show is that actually you can do
better for mhr distribution if you take
the sample and scale it down by some
factor a point A five in this case then
you can actually do a point 5 8-9
approximation for mhr distribution okay
and we also show some upper bound like
point 6 a-4 mhr distribution and we also
show that this one half approximation
for the regular distribution is tight
for deterministic algorithms
yes yeah kind of equal revenue type of
distributions yep okay and moreover like
our positive results also have
implications for more complicated
setting like the multi-agent problem
because some of these multi-agent
results use the single agent as a
subroutine and do some direct reduction
so we know that if we have some good
pricing algorithm for the single agent
problem then there's a way to convert
them to a multi-agent problem as well so
of example we can combine our results
with the results by our Demonata at all
so what they show is that for prior what
they call prior independent option in
the major environment with our ID mhm
arbiters then they can do n minus 1 over
N times point 5 approximation and we can
directly plug in our new results and
improve that point 5 2.5 89 and
similarly for the other results as well
and it's in topically we also can also
plug in our new sample complexity upper
bound and improve this that the number
of bidders they need you get 1 minus
Epson proxim a shin from 1 over epsilon
cube to 1 over epsilon 0 5 halves and
yeah just a there's a reason result by
our team and hidden are also used the
single agent problem subroutine and we
call so plug in our new results to
improve some of their ratios okay so now
let me get you the technical part and
kind of a sketch some of the high-level
ideas behind these results so let's
first consider the asymptotic and mhr
regime and explain why we managed to get
this we're 1 over epsilon to the three
half upper bound so first of all let me
briefly go over what's the you show
sample contacts the upper bound
technique so so
consider the empirical reserve price
algorithm we take these samples and we
just use that as an empirical
distribution and pick the best price the
high level plan would be I want to ask
the middle revenue or the sale
probability of each potential price
namely this V is up to your 1 minus
epsilon factor like use some
concentration banger and so on and then
we show that if we can do that and then
choosing the best from this empirical
distribution would be a 1 minus epsilon
approximation and smoke point how could
be problematic as I just explaining and
that's the intuition behind dropping the
absolute highest value and use this
epsilon garlic version and with 1 over
epsilon cube samples and also concavity
of the revenue curve we can achieve
exactly that so first of all by
concavity of the revenue curve we can
see that it's okay to drop the highest
value because up in this revenue curve
it drops at least as far as linearly
right so just dropping an absolute
fraction of in the quan how space does
not hurt your optimal revenue by more
than a 1-1 of 1 minus epsilon factor and
second of all since for any candidate
our price that we consider it in the
absent guarded version the sale
probability is at least epsilon so with
1 over epsilon cube samples we expect
you see at least one of your absence
square samples are they are larger right
so that will allow us to estimate the
sale probability of every single price
up to 1 minus epsilon factor just by
simple application of the Chernoff bound
okay and that's enough to finish the
proof of this one over epsilon cube
upper bound sample complexity upper
bound or for regular distribution now
for mhr arm it's it's pretty much the
same thing except that we will use a
simple property of mha distribution that
the optimal sale probability is at least
some constant 1 over e and that's also a
standard result in the literature and
then it's the same argument except that
now
revenue relevant prices has so
probability at least a constant so one
with one over absolute square some hoes
you expect to see one over epsilon like
sales right and that's enough to
estimate the sale probability up to 1
minus epsilon okay so that's how you get
the 10 flaps in square upper bound so
all very straightforward and just from
this analysis you may think that one
over epsilon square is the right answer
so why is that ok so what work what
could be a potential best scenario here
so maybe there's some other price p
other than p star who is only a 1 minus
q abstinent approximation but somehow
due to sampling error you overestimate
the revenue of this price by a 1 plus
epsilon factor and maybe also
underestimate the revenue of the optimal
price by 1 minus epsilon factor and over
and as a result the algorithm mistakenly
pick this price over the optimal price
right and indeed this could happen if
this sale event are treated as two
independent conf lips and then you want
to see which one is larger but a problem
here what's missing here is that these
cue events are not independent so when
we try to estimated revenue of these
prices there are correlations okay so
take this is example suppose Q star is
the optimal product the one half the
optimal price and Qi is the Quan have
some other prizes so basically what we
want you estimate it's the number of
samples that fall between 0 and Q I and
the number of samples that fall between
0 and Q star right so you can see that
the number of sample for between 0 and Q
I basically contribute to the sale
probability of both okay so there's some
kind of correlation here and in
particular kind of overestimated the
sale probability or the revenue of Qi
doesn't matter that much because those
will contribute to the silk robe the
accused as well so if you have more
example here then you overestimate both
okay the error mainly come from the
samples that falls between Q I &amp;amp; Q star
okay and that observation will allow us
to do a better analysis so
okay so that's the high level intuition
why we might be able to do a better
analysis now let me get you the
technical part so we will need a
technical Emma so record that when I say
the mhr distributions the mhr assumption
can be interpreted as saying the revenue
curve is strictly concave now this this
is this lemma would tells you that it's
actually strictly concave at least at
the optimal price so it's saying that
are when we move away from the optimal
price in the Quan Tao space then the
revenue drops at least quadratically as
how far we move away in the point how
space so here's the intuition there's
some technical definition for mhr are in
terms of this virtual value and so on
but you can call also play with it and
write it as a differential inequality
about the revenue function over the
quant outer space so it's just this guy
and then um now let me explain how to
interpret this so on the left hand side
I have the kind of a second or the
derivative of the revenue function on
the quantum space and on the right hand
side it's basically saying that how kind
of a lower bound on the magnitude so
it's saying that it's negative and the
right hand side tells you how negative
it is so what is this R of Q minus Q of
our prime of Q it's basically taking
tangent line AQ on the revenue curve and
think consider this intersection with
the our exes and then the length of this
intercession is exactly R of Q minus Q
times R prime of Q right so basically
the right hand side saying that the
magnitude of this second or the
derivative is at least as large as the
intercession with the our exes by this
tangent line uh it's not exactly the
eventual thing but yeah I do i do think
it's kind of similar yeah in some sense
yeah but yeah but but in a case like
using the fact that Q is between 0 and 1
it's it's saying that like the second
order derivative is a it's at least a
smallest like minus the intercession of
this lab the length of this intersection
and in particular if you like take you
to the Q star then this tangent line is
a kind of a flat line and then the
intercession is simply the optimal
revenue right ok so it's saying that the
second order derivative and Q star is at
least a small as- r q star and then at
this point the first order derivative is
0 right because that's the definition of
being optimal now yeah it's it's kind of
strong concavity at this particular
point okay now kind of the intuition
that taylor expansion tells you is that
now R of Q kind of drops at least
quadratically with this form okay now of
course a Taylor expansion only tells you
this hose in a small neighborhood of Q
star and we need is for any Q so the
actual proof is more complicated but at
least this is the the intuition behind
this okay so given this technical Emma
now we are ready to show this one over
epsilon to the three half upper bound so
we're given these samples and for
simplicity let's assume that the optimal
price is also an example we are pick
between them so what we want to show is
that for any sample VI with Kwan
towelettes aqi smaller than Q star and
revenue r which is a 1 minus epsilon
approximation of the optimal we want to
show that any of such VI cannot fool the
out with them will not fool the
algorithm with high probability okay
now in general of course the it may not
be exactly 1 minus Absalom but this
seems to be the worst case scenario
right it's the closest yeah this is a
bad event I want to show that this
battle and cannot happen with high
probability ok so what does it mean that
VI foods the algorithm or the algorithm
pegs VI over the optimal price P star
it's basically saying that how how much
we underestimate the sale probability of
the optimal price is at least 1 minus
epsilon times more than how much you we
underestimate the sale probability of B
i right it's this so now if you're kind
of a move abraded move around the terms
and so on and also recall this picture
basically all the error basically comes
from this number of samples that fall
between Q I &amp;amp; Q star right because the
number of sample for between 0 and Q I
kind of contribute you both and then
they do not really matter and it's not
they do not matter in the first-order
sense okay so if you play this out that
basically means that the number of Sam
holder for between Q I &amp;amp; Q star must be
very small and it's at least epsilon M
it's smaller than its expectation by at
least epsilon times M basically okay so
this is because in order to
underestimate the sale probability of
these bizarre we need to the number of
samples are therefore between q1 and Q
star to produce an error that's Lisa
Absalom fraction of the total number of
sample that we expected for between 0
and Q star and that number is at least
kind of order 1 times M because the
Optimus L probability is at least 1 over
1 over e right okay so we have some
lower bound in how much error we need we
were we expect to see kind of in this
small interval so what we need next is
to a upper bound on how large in this
interval is
and that's where the technical lamina
comes in handy so the technical MSA is
that when we move away from the optimal
quantile then the revenue drops at least
quadratically and since this revenue is
1 minus epsilon times the optimal that
means this interval cannot be larger
than roughly square root of epsilon
right okay so we have some upper bound
on this size of this interval and we
also have a some lower bound on how much
additive error we expect from this
interval and now it's just simple
application of the channel bound and
that will tells you that with 1 over
epsilon to the three halves samples what
with high probability this cannot happen
okay in fact the inventor distributions
here sorry you're using this one already
space left for a mature distributions
here hey that sell their the probability
of accepting years at least yes i'm
using the fact that the set basically q
star is at least one ovary rage so do
you all do something but I alpha
distribution yeah good so um there's a
kind of a family of assumptions that
kind of interpolate between mhr and
regular proposed between recently called
alpha strongly regular distributions so
for alpha strongly regular distributions
again the right answer is 1 over epsilon
2 3 half but the constant kind of
becomes worse as alpha goes to 0 okay so
kind of the regular case is a
singularity points when alpha goes to 0
the dependency on abstinence do um yeah
one over the absent to the 3 over 2
mm-hmm and like yeah both upper and
lower bound but in that case the both
upper and lower bound does not match in
terms of the dependency on on alpha at
least in for our current asset they did
not match but still are the dependence
on epson is or absent 3 house
okay so uh that's that brown so but
again basically it's just exploiting the
fact that when we asked the major
revenue or sale probability of these
different samples there are correlations
and this correlation is helping us and
basically although we we don't quite
estimated revenue up to your 1 minus
abstinent accuracy but that's not matter
when we overestimate we overestimate
everything and we're unless we're
underestimate everything what matter is
the kind of a relative relationship of
this expected revenue that we estimate
and one of reference to the three half
is enough in that perspective okay so
are now let me try to explain the lower
bound framework so are at a high level
we are trying to reduce the sample
complexity of this revenue maximization
problem to the same complexity of a
classification problem so suppose we
have two prior distribution d1 and d2
both are regular or both are amateur
hour depending on which setting we are
talking about and suppose they are
different enough in the sense that they
have disjoint 1 minus epsilon a 3
absolute optimal price set so what does
that mean it means that for any price if
it's 1-3 absolute approximation for d1
then it cannot be a 1 minus X to be
epsilon approximation for D to
simultaneously and vice versa okay now
if this is this is the case then any
pricing algorithm that is 1 minus
epsilon approximation for both v1 and v2
will effectively distinguish these two
distributions so basically you run the
algorithm on the samples and then you
see what's the price that is suggest and
if the price is suggest falling through
this one minus the absolute option price
lv1 then you kind of assert that the
underlying distribution is d1 otherwise
you say it's d2 and if the algorithm is
good then you will success you will
succeed with high probability okay so
effectively a good pricing algorithm
can distinguish these two distributions
so the high-level plan is to construct
these two distribution to be similar a
similar as possible subject you having
this joint are approximately optimal
price that and then use choose from
information theory to lower bound the
number of samples we need to distinguish
these two distribution and that will
translate to you a sample convexity for
the kind of a revenue of pricing
algorithm okay so how many of you may be
familiar with this but let me go through
this anyway to make sure we're on the
same page just some information theory
basics so when we when it comes to
distinguish it to distribution are we
need some measure of distance of the
three distribution and turns out the
right notion of distance here is the
statistical distance define our in the
first line and basically if you want to
distinguish these two distribution with
probably significantly higher than
one-half then the statistical distance
between the two distribution it has to
be at least as large as some constant
okay so that's the pic of a point from
this and in our case what is p1 and p2
p1 mpq will be the Joint Distribution of
kind of the MiG samples from d1 and d2
that we talked about right and usually
it's kind of pretty hard to handle or
compute the statistical distance of
these kind of distributions and that's
why in the literature usually people use
KO divergence instead so the definition
of carroll divergence is in the second
line and our carrel divergence and
statistical distance are related to each
other by what's called the pin scales
inequality so the pinnacle in quality
basically says that if does the
statistical distance is at least a
constant then the KR divergence must
also be at least as large as some
constant okay and the nice thing about
KL divergence is that if you take em ID
samples then the aio divergence just get
multiplied by M ok and so in order to
show that the the cal divergence of p1
and p2 is small it's the fastest to show
that the carriage
of the kind of base distribution d1 and
d3 is small okay okay so now as if now
if we can distinguish we have a good
pricing algorithm using only em Sam hope
that means we can distinguish d1 d2 you
using em sample it means that the KL
divergence between kind of d12 the M and
D 2 to the M is at least as large as a
constant okay now that means that the
carrier divergence between d1 and d2
must be at least as large as some
constant over m and the sample convexity
enough on the flip side says that the
sample complexity has to be s alesis
largest one over the KO divergence of
the B 1 and D 2 that we construct right
okay so now the plan is to construct to
distribution that have destroying
approximately optimal price that that
has small KL divergence okay and when it
comes to construct distributions with
small cayo divergence we use some lemon
from the privacy differential privacy
literature so it's basically saying that
if point-wise the density function
differs by no more than 1 plus minus
epsilon factor then the KO divergence is
a most absolute square okay so kind of
the the carroll divergence is just the
expectation of the law of this ratio
right so the fact that this ratio is
between 1 plus minus epsilon basically
say then the log is a most excellent so
the trivial analysis will give you that
the carroll divergence is a most Absalom
so why Absalom square so the observation
is that like these signals or samples
that you get not only give you kind of
positive signal but also give you false
negative signals okay so for example
like if two distributions are very very
similar then when it gets one sample and
you say that the density of d1 for this
sample is larger than the density of d2
you assert that it's more likely the
underlying is the p1 right but it's
possible that this underlines that is
actually from d2 and you are getting
some kind of false negatives
right and kind of mathematically saying
that since D 1 of Andy to you post 71 it
cannot be all these ratios are 1 plus
epsilon a lot of them will be 1 minus
epsilons as well and therefore basically
the first order term will cancel out and
only the second order term will remain
and then you get kind of absence square
okay all right in a case so now the goal
is to construct these two distributions
such that point wise their density is
kind of close to each other while
keeping the optic approximately optimal
price at this joint okay so i will not
bug you with the kind of a mathematical
verification that they actually satisfy
those conditions but i'll kind of tell
you what the distribution look like in
terms of the revenue curve ok so for
regular distribution we are going to
consider the following two distribution
so the first distribution is what's
sometimes called an equal revenue curve
so it's the blue line basically a
straight line the triangle that peak at
quanta q equals 0 and the second
distribution is essentially the same
except that we truncate it a little bit
at the high point house regime okay so
kind of from the picture you can see the
two distribution are very similar but
also the kind of 1 minus epsilon
approximate price that are disjoint
because the approximate price that for
d1 is essentially the blue area and the
approximate price set for d2 is
essentially the red area right now if
you calculate the Carol divergence of
these two distribution then kind of a
for a majority of the samples in the
quantile space the densities are
identical so only for the kind of top Q
epsilon 0 kwan house their densities are
different and even thought that that
kind of a small fraction of the same
house that they are different they
differ by at most 1 plus minus epsilon
factor and overall you get that the KO
divergence is as small as like absent
cube okay so that's how you get the one
over epsilon cube sample complexity
level down and 4 mhm are again very
similar
thing you kind of construct two things
that kind of shift a little bit such
that the price that are disjoint and
then show that there are Cal divergence
is small and the distribution we
consider the first one is kind of a
uniform distribution between one and two
and the second distribution we basically
kind of a scale down the density between
1 and 1 plus square root of epsilon by a
little bit and then scaled up the
density in the rest of the value space
by 1 plus epsilon factor and it turns
out that's enough to shift the
distribution enough to have disjoint
price space and then the Carroll
divergence turns out to be absolute to
the three half ok ok so on so that's
that's all the results for the Esen
products regime and now let me spend our
way should I stop Oh five minutes oh ok
so I'll give a very high level view of
why kind of in a single sample space we
get better than our point five so how do
we prove pond5 it's actually very simple
ok so recall that regularity means that
the revenue curve is concave right and
if we use the identity pricing meaning
that we take a sample and just use that
as the reserve price then the expected
revenue you get is exactly the area
under the revenue curve ok because you
take the sample then the revenue you get
for that sample is the just hide in that
quantum space right and then you take
expectation just the area under the
curve and it's concaved so this area is
at least as large as the area of this
triangle that peak at the optimal now
it's basic our job job mature right so
what's the area of this this triangle
it's the height which is ours are times
the base each which is one and then
divided by 2 and therefore the expected
revenue is at least optimal over 2 ok so
our since in some sense amateur
distribution are strictly make this run
recurve strictly concave that kind of
suggests that I'd enterprising will get
strictly better than one-half
but that's not quite a case because uh
it's not really concave in the usual
sense okay so kind of this is a
technical definition and it basically
says that for the courthouse there are
larger than cubes are meaning for the
values that are smaller than the optimal
price indeed this is strictly concave
and in fact we can show that it's at
least as concave as the revenue curve of
exponential distribution okay however
for the first half in the courthouse
base not necessarily and in particular
we can kind of truncate any regular
distribution which means that we kind of
flatten that to be a straight line and
that's the irregular distribution and
that part is not strictly concave and in
fact a point mass is a regular
distribution and four point mass and
enterprising only give you one half of
the optimal right okay so okay so now
for the okay so not exactly point mass
let's say a uniform distribution between
1 and 1 plus epsilon for very small
epsilon then basically the random person
per se is one right but the sale
probability for identify Singh is to
only one half yeah kind of a point mass
but you kind of play with it a little
bit yep so so it seems that okay so the
whole point is this for mhr distribution
the Rayleigh curve look like something
is concave as exponential plus some
truncation at the beginning so let's say
that's the case and let me explain how
to get around this and how to get better
than point five so the hard part is the
truncation part right because of for
that part identical rising gets exactly
one-half so here's the thing for the
truncation part or a point mass we have
good revenue per sale but rarely per
sale is exactly up basically but the
sale probability is kind of small it's
only one half okay now if we kind of
scale down the sample value slightly by
some factor C which is like pounding
away from one
we basically double the sale probability
right now we sell for sure and we only
lower the revenue per cell x factor C
okay so that's the improvement for this
part now of course like the truncation
is not strictly point mass and we need C
to be sufficiently small to handle all
the cases but that's the key takeaway
point now for the strictly concave part
we show that it's alesis concave as the
exponential distribution and for
exponential distribution and
enterprising is actually pretty good
it's like point six eight much better
than one-half okay so we can afford to
lose something on that part and again by
scaling down the sample value by a
factor of C we lower the expected
revenue by moc so if C is sufficiently
large then for this part our expected
revenue is still much better than
one-half okay so basically we choose C
to balance the analysis for these two
parts and then that gives us like point
five point five eight nine okay all
right so i think i'll skip the
impossibility result and so this is kind
of the summary of our results so for the
four main regime that we consider we
propose a couple of new results for the
asymptotic regime we essentially have
the right answer for both mhr and
regular distributions the sample
complicity for regular distribution is
one of your epsilon cube and assemble
complexity for MH our distribution is
one over epsilon to the 1.5 and for the
single sample regime are we get some new
upper and lower down for mhr
distribution and noticeably for MH our
distribution we can get much better than
one-half but this do some pretty large
gap between the upper and lower bound
okay so there are many interesting
future directions for this problem when
obvious one is to close the gap for the
mhr single sample regime are and another
one is to how to do the pricing with
more than one but only constant mean
many samples so so far our analysis and
technique are quite specific for only
one sample for example if i have two or
three or five samho i don't know how to
do better than this like point 5890
have for record distribution of course
when the number of sample goes to very
large nowaday synthetic part kicks in
and we can do close to 1 minus epsilon
but with a few sample we don't know what
to do so it will be interesting to see
something in that regard because in many
practical scenario we have like some
data but it's not large enough so that
those are a simple regime kicks in and
you can use concentration bound and so
on so it would be interesting you see
some techniques here and also it would
be interesting to analyze the sample
complexity for more complicated settings
like here it's the absurdly simple
setting of one buyer one item for sale
right in practice your interesting more
complicated settings and can we use some
of the techniques we develop in this
work to those more complicated settings
that may potentially get right sample
complexity answer for those settings and
also like beyond our ID samples so kind
of having access to this priors why I
the samples is more slightly more
realistic than having clothes from
descriptions of those priors but still
is a questionable assumption like as
soon as the bidders find out that you're
using their bits to design future option
they may play around it and also like
there might be like small changes in
their prior every day and so on so it
will be interesting to see if there are
some more realistic settings where we
can still develop interesting sample
complexity lower lobes upper bounds and
algorithmic questions as well okay ah so
that's all thank you yeah finish your
transferable samples it is not here
diadem should we do it just should be
the same thing go unexpected values what
is the scene yeah what's the same thing
for constant number of sample ok I just
take off this is my distribution
internal distribution and then I know
what I just pick one of these places and
I can't if this will do okay so on this
mission um so that's 11 potential
especially for 44 regular for regular I
think that might be the right algorithm
but still even for that algorithm we
don't know how to analyze it you show
that better than one-half approximation
until like the number sample gets really
large and second of all for MH are like
this work seems to shows that you want
to scale down the sample value by some
factor and that helps the analysis right
at least with one sample so how do you
do that when you have multiple sample do
you kind of computer empirical best
price and then scale it down or what
what do you do so it's not clear so
there's some also output outwardly
question there as well because if you
the error on either side is easier to
err on the door saying and so on the
price only kind of hurts your revenue by
a little bit overpriced can go from up
to zero so in that sense you kind of
tend to underprice when you don't have
enough information for deterministic oh
so that's actually very simple so first
of all since point mass is one specific
regular distribution or something
closely point mass so for the knesset
algorithm you don't want the over price
basically if you're over price then
you're doing four point mass now for
this equal rounding curve that kind of
pick a cuanto equals zero kind of
underpricing always hurts right because
basically the optimal revenue is
achieved when the price goes to infinity
now if you're never over prize then for
this distribution you can get at most
one half and other enterprising gets you
that one half
yeah but of course now you know that if
you use randomization you can do like
point 5 plus absolute so what can you
what is it oh that that's also like one
of the new results in the coming easy
yeah right Cooper the official gap
between the lower bound in the upper
bound or even actions are in standard
right for a lower bound is one over
epsilon cube and the best known
algorithm oh so it's off by a lock one
of Rapson factor for all these are both
regular and mhr regime it's off by a
logarithmic factor but the main terms
matches but so do people in information
theory know so because I understand that
now you have two distributions as you
say right okay no they're very similar
statistic wise and right that's how you
get your lower bound so two people
information theory can they do this if
they have more distribution there's to
get the law oh okay I see what you mean
so you're saying that maybe like Lauren
mechanic can I differentiate so can I
get a lower bound by not being able to
differentiate between logarithmic number
of that that's that's a good point I've
enjoyed it so that's potentially one way
to get out that that's definitely like
literature like doing this like if you
have em potential classes and you want
to do the classification out with them
like then what kind of condition you
need and some other type of conversion
some this is KO divergence right and
then for multiple classes or some other
type of divergence is needed that I I
don't know the literature literature
well enough to answer you right now or
alpha a new distributions is scaling
skating downward and whatever
I don't know we haven't tried that
actually because even for this one like
our analysis and technique are quite
like primitive primitive and I we don't
think that's the right analysis and
write algorithms yep so so that's why we
didn't like try to extend it to alpha
regular but but i would say probably
like scaling down still helps and I ok
so now I remember I thought about it a
little bit so we can definitely the
excuse above 80 yes yes so let me say
the following so all the pan calamari
use for mhr and single sample regime has
a counterpart for alpha regular
distribution as well so I'm pretty sure
we can get something like point 5 plus
epsilon but what's the number we don't
know and we probably the dependency on
alpha is not very good as well so we
never really figure out the answer this
point since it is for the exploration
district yeah that that's very so
there's a simple proof of like this is
for e / for basically this is your for
basically i dented pricing achieve you
over for for exponential distribution
and then essentially the same argument
for the regular case right so again
point mass is the regular distribution
so you don't want the overpriced and
then you can kind of show that if we
don't open over price then I dental /
identity pricing give you the best
approximation ratio for exponential
distribution and that's you before and
there's a slightly more complicated
version give you something like e over 4
minus some small constant so yeah so
your intuition is right that's from
exponential distribution kind of</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>