<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Deep Visual Analogy-Making | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Deep Visual Analogy-Making - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Deep Visual Analogy-Making</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PB7b-a0kC4c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our first talk will be by Scott read on
the visual analogy making this is joint
work with young yu ting Jang and hung
likely at U of M Michigan ok so we're
familiar with the word analogies like
the following king is to Queen as menace
to women Paris is different as Beijing
as to China bill is to Hillary as the
rock is to Michelle we know that neural
word embeddings have been found to
exhibit regularities allowing analogical
reasoning by vector addition so you can
do things like king- queen + woman
equals man approximately we can also
make up visual analogies in the same
style so you have them example
transformation pair we change something
like the color and then you want to
apply this to a query we can change the
color shape size of these shapes and we
can make up a prediction problem where
the task is to fill in the pixels of the
output that make the analogy true this
required this requires two things is
always problem you have to understand
the visual relationship of the first
pair of images and then correctly apply
this transformation to a query image
there's been a lot of related to work on
modeling relationships between images so
there was paper on separating style and
content with by linear models and where
there is a factor right representation
into style and content units that can be
separately adjusted they were just work
on image analogies a long time ago where
you could take basically local style and
texture from example images image and
apply to a new image which work on
learning to traverse image manifolds
with locally smooth manifold learning
there's been a line of work on using
higher order both machine models
training on pairs of images and learning
to represent the transformation and
having learned this you could apply this
in for transformation to query images
and so they learn to do rotations and
translations and in later work they did
more general transformations like facial
expression changes
there has also been work on learning
representations with analogy making but
as a regular riser to improve object
categorization performance very recently
there was the multi perceptron which is
a deep Network Ferguson tangling faith
identity and viewpoint there was an
extension of these higher-order
relational learning models to time
series there's been evidence that
multimode in multimodal embedding space
you can do vector addition and
subtraction to do analogy making and
actually retrieve images that complete
the analogy is when evidence that
convolutional networks can generate
high-quality images like chair
renderings there's been a convolutional
deep convolutional variational Auto
encoder that learns disentangle
representations disentangled feature
representations and this can be used to
solve certain analogy problems there's
also been working on developing models
tractable probabilistic inference over
compact community of Li groups which
include citations and cyclic
translations and later this was extended
to 3d rotation so what we do differently
here is that we have a very simple
convolutional encoder decoder
architecture where the training
objective is end-to-end prediction of
the pixels to complete the analogy and
we can also learn disentangled features
as a special case of our model and also
combine analogical analogy training and
disentangling so now I walk through a
car to an example of our approach so you
have some example transformation from A
to B this little video game character
drawing a bow and you want to apply the
same transformation to a query image
actually from a different camera angle
first thing we do is infer the
relationship of the first pair so we
encode the two images using some encoder
F and we represent the transformation as
a difference of embeddings F of B minus
F of a then we embed the query and we
need to transform the query somehow
based on the transformation the simplest
thing we can do is just edition of this
difference of embeddings of the
transformation and then we decode using
a decoder g back into the image space
the objective is just to make this
decoded image match the ground truth
completion of the analogy
so questions we study are one what form
should the encoder and decoder G take
and to what form should the
transformation increment t take so
notice that addition is is one way to do
this information but it's not the only
way in particular you could take this F
of V minus F of a and you could also
make the increment to see dependent on
see itself so this increment function t
could be some higher order interaction
like a tensor product or other things
and we studied three alternatives I'm
sorry for the encoders we use
convolutional encoder decoders for the
transformation we study three
alternatives the simplest is additive we
would take F of B minus F of a and just
add it to the query embedding ok the
second one is multiplicative where we
take the transformation f.a.b much FFA
and learn a tensor of parameters W to do
a tensor product with the query
embedding f of c and the third one we
tried is actually just concatenated the
difference of embeddings with the query
embedding and learning an NLP that gives
us the transformation increment and this
is trained jointly with the entire
network end-to-end so here's a schematic
showing these three options in a simple
problem of rotating a shape so from A to
B we take this oval it's rotated and we
have a query square we want to apply the
same transformation and so there's three
choices that affect the computation of
this increment T so as a cartoon you can
see what's happening for the additive
multiplicative and deep versions of the
model we also have a regularizer on the
embedding space so the idea here is that
we want to force the transformation
increment to match the actual step that
we take on the manifold from C to D note
that there's no decoder here so it's
nice in the sense that we get a stronger
local gradient signal for training the
encoder and in practice we found that
this allows us to help to reverse image
manifolds because we can apply analogies
repeatedly and so for training we use a
weighted combination of these
objectives so here's the algorithm for
manifold traversal once you have a
trained model the first thing you do is
you embed your query using the encoder
and then for some number of steps 1 to n
we want to add increments to Z on the
manifold using the transmission function
T so given some example 10 summation be
a to b we just keep on applying this
transmission increment also taking into
account the current position of the
query on the manifold and we decode at
each step to give the generated images
in this example we have a rotation from
A to B of this triangle and we can
repeatedly apply it to the query to do
repeated rotations we can use a similar
idea for disentangling a similar encoder
decoder architecture so we can partition
the feature space into some units for
identity pitch elevation and given two
images say of these cars we can take the
pitch in elevation from one and the
identity from the other concatenate
these and then decode we can form
training tuples like this and learn a
disentangled representation in this way
we can combine disentangling and analogy
training so we could do analogies let's
say on the pose units and disentangle
from these pose enos the identity units
we could also incorporate an attribute
classifier so maybe for some problems
like generating these 2d video game
sprites there's lots of different
character attributes like what colors
their hair their skin what kind of armor
they have what kind of weapon they have
and so you can actually disentangle an
attribute vector from opposed embedding
and again combining classification in
analogy training so in experiments the
first thing we tried was this shapes
benchmark where we have shapes of
different colors sizes rotation angles
and we want to do analogy making in this
setting so in this slide I showed the
predictions using the additive model
where the transformation function is
just vector addition and the first two
columns showed the example
transformation the third column shows
the query image and then we show
repeated analogy making we find that the
additive model is a good job for scaling
and shifting but it really fails for
rotation the multiplicative model
a little bit sharper for scaling and
shifting but also doesn't do a good job
promote a shin but the deep model where
the transformation function is an MLP
actually succeeds pretty easily for
multi-step rotations and quantitatively
you can see that as you increase the
number of steps the prediction error of
the deep model stays pretty low while
both the other models increase a lot
quantitatively for other analogies like
scaling in translation we see that the
deep model does much better
multiplicative is somewhat better but
not a lot better than additive here are
some videos so we can i'll show some
rotation here so we can actually apply
the analogy in the forward and the
reverse direction just by changing the
sign of the transformation vector you're
scaling translation we can also inner
leave different analogy so you can do
scaling and then translation or rotation
and then translation and just interleave
the application of these repeated
analogies and also note that this was
done all using a single model so we
don't have like one model for each type
of transformation it's it's one it's one
network that does all this stuff okay
that's four shapes here are some for
video game characters so here the task
is take the reference animation of this
character let's say walking and then
apply that same trajectory to a query
character and notably the query
character can be viewed from a different
camera angle actually but we should
still be able to generate the trajectory
by analogy so this is walking thrusting
a spear casting some kind of spell so
you can see it's a little bit there's
some pixilation but it gets the idea so
the way we do that trajectory transfer
is every pair of frames and the
reference we can get a transformation
vector and we can just apply this at
each time step to the query starting
from the ground truth query frame we can
just carry this forward and walk along
the manifold in this way
okay quantitatively we compared just the
additive model with disentangling models
and disentangling with also an attribute
classifier for doing animation transfer
across these different animations and we
find that having an attribute classifier
that's disentangled from your post units
gets you a big one quantitatively we can
also do extrapolation of animations by
analogy so what if you formed training
tuples of analogies where the
transformation is moving forward or
backwards in time so you could just the
mind training samples that way and then
we can fund we find that if we give an
example pair of images where the
transformation is just stepping one time
step forward we could take that
transformation vector and apply it
repeatedly to query images to actually
advance the query animation even though
the query image is from an unseen
character so we can extrapolate along
the animation manifold we also apply
this to cars we find that we can
disentangle Karpos and appearance and we
can get both good discriminative
performance and we can generate cars by
taking pose from one car and identity
from another combining them and then
re-rendering so in this figure we
connect the pose from this column the
identity from this column and then
combine these and generate the predicted
car image we can also do repeated
out-of-plane rotation analogy so in this
example we have a reference card that
rotates in some direction to produce the
output car image we can take that
transformation vector and apply it to
this query repeatedly in the forward or
reverse directions and i can
successfully to some degree hallucinate
what the car would look like after
applying this out of plain rotation so
in conclusion we provoke paper
a novel deep architecture that can
perform visual analogy making by just
doing simple operations in an embedding
space so this convolutional encoder
decoder Network it can effectively
generate the transformed images rather
than just retrieving images typically
analogies and we found that
interestingly vector addition is that is
enough to model transformations for some
simple problems like scaling and
shifting shapes but for other problems
like multi-step rotations you really
need multi-layer networks to
parameterize to your transformation
increment in particular when the
increment depends not only on the
example transformation but also on the
current position of your query and the
manifold and rotation is an obvious
example of that and finally we found
that we can combine this analogy
objective with disentangling and this
can overcome certain limitations of
disentangling alone by learning the
structure of this transformation
manifold thank
we have plenty of time for question I
want to go to any of the microphones yes
right there right this is very
interesting thank you for describing it
so well I'm curious when you generalize
to when you generalize one movement to a
new character does that also work when
the neural network has never seen that
character doing anything at all or does
it require that that character has been
seen in at least one form or another in
the training set we have disjoint
characters for for training and testing
although there is lots of regularity in
the characters so for example the
characters they may have like a
different hair color or different style
of basically a different combination of
attributes than any that you saw during
training but each individual attribute
like there was probably there was a guy
with yellow hair like that in the
training set but not combined with all
the other stuff ah so how many different
characters would just say are in the
training set we generated 672 so we had
seven different attributes and whoops
yeah we had seven different different
attributes and said that the attribute
vector was 22 dimensions in total when
you count all the choices I have to
slide somewhere listing this but yet in
total we had section 72 and we took I
think like 500 for training and the rest
for testing we used the data set we use
this thing called liberated pixel cup
it's all these like video games bright
assets and we just generated the many
many characters but obviously we could
do way more than and we did yeah very
nice so the you basically generated a
big cartesian product of characters
right took a random selection as testing
data right very nice thank you
thanks another ? around yes it's very
nice work and very nice talk thank you
and my question is if you think that
this will generalize to natural videos
as well and why we take two to do so
thank you Oh for natural videos
potentially yeah it's definitely a much
harder problem compared to synthetic
data but there is a lot of training data
444 videos and you could probably
generate a lot of training data like
mining analogy tuples in an automated
way so yeah I haven't worked on that but
it sounds interesting thank you all
right there are no other questions let's
thank our speaker
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>