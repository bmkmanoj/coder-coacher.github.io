<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Practical Learning Algorithms for Structured Prediction | Coder Coacher - Coaching Coders</title><meta content="Practical Learning Algorithms for Structured Prediction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Practical Learning Algorithms for Structured Prediction</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GSP6D3w43NE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it is our pleasure to have kai way chang
here from UIUC and he's going to tell us
about practical learning algorithms why
they are laughing yes yeah right so
sensible introductions so emotional in
technique has been widely use in many
applications right so in nature language
processing in data mining in computer
vision people use a lot of machine
learning however often when you want to
train an accurate model you need to have
a lot of data and you want to design
expressive features and also consider
the compare structure embedded in the
problems and open this caused
instability problems right so to make
machine learning practical we want to
have an efficient and effective
algorithms so I think the stability
problem of machine learning come from
two direction one is a data is too big
another is the structure embedded in the
problem is too complicated so if the
data is too big then even training a
simple linear classifier could be a
problem so for example if you look like
the splice site data set to load the
data you need 600 gigabyte memory so if
you don't have the enough memory and you
work out of trouble and only another one
that if the data if the structure
embedded in the problem is too
complicated and often you have a little
complexity of your influence and
learning algorithm would be increased
and then you also got a problem so let
me show you something simple so this is
simple show you that making a joint
predation and consider the complex
structure of the publicity photo right
so although you cannot read these
Chinese you know these signs should not
mean slip and fall down carefully right
if you're mean PK
don't slip and fall down but if you look
at the local predation it's pretty right
so this word means carefully and this
word means slip and fall down but when
these two things combined together you
got wrong so here is the more concrete
example showing why mobile edition is
important so imagine in this task we
want to identify the entity type of the
noun phrase in the sentence and also
recognize the relationship between them
so assume we have a local classifier
that can make individual predictions and
along with the competence value so the
blue one show the prediction with the
highest confidence value and you can see
that this local petition exam of the
fears mistake for example the second
argument of this own in relation should
not be a person should be a location
right so by considering that we have
more confidence on this poem prediction
rather than in this person prediction we
can use this information to fix our
prediction and then we can get the
correct prediction here and similarly we
can also fix our prediction here so
overall if you consider the global
assignment rather than consider
individual assignment you can get the
wrong two to the five percent
improvement in this date house on this
data set oh just go so here you have a
bunch of entities each entity you're
trying to guess whether it's a person or
location yes yeah we have a Motec but
it's just a restoration okay oh you're
more time and then you would normally
have a relationship between e 1 and e 3
2 possibly yeah possibility and also
there is that you rather than relation
so if then arriving us pretty right
right so however I mean if you make
goble pretty sure you can get a more
accurate model however if you consider
all the combination of this individual
perdition then the office space can be
immeasurably large and this cost sounds
capability problem okay so
the high level view of my research is
trying to design particle machine
learning matter for big and complex data
and ideally the measure should have the
following property that is theoretical
motivated it is efficient and effective
in practice and it has many applications
and the application currently I'm
looking at is NLP applications because
NLP problem usually deal with big and
complex data in nature in most of my
world are inspired from the observation
that learning is often consists of a
mini small component for example way oh
yeah nature angry processing Thanks
right so so usually when you make global
prediction the predictions based on
several local predictions and also when
you want to train a model usually the
training process involve a lot of small
updates so that if we can define some
method that we can select and catch
those informatik component you in the
learning then usually you can be used on
computations or simplifies the problem
and this can make learning more easier
and also reduce the training time so my
current research contribution can be put
into the Spain defined it by these two
directions so in one direction is the
size of the data so a measurement of
this is a what among of a memory you
need to load the data and another
direction is a complexity of the problem
and the measurement of these is what
amount of time you need to do inference
so I start from looking at simple
problem but with some medium medium size
of data so I studies on optimization
measure optimal optimization math method
the code indecent matter for bunch of
linear classifier such as linear svn or
logistic regression and at the end we
increment all matter in a library called
lab linear and this like this library
has been widely use and then I started
looking at
Robin that the size of theta is stupid
to fit into a memory so then we need to
design we need to find some better way
to deal with the data and at the end I
propose a method called selective
brought minimization matter such as I we
only need to load the data into a memory
once and then we can get a model as
accurate as you loop it loop training on
the data in when to a pitch PhD I also
working on the structure learning model
so I study the how to speed up the
training of the structure prediction
model in general and also looking at the
space of civil case that the structure
is too complex so we don't have a
credible inference algorithm and I will
talk about the word here in this talk
and I also working on trying to learn
the Latin representation of knowledge
database so in knowledge database will
recall the relation between entity in a
tensor you know like three dimensional
matrix and then we can able to use the
tensor factorization methods or like we
can then the embedding of entity as a
factor and the embedding of variation
isometric so this is a word I'd done
with a Scott E and Chris me doing my
internship in the Microsoft Redmond okay
so as I say in this talk will be focused
on the structure running so I will go in
to talk about some background of a
structure learning and then chill some
general strategy to speed up the
structure the new model and then she'll
use on specific modeling for a specific
case that is an online Kasserine problem
ok so in structure prediction task we
want to make joint prediction on a set
of output variable and trying to
consider the joint loss so usually those
interdependency can be modeled as a
structure so for example in the p
always taking task the employees are
sentence and the output is the linear
chain and incur Evans resolution that
input is that a paragraph and the output
as a caster in structure I an
Independence passes in the employees
also a sentence and the output is has a
tree structure and in sacrament ation
task that increase like a picture and
the output is a set of a binary variable
representing the segmentation so roughly
speaking in structure petition task we
want to make joint predation by to
minimize the joint loss so in another
word we are trying to learn a skull
function and the skull function can be
pero a param met relized by a wetback
the W and can be defined as a linear
scoring function so that we can use this
scoring function to assign the height is
go to the better structure over here I'm
the structure i mean the output
assignment right so you can define a
feature that based on both the input and
the output so in this example just so
you're trying to figure what's a noun
and a verb and all this and then you
have a score based on how many of those
words you get right yes or is it a score
based on like whether the whole thing is
right or how do you score so so in this
case we have a local feature so the
local feature all these that square is
that a feature so if you get this right
and you get some feature and you get
some weight for this feature and the
total score will be the sum summation of
all these wait times of features ok so
it's just that a binary case it's not
that some mistakes are considered worse
than others are you get extra points for
getting hoping right or anything like
that oh here we are talking about how to
scout the structure rather than how to
define a lowes so we in another place we
gotta find the ropes right so as you say
that in this case taking tests we have a
sentence we have a part of taking
assignment and we want to design the
feature based on top of
and and well if you don't hmm this is
that a transition probability this is
that emission probability I so for each
d square you can define a feature I'm
sorry I think I'm tasking Adams question
again but so are those weighted equally
I mean if I I find the constant versus
finding that oh now they're given the
weight no it's not sweat equity because
you have a way back to here so you wit
base on this Way's answer so I start up
in a binary classification case you have
a weight for each feature ok so those
w's or something that you're given this
party you need to run based on the
annotated data right so you know
trending you're trying to learn this wed
factor and in the inference assume you
have the web factor you're just trying
to predict the best score in structure
and a usually we can define some
constraint to reduce the size of the
output space but they for the day as the
output space is usually is financially
large and for some specific structure
that in your chain structure you have an
efficient than the algorithm an
inference algorithm that deter p
algorithm all that tree structure you
have a CKY but for the general structure
you can you can comfort lease into a
linear integer linear programming
problem and solve this by LP solver so i
will talk about that later okay so so
the learning is basically trying to find
the tableau that can if the better
performance here so to estimate the
weight factor if you use the online
learning algorithm this is a thing you
want to do that would at each time you
would is received one training data and
then you use your current model to put
it on this data and then you compare
your petition with got a notation and
use this information to update your
model so if you write down this as a
math and this is a thin you can look at
and
is similar to perceptual algorithm the
main difference is that in this first
step you need to do inference and in
solving these influences usually
expensive so then the bottom layer of
the training will be solving the
inference here and you can also use a
batch learning algorithm to train the
matter back to physical exercise of the
senses the why eyes are the labels of
the parts of speech for each word in a
sentence and when you say in French
you're saying it it's computationally
tricky to the attitude someone gives you
the parameters to find the most likely
assignment of parts as each psychic word
a sentence it's just to be clear like
when you say why is structure it has
exponentially exponentially large not
mean that its length exponentially large
you mean just basically even if you to
add just even if you are just two
possible labels for each word you will
already be exponential that's what you
mean yeah I mean because you need to
consider all the combination of the in
the future assignment right so right so
so if you had like a n words day and
each one of them could be available now
you already will elect to today and
possibly recycle it exactly right but
they're not linearly separable because
the prediction tool makes it so that
there's not likely to be multiple
sentient like verbs in the sentence that
the idea to consider all the
permutations basically or well ideally
as you want to consider all of a
mutation and you're trying to learn the
way back to such that you can
distinguish the good structure on the
best structure
okay so so if you use the better than
the algorithm to learn the model then
for example you can use chacha SVM so
this is similar to the svn folder binary
but this is in a structure case so in
this case the constraint is basically
saying that for all the tempo and all
the feasible structure you want the
skull assigned to the cost structure is
better than the school assigned to any
other structure by some loss functions
and if this is not satisfied and you
will introduce some slack variable and
add this into the objective function so
the objective function basically has a
to turn this is a regularization turn
and this is a penalty 10 and c is the
parameter to balance these two turn so
it's basically like the spm and there is
already bunch of optimization method
proposed to tell to solve this
optimization problem and all then
required to solve many inference poverty
during the running okay so there's a
scoring function right even a sentence
and a bunch of parts of speech and
that's some arbitrary function yeah and
that's why it's hard to figure this out
because you have to consider all
possible assignments and sizes of each
yeah for figuring figure out the best
one here you don't deal with it you just
say well somebody gives me you're
ignoring the computation here and you're
just defining what yeah ideal would be
that the best the true labels you have
true labels are better than any other
passing lot yes so so you can consider
that a constraint carries is that useful
measure the number of a constraint
because here is for all the tempo and
all the physical structure so you have
an explanation a number of a constraint
what are the constraints like I mean the
content of this optimization problem for
each such that I say oh so yeah so
that's why people put post using a
cutting plane matter to
yeah so the idea is basically we want to
use an inference problem to select those
comes frame that is more relevant to
them so so that you can reduce the
search space and strange is a comparison
between the correct labels and some
possibly incorrectly okay so so figure
series the basically saying I need a
separation or kill and the separation
okay needs to solve the inference
problem to zoo mm-hmm separation sorry
Oracle oh because you have exponentially
many constraints so you try to solve
this you need a separation of a kennel
and yes situation or kill requires to
solve the inference actly because I want
to find the best assignment okay so this
is a background and I'm going to talk
about a strategy that can speed up the
training process so as we see that no
matter you use online any Aboriginal
page learning algorithm you need to
solve a lot of inference but is 16
measure usually treat these separating
oracle eyes up red box and but if you
look at the inference problem we are
self during the training you will find
that most of time the inference problem
has a sense solution so this makes sense
because if you consider the possible
assignment assign I've been part of
speech heck assignment assigned to a
center with five word there are no many
possible assignments although the
structure page or I mean or other search
space is immeasurably large and also
another reason is that because we are
going to train on the same data several
runs and the model comforts quickly at
the first few rounds so when you see as
a simple that we have tell you with
before you easily to give them the same
solution so so that its many inference
problem well share the same solution and
this work is trying to it's prolly this
redundancy I catching the old inference
problem so that we can set of change
them right yeah very doing is showing
learning so you
remember no solution which your family's
change then yes yes but you parameter
change but you are getting comfort as a
parameter world only change a little bit
and if the solution is not very
sensitive to the parameter change then
you are getting the same solution that's
what I mean and then how do you okay
maybe you'll come today yeah yeah that's
the main part of those so you want to
verify that this is the case right sure
right so here is a key result basically
we can use much less in frisco to obtain
and design model so your roast so this
figures show the number of inference
code along with the iteration in the
training so the low matter basically
called an inference over every time so
this is a straight line and this is a
number of that this thing in France
solution you will observe during the
training so there is a huge gap between
these two all right so we propose to
version of our method so this matter
will give you the exact solution an
exact model and you can reduce around
twenty percent of an inference call and
this can reduce around ninety percent
for influence call while you gather a
possible solution but the bottom line is
that if you look at this level what if
bottom was like so what is this orchid
line they like the bottom of slide this
one apart on this one although Oracle
means the distinct number of an
inference solution you observe in the
training ok so I think this is if any I
shall describe your cycling to the data
updating your parameters and your work I
was just sort of telling you do you have
a news a different labeling when you
come to the same sentence I see but you
couldn't predict it in advance so you
need to do more work so that's why like
the pink or whatever the other lines
over i own a Lou yeah you read a career
I you change your perimeters you realign
your learning algorithm with the season
retrospective Lupe's discover that it
was the same one so but you still like
even if even if you know it's the same
one that doesn't tell you you can still
that's in hindsight now letting hunts I
so that's called Oracle don't
Lee you can't verify that in order to
say that this is the correct one still
requires a lot of work right right right
this is why the other lines are above
the hook alike yeah of course I Oracle
is type in the high signs of you and he
are you gonna tell us what this aprox
algorithms doing in a moment whore yeah
yeah you don't know what yeah sweet are
you just gonna say i don't know if it's
worth wasting your time or not but I I
just have no I'm an economist and I'm
relatively technical among the
non-technical people here and I'm
completely lost Oh as it what's going on
I don't know if other less technical big
moments what you mean by inferences oh
the universe here I mean to make
predictions on the training data so in
the thing that you get a data and use
your current model to make a prediction
is basically you assign the value to the
output variables and you're saying that
even doing that takes a lot of computer
that one take a lot so you're trying to
the goal is to reduce the number of
times you need to do that or increase
the efficiency of doing each of those
inferences I want to do the first one I
want to reduce a number of time I need
to call the imprint sulfur and the
inference over what is that like could
that take any form going to be an SVM
lots of different algorithms or is it
restricted to some class of predictors
or just like the well you can think
about is like a black box so whenever I
give you a model I give you this Oberon
and they keep me the solution okay so
the the loss function has this
parametric structure that you put up but
the inference protocol could be non
linearity 'try good anything anything
yeah and in your example the inference
was even the parameters your model in a
sense what are the parts of speech of
each word drive sentence yeah so my
example here is the inference that Peter
be learning always difference algorithm
so trying to assign the label for a
sequence
just roughly speaking when you run your
thing because you save about ninety
percent of the 31 10 times past oh no is
that twice time so fast because you also
need to spend time or update in a model
also to thank you verify i will show you
right so basically this amortize infant
firm world as a two-component so first
we need to have a general general
framework to represent the inference
problem and then we want to have the
condition to check if the two inference
problem at the same solution or not
alright so if you have this too it's k
when a new inference problem come in you
can just go into your cash to look at if
there is a old inference problem has a
sense solution has a new one and if you
can verify that you just return the old
solution otherwise you can you can call
your best sulfur to solve the phone
right so the trick here is solving a
problem is usually expensive but verify
the condition is simple so then you can
save a lot of time here does it have to
be exactly the same or does it is there
some notion that things sort of hash too
so forgiving by exactly the same you're
checking whether it has exactly the same
oh yeah as I will talk about that person
right so so now I'm going to talk about
what is an inference print were I use
and what is a condition so we use an
integer linear programming as a general
representation for the inference problem
so basically you have a binary variable
Y as to represent each local prediction
and then you can have a linear
constraint to describe the
interdependency between these Apple
variables right so then you have a skull
for each individual little components
and you're standing up you're opening
the black box I am becoming on purpose
yes
right so so this film will has been
whether you use in many MLP and fission
has including that dependency parsing or
sentient compressions so basically fear
we say that you can represent any
maximum probability estimation problem
with any publicity model using this iop
framework so this is a very general
framework so well a signed note here is
that although we require to represent
the inference problem as this i LP
problem but you don't need to solve a
problem using iop sulfur right in fact
we only need to use these framework to
verify the two problem has a sensor
which are not but you can solve the
problem I thought ever you want but I
mean whatever matter you want okay so by
this way we have a way to represent our
inference bobbin and now I'm talk about
how to design the condition to verify if
the two problem has a same solution so
basically we need to have a to condition
the first condition require this to
inference problem has the same
constraint and send number of variables
so this is easy to satisfy because when
you change your web parameter you only
change the coefficients of the ILP
problem in the cost constraint I'll
remind so for example if you look at the
entity reaction test I talked before the
number of equivalent cast is only 15 and
you equivalent cast that I hope people
has the same constraint oh the same
number of non phrase in the sentence in
my entity relation is simple well forget
forget about that so Tracy it's just
really simple show you that this comes
usually we only have a small set of
equivalent cast and in each equivalent
has the ILP problem has a sense set of
constraints which is so
sentences with they say the same number
of words are equivalent is that like
it's not the same number of a local
petition you need to take is equivalent
in this holds trivially for the noun
phrase for the part of speech example
because yeah the sentence didn't change
how many words it has so younger number
of variables you have is the same
somehow right and also the constraint
doesn't change because a constraint only
describe the interdependency right so
it's okay right and the second condition
is that we want to check if the solution
is sensitive to the change of the
coefficients all right so well I don't
have time to give details here but
basically you can use this condition to
check if the solution will change
because of change of co efficient so
once you have this condition then you
can apply all model so you can just
replace the player pass by these things
and you can immediately see that we can
reduce around twenty percent of the
inference code during the training and
as you said we can actually introduce
some approximation by relaxing this
constraint so we can relax this
constraint by allowing the epsilon
tolerance and this will give you some a
possible solution we've done a positive
male ratio and if you use a dual content
is that matter to train the linear to
train the structure spm then you can
plug in these approximate inference into
the denim framework and then you can
prove that the algorithm will stop and
when you stop the empirical risk is
founded and also you can use in the
property of a dual coding distant matter
so that when you use the adaptive
epsilon then you can get an exact model
okay so when you say it improves things
by twenty percent of you know
number four in schools what what data we
using to assess I'm talking about the
entity rotation they tested okay so yeah
elevator said is a particular date has
it is the entity relation your client is
at opener so when you relax this
constraint in number two is that really
so you're catching more things now yeah
you are catching more things that's why
you have the right yeah so you lose
something in the accuracy here okay so
this is a end result we have so you can
get an exact model using an adaptive
epsilon and you only require a long 25%
of inference cool yes sorry what's in
place to evaluate is there move or
you're evaluating so that you have some
sense of how loosening that is affecting
the accuracy of your dad I mean you to
find the lows see you get to define so
the empirical nose is is patek and I
mean the low so in the training data so
people usually use having ten loads and
you can mask so in there in the 1300 you
are that's right so in the part of
speech one is it just a number of parts
of speech that you got wrong right you
said this was a known this was a verb
this is whatever you come you're
checking against assistance yeah so
somebody hand labeled it as the ground
truth oh is that right yes and then you
see what all cases you're gonna have to
have some annotation yes oh that's why
you ask cool okay so the good thing is
we i am ok so if you use a possum a
sleeping with those who today even
though the computation that happens
after we're not painted by the crowd you
think the crowd could label things as
nouns and verbs and stuff yeah yeah sure
yeah is that
yeah so this part is just that for the
computation yeah so so this is show you
that the training camp can also reduce
because of yourself a lot of number of
inputs code so this is a first part of
the talk what was that should show you
look at the drop in performance and it's
about the same oh so for this three they
are you stuck model so performers oh I
cyclist in this one the performance is a
little bit different it's a little
higher yeah very small right so the
second part of this talk is how to
design a specific model for the problem
with a complex structure just go back
that for one second so they're both bad
that you put that square and their get
the computers to not risk yeah so that
shows that your algorithm the
approximation didn't change your
performance fun yeah one what do those
mean yeah you explain those numbers
whatever well yeah no this ones are like
why is the first one number smaller and
the that moves bigger like is a
boxing listing is better it's an
empirical data so everything can happen
so happened that the approximate thing
was better than the exact yeah it's
possible so so epsilon here is the same
one that controls the relaxation of the
constraint way right so how do you adapt
epsilon 0 so this tab Epsom is that you
use a large epsilon at the beginning and
you trying to reduce Absalom it's great
really at the end you'd use the also you
just of the exact means for you change
your venue oh yes we change manually
yeah we've sung to me
okay yeah so I go to a second part so a
motivation of this part is doing the
currents reach solutions poppin so
incoherent resolution give you a
paragraph you want to identify all the
noun phrase that we proceed the same
entity in the real world so for example
in this paragraph Bill Clinton president
of the USA and he are reference to the
same guy so you put them in the same
custard okay so we can model this
problem as a supervised Kasserine
problems so assuming we have a lot of
this kind of paragraph with a code and
notation and we want to learn how to do
this Cass serene so this is different
from the unsupervised clustering task
but people usually have a similarity
metric but here we also want to London
seen similarity metric along with
learning the clustering with some other
algorithm which some other person came
along and broke it down into the
entities you're not looking at the
problem oh yeah yeah we assume we have
an entity Ethan yeah right so so the
test is basically to learn how to design
a custom function to form function in
data alright so as I say that we can
define the similarity match free between
dimensions and then use this similarity
metric to lend a scoring ola caster and
then we can permit relies this Samantha
my tree by the web factor with the
feature so the feature can take them
from that a strain similarity or
position of the mention in the paragraph
so just presumably he is supposed to be
equally similar to Bill Clinton and
Vladimir Putin huh so right so that's
there's a hot oven so so how do you
decided he belongs to Clinton or not to
put it right so we have a feature like
the position in the paragraph so you
know they are close to each other then
cuz then then the poutine and well it
also in other cases
so you know the gender of the sky so if
this is a woman this is a man and you
know he should be doing to this one and
also you have other feature 250 scribe
that seems okay with the similarity may
function so its input is not just it's
not just the name big lintel it's also
like the position inside is a is a nice
balance in to show that right so here
you you simplified it because you said
oh yeah yeah the similarity between
those references it's actually involves
a lot of other features oh yeah I didn't
put it is also including a complex not
just these two work in principle if one
had a large corpus one could design
features based on site information yeah
like if New York City and the Big Apple
were frequently co mentioned in other
parts of the corpus yes then in the
context of the present sentence even if
they are not near each other in the
sentence you might learn that they have
a high similarity so not just does your
method incorporate that so we kind of
have the feature at this kind of
information so we have a feature from a
system call Andy seem so a missing is a
season to measure the entity similarity
by just to the written word in some
graph as knowledge graph so in that case
I guess my cover some kind of feature
you mentioned but maybe not oh so that's
a similarity matrix that someone else
previously produced by adding machine
learning algorithm and now it becomes a
feature in your algorithm right right
right thanks okay yeah but I mean here
we talked about that modeling so you can
use any feature you like okay so right
so so that the goal here is that we want
to learn a Custer in scope based on
these the similarity matrix and so that
we can do the prediction of a cursory so
one straightforward way to design the
kasirene scope is just used although
with in class appear and then you just
sum up all the similarity between them
so for example in this case if you get
Katherine that this then the Custer
school is just the sum of older red edge
and this blue H
but if you do this then the inference
problem of this is basically a creation
or catering problem so people know that
this is np-hard in it is incredible so
in the literature because we cannot do
this or people just using some heuristic
so what lady is that it a couple the
lending of a similarly symmetry one
doing the cast her in so they basically
use a binary classification method to
learn the similarity metric and then use
a greedy learning a greedy algorithm to
do like a stirring this problem you
mentioned is when seems very similar to
Mac Scott oh right i mean if the only
likely he only had two types of edges
where two things are you were similar or
dissimilar then in some sense it's like
Max to lean on me so it's yeah it's
problem that has like an efficient
approximation i'm going to me yeah maybe
maybe the thing I'm going to say
signature fotosearch yes so I mean to do
the binary classification busy people
get a positive data from all the with in
ages in a coke a serene and get a
negative data from the across ages from
the gold car serving so then you can use
that to train a similarity metric right
in and then and then you can do the
gritty clustering in the inference time
that these so basically you just go from
left to right in the inference time when
you sit on some mention you look at all
a mention in your left hand side and
then trying to find the one with the
height is scope and then if the school
is started in some Street hood and you
put this mention into a sand castle as
in this nation otherwise you create a
new custard okay so if you do this then
basically this create a left linking
forest such that
all the mentioned in the century are in
the same cluster okay so but as you can
see there is a obvious drawback of this
apartment of this heuristic because when
you do the learning it is decoupled from
when you do that inference so the goal
here is that can we learn a better
similarity Matri such that we can do
better when you do when you do the
inference so so that's why we propose a
best left linking later model such that
we can we can learn jointly than the
similarity measure with like a stirring
so the idea is we define a Latin
structure as this left linking forest oh
sorry at this left-leaning forest so
that we scoped the Custer by using the
edge in the forest rather than although
within cast up here and because this is
a tree structure so inference and
learning in this structure is easy so
then you can use these to jointly learn
the similarity metric and clustering
okay so here is a live demo you can go
to this web page so give a paragraph we
can generate a Latin left linking forest
to represent the conference cluster and
the good thing here is this tree
structure not only gives you the custard
but also give you some reasoning of why
we generate this custard so for example
you can see that los mention has more
information are usually in the middle of
this tree structure and I pronounced it
usually in the leaf of this structure
because they have less information and
you can also see that these two guys are
linked together because they're share
some string in their representation ok
ok so remember remaining question is how
we learn these things so recall that we
have a notation as a customer
structure but now we want to use the
tree structure to represent the kasirene
so if we want to learn this model we
used need to design a define the trees
story the Latin forest as a latent
structure so what me is that when there
is a new data coming you are going to
use your current model to predict on the
simple such that you can generate a
latent structure is that is consistent
with the gold kasirene structure and
then you can use this and you'll put
this structure to do the update and
mathematically you can write this down
as a latent structure perceptron
algorithm and also you can use other
latent structural model to learn these
kind of things ok so another extension
of this is that I say that now we use
the best left linking structure to skull
like a serene so a natural question is
that can we use a distribution of all
the left linking for we straight well
then you just use the best one in the
answer is of course yes right so you can
basically skull the probability of the
kasirene by all the left linking for is
that consistent with this clustering and
the skull of forest is summation of all
ages in the forest and after you have a
definition of this probability you can
just use the regular eyes maximum
likelihood estimator to learn the wet
factor so it's like to stand out way
people learn about beauty model ok so
for an empirical result you can see that
if you just do a local learning you can
sub optimal performance and if you use
this best left linking tree you can get
a jump in performance here and if you
consider a distribution of a Latin lift
left linking forest rather than just one
for which you can also get some and
comparing to other model when the paper
is published our message is all
performed in other models and these guys
also you see
simulating structure as us so we both
get a good performance here okay so so I
also found some other work along with
these structural petitions so far
algorithm perspective I also propose
another peer learning algorithm for
solving structure SPN and from the model
in perspective I also looking at SEMA
supervised learning for the which one so
he is the one we fight before this one
yes so so you improve things you saying
basically from 61 percent sixty-three
percent and since then like what was oh
you mean yeah so this problem has been
studied at 15 years so people readies
for a lot of even kind of method to tell
them with this and although most of them
have you risky but they also get good
performers and recently in 2012 the NLP
guys decide they want to have a shirt
ask so that people can come here with
all the matter so this is a data net
from that shirt ask and right now you're
like the right now you result is the
best that's known for this test and not
for now I suppose this wrong 2013 and
now people also proposed other measure
but the interesting thing is the
following words are all useless layton
kind of tree structure to then the model
yeah but they kind of using different
feature and consider different phenomena
in the increased this year whatever
numbers that is getting justice I guess
the so people they also change the the
smh they also change the performance
metrics so it's change a little bit
almost get along as 60 in the new
performance metric and the staff is at
61 there's an obvious like the chiasm
right so as I say I also do some
modeling word that this is done with
Kesey in the in when I was a mentor was
the intent of in we do the Sigma
supervised learning for structure
prediction problem with complex
structure and also i walk on the search
based structure prediction model in this
summer with jungle infer and hot on main
so if you are interesting i can talk
about that offline yeah so most of my
thing are applying an LP task i also
study other LP tests and also released a
package and software demo right so this
is sly is the overview of all the
research I have done so far and in the
future I want to also work on this
direction to design practical machine
learning tool a matter for large and
complex data so from the machine
learning perspective I want to design
the matters such as I can tell we speak
and large data and can end all supposed
several applications and from the
application perspective I want to study
the application and use that to inspire
the design of the algorithm alright so
there are a few project I'm working on
now and also in the nearly future that a
search best petition so these two are
like machine learning project are trying
to design the new matter for general
Dayton structure model or such a
structure prediction and this too is
that application wise study that we
trying to apply all matter to relation
instructions and also on cost document
or reference so this is a corrosion
problem that not only consider the
entity in the same document but consider
and disclosure documents okay so this is
all the people i have collaborated with
i guess you can recognize south name for
microsoft and in conclusion basically i
think the ski scalability problem agenda
because of the data is too big or too
complicated and I already studied
several idea to deal with this
scalability issue that I can do the
lighting caching or parallel listen to
reduce the training time and I talked
about two different work that I have
found during my PhD and hope I can do
more in the future and all the code and
demo can be fine in my webpage yeah
thank you I wanted to understand I thank
you for your top it's very helpful and
clear as a non technical person I
understood let's go I think they got
more than that I wouldn't understand if
you can apply this to segmentation
problems or dependency problems what
that would look like or why those are
maybe not part of your columns oh so I
mean I'm proposed to general thing work
but they are given kind of problem so
whenever you have a problem that you
want to make joint prediction of kind of
interdependent variables you can apply
this matter do you see any challenges to
app to applying this approach to this
under those other issues of segmentation
too did you see anything that wouldn't
you want like that segmentation example
of the bird in the background image some
equipment tech image segmentation did I
not apply this to image segmentation or
could you buy this I don't know I mean
we can deprive for that I mean people in
the computer vision literature they use
the short term model to do to deal with
math task yeah so even I think they even
have that implementation you can just
download
so um this is just more question about
the these kind of constraint met
constraint set methods here sort of
implicitly assuming these problems
aren't noisy in that you know this sort
of one right answer for each sentence
and it does really one right answer then
you have these constraints sort of work
but it's actually noisy you kind of
really have to consider multiple
explanations Ryan and and then well one
do we think there are real possible
bumps in performance you know like maybe
was ambiguous what the person who put in
that sentence was not to the algorithm
but you know as a person then somehow
the algorithm who needs to consider
multiple explanation you have an idea
how to deal with those issues you know
they don't give our computation I just
say sure I'd be busy and I do in France
and they're all explanation there was
not a very satisfying right so i think
yes there are certain cases that you
just use the individual assignment
should be good enough if the problem is
not ambiguous but yeah in in all case we
usually consider the case is that you
need to make over predation so that you
can get a dramatic but by over
prediction meaning that actually means
you're trying to deal with this by
inserting multiple modules explanations
yeah and well of course you can study
other perspective of the problem that
using a human effort I just you know see
you actually consider multiple possible
explanations when you're when you're
training
ah the human standards we're so i think
the the latest charger thing is more
like obviously so right you you have a
latent structure i say each latin tree
is wise mination of the Kasserine and
you consider a distribution and you can
see they're all the explanation yeah but
for the first part of general
stratosphere trigger general structure
petition framework I think people
usually assume you only have one best
best best assignment and didn't consider
different kind of is foundational
different output wait a bit maybe I find
that it's red just that you have won
best assignment it's that you know
there's noise in your training of this
as always gonna have one best assignment
write the issue is if two things that
sort of it's a quite steinberg so I
think one thing maybe i can add on is
that you can define your laws such as
you can treat with let those functions
so that you can actually model those no
easier article information dear i'm not
sure how to do that you do you do that
the ESL cases so when you said their
applications me to just question right
like if you're if you're working with
material and say poorly written so the
parts of speech example is a good one
your examples are a well-organized
paragraph but if i'm working with
someone who's written a poorly organized
paragraph then it seems like this
application would be really challenging
unless you're looking for patterns or
organization that are connected to other
language problems or language issues
like another
is that what is that way what is another
thing I'm doing but it's exactly
happened when we tell with that they has
there so in that data say is they give
you the ESL writing and you want to
correct grandma of this writer right and
we find that if you get a very poor
written task in all the puddles which
takes all the things are very bad yeah
so at the end we actually just using the
surface string as a feature to do the
things so that's this is interesting
research direction to trying to put the
others such as them
it's good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>