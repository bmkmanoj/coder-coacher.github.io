<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Directions in Computer Science | Coder Coacher - Coaching Coders</title><meta content="New Directions in Computer Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Directions in Computer Science</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0JnFsK3yEHE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">here today and and have the opportunity
to to talk to you about the the future
of computer science
yeah so so one of the things that that
is important is that you're in inter
entering the information age and just to
set the stage for my talk I'd like to
tell you just a brief story of my career
I I graduated in an electrical
engineering department in 1964 one of
the reasons I was in an electrical
engineering department is that there
were no computer science departments at
that time and I was hired by an
electrical engineering department at
Princeton but fortunate for me the
Chairman asked me if I would teach a
course in computer science and at the
time I had no idea what would go in such
a course because there were no textbooks
but what I didn't realize is that
teaching that course made me one of the
world's first computer scientists and so
whenever our country was looking for a
senior computer scientist I was on the
shortlist and one of the advantages of
that is at one time our the president
asked me if I would serve on our
National Science Board overseeing our
National Science Foundation I was in my
40s imagine if I had been in high-energy
particle physics I would still be
waiting today for the senior faculty
ahead of me to retire so that I would
have that kind of opportunity now when I
tell this to students they say you were
fortunate because you graduated in 1964
and today is 2012 but the message that I
want to leave you in my talk today is
that there is a fundamental change
taking place and if you position
yourself for the future you will have a
fantastic career computer science is
undergoing a fundamental change in the
past thirty years we were concerned with
making computers useful so we were
interested in programming languages
compilers operating systems algorithms
databases but and the emphasis was on
making the computers
useful but in the future we're going to
be concerned with what we're going to
use those computers for and the things
that you are going to be involved in is
tracking the flow of ideas and
scientific literature the evolution of
communities and social networks
extracting information from unstructured
data sources and many other uses like
that and what I'd like to do is talk a
little bit about the drivers of that
change part of it is the merging of
computing and communications and it may
be that the communications is more
important than the computing aspect and
just the wealth of data and digital form
the fact that there are networked
devices and sensors and this is going to
have fundamental implications for
theoretical computer science we're gonna
need to develop the theory that is going
to support these new directions and
we're going to have to update our
computer science education and so just
what I'm going to try and do in this
talk is give you a brief view of what
the future is going to be then I'll talk
a little bit about the science base
that's needed to support that future and
then I'm gonna try to answer a question
that I'm off it off often asked and that
is what does a science base look like
okay so one of the things that that Rick
mentioned in the talk that you just
heard is is the big data and just to
give you some sense as to what we mean
by big data we broadcast enough bits
that it's equivalent to approximately a
hundred and seventy four newspapers per
person in the world and when you stop to
think of it you know have have you read
your 174 newspapers today there is so
much data that that you can't even look
at it
you're going to need computer programs
to extract from that data the portions
of it that are of interest to you to
give you another example of the size if
you look at Facebook
they claim that they upload 250 million
photographs per day also one of the
things computer science is going to be
involved in every field of science
physicists have recently announced that
they found a new elementary particle but
one of the things that I like to point
out is that computer science played a
key role one of the things is they
looked at proton-proton collisions and
they looked at a thousand trillion of
them that number is so large that no
person the physicists didn't look at
them it was a computer program that
looked at them and found out which of
these collisions were of interest and
it's this kind of role that computer
science is going to play in basically
every discipline you can think of and
one of the the points that like to make
is that our current database tools are
simply insufficient to capture analyze
search and visualize the size of data
that we're encountering today so what we
need is we're gonna have to develop some
more theory and this is what you can do
during your careers and this theory will
be concerned with large graphs when I
went to schools the graphs could be
drawn on a piece of paper they had 10 or
15 vertices the graphs that you're gonna
be concerned with are gonna have
billions of vertices you're gonna be
concerned with things like spectral
analysis high dimensional data and
dimension reduction and one of the
things on this list which is of
particular interest to me is sparse
vectors and one of the things that I
would like to point out to you if you
ever when you start to look for a
research area if you're listening to
various talks in in different
disciplines and some idea comes up over
and over again it's probably fundamental
and so that's this this last item on the
list sparse vectors and the question is
what what is a sparse vector maybe it's
a thousand dimensional
vector but it's all zeroes for just six
or seven coordinates and you might say
well why are sparse vectors of any any
interest and the answer is is B come
because they come up in many different
disciplines they come up in tracking the
flow of ideas and scientific literature
in biological applications and sync
signal processing and what I'm just
going to do is take one application just
to show you in the biological area one
of the things you might be doing is you
might have an orchard where you have
apple trees and what you want to do is
try to develop some new apple trees
which have apples with some property
which is better than your current trees
so what you have is you basically have a
matrix each row in this matrix
corresponds to a tree in your orchard
the columns correspond to positions on
the genome and this this vector this
long vector here is an unknown and you
this the short vector way over at the
end is some observable phenomena that
you want to improve maybe it's how
bright red the Apple is or something
like that and you would like to know
what are the positions on the genome
that affect that give this particular
property well the way to do that is to
solve this linear system of equations
and determine what this vector is but
you immediately say well that doesn't
make any sense because you have many
more columns and rows and so there's a
whole vector space of solutions which
one do you want but it turns out that if
you ask for a sparse solution the sparse
solution is unique and this suggests
that we ought to understand why one of
the things you might like to explore is
the math what is the mathematical
properties that says that there's only
going to be a unique sparse solution
you can see physically why this is true
if there were too sparse solutions that
would say there are two different sets
of genes which give rise to this
property but evolution is thrifty and it
would not involve evolve two different
ways of generating the same property so
this is one of the reasons why sparse
vectors are important but also because
they come up in many other disciplines
I'm going to talk a little bit about
some of the things that you are going to
do some of the the databases that you're
going to construct in the
not-too-distant future we're gonna
digitize medical records and if I had a
an illness here in China I'd be able to
go to the doctor and the doctor would be
able to upload my entire medical history
and give me the best possible medical
care however I don't want my insurance
company looking at my entire medical
record I don't feel that's any of their
business the only thing that the
insurance company needs to know is that
they owe some doctor some money they
don't need to know why it's it's
irrelevant what medical treatment I got
the only thing that's relevant to the
insurance company is that they cover
this treatment and that they have a
rigorous proof that they owe this money
now also researchers would like to look
at this database and they would like to
look at statistical properties of it but
we want to design this database in such
a way that no individual information is
released and these are the kinds of
systems that you're going to have to
create and there's some relevant
research going on has to do with zero
knowledge proof sand differential
privacy and what I thought I would do
since you might not have seen a zero
knowledge proof is I thought I would
just show you one so you'd get an idea
that this can indeed be done zero
knowledge proof is simply a proof of a
statement that the statement is true
without providing
any other information and for a simple
example I'm going to assume that you all
know how to play the Sudoku so let's say
I want it to prove to you that I know
how to fill in this this board basically
what you do is in each row each column
and each 3x3 array you put in the
numbers 1 to 9 ok and I want to prove
that I know how to do this without
giving you any information whatsoever as
to how to do it so how do I go about
this proof what I'm going to do is I'm
going to write my solution on little
pieces of cardboard and I've Illustrated
here the the pieces of cardboard for the
top row and I'm gonna put them down on
the board but upside down so that you
can't see them okay and now what I'm
going to do is let you ask certain
questions you can say show me the first
row and what I will do is I'll pick up
the pieces of cardboard I'll shuffle
them so you don't know which was in
which cell and show them to you
and you look and you will see that I
indeed have the numbers 1 to 9 for that
row but I didn't give you any
information as to which number was in
which position then I'll put the pieces
of cardboard back down and you may check
a column you may check a 3x3 square and
pretty soon you're gonna believe that I
have a solution now I should point out
that there is is this is not a rigorous
proof in the sense that I've proved
absolutely to you that I have a solution
because what you may not observe is that
I may have put the pieces of cardboard
down back down in a different order than
I picked them up so all you know is that
with high probability I have a solution
and by asking enough questions you can
prove we can reduce the probability that
I don't have a solution to 10 to the
minus ninth or something like that now
you may say well this is a trivial
problem can you do this for a complex
problem and what I'm there's a problem
of coloring a graph with three colors
in such a way that no two adjacent
vertices are the same color and this
problem is a hard problem there's no
known efficient algorithm for coloring a
large graph okay so let me give you
suppose I'm in a business where I color
graphs and I want to do business with
you but you're a little nervous about
paying me to see the coloring before I
show you the coloring because you're not
convinced that I can do it and I'm a
little nervous about showing you the
coloring before you pay me because I'm
not sure that you're gonna walk off so
what we want to do in order to do
business is I want to give you a
rigorous proof that I have a coloring
without giving you any information
whatsoever about how what the coloring
actually is so what I'm going to do is
for every vertex in the graph I'm going
to take a little envelope and I'm going
to put in that envelope a piece of
cardboard of the color for that vertex
and I'm gonna seal them up and now what
you can do is you can ask me a question
just like in the Sudoku case you pick an
edge and I will give you the two
vertices the envelopes corresponding to
the two vertices you open them and make
sure that the colors are different now
I've given you absolutely no information
about the coloring because if I had a
coloring I could always permute the
colors so that those two vertices were
the color of the that are in the
envelopes but just looking at one edge
doesn't convince you that I have a
coloring you want to see the envelopes
for another edge but if I allow you to
see the envelopes for another edge then
you get some information so what I'm
going to do instead is I'm gonna take
all the envelopes and I'm gonna throw
them away and we're gonna start over I'm
gonna take a new set of envelopes put
the and I'm gonna permute my coloring
and put the colors in and then let you
pick another envelope another pair of
envelopes now you get no information
because the second coloring is a
permutation of the first and you don't
know that permutation and as long as the
two vertices are different color
you have a little bit more evidence that
I have a coloring but you have no
information as to what the coloring is
now after you ask me enough questions
you you get convinced that indeed I do
have a coloring but you might say well
this is going to take a long time well
well the way we do this when you do
something like this you're not going to
actually use envelopes what we're going
to do is we're going to agree on an
encryption system and I'm going to
encrypt the colors of the vertices and
then you're going to ask for the
encryption key for two vertices and I
will give you those two encryption keys
and since we're going to do this
electronically by means of a computer we
can do millions of edges per second okay
so so that's how these zero knowledge
proof swerg the digitization of medical
records is not the only system where
there's a privacy concern almost every
database that you're going to construct
this is going to have this kind of
situation so let me quickly talk about
another example having to do with cars
and roads quite often I Drive to a city
called Philadelphia and what I do the
route guidance system in my automobile
takes me along these red arrows and down
to where I get on this interstate
highway but since I go there frequently
I noticed that many cars turned off a
little earlier and took a different
route and in so I started to take this
other route and it was a little faster
and then I noticed there was still
shorter route and the question is why
did my route guidance system not give me
the shorter route and the answer is is
the route guidance system always keeps
me on main highways because they don't
know the condition of these back roads
but imagine if your automobile recorded
its GPS coordinates and every time you
went in for service it downloaded the
GPS coordinates and by using all of this
information the route guidance system
could see how local drivers what roads
they took and they could do a much
better
outing and imagine that if they just
reduced the amount of gas we consume by
one percent this is tens of billions of
dollars so these things are important
but there's a problem here I don't want
to download my GPS coordinates because
that would tell you who I am from where
my car is parked at night
it would tell you where I work from
where I park it during the day and it
would tell you everywhere I go and I'm
not sure I want to give that information
to you so again you're gonna have to
create a system which preserves this
information now in in the past
sociologists could study groups of
people with a few thousand individuals
but today we can study social networks
that where the entry see the interaction
of millions of individuals and I'm just
going to show you some of the things
that you can do one of the important
activities is how communities form and
evolve the the early work in this area
had to do with something called min-cut
they divided the graph in half into two
equal sized pieces where there's a
minimum number of edges connecting the
two pieces and there was a number of
assumptions people made but they were
thinking of small graphs with the graphs
that you're gonna deal with you don't
want to take a graph with a hundred
million vertices and divide it into two
pieces of 50 million what you want to do
is you want to pull out a community with
only 50 vertices and so how do we go
about doing that and one of the things
the early work on communities sort of
assumed that if there was a community
the people in that community had more
links to people in the community than to
people outside but that's not true this
this is a community the blue arrow the
blue area is theoretical computer
science and I put myself in there but
notice that I have many links outside
that community and because there are
hundreds of millions of people outside
community even though the probability
I'll have a link to someone outside is
lower than the probability of a link
inside just because of the numbers of
people outside I'm going to have many
more links outside than I do inside also
asking what community I'm in doesn't
make too much sense because I'm in
multiple overlapping communities and so
there's fundamental work to be done here
and I'll talk just about a little bit
about a work on finding communities so
I'm going to take an example where this
graph represents the adjacencies among
vertices and I made a very idealized
situation where in the upper left hand
corner everybody is connected to one
another and then there's another
community where everybody is connected
to one another in another and there's no
connections between communities if you
have a situation like this you would
find something which are called the
singular vectors and that's these
vectors way over on the right and the
way you would identify the communities
is you would map those the rows of those
vectors into a three-dimensional space
now this is kind of an idealized example
so everybody in one community gets
mapped to one point but if it was not
such an idealized situation if if
everybody in the community wasn't
connected to everybody else and there
were some links between communities what
would happen is you would get mapped to
an points which are clustered about
these these centers and then what you
would do is use use a method called
k-means to actually find the the
communities now one of the problems with
this is if communities overlap and now I
here I have two communities which
overlap what happens is you get three
clusters instead of two and in fact if
you had n overlapping communities you'd
have two to the n clusters so this isn't
going to work too well so instead of
using the clustering the rows what you
want to do is you want to find the
singular
vectors in the space spanned by those
three columns of vectors and once again
what you want to do is you want to spine
sparse vectors in the space spanned by
those three vectors and what that will
do is that will pull out the communities
so this is another example where sparse
vectors comes up and what you want to
find is you want to find the minimum
zero norm vector and of course that
would be all zeros so you require that
there be at least one one in there so
let's say you want to find a community
with me and so you take my coordinate
and say I want at least a 1 in that
coordinate finding the minimum zero norm
vectors is there's no efficient
algorithm for it
so we use a proxy namely the minimum one
norm vector and what I've described is
how you would find the global structure
of a network this is how you would
divide this network of a hundred million
nodes into pieces of maybe 30 million or
something like that but what we want to
do is modify this algorithm so we can
find communities of size 50 and to do
this I'm going to give you a physical
example of what we did suppose you had
three ponds which were connected with
little narrow isthmus and you dropped a
little bit of dye into one of the ponds
if you looked at it immediately after
you drop the dye you would know exactly
where the dye was dropped a little bit
later
you wouldn't know exactly where it was
dropped but you would at least know
which pond it was dropped in and
gradually that dye spreads out and
eventually it gets distributed over
everybody well what I'm going to do is
there's an analogy between this finding
the singular vectors is looking at what
happens when you do a random walk and
you walk until it converges but rather
than wait until it converges what I'm
going to do is I'm only going to take a
very small number of steps I'm only
going to take about five steps enough
steps which is long enough to converge
in a community of size 50 but not long
enough to spread out into the entire
network and so if I do this then what
I'm going to do is I'm going to find the
minimum one norm vector in in a subspace
where I took maybe only five steps I
propagated that subspace and what this
does is this is a good way of finding
communities there are some research
still needs to be done the minimum one
norm vector turns out not to be an
indicator vector it's not going to be a
vector of ones and zeros so you're going
to have to threshold it you're going to
get a vector something like this and
you're going to have to decide where
you're going to cut that threshold to
find the community and and actually you
don't necessarily want to find a vector
which is in that subspace but what you
at least like to do is find a vector
which is very close to that subspace so
what we did is we minimize the the one
norm of Y which is the vector we're
trying to find plus some constant times
the cosine of the angle with that
subspace if you make that constant
infinite then you're finding a vector in
the subspace if you make it zero you're
finding any vector the minimum one norm
vector period but you want to find an
appropriate value for that constant to
get a good answer
and some of the research questions that
people are interested in now is how long
should the random walk be what dimension
subspace should you propagate how many
communities is a person in how many seed
points are needed there's just a lot of
research and the the particular example
isn't good this is one of maybe a
hundred different areas where people are
doing fundamental research and this is
things that you presumably are going to
do in your career I'm going to move
forward a little one of the things
you're going to need is you're gonna
need to understand large graphs because
the graphs that I dealt with when I was
in school had maybe 10 15 vertices and I
could draw them on a piece of paper the
graphs you're gonna be dealing with
they're gonna have billions of vertices
and my graphs my small graphs that was
very important exactly which edges were
present
in your graphs it doesn't matter you
could randomly erase one percent of the
edges and it wouldn't change any
fundamental property and what you would
like to have is a theory of graphs large
graphs where you can prove some basic
theorems and some of the early work on
this there was work by air dish and
rainy where they came up with a model
where they picked n vertices and then
for every pair of vertices they flipped
a coin if it came down heads they put
the edge in if it came down tails they
didn't and there's a it's a beautiful
theory a lot of theorems are proved
there there are three or four good books
on these graphs and one of the theorems
is that the the vertex degree
distribution is binomial but there's a
problem
someone looked at a real world graph
this happens to be United Airline route
graph for North America and if you
create a graph where the vertices are
cities and there's an edge between two
cities if there's a direct flight and
you look at the degree distribution here
it's anything but binomial so what we've
done is we've gone on and we've gone to
models of generative generative models
of graphs where we start with a small
number of vertices and edges and as time
goes on we add vertices and edges and
you need to rule then for how you're
going to attach these new edges that you
add and if you use something called
preferential attachment which is that
you pick a vertex at random with
probability proportional to its degree
then you will get a power-law degree
distribution which is what real-world
graphs actually have and I give you one
more example about graphs because this
is a something whenever I teach a course
here I ask students to go out and find a
database that they can convert easily
convert to a random to a graph and this
is a database that I took from science
it had two thousand seven hundred and
thirty proteins and I said Oh proteins
are going to be vertices there were
three thousand six hundred and two
interactions between proteins those are
edges and then what I did is I
calculated the connected components in
this graph and it turned out there were
48 components isolated vertices these
were proteins that didn't interact with
any other proteins they're 179 pairs of
proteins was just interacted with each
other and so on these are the community
size of communities now when you write a
computer program you should always check
your computer program so what I did is I
multiplied the size of the community
times the number of communities and
counted the number of proteins and it
turns out that I've only got 899
proteins but the original graph had a
thousand 800 or it had 2730 so where are
the missing 1851 proteins is it there is
an error in my program well what I did
is I ran it a little longer cuz I'd only
gone out to size a thousand and when I
went out further I found this giant
component with 1851 proteins that's
where they were now you might say that's
strange how could that how come this
graph has this giant component well what
you're gonna find out when you steady
large graphs like this every real-world
graph will have a giant component so
it's something that's important that you
better understand why they form and so
when students go out and find their
database and do this experiment they
come up with the same kind of results
but for different databases okay so I've
been talking about a science base needed
to support these things and I'm very
quickly going to tell you what a science
base looks like safer for high dimension
high dimensions is fundamentally
different than two or three dimensions
if in two or three dimensions if you put
down points at random the and measure
the distance between them the distance
between the two that are farthest apart
is much larger than the distance between
the two that are closest together but if
you repeat this experiment in high
dimensions and calculate the distance
between points all distances are
essentially equal and you might say how
can that be well the reason being is
when you calculate the distance between
two vectors
what you're going to do is you're going
to subtract the coordinates square them
and add them up well the difference
between two coordinates squared there is
a random number and you're adding up a
large number of random numbers and
there's something called the law of
large numbers which says if you do that
you will get an answer which is very
close to the expected value and
basically that's the the reason
something else that's different about
high dimensions is if you look at a cube
a unit cube in high dimensions this
volume is always 1 if it's unit square
it's one square unit in three dimensions
it's one cubic unit and so forth what
would happen if you took a unit radius
sphere and looked at high dimensions
well when I first thought about this I
assumed that the answer would converge
to some nonzero number but that's not
true in high dimensions the volume of a
unit radius sphere is zero and I'm gonna
show you a consequence of that if you
have a Gaussian distribution in one
dimension essentially all the
probability mass is within three
standard deviations of the center and
the maximum is at the origin assuming
the Gaussian is centered at the origin
what would happen though if you had a
Gaussian in high dimensions
turns out the maximum if we centered at
the origin is still going to be at the
origin but if I ask you how much
probability mass is there in a unit
radius sphere centered at the origin
well what you would do is you would
integrate the probability distribution
over that sphere and you would get
answer 0 because that sphere has zero
volume and so what that says is that
there is no mass concentrated near the
origin even though the probability is
greatest there to find the probability
mass you're gonna have to go out far
enough to where the sphere has nonzero
volume and that occurs at the square
root of the dimension and if you go out
a little further you won't find any
probability mass because the probability
distribution
is dropping off exponentially fast
whereas the volume of the sphere is only
growing as a polynomial so all of the
probability mass is going to be in a
narrow annulus centered at the origin
this suggests that if you had two
gaussians generating random data and you
wanted to know which gaussian generate
at which point all you would have to do
you ought to be able to mathematically
figure it out because the two annuli are
not going to overlap by very much so I
did a little experiment I took two two
gaussians and high dimensions generated
a bunch of random points you'll notice
that the blue and the red overlap a lot
and if I had shown you this instead I
hadn't shown you the colors you might
say how in the world could you separate
those two gaussians but with high
probability I can tell you which
Gaussian generated every one of those
points and I think I have enough time
that I can quickly show you how to do
that if you look at the distance between
two random points which are generated by
the same Gaussian they're going to be
square root D distance apart and these
points are going to be on a thin annulus
and what I'm going to do is I'm going to
approximate that annulus by a sphere of
radius square root D and then I'm going
to claim that the average distance
between two points is square root of 2 D
and let me show you how I figured that
out I generated the first random point
and then what I did is I changed my
coordinate system to put this point up
at the North Pole okay then I generated
my second random point and I can
guarantee you that that second random
point is going to be on the equator why
is that because in high dimensions all
of the surface area is at the equator
there is no surface up near the North
Pole
that's not true in three dimensions but
when you get up to about 10-12
dimensions this is a very good
approximation so what I have is I have a
right triangle where the sides are
square root D and so
hypotenuse is square root of 2d if I
have two gal sins what I can do is I can
calculate the distance between random
points one generated on one Gaussian one
on the other Gaussian and it's again by
some little right triangles I won't go
through the technical details but what
you will find out is the distance is
square root of Delta squared plus 2 D so
if it turns out where Delta is the
distance between the Centers of the two
gaussians and if Delta is big enough
that the square root of Delta squared
plus 2 D is greater than square root of
2 D Plus this gamma then I can tell you
which Gaussian generate at which point
by calculating the the distance between
two every pair of points those which are
square root 2 D apart were by one
Gaussian those which were a little
larger were the other Gaussian you might
wonder what that gamma is is notice that
I did an approximation I assumed
everybody was on a sphere rather than on
an annulus so so that's why that term
comes in there and then if you you solve
for for Delta it turns out it grows as
the dimension to the 1 1/2 or 1/4 but we
can do much better than that one of the
things you're going to be concerned with
is is dimension reduction what you can
do is if I draw a line between the
Centers of those two gaussians and
project all the points onto that line
look what happens
since the centers are on that line the
centers stay the same distance apart but
the points get much closer together
because they're distance from that line
is simply Gaussian noise and so what I'm
doing is I'm increasing the
signal-to-noise ratio by projecting the
points on to that line you may wonder
how I found that line if I didn't know
what the Centers of the gaussians were
but there's mathematics which allows us
to do that and then it turns out you can
separate points from the two gaussians
just provide it that there's some
constant distance apart so so what I've
shown you is what a science-based or
high dimensional data might look like
and one of the things you're going to
have to do is create science bases for
many other areas ranking is important
you rank probably faculty faculty rank
students you rank movies you rank books
you rank everything and ranking is a
multi-billion dollar industry a
collaborative filtering is how someone
gives you an ad based on one purchase
and there's mathematical theory that
tells you what what would be good good
ad to give you dimension reduction
there's just a whole host of areas where
we have to develop some some new science
so what would I hoped I did and I
realized I went very fast but I just
wanted to try to convince you that this
is an exciting time for computer science
there's a wealth of data and digital
format information from sensors there's
social networks to explore and it's
important to develop the science base to
support these activities and with that I
would just like you to remember that
this this that you are in computer
science at a fundamental time of change
and if you position yourself for the
future you're going to have a fantastic
career thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>