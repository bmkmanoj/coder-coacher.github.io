<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Achieving information-theoretic limits in high-dimensional regression. | Coder Coacher - Coaching Coders</title><meta content="Achieving information-theoretic limits in high-dimensional regression. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Achieving information-theoretic limits in high-dimensional regression.</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-lgKB3QoBMA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm Clare to introduce Anthony here
Anthony is a PhD student from department
of stairs yale university today we'll be
talking about achieving eve emergency
radical image in the height of a
celebration honey well yeah so Thank You
Danny I so yeah so as he as you said
that my top is on achieving information
theoretic limits in high-dimensional
regression so before I talk about in
these information theoretic limits and
how we go about achieving it let me talk
about the general framework of
high-dimensional regression and give you
a bit of some examples just to get you
motivated so I'm sure that almost all of
you must be familiar with this so you
have a linear model y is equal to X beta
plus epsilon where the difference
between this and the classical linear
model is that the number of columns is
typically much larger than the number of
rows so in this case the number of
columns in the sample size sorry the
number of columns is the dimension it's
typically much larger than the number of
rows which is the sample size so under
this assumption if you want to say
something meaningful about beta you need
a sparsity assumption on it so the most
common sparsity assumption is that beta
has got say L nonzero entries where L is
typically much smaller than the
dimension yeah please
has to be strong fair assumption yes
it's parsad yeah I mean you could have
other assumptions also write for example
the the beta is contained in a certain
l1 ball or something like that yes
definitely but this is the most common
assumption and the talk I'm I you know
my talk is going to focus on this
assumption and you know information
theoretic limits in in this reception
yes you're right so so perhaps under
this assumption the most an important
problem would be feature selection would
be that would be recovering the the
position of the non-zeros in beta right
and this has been an area of a lot of
active interest nowadays so for example
is got applications in biology where you
are identifying the locations in a gene
responsible for a disease a graphical
model selection where you want to
estimate a sparse graph which also is
like sparse coverage metrics estimation
and compressed sensing where you have to
recover a signal from relatively smaller
measurements so before i start the main
part of my talk let me talk about two
examples that I came across recently one
is this example in face recognition in
this the the situation is that you have
got a number of people so here we have
got 10,000 people and you have got a
number of images of each person so for
example the first person he she's got 16
images and the second person has got 12
images and these different these images
are actually different the very in light
illumination and you know gestures of
the person and the goal of this face
recognition device is to you know when a
person comes in front of the device to
detect whether that person is there in
the database and if he is there detect
which person it is so this can be
formulated as a high dimension
regression problem so in this case the X
matrix would be each each column of the
X matrix would correspond to an image of
the person so for example the first
person would ask you know 16 columns
second person would I have not 12
columns etc and if a person stands in
front of the device it's like providing
the device with the Y variable and the
assumption is that you have got a sparse
linear combination of the columns of the
X matrix so since this it's this this
image is of the second person but it's
not exactly the same it's probably
differing in certain aspects so you'd
expect that this would be as possible in
a linear combination of the columns in
the corresponding the second person so
yeah this is one example another example
that I actually found recent interest I
just found this recently is this method
of out output coding for multi-label
prediction the reason I bring this up is
because the problem so I got started
being interested in high-dimensional
regression because of the coding problem
which which I'll tell you about shortly
and this this has got relations to that
so here the scenario is that you have
got a number of documents say 12 up 2000
where each document has got a few labels
from a large label set so let's say you
have got a label set of you know
thousand labels and each document has
got a few labels from that large label
set so so this these documents could be
images also like I think the paper their
paper they considered images from the
ESP game data set so this could be
images also and so so for example for
for document one you have this large
vector and with most of them zeros and
with a 10 when the label is present in
the document right now and they're so
the training data consists of these
label vectors along with you know other
explanatory variables for these
documents now because this label vector
is really hot Marge this you know it may
not be a good idea to fit a model
directly
to this label vectors so what they do is
that these they encode these label
vectors so here beta is a label vector
by multiplying it by matrix a of say
random Gaussian entries to get a much
lower dimensional vector Y and then
instead of feeding the model to this
huge label vector you fit it to these Y
vector which is much lower dimension and
then when you have got a new document
you try predict them predicting the wide
y vector first and then use a
reconstruction algorithm to get the
original label vectors back so this is
one advantage of you know so this this
this wouldn't be a good idea when the
labels have a certain hierarchy I mean
but if you only know that there is just
the sparsity and nothing else then this
might be a good idea and in fact there
are ways of you know if there is a if
you know something more about the
sparsity like the sparsity appears in
groups you can you know include that in
the reconstruction algorithm so these
are examples of high-dimensional
aggression the the questions that I was
interested in was the relationships
between sample size dimension sparsity
and signal-to-noise ratio for accurate
feature selection and to the nature in
which the above changes when one allows
for a small number of mistakes when what
i mean by mistakes is both false alarms
and fail detection so false positives
and false negatives so actually this is
important because it might be the case
that under the sparsity assumption that
i said that there are a few l nonzero
entries some of the nonzero entries may
be relatively small right so putting the
condition that you want to recover all
of them that that may be too stringent a
criterion so you want to be more
flexible and then you you want to see
how these relationships change when you
allow that flexibility but I'll the main
part of my talk would be on this the
first part
yeah please go sirisha yeah it's a
signal to noise ratio is actually the i
define it as the you can define it as a
norm of X beta Delta norm so I that that
will become clear in my contacts soon
Thanks so so so let me give you the
outline of my talk so I'll first talk
about information theoretic limits of
sparsh recovery and then I'll talk about
this communication problem which I will
describe only briefly but this was the
main reason I got interested in the
information theoretic limits and then
i'll provide a discussion of practical
approaches for example greedy algorithms
for solving you know discussing how
close to these information theoretic
limits existing practical approaches get
and then third i'll discuss the
performance of an iterative algorithm
that we propose and along with a little
bit about the theoretical analysis
actually demonstrating that one can get
these formation theoretical limits so so
let me let me start about talking about
information theoretic limit surpassed
recovery so assume that you have a this
linear model and assume that the entries
of XR ID with say variance 1 and epsilon
has got ID normal 0 1 entries so the
normal 0 1 is I mean ideally it would be
a normal 0 Sigma but that's just a
scaling so let's just assume that it is
a normal 0 1 entries and like I said the
beta the coefficient matrix par so most
specifically you assume that the
coefficient vector belongs to a set a
where a is the set of beta with L non
zeros and with the non-zeros having
magnitude at least w so they have a the
non-zeros have a certain minimum
magnitude
exactly l0l nonzero yeah all of them or
at least order was right good yeah yeah
of a magnitude of you right right so so
everything yeah yeah so write each is
pretty relax right right yeah so the
thing is that here we are worried about
information theoretic limits so so more
specifically we are interested in you
know lower bounds on the sample size n
to detect the non-zeros of any beta any
right now you are right that this is a
strong assumption but this lower bound
would hold for you know a broad class of
matrices so in fact if it cannot hold
for you know ID matrices are perhaps the
best kind of matrices that you can get I
mean for which you can get you can get
recovery with the lowest sample size
possible so so I yeah
the epic movie feature selection is
discussing the pros problem the problem
would be for something biology seems
that you have two features which are
simply identical right would you
recognize both of them or one yeah then
you would be doing a 1 you know taking a
linear combination of you know Denmark
yeah but we might just dig one of them
it might be just take the conventions to
write in fact we have right so yeah so
yeah so in real life data said you like
as you were saying that you don't have
this idea assumption so in this case so
if when they riot II this this thing
wouldn't happen right when those two
features would be identical so yes but
yeah you're you're right real-life
datasets you typically do not have that
scenario so so so the question is that
how do we arrived at the such a lower
bound or what is the lower bound right
so what what statisticians do in such a
case of trying to find a lower bound you
know for support recovery for the set a
is that they try to reduce this problem
to a testing problem so what do I mean
so you consider the set a star which is
a subset of way which is set of beta in
a with nonzero values having equal to W
so beta beta back the beta vector looks
like that and you know there are l l
nonzero entries right and so each of the
nonzero entries have a value w so the
cardinality of a star is n to sell and
an algorithm that can detect elements in
a can also detect elements in a star
simply because of the fact that a star
is a subset of a right it is a much
larger set right so the advantage of
doing this is that we are kind of
converting this original problem of
trying to recover in the set a to a
testing problem so an algorithm that you
know has to detect in the set a star is
basically doing a test right it has to
test between which of these enthuse l
possibilities of beta it has to select
right or rather is doing a
classification kind of so and the
advantage of reducing it a testing
problem is that there are information
theoretic inequalities that gives lower
bounds on the probability of error in
testing problems so what do I mean yes
sir it's not a sir issue solved
freestyle at the same set of modules
right
if you optimized for the full a you may
get a different set of may be made
better by choosing some of these 20
other others do not ah what is it true
so if you solve this with a subproblem
in trouble you find the sunset nonzero
back right right and then you went to
solve so imagine that somebody box the
football he regarded to get the same
sort of long as everybody's not lots in
balance with the same SAT across the
same features know so yeah so so the
thing is that here what so what I'm
saying is that a lower bound for
detecting in the set a start would be a
lower bound for detecting in the set a
because detecting in the set is that is
a tougher problem so but you're right i
mean you cannot automatically you know
generalize from a star to it but since
we are worried about lower bond to
provide a lower bound on a star will
also be a lower bound for the much
larger sat eight what's that right right
right right yeah so so as i said there
are information theory limits
inequalities providing lower bounds in
the probability of error so what do I
mean so the signal-to-noise ratio you
can define here since the entries of X
are iid with variance 1 the
signal-to-noise ratio is simply the norm
of this vector beta square as the norm
of this vector beta the square off of
that so in this case is l times w
squared right and an important quantity
that appears in these lower bounds is
this quantity known as the capacity
which is half log 1 plus snr and scribes
this change if you turn on
variants
so so if the errors are non unit
variance they would be a divided by the
variance of the error right yeah that's
why since that's a scaling I just took
it to be unit variance so so the lower
bound and the sample size for detection
in a star is is actually a consequence
of a very famous theorem in coding which
is the Shannons coding theorem and it's
not really meant for regression problems
but it can be readily applied to our
situation so it basically says that to
to be able to detect any beta from a
star one requires n to be at least this
quantity its log of the cardinality of a
star divided by C so so let me write
that down n should be at least log of
cardinality of a star divided by C where
C is
so so since a star is a is actually a
simpler problem than a the same lower
bound holes for detecting a name right
but we are actually interested in
detection in the set a star not a for
reasons i I'll tell you soon enough so
let me just mention that the Shannons
here coding theorem was not meant to be
used for you know for such applications
so there are stronger lower bounds on
the sample size n have been recently
proved specifically for this setup by
these researchers but these these bounds
agree with the Shannon lower bounds
under this under the setting that the
sparsity or over the dimension which is
l over capital n is small so in this
regime the Shannon lower bounds are
still the strongest that is there yeah
also detect any pressure for a star does
not mean any of the subtle places or
particular valuable trying to turn
so any beta so right yeah I mean any
enemy time so so what what this theorem
actually says more specifically is that
suppose you have an algorithm that is
therefore some algorithm that you have
for detection in a stab and that
algorithm gives you an estimate beta hat
right now what this says what this
theorem says is that the probability
that beta star sorry is not equal to the
average probability overall beta and a
star
this is this is bounded away from zero
if n is less than that quantity so if
the probability is bounded away from
zero then you cannot yeah so so the the
Shannons bound is still the strongest
that you can get in this regime where
this paucity or dimension is small but
Shannon's bounce is a converse result I
mean it need not be that you can get I
mean so a converse result so it's it
basically tells you what you cannot do
right so it need not be that there can
be an algorithm that can actually
achieve these limits so that means what
I mean by achieve is that can one find a
practical algorithm it can detect any
element in a star with n near this
quantity log carnality of a over C yeah
good the variance of the noise perimeter
welcome to be somewhat control prouder
so you could play tricks with the bound
by basically turning a pouring down the
noise model that you were working no
notice of the noise the noise I I said
had is normal with variance one right
yeah but but if you were in when one is
actually solving division problem they
would often very parameters that change
the parameters ation of knowing so you
can choose to have higher noise
your mind okay right yeah but this
covers everything I mean the role bound
is but but in the prison sorry but in
the acquaintance you were saying it
would scale the interface one where you
had the Pacific resurrection so that
would scale with the noise variance
right yeah yeah yeah so if the noise
variance is is small then the
signal-to-noise ratio will be higher and
then you will require you know the lower
bound would be smaller right okay yeah
so so sooo in other words when you wanna
if you do know that the ranch alluring
some noise varies the bound was just
scaled for his family yeah I don't
really yeah it would be like log of so
snr would be so in general if there is a
certain variants for the noise the snr
would be the normal beta squared divided
by the noise variance okay so we just
come into the log into the bottom yeah
yeah you come in the log in the bottom
yep
so can one actually find a practical
algorithm that can detect elements in a
star with n near this quantity yeah but
so since you describe this one of the
generative model within maximum
historian he giving you this exactly
this resulted
ah so I'll come to the map estimator
soon but no i mean because this lower
bound which I said that was that was
that was a converse result it was not
meant I mean it's not like we took the
maximum as episode or estimator and then
saw you know how you know what the
sample size was it was not done by
analyzing any algorithm it was just some
information theoretic lim inequalities
for testing problems so but yes we that
that is the best estimator for this
problem and yes we that is also
important to analyze that so I'll talk
about that soon yep sooo not to run just
off the back to the fear of it nice
piece yes sorry this way yeah I mean
it's not a problem stay stable to it you
could attack today the bike chance
without with a score like
so but I mean this probability would be
I mean this probability is overall beta
right it's not that you can so you might
even if your guess is 3 you have the
laws of some time that we're at the
answer would be three but your
probability of guessing right brother so
put other than you will use is bounded
away from zero by a constant okay yeah
Thanks so so that's the question there's
another way of repressing this question
so like I said n should be at least that
and we want to see whether there is a
practical procedure that can you know
make n close to that I'll tell you why
we are interested in that soon enough so
but there's another way of formulating
this if you define this quantity rate
which is law cardinality of a radar
which is the law carnality of a star
divided by N by Shannon's theorem you
need I mean this quantity rate cannot be
greater than C if you want to detect the
elements in a star right so another way
of rephrasing this question is that
devised a practical algorithm to detect
elements in a star with our near seen
rate near this quantity capacity right
so let me tell you why we are interested
in this I mean you know detection near
this set near this sample size n near
log of a star Oh see it's because of a
communication problem which we formulate
as a regression problem so the
communication problem is as follows in
communication you want to send messages
reliably through a noisy medium also
known as a channel right so
so we assume that the set of messages is
a star so a message corresponds to a
coefficient vector and a star now there
is a there's a quite a bit of story that
I'm hiding so when you are actually
talking on a cell phone you are not
sending coefficient vectors but they're
actually speaking but what's actually
done is that this time is divided into
blocks and then the whatever you you
speak is digitized and then digitize
meaning converted into binary strings
and those binary strings are basically
mapped into one of those coefficient
makers in Eastern so that that's a story
which can be there in the background so
let's just assume that the set of
messages is a star and I said you
transmit through a noisy medium so the
noisy medium actually adds a normal 0 1
noise to what a sin so let me be more
specific suppose the sender wants to
transmit a vector beta and a star okay
now what he does is that he encodes this
beta using a matrix X of ID normal
entries so this is where the
relationship with the multi-label
approach for you no coding approach for
multi-label prediction comes in so so
you encode beta 2 X beta the receiver
gets y which is a noisy version of X
beta so this is where the epsilon is the
noise added by the medium and since we
are assuming that the channel which is
the noisy medium is a Gaussian channel
the noise is normal with the you know
variance 1 the receivers goal is to get
the correct beta from a star from the
knowledge of Y and X so it's a
regression problem and the reason you
want to minimize the sample size you
want to make the sample size as small as
possible is that you want to minimize
the number of transmissions and so so
the way this the way the sender
transmits this X beta is that he first
transpose the first entry of X beta then
the second entry
up to the NSN tree and you don't want
the number of transmissions to be large
right so you want to make this n as
small as possible for that reason so so
now let me discuss practical discuss
feasible strategies for solving this
problem one feasible strategy would be
greedy algorithms which is which is the
approach we take so so let me first say
that the best estimator as you said was
the map estimator which is you find the
you find the beta in a star which
minimizes is norm right y minus X beta L
2 norm of that this has the least
average probability of error by average
probability of error I mean precisely
this however this is not computationally
feasible because you know you can't do a
manual search overall beta and a star
right because the a star is is a huge
set so however it's really important to
analyze how this estimator performs
because like like you mentioned
Shannon's theorem is a converse result
how so it tells you what you cannot do
right so even though this lower bound is
there it may be the case that there is
no scheme let alone a practical scheme
that with which you can achieve this
lower bound right so it's for that
reason it's important to analyze this
estimator first because if this
estimator performs badly you cannot even
hope that any practical scheme can
perform well right and in in our paper
we show that this estimator actually
performs quite well so we have a short
with practical schemes right so Paul
will complete yeah
this results show that the shannon jones
bound is actually tied bow right right
but but you see this estimate is not
completely feasible so just enjoy both
right that's right it's a tight bound
right so possible computes you feasible
strategies very famous something that
has received a lot of attention is
elegant a penalized methods we like i
said we use the technique of greedy
approaches so so let me give you a bit
more background on greedy approaches so
the way these algorithms work is that
they select one term in a step recompute
the fit and repeats the process that is
the general high-level way these things
work so for example one one way of doing
this would be you start with an initial
fit of 0 you update the residual vector
which is which is y minus the fit in the
previous step you find the term JK that
maximizes the inner product with
residuals right you find the feature
that is max which has the most
correlation with the residual you update
the fit using the selected term JK I'll
tell you you know ways you can update
the fit soon I mean in the next slide
and then you see whether a particular
stopping criterion is met and say a
stopping criterion would be if this norm
of this residual vector is small or not
that would be a stopping criterion if
it's not met you do again so ways you
can update this fit well if you if you
take this fit as the new fit as a
previous fit plus you know the the newly
selected term x JK but then the the
coefficient of that term would be the
actual inner product then you have the
matching pursuit algorithm if it's not
exactly the inner product but a small
portion of that inner product epsilon K
is a small pause
of quantity then you get the forward
stage wise algorithm and then if you if
you if you if the new fit is actually a
projection of Y on the selected terms
then it's this algorithm known as the
orthogonal matching pussied and this
this is this like a whole host of
algorithms that one can have and in fact
there is a whole host of ways you can
fit you know pick this next term so so
that gives rise to a huge class of
algorithms and you know such algorithms
have been used you know for neural Nets
l2 boosting learning with structured
sparsely learning in reproducible
Colonel Hilbert spaces and feature
selection which is the thing that we are
interested in fact even I have done some
work on a feature selection using the
orthogonal matching proceed algorithm so
so the results from this literature do
imply that a sample size of the order of
n choose L remember n chose L is the the
carnality of the set a star right these
do imply that a sample size of the order
of n to sell you can recover the
position of the non zeros for any beta
in a star see for example these results
to show that however we are interested
more in the constant I mean not just
order of magnitude but also the constant
I mean you get that you can recover with
n close to 1 over C right
the constant is the thing is that yeah
they're they're pretty they don't really
need to care about the constants there
so it's it's it could be nine ten it
could be anything I mean it's yeah the
reason is that they were focused on
generality of applications right we have
a specific problem where we have got the
X matrix is iodine or Milan trees and
the coefficient vector has got you know
this structure where the non-zeros RW so
the question is that can be leveraged
that to give more stronger results right
then what is their existing in with a
current literature so ya know if you
don't know the seesaw futures on sunday
was a stronger register see so no i mean
they know i mean they really don't they
say that with n of some constant yeah
yeah well so they don't really they're
quite hazy about what the constant is
like they say you know to the order of
magnitude of something but yeah so
so let's just so let me describe the our
iterative algorithm it's actually
variant of these it's a minor variant of
these greedy algorithms so so we have a
situation like this where the non-zeros
RW so notice that why is if you do not s
as the set of columns where beta the
coefficient vector is nonzero then why
is w times the sum of the X J's for over
j &amp;amp; s plus noise right so XJ is
correlated with why if and only if J
belongs to s now for the first step we
consider the statistic Z 1 Z which is
the inner product of XJ with y divided
by the norm of Y and the set of terms
that are selected in the first step are
those for which g 1 J are greater than a
positive threshold tau so I will tell
you what this positive threshold is soon
for the second stop we we consider turns
that are not selected in the first step
in in previous steps so so these are for
step you know greater than 2 so any term
selected on previous steps are selected
there is no going back and you know
changing that from previous steps you
get these fit vectors where the fit
factors are you know the sum of the X
chase over the set detected in that step
times this scalar quantity w we compute
the residual calculate this inner
product statistics so it is the inner
product with of XJ with this residual /
the norm of residual and then you select
those for which you know are greater
than some threshold and stop when either
you have got eltham selected or when
there are no terms our threshold now the
the thing about this algorithm is that
we can after some effort want actually
characterize the distribution of the
statistics EKG and the distribution of
the statistics is as follows for any
jane s recall that s is the
the set of terms wave beta is nonzero
right for any J in s zkj is bounded from
below with high probability x shifted
normal these w cages are enormous 01
random variables for any JK j in s is
the shifter normal and for any j not in
s is simply a noise vector a normal 0 1
random variable these mucus are these
mean mu k's are greater than 0 and they
increase with steps so so it is this
mean that gives you a way of
discriminating between the correct and
the wrong terms so a little more detail
oh so one thing I would like to mention
is that these results are non asymptotic
so so that was where a lot of our effort
went into because you know for the first
step is actually easy to calculate the
distribution of z10 it's not a big
effort but because of this from the
second step because you know there are
terms detected in the first step there
are lots of you know dependencies that
arise and actually getting such a
characterization required a lot of
effort what's that oh the sparsity the
number of nonzero terms right so the
error probability are the error probably
meaning the the probability which this
is satisfied is actually exponentially
small inhale a little more details since
I'm actually running out of time let me
just say how this threshold tau is
selected since for any chain not in s
zkj are iid normal 0 1 right and since
there are n minus L wrong terms and
since the maximum of you know n minus l
ron normal random variables is roughly
like square root 2 log and minus L you
would like to put the threshold as that
to minimize false positives right the
mean mu K has this form so it is a
function of it is a function of the
fraction of correct detections from up
to the previous steps where this
function is actually an increasing
function
so what this gives us is that the total
fraction of terms detected after step k
has expectation lower bounded by G of qk
minus one where G is this function it's
it's it's some function on 01 so so the
take home message from this is that let
us say for our the radar remember rate
is log of cardinality of a divided by n
suppose you take a rate of point four
five times C and you take an snr of
seven the function G looks like that on
01 it looks like that and the way our
algorithm our theory predicts that our
algorithm progresses is something like
that so the first point over here is a
fraction of terms detected out to the
first step you go like that and as the
second point with the fraction of turns
particular after the second step etc and
you keep going till this last point so
our theory predicts that you can
actually get most of it ninety-five
percent of it correct and with an error
probability that is like 10 to the power
minus 4 so this was with the rate of 0
point four five times see what if you
try to increase the rate of bit more say
oh point five five times C then a
function G looks like this so now look
it's touching this y is equal to X line
so so what our theory predicts this is
there is that our algorithm actually
just has to stop over there so you
actually make a lot of mistakes at least
our bound gives that you make you know
eighty-five percent mistakes and the
error probability is like 10 to power
minus 5 but that doesn't matter because
you are already making a lot of mistakes
it turns out that the rate R can only be
made as high as this quantity half SN r
1 plus snr in this in the in the setup
that we are in and using our algorithm
and it's a little bit of algebra can
show you that this quantity is actually
less than this capacity see so you
really cannot drive the rate hi
notice that our are not increases to
half as SN R goes to infinity and so in
an asymptotic regime a similar threshold
of recovery was noticed for the lasso
algorithm so this is a you know this is
how we show that this threshold resume
you know is in for finite SNR he
Wainwright noticed it as snr you know
tends to infinity so so but the thing is
that we we need rate to be made as high
as capacity but here using our set up
and using our algorithm we can only make
it as high as are not so how do we reach
as high as capacity so what we do is
that we we assume n to be of the form L
times B and we divide the we consider
coefficient vectors of this form so it
is divided into L sections where each
section has got be terms and there is
exactly one nonzero in each section
right and the sum of squares of the
non-zeros is equal to the SNR we
considered position vectors like that
and we take a star to be dolls all such
vectors right the cardinality of a is a
star now is B to the bar L note note is
now using this modified a star we need
to again show that we can make the
sample size as small as this a star can
be anything it could be any finite set
all we need to do is that show that the
sample size is can be made as small as
that so the weights that we consider
actually are of this form that they
decrease exponentially and they flatten
out now you run the algorithm on this
modified a star and if you do that
remember like previously I was showing
that with rate as 55 times C the
algorithm was getting stuck now with
this modify a star if you run it you can
actually make the algorithm go you know
actually detect most of the terms
correctly
and so the way this algorithm progresses
using this modified a star is that in
the first step since the weights we're
decreasing like that the initial
sections they have a higher probability
of detection so on the x-axis you have
the section number the initial sections
are a higher probability of detection
and it you know decreases since the way
it decreases like that the probability
of detection decreases the lab and the
probability of detection by the seven
step increases dramatically by the 13
step is even more and by the nineteen
step which is the last step almost all
the sections have a high probability of
detection and this can be summarized in
a theorem if you fix a any rate less
than C B where this quantity CB tends to
capacity for large p then our result can
be stated as follows for any rate less
than cb1 eyes of the fraction of
mistakes is of order 1 over log P so if
P tends to infinity this this tends to 0
and the probability is exponentially
small in L this quantity Delta is
actually the difference of CB frame are
so so what this actually tells us is
that in certain sparsely regimes it is
actually possible to achieve information
theory limits using a practical scheme
right using a greedy approach and the
error probability is exponentially small
and this is something that I didn't
stress but so the existing results on
high dimensional regression while they
are more general in nature the error
probabilities are not exponentially
small and one of the reasons for that is
that they they they they focus on exact
recovery of the nonzero terms and it's
that that is to strengthen stringent a
criterion you know to to have you know
recover all the non zeros exactly you
cannot get your error probability that
high as we can since we allow for a
small number of mistakes we
and get our air prolly to be really
small yeah first if you have wats
varsity so everything become really hard
so what is log voxel-based instead of a
linear number of instead of the number
of relevant terms main linear of the
total number of features if the number
of features you want to recover is law
so if the ringmaster for example lot of
problems you old people will kind of
flood the feature space with various
transformations of the number the number
of nonzero scan may not be linear it can
be it needs to be sub linear I mean so
the ending was lb right so if you have n
equals something vadivel if instead of a
linear dependence NL you have a lot
depends on oh oh oh oh ok ok ok ok it is
log in the number of total icic ah it's
a number of this thing is I think it
should still work I mean yeah I need to
think about it a bit more but yeah I I
don't see this in a problem yeah I need
to think about it a bit more thanks and
so this this these results actually
provide a theoretical and practical
communication scheme so the reason why
we are doing all this is that the
communication schemes that I use
nowadays they perform very well
empirically but a theoretical
understanding of how they perform for
any given rates are less than capacity
is not known and so by formulating this
as a regression problem we try to we are
trying to not only give a practical
scheme but also give a practical scheme
that you know we can have some
theoretical result of how it performs
for any given rate less than capacity so
so future work you know analysis of soft
decisions instead of hard decisions so
here in each step we selected terms on
din selected term right what if we made
soft decisions and we are pretty sure
that
you know this would improve the
algorithm significantly but the question
is that how do we analyze that
theoretically implications for the
general high-dimensional regression
problem you know like so since we are do
these results actually carry over you
know can we extend these results over
for the general high dimensional
regression problem other things that I'm
interested in learning with structure
notions of sparsity so when the sparsity
is you know this something else other
than the the fact that there are L
non-zeros you know something more than
that and you know I'm interested in
working on other problems and high
dimension statistics early matrix
completion high dimensional pc and
dictionary learning so with that thank
you and thank you for the time thank you
yeah something right face ok yeah um the
expert already killing you you made the
assumption of the X matrix the omelettes
are iid right and with variance more
right I will try understood you to me
that each element of the matrix
transform you're not talking about the
vectors the idea with unica reservations
right well if every variable is idea
variance is ID why I'm just why is the
price is such an interesting just take
the first cut first elephant there are
several same behavior in predicting y as
any other send another so it saying
their ID right then the selection thing
just seems like why doing first place
the right idea i just take the first
autumn disparity that's your assumption
ah no so so the the thing is that we
need to ensure selection not just for
the first tell so yes their ID so you
have y is equal to x beta thanks a lot
idea baker's rack system already yeah so
let their non zero value copyright in
our case yeah I mustn't using a subset
of the columns of X
so no be so the thing is that you have
to the algorithm should be such that it
should guarantee selection for any
subset of features selected not just so
you don't know with the the subset that
was selected the algorithm has to detect
that no matter so I maybe Allison as a
goal set beta because it only they once
the definitions for beta only did to the
general fund little better half yeah or
are you just fun traffic but subset of
the column of acts as there are eyes but
they're actually beta you already teased
those other ones we need here have just
put the candle one
all those are generated in I the right
decision so but in essence give peter
has a 0 coefficient for example in the
first in depth then why is your pendant
Oscar first carbon facts but who cares
the first contact the story of the same
as a second home different how much
progression yeah why tell me you're
trying to find something about Peter not
about the so the eggs that's what that's
what its X is the coding matrix yeah and
peter is the thing you're trying to
recover from the signal you get why the
best way to understand this is from the
coding the controlling communication
problem you said I think I was stuck on
the regression yeah yeah it's misleading
to think better is regression at some
level pit it's really a decoding problem
but I must think about his progression
as good as an actor the natural way of
thinking about it in that way yeah so
you see this is a practical proactive
communication skin but basically is they
steal you be kind of pre decide how many
you need to send right the reader then
after and the receiver end you need to
steal south is using the greedy
algorithm to solve regardless that's
your investor is cannot reach mess with
your solution there
which is based on a sofa yeah I'll do
you so you know the performance of that
are personal yeah so so the thing is
that so the reason why we started
working on this was that existing
communication schemes they they perform
well in practice but a theoretical
understanding is not there so we we
needed to get some how to demonstrate
that a theoretical a theoretical
demonstration that an algorithm can
achieve rates up to capacity now the
Palazzo so i'm pretty sure that under
that setup it should perform well our
algorithm empirically performs better
than the last show but as long as the
the maximum rate you can achieve with LA
so I'm pretty sure it must be the same
but how do you prove that theoretically
we originally performing you should
write this fancy how the similar
problems but right I I yeah so because I
need a certain set others ready for
setups I think the result will be the
last saw sufficiently is slightly right
more better than the video cos right so
but but the thing here is that in this
video algorithm we have made use of the
fact that the non-zeros RW and also like
so I told you that the statistics we use
way based on inner product right but in
our paper it's actually a statistic that
is close to that it's it's that is a
motivation for it but that statistics
that we analyze is actually close to
that so what I'm trying to say is that
in our analysis and of our algorithm we
we try to make use the most we can of
this normal structure and the the fact
that the non the the non-zeros RW the
lasso algorithm is it's a more general
thing so that's one reason why our
algorithm performs better than the lasso
yeah
so what are the comparable techniques
that work well practice window chemical
oh you used incoming communication
evidence so the the technique that is
used in communication nowadays is are
these LDPC codes they they have they use
bleep propagation techniques now belief
propagation algorithms they have got a
good theoretical understanding when so
these are algorithms on graphs I don't
know much about the details but the
there is a good theoretical
understanding when these graphs are
trees but the graphs that they use while
communication they have they are not
trees they have cycles in them but they
still use use these algorithms and they
work but a theoretical understanding
there is this still a gap except in some
very special channels there is still no
theoretical understanding of you know
how the codes that I used nowadays and
practice perform
and yeah
alright</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>