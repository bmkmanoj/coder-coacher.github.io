<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Predictable Data Centers | Coder Coacher - Coaching Coders</title><meta content="Predictable Data Centers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Predictable Data Centers</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gNcIZ5GHHNI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
thanks for the introduction welcome to
Cambridge as you can see it's raining
outside please fall to bits Sun for a
couple of days and now we are giving the
true Cambridge experience amitesh palani
and today I'll be talking about work
that have been doing with my colleagues
Paolo gone Thomas Greg and Annie no all
peep heart of systems and networking
group so over the past couple of years
we've been looking at the systems and
network aspects of building predictable
data centers and in this talk I want to
give you an ovary of this work and
describe a few of the systems we have
designed now when we started this work
the explicit project goal was to enable
predictable application performance in
multi-tenant data centers here the term
multi-tenant data center refers to any
data center with multiple tenants this
includes private data centers like the
big data center and it also includes the
public cloud like Microsoft Azure that
you learn about later today or Amazon
ec2 where users or tenants can rank
virtual machines on demamp however
before we dive into the technical
details I want to quickly go over how
these data centers operate today and of
course if at any point of time you have
questions please feel free to ask it's
always nicer to have a more discussion
oriented talk so today's cloud providers
offer a very simple interface to the
users or penance tenants can ask for any
number of virtual machines and they are
charged on an hourly basis so you can go
to Azure today and hopefully you'll do
that in the afternoon and you can get
small virtual machines for eight cents
an hour hopefully your afternoon stuff
will be free this is great however
there's a problem here and it's the fact
that the performance of applications in
these cloud environments can be very
unpredictable I'm going to sim use a
simple example to explain what I mean by
that statement so let's imagine users
say Ken who does in how state analytics
so every day
runs a MapReduce job on his enterprise
machines in isolation and say the job
takes four hours to finish now ken hears
about agile and he takes his business to
the cloud he executes the same job on
the same number of machines in an azure
data center however as you can see the
job completion time can be inflated and
that's not it because users are charged
based on how long they occupy their
virtual machines poor can ends up paying
more to so it's a double whammy now this
is a contrived example but the important
point here is that today both the
performance and the cost for tenants can
be unpredictable which is often cited as
a key hindrance to cloud adoption of
course an obvious question to ask is why
is application performance so
unpredictable and to answer that let's
look at what happens inside one of these
data centers so we have can renting his
virtual machines he executes his
MapReduce job this job involves
shuffling data between these virtual
machines so it results in network
traffic beyond this the job needs to
read its input from the shared cloud
service so it leads to network traffic
and storage traffic of course there are
other users in the cloud each running
its own application the thing to note
here is that the underlying network and
the shades storage service are both
shared resources so the network
performance and the storage performance
you achieved depends on what other
tenants happen to be doing so it is the
shared resources that are the root cause
for unpredictable application
performance in the calc and actually
there is plenty of evidence regarding
this in the white over the past few
years that have been a bunch of
measurement studies by external
researchers looking at various blood
providers and they measured the network
bandwidth between VMs in the same data
center and you can see across the board
there is a lot of variability in some
cases a
through five or more and this network
performance variability has been shown
to be a key contributor to application
performance variability and the same
variability applies to storage
performance to now up until now I
focused on the public cloud but the same
unpredictability problem shows up in
private cloud scenarios too so in this
example we are looking at a small
enterprise cluster which could be a
cluster at your University it could be a
cluster at any small enterprise on the
right we have a compute stabbed with a
few physical servers that are hosting
virtual machines on the Left we have a
few shared file servers now the Rev
virtual machines in this case happen to
be running an aggressive application
that generates many IO requests per
second input output requests per second
which in turn loads these file servers
and when blue VMS generate our requests
they get really poor I performs actually
if VMs get really poor I performance for
critical page dials they can even blue
screen they'll just crash which is
obviously something that we'd like to
avoid so in both these cases a public
cloud and private cloud we have the
problem of unpredictable performance
because of shared resources and this is
what motivated the predictable data
centers project where as I mentioned the
goal is to enable predictable
application performance in these shared
data center environments now the way
like to think about this project is we
are building upon a few key building
blocks dedicated virtual machines that
exist today and what we want to do is we
want the same kind of predictability to
apply to shared resources like network
in storage so in a research here we've
developed these few systems that enable
a predictable network and a predictable
storage in this talk in the first part
I'm going to focus on octopus which was
our first project in the area where we
offer
and with guarantees in cloud
environments I'm going to give you a
technical overview of octopus and then
in the second part of the talk I'm going
to briefly comment on the other systems
to show you how the research has evolved
and what are correct direction is so
first up will do octopus now in case you
are wondering where the name came from I
don't know if there are soccer fans here
it was named after paul the octopus who
gained a lot of media attention for
being able to predict the outcome of
soccer games i see a couple of nodding
faces he passed away a couple of years
ago i may his soul rest in peace anyway
the basic idea behind octopus it is
really simple all we are arguing is that
tenants apart from being given virtual
machines as is the case today they
should also be offered a virtual network
connecting these machines now this
virtual network is a private network for
the tenant and it decouples their
network performance from the underlying
infrastructure thus allowing them to
reason about their applications in a
simple and intuitive fashion so if
you're a tenant what this virtual
network does is that it ensures that
your performance does not depend on what
other tenants happen to be doing the
nice thing about octopus is that it does
not require changes to network
infrastructure and it does not require
changes to tenant applications so it's
incrementally deployable which is great
from a deployment perspective and
finally what we found was that liquid
band with guarantees are great for users
because they get predictable performance
but in our research we found that they
can benefit the provider to by
increasing the overall cloud throughput
so it's a nice win-win situation and
this is something that I will show as we
go along the talk now once we settled on
the basic idea of offering virtual
networks yep question
yeah I'll explain that in detail later
but Cheyenne's a short answer right now
is the system throughput how many
cannons jobs can the cloud provider
process in any given unit of time which
in turn influences revenue but I'll come
to this in detail later so once we
settle on the basic idea of a virtual
network the question for us was what are
these virtual networks that we want to
offer to users what are the semantics of
the bandwidth guarantees we want to
offer what are the virtual network
abstractions we want to offer to main
design goals influenced a lot of our
thinking in this direction first from a
tenant perspective we want to we want
these virtual networks to be simple and
intuitive we want tenants to be able to
reason about the performance of their
applications when running on top of
these virtual networks second from a
provider's perspective we want to retain
provider flexibility we want the
provider to be able to map many of these
virtual networks and many tenants on
their infrastructure at the same time
now these two are competing goals
tenants want strong guarantees while
providers would want to offer we
guarantees so that they can achieve a
lot of multiplexing come in all the
virtual networks we've designed there's
a common theme and that is this notion
of aggregate bandwidth guarantees which
in turn allows us to balance these two
competing goals now the best way to
explain what I mean by an aggregate
bandwidth guarantee is to look at some
of the virtual networks we've designed
so let's do that the first virtual
network we designed is something called
a word 12 cluster and it's motivated by
how talents run applications in
enterprise settings or any private
setting these applications run on a set
of physical machines that are connected
by an Ethernet switch and I'm sure
you're used to such an set up at your
universities when you run your
experiments and we
want to replicate the same kind of setup
in a cloud environment the hope being
that this will make it users to migrate
to such cloud environments so with this
abstraction a Terrence request is
characterized by two parameters N and B
what the tenant gets is a set of n
virtual machines each connected by a
virtual link of capacity be to a virtual
switch so in effect the tenant gets a
private network that is very similar to
the physical network they're used to in
their enterprise settings do things I
want to reiterate here first this is a
virtual network that's separate from the
underlying physical Network so if you're
a tenant who's renting and virtual
machines I could place these VMs I the
provider could place these VMs anywhere
in the physical data center and all we
are doing is we are offering this
illusion of a virtual cluster that
connects your bm's second with the
virtual cluster abstraction each VM gets
an aggregate bandwidth guarantee for its
outgoing and its incoming traffic so we
have a scenario like this where vm one
is generating a bunch of flows what our
abstraction says or what we are
promising is that the aggregate rate for
these flows should be limited to be MVPs
similarly if the vm is receiving a bunch
of incoming flows again the combined
rate for these flows should be limited
to be MVPs that's what our abstraction
is promising so that's an example of an
aggregate guarantee taking a step back
the way the system works is when a user
comes in he specifies the number of
virtual machines he needs and the
virtual network he requires to offer
this illusion of a virtual network
octopus relies on two main components
first there is an admission control and
a vm placement component that carefully
places a ten-inch VMS such that their
bandwidth guarantees can be met of
course
if the guarantees cannot be met the
tenant request is rejected once a parent
request has been accepted and it's VMS
have been placed we then need to enforce
these bandwidth guarantees we need to
make sure that each VM gets the
guarantee it has been promised no more
no less so for the next few slides I'm
going to comment briefly on these two
components to give you a feel for how
octopus works first up is vm placement
and i'm going to use a simple example to
give you the intuition behind our
placement algorithm so we have a toy
data center with four physical machines
or four physical servers with to vm
slots also let's imagine a tenant who is
requesting free virtual machines that
are connected by a virtual cluster as
shown on the top right hand corner of
the slide now here have shown a
placement of these VMs and I've
highlighted the physical Network links
that connect these VMs now it is these
highlighted links that may carry the
tenants traffic and so it is on these
highlighted links we need to ensure
there is sufficient capacity to satisfy
the tenants bandwidth guarantee here
I've shown an alternate placement of
these three VMs whereby we pack the VMS
closely and you can see this reduces the
number of physical network links that
have to carry the tenants traffic and so
we have to reserve bandwidth for the
talent on a fewer number of links so
from the providers perspective this nice
compact placement is preferable to the
one that I'd shown earlier and the big
question for us was how do we find such
nice compact placements as part of
octopus we've designed a vm placement
algorithm that achieves two things first
it achieves correctness that is if a
tenant's VMS are placed and he's
admitted their bandwidth guarantees can
and will be met second x prize for
efficiency that is it tries to find
these nice compact placements that
minimize the impact of the tenant on the
underlying infrastructure thus allowing
the provider to pack many tenants on the
data center now I'm going to skip the
details of the placement algorithm for
those of you who are interested it's a
greedy first placement heuristic that
was inspired by heuristics used in
multi-dimensional bin packing if you're
interested or if you work in the area
please come talk to me and I'll be happy
to show you the details over a
whiteboard or take a look at our paper
now once a tenant request has been
accepted and it's VMs have been placed
we then need to enforce their bandwidth
guarantees and again I'm going to use an
example to give you a feel for how this
works there's a tenant who's renting n
virtual machines he's been promised a
virtual cluster and remember this is
just an illusion we are offering now
this virtual cluster promises that each
VM gets an aggregate guarantee for its
outgoing and its incoming traffic so if
you have a scenario like this where vm
one is generating a bunch of flows the
combined rate for these flows should be
limited to this value B this is easy
enough to achieve simply by putting a
rate limiter at BM one a simple software
component now the virtual cluster
abstraction also promises that the
combined rate for incoming flows should
also be limited to be mbps so if you
have a scenario like this where vm one
is now receiving a bunch of incoming
flows again the combined rate for these
flows should be limited to be MVPs this
is a little bit harder to enforce
because we now need to control the
aggregate sending rate of a set of beams
actually taking a step back since all
our virtual network gaps fractions have
this notion of aggregate guarantees in
order to enforce them we need to control
the sending rate of
buted set of memes which is precisely
the distributed rate limiting problem so
we distributed rate limiting or DRL we
have a set of distributed entities and
the goal is to control their aggregate
centigrade in our case are distributed
set of virtual machines so what we
realized was to in order to enforce our
abstractions we need to solve this DRL
problem now let's look at a quick
example we have n virtual machines and
we want to control their aggregate rate
to some value be the knife solution here
would be to stew Sadek partitioning each
VM gets an equal share which in this
case would be B divided by n however as
you can imagine that strictly suboptimal
because if some memes are not generating
traffic and not using up their quota
other VMS are not allowed to pick up the
slack so what we really want is a work
conserving solution whereby VMs with
traffic to send are allowed to send it
as long as the aggregate rate limit is
not exceeded and this is precisely what
our DRL solution tries to achieve so to
give you a sense of how it works let's
go through a simple example again a
Toyota Center with a few physical
servers and a physical Network there are
two tenants or two users here AB
lieutenant a red tenant the blue tenant
has been promised a virtual cluster with
some aggregate guarantee for each VM now
as I mentioned to enforce this
abstraction we need to do DRL and to
achieve this we associate a controller
with each tenant here I'm showing the
blue controller which is responsible for
the blue we amps and periodically this
controller gets information regarding
the traffic statistics of individual VMS
based on this information the controller
uses the algorithm to determine the per
vm rate allocation such that the
aggregate rate limit is not exceeded
this powervm rate allocation is then
communicated to the individual physical
servers where it is actually enforced in
a prototype this is done inside hyper-v
inside the virtualization layer and that
has two implications first we don't need
to trust the tenant applications or we
don't have any trust issues because
tenant applications cannot muck with our
VM rate allocation and second we do not
need to modify the guest OS nor do we
need to modify the tenant application
which is great from a deployment
perspective so that's how DRL works in
our framework now as I mentioned we came
upon the DRL problem because we were
trying to enforce a virtual network
abstractions but broadly speaking DRL
can be seen as a building block that can
be used to enable novel scenarios that
require predictable performance for
example I showed you how to do DRL for
network but you can apply the same
concept to other shared resources for
example in follow-on work we have done
DRL for storage and that's something
that I will be talking about related
with talk so the point here is that DRL
is this key technology that allowed us
to enable predictable network and
predictable storage on top of which we
plan to do predictable data centers so
let's see how this works in practice we
built this weed applaud this on a small
test bed upstairs on the third floor I
actually the test bed is on the fifth
floor but if some of you are interested
I can show you a demo later we also used
a simulator to see how octopus word
would work at data center scales and the
key message what we found was that
guaranteed network performance is great
for users because they get
predictability but surprisingly it's
great for providers to because it leads
to increase system throughput or cloud
efficiency so the use of virtual
networks benefits both tenants and
providers and this is something that
shows up in our results too so just a
quick experiment what was happening in
this experiment
is tenants were coming in each tenant
was requesting some VMS and a virtual
network on the x axis we have the target
data center utilization or how loaded
the data center is so as tenants arrive
faster the utilization increases on the
y-axis is the percentage of tenant
requests the provider had to reject so
lower is better now I wanted to focus in
two lines the red line which is baseline
and it represents the operation of
today's data centers where users don't
get any bandwidth guarantees and the
green line which is the virtual cluster
abstraction the other two lines please
disregard they are other abstractions
that I haven't described in the stock
now you can see almost across the board
virtual cluster allows the provider to
accept more tenant requests as compared
to baseline to put this in perspective
most data center operators target a
utilization of around seventy to eighty
percent and at seventy percent we find
that with the virtual cluster
abstraction the provider is able to
accept sixteen percent more requests as
compared to baseline now that's a
counterintuitive result because with
baseline are there are no guarantees
whereas with a virtual cluster a tenant
request is only accepted if we can
satisfy their bandwidth guarantees so
what's happening here is that with
baseline tenants don't get guarantees
some of them get really poor network
performance they become outliers which
drags down the system throughput as a
contrast with octopus tenants get
guaranteed bandwidth there are no
outliers and so we don't have that
problem and this shows up or this has
implications for cloud pricing too so we
used today's as your pricing data and we
did a simple cost analysis and just as a
simple sample point at a load of around
75 percent we found that the median
tenant cost with octopus reduced by
forty two percent while the provider
revenue increases by twenty percent
and again that's a continuity of result
but it comes from the fact that octopus
increases system throughput more tenants
pass through the system so individual
tenants can pay less while the provider
revenue increases so this is the nice
win win situation that I was talking
about which is great because both
tenants and providers are happy so
that's the octopus part of the talk now
when we did this work and I went out and
gave talks we got lots of useful
feedback from our product groups in
Microsoft from our colleagues in ms are
from the broader research community and
that has influenced some of the
following work that we've done actually
we pursued three main research
directions which you can see were
greatly influenced by three questions we
were often asked and I want to comment
on each of these briefly the first
question we were often asked is network
bandwidth guarantees are great but can
users actually specify their network
requirements very few users are
sophisticated enough to say I want 10
virtual machines with 500 megs of
network bandwidth and that's a fair
comment so this motivated us to design
bizarre a system that enables a higher
level interface between tenants and
providers in these cloud environments
with bizarre tenants only specify high
level goals regarding their applications
these could be performance goals I want
my job finished in for three hours or
cost codes I have a maximum budget of
three hundred dollars and these
high-level goals are automatically
translated onto the underlying resource
requirements for example to finish a job
in three hours you need 25 virtual
machines with 500 megs of network
bandwidth between them now I think this
is pretty cool but to achieve this
vision we have to solve two key
challenges the first challenge was
performance prediction given a job and
given some set of resources we need to
be able to predict how long the job
would take to complete
now performance prediction is a very
hard problem there is tons of existing
work and actually for arbitrary
applications I'd like to claim it's
almost an intractable problem so we
focused on a specific yet important
class of cloud applications data
analytics and we built a performance
prediction tool for MapReduce jobs the
way this Tool Works is given your
MapReduce job and some sample input data
we are actually going to execute and
benchmark the job we're going to collect
some statistics which is then used to
predict the performance of the job if it
were running on the actual input on some
number of resources the second challenge
we had to solve was there may be more
than one way of satisfying a given high
level good for example if I had to
finish your job in three hours we could
use 25 virtual machines with 500 mix of
network bandwidth or we could use more
VMs and less network bandwidth trade off
one resource for the other now from a
tenant's perspective as long as the high
level performance goal is met it does
not matter which resource combination we
use but this flexibility of the resource
combination is important from the
providers perspective so we devised a
resource selection heuristic that
chooses the best resource combination
from the providers perspective which in
turn would allow the provider to accept
more future tenant requests the really
neat thing about Bazaar is its
implications for cloud pricing today
cloud providers do resource based
pricing right you go to as your you pay
some cents per hour for each VM with
bizarre we can enable job centric
pricing so we can move away from
resource and click pricing the provider
can say things like to finish this job
in three hours will cost three hundred
dollars actually the provider can go you
will further and he can offer multiple
cost performance options to the user
which is great because users get
predictable performance and predictable
costs so we built the bizarre system and
just just to give you a sense of how it
performs the graph is similar to what
I'd shown earlier x-axis is the target
occupancy or how loaded the data center
is y axis is the system throughput or
system good put with bizarre as compared
to baseline and again at a load of 75%
we find that bizarre improves the
overall system throughput by around
fifty six percent which is great because
tenants their jobs finish on time their
performance goals are met while the
provider has increased system throughput
because of the extra flexibility which
hopefully would lead to higher revenue
so that's bizarre now the next question
we were often asked is network bandwidth
guarantees are great but what about
statistical multiplexing with octopus
users get bandwidth guarantees but they
are not allowed to exceed their
guarantee even if there is unused
network capacity which is a little bit
funny so this caused us to revisit our
network sharing requirements to address
the multiplexing question what we want
to do is offer minimum bandwidth
guarantees that tenants can exceed if
there is unused network capacity and
this is great because tenants get
predictable performance while the
underlying network is efficiently
utilized another requirement here is
payment proportionality so the
guarantees that i'm going to give you as
a tenant should be in proportion to your
payment now next quickly look at how
existing solutions fair with regards to
these three requirements so in today's
data centers you can use the transport
protocol of your choice you can use tcp
you can use UDP you can use any
transport protocol and what most of
these ensure is high utilization of the
network but users don't get bandwidth
guarantees
and there is no notion of
proportionality so this has prompted a
lot of work in this area as I mentioned
with octopus we offer bandwidth
guarantees that you cannot exceed even
if there is unused capacity now I'm not
going to go into the details of all the
solutions but the key point here is that
none of these satisfy all three network
sharing requirements and all of these
focus on traffic between VMs of the same
tenant or intra tenant traffic actually
we've done measurement in about eight as
your data centers and what we found was
that a significant fraction of the cloud
traffic is actually between tenants so
this motivated us to design hadrian a
network sharing framework that caters to
both inter and intra tenant traffic and
it satisfies all three network sharing
requirements now just like octopus
hidran comprises of three components
there is virtual network abstractions
there is a placement algorithm and a
bandwidth allocation strategy and all
these are more sophisticated than
octopus because of the more advanced
network sharing requirements for example
the bandwidth allocation strategy in
hadrian is rated sharing of the network
as compared to reservations in the case
of octopus and what this allows us to do
is offer minimum bandwidth guarantees to
users that they can exceed if there is
unused network capacity again just to
give you a sense of hard performs
similar sort of a graph comparing the
baseline scenario against octopus
against Hadrian and by utilizing the
network more efficiently Hadrian is able
to accept more requests than baseline
and as compared to octopus so that's the
statistical multiplexing question now
the final question we were often asked
is what about other shared resources you
know when you run a MapReduce job in a
cloud it leads to network traffic where
it also leads to axis of the shared
cloud storage service and it's this
question that motivated the design of
splitter which is the predictable
storage component of the PDC framework
splitter allows the cloud provider to
express high level performance policies
regarding storage flows from VMs to
storage servers now the structure of
these policies is shown here in red for
all I owe traffic input output traffic
from a set of VMs to a set of storage
servers we can specify a service level
agreement or an SLA this SLA could be a
bandwidth guarantee it could be a
minimum bandwidth guarantee or it could
be a priority level which I think of as
poor man's latency control now the thing
to note here is that splitter allows
both point-to-point guarantees that is
from a single VM to a single storage
server and it allows for multi-point
guarantees from a set of VMs to a set of
storage servers which is again an
example of our aggregate guarantee just
a quick view of the architecture this is
a block diagram don't get scared I'll
explain this this is a block diagram of
how iOS flow for application inside a vm
to the actual disk so when you do a file
read or write what happens is you're
going through the file system and the
block device inside your guest operating
system and so on and so forth till you
get to the actual disk now splitter
comprises of two main components first
we've added queuing functionality at
various stages along the i/o path and
what this allows us to do is it allows
us to direct our your packets to
different queues and thus treat them
differently but offered performance
differentiation the second component is
a controller just like in the case of
octopus that interacts with these stages
and this controller
Slate's high level performance policies
into configuration of these individual
stages so this configuration includes
how packets should be cured as in given
a packet which q should it go to and
what are the performance properties
associated with these qs now as you can
see this design is very similar to
octopus and the challenge in splitter
was to account for storage
idiosyncrasies for example if you think
about rate limiting which is typically
done in token buckets I don't know the
network probably understand that we've
known that in the network community for
years if not decades however rate
limiting for storage is particularly
problematic because the capacity of the
underlying resource varies depending on
the workload which is not the case in a
network in the network a 10 gig link
always behaves like a 10 gig link in the
case of storage if you get a disk mark
at 10 gigabytes per second the actual
throughput you're going to get depends
significantly on the workload that
you're offering so that's an example of
a challenge that we had to solve in the
spirit framework and this is some
ongoing work where we are collaborating
with the windows file server team to
enable storage a sleaze in the Windows
Server framework so that's the last
question and I don't know how we're
doing on time I do not oh we have plenty
of them so but I'm going to finish every
but I have tons of backup slides so if
people have questions we can do
questions or we can go into our various
details so we started with octopus which
is the predictable network part of the
PDC framework with octopus tenants get a
virtual network that is similar to the
physical Network they are used to in
enterprise settings and what we found
was that this can benefit both tenants
and providers or the course of research
we've designed a few systems that go
beyond octopus and the enable
predictable Network predictable storage
and they offer different kinds of
facilities and in ongoing work we
trying to put this all together and
offer end-to-end service level
agreements scenarios where you have
network and you have storage and you
want to offer a guarantee when these
resources are combined now I realize
there were lots of details but the one
bit of information that I hope you take
away is that in our research we've
enabled higher level interfaces between
tenants and providers and what we found
is that they can lead to a symbiotic
relationship between tenants and
providers because both parties benefit
and this is something that I generally
believe can spur the adoption of these
systems and these interfaces so on that
note our thank you all for your patience
thanks for your time and we can do
questions if people want more details
I'm sure I have some slides or I'm sure
there are markers here so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>