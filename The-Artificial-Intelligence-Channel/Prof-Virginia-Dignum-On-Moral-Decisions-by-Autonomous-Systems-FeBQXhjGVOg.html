<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Virginia Dignum - On Moral Decisions by Autonomous Systems | Coder Coacher - Coaching Coders</title><meta content="Prof. Virginia Dignum - On Moral Decisions by Autonomous Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Virginia Dignum - On Moral Decisions by Autonomous Systems</b></h2><h5 class="post__date">2017-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FeBQXhjGVOg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so our second speaker is Virginia
Dignam Virginia is a professor at the
Faculty of Technology Policy and
management Delft University her
background is in AI research and she's
done a lot of important work over the
years on agent-based models of
organizations and the dynamic aspects of
organizations and how all that plays out
in an AI context he's now leading some
important grants in in Europe on design
and values with particular reference to
AI and she's co-chair of the European AI
conference this year
today Virginia will be speaking about on
moral decisions by autonomous systems
please welcome Virginia Tech good
morning thank you for your introduction
and it's a pleasure to be here and to be
able to discuss and present the work
we've been doing in the aft and with
some other colleagues in Europe on the
moral decisions by autonomous systems
but more generally on responsible III
and
nope no I lost it
okay so our work is not so much on the
long-term consequence of a high but on
the very short-term and the decisions
that systems are already doing now and
like Peter dooming said some time ago
machines are very stupid now and are
already taking a lot of decisions for us
so how can we make sure that those
decisions that they are making are
beneficial and understand our how are we
dealing with those decisions who
determines which who is right and wrong
what is right and wrong and I should
which me which decision should the
machines being taking and which
decisions should we not want the
machines to be taking and moreover what
is our role as users as owners as
developers and as society in general on
the decision that machines can or not be
taking the work we do is basically
considering we take two directions in
the considering the social implicit and
social implications of artificial
intelligence as it integrates our lives
now and changes and enters the
traditional systems that we are usually
used to deal with we look at two
different aspects in one end the
integration of ethical decisions into
the machines themselves and on the other
hand on the what we call the research
integrity and the our role as designers
of the systems and our role in the
process of designing those systems which
is where we look at the design for
values approach I will talk briefly
about both aspects starting by the
ethics by design or the responsibilities
by design and like I said that is about
how do attics take a role and overall in
problem-solving on the decision-making
and on accountability that systems
should or should not have taking a
general approach to
official intelligence we like David said
I have a background on autonomous
systems but most of us will agree that
an artificial intelligence system will
have this three aspect adaptability or
learning interaction and autonomy and if
we want these systems to be somehow have
a sense of responsibility of the reticle
awareness of what they are doing we
contend that we should extend this
traditional view of an intelligent
system with a few more autonomy we
should and to take into account the
responsibilities and any of you like me
as a parent now that that's one of the
main issues of educating children is to
learning them to have the autonomy and
taking the responsibility for the
decisions on the accountability and
being able to explain out what you did
and now that decisions took taken place
in interactions is another aspect which
we should look at and finally the
unlearning and adaptability we should
look at the transparency of what we're
doing so it is about the deciding the
explanation and the inspection of the
systems that we have considering the
decisions of response taking
responsibility for decisions there is a
lot of work and it has been mentioned
both by David and by Nick already on all
moral dilemmas and all issues that at
this moment to the car should or not be
driving over old ladies or people or
young people or whatever it's not so
much my issue here but it is a very
simple and very visual and an example
that people understand and can relate to
very easily so it's an example which we
we use as a pedagogical example let's
say so what we want to look at is this
dynamic were not necessarily the trolley
dilemma in itself but moral decisions
are per definition imply a lot of
dilemma because there is not a one right
solution
and you have to design between two less
optimal solutions and machines have to
do those decisions as well so what we
have looking at is that it is not just
about letting the machines deciding and
learning how to take the decisions on
such a moral dilemma of the trolley who
should you kill or should not kill on
the cars but it's very very much there
are options there are more options than
this one so we should look at different
options on it so if we take into
consideration both who is taking the
decision in itself and how those
decisions are taking we come up with
four possible trolley dilemmas scenarios
of which this one is what is been
discussing very much the machine itself
internally to the Machine the decision
should be Mike and the machine
deliberates and takes into account
whatever information it has from the
environment to take that decision but we
contend that there are other ways to do
it
the simplest one of course is to go back
to the traditional trolley dilemma
humans are there humans are in the loop
and in many cases it's the best decision
for the machine to tell I don't know
sorry in this case you decide I'm not
one with going to decide because I
cannot choose between the two different
situations in this case of course if the
human is in the loop then we have to
take into account the issue of shared
awareness how is the machine able to
alert the use the user or the person who
is going to decide to what is the
situation at this moment and now be
ensuring that the user is keeping aware
of the decisions of the situations and
not just disconnect and let the Machine
go and do whatever it is and wake up at
a certain moment and have to take a
decision without having the the full
information about the situation so the
situation awareness and shared awareness
is a very important issue here which we
can do much more than what is they're
not ready of course the other one is the
regulation one is about creating the
either physical or regulatory or
thorry infrastructures which take the
decision or make sure that there is no
decision to be taken one example I give
my students very often is that if you go
to the Metro there are those huge doors
metal doors which only open when you
have the ticket so you don't have any
more to do they liberate yourself
whether I should buy a ticket or I
should try to ride the train without
buying the ticket because the old
infrastructure doesn't allow you to get
to the Train without the ticket so those
kind of infrastructure also regulatory
or governor gonna cover governance the
inner structures are also wise to deal
with the moral decisions and then the
decision is outside in the outside the
system who is doing it and of course the
last one is the random one okay we just
take a decision and we'll see what what
happens at NUI we can guarantee that
it's random so I'm not to blame high at
no information whatsoever I just did the
first thing which got to me and of
course then I'm always off the hook
let's say whether this is the most
accountable and the most moral decision
we can discuss but it's a way to deal
with decisions by machines
so this concerning responsibility now if
you look at accountability there is been
quite a lot of work in either which
people say that when we work with
machines
you give machines and there is a lot of
work on that that they you give machine
the same kind of feeling of partner or
teamwork or agency as you do to people
but in other hand there have been quite
a lot of experiments also with the
trolley example in which it shows that
people expect the machine to take the
more rational or the more utilitarian
decision than the person so while people
expect from each others in such a
trolley problem that you shouldn't take
the decision of actively switch the
lever to kill or not kill someone in the
case of if it's a machine which is
taking the decision people expect them
more to take the decision in which more
lives are
saved so it has a lot to do in this case
when the machine does take a decision or
by itself about how to explain and how
to take that explanation at a level that
the person understands and that it's not
just a very technical and very deep
neural network chain of events which no
one can understand but to make those
explanations in a way that people can
understand we as persons ourselves
happened very often to make those
decisions not all those the explanations
we give are very often created a
posteriori by creating a kind of a shine
of events which are plausible for the
actions that we took whether we want
machines to use those same social
eristic as we as people are doing is
something we also should look at line it
and as we talk about accountability
one other issue and from the research
that I do which is a lot on persuasive
systems and those systems they somehow
not at all like this but somehow tend to
have some human-like characteristics
that's another issue which is important
and accountability more human-like
system here and more human-like the
system returns or aims that make you
believe that it is the more the
accountability that system should take
for explaining what it is doing this
image here down of the old lady Ellis is
from the commentary that for the Dutch
TV one or two years ago in which they at
that Ellis the robot live live with some
people in an elderly home and Ellis
really people really engaged and become
big friends with Ellis and they would
show the pictures of their children or
of their youth to Ellis and engaged with
Ellis in a very natural way and they
didn't knew that Ellis was not a real
robot in the sense that it was taking
decisions itself but was a whole wizard
of horse experiment so this kind of
dealing with
human-like and especially with the
vulnerable users it has a lot to for us
to be accountable for as the way to we
want these systems to explain themselves
or to be dealt with and be understood by
their users which brings me to the
transparency issue which has a lot to do
about we cannot expect the systems and
especially machine learning systems to
learn and to know it all right away at
once and being fully able to take
decisions from the very first beginning
as we don't expect our drivers when they
start the driving to be fully
understanding of the traffic laws and
whatever and in many countries countries
you use those out lights to indicate I'm
just learning and excuse me for the
mistakes I might I might make there is
that kind of issues we can also look at
that and make it aware give the
awareness and the explanation to people
that the system is just learning is not
yet a full-fledged system next to that I
think one of the most important issues
is to look clear I think a new focus on
the old machine learning algorithms
which is not so much to focus on the
performance to the functional abilities
of that system but to use the feedback
loops also to learn some moral
principles to those systems in which
guide the ways that systems will learn
and will evolve from where they come
from and of course a lot of the learning
has to do with the data that we give
these systems the systems can be become
sufficient goods to learn from their
data but neither end there is a lot of
evidence
and a lot of studies which show that any
system which is learning based on data
which is generated by us as people as
societies will necessarily take take
into the learning the bias that we have
ourselves and we if you don't make that
explicit and we don't really assume that
those issues are there then it makes it
very and an irresponsible let's say to
have the systems taking decisions
without that explicitly making the
learning and the bias the assuming that
buyers are there so and another issue
which I would like to look at is the
ownership so these systems are typically
owned by corporations and those
corporations have their own interests as
well their own biases which are also not
explicitly put into the Learning and
Development of the systems which then
leads me to the other side of my talk
which would be a which is about the
design for value so about our own
ethical and moral responsibility as
designers as users as owners of these
systems and which is of course here in
the New York University there is a long
tradition of design for various research
in Delft we also have quite some work
which we do on that and and it's like I
say about our awareness of our role and
as we do that then one of the main
issues is to understand which are the
values the moral values which are we
building on we are taking into
consideration in the design and in the
systems that we are building from moving
those values come which of those values
are important and is it the question of
that the system should follow those
values must follow those values or could
follow those values so that
some issues which we don't really have
looked much yet in a simple project that
some of my students have done we have
looked at argument I online the
deliberation systems in which people you
can get like what if you want the social
acceptance about a certain topic and
then we ask those same people who agree
on the social acceptance of smoking or
not smoking or the selling fried food in
the university restaurant all kinds of
stuff on political issues we ask them
after they have done the social
acceptance of what we think it's as
democratic the democratically Charles
decision we ask them to look at the
moral acceptability of those decisions
in terms of fairness in terms of harm in
terms of ownership in terms of authority
and the results get quite different so
things which we and we have seen that
again and again also in the recent
crisis of referendums all over the world
that what we vote has a majority we
think that this is the best solution is
in most cases against our own principles
of the same groups which vote for it if
you take their moral acceptability so we
are doing quite a lot of work in this
putting bringing together and aligning
the moral acceptability and the social
acceptance but of course then we get
very quickly into the moral overload we
want too many things at the same time
which are uncomfortability incompatible
we want sustainability and leading to
having natural gas buses which then are
not as safe as we would like them to
have we want to have both or we want to
have efficient energy use but then those
smart meters leads us to less privacy
and less security for the owners of
those things so how to deal with the the
moral overload in these cases is also an
issue which requires a lot more work
than what has been done so far
which leads me them to the bleeding a
bit to what nickel so have the issue of
governance and there is a lot of
discussion on shall we regulate first or
shall we allow for all developments in
all kinds of directions to go and what
comes first shall we first it's easier
to ask forgiveness than to ask
permission it's one of the things which
you use which we are very often but in
very much in line with what Nixa is
actually those two lines should go
together it's not like we should first
regulate and then develop or first
develop and then regulate now we should
really take both passes into account at
the same time and integrating them
together as as one so I'm getting to the
end of my talk in which basically the
issue is the as we get into the issue of
responsibility by artificial
intelligence systems it's not so much
just the responsibility by the systems
are about the systems but actually we
are the ones who are responsible thank
you
okay
hi I wanted to know if there was any
thoughts about these kinds of
developments in terms of Education
like educating the machines we don't
raise children in school thinking how
are we going to make sure that we can
have total control over them or that
they don't do what we don't want them to
do yeah but I don't see any sort of
educational we raise children to learn
how to be responsible for the decisions
they take not that we want to control
them but we want them to be itself
responsible for what they are and that's
very much what I claim that we should
learn machines to raise machines to do
and it's not something which should we
should accept or expect to be
automatically and the first version of
that machine that it will be fully
responsible for what it does but we
should create the Sun boxes or the safe
environments in which the machines can
learn about taking their own
responsibilities without being right
away in the positions in which they have
to take the life-or-death decisions hi
my name is dawn my question is you're
saying that the interface from human
beings and telling the computers of the
artificial intelligence what is good and
what is bad in choices maybe in medical
decisions for children for example you
tell someone it the child it's good or
it's bad eventually they will want to
investigate the opposite of what you've
told them and how could you ensure that
the artificial intelligence would not
want to investigate that even though
it's been told in an algorithm that you
know this is the good portion that you
should be focusing on that's a very yes
that is a risk I think that like I said
if you
do have some experimental sandbox
environments where machines can
experiment before they really are
interacting in a full-fledged real-world
environment that would be the place
where we could let machines experiment
with what is the opposite but of course
like I say at the end of the day we are
the ones who are responsible we make
those machines so if we let them explore
or we create the possibilities for
machine to explore and also to explore
the wrong or the BET's paths we should
also take into account probably in
governance and in regulation to deal
about do to deal with the consequences
for who and for whom and how to deal
with consequences of wrong decisions by
machines
hi so the up so now it seems that
there's two separate issues that we are
be discussed namely how to make machines
morale and on the other hand how should
we treat machines morally but shouldn't
they be very closely connected as they
are with human beings namely why do we
behave morally because it gives our
lives meaning and we also expect to be
treated in a moral way so for machines
wouldn't that hole as well once we start
treating them in a certain manner they
might also be inclined to treat us in a
certain manner I think that connects
also to what Nick was saying about
comparing machines and mice and indeed I
don't really know what that experience
experience comes from but there was a
few months ago some experience
experiments going on on which you showed
people films of people kicking kicking
robot dogs and how people felt outraged
by the feeling that they were
mistreating those robots because you
could not really just go and kick the
robots poor robot so you really give the
robots an anthropological or some
feeling of agency and of being an entity
in itself with feelings or with the
right to be treated morally that's
indeed a very important aspect which I
think it has been very little research
so far but definitely it is an issue to
consider and to give machines the
feeling of being moral being itself and
use that as a basis for the decisions
that the Machine would take in terms of
treating earth that others also morally
or not
hi I'm Sasha so I have a question about
so I as humans like we've come up with a
set of constructs to discourage like
anti-social behavior in humans or to
punish antisocial behaviors such as you
know like social alienation or like I
don't know jail time things like that so
should there be such a set of constructs
equivalent constructs for AI systems and
if there should be then what would that
be to discourage behavior there is yes
we definitely wilt into the feedback
feedback loops and reinforcement loops
of the learning capabilities of machines
the moral aspects so to encourage or
discourage moral encourage monobehaviour
and discourage immoral behavior that
it's definitely something which should
be taken as much into account into the
feedback loops and the the the learning
algorithms of systems as much as optimal
identifying of the correct decision in
terms of the functional aspects of it so
that's one of the things we should do
how we are going to build those moral
features into the learning algorithms I
wouldn't really know myself but I
believe there are people in the room who
probably have much more to say about it
tomorrow I believe Stuart so there are
work done on that not by me
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>