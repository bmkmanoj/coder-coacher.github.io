<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Nick Bostrom - Artificial Intelligence Will be The Greatest Revolution in History | Coder Coacher - Coaching Coders</title><meta content="Prof. Nick Bostrom - Artificial Intelligence Will be The Greatest Revolution in History - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Nick Bostrom - Artificial Intelligence Will be The Greatest Revolution in History</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qWPU5eOJ7SQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I work with a group at Oxford University
running this thing called the future of
humanity Institute which maybe I should
just say a couple of words because it's
a sort of slightly grandiose name but we
are about 20 people mathematicians
computer scientists philosophers and our
remit is to try to think carefully about
the really big picture questions for
Humanity and in particular what
technologies might fundamentally change
the human condition so what kind of
thing would count as a fundamental
change in the human condition you could
argue that if we look back over history
there has really only been two events
that have fundamentally changed the
human condition the first being the
Agricultural Revolution some ten or
twelve thousand years ago in Mesopotamia
where we transitioned from being
hunter-gatherers small bands roaming
around to settling into cities growing
domesticating crops and animals with
that you get social stratification if
there's a lot of grain you could have a
king or Pharaoh who extracts the surplus
you could have standing armies you can
have war you can have higher population
densities specialization of Labor and
from that point on innovation grows much
faster and population grows faster as
well as a result the second fundamental
change in the human condition Industrial
Revolution where for the first time you
have the rate of economic and
technological growth outstripping
population growth and so only when this
happens can you have an increase in
average income right before that that
was technological growth and economic
growth but the economy grew ten percent
the population grew ten percent
everybody's still in a Malthusian
condition now you could say that there's
been one other event that has sort of
made a fundamental difference which was
the rise of Homo sapiens in the first
place and so I think that there are
relatively few potential technologies
that could have this kind of impact and
perhaps foremost among them is machine
intolerance and in principle you could
argue that it could have an even bigger
impact than this you could say that if
you think about it the difference
between our great ape ancestors and the
human species was some relatively small
rewiring of our neural architectures and
some scaling up so maybe the human brain
got three times bigger
but in principle in the longer term the
difference between the human brain and
sort of a mature machine intelligence
civilization could just be vastly bigger
than that there are obvious ways in
which information processing in machine
substrate is not subject to the same
limits as information processing in
biological tissue you could have a much
bigger machine brain can be the size of
a warehouse or in principle the size of
a planet or whatever it doesn't have to
fit inside the cranium can operate at
much faster speeds as well so if we take
a look at where AI has been and where it
is today so it's for a long time be
possible to build these expert systems
where some human engineer kind of
handcrafts knowledge tokens puts them in
a big database and then you can make
simple logical deductions from that but
you really only get out what you put in
and these were very popular in the 60s
and 70s and they still have various uses
that they can be useful but there's a
sort of limit to how possible it is that
this approach would scale turns out that
these are quite brittle hard to maintain
and once you go above a certain critical
size it's kind of hard to make all the
pieces fit together
another thing that AI has traditionally
been good at are solving problems where
the rules are very clear like the
rubik's cube you might remember from
when you were small so this kind of
thing you do get a very high level of
performance or
but in recent years and I don't know how
sort of close do you have follow this
but in the last seven eight years or so
there has been this great breakthrough
and excitement surrounding that in that
we have now figured out how to solve
perception basically this used to be
very difficult but some five or six
years ago it became possible for the
first time really to have a machine
intelligence that can look at pictures
just gets the raw pixel data and see
what's inside them with some you know
reasonable but not perfect degree of
accuracy so this is captioning here
where in many cases you just feed in
these pixels and it can see that it is
like group of young people this is very
hard to do kind of impossible with the
old-fashioned approach to AI you can't
write down a set of logical rules that
define which combinations of pixels
represent a cat or a dog they're just
too many different kinds of cat and dog
different light conditioned different
poses its infeasible so what you really
need to do to do this to solve this kind
of problem is machine learning you need
and how many of used to get sort of a
sense for your level of exposures I mean
if you err no no like the term deep
learning how many of you have forgotten
how many reinforcement learning
generative adversarial networks
all right so okay so that's good see at
you that's I get just engage sort of
that so I think this is right for the
right so this is kind of exciting and
the other things you could do you could
do these hallucinations this is from a
few years ago where if you run these
image recognition and networks in
Reverse
you can kind of get them to hallucinate
so if they've been trained on a lot of
pictures in in some data set with
pictures from the internet with a lot of
dogs and stuff they could kind of dream
up these creepy surreal images and the
reason why there are so many like eyes
and stuff is that they can't really
count they don't they have a sort of
sense for the local structure but not
really
the ability to count also there is some
sense that these systems are capable of
forming visual intuition so you can feed
in say a picture and painting and have
the network output the picture rendered
in the style of that painting there is
another picture and the front door and
it can produce a rendition that is sort
of somewhat in the style of Hangul so
they suggest that it has at least some
notion of sort of the the essence of
hunger this style of hunger it's not
just looking at individual pixels light
it's kind of limited but but it's still
something that would have been very
difficult or impossible to do even just
ten years ago more recently this has
been done this kind of style transfer
with moving pictures as well so you feed
in a little image sequence of a horse
and in real time input see bride can
kind of render the same video but in the
style of zebra and you can see it's
pretty good to doing this not perfect if
you look at the tail of the zebra you
can see that there are some artifacts
but but this this is this is very
difficult to do you can't write down a
rule for this imagination so these are
scenes that don't exist
this is networks rendering different
imaginative video sequences on the theme
of beach and you can see that it
certainly looks Beach like it's certain
not photorealistic exactly but it
definitely has some sense of what like a
video sequence at the beach here's
another of Gulf different for Asians and
you could have the same Network generate
sort of arbitrary numbers of these
different imaginative sequences on a
given theme so these you how many of you
have seen this stuff okay so this is so
in addition to doing this kind of
perceptual networks you can also train
agents that is you can train a little
programs that can drive some actuator in
in this case Atari game actuators where
you sort of move
joystick and you can train it end to end
so all that it gets in or the raw pixel
data from the the video screen and then
through trial and error it gradually
learns to play this game initially it's
very poor but it kind of gets better
400 training examples
so this is a fairly simple algorithm
it's not been pre-programmed with any
knowledge of this particular Atari game
it just has the general ability to try
things out and learn from experience and
so fairly soon at least in this game
breakout it figures out an optimal
strategy which which the people program
this didn't have a clue that this is
like how how you do really well with so
it can kind of invent new strategies and
Pub discover new strategies and policies
that were nap reprogram this is more
recent where it gone exactly the same
thing Bob both of these are from Atari
so you can now do this in more
complicated 3d environments and such and
the exciting thing here is not that you
could program a computer to solve
breakout or to navigate the maze
impossible to do that for a long time
but that the same one and the same
algorithm can learn to play any of a
wide range of Atari games without any
previous general learning algorithm you
can kind of do the same thing not just
with visual perception but with other
sensors as sound you can have networks
composing music
it's not
period but as you can hear it's kind of
if you only listen to a three or four
second clip of it it sounds pretty good
but it lacks any sense of the the high
order structure that makes it any sense
of the emotionality of it or the more
the compositionality of longer pieces so
it still is weak on that longer range
structured more recently also work in
trying to combine these kinds of neural
network structures with more traditional
AI approaches by for example having a
neural network that is trained to be
able to use an external memories you
have these memory of mental networks
where you can have a sort of readwrite
structure outside the network that it
can learn to operate and the hope is
that with this you can get better
ability to do things like counting that
is hard to do with just the simpler
structure so a lot of excitement in this
area a huge boom in the number of
talented people going into this field
technical conferences growing by fifty
percent a year burst of papers a lot of
talent funding and of course a parallel
investment also in the hardware that has
been fueling this and there's no more
work in producing circuitry that is
specialized for running these kinds of
machine learning applications some major
outstanding research challenges things
that still are very hard to do is we
certainly would need more effective
learning algorithms a lot of these
examples I saw were trained on huge
numbers of examples thousands of
examples of each category if you wanted
image recognition you would want to do
better unsupervised learning where we
don't have a label but so humans and
when you grow you learn a lot just by
running around randomly in the world and
seeing even if nobody is telling you
whether you're doing right or wrong but
most of the time we're just sort of
observing and and extract data
structures from that one-shot learning
so we humans can often learn something
if somebody shows it starts once one
example is of
in US and then we get it we don't need
the house and iterations of everything
transfer learning which these things are
able to do to some extent but you need
more powerful that's when the thing
you're learning about one domain can
make it easier for you to learn about
another domain so if you learn to play
somewhat horror games you maybe it's
easier than to learn a new Atari game
you haven't seen before but we are quite
good at doing this across a wide range
of different domains reasoning this is a
big area where these are very weak still
natural language understanding and deep
hierarchical reinforcement learning and
a sort of planning in a more structured
way so we humans can conceive of some
goals some end state that one achieve
and then kind of recent backwards from
that like what sub goals do we need to
attain in order to achieve our bigger
goal break down the big plan into
smaller steps that then makes the whole
thing more tractable so that's something
that also is is lacking currently so hmm
one question one way to ask is what is
the timeline for actually solving these
problems these capabilities that would
be required to get full human level
intelligence things that can do all the
things that we can do you would need
these things we did a survey recently of
some of the world leading experts in
this field technical experts and for the
purpose of the survey we define this
notion of high level machine
intelligence as any machine intelligence
that can accomplish every task better
and more cheaply than human workers
setting aside those were sort of humans
by definition if if only humans are
allowed to be during members it would be
unfair to require the AI to do that but
every sort of functionally defined task
and and here is the outcome from that so
on the 100 have a the probability of
this human or high-level machine
intelligence being achieved it goes up
over time obviously
and you can see two things here so these
faint length lines that you can just
barely make out there is some random
samples from different individual
responses and you can see they're kind
of all over the map so there is really
no consensus among the expert community
as to the timeline for this some things
is just around the corner something it's
never gonna happen or it's gonna be
hundred years it's just a lot of
different opinions nevertheless if you
do take the median you get this this
this curve here in the red and sorry if
we take the median opinion among this
expert community and we look at by what
year is there let's say a twenty five
percent probability that this level of
capability will have been attained you
can get out to sort of roughly twenty
twenty-five years so that that's within
maybe the working life of other people
in this room there's like it the expert
median opinion is there's twenty five
percent probability that yes can do all
that humans can do meaning there's no
jobs left for humans that humans need to
do like it's right so this is not just
oh I get a little bit more advanced this
is like literally they get all the way
to full human level intelligence not as
in the sort of limited citizens but yeah
real human level yeah and and the 50%
mark somewhat later but still you know
within the lifetime of a lot of people
today so it's not the case that the
hypothesis that we might get full human
level intelligence is only taken
seriously by people who don't know their
stuff like crackpots kind of you know
hype stories like now actually this is
the median opinion in in the field but
with big o certainty on both sides it
could take a lot longer or it could
happen sooner so we might then ask and
this is a question that that I and my
research group think a lot about which
is that if and when human level general
intelligence is developed then what are
the likely outcomes of that I think it
can be instructive to look at at this
recent episode from a year ago with
alfa garden how many of you
follow this about half so again this was
the google deepmind that decided to
tackle this right this is basically
using the same kinds of algorithms that
you saw in the previous examples but
applying this to the game of go on some
other things so in October 2015
this alphago system had played a
training match against the european go
champion and then that was planned a big
championship match in korea fall afford
for the following spring against Lisa
doll and he had observed the performance
of alphago in this training match and he
said that based on the level of
performance in this training match I
think I will win the game by a near
landslide so he's very confident at this
point in October 2015
then in February 2016 notes that I've
heard that google's deepmind is
surprising strong and getting stronger
but I'm nevertheless confident I can win
at least this time then after the first
it is the best-of-five after the first
game I was very surprised I didn't think
I would lose after the second game I'm
quite speechless I'm in shock I can
admit that the third game is not going
to get easy for me so now it's starting
to realize that you know maybe it's not
going to be such a rollover and then
after the third game in the best-of-five
I felt powerless not know the timeline
here so this is October 2015 the system
is performing at the rough sort of human
level half a year later it's at the
superhuman level so I think that the
answer to the question if human-level AI
is developed what are the likely
consequences is superintelligence is the
likely consequence and maybe not very
long thereafter like it might take quite
a while to get to human level general AI
but if and when you reach that point I
think super intelligence
it's gonna happen soon thereafter I mean
maybe hours days months maybe a few
years but I doubt it would be decades if
you get to human level until you would
have something that radically outstrips
all the best human brains in all fields
so we can kind of think of different
context here because the kinds of issues
that it makes sense to think about that
they're serious legitimate sane issues
to think about depends on which context
you are talking about so you have all
these science-fiction movies we like the
Terminator and all of that and then on
the other hand you have people trying to
actually get some AI system to work in
real life today and these are just very
different contexts so the issues that
the series in one context are not
serious in another context it would be
kind of crazy to worry about terminator
things if you're trying to get your
system actually to just do anything
useful right now but similarly if you
are actually having a discussion about
the ultimate outcomes of this it will be
equally silly to be focusing on the
current limitations and then these more
radical capabilities is what would
constitute the plausible parameters now
I think it actually looks more like this
so that there is this short term context
which is what we can do with current
technologies in over the next 10 years
and a little bit more then there is this
transition period where we go you know
to human level and then
superintelligence and then a very very
long deep future after that if things go
what might last for billions of years
like once you have reached technological
maturity that that could go on for very
long long time so if we look at
short-term issues in terms of ethical
problems that might crop up there are
various ones that have been discussed
this is like this self-driving car what
happens if you have to either run over
you know two old ladies are three
kindergarten kids like which one should
it run over it so I think these are
it's probably the least important of all
ethical problems that exist and I just
don't think that's where the action is I
think I mean as long as they can drive
better than a normal human driver under
reasonable conditions is well-rested
like at that point I'm very happy for
the self-driving cars there's been some
conversation about algorithmic
discrimination like if you have these
systems that decide who gets alone who
gets her all that cetera I could not be
sort of prejudices embedded in these
data ownership with privacy and such the
the ability to maybe use some of these
AI techniques to create better
impersonation so you can now have it's
not quite perfect yet but the ability to
say take somebody's video recording of
somebody speaking and then make a video
of what looks like the same person
saying some random other thing that you
have made up and the mouth moving and it
sounds like it's their voice except that
technology has moved forward quite
rapidly and you now get things that
definitely look like the person and
sound like the person but you can still
hear that there's something off right
now but that could create new
opportunities for fake news if you could
just have some famous person saying
something outrageous or confessing to
some horrible crime filter bubbles like
virtual bias has not been a big issue
yet but you could avoid it as well if
these imagination capabilities get we
get more powerful if you could sort of
have in your virtual reality like
perfectly lifelike simulated rape or
something or sort of new versions of
people you know you take a picture and
it kind of creates these like that that
could be a whole social conversation
about yes nobody's actually harmed its
virtual reality but maybe it's still
wrong to for people to engage in that
kind of stuff
killer robots and cyber weapons and such
labor market impact more internal the
sea of dudes is this observation that
among the people developing this
technology is like very heavily
male-dominated if you go to these
conferences and certain demographics and
that maybe that distorts if the values
of the people developing it techno
somehow influence is where the
technology goes that could be a problem
as well and broadly the question behind
here is like and as with other
technologies like how can society
flourish how can we adopt cultural norms
to have a thriving good culture with
Winnie's do this is when when when when
people we even invented like these
phones like mobile phones then in the
beginning people would have them on all
the time so when you went to the cinema
it would keep ringing and it took a
little while a few years for a sort of
culture to figure out that no you're not
supposed to have your phone on when you
go to the Opera or so so there it's like
a kind of adaptation we have to learn
how to live with these new technologies
so these are all near-term issues but
what about wishes here in the in the
long-term and beyond so I wrote a book
about this which kind of discuss I had a
big impact on some people
and I mean it's it's actually not so I I
do this I do I just put a lot of pages
there trying to I confess describing
what could go wrong but it's not because
I'm convinced that that the outcome will
be bad it's rather that it seemed to me
more important to have a detailed highly
granular understanding of where the
pitfalls are so we can make sure to
avoid them whereas it seemed to me we
could get by with a more vague sense of
all the cool things that can happen in
the long run so we're in sort of map
from the number of pages allocated to
topic 2 my probability estimate that
they saw the outcomes but III do think
that are certain significant risks
associated with this transition to the
machine intelligence era including
existential risks when one one class of
these have to do with the control
problem which is if you had the
abilities which we don't have now but
might have at some point in the future
the ability to create super intelligence
something that is radically smarter than
you are how could you then control this
thing how could you make sure that that
it will do what you intend for it to do
that and what one way to kind of
illustrates part of the difficulty here
is in this you have these myths right
with when somebody gets granted three
wishes like there's a genie earth in
this case it's the King Midas myth where
King Midas was granted his wish that
everything he touches to be turned into
gold which sounds like a great idea
endless riches and then he touches his
food it turns into gold eye touches his
daughter it turns into gold and it turns
out that what sounded like like a good
thing to wish for actually if you think
through the logical consequences leads
to disaster so there is this difficulty
of specifying an objective function that
doesn't have and in this literature this
is since over the last over the last
couple of years not so much when I was
writing the book but since there has
actually sprung up a
research field the standard example
there it's a kind of cartoon example but
it stands in for a wider class of
failures which is this idea of the
paperclip AI of an AI designed to
maximize the production of paper clips
maybe you build this to run your
paperclip Factory and while the AI is
weak this works out quite well because
the only way you can make more paper
clips is by running this factory more
efficiently so it does what you intended
for it to do but when the air becomes
sufficiently smart and powerful the
context changes and now a new set of
strategies come into view for this AI
that it can pursue that it predicts will
lead to more paper clips such as taking
control over more factories to make them
produce paper clips pushing humans aside
transforming the planet earth into a
giant paper clip factory or with launch
platforms to launch colonization probes
to transform the universe into paper
clips and so in this case it's paper
clips it's a silly example but it stands
in for a wider class of failures if you
write down an objective function and you
really think what would happen if you
took a sufficiently powerful optimizer a
super intelligent optimizer that just
tries to perform the actions that would
score best according to this objective
function for most objective functions
the predictable outcome would be a world
where were humans and the human habitat
would be destroyed and they would have
instrumental reasons to prevent us from
interfering with that not because they
hate us or resent us but just because
they predict that if they allow us to
switch them off there would be fewer
paper clips and if that's all I care
about then that would give them an
instrumental reason to try to prevent
our interference and so what what what
you have is this risk of miss
specification I've argued before I call
it the orthogonality thesis which is
that there is no necessary connection
between levels of intelligence and
motivation it's not the case that just
because you're sort of sufficiently
intelligent the automatic can become
benevolent and wise and good and you
could have any combination it could be
like very stupid and very nice very
smart and very evil or very nice or like
there's just these are just two
different axes and so that what we need
to develop are these scalable control
methods that would ensure that these
intelligent systems intelligent agents
continue to behave as intended no matter
how intelligent they become you can pour
on arbitrary amounts of intelligence and
they will still be safe and that's
currently an ongoing area of research
with this kind of grown-up a little
technical research agenda around this
with different ideas but it still we
still don't know the answer to it and
another sort of longer-term issue here
is the a governance problem in in a
world where there was this level of
capability super intelligence how how
would governance work how would
international relations how would you
have some stable arrangement what would
we actually want to do with this how do
you prevent arms races in this kind of
if this technology is that powerful and
there there is like some early efforts
but this is at an even earlier stage
than with a technical control problem
there has been some interest from from
the policy world in the last year or two
with sort of studies and reports by the
White House UK government some
philanthropic funders have come in and
given money to different enterprises the
open AI is this nonprofit that is meant
to be developing AI technologies for the
common good so that they're not all
monopolized by big companies is a
partnership on AI which is this industry
Alliance just announced I think earlier
this year the future of humanity
Institute has also joined kind of
creating a forum where best practices
can be shared and and hopefully as we
sort of move into this more radical
future that also these more fundamental
issues and safety challenges can be
discussed perhaps so
so these are the kind of fishes here now
as for the longer term
deep future questions I think
yesterday's like what what would
actually do if we succeed at making
human labor redundant and I think at
that point really you need to rethink
things from the ground up at that point
so many different things have changed
that you're no longer just tinkering
with you know should we change the
retirement age or should we like change
the education system at that point you
don't just have AI you have sort of
technological maturity across the board
if you have research being done by super
intelligences you're gonna like very
quickly do forty thousand years worth of
human research and so you didn't enter
this very different kind of post human
world where you need to sort of but I
think that there are these interesting
philosophical questions one could ask
about that but to some extent we don't
need to worry ourselves too much about
that I think that if we get this chunk
right then what would constitute success
here is that we end up on a path where
people would be in a good position to
then start to address this longer term
like if we got through this critical
transition in fact why isn't in control
of our destiny then we can sort of have
a long time available to figure out how
best to use them but that if we get this
wrong then we might end up on the wrong
track or go extinct or something like
that so these sets of challenges I think
are very important although there are
long term I think they spill over to the
shorter term in that we can kind of see
now that we need to be at a certain
place here we need to have solved the
control problem in particular we need to
have some reasonably workable solution
to to these governance challenges to
sort of avoid some disaster happening
there so I think that while we can
largely bracket the ethical questions
related to the deep future we do need to
at least some of us think about the long
term and start working with that whilst
of course also paying attention to the
short term issues related to the
applications as we roll them out thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>