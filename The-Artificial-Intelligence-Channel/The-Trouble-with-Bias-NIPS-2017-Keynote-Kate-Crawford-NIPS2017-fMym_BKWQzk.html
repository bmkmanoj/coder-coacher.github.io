<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Trouble with Bias - NIPS 2017 Keynote - Kate Crawford #NIPS2017 | Coder Coacher - Coaching Coders</title><meta content="The Trouble with Bias - NIPS 2017 Keynote - Kate Crawford #NIPS2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Trouble with Bias - NIPS 2017 Keynote - Kate Crawford #NIPS2017</b></h2><h5 class="post__date">2017-12-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fMym_BKWQzk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well that was such a generous
introduction Thank You Hannah I think
you are the original badass so I can I
can but step in your footsteps hi
everybody this is a truly epic room I
kind of feel like I need like a smoke
machine and a really big electric guitar
sort of justify this space but I think
we'll just have to press on regardless
so it is an honor to be here I want to
say a particular thank you to the
organizing committee for inviting me to
be part of nips 2017 my name is Kate
Crawford and I've been studying these
social implications of large-scale data
machine learning and AI and I think
we're at an extraordinary moment right
now we're basically at an inflection
point where the power and the reach of
machine learning is rapidly expanding
into many areas of everyday life from
healthcare to education to criminal
justice just have a look around this
room right now machine learning is huge
the the old school crew have just been
telling me that this conference used to
have 200 people you are now more than
8,000 people strong so something very
big is happening here and I tend to
think of the rise of machine learning as
being every bit as far-reaching as the
rise of computing itself or of mass
media in the 20th century a vast new
ecosystem of techniques and
infrastructures are emerging and we're
just learning their full capabilities
but amongst the very real excitement
about what we can do there are also some
really concerning problems arising and
as this community knows better than
anybody forms of bias stereotyping and
unfair determinations are being found
everywhere from machine vision systems
and object recognition to natural
language processing and word embedding
and you probably saw the many high
profile news stories this year about
bias from women being less likely to be
sure
high-paying jobs to gender bias and
object classification datasets like ms
cocoa to racial disparities in education
AI systems now just last month we saw
that the sentiment analysis tool in
Google's new natural language API is
labeling terms like black gay and Jew
negatively meanwhile terms like I'm
straight
or even white power we're getting
positive ratings last year we learned
that Amazon's
same-day delivery service was bypassing
zip codes that have large
african-american populations now this
matters because for people who live in
areas where they don't have ready access
to a grocery store or a supermarket
places that are called food deserts they
can rely on same-day delivery to get
fresh food so it really matters if
you're being cut off from the service
but take a close look at this map it is
eerily familiar and it because it looks
just like these historical redlining
maps of the US in the 1930s when
predominantly black neighborhoods were
literally circled in red pen by the
Federal Housing Administration and
people in those areas were being denied
mortgages and these maps are remind us
of the deep segregation that this caused
so the long histories of discrimination
live on in our digital systems often for
very complex reasons and they become
buried into the logics of our machine
learning infrastructures now of course
the biggest of the bias blockbuster
stories is this one by Pro Publica which
looked at the use of compass scores in
over 10,000 people in Florida and found
racial bias across the results now this
study has been quite controversial and
there have been several important
follow-on papers including those by Joe
LaCava Kleinberg Mullen - and others but
these are just a few examples of some of
the most well-known bias stories in the
last couple of years so put up your hand
if you kind of heard these I'm expecting
most of us yeah excellent that's what we
want to see so in my case I've been
researching issues to do with fairness
and bias for the last
seven years but have a look it's really
just in the last 18 months that it's
gone for being a few of us working on
this topic to it being a huge area of
research interest a shout out here
tomorrow art who made this fantastic
chart and of course Mart's and solan
Baracus hosted a fantastic tutorial on
fairness and machining yesterday and I
know a lot of you are there and this
surge of interest in biased questions is
totally justified because machine
learning systems are starting to impact
millions of people every day
so bias matters but keep in mind that
the common examples that I'm just
sharing out with you today adjust the
tip of the iceberg there are countless
back-end systems below the surface often
applying off-the-shelf machine learning
as a service systems that can propagate
bias in ways that don't have a customer
front end and these ones are much much
harder to see and the scale of this
problem is now being acknowledged by
leaders in the industry in just the last
two months John Jay and Ray are at
Google to Saturn Adela at Microsoft to
Mustapha Suliman who's one of the
cofounders of deep mind have all called
this a core problem for the field in
mustapha's words because of this scale
are these systems we can be hitting a
billion to two billion users of per day
and that means the costs of getting it
wrong are very very high so this
interest in bias is growing because
machine learning is now a huge business
so when major platforms or services are
found to be producing problematic
results that can be expensive as well as
disastrous for the people it affects but
just as we're beginning to realize the
scale of the bias problem we're already
seeing stories like this basically that
science has cured biased AI well this
kind of reminds me of what was happening
five years ago when people were saying
hey data is neutral and then we realized
it was anything but
well now we're hearing data can be
neutralized and while I think it's
really tempting to seek out silver
bullet solution to cure bias it's not
going to work one of my fav
at conferences in this area the fairness
accountability and transparency and
machine learning conference which has
been going for about this is going to be
at six the year has a lot of papers to
show just how hard this problem is from
a technical perspective and you should
check those out they're all online if
you haven't read them already but here's
what I want to talk about today when we
consider bias purely as a technical
problem which let's face it is hard
enough
we are already missing part of the
picture bias in systems is commonly
caused as we know by bias and training
data and we can only gather data about
the world that we have which has a long
history of discrimination so the default
tendency of these systems will be to
reflect our darkest biases now
structural bias is a social issue first
and a technical issue second and if we
are unable to consider both the social
and the technical halves of this problem
and to see it as inherently socio
technical then these problems of bias
are going to continue to plague the
machine learning field so even though
some articles would like to tell you
that the problem has been fixed we got
to brace ourselves it ain't fixed so I
think there really is no silver bullet
to this problem but there has been some
extraordinarily important work on it
done by this community and there's a lot
more that we can do which I'm going to
address today but the problem of bias
runs much deeper than is normally
admitted publicly so I think it's time
that we start talking about why bias is
such a hard challenge because if we seek
out the quick fixes it's not only gonna
miss the deeper problem we could
actually make things worse and I think
that matters for the people who are
going to be subject to skewed decisions
but it might also matter to the machine
learning field itself because if our
systems keep producing biased results if
people are unfairly kept in jail or they
can't get insurance or they receive
incorrect medical treatment then people
will no longer trust these tools all
want to fund this type of work so as a
twist on Game of Thrones
that could mean a new AI winter is
coming something we don't want so there
are a lot of reasons to care about this
issue
and get it right so my talk today is
going to address five themes the first
is what do we mean when we say bias then
I'm going to talk about two concepts
that I'm going to explain in a moment
the first is harms of allocation which
are the types of harms that basically
most fairness and machine learning
research is focused on right now the
second category is harms of
representation which is where we
urgently need more work but then I'm
going to talk about the big picture the
politics of classification itself and
what happens when we classify and I'm
going to end with some suggestions for
what can be done to tackle these harder
problems so first of all what do we mean
when we say this word bias well the big
problem here is that bias has
overlapping and sometimes contradictory
meanings and this is adding a lot of
unnecessary confusion to some critically
important discussions even the history
of the word itself has different
mathematical and social meanings if we
go back to the 14th century the word
bias first emerges in geometry to refer
to an oblique or a diagonal line by the
16th century and it acquired something
like it's common meaning today of undo
prejudice but by the 1900s bias had a
technical meaning in statistics where it
refers to systematic differences between
a sample and a population and as we know
we start to see selection bias as a
concept about errors and estimation when
some members of a population are more
likely to be sampled than others and
here's a picture that we all know and
love well in the machine learning
community of course on the Left we have
the classic visual representation of
bias through underfitting where a
supervised model fails to capture the
underlying trends and data that's a
situation where you have low variance
but high bias contrast that with
overfitting situations of high variance
and low bias where models are extremely
sensitive to small fluctuations
capturing noise and data along with
signal but this is really different I
think to the popular and the legal
definitions of this word bias so in the
law as you would have heard in
yesterday's excellent tutorial bias
means judgment
based on preconceived notions or
prejudices as opposed to say the
impartial evaluation of facts now
impartiality is one of those ideas that
undergirds many of our legal processes
from juror selection to due process to
the limitations placed on judges now
this sense of biased is much more
difficult to fix with model validation
techniques and it can happen even when a
model perfectly captures signal for
example if it's reproducing biases
because it was trained on datasets that
reflected structural inequalities so you
can have an unbiased system in a machine
learning sense producing a biased result
in a legal sense so no wonder there are
some real barriers to collaborating
across disciplines on this topic we're
all speaking different languages but it
is precisely this ability to move
outside of our disciplinary boundaries
that we're most going to need if we're
going to crack this problem and let me
give you some examples of why so where
does by some machine learning come from
well obviously one of the most common
ways is from the data it was trained on
now training data can be incomplete
biased or otherwise skewed it can draw
on non-representative samples that are
poorly defined before use and sometimes
the problems with the training dataset
just aren't obvious because it was
constructed in a non transparent way for
example additionally given that
sometimes we have humans labeling the
data and sometimes we don't there are
other ways that human biases and
cultural assumptions can creep in ending
up in either exclusion or over
representation of sub populations let's
take stop-and-frisk for example this is
a program that was run by NYPD where 4.4
million people were stopped on suspicion
between 2004 and 2012 now
83% of people who were stopped and crist
were black or Hispanic so you got an ml
system that was using this data to
refine its training model one
interpretation is that black and
Hispanic people are just much more
likely to be potential criminals but if
you bring in expert
from different disciplines say like a
constitutional law professor or a
historian they're going to tell you a
different story and point to decades of
systemic racial discrimination in
policing now that's actually what
happened when stop and frisk was tried
in the courts it was actually found to
be illegal as a form of racial profiling
so the social sciences and the
humanities have decades of research on
bias on social systems that I think have
a lot to offer this current debate about
bias and machine learning so let's use
this common meaning for a bit that bias
is a skew that produces a type of harm
so what types of harm should we as a
community looking at this problem take
into account so I want to share with you
some insights from a forthcoming paper
by my colleagues Saul and brokers Alan
Shapiro and Hannah Wallach over the last
year we've been studying all of the
existing literature on bias and machine
learning and how ml researchers are
basically conceptualizing this problem
and what we found is that the majority
of the literature currently understands
bias as producing harms of allocation
and let me tell you a little bit about
what I mean they're allocated harm is
when a system allocates or withhold
certain groups an opportunity or
resource now this is primarily an
economically oriented view and it's
centered on harms like who gets a
mortgage who gets a loan who gets
insurance and an example here would be
if say a mortgage support application
just continually denied mortgages to
women or to people under the age of 30
but this picture gets a lot more
complicated when you look at systems
that represent society but don't
allocate resources these are
representational harms and they occur
when systems reinforce the subordination
of some groups along the lines of
identity so that's race class gender etc
this sort of harm can take place
regardless of where the resources are
being withheld to members of a protected
class classic example there of course is
Google gorilla where we saw
that google photos was labeling an
african-american woman a gorilla
straight-up representational home let me
give you another example many of you
know Latanya Sweeney 'he's classic early
study on discrimination in google ad
delivery systems back in 2013 she
uncovered a pattern by which names that
are associated with african-americans
were yielding ads for criminal
background checks now in this paper
Sweeney argues that employers who are
doing searches on job applicants will
see these results which may then lead to
race-based discrimination in hiring but
there's a different way that you can
look at this potential harm rather than
moving downstream to the effects on
opportunity and allocation we can
actually move upstream because
representation is the first step in the
chain so our paper suggests that the
perpetuation of stereotypes of black
criminality is problematic even if it is
outside of a hiring context it's
producing a harm of how black people are
represented and understood socially so
instead of just thinking about machine
learning contributing to decision making
and say hiring or criminal justice we
also need to think about the role of
machine learning in harmful
representations of human identity so you
can kind of see why allocation has
tended to receive all the attention so
far because allocation is immediate it's
a time bound moment of decision-making
whereas representation is a much more
long-term process that affects attitudes
and beliefs allocation is much more
readily quantifiable whereas
representation might be more difficult
to formalize put differently allocation
raises questions of fairness and justice
in discrete and specific transactions
whereas representation is about this
more diffuse depiction of humans and
society generally so one is
transactional the other is cultural so
representation bias issues have been
neglected by computer science mainly I
think because it's just harder to
formalize and track but it is still
incredibly significant and it's at the
root of all of the other forms of
allocated harm
so what types of representational harms
are there so in this paper we look at
five different types and I'm just going
to quickly address a few stereotyping is
probably the most well considered so far
the classic Bullock bossy paper from
2016 onwards was one of the first to
really look at these gender
stereotypical associations and that the
distance between gendered pronouns and
specific occupations of course the
Princeton group Callahan Bryson and
Orion and found similar problems in
their science paper this year you
probably all know this case of Google
Translate which is also producing
stereotypical translations even from
gender neutral languages like Turkish so
if you see here you type in here the
nurse she is a doctor you translate it
into Turkish you translate it back and
it actually swaps the genders of the
pronouns this is because some of these
issues deep in our natural language
models are actually just going to keep
giving us these returns and it's a
problem that I know a lot of people in
this room are really working hard on the
next area are harms of recognition these
occur when a group is erased or made
invisible by a system now in a narrow
sense the problem of recognition in
machine learning is purely a technical
one does a system recognize a face in an
image or in a video but there are some
bigger implications here it's also the
failure to recognize somebody's humanity
recognition in this broader sense then
is basically about respect dignity and
personhood and there is a broader harm
here than just whether or not a system
works for you and here I want to give a
shout out to the work of gerbil and
wienie
who is a PhD student at MIT who's been
studying the way that facial recognition
software cannot process darker skin
tones she actually ended up having to
wear a white mask on her face for the
vision system that she was studying to
work for her and she's got a poster here
today at nips that you should go and
check out there are also similar errors
in work with Mike Antony
we've been looking at how Nick owns
camera software mischaracterized
Asian faces as blinking and
hewlett-packard's algorithms had
basically recognizing anybody who is a
darker shade of like completely pale the
third area is denigration Hans now this
is just pretty straightforward when
people use culturally disparaging terms
though you had a really big one this
year of course in autocomplete we're a
huge media cycle got kicked off when
people realized that if you typed in
Jews should into a search query the
first thing you got back was Jews should
be wiped out again the case of Google
photos and the guerilla epithet is a
pure case of denigration what made it
offensive here wasn't just that it got
it wrong or it failed to work but that
it applied a label that has a long
history of being used purposely to
demean people so understanding problems
of this kind requires understanding
culture and history something that's
very difficult for a deep learning
system to deduce and it would really
have required somebody in the room to
say that this label could be offensive
finally there's this problem of
under-representation now I don't know
how many of you have tried doing an
image search on the CEO lately but
you're gonna get a lot of white dudes in
suits I'm sorry it's just what you're
gonna get and of course there's a really
important University of Washington paper
that looks at how this happens in a lot
of professions physicists also a lot of
dudes but when we looked at this
actually about six months ago for the
first time we were really curious who
the first female CEO would be can you
kind of guess who it would be she's
right down the bottom on the end its CEO
Bobby yeah seriously first female CEO
Bobby not a great look so we ended up
creating this table to map out some of
the most well-known
bias cases against different types of
representational harms as you can see
something like the Google gorilla case
is a pure case of denigration but if we
look at the word embeddings implicit
bias paper by Bullock bossy etc it hits
across all categories from stereotyping
to denigration to underrepresented and I
think doing something like this is
actually incredibly helpful
because we get a more fine-grained sort
of granular analysis of what the harms
are so we can actually start to
remediate them better so how do we tend
to address these issues technically well
in this paper we've basically looked at
all of the technical responses so far to
allocate of harms and these are the full
set but I'll just speak to a couple
scrubbing to neutral is basically one of
the most sort of common responses that
I've seen where people say we'll just
remove the biased data or we'll break
the problematic Association and say a
word embedding model but who gets to
decide which term should be removed and
why those ones in particular and an even
bigger question here is whose idea of
neutrality is at work do we assume that
neutral is what we have in the world
today and if so how do we account for in
some cases hundreds of years of
discrimination against particular
subpopulations this problem also comes
up when we try to technically address
the under-representation issue so should
you go to demographics and basically
make your representations adhere to
current distributions in society well I
mean let's take the CEO image search as
an example right so we know that less
than 8% of CEOs in the world right now
are women so does that mean that your
image search results should show 8
percent or less of women or do we
already sort of think about the fact
that there are studies that show that
have been discrimination against women
getting into the c-suite so do we then
try to like change the image search
results to have distributions that we
think would be fair or as we would like
them to be this is actually a really
hard decision this is not a
straightforward question and it actually
has a lot of political implications as
you know so while these kinds of
technical responses are incredibly
important and we need more of them they
won't get us all of the way to
addressing representational harms to
group identity that we've outlined in
this paper representational harms often
exceed the scope of individual technical
interventions we're basically talking
here about how we represent human
culture and that requires
a different theoretical toolkit in short
only developing theoretical fixes that
come from the technical world for
allocated harms is necessary but it's
not sufficient so we need to consider
the bigger issue underlying fairness and
bias so far I've shown you how we
consider bias harms from both an
allocated and a representational
perspective allocation focuses on
resources and economic benefits while
representation focuses on identity
categories like race and gender as they
already exist today and how they're
being reinforced or denigrated but now
we can take a step back and ask where do
these identity categories come from
because the most common approach to
think about bias today is to think of it
as a kind of pathology an error that
turns up that we can fix but what if
bias is actually a deeper and more
consistent issue with classification in
other words what if bias is always going
to be a problem the fact that bias
issues keep creeping into our systems
and manifesting in new ways suggest that
we need to step back and understand
classification is not simply a technical
issue but a social issue one that has
real consequences for people who are
being classified so in order to show you
why I think this is the case I'm gonna
take you on a high-speed tour of the
very weird history of classification
which i think is about some really
applicable lessons for the machine
learning community but there's two
themes too I want you to keep in mind
the first is that classification is
always a product of its time and a
second is that we are currently in the
biggest experiment of classification in
human history should understand
classification we're gonna go back we're
gonna go way back to one of the most
founding figures in this entire area who
is of course Aristotle the guy in the
blue robe who is staring adoringly at
his teacher Plato in the pink robe so
Aristotle's work on natural
classification was revolutionary at the
time but has now become basically
scientific common sense he went out into
the
he made observations of living things
and then he drew general conclusions
yeah this is like basically the
inductive method of empiricism but even
from really early on we can see how
classifications also reflect to the
social order of the time have a look at
this image I love this one it's from
about 300 years after Aristotle it's
from a really influential zoological
illuminated manuscript and it shows how
religious themes were being built right
into these classifications of the
natural world so you've got animals
marching two by two and you have a
Christ figure that's sort of ushering
them along so we're starting to see
religious ideas mix freely with
zoological classification so the history
of classification shows us time and time
again that every attempt to classify
will always reflect the social cultural
religious and political issues of the
time now during the Enlightenment
philosophers are very emboldened by the
success of the Natural Sciences they
wanted to create a taxonomy of the
entire universe this is actually one of
my favorites it was created by the 17th
century scientist John Wilkins he's one
of the founding members of the Royal
Society and he publishes this formative
book that classifies the entire universe
into 40 categories kind of handy right
it's nice that it's 40 makes everything
pretty straightforward and if
essentially Aristotle was taking an
empirical approach to classification
Wilkins shows the later popularity of a
linguistic approach where human language
is ordering all of our experience and
another 17th century thinker
Thomas Burkhardt proposed an artificial
language called the logo pendente in'
and this classified the whole world into
11 genders so you had men and women but
you also had gods and goddesses and
beasts and inanimate objects etc etc so
while these classification schema can
seem kind of arbitrary and funny today
the takeaway here is that the work of
classification is always a reflection of
culture and therefore it's always going
to be slightly arbitrary and of its time
and modern machine
making decisions that fundamentally
divide the world into parts and the
choices that we make about where those
divisions go are going to have
consequences so let's talk about gender
for a minute
her heart says there were 11 genders
that's back in 1653
2017 facebook says there are 56 genders
okay that's a very precise number and of
course four years ago Facebook said
there were only two men and women so
obviously something very interesting has
happened in the last four years and
while you might say this is an
improvement it's still pretty arbitrary
you can imagine that something like this
is basically the product of a design
meeting where a bunch of people are in a
room with a whiteboard and they're
trying to brainstorm every single gender
category they can possibly think of but
of course they could have also just gone
with a free text field for self
identification or just not had gender at
all each one of these design decisions
in effect has consequences and powerful
social implications let's consider the
world of machine vision for a second and
the labeled faces in the wild training
set it's incredibly important it's been
cited over 2,000 times and it doesn't
claim to make any sort of grand
classification of all of the faces in
the world but it does have some notable
biases as Joyce pointed out it's seventy
seven point five percent men 83.5% what
so those are the people for whom a
system trained on this are going to work
best for and who do you think the most
represented faith is in labeled faces in
the wild have a guess anyone know this
one put up your hand if you've kind of
heard this story before anyway oh this
is great it is George W Bush yeah he is
the most represented face he's in there
530 times out of 13,000 and this kind of
makes sense when you remember that
labeled faces in the wild is based on
faces in the wild which came from photos
of Yahoo news from around 2002 to 2004
with the idea that news photographs
unlike those taken under lab conditions
are more true to life so no wonder that
davia is every
because of course presidents get a
disproportionate amount of news
attention as we've kind of discovered to
our peril this year and this is
ultimately a great reminder to me of a
couple of things the first is that
datasets reflect the culture but also
the hierarchy of the world that they
were made in who is powerful is going to
appear a lot a lot more frequently than
who is not so we have to ask what
happens when you choose news photography
and does that change your interpretation
of faces what kinds of people are always
in the news and who's not there secondly
I think this really reminds us that our
current datasets always stand on the
shoulders of older classifications so
imagenet draws on a taxonomy of words
that come from word net and word net
inherits from many sources including the
brown corpus from the 1950s so while we
might look at a picture like this which
comes from Diderot and Dalembert z'
encyclopedia from 1751 and think well
this is pretty old-fashioned I mean you
can't really categorize the world into
so few categories
I think machine learning is also
developing its own sometimes arbitrary
sometimes strange culturally specific
classifications so what would it look
like just imagine if we try to create an
encyclopedia of machine learning like de
toros image here well we thought we'd
give it a shot just for nips so here you
go it looks a little something like this
you can see here how early various
contemporary data sources are building
on earlier corpora so let me zoom in for
a bit you can see here current data sets
like CFR imagenet and it's cocoa and
kinetics and you can see what they drawn
and what came before them you can also
see how training data sets are already
conducting the largest clasificado
experiment the world has seen it's
everything from human faces human poses
every human action to millions of
objects and places and natural phenomena
but some of these classifications are
going to change like Facebook's gender
categories but some of them are going to
hang around because classifications can
be
dickie and sometimes they stick around a
lot longer than we intend them to even
when they're harmful let me give you an
example this is the cover of the
Diagnostic and Statistical Manual of
Mental Disorders first published back in
1952 this listed homosexuality as a
serious mental disorder now that took
over thirty years before that was
finally dropped and if that sounds like
a really long time to you I was kind of
amazed to find out that the Dewey
Decimal System you know what we use to
categorize books in libraries listed
homosexuality in 1932 as a mental
derangement and it has remained there
until two years ago 2015 yeah so people
who are classified as gay were then
being further classified as having a
mental disorder and being a social
problem and it had devastating
consequences for people who are
identified this way and it took an
enormous amount of protest and advocacy
to address this kind of clasificado
reham and frankly it continues to
reverberate even when the worst labels
are removed but now history is repeating
itself yeah you know where this one
comes from it's through the Wang and
Kosinski paper that is on deep neural
networks detecting sexual orientation
from facial images and it caused a huge
controversy this year now they use the
vgg face from the Oxford vision lab and
they trained it on facial images of
white men and women from a dating
website unnamed
and Facebook and their classifier
achieved 81% accuracy in identifying gay
men and 71% accuracy in identifying gay
women now many members of the scientific
community have expressed concerns both
about the representative
representativeness of the researchers
sample and whether these images are just
reflecting cultural markers of sexuality
rather than purely physical features
that they didn't control for but I
actually don't want to talk about these
methodological problems right now I
think we need to think more about the
ethics of classification here
particularly when we think about the
fact that homosexuality is still
criminalized in seven
eighty-eight countries some of which
applied the death penalty what we see
here is how easily machine learning can
be deployed in contentious forms of
categorization that could result in
criminal prosecution jailing of people
or worse so our responsibility for the
systems that we create particularly in
terms of these classifications has never
been higher so we've shown you how
classification systems are often sites
of political and social struggle and as
balcan and star showed in there
basically now canonical book about
classification called sorting things out
political agendas sometimes presented as
purely technical and then hidden away
from the public and they gradually
become taken for granted they write
about a really important study actually
of apartheid South Africa to show how
classifications can be used for
authoritarian political control this
image comes from something called the
book of life and it was used in South
Africa during the height of apartheid in
the 1970s it classified people into one
of four categories colored Indian white
or black and it was then built into a
technical possible system by IBM at the
time now depending on what category you
were classified in it would determine
where you could live what job you could
have and who you could marry and you are
basically classified according to this
really ambiguous criteria of your
appearance and general reputation so you
can imagine how messy that got this
person here is Victor Wilkinson he was a
jazz musician and he got racially
recategorize five times by the South
African government each time producing
huge disruptions in you know what jobs
he could do and his freedom of movement
now fast forward to 2016 when we when
Zhang published this paper that claimed
it could predict the likelihood that a
person is a convicted criminal based on
nothing more than a photo of somebody's
faith now as we know they did this by
training their system when around 2000
Chinese government issued IDs and they
then concluded that they created the
first-ever free of bias criminality
detector right well I tend to think we
should be pretty skeptical of these
claims
basically it's saying that this is a
neutral system superior to human
judgment which is basically the big red
warning sign given the human biases that
affect who is arrested and who is
charged with crimes so this is not free
of bias this is just bias encoded and of
course this type of face based
prediction has all happened before
physiognomy and phrenology were
considered Sciences back in the 19th
century but they had a very nasty
pattern of being used effectively to
justify the unjustifiable including
slavery in the US and being used by race
scientists in Germany in the 1930s and
if all of this sounds like the distant
past know that they are already startups
making money based on these questions we
just lost the slides but that's okay I'm
sure they'll come back making money on
these sorts of issues by saying that
they can predict who is a terrorist as
well as who is going to be something
like a brand manager so I think these
are the sorts of questions that should
really begin to concern us as a
community and I'm gonna see if I can
bring us back with some pretty images to
conclude but maybe that's a bit
optimistic all right it was optimistic
we'll do it without what I want to do
now is end on three things that I think
we can do as a community to start to
contend with these problems and I'm
gonna do it straight off the top of my
head so the first one that I think is
really important here is that we need to
start working on fairness forensics and
what I mean by this is that there are a
lot of things that we can start to do to
really test our systems this includes
everything from building pre-release
trials where you can see how a system is
working across the different populations
so that means you actually know if it's
affecting black populations differently
to white populations yeah not my slides
but that's okay we can also start
thinking about questions like how do we
track the lifecycle of a training data
set to actually know who built it and
what the demographic skews might be in
that set I also think it's time for us
number two to really start taking
interdisciplinarity seriously that means
working with people who are not in our
field
but might have deep expertise across
other areas now personally one of the
things I did along with Hannah Wallach
and others was to build the fake group
at Microsoft Research that stands the
fairness accountability transparency and
ethics and this is something that you
can do too if you are working at a
company that is basically building high
stakes decision-making systems then you
can actually think about building an
interdisciplinary group that starts to
test how it's working I think this is
incredibly important the second thing
that I've been doing to work towards
this goal is I've just launched a new
research institute called the AI now
Institute along with my co-founder
Meredith Whitaker the reason we did this
is because we think it's incredibly
important that we start to have spaces
where disciplines can actually work it's
cool we're gonna do without slides we're
gonna freeform it where disciplines can
actually start working together on these
problems so for the AI now Institute it
includes computer science engineering
social science law and business so in
that sense I think we can start to build
these spaces for collaboration on these
questions finally my third
recommendation is that I think we need
to think harder about the ethics of
classification right now you might have
heard that the Trump administration is
asking the machine learning community to
help build tools for extreme vetting at
the border now I and around 50 other
academics some of whom were in this room
have signed a letter saying that we
think this is actually a deeply
concerning system and frankly I think
it's concerning not just because of the
potential technical problems but also
because I think it raises huge ethical
questions so the question here for the
industry is are there some things that
we just shouldn't build and if so how do
we start to have that conversation and
also I want to talk about us as
individuals how do we start to make
decisions about what we should do and
what we shouldn't do because this
community in the room right now has an
enormous amount of power and your
decisions are going to make a difference
so I'd like to remind you of a
particular engineer that is a
oh of mine called Renee Camile who back
in the 1940s sabotage the hollering 'the
machines that were being used to
classify jews and other ethnic
minorities in germany and basically by
erasing the 11th column on the hollering
'the machine
he made an individual decision that
saved thousands of lives so ultimately
when it comes to the question of
fairness and machine learning i think we
have to ask ourselves the big question
who is going to benefit from the system
we're building and who might be harmed
because if we're really interested in
the question of fairness that's the most
important question that we can ask thank
you very much
thank you
I think we have time for a couple of
questions if if panner is gonna permit
us yeah good time for one or two
fantastic thank you very much just um
there a couple of mics up here if people
have a question feel free to - to one of
these yeah we just have one over here hi
yeah I can hear
I love this question this is a really
important design question I'm going to
repeat it for people who couldn't hear
that the question is who gets to decide
how we tune our systems because
traditionally this has been done by
computer scientists sometimes thinking
about a specific user group that they're
trying to sell a product to how do we
start to open up that field of questions
that are asking different populations
fair enough question yep
that's it so the issue here is that
right now we interview the customer to
find out what they want and I think
that's exactly right this is a history
of how we've worked as a field for
decades I think we now have a slightly
different set of questions to ask and
I'd put it this way if you're building a
high-stakes decision making system
that's going to affect some populations
more than others so here criminal
justice is a classic case in point you
need to start thinking about different
questions than just the customer you
need to say okay this is a complex
social system so who's going to know the
best about it let me give you an example
right now we have predictive risk scores
as you know that are being used in
criminal justice contexts and one of the
big areas that it looks at is this idea
of failure to appear so a defendant will
get a score based on how likely it is
that they're not going to turn up to
their court date that kind of makes
sense right but if the judge looks at
that number and says okay this person is
high risk then basically they are
detained they are jailed they actually
don't get out on bail so this is a big
issue failure to appear now if we start
to look at research outside of say you
know just how do we tune the predictions
you will also look at research from
sociology and anthropology that shows
you that the thing that really affects
failure to appear for court dates is
things like did that person have
transport could they get there do they
have child care if the primary care of
their family so the things like trying
to get people transport trying to set up
child care for court dates might
actually have a much bigger difference
than just giving people a score of high
risk and not letting them go so I guess
this is a much more complex set of
questions than computer science is
normally having to deal with and it's
something that I've called a social
systems analysis and I've written about
this in nature with Ryan Kahlo last year
and I'm happy to share that with you if
it would be useful but I think this is
precisely the moment where computer
science is having to ask much bigger
questions because it's being asked to do
much bigger things but that's a great
question thank you thank you have a
question here yes please
yep
Wow this is a this is a really big
question and let me repeat it for you so
the question is if we take for example
this community being asked to do things
that could be quite socially problematic
how do we compare that to say what was
happening with the nuclear scientists
who you know actually form groups and
said no some of these things are
actually too dangerous for us to do and
because there was really only a small
group of people who were skilled enough
to build those large-scale systems if
they walked away it wasn't gonna get
done but that's not the case in machine
learning if this community says hey
we're not going to work on these tools
somebody else is just gonna step up and
do it really badly so how do we deal
with that question that if the community
of people who have the most training say
something is really suspect and
problematic or ethically concerning what
happens if somebody else just goes and
does it and does it really badly so I
actually think this is a really hard
question particularly now that we're
starting to see a lot of these sorts of
tools basically be sort of shared out so
really anyone can start to play with
things like tensorflow etc so I actually
think still this community has an
enormous voice to say things like okay
we don't think the system is OK now that
doesn't mean that we're going to stop
people from doing it and particularly
this gets really interesting when you
think about the geopolitics of how
systems get built even if one country
decides it's something it's not
acceptable
what happens if that's being built
somewhere else I I don't have an easy
answer for you for this question
but I do think that particularly the
senior people in this community in this
room have a huge sway to be able to say
hey these are the kinds of things that
we can start to make progress on so to
some degree I think it's up to us but I
know that that's going to be a really
hard path and some of these questions
are just gonna get more complex not less
that's Hannah giving me the signal thank
you so much for your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>