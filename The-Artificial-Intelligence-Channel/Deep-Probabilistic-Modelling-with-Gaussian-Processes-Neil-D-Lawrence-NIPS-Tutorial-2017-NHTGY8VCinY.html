<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Probabilistic Modelling with Gaussian Processes -  Neil D. Lawrence - NIPS Tutorial 2017 | Coder Coacher - Coaching Coders</title><meta content="Deep Probabilistic Modelling with Gaussian Processes -  Neil D. Lawrence - NIPS Tutorial 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Probabilistic Modelling with Gaussian Processes -  Neil D. Lawrence - NIPS Tutorial 2017</b></h2><h5 class="post__date">2017-12-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NHTGY8VCinY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's let's go for it he's very well
known for his work on Gaussian processes
and in particular latent variable model
Garcia processes and deep Gaussian
processes and all these other cool
things and going through his bio I was
trying to find instead of interesting
factoids to share with you but I'll have
some feel later but first let's go
through the more boring stuff he did a
PhD it's boring but impressive I would
say he did his PhD at the University of
Cambridge he then went on to work for
Microsoft Research he became a lecturer
at the University of Sheffield then
moved on to the University of Manchester
only to return to the University of
Manchester again as a sort of
collaborative chair in neuroscience and
computer science three years later but
what did Neill do before he did his PhD
on he guesses he worked on a oil rig so
that's very interesting factoid I want
to talk to you about that after the
tutorial how that is but more seriously
Neil actually has served our community
tremendously I'm gonna list for you
basically all the things that he did for
the machine learning community and
that's a truly impressive list I think
so he was an associate editor in chief
for TPA which is basically one of the
highest ranked journals in the machine
learning and computer vision was a
founding editor for the Proceedings of
machine learning research he was both
general chair and program chair for AI
stats and he was both program chair and
general chair for MIPS he was also one
of the founders of the European version
of nips which is Dali and he also
founded data science in Africa which is
also very interesting initiative where
people teach machine learning in
forgot so you see Neela's all over the
place and he's really shaped our field
to a tremendously and more recently he
had a new adventure and joined the
corporate world and he joined Amazon and
he founded something yet again in this
case the Amazon research center in
Cambridge where he will be or he already
is the director of machine learning it's
a very very impressive resume and we are
looking forward to are you to your
tutorial on deep probabilistic modeling
with Gaussian processes a big hand for
Neil
thanks very much max I don't know what
that sounds like over there so people
can show I don't know what you can do
you can shout but I won't hear you
trying to remember my password okay so
um it's a real pleasure to talk at this
meeting about deep Gaussian processes
because it puts me back in mind so my
first nips and the first thing I saw it
was 20 years ago 1997 was the tutorial
on Gaussian processes by David McKay
David McKay was a big influence of me
and many others here and what I'm gonna
try and do in the tutorial today it's
very hard to talk about all the things
that have happened in the 20 years since
then and when I look out at the audience
I see some people who were like totally
expert in this stuff and some people who
probably just saw deep in the title and
thought they'd come uh-huh so it's a
mixed audience I would say so I'll see
how well I do okay so I thought I might
start with what my perspective on
machine learning is so I think of data
and models meetings predictions as being
what drives us in machine learning and
Gaussian processes are about the models
so the data are the observations we
actively or passively acquire and the
models are assumptions based on previous
experience other dates are transfer
learning beliefs about the regularities
of the universe or inductive bias and I
want to make these predictions with as
rich a family of modeling as possible
and actually deep neural networks are a
very rich family and I was very inspired
by work early work by Geoff Hinton on
deep Boltzmann machines to look at the
deep methodology in the context of
Gaussian processes and what's
interesting to me now is that I
definitely never was doing AI but
apparently now I am doing AI
I've already been someone who was doing
machine learning but that's because was
she learning has become the
so the mainstay of AI and that's because
of the importance of prediction now the
thing I'm going to be arguing today
is that - that's not enough to do AI you
need to make decisions on top of
predictions so it's one thing to be
making a lot of predictions about the
state of the world that you actually
have to decide how to work now some
areas in machine learning do that like
reinforcement learning but my argument
or my belief have always been ever since
um the early years of machine learning
that in order to do that well you need
to use uncertainty now to combine data
and the model together I always think
that there's two functions you need so
the prediction function which is the
thing you're going to use there's this
new word inferencing so I'm not sure
where it came from
it used to be called prediction when I
say inference I'll be using it in the
classical sense not meaning prediction
so the prediction function which we use
to make sort of predictions at test time
very often and that often includes those
beliefs about our regularities universe
and the objective function which defines
the cost of miss prediction now I think
there's a misunderstanding with a lot of
probability is that basically the cost
of miss prediction you you actually
defer that so the idea in the
probabilistic approach is you simply do
your modeling so your objective function
actually becomes to fit your
probabilistic model to data but the
objective function in the real world
also normally encodes other things like
costs and so forth now I mentioned
uncertainty so I wanted to be clear that
uncertainty comes in in all sorts of
waves so the main uncertainty we're
going to be looking at today is that
arising from scarcity of the training
data so even if you have the right set
of prediction functions if you don't
have enough data to determine your
parameters that leads to uncertainty
there's also your model choice so the
mismatch between the set of prediction
functions we choose to use for practical
reasons and all possible prediction
functions that's a major interesting
source of uncertainty and one that when
we're doing try to understand the
quality of decision-making we need to
account for now I think my favorite type
of uncertainty is one that you rarely
had talked about which is actually
the uncertainty in the objective or cost
I mean I don't even know what I'm doing
why should I know what anyone else is
doing so I think we're the main thing
you see in real intelligent systems is
massive uncertainty in objectives and
that's I think something we're very
limited in machine learning we write
down objectives and then we then we riff
off those but I'm not going to talk
about that at all today so I'm actually
going to be focused mainly on this idea
that I've chosen a class of models and
I'm going to be have uncertainty in the
training data so um I was thinking if 20
years David presented a tutorial on
Gaussian processes and that makes me
think also about the history and if you
have to go all the way back to McCulloch
and Pitt's I actually got out the paper
it's not very clear their model of the
neuron but that underlies a lot of the
work that we are doing what people are
doing in the machine learning community
at the moment of course there's an
association with those models but
actually the really interesting thing
about these composite deep neural
networks is their ability to sort of
express what we think about the world to
use highly complex functions so that's
the same motivation for why you're
interested in deep Gaussian processes
but what I'm gonna have to do in order
to introduce that to those who aren't
familiar is introduced the idea of also
to review the idea of a one layer neural
network because that's going to make it
easier to build up deep Gaussian
processes from that so I said prediction
function objective function and I'm not
going to talk about objective functions
at all today I think as I say they're
critical and in fact how you define your
objective function can lead you to all
sorts of different directions but all
I'm just going to focus on is the
quality of that prediction function the
thing that encodes the nature of the
regularities of the universe now I'm
going to use this notation it's a little
bit ugly f of X is my prediction
function here and then I'm saying for a
neural network I'm just going to use
this Phi which comes from statistics so
it's a basis function notation but think
of that as the activation function so
the one layer neural network and of
course I've not put any output nonlinear
so this is like a regression neural
network is equal to the inner product of
some vector of weights with one output
in this case times set of basis
functions now those basis functions
themselves are a function of the input
and a matrix W so if this was like a
classical multi-layer perceptron neural
network this function here would be W
times X W is a matrix of weights from
inputs to the hidden layers X is the
number of inputs and Phi would be just
take the hyperbolic tangent from that
vector so W times X would be a vector
that would go into the hyperbolic
tangent and then this says weighted
linear sum of that output so I'm just
laboring this because this is the
notation I'll use because it makes it
easier to turn it into the calcium
process set up so that's very simple
neural network it's the sort of thing
that in 1997 I sort of got interested in
the field in 1996 even and this is what
people tended to be working on they
worked on confidence as well but there
was a lot of work on this now I like
this set up because it shows this
important relation with statistics in
statistics as I say that activation
functions would be known as basis
functions and we would think of this as
the linear model now that terminology is
important I don't mean it's linear in
the predictions it's linear in the
parameters so in statistics when they
say linear model they're referring for
the fact that these parameters here W
it's not linear in these parameters W 1
W 2 of a lots of w's sorry but it's
nonlinear in the inputs right so when
people in machine learning later view
say as a nonlinear model they mean
nonlinear in the inputs but the
statisticians noticed this nice thing
that you could have something that's
nonlinear in the inputs and then you
just concentrate on these parameters but
that's an important difference so in
statistics they typically would choose
this set of parameters by some other
means you wouldn't look to fit them
simultaneously so the sort of
interesting phenomena in the neural
network certainly already there in the
first wave of neural networks was that
these are adaptive basis function space
dysfunctions
you attempt to fit this matrix of input
to hidden weights alongside the the sort
of hidden to output their weights so
they would call that a linear model and
in that case the W 1 would be static
parameters there's lots of advantages so
between when you watch how a
statistician works if you're a classical
machine learning or see they work in
many different ways that's mainly
because they're driven by different
objectives and interpretability is key
among them that's going to come back to
us actually because the interpretability
of our models is also going to prove
important they focus on interpretability
more than prediction so they tend to
look at model classes and treat them in
such a way so as to keep things and
serve tables so they might be happy
sacrificing W making them static to give
them worse prediction performance
because by staying in this class of
linear models they get better
interpretability
on what's going on and by the way this
class of models is also what gauss
fitted by the gaussian is named after
him when he was trying to plot the orbit
of series and predict where it was gonna
reappear when he was 23 years old and in
his case the basis functions were
derived from understanding of the way
the planets and they move around the Sun
together and they were sort of derived
that way and he had to fit the
parameters inside here as well that's an
incredible achievement it took him six
weeks by training some deep neural
networks today ok so machine learning we
optimize w1 this sort of it could be a
vector or a matrix of output weights as
well as w2 now in enmity in statistics
you would normally denote W 1 by beta
and I like that separation if I ever use
beta it normally means I care about the
value of those parameters for some
reason other than the predictive quality
of the model if I denote them as W it
means I don't care what they are I'm
just they're just a mechanism for trying
to get predictions out so this tutorial
is sort of revisiting that decision to
optimize W 1 and W 2 and follow this
sort of path of Radford Neil and David
McCoy who I mentioned was my inspiration
so David wrote
PhD on Bayesian neural networks and that
was something I also looked at during my
PhD thesis and Radford Niall also did
his PhD thesis on that and those theses
are both still classic today but we're
going to go beyond base in Ural networks
as well but I'm gonna start by looking
at the probabilistic approach to that so
classically I introduced machine
learning as making predictions from a
prediction function and using an
objective function to try and fit that
predicting function the way the
objective function is there to tell me
the mismatch between the training data
and my um my predictions and then I
hopefully design in such a way such AI
generalize well but something I don't
think we've talked enough about is that
actually we're going to predict
probabilistically it's in some sense
it's an entirely different ballgame
so probabilistically what you want to do
is you kind of want to have a joint
model of the entire universe right so
you actually want to have a joint
probability distribution that tells you
how every variable that you've observed
and every variable you could observe in
the future is related to each other and
then you want to make a prediction by
taking one of those things that I want
to make a prediction over Y star and
conditioning on everything I've observed
so that's why here so if I observe Y
things in my life if I'm going to make a
prediction and I've got uncertainty over
that somehow I'm modeling this joint
distribution and then of course I'd have
the X of the Y and X here are the
training data an X star would be a test
location if it's a regression problem if
it's not a regression problem it's just
pier y star given Y that's unsupervised
learning so you're trying to model the
joint distribution of everything you're
going to see in the future and
everything you've seen in the past
that's really what I want to do I mean
so in practice the way we'd actually do
this in this parametric world that I've
spoken about is we compute posterior
distributions so this is the classic
Bayes rule for prediction so what we
have is we typically relate our test
data output to our test data input by
some set of parameters
so here I've conflated w1 and w2 into
the same matrix and we combine that with
what we call the posterior distribution
of all those parameters given what we've
seen before we use Bayes rule to compute
them so that's the classic thing that
people are trying to do with Bayesian
neural networks and we integrate over
these parameters but the parameters I
think one thing you shouldn't forget we
don't care we're machine learners we
don't care about the value of those
parameters for anything other than there
are sort of intermediate step for making
a prediction so that's something not to
forget right so we're integrating them
out and then we have this object here
which is what we make predictions over
now in practice this is hard because of
a lot of the integrals required to get
these probability distributions out are
difficult to work with so in practice we
might do things like make map
approximations here or other
approximations to try and get things to
speed up it's a very elegant framework
but difficult to apply in practice okay
and of course when we're building that
we also that's constructed based on the
likelihood this term here is the
likelihood of the testator but the
likened of the testator is typically the
same as the likelihood of the training
data so the likelihood P of Y given X
comma W is what we tend to work with and
typically and now I'm going to mention
independence for the first time we
assume independence but be very very
careful with the independence assumption
it would be stupid to assume your test
data was independent of your training
data if it is you've got the wrong
training data your test data is
correlated with your training data so
when you're making an iid assumption
you're not making an iid assumption
about the and people sloppily say this
all the time about the data points
between each other you're making an iid
assumption conditions on the inputs and
w/w the parameters so actually what
you're doing is you're making an iid
assumption about the noise not about the
actual underlying function and the
different thing when you start looking
at Gaussian processes is you sort of
throw that away or if you look at the
general probabilistic approach you throw
that away so when people think they're
being probabilistic very often they're
just being probabilistic about the noise
and that's a fine thing to do but it's a
very small
action of all the framework of
probabilistic modeling you could do and
in fact it was proposed by Laplace to do
this and it was the approach Gauss used
he had the probabilistic justification
for his model when he predicted series
very commonly we might have a Gaussian
likelihood and we like Gaussian
likelihoods the reason Gauss didn't
actually invent the Gaussian
distribution Laplace invented it Laplace
invented it for doing Bayesian inference
that's why it's called the Laplace
approximation when you do Bayesian
inference with this so this form of
distribution Gauss noticed that when you
maximize the log likelihood you got a
quadratic and that was easy to solve and
that's what the likelihood he used for
series so that's classically what we
used and people often think of doing a
model fits that it's interchangeable
between a least squares objective and
Gaussian likelihood we could also
consider the priors over latent
variables so the other thing that very
core to what I do is to take regression
models those prediction functions and
turn them into unsupervised learning
models and the approach the way I do
that is to integrate over distributions
on the inputs and of course this is what
a Dan is doing as well although it's not
fitting the likelihood which is a very
interesting thing and you know other
types of models you're viewing these as
Mutants parameters so this is my
framework for trying to construct a
general model but I want P of Y star
given Y so I might not even have
regression inputs but the way I build it
is I try and build from a likelihood
which I typically seem to be independent
there's an implied posterior in there
and then I integrate over P of X but
this is for me what it's all about all
that stuff looks very very complicated
relative to what we're actually trying
to do machine learning for me is very
simple you have data Y you have a model
y comma Y star Joint Distribution of and
you do conditioning in order to make the
prediction but actually very few people
even think about this when they're doing
machine learning and the main reason is
because in order to get from this joint
distribution to this prediction here you
have to do high dimensional integrals so
in this case
this is a shortcut by going through W
right so we could build a model in such
a way that we can have this sort of
decomposition but in this case here you
would just have to integrate over all
future things that are going to happen
in the world integrate them out and then
you would divide that you would that
would give you P of Y divide that into
the joint and that gives you your
predictions but of course that's a
pretty hefty integral right you've got
to now integrate over everything that
you could possibly see in the world in
the future which is um sounds tough
okay of course there are other
frameworks that have looked at this and
it's nice to see last year's nips a
number of those coming up so people
combining graphical model framework with
deep learning frameworks and I'm gonna
deliver I also sort of believe in this a
lot so I guess the work that I'm talking
about now does go back to nips one of
the Vancouver ones so I don't remember
really which year probably about ten
years ago where Kevin Murphy who was
working a lot on graphical models wrote
a nice book on it and I were talking
about these two different ways of trying
to build joint distributions and I was
to the saying I like doing it this way
where this for me was a regression model
and then this is a latent variable so I
did the dimensionality reduction and
that seemed to be a nice way of trying
to specify a large joint distribution
with a reduced number of parameters
whereas Kevin was very active working on
probablistic graphical model inference
now what that is is is assumption about
the way this Joint Distribution
decomposes where factorizes so this
chain here is a Markov chain and it's
saying it factorizes in that way and i
started out by having a little
discussion with Kevin about he probably
doesn't remember this about how surely
this was the wrong way and what I was
doing was the right way but then I
concluded like as many of these things
that the combinations were two of them
was the right thing now that combination
of two of them to my mind is what people
are doing when they're doing deep
learning and I think also to the early
researches in that field
they also saw it very similarly because
this is the challenge with the graphical
so I just searched um I think earlier
this morning for a paper the promise of
graphical models is with these sort of
decompositions we could put joint
distributions over everything we cared
about and we can make predictions about
other things we cared about but in order
to do so you wind up with quite complex
graphs so this is actually quite a small
graphical model it still wouldn't fit
very well on screen you cannot read it
and the title of this paper is predict
parry operatic risk of colostrum
difficile infection following colon
surgery so this graphical model is for
predicting one thing about a patient
after a particular surgery right it's
quite a lot of effort it's not really
going to give you this thing that you
want of being able to predict all future
things given the past and one of the
reasons is I think because the
relationships between these different
nodes in the graph are relatively simple
when you're just using conditional
probability tables so you can't capture
the richness of what's really going in
you can't abstract things I think that
the key thing about deep learning for
meters is its ability to try and
abstract things and so in some sense
this was always along because actually
you lose interpret ability as as the
graph size increases but combining these
things with other models that do
powerful for function prediction would
seem to be quite promising so I think
I've said these things already it's easy
to write everything in probabilities but
underlying that so you can write these
things down in terms of probabilities
you can make any distribution up you
want but then you find you can't
normalize it because you can't do the
high dimensional integral the worst AI
dimensional integral being I now have to
integrate over all future data but I've
never seen before
ah
somehow I've lost the damn it compile
talk hmm okay so just trying to work out
what that's a equation what you can't
make out anyone who can read latex can
tell me
so statistics that focus more on that
linear model so that equation what is I
believe just simply was that there was
an error in it
by fixing the error I've now made it
unreadable come on huh way back that's
why you needed the reminder oh it's that
sort of neural network equation oh yeah
I've done a lot of the talk already okay
now it's doing that's great okay I might
intervene now knowing when to intervene
is critical okay let's see if I do that
okay it's I think the Internet's flow I
think that that's causing me problems
okay okay that's going a little bit
better now
okay back to that quiz it's that one
I've got to go forward again
okay there is a good way of doing this
on this thing but I'm not really that
familiar with it okay okay he wasn't
half far away there was something weird
that wasn't okay predict over everything
uncertainty is critical all
decision-making so that equation I
didn't give you any chance to look at it
it's the W transpose Phi of W X and then
in statistics you might focus on this
linear model and then if you want to do
a probabilistic treatment it's actually
quite nice it's quite easy to do because
you can place a Gaussian prior over
these output weights the W's that they
would call beta and now you define y AI
is equal to f of X where f of X is
defined by that missing equation f of X
is equal to w2
transpose Phi of W 1 comma X the single
their neural network equation and then
this is how you define your
probabilistic model this is what Gauss
did actually in Laplace before him
Laplace was the originator of this type
of idea you say that you've got some
function which is a parametric function
in this way and it's being corrupted by
noise epsilon I and then you define that
the noise is sampled from a Gaussian
distribution that's the iid bit right
okay so normally the integrals are
complex but for this Gaussian linear
case a trivial and that's kind of why
cows ting protesters are very special
they've got this very unusual property
that I'll just go through now so
Gaussian distributions do so univariate
gaussians have this property which is
that if a Y it's not test Y so general Y
now in this case for training why it's
just a general Y is sample from a
Gaussian distribution then adding those
Y's together the result of that will be
drawn from a Gaussian where the mean is
the sum of the means and the variance is
the sum of the variances that's a really
really nice property it's also unusual
in fact when you do it to most other
distributions they start becoming
Gaussian right because the central limit
theorem you're adding random variables
together so it's unusual to find this
property what songs do it and gammas
with certain parameters do it it's
alright I'm not sure about the gammas
can't think of my feet possums do it but
when possums do it they said they'd
start looking like gaussians as well so
also scaling a Gaussian leads to a
Gaussian so that's not that unusual so
scaling a Gaussian lots of scaling lots
of distributions keeps you in the same
class so this is the sort of
relationship there but the important
part about that is that when you do
linear algebra on Gaussian random
variables linear algebra is just scaling
and summing together so everything stays
Gaussian and that's super super
important that's why Gaussian processes
and Gaussian variables are so critical
because computers do linear algebra
really well matrix vector multiplication
or um what is it
computing and neuron in the other
parlance that's what GP used a good at
right so now we have if that's the case
we've got this multivariate consequence
that if X is drawn according to this
distribution and if we say Y is equal to
a linear algebraic form W times X so Y
is a vector W is a matrix X is a vector
then Y the vector Y is drawn from a
multivariate Gaussian of this form which
is just a modified variant of this that
is normally considered to be a very
difficult integral because that
effectively this is defining P of X this
is defining P of Y given X and this is P
of Y so multiplying these two
distributions together gives you the
joint and integrating out gives you the
marginal that's the difficult thing but
in a Gaussian process or in a Gaussian
setup it's trivial it's just linear
algebra which is amazing that's why we
like so these linear Gaussian modes are
much easy to deal with but the really
interesting thing and this was the sort
of insight from Radford Neil is if you
go to these models you can even you can
deal with that W to the output weights
but we're also going to look briefly at
the fact that you can deal with the
input weights the weights inside the
basis function under certain conditions
by considering a particular limit and
that's how it gets us to a Gaussian
process so I'll skip that one so just to
give you a little I'm going to move to
some matrix notation so I just want to
introduce that matrix notation so before
the activation form
Asians were of this form but there was a
vector of them right so it was W
transpose Phi vector because that's all
the hidden units this is like the result
of one hidden unit for a particular data
point I so the ice data point
it's the output the output of one hidden
unit the j7 unit because the vectors J
for the eyes data point and what we
typically do in statistics and also I
try and copy their approach to machine
learning is take these values which form
for a given training set a matrix of
values so this matrix is the first
hidden unit the first data point the
first hidden unit the second data point
so on up to the sorry the first data
point the first hidden unit first data
point second hidden unit first they
point each hidden unit so each in total
and then we've got a number of rows
going down here one for each data point
now the advantage of that nice form is
that we can re represent that output of
the neural network or the basis function
model or whatever and call it in this
matrix form so sorry that shouldn't be f
of X there because I've corrupted by
noise that should be Y little error
there funny how you only spot the errors
when you actually give the tour vector y
is equal to the full design matrix which
is n rows and H columns W is the hidden
to output so that's eight rows and one
column and then this is a vector of
noise that's going to corrupt it so the
result of this is n by one so this is a
way of writing down in matrix form what
the output of that neural network looks
like so we pre compute the basis
functions and then we just well do
linear algebra now the nice thing about
that is if we now define a probability
distribution all over this parameter set
w's we can use those rules of
multivariate gaussians to immediately
see that y is jointly distributed by a
zero mean Gaussian with this covariance
okay so let's go back through that a
little bit slowly because it's kind of
cool I think so this is what we're
saying Y is and we're saying Y is matrix
Phi times vector W plus epsilon epsilon
is noise
going back to here oh not this one let's
go to this one if Y is W times X then
this is the consequence that y is
distributed as W times the mean the mean
in this case was zero and the covariance
was in our case later on its Phi times W
there's a little bit confusing there
isn't it it's a bit Kappa Phi times the
W vector so that comes out here as Phi
times the covariance of the prior over W
times Phi transpose and that's what
we'll see in the result so this object
here its Phi times Phi transpose that's
coming from the first term that Y is
equal to capital Phi times small vector
W and then there's an alpha popping out
front because that's a scalar now
because we corrupted by noise we added
Gaussian noise to this Gaussian thing
there's also a plus Sigma squared now
the really interesting thing about this
object is it's no longer iid right so
people use this for the training data so
people are used to describing their
training data as iid so they have
independence over why this object is a
zero mean multivariate Gaussian the
covariance of which is n by n right so
it's the size of the data set so what
this object states is all your training
data are correlated to each other in the
Gaussian way and we tend to call that
object the kernel matrix the kernel
function so this one that I've derived
here is of this form that's the one for
say a neural network so it neural
network is already a Gaussian process
where you've integrated out the front
weights but it's a particular type of
Gaussian process in fact it's a
degenerate one so but rather than
writing down these basis functions what
we tend to do in the Gaussian process
world and also kernel methods have the
exactly same object with a totally
different motivation by the way be
careful with that there's there's a
challenge that there's in there's only
about certain amount of maths that works
right
most maths doesn't work you can't do
most integrals and so and so forth so
this is very misleading for
that people use the same set of maths
with a different goal right and then
people argue about whether they were
using the maths right so the way I
imagine that is we're crossing this
massive desert of a landscape where we
can't go anywhere we're all trying to
travel in different directions on the
other side but when we go along the way
we often use the same set of maths so
the motivation as to why they use this
kernel function in svms is very
different it's all about hilbert spaces
and complex maths in Hilbert spaces but
the same object comes up in Gaussian
processes and you can do a lot of the
same analysis but the initial motivation
utterly different and motivation is
important in terms of what steps you
make as you're designing your algorithm
so I said we representing the covariance
function and this is the form of the
covariance function in this case this is
now the vector of all hidden unit
outputs for the eigth data point in a
product with the vector of all hidden
unit outputs for the j data point and
that's the correlation for your function
now that it's also got a noise term out
if I'm just looking at this part of it
here that's the correlation for your
function so it's this inner product this
sum here okay so that's the Gaussian
process basically primarily introduced
I'm going to give you some intuitions
about it in a minute so instead of
making assumptions about our data as
being iid we do have a functional
assumption in there actually we've got a
functional assumption we said it was a
neural network but it's led to this
object that is multivariate density over
all data points the covariance matrix is
a function of both the parameters of the
activation function w1 so the inputs to
the input to hidden layer and the input
variables X so all the input variables X
the covariance makes use a function of
both of those now w1 can be quite large
if we have very high dimensional inputs
and we got large number of hidden units
as we might like that could be a very
big matrix so we've integrated out
actually the very few parameters
relative to the number of parameters we
might have on our input okay now those
basic functions themselves can be very
very complex and you'll see a lot of
work now on people doing various things
you can make that basis function a deep
neural network
neural network you can have it as some
outfit from a recurrent neural network
you can do all sorts of things but you
could also look at neural networks in
this way and actually be often Sogeti
paper on batch normalization you can
further see the analysis they're doing
is very similar because it's about the
output of activation functions and
normalizing them to stay in the range
where you've got gradients right so the
map they're using is Gaussian process
maths
to try and sort of I would view it that
way other people might do it slightly
differently we all traveling on the same
row in order to come up with the
conclusions for that right so it's very
useful to understand this perspective on
these things even if you're doing your
own there something about that process
is that it's degenerate so what do I
mean by degenerate
well this covariance matrix is rank of
at most H so what I mean by that because
if we look at the way the covariance
matrix is formed here this is an N by H
matrix by an H by n so that's the bit
that's looking after F this is the
corruption that gives us why if we look
at the rank of this part here it's rank
is at most H so that means that you've
got problems when you try and use it
within a covariance in a Gaussian
density because the Gaussian density has
in the normalization the determinant of
the covariance so if you drop the noise
term if the noise goes to 0 then k has a
determinant of 0 which means it's not a
valid Gaussian distribution so when n
goes to infinity that's what happens
they make the covariance matrix isn't
full rank and that's indicative of a
problem right it's indicative of the
same thing that happens if you start
within your own network let's say I
don't know probably a lot of you have to
train your own networks for a living and
someone knocks on your door and says I
need a neural network I've got a
thousand data points ok and you go away
you build your network I don't know do
something with the data
I know knowing you have a one layer
neural network so you have 100 hidden
nodes ok but then you know it's an
industry environment so next week they
come back later and they say oh actually
I made a mistake I've got a billion data
points are you gonna use the same neural
network you trained on a thousand
probably not and the same thing sort of
going here actually so the number of
hidden nodes you're probably going to
increase them but what happens if that's
what's going on in real life if in real
life your neural network keeps
retraining and keeps having to learn new
stuff right sees a billion things you
don't know how long it's going to be in
production you've actually a priori
designed the capacity of your neural
network and that's coming through here
in this degenerate Gaussian process
that's what we call the degenerate
Gaussian process this I considered a bad
thing because it means that the model
can't respond to the data as it comes in
the models parametric it's not what we
would call nonparametric so Radford Neal
he was doing his PhD thesis on Bayes and
neural networks if you are working on
based in neural networks and you haven't
read Radford PhD thesis
you should stop working on babes in
neural networks and go and read
Radford's PhD thesis first it's still an
amazing read as is David but Radford's
thesis he actually looked at Bayesian
neural networks and you say well what
happens in the case where I decide to
have a base in neural network with
infinite hidden units so he said people
want larger large number of hidden units
and this is page 37 in his thesis and
this is where he decides to take the
limit as the number of hidden units goes
to infinity now as he does that he has
to scale down the outputs weights on
those units to make sure the function
doesn't go to infinity and roughly
speaking this is the sort of idea that
the kernel function we've just described
is of this form alpha in a product
between hidden units associated with
input I and hidden units associated with
input J so that's a sum right I like to
use the linear algebra but just for
clarity that's the sum over all the
different basis functions now the
classic way you you go from a sum to
take the limit to infinity is you
replace that with an integral and so the
analysis that Radford did says well what
I'm going to do is I'm going to have
infinite hidden units and I'm gonna
treat them by by every hidden unit has
its set of parameters doesn't
so every time I get hidden unit I sample
the parameters from a probability
distribution that I've written here P of
w1 and then I keep doing that for every
single hidden unit and then so rather
than getting a fixed set of parameters
here and if you do that then you get
this integral instead or roughly
speaking you get this integral I mean
you can read Radford to see the nice
some results but because of the central
limit theorem so it turns out it doesn't
P of W doesn't even need to be Gaussian
the prior over the input variables
doesn't need to be Gaussian what you do
have to be careful of is you have to
scale down as you increase the number of
units to infinity as I said you have to
scale the output variance of the whole
thing by 8 otherwise the output of the
function blows up to infinity so just to
give you a sense of what you're doing
here there's a little probabilistic
program not very complex one what you're
saying is I'm sampling that every time I
get a new activation unit I sample the
parameters for it from a probability
distribution and then the activation
associated with that unit if I call that
by I that's now a random variable right
if you think about that if I'm sampling
random variables on the input Phi is
around and variable if that has finite
variance what's going to happen next
well I'm just going to sum up all these
fires weighted by the W's right which
are also sampled from a poverty
distribution as I take that to infinity
central limit theorem applies as long as
the scale output of these things has
finite variance yeah so as long as the
scaled output of these hidden units it's
finite I can take this limit and I get a
Gaussian process nice so so further
reading on that um I think chapter
choose to read of Neil's thesis is
excellent the rest of nielsie sisters
excellent all of David mechanics thesis
I mean they're very informative about
these things but the result is this sort
of amazing thing which is this object
where you can so this what I'm going to
show you now it's kind of all Bayesian
inference the objective in Bayesian
inference is you have this model and it
to give you ways of expressing like you
can sample from this model about what
you think the world looks like so I've
taken like a thousand
samples from a Gaussian process I'll
give you a bit of intuition on how I do
that now in a moment but then when I
combine them with data I've got three
data points here I throw away all those
functions that don't go through the data
and I just look at the ones that remain
that's actually not dowsing processes
that's just what you're trying to do in
Bayesian inference you've got some
complex model of the world in fact
there's a wonderful technique called ABC
that does almost precisely this very
complex model the world you sample from
the model of the world many many times
and you throw away samples that don't go
near your data it's extremely expensive
but sometimes it's worth doing
particularly on AWS that's actually a
distribution over functions so what does
that mean where did these samples come
from so I've got this process where I
can sample these things over all the
space here and then I've got to throw
away any data that goes through doesn't
go through those points that means I
should be able to sample these processes
so I'm going to try and give you an
intuition about how we do that so what
I'm going to show you on the next slide
is one single sample 1 sample but it's
not 25 points in because it's from a
multivariate Gaussian so it's trying to
give you this intuition about what's
going on so one sample here from a
multivariate Gaussian distribution right
so I've plotted these points against
their index so it's a 25 dimensional
Gaussian so one sample but it's got 25
points 25 dimensional Gaussian one
sample but this is the covariance
associated with it so the multivariate
Gaussian has a covariance of this form
where the cyan is correlated and the
yellow is uncorrelated so if I look at
any two neighboring points this is why
Gaussian processes work what I'm about
to do effectively to look at these two
points I'm let's say that these are my
two wives right like I said before and
then there's Y star which is the things
I want to predict in the rest of the
world but these are my Y's so in order
to work with these two points I have to
integrate out all the y3 up to Y 25
that's typically hard that's typically
very
very hard in a marginal in a Gaussian
density like this all you need to do is
look at the covariance this is a zero
mean I should emphasize that as well
I've sampled this from a zero mean so it
looks like there's a curve here but the
mean is zero so all I have to care about
is the covariance and all I have to do
is focus in on the upper two elements of
the covariance that's the covariance
that I need to give me the Joint
Distribution of y1 and y2 it's beautiful
and since it's gone cyan there now I'm
showing you the values so what it says
is that those two points are strongly
correlated if we will look at
correlation coefficient to be not 0.98
with each other so y1 and y2 a very
strongly correlated with each other
under this covariance doesn't matter
what the other date we're doing this is
the distribution for those two and I can
even plot it in a two dimensional
contour so this contour is showing you
the Joint Distribution of what's called
f1 and f2 I'm interchanging between when
I say F it means the noiseless process Y
is the process with noise corrupted I've
not been very clear on that apologies so
what I have here is this contour this is
one contour of that multivariate
Gaussian so it's very strongly
correlated and I can ask the question
you know what I say before I said that
machine learning is all about prediction
so I can now use this to ask the
question if I've observed f1 what should
my prediction be of f2 well that's the
probability of f2 given F 1 which is
Gaussian conditionally and it turns out
to also be Gaussian that's also very
unusual but most distributions won't do
this for you so what it says is if this
is my covariance that's coming from that
large multivariate covariance and if I
just look at these two points here and I
only know F 1 and I want to predict F 2
it says that F 2 is also Gaussian and
look F 1 was in the negative quadrant
it's saying F 2 is distributed along the
x axis F 1 is on the y axis so I've
drawn the pink line where we observed F
1 and then I'm drawing the cyan line
we're observing F 2 and it says that F 2
is also likely to be in the negative
quadrant although it could there's some
small probability of it being in the
positive quadrant if I look
f-five and f1 I actually see things are
less well correlated well there's an
error here actually at about 0.5 the
correlation is I don't know where that's
not updated now if I observe f1 and then
i condition on that and I look at five I
see that there's much higher variance
right so as I move away from the
training point the variance is
increasing that makes sense right
because I've got less information as I
go away typically from that point so the
key object is this covariance function
because that's all I needed in my model
plus data to get to prediction I just
need to do the covariance combine it
with data and then I make a prediction
over what I think's going on in the
world with uncertainties notice the
prediction is a distribution it's got a
mean we can quote the mean prediction
but it's got a distribution very elegant
you can imagine um what I felt like 20
years ago as someone who'd been
interested in neural networks and
graphical models hearing David talk
about this but it actually took me about
three or four years to get in the
situation where I started working on gas
stream processes so the really nice
thing about this setup is that you can
make the prediction of that mean that
mean becomes a function because K itself
in like those is a depend on those basis
functions so the mean is a function of
the covariance matrix and it involves
this inverse of the covariance matrix
here K that's the covariance matrix for
the training data and we can compute the
covariance between future test points
and the original training points and
that's our training observation so it's
dependent on all those things it's like
a neural network prediction would be
depending on the basis functions but as
well as the mean we can predict with the
variance we got a posterior covariance
associated with it which is very cool so
this object here is the posterior
covariance the posterior way that those
functions move and then rather nicely we
can actually turn it into this in your
product where alpha is a sort of vector
here okay so going back to the thing we
had before all Bayesian inference
methods can do this throw away data
that's not near the samples that's easy
what Gaussian processes allow you to do
is this you can analytically compute
what the mean of that population is and
its covariance at all points it's
beautiful
okay so different covariance functions I
said that the object that was at the
heart of this was the covariance
different covariance functions have
different effect so what you're seeing
here is the covariance function I use
for that prediction and this covariance
function comes about there's a really
nice knits paper by um Chris Williams
where he derives two of the covariance
functions I'm about to show you this one
is to write people know like RBF
networks if you take the infinite limit
of an RBF network under certain
assumptions you get this covariance
function where you're taking the
infinite limit of the RBF Network and
you're distributing all those basis
functions all across the space you're
not centered in a particular location so
this one here is very commonly used and
it's called the called my eight things I
call it the exponentiated quadratic just
for fun but um it's call so called the
squared exponential the RBF covariance
or the Gaussian covariance I don't like
calling it Gaussian because it confuses
with Gaussian process it's just
coincidence but it has this form okay so
let's look at some data and I like this
this this data set here um max mentioned
data science Africa and the first time
we taught in Africa I was talking a bit
about Gaussian for a since I'm using
this example it's just even Kip to teach
who I was teaching in Uganda who is the
Ugandan gold medalist in London 2012 for
the Olympic marathon
so the data we're going to look at is is
the data from the Olympic marathon over
the years so marathons varied in length
in the early days to what I'm showing on
the side is faith in meat and minutes
per kilometer so I'm the most recent
winners win in about three minutes so
Kip which was running about three
minutes a kilometer I don't think most
of the audience here could run 100
meters in three minutes it's very very
very fast um
this an odd thing here this was the
marathon instant Louie where they
actually got lost and they were
following cars it's is 1904 so the cars
presumably weren't that healthy and they
were also kicking up dust so they don't
only got lost but they were like caked
in
just and everything um my fastest
marathon time is about here a little bit
slower than they ran that in um so
there's an outlier which is why I like
this data and it's sort of going down
over time and we have a sense about how
it should change so this is a Gaussian
process fit through it so I've told you
all these wonderful things about this
but in many respects some of the
challenges with even fitting this
marvelous framework becoming apparent
because the model is assuming constant
noise and there's an outlier it has to
put quite a large noise out and it's
placed there in all locations on the
input space it's also sort of stationary
so it can't sort of assume that they're
things were going on in the past have
changed it looks like there's been some
transition as people worked out how to
run marathons
by the way Alan Turing runs a marathon
about he was really fast two hours 46
minutes I can't work it out but he
wouldn't have won the 48 Olympics but he
would have been like 10 for something
which is really amazing very fast
marathon runner this is about two and a
half hours these people are running here
so there's some issues right because
these uncertainties are way too large
and it's because that covariance
function the thing when I saw this stuff
for the first time I was just shocked I
would thought if we can do that why the
hell are we bothering with these neural
networks because it just seemed to me
everything I wanted to do within your on
that work and get uncertainty out of it
and everything else was possible with a
Gaussian process now there's many
reasons why we bother with neural
networks but the thing I didn't
understand at that time is sure we can
do nonlinear functions but when you move
into the world and on linear functions
then those classes are nonlinear
functions so this one describes a class
that is infinitely smooth it turns out
infinite smoothness isn't um isn't
always a good idea I've just been given
the five and what I was going to do is
actually just um show you a couple of
other covariance functions then start
for questions we'll have a quick break
and be back here at UM
twelve so actually this here is another
covariance function that looks similar
to that first one but it's actually a
fixed basis function so there's only
there's four basis functions across the
real line here and you can see how it
kind of doesn't look too dissimilar
these are a Gaussian basis functions
this
here is a brownian covariance so
Brownian motion is a Gaussian process
people don't normally write down the
covariance function so the covariance
function some Brownian motion is that
what I move at every time step I move in
a Gaussian random walk right so you can
see that these are non differentiable
paths something you probably associate
much more with the Gaussian
non-differentiability but a lot of the
processes we use are either infinitely
Dementor differentiable or multiple
times differentiable so just because
it's stochastic that's a common
confusion it doesn't mean it's not
differentiable very interesting that so
but because the Brownian is not
differentiable people often think of
gaussians there's not differential the
form of this covariance is the minimum
of the two times so this is valid only
in time this process as I've written it
so over time what happens as you're
moving forward the variance is along
this diagonal here and the variance is
just time and of course we know that the
standard deviation of a Gaussian random
walk goes up with the square root of
time so you see that directly out of the
covariance here but this is giving you
the cross correlations as well here I've
started at zero so it's a non stationary
process as well this one is one of my
favorites because this is like a neural
network covariance so this one here oh
and by the way the animations of the
functions are something that Philip
henyk came up with they're like round
world tours of functions so in these
very infinite dimensional spaces you
sample a function you sample another one
and then you tour between them to give
you an idea of what these functions look
like how rigid they are and this sort of
thing
this multi-layer perceptron covariance
was derived again by UM Chris Williams
in that 1996 nips paper which is what
happens if you take the limit if it's
not hyperbolic tangent because that
integral is not tractable but he
substitutes that with earth functions
which look very similar the limit of
infinite earth functions so actually
what I probably should have extended the
list here these things saturate as you
leave the center because these these tan
H's are in the middle so it's one layer
network as well so you could sort of a
get quite a lot of intuition about these
things this is my favorite Gaussian
so I'm not an expert on this at all but
Karl Kramer was talking about this at
last year's knits this is the cosmic
microwave background so it's the moment
after I don't know what happened in the
universe who knows and I'm sure lots of
people in the audience do so I'll just
embarrass myself by speculating um so
there were some sort of plasma that
light couldn't go through and then it
was some moment at which it somehow
condensed because the energy is reduced
enough and at that moment what we see is
the echo of that moment when visible
light could first exist and that's the
cosmic microwave background because that
was so hot and these particles were
moving so rapidly then it's actually as
far as I can tell and all the physicists
tell me this is true and Kyle says this
is true a Gaussian process it's um
ok it's in some funny coordinates
because we were observing it so I don't
really can't think well enough about all
those things but the way in which these
temperatures are fluctuating has a
covariance function which is dependent
on like the amount of dark matter in the
universe the amount of photons so it's a
covariance function that I think has
about five or six parameters including
dark energy so fitting the Gaussian
process to that tells you why our
universe is composed of which i think is
pretty cool what we're going to look at
and gasifiers have lots of uses in their
vanilla form but of course the universe
today isn't as Gaussian as we might like
if you lived in the universe then you
could use a Gaussian process to predict
across the whole universe all states
probably the Sun level in which it
wasn't Gaussian but certainly in this
large level you could have used that to
predict the entire state of the universe
which is pretty cool it's kind of
fortunate that the universe isn't like
that today because we wouldn't really
exist but the argument for the next part
is what the universe is like is a
nonlinear function so I think that this
is also to the valid that the observable
universe today is just a very very
complex nonlinear function of that
underlying Gaussian process so in the
next hour that's what we get to deep
down in processes so what we're going to
do is we're going to look at the case
where we take this Gaussian process here
we feed it into a nonlinear function and
then we do inference over that process
as well and that's the first stage of a
deep Gaussian process
so we can compose functions together one
after another in order to arrive at the
earth maybe I won't do that full
prediction the data sets have a little
bit smaller actually um okay so let me
just see if there's any questions I
shall take questions now or just go for
break what do you think max
he said questions please don't be shy
it's not a big audience so there's
microphones with one is there there's
another microphone right there so if you
have a question you have to walk all the
way forward to ask your question well
let me ask a question then so a Gaussian
process always has a also has a mean
function that you can fit but nobody
fits the mean function why does nobody
use the mean function what's wrong why
is everybody setting it to zero and in
the mean time when he answers the
question and you think of your question
please move forward to the microphones
so people do fit the mean function um
but it's sort of less they're typically
when they have an application or they're
doing something specific and they have
some fundamental knowledge about how the
mean will very often in physical systems
you have that statisticians do it a lot
and they'll parameterize the mean
function
we'd like a beta times a number of basis
functions the reason why I don't like it
and there's a paper that does it but
neural networks I think it's the wrong
thing to do because like you're trying
to well it depends what your objective
is like but if you're just doing general
modeling then you've just put back in
something that's not probabilistic and
lost all your nice error bars you now
have to do the uncertainty on that and
that will be highly correlated with the
uncertainty on the Gaussian process so
you can do it if you've got a really
good reason to do it I don't think it
should be the default thing and when you
do do it it turns out to be very hard to
do the sampling well because there's a
lot of as you will know or you need max
to do
for you because there's an enormous
amount of correlation between the output
of the Gaussian process and whatever the
mean function you may have put in if you
treat it probabilistic in here if you
don't treat it probabilistically then
that's a very valid thing to do in
statistics where you care about the mean
function and you're often and they'll do
this a lot in geospatial statistics and
you're actually using the Gaussian
process for the noise so that's a really
common thing that's what you'll see in
spatial statistics is um you might see a
linear model of disease so we might have
like spatially we look at disease across
they come up a lot in Africa actually
across an area and you're trying to
predict disease based on some factors
like poverty and rainfall and things
like that but then the spatial noise and
then you use the Gaussian process for
spatial noise and try and pull all the
prediction in the mean function it's a
question there so please go ahead yeah
this microphone work oh it does um let's
talk about kernel hyper parameters so
you covariance function you to be like a
link scalar these variances you could
have put in front of these things in
order to sort of fit these Gaussian
processes and the thing you'd like to do
is of course how to integrate out these
parameters for the variance function but
these are one of these so-called hard
integrals and in many cases so a lot
people try to estimate these from
accident likelihood or something like
that it doesn't this sort of what break
a little bit of the elegance of the
Gaussian process sort of setup and like
I see this sort of an unsent unsolved
challenge if you can you comment on that
a little bit on life where you see that
going and whether it can be solved or we
have to find a different framework so
the question was about kernel hyper
parameters yeah yeah so what you what
you end up with is the priority used to
put over the parameters your basis
functions also have parameters
themselves and but they tend to be much
fewer so there's like two three
parameters in the covariance function I
just fitted to the Olympic data so you
hope that that's better because they're
fewer if you have a lot of data they
tend to be well determined but yes as
you get very complex covariance
functions you start doing inference over
those so you tend to look for well
people have a range of approaches and
that's complex yeah but it's sort of
like I
when you look at Bayesian neural network
people often just keep those parameters
fixed they don't even move them so yeah
that's one problem with being Bayesian
it's a bit like where do you stop
I mean you should also integrate over
all classes and models that's the model
misfit I talked about so my own approach
to the hat is actually if you put a
Gaussian process over the inputs of the
Gaussian process then that gives you an
incredibly flexible because a lot of
your - parameters are encoded in that so
that's the direction I've chosen to go
on but there's lots of interesting work
and a lot of it is dependent on what
your objective is thank you ok so let's
take a 15 minutes break so that means
we're going to be back at 5 minutes past
12:00
and please be back in time thank you
okay so um at some point the universe
was a Gaussian process and it is no
longer and one reason for that is that
if you start pushing nonlinearities
around Gaussian processes they become
non Gaussian and that observation was
what inspired this idea of deep Gaussian
processes okay so this is Chi welcome
Aaron so this is a button it's kamek
together I think we did a bit of
photoshopping and something there's
probably a lot of teams like nips like
this that here to do this we want to be
taken seriously by the research
community and we need to come up with
some theoretically driven research
projects one model to rule them all
Rajon see large neural networks I guess
that's someone like me sparse Gaussian
processes yeah okay that was a kind of
shared that on Twitter I liked it it's
hanging he kindly gave me a copy of it
that's hanging in the wall of my office
so um sparse Gaussian process is
something that we have to do in order to
operate with Gaussian processes in
practice fact someone just asked me
about this in practice you do low rank
so I'm not going to UM talk about it
I've lost the slides one second okay
there we go
look at that fixing life that means that
particular slide of what she isn't
online I'll fix that later so this is a
little example actually on my postdocs
James then when originally put together
and just showing yeah I don't know how
clearly you can see the error bars but
there's error bars the faint yellow is
on the slide Paolo geez for the color so
this is a Gaussian process fit to some
data that's in two clusters with a large
sort of error bar either side
now that's a full gallon process fit
what we do in practice to do Gaussian
processes at scale and it auto is the
key approximation underpinning a deep
Gaussian process is will very often do
what we call the sparse Gaussian process
so sparse Gaussian process is this sort
of thing where we say instead of using
the full covariance function we're going
to use a low-rank approximation to it
now as someone just asked does that mean
we're just going back to the parametric
model my answer is a firm no but I'm not
going to go into the details of why that
is now so this is what happens if we do
it to that data right so not a great fit
so we've chosen six points what we call
pseudo observations and what we do is we
choose to compress the information in
the training data into those pseudo
observations and then we store any of
those pseudo observations rather than
storing the whole data normally the
Gaussian process is a nonparametric one
requires you to store the whole data so
if you do this naively you get quite a
bad fit like this and so here these
inducing variables are at fixed
locations on the input somewhere near
where the training data was so this
would be a bit like um it's a little bit
like the random Fourier features
approximation but we tend to go much
sparser and the reason is well if I now
optimize the kernel parameters which I
can do so we typically do maximum
likelihood and fit this model what it
will actually do is it will try and fit
through with a model that where the
kernel parameters make up for the poor
quality of the approximation but we
don't just do that in practice we also
optimize with respect to these inducing
variables so these inducing variables if
you watch them here if I optimize with
respect to their location they are
moving out now closer to where the data
is and that's the sort of approximation
we use in practice so we try and
optimize over inducing variable
locations and kernel parameters jointly
and if you compare that with the true
approximation it's not perfect but it's
not bad
given where um reducing it to six data
points the hope is that's a very large
data for very large data that we can
approximate the process we're interested
in with your inducing variable
then there are data points and we can
gain computational efficiencies because
the problem with the gas in process is
the computational complexity is cubic in
the number of data because we have to do
matrix inverse and the storage is
quadratic and it's the storage that hits
you first you know you'll get memory
problems before you run out of patience
so I'm not going into the details of
these approximations but that's sort of
what we use um there's a really nice
review of the latest perspective on
these just published in jml are a
unifying framework for Gaussian process
pseudo point approximations using power
expectation propagation trying buoy Rich
Turner and others and then I would
advise if you're interested in how um
these approximations work in practice
then the PhD thesis of andreas Damiano
deep Gaussian process in variational
propagation of uncertainty if you really
want the details of how these things are
done for the Gaussian process and the
top one is for GPS in general and the
bottom one is for deep Gaussian
processes okay so um that's the sort of
really tricky Mouse out of the way
without any math um there's a really
interesting thing about Gaussian
processes is that they're mathematically
beautiful and elegant but
algorithmically they're very well
they're relatively hard to deal with
you're all networks on the other hand
are mathematically not very elegant it's
difficult to say things confidently
about the nature of the functions you're
using when you fit in your own network
but algorithmically they're really
simple so this is interesting dichotomy
but of course you know you can use both
you don't have to go in one way or the
other you can use both depending on what
you're interested in doing so this is
the reason I joined the field in 1995 or
something because I like drawing your
own networks so this is a a neural
network where we've got some inputs and
then we've got fully connected hidden
layers to some fully connected hidden
layers phone probably connected in there
to some out I'm not looking at a sort of
calmed net here I'm just looking at a
multi-layer neural network now what I
want to look at is the maths of how that
comes about so it's similar to before
but we now have multiple layers of basis
functions
this is why I was using W all the time
before so I can keep using W so each
layer of weights is represented by W
this is the first layer after the inputs
is the second layer third layer so I
take the inputs multiply them by W
through the activation functions here
I've just kept all the activation
function is the same now the interesting
thing about this is what you're here so
in the Gaussian process we talked about
taking the width of the hidden layer out
to infinity in an effort to improve the
capacity of the model and sure enough in
neural networks people use very large
with hidden layers eventually thousands
of units and if you've got two sets of
hidden layers well next to each other
with a thousand units in each then the
matrix in between has about a million
parameters so I'm one way of reducing
parameters is so that's just the maths
of each hidden layer sorry is to use
drop out so you actually take out your
any trained half or some portion of the
hidden nodes at a given time but what I
want to sort of say here is an
alternative solution because it will
lead me to a DGP so the alternative
solution is that you can parametrize W
with its single value decomposition so
what do I mean by that so W can be
represented as a matrix u where u is its
as K 1 by Q Q is gonna be some small
number there's k 1 hidden units in the
left layer and k 2 in the right layer or
whatever the lower and upper layer and
so U is going to be K 1 times Q lambda
is Q by Q diagonal matrix U is an
orthonormal matrix and as is V and V is
Q by K 2 so if you multiply these out
you get a matrix of that size actually
what we actually do in practice is throw
away that object there and just look at
two matrices U and V so give you a
pictorial representation of this if this
is the full matrix W of all the weights
what we're doing sorry that use hard for
me to see on the screen what we're doing
is we're decomposing it into a sort of K
1 by Q sized matrix multiply it by Q by
K to size matrix in the modification of
those two leads to a W which is low rack
right so that's another way
of reducing the number of parameters
because now I have K 1 times Q plus K 2
times Q instead of K 1 times K 2 so we
can draw that and that's what I think I
think nowadays they called bottleneck
layers when I first introduced the DGP
in this way I didn't know whether they
exist it or not but nowadays I think
that they you can do this you can put
them together in pencil flow and it
looks something like this so I put for
these linear layers what's in effect is
you've got a matrix u being multiplied
by matrix V transpose before going into
H so if we look at the mass now we've
sort of given an X we take a
multiplication by de to give us that and
then that Z goes into this basis
function here multiplies by you to give
us the output at this first set of
hidden nodes and we do this so on so
forth so at each layer here the output
of the layers is being bottleneck down
to something in these new variables I've
introduced called said H is the old
hidden layers we have so mathematically
we can write that in this form or
alternatively we can just write this we
can say that said one the first of those
sort of new introduced layers is just V
1 times X but then the next of those is
there to be two times the hidden layers
now look at this object here what that
looks like is just a single layer neural
network with multiple outputs and what
we can see we're doing is we're stacking
these objects so these are like vector
value functions now right so the first
bottleneck is linear actually in this
case but we could even drop that
bottleneck if we want and then the next
bottleneck is is a neural network the
next is a neural network and then we
take those said and we just make a
linear mapping we can even drop this
bottleneck if we want and just start
your all networks together each one a
single there your network point I was
making before is that where we have a
neural network we can decide we can
decide to take the number of hidden
units to infinity integrate over all the
parameters on the input and the output
layer and then we have a Gaussian
process that's pretty cool because if
you want to get rid of all these
parameters and then you're
work you can integrate them all out and
just write that instead so you've got a
series of functions where each function
X is a function of X initially and then
is a function of a previous hidden layer
but it's a Gaussian process and in fact
a vector-valued Gaussian process so
there's many different ways you can do a
vector value but in this case we just
assume each function is independent or
you could do whatever you like so that's
the idea of the deep Gaussian process
you might be asking yourself Oh what
happens if you take Z to infinity as
well the inner dimensional layers well
then you actually get back to just a
stand-in Gaussian process again you
don't get any effect these bottlenecks
fan out to be very very important in a
deep dieting process so if I make these
bottleneck layers larger and larger and
larger and larger all I'm building is
what's sometimes called a deep kernel
putting that in a Gaussian process
because what happens is that the
distances between points as I take the
width of Zed two infinities concentrate
and the expectations disappear so if I
take so actually it's really interesting
that I can take the width of these
hidden layers for infinity and that's
good that's very nice and complex but if
then I take the width of these
additional latent layers well it's it
you still get a model but it's just the
Gaussian process you can just flatten
everything down again so a deep Gaussian
process where you don't have these
intermediate bottleneck layers is just a
Gaussian process ok so mathematically
this is just a composite multivariate
function and as is a neural network I
think that we like to make a big fuss
about these deep neural networks and but
when you if you show it to a
mathematician they'll sometimes say
things like well why have you bother
writing that thing down on the right why
don't just write G of X it's just fair
enough and the reason of course we do is
because we can include all sorts of
these beliefs about the way the world
operates if you're putting a
convolutional neural network in one of
your functions or if you're gonna use a
recurrent neural network you're encoding
something about you where you believe
the world operates and the nice thing
about these frameworks
use is that um the end result is the
composition of simpler functions so
that's true in neural networks and it's
true also in Gaussian processes but the
interesting thing in a Gaussian process
is that this is a composite multivariate
functions so Gaussian process on its own
f of X is a stochastic process so this
is a composite stochastic process so
it's not just saying something so
stochastic processes are amazing they're
very powerful but they're frustrating in
that there's only certain classes of
stochastic processes that are
analytically tractable like a Gaussian
process or Poisson process Markov
process jump processes so we should
always be on the lookout for new ways of
creating stochastic processes based on
these component parts and this is one
and it's the first time I've seen it
done is with a deep GP although the
people may have done it elsewhere and
the nice thing is it gives you a
different class of stochastic process
the nasty thing is is it becomes non
analytic so another way of writing this
down is is through a Markov chain so um
that's the functional way but in this
case we're putting a prior over each
function so implicitly underlying each
of these is a sort of probability
distribution of what F 1 is that speak
out in process that's a Gaussian process
that's a Gaussian process so and so
forth and then what you typically want
to do is integrate over all the
intermediate paths and that turns out to
be an intractable we have a lot of
approximations to dealing with that
there's another tutorial in here later
so I'm not going to talk about him but
this is what we get is this chain it's
just a chain in fact most Neurol
networks are kind of changed you can
have skipped connections jump
connections that would change the nature
of the chain but it's very simple
probabilistic object so I've put it that
way up I like to think of the latent
variables often being in here so I like
my models that way up my observe
variables out here then you're all
network we're all from the other way up
so why deep I mean if we look at the
Gaussian process itself it has some very
elegant property so has this interesting
property in particular that the
derivatives of a Gaussian process are
also a Gaussian process that always
fascinated me like when I heard it from
day one I thought that was amazing and
it took me a while to understand why it
is but the reason is quite
simple it's because the derivatives
derivation of a function is a linear
operator on the function and any linear
operator is equivalent to in the
discrete case some sort of matrix
operation so derivation is simply a
linear operation
so therefore the derivatives of function
of a Gaussian function are jointly
distributed as Gaussian for a Gaussian
process for particular functions these
governance function is a universal
approximated so that's thought to be a
big exciting thing I think it's a very
uninteresting thing the last time you
had infinite data will be the last job
you work on but you know we never have
infinite data so we don't really care
about the behavior of things in the
infinite limit of data what we care
about is how rapidly they approach the
sort of behavior we want so I'm not so
worried about that but this Gaussian
derivatives might ring alarm bells and
in fact it is quite a bad thing so it
can approximate anything but it's got
Gaussian derivatives and there are lots
of functions that don't have gas and
derivatives so any kind of jump function
isn't Gaussian right so flat flat flat
flat step flat flat flat flat that is
very um that's an infinite derivative at
one point and everywhere else with zero
so any fairy-tale function and rad but
has really nice examples of this in this
thesis practices that aren't Gaussian
processes with heavy tail functions with
big jumps and is not a Gaussian process
it doesn't mean by the way you can't
handle the jump with a Gaussian process
you can do because you can
parametrically encode the jump in the
mean function as Max was talking about
earlier or you can make two independent
Gaussian processes either side of the
run jump the problem is you have to
encode in it's your back to that old
basis function problem before so to go
out of that class models the deep
Gaussian process from a process
perspective is this process composition
if I compose several processes together
I've got a new way of constructing
complex processes based on simpler
components now this is something that I
think for me this is totally obvious but
people don't really seem to spot it so I
just want to highlight it in one part
we've got a process like this we are
absolutely not constrained processes
like this I think it's very important to
believe in models where you could take
an observation off the side right within
this framework this is just graphical
models with Gaussian processes pumping
between them right
so every time I hear in multitask
learning or some kind of learning this
other type of learning you can always
write that down there's a probabilistic
assumption I mean it's a bit frustrating
how many different names we have four
types of learning where we should just
actually explicitly say what our
assumption is about our data so they're
often conditional independence
assumptions and a really nice way of
expressing conditional independence
assumptions is with a simple graph it's
difficult if you've got many many nodes
because it's very unclear what's going
on but if the links between the nodes
are very very powerful then you can
express those conditional independence
assumptions in the graph so here I'm
just postulating that we have an
observed variable that is for perhaps
closer to the input and another variable
that is further away and I'll show you
another example of my vision on that
later but the real challenge for these
probabilistic processes is that you're
propagating this probability so at each
of these stage this is a probability
distribution here and integrating across
those is equivalent to pushing a
probability distribution through a
nonlinear function not just a nonlinear
function a nonlinear process so if I
imagine if I had a Gaussian distribution
on this thread here and then I map it
through this nonlinear function here
then it's not gonna look very Gaussian
in fact it will typically um be non
Gaussian and in fact all so I sort of
thing people do and David McKay had
really nice early work on this called
density networks which incovenience at
more recently but we're ignored for a
long time where you represent your
distribution with a set of points and
you push those points through the
nonlinear functions in order to
represent your distribution on the other
side so that's a like that's like a
Monte Carlo method it's a simulation
method quite quite a good way of dealing
with it so this has got cut off at the
top here but even in one dimension if
you put a Gaussian variable through a
Gaussian process what you end up with on
the other side is a nonparametric one
dimensional probability distribution so
people get excited oh I've got a mixture
of gaussians it can model five bumps if
I've got five nodes if I put a Gaussian
through a Gaussian process the thing on
the right
could have infinite modes
I mean it's already a really powerful
distribution that's one view to 1d
having said that it's not structured
enough I mean there's lots of things
wrong with it will often not be a good
fit so that's why you often want to go
deeper so I really got into the sort of
thought of deep when it wasn't called
deep I think it was about 2001 when
geoff hinton came to max said I was
there briefly at Microsoft Research and
was talking about I'm stacking Boltzmann
machine models and the point he made is
that you could have abstract layers at
the top and more detailed layers at the
bottom and that's something I'm always
thinking about so I'm trying to not just
build regression models but also build
models where I'm integrating out the X
at the top like in again and then I've
got um those are the abstract features
and I drop down the model I can get more
and more detailed as I go through these
nonlinearities so you know think of the
universe that we're in the latent
process is that Cosmic Microwave
Background and over time we've pushed
forward with a series of nonlinearities
to produce us today approximately well
so if you do this with PCA you don't get
anything this thing is really nice
because szostak PCA or stat regression
so I think a piece stay because I assume
I'm putting a Gaussian on top here and
in each layer that Gaussian is going to
sort of drop down into these different
square and you don't get anything
nonlinear when you do that and of course
we don't build neural networks with only
linear activation functions for that
reason um actually notice that the
properties of this change right so it is
true that the properties of this change
so you came to get lower rank gaussians
as you drop down the network so it's not
that the same that you could put a
univariate gaussians
this tends to be lower rank this is low
rank still the rack still low rank still
and that always goes in that way you
can't unpick that so if you do do this
with PCA something does happen it's not
very useful so I'm doing the same thing
here with a stack GP so I'm just sort of
taking the original GP and look if this
one's got quite folded over on itself
like how complex it's got at the bottom
but this was these are motivation I
think that these motivated people doing
some work on these processes to see what
they look like and I'm gonna mention
that in a minute but this square now has
had this corner folded over onto itself
now these are two-dimensional remember I
said at the beginning the beginning in
the middle
the width of these dead layers is very
very important in exciting the
properties and process well here to
visualize it's just too so I'm mapping
from the Gaussian process I've got a
random sample of a Gaussian process
outlets which happens to folded this
Square over onto this corner of the
square here you can never unfold that
because these two points will always map
to the same place right so that's a
property of this and that happens in low
dimensions if I was doing this in
infinite dimensions it would never fall
longer onto itself
everything's equally far apart in
infinite dimensions and actually these
dimensions the way we set it up are
independent so that folding over
wouldn't ever happen and that's why it
stays a Gaussian process as we go to
infinite dimensions because it's bowling
over doesn't happen so as you reduce
down to these lower numbers you start to
get these falling over effects occur and
that gives you these very mongaul see
defect
only if it's Valentine's Day on a bird
miraculous when does a man see gee I had
actually checked me so they're pretty
good ok so now I think seeing those
slides
David Domino was only interested in this
and with Ryan Adams and others including
even gay Romani they looked at this sort
of view this is a kind of pathology
which I disagree on I I think it's a
property and this is a parameter you
should be controlling for the
dimensionality these late layers you
should be optimizing for it but he did
they did a very nice analysis and what
they showed is that the derivative
distribution becomes more and more heavy
tailed as you go through the deeper
network so you stand tend to get the
ability to model these jumps now there's
another really interesting paper that's
just out on archive a week ago of
prompted by me saying can you put on an
archive so I can talk about it how deep
are deep Gaussian processes and this is
an analysis by don't lock your honor me
Andrew Stewart and others which is very
interesting because you you've got these
elegant processes right now and if you
think of the vanilla Gaussian process
it's just a Markov chain so they look at
how quickly that Markov chain mixes and
then you can answer questions like
well if you want to understand the
stationary distribution of how that
Markov chain mixes as you go deeper if
it's a gothic you can show that when
it's all ghatak you can sort of say well
actually I don't need to have a hundred
depth of liars
if the Markov chain is already mixing
after five it's pointless because my
prior probability is already telling
everything you want I think that's true
and it's a great observation the only
caveat I would have is typically you're
optimizing hyper parameters of these
models as well and you will the
assumption that they have is that those
hyper parameters are static the other
aspect I think both these papers don't
really refer to is this important aspect
that you very often want to structure
structure the model so you may have the
fact you may have those side
observations will really affect the
conditional independence structures on
this graph so they're lovely theoretical
analysis but we'd be careful by drawing
up too much too general conclusion
that's always true of theory which is
why we do practice Oh YouTube service
DNS address could not be found so this
was a nice video that some David did
that's associated with this paper you
can look it up of showing this sort of
shape ways in which these um these
processes change the boundaries between
places as you go deeper and deeper it's
a some a nice watch apologies I can't
show ok so um that marathon data I used
as a motivation example before remember
1904 they got lost and we're almost as
slow as I am so just just from memory
this is the Gaussian process that we had
before so what the way I've chosen to
fit this is just on a two layer Gaussian
process so one Gaussian process going
into one dimension and then one
dimension going out again and the reason
I chose from that is so that we can
visualize what it does inside so I
fitted that deep Gaussian process now
bear in mind this is um 20-odd data
points the size of this already is
infinite parameters right but because
they're being treated in the bayesian
way we're not getting over fit the
actual parameters we phaeton a maximum
likelihood sensor
three in this case so the interesting
question is what happens when we fit the
deep TP well this is the fit so what you
see now with the deep CP is it's done a
much better job of understanding the
sort of stochastic city in the process
early on what I like about this example
yes you can do very very complex mean
functions with a Gaussian process we
already saw how complex it got after a
few layers but in this example the main
thing it's dealing with is the fastest
item in the process so what you're
seeing is that it's starting to tunnel
down to understand that actually in the
modern ear of the variance around
marathon times as low of course as we
move away from the modern ear as it
predicts out into the future of it's
starting to head back off it will revert
to something quite large but on the left
here it's sort of seen this crazy result
here and it's giving very large variance
here so how is it doing that well what
we'll do is we look so pay attention the
bottom this is year and this is going
from first as 1896 and the last one I've
got recorded here I think is 2016 I
didn't record 2020 yet so this is the
first layer so this is the input to that
hidden variable we've got inside look
how simple it is it's not doing anything
super complex right but it's kind of
informative what it is doing is that's
actually the outlier point about mapping
into the hidden variable right so this
isn't the output yet but because it's an
outlier it's already an outlier in the
hidden variable it broadly speaking
saying it's linear but most of the time
but then crucially it starts to curve
off very subtly and it flattens out so
all these are predicting to the same
distribution of the hidden variable on
this side right so that since 1980
approximately we're seeing everything is
up here before 1980 they're moving from
negative hidden variables up to positive
in mobile this is the hidden variable to
the output notice it's much more
confident here but because there's
uncertainty being injected into this so
this is conditioned on the input right
so there's uncertainty being injected
from the hidden into here so look what
happens
as we come in the mean of these points
I'm not representing the uncertainty the
mean of all those points is clustering
right so because the mean of those
points is clustering they also have a
variance which moves them side to side
in that way but the function here is
also pretty flat so basically they're
all around the 3-minute per kilometer
mark whereas these points here are
changing linearly through time so in
this in this sort of section here it's a
linear function broadly speaking of a
linear function but it's ambiguous where
it could put the uncertainty but it's
actually chosen to put the uncertainty
on the first function and why is that I
don't know but it's chosen to do it and
then on this bit here although the
uncertainty is of high it's going
between 2 &amp;amp; 1 the function is also flat
in this region so the end result is
something that looks like that so that's
heteroscedasticity and effectively a
change of length scale as well because
what's happening the effect of doing
this the effect of flattening here is
equivalent to saying that the length
scale has gone large so it's actually
allowing the process so here the length
scale is saying broadly the same and
here the length scale is going large so
that's why you're getting those
flattening functions okay so this
dataset is one that Brian Varys
collected for a collaboration of myself
and dieter Fox where we were some years
ago looking at some inferring location
given given Wi-Fi access point strengths
the interesting thing about the data is
I think I think Brian just walked around
in a circle I think he walked I don't
know how he got the data but maybe
looking at how many access points you
can read and what level they're at and
it's got a really annoying noise
structure because um when the access
point drops below a certain point it
just reads minus 0.5 in this case once
it's been scaled and then in these
things here right it's all discrete
levels as well so you can see that
there's no it's not continuous and this
is real data you match data that we look
like look at English eLearning
in a real data looks as crap like this
in it by the time we get around to
running our algorithms on it it's often
being cleaned up so in order to deal
with this it's hard to go for a very um
this is a Gaussian process bit so it's
hard to go for a very short length scale
to sort of deal with the rapid change
which is artificial obviously we should
get rid of this we should deal with this
somehow this isn't a Gaussian likelihood
isn't correct in this case we should
somehow reflect what's actually going on
but chosen not do that okay this is the
deep Gaussian process now it does have
some interesting slightly different
artifacts and I haven't investigated so
here it doesn't go flat it didn't work
out that the length scale in this region
is very very long why because other it's
difficult for it to do that because
other Wi-Fi access points are moving at
that time right so it's only for this
single output where it's dropped below
and I haven't given it the conditional
independence structure for it to be able
to work that out with enough data it
might be able to learn that and that's
something we should experiment with but
this is relatively little data
200 data points but what we do notice is
that the look at the quality of the
arrow bars have really improved so it's
sort of really instead of diving down
here and having a small arrow bar it's
it's not doing that anymore I mean it's
a different fit I'm not even sure I
think it's necessarily better but it's
very different quality and you can see
you get this heteroscedasticity coming
in as well this was um this wasn't known
to the model so this is the UM the more
the model has in this cases and the
input regression is time and it's
regressing from time out to 30 odd
access points simultaneously yeah the
Gaussian process has the fit of
advantage of doing this fit
independently on its own right so that's
why it gets this quite well it's doing
that on its own so this is the path that
Brian walked around I think it was the
Paul Allen building I'm not sure in the
UW and this is actually what we did here
is we've got a one-dimensional input
going to a two-dimensional latent space
going to a five dimensional Leighton
space going to a 10 dimensional latent
space right so it's quite a deep model
and this is what happens if you look in
the two dimensional latent space it's
sort of reconstructed
not perfectly it's got to fold over on
it right so that's wrong but this is the
this is the loop that ignore the fold
but if you go around like that
that's the loop he did the folds
actually quite difficult to get rid of
if you start with a 2-dimensional
Leighton space I didn't have the
patience to train it for long enough to
deal with that ok so next example is old
example that I'm andreas did I think in
the first paper we had on this which is
illustrating one of the nice
characteristics that it's very easy to
build these type of structures where
you've got some sort of time affecting
some latent variable and then you're
going out from that to y1 and then out -
why - so this is paper from like 2013 or
earlier five years ago or something and
then say oh well this person also has
something else going on so what this
data is is it's some two people walking
towards each other and high-fiving each
other so because they're two separate
people I've created one output for each
of those people a shared latent space
which is about what they're doing
together and two separate latent spaces
that are independent of that shared and
then the whole thing is governed by time
so actually I think there are
connections I think I've got this wrong
I think there are connections from time
to Zed and time to said - we'll see on
the next slide and this is the sort of
result you get in those latent spaces so
yeah that's right there was a one
dimensional going to two dimensional
going to three two dimensional is what
it learnt and in this case it will just
reconstruct as you go along here just
reconstruct two people walking towards
each other and high-fiving but the
interesting point is that has these
factory space that it learns about the
separate spaces so you can sort of do
these multitask learning things very
easy and it it's very very effective for
multi task learning actually it's
probably we started building these this
type of model in an effort to get the
structural learning right for the GP but
this this type of model without the time
on the top this type of model on its own
it relates to inter battery factory and
hours into battery factory into battery
factor analysis work
by one of the people that does is it
Tucker
I think it's Tucker which is similar
structure but with linear models so this
is a nonlinear variant of this and Samy
Kowski does a lot of work on Bayesian
linear versions of that as well with
article Armenia and others and this
works very effectively
even without the the deep stuff in that
case so digits dataset so another
question is um whether we actually
really need such deep hierarchies like
when we've got very low data so I'm
showing you quite low data there's a
reason for that it's because these
models are relatively slow to fit there
are a class of approximations that use
stochastic variational inference where
people have been doing billions of data
points but for illustration today it's
just easier to deal with these smaller
examples where I have a better
understanding of what's going on so in
the first paper we sort of asked the
question um do you really need a deep
model when you've got only very little
data so for 150 data points from M
missed so 60 zeros and ones so in the
first work although I'm not sure one of
my other collaborators were saying
didn't find easy to reconstruct this but
what we did is we tried these w's are
not the W's in the neural network these
w's are parameters we used to learn the
dimensionality of those latent spaces
which we can do because it's a Bayesian
setup so what andreas who did this work
is showing here is that we started off
we never think a 1 2 3 4 5 6 7 8 9 10 11
12 13 14 15 17 dimensional initial
latent space sitting on top of the usps
digit and then we brought that down to 1
2 3 4 5 6 8 9 10 or something like that
then down to 8 6 4 or 5 or something
like that
right but the model itself learned to
switch off two of these dimensions on
the first layer and the model also
learns the Det
so it actually learns how deep it needs
to go so that's all done by marginal
likelihood maximization which we can do
because we've integrated out all the
parameters and what you'll see what
Andreas did is found a point in this
high dimensional space
so represented by this zero and he moved
it found a direction where when you
moved it opened and closed the top of
that zero yeah so these are continuous
spaces we're not looking at like the
output of an individual activation unit
these are full functions so they don't
they don't interpret the same way as
when looking at a neural network it's
like looking at those additional hidden
layers so um so it's it's learn a
localized feature that opens and closes
in one position the top of a zero in
that high dimensional space it is
another feature he found on this second
layer which opens and closes the base of
the six but a six that is opened and
closed there so that's a bit of a
broader feature and then at this layer
here the second close to the top
he found a feature which moves between
zeroes and sixes and at the top layer
there's a feature that goes between 1 6
and 0 so the nice thing about that is
that you're actually managing to fulfill
this this slight belief of the more
abstract as you go higher up of course
when we did this Gans didn't exist they
were a long way off five years off and
this is all driven by probabilistic
modeling but bear in mind it's 150
digits I I'm not super familiar on what
everyone's doing in a low data regime
but one of my mantras is actually in
practice when all said and done all day
two is low data I'll come back to that
in a moment this is a I've got some
notebooks that I'm gonna make available
where I've fitted a deep Gaussian
process to some one two threes and fours
these are just samples by in a
two-dimensional latent space I just take
a tour going around that two-dimensional
Aiken space which is sample from a
Gaussian process so this is just work I
did like yesterday or something to show
that to check my sanity that well
actually in genuine actually mainly
wrote the code that that these samples
were working so with in this space if
moose notice interestingly how quickly
it moves from one to two there's not a
super large regime in between like you
often get with these variational auto
encoder models here's a region where
it's gone somewhere where there's not
much going on but actually this is just
showing the mean function these aren't
samples just showing the me
function okay so there's actually a
little variance around these samples as
well and the variance tends to increase
when you go into these dodgy regions so
just to show you what the Layton
visualization looks like these are the
digits here there's digit 1 nicely lined
up this is the top layer space
two-dimensional and that's going down to
a 5 dimensional space and there's some
views on the 5 dimensional space which
it projects to the right so all those
Blues here are effectively a nonlinear
Gaussian mapping from this section of
blues here so they have broadly the same
structure with some sort of sub cluster
that's probably the ones that go in the
opposite direction but they are
distributed across the 5 dimensional
space now I want to leave some time for
questions so I'm going to rush through
the bit I have on uncertainty
quantification which I think is really
important well pause at some moments
I'll show you a notebook you can use to
play with but but this has always been
my vision like for a number of years now
this is the sort of thing we should be
trying to do so it's idle deep health
and what it's trying to show is that if
we really want to do the whole help the
whole individual we need something much
more complex and rich than these simple
probability trafficking models that only
interact with certain levels and we need
to have these things say environment
epigenomics genomics gene expression
social network biopsy x-ray treatment
clinical measurements survival analysis
and within that framework you can
envisage pulling all this together like
a jigsaw because it's like these very
flexible low dimensional latent variable
models that can be stuck together and
just to sort of illustrate that you know
other people are thinking about this
type of thing
this year's niche there's Gaussian
process nonlinear latent structural
discovery multivariate spike train data
that's what all that's a paper this year
that paper is one layer GPL VM with
Poisson noise on the output doubly
stochastic variational inference for
deep Gaussian processes by Sun in vein
Ian Dyson rock that's a paper that
allows you to do stochastic variational
inference for regression deep Gaussian
processes and I think they've done up to
a billion data points um I'm very very
excited about this I haven't read it in
detail but this is exactly the sort of
thing I want
cdeep multitask outing processes for
survival analysis with competing risks
bala and van - ah and this last paper is
I see it's a talk I'm not sure if the
others and some of them spotlights the
others at least maybe even talks
counterfactual Gaussian processes for
reliable decision-making on water
freezing reasoning Solomon sorry now I
want I particularly like because it's
actually about single Gaussian processes
but it's about understanding how to deal
with treatments in the clinical
environment and causal modeling in that
domain and but it shows how a rich
probabilistic model can be used with a
nice framework like counterfactual
reasoning to really make practical
judgments in the real world and you know
my kind of hope is that these deeper
gaussian process models they will
hopefully if we can fit them at the
scale we want they can pull that
complexity of model together and do
things like causal reasoning and the
other thing we've got a archive paper
where we put differential privacy on a
Gaussian process and of course that's
with all this in mind basically anything
people can do on a Gaussian process you
can then put into these frameworks and
differential privacy is something very
important if you're sort of if health is
important so other works that I kind of
find interesting deep survival analysis
Ranganath at r which also is i think
thinking along similar lines but that's
a not an old one something that i
haven't talked about at all today is
that we can do all this for recurrent
neural network type structures all the
same tricks apply and we had a paper ICL
are good heavens two years ago where we
were showing how that's done and that's
closely related to what people would
call gas and state-space models and
there's work on Gaussian state-space
models at this year's conference and
then i want to shout out to my student
Alan Alan Saul who's thesis on Gaussian
process approaches based approach tips
for survival analysis is also looking a
lot of these challenges around how you
do survival analysis in this context we
survived Alaska sits very difficult
because the information is very sparse
ok so that's three minutes before the
questions right
okay cool so I just want to wish through
in the last three minutes before the
questions that why I care about this so
um
multi fidelity modeling I mean deep nets
uh so they're very powerful approach to
images speech and language believe it or
not that's not the only challenges we
have my own approach is that you you can
also look at these with DEET Gaussian
processes that's fine I would suggest
that you know I always think if I ever
were to play I don't know Real Madrid
I wouldn't say I played the same
position as Ronaldo I would say I played
goalkeeper or something you know
don't try and compete with other models
where they're already clearly strong and
those models are very strong in these
very large data where you don't need the
uncertainty so much so you can certainly
augment them with Gaussian processes and
we've done that others have done that
but actually you should look at other
things so I think one of the most
interesting areas is um uncertainty
quantification and this don't look at my
spelling it's all terrible um and it's
related to other areas like probablistic
numerix surrogate modeling emulation all
these areas are similar and they're very
much about the way the real world
interfaces with the simulated world and
choices you make decisions you make
about should you fire another simulation
off she do you acquire data in the real
world should you take an action all
these things need to be driven by
uncertainty in the general area of
uncertainty quantification you cue
basically stipulate sir for me I'm not a
fan of AI as a term in fact I really
dislike it I'm constantly now being
asked to talk about why things I do like
humans when they're clearly not but
there's loads of field where people have
been thinking about these challenges in
particularly UQ if you read like what it
says about UQ uncertainty quantification
is the science of quantitative
characterization and reduction of
uncertainties in both computation in
real-world applications it tries to
determine how likely certain outcomes
are if some aspects of the system are
not exactly known of course you could
say we're trying to be brains and you
might get more people coming to your
conferences but these guys is filled
with applied mathematicians and
statisticians and very other things and
for me like at Amazon this is a vital
tool where you're interacting between
the physical and the virtual world
Amazon to me is it's the only place
where
you can imagine any advance in AI and it
will immediately affect the bottom line
of the company
there's applications for anything you
invent like today like now because it
has this massive physical infrastructure
and a massive virtual infrastructure and
so for me uncertainty quantification is
very important in that space another
example from former life consulting
designing like an f1 car you need to do
computational fluid dynamics you need to
do wind tunnel you need to do track
testing how do you combine them to make
a decision about whether the
modification you've made your car is
making the thing go faster how do you
choose to do which to do next
particularly when you're on the limited
budget as they are so they restrict you
how much CFD they do they restricted how
much wind tunnel they do and they're
very restricted and how much track
testing they do so these are the sort of
questions that you know I don't know
what a is
but decision making under the presence
of uncertainty when you've got
constraints seems to me to be something
that we should be looking at and broadly
speaking that's covered by the domain of
uncertainty quantification so we've done
a little bit of work on that um with
collaborators at Brown and MIT and I
think that's a really exciting area and
the way we do it was we combine these
different facets of data together
different fidelities and multi fidelity
optimization through deep Guardian
processes there's a notebook online that
I don't really have time to show you I'm
getting a zero from down in the benches
so I'm gonna stop there and take
questions but just as I say I was very
lucky that a number of people including
my wider research group particularly
these people have put a lot of help into
feedback on the presentation and
preparing datasets and the main
conclusion is what's stopping these
things going forward well if you compare
two neural networks we just don't have
the quality and size of code bases that
allow us to fulfill the promise and
basically it requires a lot of
algorithmic understanding to deploy
these and it shouldn't be like that if
the promises that I can just compose
models in these various ways we need to
have the power and flexibility of the
neural network frameworks so that people
can do that in the comfort of their own
homes without sort of core
you up and saying can you fix my
algorithm so I focus has been a Gaussian
process since I'm being driven by M X
net the neural network training
framework and we're putting a lot of
effort into scaling things up and making
them work well for a composition of GPS
with neural networks and other methods
all right so don't everybody walk away
we have some time for questions so
please go to the microphones right there
and there so I have six questions and I
cannot believe that 7,000 other people
can not come up with one question
there's a question there thank you very
much thank you so much for interesting
talk
one battle next the conventions in
applying GPS to large data sets is
representing the kernel matrix and the
suggestion that you showed in the talk
that worked very well was like a tractor
points and low rent approximations and
the work that I'm work that I'm
presenting at nips with Yi Jing and
Rishi Condor proposes a multi-resolution
approximation to large matrices like
this it seems to be a tension in growing
datasets of whether you should make a
low-rank assumption or some more
complicated assumption about the
structure of the data I wonder what your
thoughts are in applications you've seen
and where you see that heading so um
yeah there are other approaches and my
main thought is the biggest problem we
face is that we all make these
approximation methods try them on our
five datasets and they don't get tested
in the real world so if we could just
get them out in software so that people
could really find what works in practice
because we're so busy doing new methods
we often aren't deploying them and we
need to bridge that gap but there's many
approximation methods as most possible
it's great thank you so much for the
nice talk
there's something I couldn't understand
it's about learning and coming from like
the neural network community that where
I need to have an online adaptive
setting I was thinking of whether this
framework could do something about
control theory applications such as like
tracking do you have a learning rule how
would you that does this a deep GP
learning rule yeah yeah so you can
compute gradients and the learning rules
vary depending on what approach you use
one of the most promising is this thing
called stochastic variational inference
where you combine variation
approximations with stochastic gradient
descent it's an idea
from David Bly and Matt Hoffman and
others and that seems to work very well
we got some processes for big data but
there's still again we just need better
software and to deploy it in more places
it's not easy to implement that's one
problem so if I have samples new samples
and online city candid deep Gaussian
process if you were online yeah so
that's another really nice paper rich
Turner has a paper with time buoy again
on continual learning with Gaussian
processes in the online setting where
you actually want to modify your model I
think that's a very very interesting
work to take a look at but yeah it
depends with you mean like I've got a
static model I'm training with a
stochastic gradient or I'm actually
deployed the model online thank you so
much hi nice talks Thanks so I have a
question about the uncertainty
quantification I agree with you or not
about we need to be always uncertain
doing uncertainty quantification because
when we want to apply machine learning
to real problems we need to know what's
the confidence in her role what's the
uncertainties there but for applies to
mathematicians they already even out of
your heads for uncertain even
quantification like the generalized the
polynomial chaos among panel based
method what kind of advantages do you
think that the mercenary
or come into this field I don't
personally like to think of as I've
grown older machine learning people and
statistics people and other people I
just like to think of problems and
solving them
so polynomial chaos expansion I don't
know much about but the other main
technique people use is Gaussian
processes and there's clearly situations
where they're limited in approximating
and we've already done stuff time show
where you use deep Gaussian processes
your surrogate model so I like working
with those guys I mean they've got a lot
to bring methods are kind of suffered or
not fun curse over that - yeah it's far
bit queue behind so there's a long queue
behind you and in fact we already
running over time so I'm gonna have one
more question and then the rest of the
people you know maybe they can come up
afterwards and ask to come here to ask
talk with Neal I know thank you for the
talk and great job recovering from the
technical difficulties I have a question
about uncertainties so you have there is
a variety of ways people are trying to
come up with uncertainties out of neural
networks garingal sees this and what you
describe in a few others what Xuan's are
better when in your opinion and what's
so um I think the question is up because
I have trouble hearing with the echo but
was it about the uncertainty methods in
neural networks yeah basically which
which other we in your opinion with are
the advantages or disadvantages or other
methods I have to say I'm really
skeptical about them I mean it's very
hard to do good Bayesian inference when
the parameters highly correlated I did
it in my PhD thesis hybrid Monte Carlo
works really well but to get the quality
of uncertainty estimates you really want
the nonparametric number one that's
really important so that error bars go
up where you haven't seen data and I
mean
you see ports and you sometimes wonder
okay is that the real part you got
because when I've had people working on
it the arrow bars don't look very good
they tend to look but the problem tends
to be that if you don't get the
correlation structure right all those
error bars are everything moving at the
outside is to do with weights moving
together and bases functions flopping
around
so though those tend to be difficult to
find I mean that match has some of the
best techniques for doing this at scale
so maybe he knows better than me but I
think we should really stop here and
think kneel again for a wonderful
tutorial</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>