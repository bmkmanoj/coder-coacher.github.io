<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind's Richard Sutton - The Long-term of AI &amp; Temporal-Difference Learning | Coder Coacher - Coaching Coders</title><meta content="DeepMind's Richard Sutton - The Long-term of AI &amp; Temporal-Difference Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind's Richard Sutton - The Long-term of AI &amp; Temporal-Difference Learning</b></h2><h5 class="post__date">2017-09-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EeMCEQa85tw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everybody thank you joy well
it's true I'm kind of the I have the
most gray hair here I've been around for
a long time in and I want to kind of
talk about the longview a bit actually
so this is the challenge its first let
me say though it's really exciting just
to to talk to all you young people
learning about the field learning about
I want to get into AI and reinforcement
learning and it's just like it's a
special opportunity I really appreciate
it and so so when I do this I have to
think what can I try to communicate to
you because you know we have really a
brief amount of time and the field is
immense written a whole book on the
subject and so what do we what what can
I try to do now my topic nominally its
days is temporal difference learning and
certainly I'm going to talk about that
but I also want to you know to speak to
you as young people entering the field
and just tell you what way I think about
it from a sort of a longer view and well
so I think the first thing to be said
along those lines is that this is a
really special time and we can look
forward now to to doing an amazing thing
which is maybe understanding the way the
mind works way intelligence works this
is this is like a monumental event not
just you know this century you know for
thousands of years maybe in the history
of the of the earth when intelligent
beings animals things that can replicate
themselves finally come to understand
the way they work long enough to by
design create intelligence and so the
big thing that's gonna happen is that
we're going to come to understand how
the mind works how intelligence can work
how it does work and just the fact of
our understanding it is going to change
the world it's gonna change obviously
there'll be lots of applications but
just it'll change ourselves our view of
ourselves what we do what we play with
what we work at everything
it's a big event it's a big event and we
have to we should keep that in mind and
as I say you know I'm not saying it's
gonna happen you know tomorrow
it's not good but you know 10 years it
could happen 30 years it could happen
and then the great sweep of history
that's a small amount of time it's gonna
happen within your lives with high
probability so so the first thing we
have to ask is why is this happening and
I think it's really comes down to like
Moore's law okay so I took a slide from
Kurzweil and this is his standard slide
of the increasing computation per dollar
and it's just reaching an a point where
it starts to compete with the
computation that we can do with our own
brains with natural systems okay that's
happening now and it's just going to
continue increasing dramatically I'm
sure you're seeing these kind of slides
here we have time years along the bottom
and on the other we have computations
per second per dollar okay and you see
it's a log scale so of course a straight
line would be exponential increase and
Kurzweil argues that it's actually
slightly super exponential but the point
is the computations become available to
us has become enormous and that changes
things it's the rise of deep learning
and all that is purely because the GPU
is in computation getting cheaper and
that will only continue and it will
become more extreme as vastly increased
computation so that alters everything
and it really is profound and I just was
trying to slide to say what I conclude
from this as I think it's an answer to
like a question for AI this had for 6070
years
okay so the answer I'm not gonna I just
gonna do a couple of slides like this
but I guess I should one one one last
bit of preface is that mice might my
talk today is gonna be like half
he's on these big things okay and like
I'm starting to do now in half like
really small things where you go back to
the foundations real stuff that's that's
basic that we need to
okay I think the details the big picture
is important but the details also really
matter and like we heard a lot of
specific algorithms early today exactly
how they work just really important the
details matter but the big picture
matters and sometimes we lose sight of
the big picture sometimes something
comes so obvious that we overlook it I
think
the computation is as long that line
stand its implication its implication in
a phrase is that methods that scale with
computation are the future of AI okay
and that's that's by scale computation I
mean as we get more computer power those
methods become more powerful and this
has this has not always been tried true
but certainly it's true of learning and
search methods these are what we call
general-purpose methods and it's the
answer to this one of the oldest
questions in AI for sixty years old
do we want weak methods that's as they
call them in the old days they call
general-purpose methods weak because
they just they just use the data or use
computation they're gonna leave their
their general-purpose their general
their weak okay that's what they calls
me the old days the strong ones were the
winds that would lose human knowledge
and human expertise and human insight to
make their systems so much better okay
now probably you guys are thinking
that's crazy the general-purpose ones
are you thinking that with me
general-purpose how many of you great to
go human insight all right some of both
okay yeah that's that's that's nice
compromising sort of position and so all
today I'm going to try it all in this
big part of the talk I'm gonna try to
talk about I'm going to present strong
views okay really maybe you should do
compromises and nuances but it's good to
talk about strong views because they
give you a working hypothesis to give
you a point of view and you know you can
say this is a strong one you don't have
to believe it you have to say well
that's the strong point of either that I
shouldn't be thinking about and then
maybe you have several of them anyway so
I'm gonna that's a strong way to either
this is all this question has been
answer
it's been imagined in favor of the weak
methods and yeah and now I don't want
you to I could talk about this all day
but I'll refrain from it I'll just note
the next thing to say is that you may
you're thinking you're good you're
thinking you're into deep learning or
reinforcement learning so you're on the
right side of history but I'm not sure
that's right
okay because we take things like any
kind of supervised learning even
model-free reinforcing the thing I love
more than anything right it's only
weakly scaleable if I had you know a
million times its computation I'm still
limited in my model for reinforcement
about how fast I can gather data and you
know I'm only learning a value function
or maybe a policy what's the big deal
that's a tiny object and some map from
States to what to do okay and that's not
a big thing and okay how how many
features you have it's yeah and super
certainly for supervised learning is
only weakly scalable because it requires
people to provide datasets
okay and there they become the
bottleneck if you have you know you get
a get people to label things on the web
things scale but only weekly only as
faster you can gather to the data and
unless eventually that becomes a
bottleneck you really want to be able to
learn from raw data you want to be
scalable okay so if these things that we
love they're not scalable what is
scalable what would be fully scalable
well my answer is simple it's what I
call prediction learning prediction
learning means learning to predict what
will happen okay and so it's the
unsupervised supervised learning because
we have targets where supervised
learning we have we we just wait and we
see what does happen and that's our
target okay and so you don't need to
have human labeling or you have a target
but you don't need a human to provide it
just get it from the data so you see on
super high supervised learning
but anyway it's it's definitely the
scalable
model free learning and maybe it's it's
the scalable model free learning and
prediction learning is at the heart of
all of our control methods where you
learn value functions I think maybe
that's sort of the argument I wanted the
big argument about temporal difference
learning is that it's the scalable model
free method and of course I haven't
given you even the step towards that yet
I'm just saying that the idea of
predicting predicting having learning
that predicts is the key problem it's
what we should be thinking about and you
may say well deep learning supervised
learning is all about prediction
learning but that's not true it's all
it's about predicting what the other
what the label will be but it's not
about predicting over time it's not
about predicting where you have to wait
and see what happens
and that makes all the difference okay
so so with all this words I want to
ground us a little bit remind us what
the data what the data looks like so I
put in the slide of this is what real
life is like real life is a temporal
stream we have things like playing
soccer and we have to make actions at
every moment we're maybe a hyena being
chased by lion and trying to predict
whether it's gonna live or die we're
maybe a baseball player and we're where
our eyes are watching this tiny little
ball flash by us really fast I have to
swing or just the right moment to hit
the ball or maybe you're talking to
someone you're trying to predict what
they will do so this is the data streams
that AI should be learning to deal with
so we should always keep this in mind
when I say learning to predict well I
think I didn't think of that like a
hyena trying to predict you know fear
fear is your is your prediction of are
you gonna die okay so he's trying to
predict it several times it looks good
and bad and and at the end here it's not
looking so good
okay so those premises
let me start talking about temporal
difference learning temporal difference
learning temporal difference learning
it's a method for learning to predict
it's widely used in reinforcement
learning to predict future award value
functions and it's basically the center
the core of many methods you know about
cue learning and sarsa TV lambda deep
cue networks TV gammon the world
champion backgammon player using deep
reinforcement learning from 15 no 25
years ago my god 25 years ago deep
reinforcement learning 1992 but not all
reinforcement method so alphago it
happens not to use TV learning uses
other kinds of reinforcement learning
the helicopter thing that you might have
heard that doesn't use it and then
there's sort of pure light like Peters
talk was interesting because he talked
about policy based methods and some of
those don't use temporal different slang
but eventually he would get to it and
put it in to make things better
so it's sort of
yeah yeah so it's sort of optional it
feels like it's optional many people say
it's optional but I do want to argue
that you do want to use it that really
you should always be using it it's
ubiquitous and okay it seems you how the
brain works what else today oh now it
can be used to predict anything it's for
general method for prediction learning
not just for rewards okay so don't be
fooled by and reinforce something you
read all about TV it's all about
predicting value functions reward but we
can use it for anything and since my
talk is just about TD I want to be sure
to to think about this general use
question yeah I forgotten exactly the
details the initial learning system was
learned from from actual outcomes actual
games and they would go all the way to
the final outcome if they make a
prediction look at a position make a
prediction
how am I gonna win or lose and they
would go ahead and see who actually won
the game at the end they would use well
who actually want instead of a temporal
difference learner so that's what we
want to work for do do they wait and see
who actually won do they see the outcome
or the return or do they do the updated
guess from a guess okay let me make that
clear so that's my next slide is the TD
learning what is TD learning well
basically it's learning a prediction
from another later prediction okay so if
alphago was looking at position make a
prediction and I'm zooming all the end
of the games he won and then it's not TD
it's not learning that prediction from a
prediction on the other hand if you make
a prediction from a position and then
you make one move and you see your next
prediction and you use that next
prediction as to form a target then
you're doing TD learning okay so the
quick word quick phrase is we are
learning a guess from a guess okay
sounds a bit dangerous doesn't it
how it would constrain it what would tie
it down okay but that is the idea we
want to learn an estimate from an
estimate and we have to talk about
whether this is good or not okay the TV
air the TV air is the difference between
two predictions to temporally successive
predictions right so if you're playing
your game you say I think I'm winning
then you take another move you know now
I think I'm losing you try to learn from
that you don't know who actually is
gonna win not yet what you try to learn
from the temporal difference in your
predictions okay now after that it's the
same as what you're all used to you just
have an error and you send it back
proper through or whatever you're doing
and so it's just really where does the
error come from or where does the target
come from is the target come from the
end of the game or does the target come
from the next prediction okay so here's
the the example of TD gammon originally
1992 and he had a chest a backgammon
position and he would send that into a
neural network which would filter
through actually just a single well he
had many versions the standard version
was a single hidden layer and he'd end
up with this probability of winning and
the error then was the probably winning
in one position - the probably winning
in the next position so we look at the
change in the estimated probability of
winning and that was used as the error
that would be back propagated through it
and this would learn just stick it in a
corner playing against itself learning
from itself from its own trial and error
and it came out to be competitive with
the world's best players really the best
in the world okay so that's familiar now
but well I'm trying to get to this
question those questions do you need to
use TV learning because this all this is
is to motivate the motivation is the
most important do I need to use T learn
here can I get away with it because you
go to the field now maybe even in
reinforcement Lane you'll find a good
fraction of the people don't believe in
T dealing and I think they can get away
without it okay and so it's a real
we should all be asking do we need it I
want you to understand I want you even
as people learning about the field to to
be able to engage with this question and
and know the basic facts pertinent to
whether we should we need to use TD
learning okay so I will skip over over
Atari and as you've seen that already so
TD learning Wendy when do we need it
okay well it's only relevant and what
Icahn multi-step prediction problems
that's the first thing
only when the thing predicted is
multiple steps in the future so
obviously if you're predicting the
outcome of a game that's multiple steps
in the future but if you're predicting a
label or or in alphago where the data
was the initial data at least was here's
a guess and then I'll just see who won
the game and if you just use who won the
game then it's essentially a one step
prediction okay okay so now I want to
say that it's really broadly applicable
you should I said it's only applicable
when you have multi step predictions but
really everything you want to do is
going to be multi step prediction if you
want to predict oh I have some examples
multi step prediction if you want a
predicting outcome a game you want to
predict what a stock market index will
be well really you could just predict
what it's going to be but you get more
data on every day after that and you'll
mean like new prediction so you make a
mole you like repeated predictions about
a long-term outcome if you want to
predict will be the next president you
can predict that you know every every
day as a new event happens if you want
to predict who the US will go to war
against next so ll these are long-term
predictions they don't they don't jump
to the end now even even if you want to
predict a sensory observation if you
want to pretty just the very next
sensory observation that would not be
multi-step prediction but if but but if
you were to predict you know even ten
steps ahead or a discounted measure of
the future those are long term or
multi-step predictions
and yeah we think about go back to the
real world the the the hyena and the and
the Lions or the the conversation or
Messi playing soccer he's got to make
long-term predictions it's not you know
what's the next thing it's am I gonna
make this goal will I get around that
fellow where's the ball going to be in a
few milliseconds from now all these
things are multi-step predictions now
can we can we treat it can we just use
our one-step methods like you know sure
things happen bit by bit but ignore that
just wait and see what happens
you know like see you on the game and
use our one-step methods and or can you
learn a one-step model and then compose
your model okay and the answer is that
we really can't do these things and I
want to try to give some sense of that
today and I'm really going to talk
mainly about this first question can we
think of the multi-step case as one big
step or do we have to deal with it bit
by bit and but I wanted a one slide on
the second bit the second bit is can we
learn one-step predictions like a model
and then iterate them to get a multi
step prediction when you need to and I
just I just want to say that I think
it's a trap okay
and I know I could really properly
explain this to you but I think it's a
trapped I think that it's enough to
model the world to make it like a
low-level simulation of the world to
make like a think tray to throw is a
Markov decision process and model the
transition probabilities or treat the
world as a as a as a engineering model
where we just have to learn the
velocities and the accelerate and the
effectively on the accelerations or
actions and then integrate this
low-level differential equation Lisa
this is all a trap these short-term
models and then iteration is it feels
good because we know we know that if it
can be done perfectly on the one step
then it can be done perfectly however
far we want to look into the future but
there's two problems first of all we
can't do it perfectly and when we do it
imperfectly with it because we're always
gonna have an approximation then when we
try to iterate them we we get a
propagation of errors and compounding of
our errors and we get a useless long
term prediction
and secondly of course it's
exponentially complex because as we look
ahead each step there be many
possibilities the world is stochastic
and also our actions may be we have
we're different choices we might look at
for actions so it quickly becomes
computationally intractable it will
always be computationally intractable to
try to look ahead many small steps into
the future I like to try to try to
iterate your model of physics to get to
you know how much fun will I have going
to Montreal and taking the summer school
okay it's crazy and it just doesn't if
there's no future in that that way of
thinking it's a trap and lots of people
are in my opinion are falling into it
okay but let's go back to the other side
or the other side remember I'm just
going backwards here we have these two
things two ways to get away from TD if
we don't mind get away we don't like TD
can't we learn a model and iterate it
that's the second one or the first time
can we think of it as a one step thing
and just use do the one step thing okay
so the one step thing I'll do it I'm
gonna do it in I'm about to transition
to my low level part of the talk but I
don't want to try to answer it here just
at the high level and then maybe we can
even take questions after this this high
level part of the talk can't we just use
our familiar one step supervised
learning method and and reinforce
something these are known as Monte Carlo
methods I mean it's just roll it out or
whatever see what happens and use that
what happens is I've target okay so this
has costs number one Costas is you have
to you make you're making this
prediction then you're rolling it out to
the end and really you're gonna make a
prediction in every moment in time so
you've got to remember all whole mess of
predictions as you as you go out to the
end to see the outcome or really uh if
you have returns you get the outcomes at
different different steps different
outcomes you have to relate them back to
the to the earlier situations it's
horribly complex it's nasty okay it's
first of all you need to remember all
the things you did think about yourself
maybe you're the lion there you're
trying to make a good prediction okay
and what do you have you have all the
stuff swirling around you
I had the hyenas running away you have a
glimpse of him your your feeling of your
feet you have all the stuff that you can
sense and and then you want to relate
that to how well it's going how how good
you should feel about this chase okay
and so what is it sensible to think or
just be much better if you can do it now
right now we know all the stuff is in
your mind and in your sensors learn if
it's going well or going poorly and
learn now as opposed to wait wait five
seconds later when you guys have you
know a whole number a large number of
frames of different different sensations
and different patterns of sensation
you've forgotten you know but if you
have to wait five seconds till the end
of it it's too late you can't remember
all that whatever you remembered will be
a tiny shadow of the real vivid
representation you had at the time it
happened okay
and of course the computation is is
poorly distributed you can't learn now
so what are you doing now ladies find an
outcome you have to do all the learning
then just a poor temporal distribution
and you can avoid these problems with
special methods that's what really what
TV is about specialized methods for the
multi-step case and another reason that
you don't want to wait is that sometimes
you never know the target like I don't
know let's say you're playing your chess
game and there's a fire alarm and you
never finish your game so you never see
a final outcome but if you're TD you
know maybe thought you were winning and
then you know it was going really poorly
and you say you did something bad you
can learn without waiting until your
checkmated maybe the fire alarm just for
like one move away from the check game
so technically the game never ended and
but you can learn a lot from your
experience so obvious we could try to
ignore all these things think of them as
nuisances but I think of them as clues
these are hints from nature but how we
should proceed okay okay so
now now I'm going to get down to it more
technically but I hope you're starting
to see the view I'm trying to present we
really need to learn from new
predictions so that we can do it as we
go along and I think it's really
ubiquitous and kind of and all the
different kinds of learning we're
looking at to nai okay so I'm gonna use
notations a little bit different than
what we've heard earlier today that's
what I call my my new notation and I use
it in the second dition of the
reinforcement learning textbook the big
thing is that we're trying to use the
students status to Texas statisticians
convention that random variables are
capital letters and instances are
lowercase letters so all of the things
that happen that make up life are
capital letters Azir the random events
that have actually happened they're not
possibilities they're whatever happens
so s0 is the first state a 0 is the
first action taken in that state are one
is the first reward that depends on that
state and taking that action in that
state and then as the same time as we
get the reward we get the new state so I
I like to give them the same temporal
index r1 + s1 they occur together
they're jointly determined in fact okay
and then life goes on on and that's the
data that's all we have terms of data we
have a trajectories and maybe a single
trajectory and we're interested in
classically in the return and the return
is the sum of rewards and I'm not gonna
use capital R for the return because
capital R is actually the actual rewards
the sequence of rewards and so I'm going
to find the return I need a new letter
I'm calling a G capital G because it's a
random event the R and whatever some of
future of rewards after time T actually
was we're going to call that G of T G of
T is the return and as we note here if I
can use this thing this dot just means
that this is a definition
it's not a a statement of something
that's true because it follows from
other things that I've said it's a
definition so G of T the return is the
sum of the future Awards with the
discount rate is most common way of
dealing with it and that of course can
be written now really as equality this
is not a definition that equals the
first reward plus gamma times the sum of
all the later rewards and we're just
taking one factor of gamma out of all of
these guys so this is no no no again as
this one is one guy right out of this
guy's - gammas we just take the one out
and then the rest of this is the psalm
of future rewards from from from one
time step later okay so it's a bit like
what we started with it's just like the
same thing it's G of T plus one okay
this is just a definite this is this is
this is this is a true equality or any
return be written as the first reward
plus gamma times the next return okay
and that is gonna be the basis for our
temporal difference learning because
we're going to race T gonna use it this
is um we're gonna use this as a target
we can use the reward extra ward post
gamma times the next return essentially
as a target I guess that's gonna be
explained right right next we look at a
state value function okay since we're
using capital letters for the random
variables I can't use capital V for my
for the true value of the function okay
it's gotta be lowercase and it's the
difference function of the policy so V
PI of s where s is any particular state
it's an instance any state lowercase s
its value is the expectation of the
random variable to return if we started
in in step state s okay so what we can
expect the return can be under PI pi as
the policy so it's some way of picking
the actions of course the value of state
depends what you do if you dance at the
top of the Grand Canyon it might be bad
but if you consider a walk up to the
railings it's good so policies
Val you found value is depend on
policies okay so then you know this is
the return that we can we can just use
the above equation the return can be
written as the first reward plus gamma
times the the rest of the return and
then it's a retained expectation of this
this is the expected next reward and
expected value of the next state okay so
that naturally leads to it the notion of
an error we can use we can compare the
the estimate estimated value of a state
at some time to the reward and the
estimated value of the next state okay
so that's gonna be our TD error this is
what we're gonna use to replace the
normal conventional error now this V
missed is is a random variable this is
our estimate their estimate will depend
on what happens
and so that is random the bat the
estimated values are a random value or a
random function and so it's capital and
that's our two-year you got it a TDR was
that clear
good okay so now let's talk about our
methods I want to contrast supervised
learning and remember I said supervised
learning is called Monte Carlo in this
context so what exactly is that that's
we take our estimated value function for
the state that we run into V of s of T
okay we're going to update it based upon
some experience okay here's the
experience here we are des of T and this
is the tree of those things that might
happen like we might pick either these
two actions and if we did pick this
action either these two states might
arise so yeah black dots are actions the
open dots are our states so basically
this is this is the tree of all the
futures that might happen and in this
case we're imagining that there are
terminal States
we're basically adding things up until
we reach a terminal stage so here is a
particular trajectory that might happen
let's say it did happen from that state
we would this this this this this and
then we terminated okay so we now once
we've terminated we know what G is you
know
turn is and we can do this update rule
we can compare our estimate for the
state at time t you up here to the
actual return and we make that that
error and then we between us an
increment I should this is the step size
alpha is a number like 0.1 so we
increment towards towards this target
okay that's that's a standard would be a
Monte Carlo learning roll a supervised
learning roll and that's the the
competition for the TD method the
simplest TV method looks instead like
this we only look ahead one step we're
at s of T we look ahead at one we see
the the reward that happens and the next
state that happens and based on those we
form this TD error which is again is
comparing comparing or updating this
estimate of this for the state at time T
so we're gonna make an error between we
we're guessing for that state and this
new target the reward plus gamma times
the estimated value of the next state
okay now you've probably also heard
about dynamic programming you can think
you can put dynamic programming in the
same figure the same kind of figure and
if you were the dynamic programming
version looks like this because it's not
considering a single line through the
possible tree it's considering all
possibilities it's considering both
actions and both possible next States so
this where you need the model the world
because although you you know you're
probably picking each each action but
probably that the world will give you
possible next States will be known only
to the world but in dynamic programming
you assume you know all that so I never
programming the equation is that the
value the estimated value for a state is
move towards the expectation of the
first reward and the expectation of
gamma times the value of the next state
okay so there's this expectation and
that's what makes it dynamic programming
because you've seen me know all the
probabilities you can figure out that
expectation doesn't give you the answer
because your value will still be you're
still learning and guess from a guess
learning your new estimate
still from your old estimate my that's
sign number for you so so really we can
say the following though it's special
about T methods to say bootstrap and
sample so bootstrapping is this idea
that your target involves in a guess and
an existing prediction
okay so Monte Carlo and Monte Carlo or
the whole point is that it doesn't
bootstrap it's just looking all the way
to the end and seeing what the return is
there's no there's no estimates playing
a role in the return dynamic programming
also bootstraps and they're boring says
look ahead one step and look at the
expected value of the next state and
back it up so you're only you're using
your estimates and your estimates
gradually get better TD of course also
is using your estimate yeah it's like
Monte Carlo and TD are learning methods
I guess that's what my next point the
learning methods Monte Carlo and TD they
sample they sample what happens because
you don't know how the world works and
what dynamic programming does not sample
it just uses the expectation it assumes
you know what will happen what could
happen so those are the two basic
dimensions
whether you're sampling and therefore
learning and whether you are
bootstrapping
you're using your your your
bootstrapping your estimate from other
estimates your learning guesses from
guesses and so TV prediction
basically I'm just saying this is this
is the update you saw before Rupp doing
the Monte Carlo is here and the TV is
there so just the contrast is that one
the target is the actual return and the
others the target is this sort of one
step estimate of what their return will
be okay now let's think let's do an
example so here I am I'm coming home
after working a hard day at the office
and I'm trying to guess how long it will
take me to get home okay so I'm I'm
leaving my office it's Friday at six
o'clock I have some other features and I
make a guess of how long it will take so
I will I'm gonna guess it'll take 30
minutes to get home okay
so and that's that's my prediction of my
total time because I haven't gone you
know I'm just starting now so my elapsed
time is zero now as I come out of the I
come out of my building go to the
parking lot and I see it's raining okay
and it's raining you know it's gonna
take me longer because everyone drives
slower in the rain so I think well first
of all it's already I've already spent
five minutes just getting down to my
office into the parking lot and but I
also think it's gonna take me longer I'm
gonna think it took me thirty five
minutes from now for a total of 40
minutes okay so I what I want you to see
the first thing you ought to see is that
my guess by how long it's gonna take me
and my guests enough total time to go
home it's constantly changing as I get
more information I revise my estimates
okay so to carry the example through I
get I get I I start getting my car I
Drive on the highway it turns out I
didn't take it didn't take so as long as
I thought I've spent 20 minutes total
now and I think it'll only take me 15
more to go home and we're so bad between
the rain and so that's 35 minutes total
now this keep this car in this column is
my total estimate as it goes up and down
and then I get stuck behind a truck on
secondary road and so I think it's gonna
take me longer and then I reached my
home screen I think I'll take me 43
minutes and it does take me 43 minutes
okay so that's a possible thing that
might happen a possible trajectory and
what I want you to ask is what you might
learn from that okay so if you're doing
a Monte Carlo methods you just say well
it took me 43 minutes to get home that's
the answer so all my estimates my first
initial estimate of 30 minutes that's
going to be moved towards 43 minutes
that's the error in fact all of these
will be moved up towards 43 minutes
because whatever guess I made at each
point in time it should be moved towards
what actually happened or whatever
whatever is remaining in the future at
that point okay now if you're using a TV
method if you're using your your guests
you can learn a guess from a guest
that's something very different happens
so even some of the signs change so your
first prediction will move up because
you start out 30 and then after you
found out it's raining so you'll move up
but this one for example will move down
and the actual that although all the
errors are different all the errors are
different and the long long long rounded
wall washout but for all actual learning
is a law is it's very different okay now
I also want you to think about the
computational consequences okay if
you're doing TD then you know when
you're here and you go to the next stage
you get an error and you can a can
update right away you can say well why
did I make that prediction what are my
features their house and how shall I
change those what are the contents of my
deep network that led me to make that
prediction I need to change those and
and that's true at each step and you so
when you go from here to here to here
you can update this guy and then you can
forget about it but you're never gonna
update him again whereas in Monte Carlo
you have to remember why you made each
one of these predictions until you get
to the end then you have to go back and
say well okay why did I make that line
and then and then adjust its weights you
know it with knowledge of its feature
vector and content
your network and and so on yeah and it's
terrible its distribution because you
keep all this time you're doing nothing
you're driving home you can't do any
learning okay you can only wait till the
end you know the answer and then you can
go back and add to all the earlier
things and learn them so the
distribution of computation is poor the
memory is poor it's it's it's just kind
of inconvenient it's much more
convenient if you do just go along and
you think about it you're in your car
you're trying to drive home you get
stuck behind a truck do you say you say
you say this is bad you know I say it's
gonna take me longer than I thought I
was too optimistic before you don't say
well you know maybe this truck will
disappear and you don't say hold the
whole judgment you could hold judgment
until you get home but you know my
feeling is I'm learning as I go along
and I'm responding to what I see and we
actually do learn as we go along okay so
I think I've said these things and TD
with Monte Carlo you can be fully
incremental learning as you go along you
can learn before you know the final
outcome this means you need less memory
and less peak computation you don't do
it all at the end you can even learn if
you know if you never find out how long
it takes you takes you to actually go
home you know maybe you're get a phone
call and you're you called away for
something important and you never find
out but you can learn without knowing
the final the final outcome now when you
do the math both of these methods will
converge and but so the only question is
which is faster okay this is the only
question but it's a big question okay so
I don't know let's just do a simple
experiment to find find out okay so
here's a trivial experiment famous not
famous I know I did this a long time ago
just a random walk and it's meant to
test to idea which one of these is
better so we're gonna we're gonna have
five states and we're just gonna have an
estimate for each state of what the
outcome will be okay this random walk it
takes 50 step right and left and you
start in the middle and you
back and forth back and forth back and
forth until you and at one side okay if
you enter this side you get a zero and
you do get zeros all along the way for
your reward but if no way you get a
nonzero is if you end on the right side
you get a you get a reward of one okay
so are you with me what's the correct
prediction so the correct prediction
there's there's no discounting here so
we're just trying to predict the sum of
the rewards up until the end what's the
correct prediction for the start state
see you're in C what's the correct
prediction for the expected value of
your return yeah the squared gamma is 1
so the expected expected return so if
you if you end if you go blah blah blah
blah blah and you end on this side the
return is 1 if you and on the other side
the return has to be 0 now you start in
the middle what do we expect the return
to be you know by symmetry it's gonna be
like 0.5 okay and state B I don't know
it's gonna be less than 0.5 and state a
still less anyone want to guess what
they're good at what they are the true
values of all the states C is definitely
a half what do you think B is guess just
guess 1/3 yeah I thought it is 1/3 and
and the next one is 1/6 yeah these just
go up by six one six two six three six
four six five six and those are plotted
here this Lin line is supposed to be the
true values so a state a true that has a
true value that's one six and state B
it's a true value this one-third this
has a true value of 1/2 and so forth and
these other lines are the estimated
values from from applying TD to it
okay so TD you do have to care about the
initial conditions because it's making a
guest from a guest right so your your
guess is you know effect things
they either pollute things or or
brilliantly provide good guesses and
value okay they so the initial guess is
zero so our time at episode zero all of
the estimated values actually excuse me
yes all the estimated values are half
because since there could be zeros and
ones for possible returns it seemed
reasonable to start the estimated I is
all at a half we've got happens to be
right for the middle state but it's
quite a bit wrong for the other states
and then we gonna do episodes where you
and we're gonna learn it on every time
step and we're gonna update the states
according to the TV rule and after one
episode we have this this darker line
this is the episode number after one
episode we have these values for our
estimated values of the five states
right so what do you know happened on
the first episode you ended on this side
because well what's gonna have what does
this TV rule gonna do what is what is
the TV air gonna do let's say that we
start in the middle and we could go
either way we move around well what's
the TV air going to be it's gonna be the
reward the reward beginnings can be 0
and then it's gammas ones and forget
about gamma and then we just basically
the change in the value okay and if you
went from say this state to this state
what's the change in value 0 cuz they're
all their estimated values are all a
half and so we went from one state from
an estimate of a half to another state
whose estimate of a half so as we go all
this bouncing all around nothing is
gonna happen really until we run into
one of the ends well we ran we can run
into this end or we can run into that
end if we ran into this end we'll go
from a state that was a half
- oh the terminal state thermal state
always by definition has a value of zero
okay so so over here if you way if you
did this transition you get a reward of
one a reward of one the starting state
sonic state here would be a half and you
get this thing that has zero okay so
it'll be 1 minus 1/2 it'll be positive
1/2
anyway the the estimate of state a would
go up and that didn't happen because
here's the estimate of state a it's
still at 1/2 instead what happened is we
ended on this side we went from here
that had estimated value half to this
thing which has estimated value 0
terminal state and so we went from 1/2
to 0 and our ITT air is minus 1/2 so we
moved down from 1/2 toward 0 and you can
actually see how far removed we actually
moved by from 1/2 to 0.45 and so our
step size alpha was 10th was 110 okay we
can understand this algorithm is very
simple and and then as you get more
episodes you get closer and closer to
the true values after 10 episodes get
the blue line after 10 hundred episodes
you get close to the true values you
never get exactly to the true values
because there's always randomness in the
individual episode and and alpha is
nonzero it's 1/10 and so you keep it
bumping around bubbling around the true
values so that's that's an example now
let's compare now Monte Carlo versus TD
on this problem ok and we have to draw
whole learning curves now and we have to
worry about what's the value of the step
size ok so what I'm showing you and this
is a learning curve meaning the x-axis
is time or episode number and the y-axis
is some measure of error that's actually
the root mean square error averaged over
the five states and many many iterations
of the whole
I guess 100 iterations of a whole
experiment okay so as I said they're
going down you know everything's getting
better over time but they things will
not go to zero because well we have the
step size 1/10 and or whatever our step
size is it's it's always going to be
there and we're always gonna have some
residual error I don't know which one
should we look at first maybe Monte
Carlo it's simpler we just Monte Carlo
you just wait till these you know what
the final return is then you do your
update for all the all the states that
were visited so take a nice slow
learning rate 100th we just gradually
moved down towards zero error and it's
actually this this alpha equals 103 very
slow will actually get the closest in
the long long long run to zero error but
it's very slow so you might want to go
faster if we take alpha is 150th we go
down faster but we're not we're gonna
start to bubble and what we try to try
going as fast as point oh four and we do
go the initial part is fastest but now
we're definitely bubbling and you can't
really do better there's no step size
which will do better than the ones that
are shown here if you try to go faster
you're gonna you may be a little bit
faster very beginning but you're gonna
leverage level out at a higher level
okay and for TD we see a similar pattern
but all the numbers are lower okay so
here's our the one of the the slowest
one I'm showing here is 0.05 and it goes
its slowest but it gets slower and then
other ones are faster and of course they
bubble more and they they don't get as
low in the long run okay now if we have
long someone may ask you you may some of
you may be wondering what's going on
with that TD stuff because it seems
because think they go down and they
start to come up again your
anybody wondering that anybody wondering
that yeah it's weird isn't it
and it's real it's not a bug in my
program that's the first thing to be
sure of yeah it's it has to do with the
fact that actually starting out the
estimates at 1/2 is not it's not stupid
it's actually a reasonable guess if you
started all the estimates out at
something really bad then you wouldn't
see that balance I got the balances we
go down and we seemed to bounce we come
up a little bit higher and the balance
is really interesting it has to do with
the fact that we do have some some
effect of the initial estimates in TD
and whereas we don't really it's not as
much for Monte Carlo ok so so this is
just a random walk and I've survived
been systematic about the random walk
and I don't know the big picture is that
TD is faster ok there's a balance ok
whatever but it's still much faster but
this is just one problem this is just a
random walk ok maybe it's something
special about the random walk maybe if I
did that I'm you know Atari games I
would get a more fundamental result and
I like to do simple things question
oh no they they all converge even with a
non well with a non zero if you don't
reduce the step size then you don't
expect anything to converge right they
would converge in the mean ok and all of
them will converge to a mean that
depends on the step size and the higher
step size would be higher lower step
size be lower convergence point yeah the
convergence properties are roughly the
same in both cases
I want I wanted to I want to ask now can
I say anything about the general case
not for the random walk what can I say
general case actually no I'm gonna do
I'm gonna do that in a minute but first
I'm gonna do the random walk again under
this this setting can I call batch
updating okay that's updating means we
we take some training set like a hundred
episodes or ten episodes or whatever and
we present it over and over again until
it does converge so even her finite sub
size will get complete convergence if we
repeatedly present the same training set
okay because there's no randomness and
random samples you're just putting the
same data over and over again and you
will converge the two methods TV and
Monty color will converge to two
different things and this is for
constant alpha these trigrams converge
to different things as long as your step
size is small enough it won't depend on
on your step size yeah all step sizes as
long as they're small enough so that you
don't diverge will converge the same
thing okay and they converge to
different things the two algorithms
converge to different things so we can
ask on this problem which one converges
to a better thing if we present the data
over and over again to the algorithm
okay and here's the results on the
random walk again you have we have
different numbers of different sizes our
training sets we're increasing that
along here but for in each case they say
with 50 your training set of 50 episodes
we present those 50 over and over and
over again until we converge and we
measure the the asymptotic error there
is independent now of the step size site
I can now eliminate this The effect of
the step size just to measure you know
which algorithm is giving me a better
result on this problem okay and TD is
faster and by this measure I mean we're
doing a lot more computation we have to
go to convergence we have to repeatedly
present things none of which I like
but it's getting us
insight into what were the real
difference between the two algorithms so
it's like TD is moving towards a better
place even on a single example as
suggested by the initial results and if
you if you go over and over again you
can get that okay
now this again is all random walk and
you have to ask if this is happens on
all problems so one approach would be to
do all problems and that's obviously not
satisfactory so what can you do instead
you can try to prove a theorem okay and
you can you can also try to get insight
I guess I'm gonna try to get insight
first and then we'll do the the formal
result so let's try to get insight into
us as people I want you to need you to
be the predictor imagine you were having
some experience so an imaginary nsync a
training set of these eight eight
episodes these are all very short
episodes right so most of them are
episodes like B 0 means I'm in state B
and then I get a reward of 0 and the end
of the episodes the end of the episode
okay or I see state B I get a reward of
1 and that's the end of the episode the
only non-trivial episode is this first
one where I'm in state a I get a 0 and
then I go to state B and for B I get a
reward of 0 and that's the end of the
episode ok so that's the day that you
see just these eight episodes and I want
you to tell me what's what prediction
would you make ok the first question is
what prediction would you make for state
B if you found yourself from state B
what would you guess for the expected
return ahead of you from state B say
again
three quarters I agree because what
would we do that we said well I was in
state B all eight times and six of them
ended up the one and two of them ended
up at the zero so you're gonna guess
three quarters okay okay that was easy
one what about state a state a it's
really much more uncertain you only been
in state a once and what are you gonna
ask her state a just take a moment and
think about it what are you gonna guess
the return if you find yourself again in
state a what is the estimated value well
what would you estimate the value of
state a as okay now I'm gonna say right
away that this is a question there's
multiple good answers too okay so I'd
like someone to raise their hand and
give me one give me one answer and why
it's a good answer okay how would you
you always if you always go from A to B
and the only time you've seen a we went
from A to B and B has value 75% three
quarters then a should also that sort of
makes sense what's another good answer
yeah I don't know if everyone heard that
you said you've seen a once every time
you saw it the return was zero so you
know why not predict zero okay now those
two answers those two answers are the
two are Monty of the want to Carla's
answers and TD's answers okay so we
could say zero
that's what Monte Carlo would say a
Monte Carlo just looks at what happened
I was in a once and I it was a outcome
was zero so I should predict zero Monte
Carlo now the other one the other one is
what TV predicts it's also what you
would predict and this is the gentleman
explained what was going on in his head
he was saying well I'd seen a go to be
and with and all that sort of stuff
he was building in his head this model
he was saying I'd see the only time I
seen a it went to state B by the way the
reward was zero on that transition so
let's let's guess that the that's that
happens every time and then in B B I saw
eight times and six out of the eight
went one way to a one and then stopped
and two out of the no I saw an eight
times six out of eight went to we're to
one and two out of eight went to a zero
so I'm building this in my head okay
this is like and this has a name this is
called this the maximum likelihood model
of the MDP it just just just means what
you'd get by counting say how often from
do you go from here turn those into into
probabilities okay and this is the
maximum likelihood model of the
underlying Markov process and then if
you take this model and you solve it if
this is this is the true world then the
true value is 3/4 okay and so this is
the general phenomenon of what TD does
if you present a training set over and
over again to it it it it gets the
answer that you would get if you
collected all the data I mean the
maximum likelihood model of the world
and then solve that model with dynamic
or with any method the true the true
solution if that model was was this was
the reality okay and so that's well
that's that's why TD is is can be faster
can be better because it's using this
Markov property that saying oh and I've
gotten to be I know OB is a Markov state
and whereas Monte Carlo just says I
don't care about what happened between I
ended up with the getting a zero okay so
to summarize that the prediction that
best matches the training data is the
Monte Carlo Lester best matches the
training data remember the train data
okay and if you saw you saw a once and
it ended up with a zero so you want to
match the training data the right
prediction is the value of a is 0 that
is the prediction that will best match
the data okay now of course I want to
tell you we don't want to match the data
if we talk we don't want to minimize the
mean square error on the training set
weird huh it seems like we should want
to minimize the mean square error on the
training set and that's why I've gone
through it's some length this example
with you guys so I want you to have some
intuition and why we don't want to
minimize the we mean square error on the
training set so what can I offer you if
you if I can't offer you minimizing the
mean square error on the training set
it's gonna be minimizing the mean square
error on future experience because we
don't really care about the training set
past it's print we care about the future
and so we think if we believe we have
real stakes here we would think that the
estimate the value of a is 3/4 will
actually be a better match to future
data okay we get a new experience with
say day it's probably going to be ending
a 1 3/4 chance of being
why okay so it's interesting now we have
to really distinguish between minimizing
error on the training set minimizing
error on the future these different
things and TD can be faster because it
could take advantage of the state
property and match future experience
better now even as I said that you you
may be able to get immediately a sense
of possible limitation of TV methods as
I said they're gonna take advantage of
the state properties that I know when I
get to be it doesn't matter how I got to
be but in in in real life
you don't normally have complete state
knowledge you have incomplete state
knowledge if you're anytime you're using
function approximation here we're just
using discrete States and any time using
function approximation
you're going to have imperfect knowledge
in perfect state information and so so
in the end this could be a mix it's
going to be a question which gonna win
in practice but in the end it's going to
be TD that wins in practice I'm thinking
okay in the end dad okay okay so yeah
good good time for questions
yes Novi is still predicting the some of
the other awards from that's from the
state to the end and so you remember the
example was a is followed by zero is
followed by B is followed by another
zero okay thank you for clarifying that
nation then of course in this case TV in
Swissvale
in Ebisu premiere yes depending on the
size of the representation of the so
let's let's think that through a so
let's assume that instead of having a
table lookup this is all table to come
but instead of that we have a
complicated neural network right and and
so then when we get a new error we have
to back propagate through the network we
have to do somewhat expensive
computation but it's not should we
consider that expensive I'm going to say
no because even the back propagation of
one error back propagation to the
network that complexity is is the same
it's the same order as a forward pass to
the net
so we already spent the fort we had to
make a forward pass in order to get the
prediction and so there's an equivalent
complexity to do the update so even
though it's it's a bunch of weights it's
it's it should we should consider that
cheap ok ok it's it's linear in the size
of the network good good any other
questions good ok so so I've just done
one step methods tabular methods model
free methods all these qualifiers can be
generalized but even here in this
simplest case one-step method meaning
we're looking from one step to the next
step rather than one step to five steps
ahead like in AC 3 but there we can see
the basic ideas and it's tabular tabular
is easy to think about but it all the
ideas really do
to the to the network case complicated
function approximator we've seen the
basic things is that we're gonna
bootstrap and sample combined aspect of
dynamic version like to carry Carlo
UTV methods are computationally
congenial just a little bit of work on
each step and you don't have to wait
till the end and then do a whole bunch
of work and if the world is truly Markov
then TV methods are faster that's what
we see and it has to do with the past
data versus the future data now before I
go into a summit new thing I like to
also try to summarize where we are in
terms of pictorially ok what today we've
talked about contrasting TV methods a
one step TV method which is like this is
what I this little picture is what I use
to summarize the math exactly the thing
of the table is something updating it I
and this says I go ahead one action and
one next state and I use my estimate
here to improve this guy this is like a
picture of the algorithm and the same
kind of picture for Monte Carlo as you
want to estimate improve the estimate of
this guy this value you go ahead one
state on action state action states then
I'll leave at the end and you see the
final outcome and then you back all that
up ok so and this is like a dimension
you can occupy intermediate methods you
can do two two step methods three step
methods for step methods five step man
this is like an infinite step method
where you go all the way to the end of
the episode and then and then there's
the parameter lambda you might have
heard about in TD lambda the eligibility
trace parameter it's really a
bootstrapping pattern that if determines
it's not the number of steps but it's
it's analogous to the number of steps
and always though this is really a
dimension and we can occupy any point
along this dimension okay and that's now
there's a second dimension which is are
we going to use planning okay are we
going to use knowledge of the model of
the world okay
dynamic programming dynamic programming
is this corner
it means we're still going to do one
step methods we're only dynamic we only
had one step and use your estimates at
then one step
and look ahead into the future okay
and so that's moving along the top here
you says keep keep in these short
backups one-step backups but instead of
being a sample do all the possibilities
the next dynamic programming and then
there's a fourth corner where the
analogous of Monte Carlo but with
planning is like an exhaustive search
you consider all the possibilities all
the way to the end and so we can get
these four corners as classic methods
and then we can occupy you the area in
between them and that's kind of a big
space of reinforcement learning methods
although it's it's certainly not the
whole space okay now where should we go
next I don't have time to present all
that I wanted to say mm-hmm but let me
just sort of we've done a good group
here I can almost sort of wrap up and
talk about the future from here but let
me just tell you some of the things that
we had more time we might talk about
okay first we will talk about estimating
instead of state values we toss about
estimating action values because as you
know really for control you want to do
the action values and it's not that
different you would just estimate q pi
instead of V PI and it's gonna be
lowercase because that's this is the
true value function Q PI and you would
estimate it with with an action value
estimate since it's just tabular I can
say big q of Si for the actual state and
action counter then this update this
update is essentially the the TD update
it's just on state action pairs rather
than on States right this is still a TD
error this is my original estimate this
is the estimate for the next state this
is the sarsa algorithm that's that's
quite straightforward and so the source
algorithm ends up being that rule that
update done over and over again and
and some examples q-learning q-learning
is it's almost the same the rule is you
know we're doing out there state at a
state action value the new one is the
old one plus the step size times the TV
air but the TD air is a little bit
different the TD air we you know
preparing our original estimate to mix
reward plus something of the next state
but it's the max of our possible actions
at the next state that's Q learning I
like to draw this picture this is its
picture the picture says you know I'm
updating a state action pair all the
possible estimated action values take
the max event that's this this part of
the rule right max over all possible a
and and then I back up the max to
improve the estimate of this guy at the
top that's Q learning it's a TV
algorithm with that particular target
and this is a nice example if block is a
nice example comparing sarsen hew
learning sarsa is a non-policy method Q
learning is an off policy method and we
see that actually here the y axis or the
y axis is reward for episode sarsen will
actually work out better I don't have
time to explain this example
I wouldn't I didn't want to do something
it guys didn't really understand better
to skip over it okay here's one sort of
new algorithm expect this salsa expected
star so if we look at the picture right
q-learning is taking all these possible
things you might do and you take the
best of them take the max the arc these
max and if you have if you don't have an
an arc then it means expectation okay so
an expected star saying you don't take
the best of the things you might do you
take the the expectation based on how
you would actually do them according to
your policy so here we are we're summing
over all things we might do how likely
are we to do it under our policy which
we know we know our policy we know how
likely we are like it maybe it's Epsilon
greedy and we take the expectation the
action value times our likelihood of
doing that action and we backup that
okay and that's that's arguably Atlas an
improved version of sarsa and it can
also be made an off policy version of
sarsa and there are some other novelties
you can you can do an off policy version
to expect this arson now I've used the
word off policy a couple times not
explaining it I'm sorry about that but
the off policy means that you're
learning about a policy that's different
than the one you're following okay and
on policy means you're learning about
the same policy as the one you're
following the same one that's generating
the data so the way to remember it is
that on policy is almost one policy and
in the non policy methods there is only
one policy it's a policy you're doing
its policy or anybody but very often
these want to be different like you want
to do something that is more exploratory
and you might want to learn the optimal
policy okay so if you're gonna learn the
optimal policy but you're gonna actually
get your data in an exploratory way
we're just not going to be optimal then
you have two policies okay
and and then you're in the realm of off
policy learning q-learning does this
butt off policy learning is is
theoretically more difficult and more
challenging for our methods okay that's
that's off policy okay so so I basically
just extended these things to control
okay
now and we've seen some some methods
that can do the on policy case in the
off policy case we didn't talk about
double Q learning okay so I've talked a
lot about about do you want to use Monte
Carlo or do you want to use T V okay and
it's it is there's a sense in which we
don't have to choose because use an
algorithm like if you use TD you can
parametrically very lambda or very the
the height of your backups to get to
give you any intermediate point between
one step TD and Monte Carlo you can get
both and a key mean way of doing this is
with the parameter lambda the
bootstrapping parameter so you really
talk about but it is a way to very
parametrically between TD and Monte
Carlo
so if land equals zero which is the left
side of all these graphs
that's pure TD pure bootstrapping okay
if land is one that means you're
watching any bootstrapping
its Monte Carlo okay okay so now all
these graphs have land I've gotten
across the bottom so it's basically like
this - pure - - no bootstrapping and
they all have a measure performance on
the top where in all cases lower is
better okay so it's like it's like
Mountain car and you want to have a few
steps to get to the top of the hill okay
so and what you see looking at this is
that you know performance depends on
lambda this is this is a random walk and
it's actually not best at lambda equals
zero
purity D is not the best you can do
better if you do some amount of TD
intermediate between pure TD and Monte
Carlo
but if you go all the way the land
equals equals one then things get really
bad that's like the worst case in
general and that's the pattern land
equals zero is Monte Carlo and Monte
Carlo has really high variance and it
has to be ruled out it's not very happy
if you are committed to Monte Carlo now
you couldn't do TV and say oh I can pick
any step in between now that's what you
you want to have this facility of doing
some bootstrapping and that's sort of
some evidence for that even though this
is old data I think you know like Peter
would agree that that Monte Carlo is is
not really an efficient strategy to do
it in a pure way okay now another I want
to give it one slide also for taking
questions on the linear case with a case
with the real function approximation I'm
not going to go to nonlinear networks
but I want to go to something which you
five talk so much theoretically about
which is the linear case so suppose
we're doing linear function
approximation linear function means that
our estimate estimated value is formed
as an inner product between a parameter
a weight vector and a feature vector
okay so future vectors are fee fee for
feature V of T is our feature vector for
the state at time T and the parameter
vector is theta and that might think
about as the weights of your your
network this is a linear network okay so
do we take the inner product and so this
transpose thing means the inner products
of theta inner product with fee is our
estimated value of state T if you just
sit it's just made value of the state at
time T because this is the feature
vector for the state at time T okay so
this is our estimated this is an
estimated value of time T this is our
estimated value at time T plus 1 so this
really is a TD air and this is a TV rule
the TD rule is that the parameters
are the old parameters plus step size
times RT d air and that's the gradient
in the general in general nonlinear case
this would be a gradient of the of the
prediction with respect to the
parameters in the linear case is just
the feature vector five feet excuse me
okay so that rule should be fairly
familiar to you now it's just a TV rule
using a stochastic gradient descent
it's Lots can be said about that but
that's that's the standard routine
linear TV t zero and if you look at this
you can of course write it like this you
can take the five the fee and carry it
inside here it's there and you carry it
inside here you with a little some
transpose e stuff you can write it like
that and this is a vector if you take
the expectation we're gonna take the
expectation okay so an expectation the
the new feature vector is the old one
plus a step size and this this thing
this thing in expectation what is it
okay well I'm just gonna make some names
for this thing is a vector and this
thing is a matrix times theta okay
so B is going to call that vectors that
vector B is just the expected value of
this thing it's it is a well-defined
vector you don't know it but it's it's
it's there and this thing is a matrix
because it's an outer product of feature
vector with the change in the feature
vectors so the expectation of that
matrix is what I'm going to call a so
that mean lets me write a whole
expectation like this and now I'm
interested in the K and what happens at
the fixed point where will this settle
and extreme expectation where will it
converge
where will the expected update be zero
well the expected update is basically
this part so I want to know when is that
zero well that's gonna be zero when B
equals a theta or when B minus a theta
is zero okay
and that that theta that's a special
theta
for which this is true so B minus a beta
equals zero I'm appalled at theta TV
because it's the fixed point that TV
converges to the linear TV converges to
okay and then you can just compute it
know B is is is a times theta and so you
have to take the inverse of a and you
get the TV fixed point is the a inverse
B and which is which by the way it's a
key to another algorithm least-squares
Agra mez estimate a directly even though
it's a matrix take its inverse and I'll
sestamibi directly and then multiply
them together to get that's the least
squares TV works okay but but this way
of computing what what what the
algorithm converges to and then you can
say something theoretical about it this
is your guarantee that we get that the
mean square value error measure how off
the values are is bounded by an
expansion times the the mean square
value theorem the best data so this
means that we don't find the best theta
okay but we do get an expansion of it
anyway so that's what the theory would
look like if you had more time to talk
about it I just wanted to mention
quickly some pots some of the frontiers
and things that people are working on
now so off policy prediction is a big
area people are working on trying to
generalize to the off policy case also
we'd like to talk about non has some
theory for the case of nonlinear
function approximation there's just a
little bit of that there's also a very
little convergence theory for control
methods period and I think Chava
maybe we'll talk about that tomorrow um
and we also like to say things beyond
convergence we talked like to you know
how well can you do in a finite amount
of time how fast you converge now when
you combine TD with with a deep learning
a lot of different issues come up and I
think there's just a lot of uncertainty
do we really need a replay buffer so
that one of the folk theorems is it Oh
especially you have instability and
correlation quote correlation so we need
this thing
all the replay buffer but I think it's
really there's lots of questions about
what happens when we combine TD with
deep learning and finally the idea of
predicting things other than reward
remember I started with that we might
want this is TD is a general prediction
method multi-step prediction methods we
want to use it to predict other things
and particularly we want to learn it to
use learn a model of the world so in
conclusion I guess what I want to say is
something like this at e-learning is a
uniquely important kind of learning
anyway maybe it's ubiquitous we're
always going to be using it and I think
this may be true it's a hypothesis so
anyway it's learning to predict we're
just perhaps the only scale what kind of
learning it's it's it's a kind of ring
that's specialized for general
multi-step prediction which may be the
key to perception modeling the world the
meaning of our knowledge it's key idea
is to take advantage of the state
property which can make it fast and
efficient but can also make it a
syntactically biased and so the key
claim to fame as is computationally
cheap congenial and we're only beginning
to use to explore different ways we use
it for things other than reward thank
you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>