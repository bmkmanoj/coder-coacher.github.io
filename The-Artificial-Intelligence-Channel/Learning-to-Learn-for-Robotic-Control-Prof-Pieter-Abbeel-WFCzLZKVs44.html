<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning to Learn for Robotic Control - Prof. Pieter Abbeel | Coder Coacher - Coaching Coders</title><meta content="Learning to Learn for Robotic Control - Prof. Pieter Abbeel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning to Learn for Robotic Control - Prof. Pieter Abbeel</b></h2><h5 class="post__date">2018-03-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WFCzLZKVs44" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to tell a little bit about
learning to learn for robotic control
specifically maybe a good starting point
is to look at the difference right now
between how fast reinforcement many
agents learn versus how fast humans
learn and so this picture here is from a
paper from Justin Imams lab when we see
her in black dots is human players
horizontal axis is minutes played
vertical axis is score in the game so
what we see is that after about 50
minutes of play humans are outperforming
double bqn after has played for 115
hours and often after this played for a
920 hours this is a pretty big gap here
between what's possible with current RL
and what you miss are able to do it's a
question is how we're going to bridge
this kind of gap like the first take a
little step back here and see this is a
slide adapted from under car parking and
on the horizontal axis we have the
quality of agents in some sense how much
compute they're using and the vertical
axis is the kind of environment deal
they'll be faced with if we look at
historically building controllers
initially were just hard-code them it's
just a simple environment you can hard
code a control or one environment that
you're done then later over time we
developed as a community validation
policy tration and so forth we could
allow us to solve bigger blocks world's
problems and maybe even some cardboard
problems things like that moving on we
start using function approximator
somewhat successfully 2013 results an
atari some universe games is so forth
and so we see here that as we use more
and more compute we let the data speak
more and more for itself and we can
solve harder and harder problems there's
still this kind of zone of words we're
just not getting to for example if we
wanted to solve big multi agent
environments or maybe real digital
worlds or maybe solve real-world
problems these methods are not really
getting there for now and maybe they
won't get there so them can advocate for
is that to get there you have to pass
this possibility frontier we actually
need to somehow switch methods and do
something that relies a lot more on data
and compute and less on human ingenuity
and so what we'd have in the next
generation of methods would be RL
squared let's say men
our ell method or even cogeneration cos
helped me often what's underneath what
we do is code and so I think to get to
this real world type challenges and try
to solve them we'll need these kind of
methods the FIR to move to the right on
this axis the more compute we need so
this will require that we have more
compute that we have today but we're
also projecting that in the future we'll
have a lot more compute than we have
today for example many companies are
building dedicated machine learning
compute that we could leverage to move
to the right on this axis Chris has a
lot of metal learning work already for
optimization or LLL summarize some of it
for classification for generative models
here I want to focus on the control side
of things so one way to learn to control
is reinforcement learning or you learn
from demonstrations where you learn from
your own trial and error the other one
is imitation learning where you learn
from demonstrations so let's look at the
reinforcement learning setting we have
an agent interacting with an environment
and hopefully it learns from that
interaction with the environment to do
better and better over time
underneath usually we have a reinforced
learning algorithm that updates a policy
based on past interactions with the
environment and hopefully over time this
policy becomes good when when you have a
new environment will say first
environment a your algorithm will learn
a policy for that environment when you
have a new environment you can use the
same code the same reinforced learning
algorithm but the policy will be
relearned to now be tailored to
environment be overall what we see here
is that the agent consists of two parts
it was a reinforced learning algorithm
and a policy could also be a cute
function human experts developed the
reinforcement learning algorithm and
then let the algorithm fine-tune the
policy but even though we've spent at
least 50 years now on designing
reinforcement learning algorithms was
still done of any algorithms that are as
good as humans in terms of speed of
learning so maybe it's time to also what
the other part be learned also learn the
algorithm so learn the entire agent
rather than only learn the policy and
design the algorithm by hand so what
would that look like
and metai reinforcement learning what we
do is we look at learning the
reinforcement learning algorithms so
what that means is that we somehow are
hoping that we can develop an algorithm
that's learnt that's better than what we
designed by hand and the way that's
gonna play out likely is that this
algorithm is learned by seeing a lot of
interactions with many environments and
from that buildup of trier over what the
world tends to be like and could that
inside the algorithm that now has a very
strong prior allowing it to learn more
quickly so it might interact with
environment a B and so forth and then
somehow this meta RL box spits out a
faster L agent that is good at solving
new environments very very quickly how
can we formalize this
how can formalize learn to reinforcement
learning actually this formalism that
was published almost identically out of
open era in Berkeley and then deepmind
these are the two papers at the bottom
here's the formulation we'll try to find
an agent which means some parameter
vector theta that parameter SS the agent
that could be code it could be a neural
net whatever you think is a good
representation of your agent we want
that parameter vector theta to be such
that if you sample an environment at
random and then sample trajectories
executed by that agent in the
environment that somehow that agent
collects a lot of reward now keep in
mind this agent is not just a policy
it's a lot to adapt over time so it's
gonna be dropped in an environment let's
say two times over this two episodes
hopefully has adapted learn to do well
in that environment and then it gets
moved to a new environment and we're
going to Train it hopefully to be good
at collecting reward in just capital K
episodes in a single new environment
that's never seen before so you need to
make some choices for put underneath so
here's our training objective when you
choose what this agent is going to be
one thing we could put underneath is a
RN n the beauty of putting an RN
underneath that it's very generic it can
include any algorithm it can include
prior about environments and so forth if
you look at little more deeply the
weights in the RN n would correspond to
both the reinforcement learning
algorithm and the prior over
environments somehow included in there
and then the different activations that
the agent has over time would correspond
to it adapting its policy as its acting
it in a new environment the meta
training objective at the top can be
optimized for with standard
reinforcement learning so we do is with
bootstrap off of standard reinforced
learning algorithms let's say PPO TRP
oa3 C and so forth whatever is your
favorite one used that to optimize this
objective and the consequence would be a
recurrent neural net that itself
an agent that has embedded in it
reinforcement learning algorithms that
are faster don't have to put an iron end
underneath that necessarily can put
something else for example a wave net
black architecture so dilate the temple
convolutions but then maybe with a
tension also so you can see more detail
from the past and a wave net would allow
you to see you can use the exact same
objective just a different architecture
for the agent underneath that's being
trained or you can do yet something else
you might say well we know grain descent
is pretty effective let's stick with
gradient descent and let us just see if
we can pre train an agent so that it's
ready for fine-tuning when it's faced
with the new environment how would we
set that up this is called mammal at
test time you're going to be fine-tuning
so at test time you start with some pre
trained parameters you get some new data
computer gradient to an update get a
fine-tune parameter vector and the hope
is that this fine-tune parameter vector
is good at solving the task how do you
train for this you can train this
end-to-end you can add training time
there's no meta training time you can
set up a set of tasks you try to find a
parameter vector theta that is such that
if you sample a task and then take a
gradient step on the training data from
that task that you do well in the
validation data of that task with just
one gradient step if you find a
parameter vector theta that does this
and that's a really good pre-trained
parameter vector that you can use to
solve many tasks in the future once you
have any of these architectures sinter
seem to start looking at how well can
they solve problems that humans have
already solved for example bandit
problems which are canonical
reinforcement learning test problems
where you choose and any given time
abandoned to play that banner has a
probability of payoff but you don't know
ahead of time what the probability of
payoff is for each of the bandits and so
you need to pull some men-at-arms figure
out which one is higher probability of
payoff keep pulling that one humans
actually expert humans have designed
asymptotically optimal algorithms for
this kind of problem and so now we could
look at can our learn fast
agent be as effective at solving a
bandit problem as these human design
algorithms so setting here is that after
training this agent gets dropped in
front of a new set of bandits it's never
faced before
and needs to start exploring and
exploiting in that new situation here's
a table with results the further we go
down in the table the bigger the problem
sitting is what we see is that all three
approaches some more than others are l
squared mammal and snail are able to be
fairly competitive with human designs
asymptotically optimal algorithms to
solve this kind of problem so this is
very interesting because it means that
we can learn something that's as good or
almost as good as the best you can hope
for in this kind of environment and all
we need to do is run some learning you
don't need a lot of expertise into how
to design bandwidth algorithms here's
another example let's say we want an
agent that is really good at adapting
what it needs to do when controlling
this cheetah robot so different tasks
here correspond to different speeds or
different so here the goal is run for it
as fast as possible here the goal is run
backwards as fast as possible
now the goal is to run at zero speed
which is staying in place and what we're
watching here is the fully trained agent
Mehta trained agent being dropped in a
new environment which means a new target
that is supposed to achieve and it in
the first episode we're watching the
first episode every time adapts to what
it's supposed to be doing here we did
something similar for an Throwbot so you
see here again as the agent you see the
agents very first episode controlling
against that specific task after in the
past having Mehta trained on the ant
robot and acquired a wide range of
skills in the anthro BOTS and they're
now getting dropped into a new
environment where a new target speed is
requested here's another example a
scenario where you need vision and
control so here an agent gets to see
monocular images in front of it
kisses Theo two degrees to the left two
degrees to the right or go straight it's
supposed to navigate this maze and go to
the target destination
but it doesn't have the map of the maze
it only has the monocular images and so
what we see here in action is this agent
reliably navigating this maze it doesn't
know the map so it's actually doing
something quite reasonable it's
exploring this maze very effectively
it's never been in this maze before but
it's trained to become good at
navigating mazes it's never been in
before
reliably explores this maze and
exhaustively explores what's possible
without waste
time on anything and finds its way to
the other side again it's an unlucky
turn one more time and then finds the
goal location runs right to it so it
learned a whole memory system to
navigate mazes and low-level control as
well as low-level vision processing if
you want to do something that is
extended actions over time again for
example train an architecture like this
where you have a master policy and then
sub policies phi1 phi2 phi3 which
executes at full control rate the master
policy switches between them now you can
train this in the meta learning setting
is the original RL square type meta
learning setting here what we then do is
we say let's find the subpar C's
parameterize by Phi such that if an a
master policy is being trained by a
reinforced learning algorithm and that
master policy is initialized to randomly
with the parameter vector theta zero and
it's randomly dropped in an environment
M and gets to experience capital K
trajectories those sub policies should
be really good at allowing this agent to
succeed to Train on top of those sub
policies well you see then is that for
example for the moving bandit problem it
learns sub policies to go to specific
types of bandits for an ant that's
supposed to navigate mazes it learns
different low-level gates moving
diagonally down sideways or diagonally
up and it discovers those not by us
telling that we want these gates but by
that those are the gates that allow a
master policy on top of this to
maximally quickly adapt to new
environments here are a few references
that you can look at later
we can't just reinforcement we can also
learn to imitate imitation learning has
been quite successful in many ways but
typically this canonical paradigm is
used where your imitation learn for each
task separately so maybe assembling a
chair then assembling a table and so
forth manner learning would be the
notion where you actually see a lot of
demonstrations and then learn something
that can in the future imitate from one
example so you give one example of a new
task any media learns how to do it the
way you can set this up is you would
train a one-shot imitator a big neural
net in this case by showing it to
demonstrations of the same
tasks and it's supposed to be looking at
the current frame in demo to predict
what the right torque is to apply for
this robot by parsing demo one if it can
do that reliably it means that it can
from that one demonstration demo one
particular needs to happen in the new
situation so this is just a supervised
learning problem the way this is set up
to train a one-shot imitator we looked
at this for block stacking I'll skip
over this video would look at this for
robot execution so let's watch that one
so here on the left is the demonstration
robot is placing the object into the
white bowl and just from this rostrum of
pixels and the torque value is applied
to the robot one example it can learn to
do the right thing in the new situation
on the right if the demonstration puts
the object onto the yellow placemat
it'll do something similar in the learn
behavior again just from one
demonstration going from raw pixels to
motor commands okay so what are some
current directions
I think the examples I've shown to you
are promising but are still solving
relatively simple problems compared to
the real-world problems you really want
to solve and a lot more work is needed
to figure out how to do that some of the
work might be algorithmic I think a lot
of it will also be representational II
different neural architectures maybe I
have more memory in them than the ones
I've shown here maybe code that is
standard programming type
representations because those generalize
really well typically to new situations
another big challenge ahead is to
actually move from the standard middle
learning sitting I've shown you where
you get dropped in a new environment
that need to do well and then it's over
to something where you continuously are
faced with a changing environment this
would be the lifelong learning setting
this could be because the environment is
non-stationary
or it could be because you have
competition if you have competition and
the competition the competitors are
constantly adapting you need to adapt to
what they are doing are you going to
fall behind and constantly be losing so
example watching here the red robot the
four-legged one is a meta learner and so
is better at adapting to its opponent
and while it starts out as a weaker sumo
wrestler over time it actually starts
consistently beating the green six
legged robot because the
one is just using regular reinforcement
learning and hence doesn't adapt nearly
as quickly to changes in its environment
thank you
that's a question that I'm not sure that
around stand wise melanin is very
important for the reinforcement learning
problem why the can improve the
performance it still is also presumes
SPG I'm not sure but I guess one the
real injured as she may not enough to
search in just very highly non compact
space so the methylamine can't help the
SPD for that am i right so I think
there's a few a few parts to your
questions so one question one part of
the question is why would meta learning
help for reinforcement learning yeah and
so I think that's actually quite similar
to why would help for some of the other
problems and the reason they'll help in
reinforcement learning is because if you
have a strong prior over what the world
is like you can learn more quickly so to
see it is in the extreme what's imagine
you did model-based reinforcement
learning in a model-based reinforcement
only to do is figure out the model of
the world that you're acting in and what
their reward function is once you have
data and if you have a really good
planner
you're done so then it's just a matter
of identifying what world you're in in
that specific scenario which is the
easiest one to think about what happens
is if you have a real a good prior over
what the world can be like from a small
amount of data you might be able to
identify what the world really is like
that you're faced with or as if you
don't have a good prior it's gonna be
very hard to identify exactly what the
world is like it's going to take a lot
more data collection on the spot to get
that kind of expertise no help to what
model something about the environment so
that quickly yeah the best way to thing
is that the meta learner models what
environments can be like so there's many
types of environments you could be faced
with and the meta learner models a
strong prior over what the environments
are that you might face in the future
and then at meta testing time it'll
essentially exploit that prior to adapt
much more quickly than a system that
does not have this kind of prior okay
thank you very much
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>