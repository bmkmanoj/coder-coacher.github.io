<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Matthew Liao - Artificial Intelligence and Moral Status | Coder Coacher - Coaching Coders</title><meta content="Prof. Matthew Liao - Artificial Intelligence and Moral Status - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Matthew Liao - Artificial Intelligence and Moral Status</b></h2><h5 class="post__date">2017-09-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qPIqZ1rs-j8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so art that we our last symposium
was on the somewhat self-interested
topic of how we can get AI to do what we
want now we're going to switch to the
point of view of the AI itself and ask
about the moral status of AI and with
questions of the sort of what the rights
of AI are how how the moral status
depends on cognitive abilities our first
speaker is Matthew Lau he's the Arthur's
it Rijn professor of bioethics at NYU
the director of the Center for Bioethics
the editor of the Journal of moral
philosophy and the founder of the ethics
etc group blog he's interested in a
broad spectrum of philosophical issues
centering on ethics bioethics and moral
psychology and his title is going to be
artificial intelligence and moral status
icky net in it can everybody hear me
okay great okay so a couple years ago
when I shared an office well not sure
enough is that when I have my office
next door to Nick Bostrom we actually
tried to write a paper on this topic on
artificial intelligence and moral status
but we never got around to finishing it
so I'm really glad to have this
opportunity to think more about this
topic so machines and artificial
intelligence are acquiring more and more
capacities we have the blue winning
speak into the okay we have deep blue
winning the chess
winning in chess Watson winning in
jeopardy and alphago they're also some
of them are acquiring mobility they can
move around we have the Google
self-driving cars and efforts are
underway to build machines that can
recognize emotions so the so called
effective intelligence and the
possibilities that a eyes may acquire
human level or even greater than human
level intelligence is being seriously
considered indeed by many people in this
room as a UH as a ice came more and more
capacities the issue of whether they
will acquire greater moral status
becomes salient moral status is the
standing that an entity has that gives
more agents a potential reason to act
towards it in a certain way so for
example cats have a certain status in
virtue of being sentient they're more
moral status qua sentience gives more
agents at least a pro tanto reason not
to to pretend a recent act wears
something a certain way so for example
not to cause them unnecessary pain so
what kind of moral status will a eyes
have as they gain greater and greater
capacities well they one day have human
level moral status well they have
greater than human level moral status to
answer these questions we need a theory
of moral status so I'm gonna begin by
sketching such a theory
so to start here's a list of entities
that could have or could have moral
status and it's not intended to be
exhaustive so we have in animate objects
rocks the environment non-human
terrestrial living things plants animals
bacteria etc etc no more functioning
human beings with full physical
cognitive emotional and social
capacities so normal adult human beings
damaged human beings the comatose the
severely mentally disabled human beings
at the beginning of life so fetuses
infants possible future human beings
future generations non living human
beings dead human beings and you could
also talk about aliens
you know if they exist and we're gonna
apply this to AI and robots so how do we
determine what kind of moral status each
of these entities has
so here are some constraints first it
seems that we need some kind of
objective empirical methods and the
reason is that there doesn't seem to be
an a priori way of knowing what kind of
moral status an entity has so for
example if we were to me some alien
beans when we go to Mars and we want to
know what kind of moral status it has it
seems that we would not be able to know
this a priori - to find out it seems
that we would at least have to
investigate empirical empirically what
attribute has and consider whether these
attributes are normally salient such
that the alien should have a certain
kind of moral status a second constraint
is something called the species
neutrality requirement so the idea here
is that the criterion for moral status
should not exclude any species in
advance an example of a criterion that
would exclude other species in advances
being human being human was exclude all
other human species so that so being
human doesn't satisfy the species
neutrality criterion a third constraint
is that the empirical criterion should
be based on the intrinsic properties of
an entity so this is kind of
controversial but most people think that
it's based on the intrinsic properties
of an entity so the intrinsic properties
of an entity are properties that are
internal or inherent to an entity the
extrinsic properties of an entity are
properties that depend on an entity's
relationship with other entities so for
example being a moral agents is an
intrinsic property that a no normal
functioning adult human being typically
would have whereas being a spouse would
be an extrinsic property that depends on
someone's being married to another
person so suppose that you and your
spouse are both drowning you would of
course value your spouse more based on
your relationship with her and try to
save her first but in one respect your
spouse and your the stranger would still
have the same moral status in virtue of
the fact that they're both moral agents
okay
so here are some criterion for moral
status whether an entity is alive
whether an entity is conscious whether
it's sentient whether it can feel pain
whether you can desire whether has the
capacity to know something about the
causality such as if one does X then Y
would happen and the capacity to bring
about something intentionally so
rational capacity and whether has the
capacity to understand and act in light
of moral reasons so moral agency so next
it seems that some entities are going to
have greater moral status than others so
for example if you compare a rock with a
plant it seems that the plant would have
greater moral status and an obvious
explanation is that the plant is alive
but the rock isn't and among entities
that are life arguably some will have
greater moral status than others so if
you compare a plant in a turtle it seems
that the turtle would have greater moral
status then then the plan and
explanation of that is that the turtle
is sentient and the plan isn't and
finally among entities that are sentient
obvious arguably some will have even
greater moral status than others so take
a normal functioning adult human being
and an adult it seems that the human
being would that have greater moral
status than the turtle an explanation
here is something like the human being
has moral agency while the turtle
doesn't I'm gonna come back to that in a
minute so let me say something about the
basis of human moral status the kind of
moral status that human beings have is
often called right holding they're
called right holders so one the question
we want to ask is are all human beings
right holders so the Universal
Declaration of Human Rights 1948 says
all human beings are born free and equal
in dignity and rights but it turns out
that is actually quite difficult to
defend this claim philosophers who have
examined it have tended to find
themselves either agreeing that no not
all human beings are right holder
or adopting what Peter singer calls a
specious position where species is
defined as morally favoring a particular
species in this case human beings
without over others without sufficient
justification so what why is this well
the problem is that there doesn't seem
to be a real of an empirical attribute
that would apply to all human beings
so take actual sentience for example
some human beings such as an encephalic
infants or comatose person lack lacks
actual sentient or take actual more
agency many human beings including
newborn infants lack actual more agency
these human beings will not be right
holders on these accounts of right
holding and this is called the the
problem of marginal cases so elsewhere
I've actually argue that we can have an
account that doesn't have to be sea
species and that appears to allow all
human beings to be right holders and
what this account says is that a
sufficient condition for an entity to be
a right holder is if the entity has the
genetic or more generally the physical
basis for moral agency let me just very
briefly sketch this account so the
physical basis for more agencies the set
of physical codes that generate more
agency in human beings this set of
physical codes is located in their
genome and we know this because a lot of
complexity is needed as the
developmental basis for a complex
adaptive phenotype like moral agency and
the genome contains a significant
portion of this complexity so to have
the genetic basis for more agency the
genes that make up moral agency must be
activated and be coordinating with each
other in an appropriate way a beam does
not have the genetic basis for a certain
attribute if it just possesses somewhere
in its genome the genes for that that
could make up the attribute but these
genes are either not activated or are
scrambled in such a way that they do not
coronate with each other
appropriately let me just flesh that out
so consider suppose there's a book that
contains many random words which if put
together
would result in a Shakespeare book this
book would not be a Shakespeare book
because it just contains the correct
words these words must be organized and
be coordinated in the right kind of way
so I've argued that it can actually be
shown that all human beings including
infants the severely disabled those in
persistent vegetative States and so on
have the genetic basis for more agency
so given this on my account they would
all be right holders and this account
avoids species um because if it turns
out that gorillas and chimpanzees and
other animals have the genetic basis for
more agency or more generally the
physical basis for more agency they
would also be right holders and let me
just add that this account is compatible
with permissive views about abortion so
we can follow Judith Jarvis Thompson and
say that even if a fetus is a right
holder a woman still has a right to
determine what happens inside her body
that is she has a right to bodily
integrity okay let me just mention one
other feature of this this account so
there's this idea of the function of
right holding which says that if and
when the right holders interest is in
conflict with the same kind of interests
that is with a comparable interest of a
non right holder the right holders
interests should prevail so just to take
an example suppose that you have a
choice between serving at saving a
turtles limb and a human beings limb the
turtles limb the turtles interest in
keeping the limb and the human beings
interest in keeping the lien of the limb
appear to be comparable the function of
right holding says that you should save
the human beings limb which seems to be
the correct verdict okay so is the let
us now consider what kind of moral
status a eyes can have is the AI alive
is the AI conscious is the AI sentient
can the AI feel pain can be AI desire
does v AI have rational agency does the
AI have more agency if an AI can acquire
some or all these capacities it will
gained the kind of moral status that
accompanies each of these intrinsic
properties and I just realized that I
printed my ran out of paper technology
okay so yes yes okay I can do that very
easily
yes more paper clips that's right so so
the idea so you know I mean one of the
issues that we're interested in finding
out is whether AI scan human have human
level moral status so I think that
there's a way for a eyes to have human
moral level status and that is it's
sufficient if an AI has the physical
basis for more agency so that sort of
comes out of my account of right holding
and again that physical basis has to be
active and be coordinating in the right
way so it's not enough that these
physical calls I just sort of randomly
strewn somewhere it has to be activated
and in the right kind of way so then
there's a further issue which is whether
AI can have greater than human level
moral status and I think the answer to
that question is sort of possibly but I
want to argue that if that's true it's
not going to be based on extending the
current criteria that we have for moral
status such as rationality or morally
more agency so let me explain why so
take here two people so take rational
rational agency for example so first you
got two people right you have a bright
who's above average intelligence say 150
IQ then you have average seems average
intelligence say 100 IQ it seems that
even though bright is a lot brighter
than ever
we would say that they have equal moral
status they're both right holders so if
there's say they're both are drowning it
seems that we would maybe we should toss
a coin we wouldn't think that we
necessarily have to save above average
right and we wouldn't think that we
would be doing something morally
impermissible if we were to save average
okay now suppose you've got somebody
who's exceptionally bright so the this
person has exceptional intelligence 350
IQ and then we're still have the average
here again it seems that we wouldn't
necessarily think that we should save
accept accept exceptionally bright okay
and if that's the case then if an AI is
also exceptionally bright say has 350 IQ
it doesn't seem like we would say that
just because it's really smart it has
greater moral status so it's it's not
gonna be they're gonna have the same
kind of moral status even though they
have different they they differ in their
levels of intelligence what about more
agency a lot of people you know one of
the one of my my accounts says that you
know there's something really important
about moral agency so you might think
well how about if you know someone is
really moral what it what you know what
if we amplify moral agency the capacity
for more agency again I don't think that
it's gonna change whether these two
people are right holders so or have
different moral status so take here Joe
has averaged more agency and Teresa has
exceptional more agency again we
wouldn't think that Teresa we should
always save Teresa and not Joe and we
would think that it would be
impermissible to save Joe if you know if
say they were both in some sort of
emergency situation and so you can do
the same thing you can imagine a ice
that are have expect exceptional more
Asians see and we wouldn't again say
that it seems like we would say the same
thing that they have equal moral status
so the upshot of that is that
if a ice these AIS are going to have
greater than human moral status it's
gonna and I'm not saying that that's not
possible it's that it's gonna have to
come from somewhere else like some other
attribute some other normally salient
attribute okay so let me just finally
conclude by talking about how we can get
to artificial moral intelligence this is
talking about the future I have stuff to
say about self-driving cars as well but
I'm just here just only focusing on the
future artificial moral intelligence I
think there are two ways to get there
through gradual substitution and through
coding so a lot of people have talked
about gradual substitution so I'm gonna
be very brief so basically the idea here
is that an individual's brain is
replaced by an organ and make
substitution that your brain and you
gradually replace it neuron by neuron
with sort of an ink or inorganic
substitute now assuming that
consciousness is maintained this is a
big assumption there's a huge question
about whether you know when you
substitute the neurons whether at some
point the the consciousness might just
disappear but just so I'm you know
making this big assumption that
conscious consciousness is maintained
now there's a question is suppose that
the resulting beam survives right it
seems that this would be an artificial
moral this would be an an entity with
artificial moral intelligence so that's
sort of one way by which we can get
artificial more intelligence here's
another way of and but but you know you
know a lot of people are talking about
coding artificial intelligence and here
I want to say and they're talking about
sort of I think they're talking about I
think this is true whether it's top
top-down or bottom-up right they're
talking about coding things putting
moral theories like consequentialism
deontology and virtue ethics into these
AIS and I don't think that's gonna work
I think that these principles and norms
and are too specific and too high
for us to try to code into a is not the
least because there are so many of them
and it's just it seems like it's
impossible to put in all the different
rules etc etc so I want to suggest a
different alternative and this is based
on sort of some work in moral psychology
on the universal moral grammar so the
idea here is that the mind may be
equipped with a universal moral grammar
that enables each of us in different
cultures unconsciously and automatically
to evaluate a limitless variety of
actions and generate more evaluations
such as right and wrong so this came
from John Rawls and Mikhail drawing on
Chomsky's work so the the way the moral
faculty works is that it has
domain-general cognitive mechanisms
generating representations of actions
based on variables such as agent
intention belief action receiver
consequence and more evaluation and then
some cognitive mechanisms for example
the moral faculty then combine these
representations to generate moral
judgments such as impermissible
permissible in a booth Kotori I just
published the book called the moral
brains that has a lot that cover
discusses a lot of these topics so in
case you want to look at it okay so the
idea here is that maybe we can draw on
the universal moral grammar to have some
sort of artificial moral grammar so what
we we need are codes that would
represent various variables over there
and then combine these representations
to generate moral judgments okay and one
other thing is that I think that genuine
artificial moral intelligence are going
to be are going to need to understand
why a certain action is the morally
right or wrong action that is there
gonna it these sort of the way we tell
whether we've succeeded is whether they
can understand whether an action is
right or wrong it's not gonna be enough
that they know a bunch of moral
propositions and so I think the holy
grail of artificial more intelligence is
going to
be moral understanding so just to wrap
up I think a ice can achieve human-level
moral status when they have the physical
basis for more agency to have greater
than human level moral status AI needs
to acquire some other known normatively
salient capacity
besides rational agency and more agency
and a genuine artificial moral
intelligence will have the capacity for
moral understanding thank you so with
your presentation you just gave us it
seems like the UM dwellin terminology
that you defined in the first half of
your presentation particularly moral
agency I was a little confused about I'm
one of the examples you gave us and even
though this is a very sensitive issue to
a lot of people I hope you can deal with
the terminology in particular according
to your claims an unborn child has the
genetic or physical basis to have a
moral agent to have moral agency because
a child unborn child has its own genetic
code how then do you rationalize
abortion does a woman's right to bodily
integrity supersede an unborn child's
moral status yeah I think it does so I
think I cover that so they have the
right to bodily integrity and elsewhere
I have a whole discussion about sort of
a human right so I think bodily
integrity is a fundamental condition for
pursuing the basic activities and so so
they have a right to that and that's why
yeah so hi I'm Bruce Gafsa from
granite-- Forest Sangha and I noticed
you had stated somewhat axiomatically
about the differentiation we would make
if someone were drowning and I maybe I'm
alone in the room but I struggle with
that as an axiom that we wouldn't say
save Mother Teresa
before the boy or perhaps allow Adolf
Hitler to drown before the boy right so
I wonder if you could just kind of speak
a little more on that and if you would
like say what you were gonna say about
automated cars I think I will talk about
because I'll take another paper talk but
I will say that so Hitler or something
like that I mean you know if someone
does something morally wrong they're
sort of different theories about
desserts and that's sort of you can sort
of think that the claim is and other
things being equal claim so other things
be well if you're talking about two
stranger seems like you should you know
you should flip a coin and if one is
just smarter so you know if you if two
patients go into the you know you know
the emergency room you wouldn't say hey
let's see this one is smarter let's save
this one first you know like that which
it seems like that would be totally
wrong right and so but they're
disagreements about that I mean some you
know if you're consequentialist you
might think oh no we really if we could
tell then we really should say the
smarter person I disagree with that but
I agree with you that there is an issue
there
so thanks Matthew that was great I I
just wanted to ask you said at some
point that you thought the method for
figuring out moral status you have to be
empirical and the reason you gave is
that you'd have to empirically figure
out what attributes a particular
organism or creature has and of course
there's the question what attributes
they have and the question of what the
moral relevance of those attributes is
given that they have those attributes
and it seems to me that last bit has to
be a priority sty don't know if any a
posteriori ways of doing that did you
mean to include both yeah I meant to
include both I just sort of so it partly
has to be empirical it's a partly
empirical inquiry where you at least
have to look at you know what attributes
they have as an empirical question but
the conditional conditional if they have
these attributes then they have moral
stages that's presumably a priority
really that's right
that's right you need to have a
normative premise and that normative
premise is a prize thank you Paul for
clarifying that so if you're taking the
analogy between Lane
which in moral and morality what
roadblocks do you think will face trying
to program in a universal moral grammar
that would be analogous to programming
than language I don't know either so I
wish Stephen Wolfram was here he can
sort of answer that question but I think
that oh yeah oh maybe you want to say
that's right so so I'm relying on people
like Stephen to solve that problem for
me but the basic I mean the general idea
is that we need to operate at a much
lower level this whole level about value
alignment at cetera et cetera
maximization seems like that's already
there's too much too much too many it's
too Theory Laden there are too many
things sort of the basic they're sort of
before any of that they're sort of so
this is not even a bottom-up approach
this is sort of pre bottom-up approach
in a sense that this more faculties just
sort of it's the way we view the world
and we see moral properties and more
values things like harm persons etc etc
and then some sort of mechanism for
being able to combine those different
things and wait them right and so we
need that's seems like that's the way to
go and then it's a further issue how
these artificial moral agents how
they're gonna make decisions they're
maybe they'll make better decisions than
we-well because they'll have more
information maybe they'll make word
eyesight rise that's an open question
and I don't say anything about that but
just see in terms of one of the things I
do want to say is that for AI
researchers in the room it seems like
you know that could be explored that
could be fruit fruit fully explored
Thanks
all right so I was just thinking cuz you
put a lot of emphasis on like the
material basis and that and like it
seemed peculiar to me I mean like humans
from a few hundred years ago clearly
don't have the moral development that we
have even though they have practically
the same DNA yeah and you might have
beings that have such bizarre forms or
you might have virtual entities right
that like don't have DNA at all or a way
to assess like but yet they might be
extremely sophisticated AI yeah that's a
thank you I agree that's why the account
is more
generally is the physical basis for more
agency and that's why it can accommodate
artificial intelligence so it just so
happens that all the living things that
we know have the Jeanette you know use
you know have genetic base you know
genetic codes but that might change in
the future and if that were to be the
case then these other entities could
also have more agency so thanks very
much I wonder if it might be useful to
distinguish between something like moral
standing and value so moral standing you
might say gets you into the category of
being a rights holder and then you might
say no matter what your intelligence or
other kind of value would be that you
should have rights just like all the
other rights holders but I don't know if
you want to say that rights holders
always have precedents so even a day
Allah just is probably willing to say
there are some times when sacrificing a
small human interests if it would indeed
reduce the suffering of animals let's
say in a very substantial way that that
would be a morally good thing and so
even a Dan talk just isn't committed to
the idea that we have only one notion in
play here standing and that it's got to
be graduated in the way you suggest we
might have it as a qualification for
certain kinds of moral consideration
holding responsible things like that but
we don't want that to so to speak be a
bar against considering against human
interests or against even human claims
that would be based on some kind of
freedom of movement or action completely
ignored they disavowed you that we could
cause to other species that sounds
absolutely right so that's why when I
define moral status I talked about pro
tanto consideration so it's just it's
one reason sort of other things being
equal and then the other thing I say is
that when I talk about the function of
right holding I said that it has to be
comparable interests so it's like one
tiny interest of yours but really you
know if your tea is gonna get cold and
then there's a cat who's you know being
run over by it seems like you could go
save the cat right because the interests
are not comparable so I totally agree
there so it's not
it's not it doesn't have lexical
priority there's a sort of Trump so the
general principle that right holders
have presidents it it's a pro tanto when
the interests are comparable just sort
of sort of there's a reason that sort of
prima facie reason they're sort of a
support I don't know I've just used yet
yeah there's a there's a it's there say
it's not an All Things Considered reason
right so yeah okay so if I'm
understanding correctly you're arguing
that whether entity is a right holder it
can be understood by its potential as a
moral agent right and so is not is the
AI itself not a moral agent and
therefore a right holder on equal
footing with a living but otherwise what
we understand to be a living being and
therefore also with your the second
point that you made about a person of
average moral intelligence not being
being on equal footing with someone if
exceptional more intelligence you could
have an AI of low moral intelligence
that it has an equal moral status to an
exceptionally moral living human I'm not
sure if I quite follow that but I think
that so what I wanted to say was that
sorry can you just I is is the AI not a
moral agent oh yes and by your
definition then no is right holder on
the same footing as a human being the
eyes could be more agents if they have
the physical basis for more agency so
there's nothing that bars them from
having more agency in terms of the
degree I think that so I'm I'm around I
think that there's some sort of
hierarchy of moral status so I think
that more agency maybe more this is
something that hasn't been developed but
I mean kinda think that more agency has
value over rational agency so I'm
imagine there's you and then there's a
really smart paperclip make
being right and which one should I save
this seems like I should save you rather
than this super intelligent paperclip
making beam and if that's the case this
seems like more agency has some sort of
value overriding rational agency but
that's controversial</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>