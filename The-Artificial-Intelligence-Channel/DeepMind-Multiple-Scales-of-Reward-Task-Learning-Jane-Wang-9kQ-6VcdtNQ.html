<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind - Multiple Scales of Reward &amp; Task Learning - Jane Wang | Coder Coacher - Coaching Coders</title><meta content="DeepMind - Multiple Scales of Reward &amp; Task Learning - Jane Wang - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind - Multiple Scales of Reward &amp; Task Learning - Jane Wang</b></h2><h5 class="post__date">2018-03-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9kQ-6VcdtNQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm pleased to introduce last but not
least Jane Wang who is a research
scientist at deep mind her background is
in computational and cognitive
neuroscience complex systems and physics
and looking forward to your talk thank
you very much to the organizers for
setting up this incredible symposium on
meta learning I've learned a lot so far
and I really enjoyed myself today
and thank you to all of you for staying
to the last talk today I'll be talking
about metal reinforcement learning and
in particular thinking about learning
across multiple scales of reward based
learning and answers work done with my
colleagues at deep mind here are two
examples of games that a human might
play one is a driving game the other is
Mario they're very different but a human
after just a few minutes would probably
do fairly well just from from playing
around on these same games machine
learning models like dqn require
millions or sometimes even billions of
frames to Train up that is the same
level that a human would get to in
minutes so taking this beyond just games
here's an example of a question you
might get on an IQ test called a red
Ravens progressive matrices so you have
to infer the missing pattern here based
on the sequence of other patterns in
this case it's not enough to simply
memorize the mapping between the input
and the correct answer as you have most
likely you have not seen this exact
sequence of patterns before but you had
definitely performed analogical
reasoning in your lifetime before and
you have the ability to notice it infer
patterns you therefore have a prior
about how to how to perform this task
that you learned through previous
experience this is a general principle
of meta learning learning that learns
faster with more tasks benefiting from
transfer on related tasks and across
different tasks so essentially we're
learning an inductive bias or a prior
these ideas have been around for a while
they were discussed by Schmidt who birth
roundup rat over two decades ago and
also earlier today by many people but
the earliest experimental study of
learning to learn actually dates all the
way back to 1949
in this experiment Harry Harlow
presented monkeys with two novel objects
one of which was rewarded upon selection
with food underneath and the other was
not for a set number of trials
constituting an episode I think this was
six
the monkey was presented with the same
objects with randomized left-right
placement two new objects are introduced
for every training episode over the
course of training with many object
pairs the monkey the monkeys gradually
learned to identify the rewarded objects
just from observing the outcome of the
first trial so that by the end of
training indicated by this red line here
after many episodes its selecting
randomly on trial one but then it uses
the reward outcome in order to perform
perfectly at trial - and thereafter so
in effect if it's able to do one-shot
learning and it's learned an abstract
inductive bias with roles for rewarded
and unrewarded objects and these roles
can be immediately filled with new new
objects so this ability to learn more
quickly from reward feedback with more
up and more more episodes is what I'll
refer to as meta reinforcement learning
broadly speaking we can conceptualize
different two nested timescales of
reinforcement learning that are
happening all in parallel with the
shortest time scale corresponding to
learning within a single episode such as
which specific object is rewarding at
the next level we have learning across
episodes so learning task structure
which we can also think of as priors
learning at one scale makes learning at
the lower scales easier in this case and
of course it's possible to take this
even further to think about how we can
learn these priors themselves like sorry
how we can learn to learn the priors
such as having the notion of physical
consistency objects object permanence
and physics and this is the kind of
learning that's done over the course of
a lifetime or even multiple lifetimes
such as with evolution of the sort that
we've been talking about earlier today
in earlier set sections but in this talk
we're just going to focus on the first
two levels and in particular how we're
going to be learning structured priors
now there are many ways to build a prior
into your system you can of course
handcraft them and there's been a bunch
of recent work that's been done recently
has been talked about and reviewed
wonderfully by Peter Beale and others
such as learning code initialization is
learning a meta optimizer and so forth
but today I'm going to be the word copy
focus on is most closely related to work
done by Adam Santoro and colleagues in
which they implicitly learned the prior
with with the recurrent neural network
and an external memory system and they
did this in a supervised learning
setting but what all these approaches
have in common is a way to build an
assumptions that constrain the space of
hypotheses to search over which is what
allows learning at the next scale to be
faster but we're interested in
reinforcement learning and the kinds of
priors we want to learn involve how we
map the past history of observations and
rewards to future actions this is why we
need a recurrent neural network
concretely we wants to learn the priors
by training the weights every of an RNN
which has access to previous
observations and reward maintain a
hidden States and it integrates
sequential information over time and
this allows it to implement tasks
specific RL in the recurrent activations
at the same time we constrain the
hypothesis space by training on a
distribution of tasks that are
correlated in some way and this
correlation is is in the prior that we
want our weights to learn but it has to
be different in the ways that we want to
abstract over so how is this different
from a normal reinforcement learning
well everyone's already seen this figure
a million times by now so I'm not gonna
really labor it it's essentially just
illustrating RL as an algorithm that
tries to learn the best policy given
observations from an environment and
want to maximize reward the learning
algorithm is typically q-learning your
policy gradient or some variant and the
policy that you're adjusting is some
kind of a deep neural network if we're
doing function approximation over the
course of training the weights these
weights are then adjusted to create the
best policy if we want to learn on a
sequential task we need an RNN but if we
want our algorithm to metal
reinforcement learning we need two
additional ingredients here so as
mentioned before we need a distribution
of tasks that all share task structure
we can't just learn on one task and
second we wants to augment the
observation with the past history of
rewards and actions
in this way the recurrent neural network
is able to map the history of past
observations and actions and rewards to
future actions in order to pick them to
pick up on the task structure that
exists in our distribution of tasks so
this mapping is done via the hidden
States the activations of the RNN which
also refer to as as activity dynamics
since they can dynamically change from
time step to time step based on reward
feedback we're essentially have the
emergence of a secondary RL algorithm
that is existing just in the in the
activations so what this means is that
if we if we completely freeze the wait
so we take away the RL algorithm that's
adjusting the weights we still have the
existence of a fully functioning RL
algorithm learning from a reward
feedback completely in the dynamics and
we call this meta RL because one RL
algorithm is used to Train another that
we can then implement during test and as
Peter mentioned earlier this won't work
was released almost at about the same
time with opening ie so going back to
the classical Harlow task I was talking
about before we trained our meta RL
agent on an analogue of this task using
pics pixel based inputs so each episode
to imagenet images are sampled one is
rewarded the other is not and we see
that even with the network's weights
held fixed the network learn to
implement the same one-shot learning
procedure as the monkey so learning from
the from just a single the first trial
and remember these are completely novel
images so this is a pretty hard task to
do just just in the active in the
activations this behavior is emerging
over training in a similar way where
with more episodes you start to see the
emergence of this one-shot learning
behavior so to summarize the ingredients
we assume a generative model of our task
environment parametrized by a set of
parameters Phi and on each episode I we
sample a set of parameters Phi I to
train on notice the episodic nature of
the setup which is important because the
goal is to do well over episodes and not
over single trials also note that this
is not the same as a multitask
environment we are assuming a
distribution of tasks with structure I
wanna make the point that if we have a
space of arbitrarily structured tasks
these can actually conflict and make
learning useful prior on-base or useful
inductive biases basically impossible so
it is important that we have this
structured distribution so I'm gonna use
this schematic here to refer to the meta
RL model it's simplified but it contains
the key components that I'll be making
reference to these are the primary RL
algorithm that's adjusting the weights
we use a Venter actor advantage actor
critic and we turn off the turn this off
during tests we have these auxilary
inputs I mentioned the reward last
reward in action and then of course we
have a recurrence which we just used in
Ellis TN and we're integrating the
history and so again the combination of
all these properties allows for the
emergence of a secondary RL algorithm
that's implemented in the activations
and the secondary algorithm has
potentially radically different
properties depending on the task
requirements so we first start with a
classical reinforcement learning task
the multi-armed bandit it has the
benefit of being very simple while
requiring a non-trivial policy of
balancing exploration with exploitation
on each trial you're asked to select
from n slot machine arms each of which
is paying out with a certain probability
we're just sampling that from right
Bernoulli a uniform distribution we hold
these probabilities constant for an
episode which is just a hundred arm
pulls and during tests we hold all the
network weights constants and this graph
here is visualizing 300 of the episodes
during tests trial numbers on the x-axis
and each dark tick mark Maret marks a
sub optimal arm pole so you can see that
initially the agent is exploring and
then it eventually settles on the
correct arm so now we're plotting
performance in terms of cumulative
regrets so lower is better since it
measures the gap between the actual
reward reward and what the reward of the
the best arm would have given an
expectation averaging over 300 test
episodes we see that we're doing
essentially about the on par with
standard existing algorithms that were
made specifically for this independent
bandit task so gettin's indices UCB and
Tufts and sampling these are these all
achieve sub linear regret these are
known to be state of the art with
respect to this bandit task but
nevertheless we achieve
basically the same kind of performance
even though this is a general-purpose
meta learning algorithm we can ask now
which aspects of the system I described
are crucial to to this test by
conducting ablation experiments where we
take away each component and we see the
effects on performance so I'm just gonna
readjust the axes here if you see that
the ablation experiments show that
losing access to any one of a last
reward last action and an occurrence
drastically reduces performance so that
now the models performing with linear
regret without any one of these so
meaning that there are a lot of episodes
in which it never never settles on the
right arm so all of these seem to be
important let's move now beyond simple
independent bandits because one of the
benefits of being able to learn a prior
over the distribution of environments is
that the prior learn will exactly match
the complexity of the structure that
exists in the training tasks so in this
graph the green curve here is the model
trained on the independent bandits as
before the blue curve indicates of
performance when we train on balance in
which the two arms are exactly
negatively correlated and you can see
that mid RL performs more optimally on a
tester distribution that has matched to
the distribution that it was trained on
so why does this happen
if this were neuroscience we would have
to implant some electrodes and record
neural activity while rats are running
around a maze and it would be a big
nightmare but it's not this is a
simulated Network so we have access to
all the unit activations and we can
relate them to the task parameters as an
so we can now ask how the network is
solving the independent and the
correlated bandits differently the two
tasks have different structure and that
the independent bandits has two degrees
of freedom and the correlated bandits
only have one after we trained two
separate networks on these two different
test distributions we can now hold the
weights fixed and examine how the hidden
states look at the end of the episode
for different arm reward probabilities
so again we're looking at test
performance so we we're holding the
weights constant and we're testing on
just the correlated bandits to see how
the activity dynamics are differ a
different based on how we train we
perform a dimensionality reduction on
the activity of the recurrent active
activations and we plot the first two
principal components and color code
according to the arm reward probability
so as you can see down here on the lower
right for the model trained on
correlated bandits the system has found
are very simple 1d manifold but the
model train our independent bandits has
found a different more complex manifold
to represent the past this complex
manifold does not optimized for
correlated bandits which explains the
worst performance we train met our L on
a variety of other tasks but I don't
have time to get into these today the
outcome of these experiments shows that
the RL algorithm implemented in the
recurrent activations of the LS TM it's
capable of conforming to a wide variety
of task structure including making
choices to gain information which even
if it's not immediately rewarding and
displaying different effective learning
rates based on the volatility of the of
the environment even when the actual
learning rate is held to zero so one
drawback with using RNs to learn
inductive biases and task structure is
that you can't learn anything that
extends beyond the length of your unroll
and the hidden state has reset at the
beginning of every episode so you have
effectively this time horizon in which
you lose information in addition
learning priors is useful when the tests
that we encounter are randomly drawn
from some distribution but in real life
we often encounter the same situations
over and over again and to be really
useful to have access to these past
experiences so for example we often go
to the same restaurants again and again
in our city and when we go back to the
same restaurant and we're trying to
decide what to order we usually use our
specific memories of maybe the last time
we were there and how we felt about the
meal that we ordered so how can we
extend that RL to handle situations like
this we can think of each restaurant as
being a different bandit task with
different arm reward probabilities but
now each task is is tagged with an
observable context say the name of the
restaurant and we know so we know if
we're reencountering the same task so we
call this the contextual bandit and what
we want is a way to reload critical
tasks relevant information from the past
the simplest way is to just use a lookup
table like a dictionary and and when we
do that the context now naturally serves
as the key because it's our it's our tag
and I showed earlier that by the end of
the
so the hidden state of the lsdm contains
critical tasks related information like
estimated reward probabilities and
policy so we can then store this hidden
state as the value in this diction in
this dictionary and if we do this a few
more times and we see after seeing some
more contexts so we're querying the
dictionary at the beginning of each
episode using K nearest neighbor lookup
when we see a completely new context as
has happened in the last couple of
episodes there's no match this means
that learning proceeds just the same way
as with normal meta RL but now if we see
context 1 again when we query our
dictionary we get a match in this case
we simply initialize the hidden state of
the LST n so the values that we
retrieved this is in a sense allowing
the network to continue where it left
off the last time it was in this context
so how does this look in practice as a
first proof-of-concept we used contexts
that are just perfect keys so just bar
codes meaning that there's no
uncertainty about whether or not you've
seen this particular context before we
eventually send it to to loosen this
constraint we tested on a very simple
distribution of tasks in which there are
only two arms so the arm probabilities
are always either point one or point
nine and now if we look at cumulative
regret we see that upon the first
exposure to a context we perform the
exact same as in normal matter RL
meaning we need to still explore a
little bit we achieve nonzero regrets
but upon repeated exposure when we come
back to the same context the regret is
now zero because age is able to simply
reload the test critical information
from the last time and it completely
eliminates the need to explore so this
is an effect arbitrarily elongated the
time horizon over which we can learn and
it's complimentary to the priors that we
learned about general test structure so
to recap I've explained the various
components that are required for meta RL
to occur and such as recurrent dynamics
that integrates past reward history and
observations and past action primary we
have a primary error based
RL algorithm that uses a reward
prediction error to adjust the weights
we need a distribution of interrelated
tasks
and the resultant effects are that we
have a the ability to absorb the
structure of our distribution of tasks
as priors and this leads to faster
learning with more tasks and more
training our learn our ell algorithm is
implemented in the recurrent activations
not in the weights and therefore it has
the potential to be drastically
different from the base RL algorithm and
is matched to the task structure so I've
shown only a subset of the types of
tasks that can be learned with matter RL
I hope I've convinced you that a
recurrent Network has high expressive ax
t in the types of RL tasks that it can
learn to perform in its activations I've
shown you one possible extension a meta
RL that allows us to elongate the time
horizon over which we can learn task
structure with more sophisticated
architecture we can continue to expand
the scope of what we can train on to
perhaps look at things like continual
learning or lifelong learning and with
that I'd like to thank my co-authors and
colleagues at deep mind and I'm I'll
take questions
okay
thank you for
oh great presentation just a quick
question so well what's the kind of
performance we have when it's a
correlative and it problem and also what
happens when for example you there are
way more bandits now you could
potentially explore so think of going to
a restaurant at the other day you
probably have tried like 50 percent of
the dishes or something like that let's
say the cost of trying something
completely new is is very expensive but
then if there's beef dish a is good
you're kind of hypothesizing a beef dish
B is also good you might at some point
try to explore it yes so I don't quite
understand the question so so if there's
correlation between the Pandits and yeah
you can't afford to train to like test
everything now what's the kind of
performance or intuition in that
scenario well I guess I've seen a few
parts to your question so one that
you're asking about if there's
correlation in the on reward
probabilities and then the other is if
there's way too many our arms for you to
reasonably explore yeah yeah right
well well basically so the reason that
men RL can be quite powerful is because
it can leverage these structures that
exists in your task so if you do have
correlations that allow you to reduce
the amount that you need to explore
so if say by exploring a small part of
this your arms a few of your arms that
you don't really need to explore other
arms because of these dependencies then
it would learn to pick up on this and it
would learn to do better than an
algorithm that's just learning assuming
independence in the arms yeah what
happens if there's just way too many to
explore so there's if there's way too
many then I mean I think something like
the the architecture that we have with
the dictionary and we can reload the
memory from the last time is quite
useful because you in that case you
don't need to sort of re-explore every
single time you can just think to
something that maybe was good enough the
last time that you had tried
okay okay yeah thanks
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>