<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Moral Status of Artificial Intelligence Panel | Coder Coacher - Coaching Coders</title><meta content="Moral Status of Artificial Intelligence Panel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Moral Status of Artificial Intelligence Panel</b></h2><h5 class="post__date">2017-09-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K3W7lYOW5d8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">actually let me start by asking the
panelists a question so Eric is this
working yeah Eric raised the issue in in
their talk about whether some issues
that might come up might be actually
under determined by the ethical
intuitions we have I mean our ethical
intuitions are based on cases involving
you know relatively filth familiar
considerations but we might be
confronted with cases where the ethical
intuitions that are the basis for
ethical theories just run out and they
don't have anything to say but it seems
like we've got to make some kind of
decision anybody want to speak to that I
think that's true and it raises problems
about what to do in those cases but I
don't think that's a special problem in
ethics me and Eric we're talking Esther
about the epistemic normativity and our
epistemic norms and they're gonna be
have similar problems so I'm not like to
hear more about why that raises
challenges it seems the moral status
case is actually not that hard there's
problems with determining which could
when we're gonna detect capacities and
and which capacity we could fight about
which capacities give rise to moral
status but there's actually not that
much intuition ago on we almost I think
everyone up here would agree that the
capacity for suffering is a morally
relevant capacity either it's relevant
because we desire not to or just added
space and so we all agree on that and so
once you once you have that if everybody
agrees that that tells you a lot about
one thing to protect ir from so are
there other capacities that we haven't
imagined that we should be sensitive to
maybe but you can get a lot of ground
with the agreement we've I think we all
have um I actually have a slightly
different view which is that I think
that as technologies get better we're
gonna these fakeness problems are going
to arise and we're they're gonna be hard
so let me just give a few examples so
now people are putting embryonic stem
cells into sort of mice and maybe
sometimes chimpanzees and they're real
issues about whether they're crying
human-like levels
capacity so these mice can do way better
that you can see their dendrites sort of
totally growing you know like much
denser than compared to normals and so I
think that's gonna raise issues like
it's gonna raise issues about what's it
like to be X you know we're we just have
no idea so we got these ordinary cases
but then they're these sort of and then
the other thing is sort of with genetic
with CRISPR and genetic engineering
they're issues about sort of calm URIs
human animal cameras other animal
cameras and they're all I think that's a
really good question and I I don't know
if I be I would I think they need some
sort of we need to think more carefully
about these issues I had a question the
last talk so I was wondering why I was
thinking one of the things about the
committee that you were full you the the
committee as you were talking about it
was entirely focused on risks to the AI
participants right but you could imagine
a similar committee which would also be
tasked with looking at risks to humans
and society and you know those sorts of
things the kinds of things that have
been emphasized by boström and you
cough suki and those you know that
people and so i was wondering
and it seems like would draw upon a
similar range of expertise and also that
you might get more buy-in because people
might not care much about the status of
a eyes but they sure care a lot about
the future of humanity so i was
wondering why you weren't thinking about
the committee in terms of taking on both
of those tasks yeah i think that there's
things that are exactly analogous right
so you you propose certain design
principles that you put in to the
artificial intelligence to make sure
that it's moral status is recognized
this is to set off a certain set of head
off a certain set of problems and this
morning we heard a lot about trying to
figure out how to build in other kinds
of architectures into the AI to build up
to foreclose all kinds of other types of
threats to human beings right and in
both cases you have a an issue of
of how do you ensure that these things
are actually done and so you'd have an
oversight model so some kind of
oversight model and maybe a committee
based model would be appropriate for
that so this wasn't meant in any way as
being exclusionary of that we do have to
think a little bit about whether the
same type of committee model is
appropriate we had we talked a little
bit actually about this over lunch and
and so we weren't we're not a hundred
percent sure about that but it could
probably be extended that way thank you
very much mr. liao is it your theory of
the moral worth of a human being by its
genetic Constitution would entitle
frozen embryos to be protected until
they could be implanted in somebody
willing and also that if in the future
it becomes possible to transfer a fetus
into an artificial womb that pregnant
women would be obliged if they want to
terminate their pregnancy to have the
child safely transferred now I actually
agree with this implication I would go
farther than that but just based on your
principle this is what it entails do you
think I think it doesn't tell that so I
think that one of the one of the things
I've actually argue in print is that
when we do IVF right now we're creating
too many Embree access embryos anyways
and so you know it's a reason for just
sort of producing enough the embryos
that you're actually going to implant so
that you know it could be sort of as
they were a technical problem in the
future so
okay um so all of you in to some degree
or another touched on the idea of a
human grade artificial intelligence
system and to give some context to this
question I'd like to give two short
scenarios the first being a family
decides to have a child resulting in a
family which is obviously fine but then
you imagine a company who decides to
produce a bunch of babies for a
developmental study and so my question
is a lot of you talked about like doing
research to to explore human grade
artificial intelligent systems but what
kind of ethical implications surround
the creation of these systems this gives
a lot with them the last talk we had but
I'd like to hear your thoughts you know
well my inclination would be to think
that we want to be very careful about
having companies create beings with
human grade moral status you know
there's definitely a risk that that
would create conditions of slavery for
the artificial beings if they really do
deserve moral status it might also
create the artificial appearance of non
slavery if they're created with
self-destructive desires or desires to
do the bidding of the company in a way
that violates the self-respect design
principle so yeah so I think Mara and I
would would suggest that we should be
very cautious about about allowing that
an issue that's been kind of in the
background for a lot of the discussion
during this session is that a
consciousness but it's been something
that's been kind of blocked B's it's
tricky it's really tricky to say about
consciousness or how to detect it but I
guess I want to ask a background
question to that is do you do you any of
you do you think that the capacity for
consciousness is a necessary condition
for moral status and we make that more
concrete suppose we have a very very
sophisticated AI very behaviorally
complex could do lots of apparently
goal-directed things but we're convinced
on independent
it does not have phenomenal
consciousness there's nothing it's like
to be this AI does that mean that this
thing is just qualified from having
moral status and if so what is it about
consciousness that is that it gives more
status well I mean it might be valuable
as a very sophisticated artifact without
consciousness but presumably you're
caring about that thing in particular
and when you're giving something moral
consideration you're giving it like
consideration from its point of view or
like you're considering its ends and it
doesn't make sense to do that if it
doesn't have a point of view so I guess
that's kind of where I see consciousness
being important although like difficult
to pin down or being its own issue so I
don't think you need to have
consciousness in order to have moral
status so if you look at the slide that
I have look at plants plants let's
suppose unless your pen psyche is you
think they don't have consciousness and
but they have moral standing so we
shouldn't just go around chopping off
plants so you might think they're there
they're already our entities that have
Boras moral status without consciousness
so you know the rationality might give
you something else so if you're zombies
there it might give you a different kind
of moral status but I don't see why in
principle you couldn't have moral status
just because you don't have
consciousness so sort of also deeper in
the background and these all of us when
we talk about moral status I think
really mean a specific kind of moral
status not just something that means
you're an object of concern but your
welfare is subject to our concern and so
the disagreement you're getting here is
a disagreement over the nature of
welfare so if you're any kind of
Mentalist or subjectivist about welfare
you're going to think it's a necessary
condition for moral status under this
framework and if you're not like it
sounds like mat is not if you're an
objective lister that has natural
functioning or teleological hones then
it's it's not going to be necessary
that's right I'm trying to sort of
visualize how this institutional review
board model would work and I have some
difficulty just thinking about the way
that AI research is structured in the
tempo of research
so within vertebrate research typically
would plan your experiment many months
in advance and you do few experiments
per year for
but with an AI a lab maybe around 10,000
experiments in an afternoon with slight
permutations and then midway through you
see it doesn't work so you think what
would happen if we fiddled with this
thing it's just harder to see how that
in a practical way could work if you had
to apply for pre-approval by some
committee also most medical research is
sort of done in an institutional setting
in academia by industry worth a lot of
AI research still today you can do it as
an individual and you're lucky you
really don't need this it doesn't have
the same resource requirements so would
this only apply to the research done by
institutions or would it apply to
anybody who just wants to play around
with tensorflow and implement some new
idea they have yeah that that's a really
good question can I answer the second
one first say something like maybe the
industries have to get they have to get
approval but then you could sign an
agreement to do experiments using their
code or their implementations or
something in their approved via approval
of the institutions so maybe you could
work around it that way I hadn't thought
much about that challenge but it is a
serious problem that you can do these
things sort of in your backyard so to
speak that's true of human subjects
research I guess um but also this thing
about like numbers of experiments I
think if you before IRB is if you had
told researchers that that you were
gonna do this they would say well that's
not how research works we start a thing
and then we you know we might start
another experiment in parallel once we
see some results and things like that
and we just don't allow that anymore
right you have to get protocol so you
could imagine protocols that give them a
lot of flexibility with what they're
allowed to do and how many things
they're allowed to run in a given
protocol but they don't get total
flexibility so you might not limit the
total number of experience they can tell
you in advance these are the different
number of ways we want to try to
instantiate learning and we don't see a
problem on any of them can we get
approval for this wide range and then
you let them run within that range first
get first pass to two important
precursors to the no difference argument
on the first is Andy Clark and Dave
Chalmers parity principle for extended
mind when an object counts as part of my
extended mind it has to be parody which
means there has to be no difference
between its processing mind the more
important precursor is Turing's test
which hasn't come up in the
which is kinda interesting so Turing
test is over 60 years old and it hasn't
been convincing so I wonder why did no
different argument is gonna be more
convincing than the Turing test which is
basically a no difference argument right
yes so the Turing test is a no
difference argument but it's one that
relies on a limited range of types of
evidence right so there's no observable
difference in verbal exchange as judged
by a human judge of a certain sort right
so so that's a very limited range of
evidence and I would hope that you know
a fuller examination would and draw on a
diversity of evidence and not just that
so I do think that's an important
precursor that's similar in spirit but I
guess I just wouldn't limit it in quite
that way I'd like to ask a question that
perhaps extends the idea of moral
standing to moral importance in us
humans when we suffer the loss of
someone with whom we have a an important
relationship we suffer so let's say
somebody kills our spouse our kill some
kids the leader of an important movement
there may be one or many people who
suffer greatly psychic pain and
emotional suffering from that loss my
question is if we have a a virtual or
artificial intelligence entity say
without consciousness without any
physicality without sentience desire
without any capability of moral agency
might the elimination of that entity be
of such moral importance that it might
be equivalent to the loss of someone who
has moral standing or even of greater
moral importance I wouldn't have thought
so I mean you know once you don't have
sentient and all the things you kind of
wrote out everything the
like as my phone I mean I feel bad when
my phone drop you know drop it in the
water and I'm sort of mourning for a
couple days because I lost all the
photos but you know but I mean that
would be that level right so it's when
the interesting issues when you add in
all those different things then whether
we should be mourning and artificial
moral intelligence the way we would
mourn a person and that what you're
raising is something so one of the
things I mentioned is intrinsic
properties that gives us that's give
something more spinning but then in
addition you might have extrinsic
product you might have need a simulation
so once you have a companion AI you come
to develop some relationship with the AI
then you might feel attached and in
which case you would have these
additional considerations towards this
entity I was just going to say yeah like
I mean when you design an artifact you
could have ethics around its design
because maybe it's a weapon and it's
going to harm people and it shouldn't be
built or something like that but the
point with AI would be that you're also
taking into consideration in the ethics
of the design consideration for the
thing itself not for what impact it will
have so something like that I don't
think would have that kind of
consideration subject committees think
about what kind of treatment of the eyes
is relevant kind here I mean you talked
at one point as if it was mostly pain
and suffering but these are all kinds of
things we might do to systems beyond
pain and suffering that seem very
morally relevant yeah thinking about for
example what goes on in a machine
learning system you know with big data
and you know long-term supervised
learning of a time trial after trial for
up to you millions of trials that's the
kind that we might call cruel and
unusual punishment if it was it's a
human child was there gonna be a moral
issue there not to mention the
possibility still of brainwashing as you
take a a AI system and retrain er and a
whole bunch of new data into different
directions at what point do these issues
arise good question so I didn't mean to
when I say suffering I want to
understand that broadly including things
like frustration and feelings of stress
and other things like that
so I have this little paper where I just
argue that at least it seems like a
capacity for enjoyment and suffering
might be necessary even though it's it's
not maybe not sufficient for moral
status but it's necessary there's
nothing wrong with a there's let's say
there's a computer that can have
experiences of color and that's the only
phenomenal state that can have it seems
to me morally unproblematic to make it
see green forever rather than red
forever or alternate the colors maybe
that's not true I try to argue for that
so it's got a there's got to be certain
sort of attitudinal responses or it's
got to have desires or something like
that in order for it to be problematic
so the machine learning case it's only
going to be bad for it to be subject
he's infinite tests if it cares about
being subject infinite tests thank you
let me take this opportunity now I do
want to comment on the disagreement that
was implicit between what Mara was
saying what Matthew was saying look it's
it does seem to me that the distinction
which I think Peter Railton alluded to a
few questions back between having value
in having moral status is very very
important in this connection so there
are many things that have value in which
it would therefore be wrong to destroy
without those being things that you are
actually wrong in so plants I think are
probably a painting is like beautiful
painting my Titian is like they would be
wrong to destroy it but not because it
would suffer but just because it's a
valuable thing that it would be wrong to
destroy and I think for these purposes
they nothing could be more important
than that distinction now
whether being the kind of thing that can
be wronged
it requires consciousness or how that's
exactly to be understood whether that's
suffering I'm inclined to think that it
is really a phenomenal notion and not
just that of having a point of view you
could think of that a machine as having
a point of view in the sense of a
representation of its place in space and
time and so forth without undergoing the
kinds of states that are as moral
concern namely pain and suffering yeah
great so so thanks so yeah I think I
agree with that I think that there's a
distinction between being
interested in something and having an
interest in you know just having an
interest in something so being
interested in something requires that
you desire you know you have
consciousness etc etc but then there's
the other thing which is just having an
interest and some people I mean this is
up for debate some people think that
plants can have the late latter kinds of
interests and that is sufficient for it
to have moral status whereas you might
think I mean it's it's it's an open
question whether you know artworks can
have you know I mean kind of thing art
works it's more like have extrinsic
properties rather than intrinsic
properties but you know certainly plant
you might think in virtue of their
complexity they have some sort of
intrinsic property and it's in their
interest it's in their interest to be
water to get sunlight and cetera et
cetera whether they can desire or not so
but but I understand the problem</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>