<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Uber AI Labs - Evolving to Learn through Synaptic Plasticity - Ken Stanley | Coder Coacher - Coaching Coders</title><meta content="Uber AI Labs - Evolving to Learn through Synaptic Plasticity - Ken Stanley - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Uber AI Labs - Evolving to Learn through Synaptic Plasticity - Ken Stanley</b></h2><h5 class="post__date">2018-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hJYyTHQaHxY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so our next speaker is Ken Stanley I
became I came to know his walk through
happen happen it because I actually had
an implemented implementation of
happening in my computer when I was in
grad school he's a professor at computer
science in University of Central Florida
and his co-founder of the geometric
intelligence and he knows at uber at AI
Labs today he's gonna tell us about his
recent work on evolving to learn through
similar synaptic plasticity Thanks all
right so evolving to learn through
synaptic plasticity so this is a field I
think that is not very well known as the
idea of evolving plastic networks I want
to introduce you to this idea and kind
of try to motivate why this is
interesting now interesting to think
about in general across deep learning so
let me just give you a little bit of
motivation though just for why this
field is something that interested a
small group of people brains in nature
are of course a product of evolution but
brains and nature are not static
structures we learn over our lifetimes
as do most of animals with brains and so
if we're using evolution and neural
networks which we sometimes call neuro
evolution to say just solve a problem
like one problem and then we might say
ok static structures are ok because we
just need the weights to solve the
problem and that's fine but if we're
using their evolution to evolve
something like a brain like something
that we think is some somehow analogous
to brains we see in nature then
plasticity may be essential in other
words brains in the sense that they can
learn over their lifetime and that's
where the meta learning concept comes in
because brains do learn but also brains
are a product of a process that some of
us sometimes characterized also as a
learning process which is evolution so
we're definitely talking about meta
learning and I say it may be essential
to have plasticity because it's true
that theoretically a recurrent Network
just the recycling activation alone can
also do a kind of a learning over a
lifetime so it may be a somewhat of a
topic of discussion or debate whether
it's essential to have plasticity but I
think what most of us would agree on
altum Utley if we look at the problem
long enough is that in practice probably
plasticity will make the search greatly
more efficient than if you had to rely
on only recurrence and we'll see some
evidence of that so what are we talking
about when we talk about plasticity in
practice now you could use evolution to
discover the Delta role which is
basically like the foundation that
precedes back propagation and actually
this was done in 1990 by David Chalmers
he's now like a actually a well-known
philosopher but it was way before that
so this is kind of interesting that he
did this but I think that this is not
really what we hope to see in this kind
of research in the long run because we
already know about the Delta rule so
what we're trying to find something new
and so we're trying to look for what has
come to be called local learning rules
and there's a few reasons maybe so it so
I can motivate it a little bit like why
not just use back propagation in the
first place back propagation is itself a
kind of plasticity well but we're trying
to learn how to learn so we're trying to
learn something different than back
propagation and here's some reasons why
so first of all the Delta of course the
Delta Rho is already known the second
thing is not entirely clear whether
backprop is biologically plausible I
sure just paste it in here a slide from
Hinton and where he said there are three
obvious reasons why the brain cannot be
doing back propagation but that's
obviously a topic of a great deal of
debate as well and and this is not even
his own final thought on this and I
think maybe the most important
motivation for looking at local learning
rules and learning them is because
domain-specific learning rules are
likely significantly more efficient than
generic learning rules which is what
sort of back propagation of gradient
descent would be as a generic way of
learning anything but for a specific
domain we may not need all of the power
of generic learning we may need domain
specific types of learning and that's
where specifically evolved rules for
that specific kind of problem can make
learning much more efficient so let's
look at a little more detail what that
means so the question is what can happen
at a synapse we talked about a local
learning rule I mean local to a synapse
the kinds of changes that can happen to
a wait at a synapse well there's a lot
of things that can happen weighted
signal transmission is the most familiar
one just give it a weight and the weight
basically module
a signal but there's things like
strengthening weakening sensitization
habituation heavy and learning
neuromodulation and many many others
basically an infinite number of possible
functions that you could perform to
change the weight at a synapse so this
leaves this question kind of oh how
should we allow weights to change and
remember we're gonna do metal learning
so we're gonna allow evolution to decide
that so basically we need to describe
some kind of space of possible learning
rules so that evolution could actually
design the rules that then decide how at
individual synapses changes occur over a
lifetime and I'm showing you something
kind of historical here which is a set
of rules proposed by blind Ellen
Floriana and Floriana was kind of a
pioneer in this area going back to the
90s of evolving plasticity and he's
proposed basically this set of rules
which is not necessarily definitive in
any way this is just sort of a proposed
set of rules that could be assigned at a
synapse and also could have a learning
rate assigned to the rules so that
basically that learning rate could even
go down to zero which would mean that
the synapse is static and so we could
decide within a large network what the
different plasticity rules should be so
like in this Floriana encoding it was
very simple like you could basically say
there's a couple bits that tell you what
rule you have and this is for a
particular connection there's a couple
of bits that tell you the learning rate
and that will then define what happens
at the synapse and so this is a classic
experiment just to kind of illustrate
what this is about and this is a fairly
large at the time recurrent neural
networks is just a depiction of a
recurrent neural network but every one
of those connections has an evolved
plasticity rule and so the task here is
just this robot has to go over to this
side and if it gets over here then it
switches on a light then it has to go
back and you stand under the light so
it's a very simple task and it's almost
not a learning task except you might
think that there's a little bit of
learning sort of when it gets over here
and switches the light which is that
basically has to decide remember that it
did that so now it has a new goal which
is to go to the light and so there's a
policy change in mid task and in theory
if you reconfigure your weights at the
time you do the policy change it's kind
of like remembering where you are and
then you change your policy all at once
of course this can't also
be attempted through recurrence alone so
not having plasticity but what's
interesting is qualitatively if you look
at the trajectories of individuals who
are evolved with plasticity and without
plasticity both of them are recurrent
networks but one has plasticity at the
synapses they're just qualitatively very
different so like the synaptic
plasticity version of the network has
sort of a much more intuitive trajectory
where it just turns around simply when
it switches on the light and goes back
to the other side so it reconfigures its
network whereas recurrence does
something weird which is this kind of
loop-de-loop pattern which is it's sort
of like cheating like it just bounces
around sort of and gets lucky gets over
here and this herb ounces around until
it gets the other side so it doesn't
really have a like a definitive task
switch that happens and it gives you a
little bit of hint doesn't prove
anything but that there's something
interesting that happens when we
actually learn rules for plasticity now
we have found like there were
experiments long time ago with neat
where like for example with food
foraging where some food is poison and
some is not you have to remember which
is which that actually we found
recurrence alone did do better than
evolving plasticity which kind of
triggers some of this uncertainty about
like why is it essential to have
plasticity we do have plasticity in our
brains but could there have been
something like a human brain without
plasticity and just only recurrence but
I think almost surely plasticity matters
in general because the size of the
networks that you would need to have if
it was only through recurrence to do the
kinds of memory based tasks that we can
do would be just astronomically large or
most likely and therefore plus to see as
a practical matter is probably essential
so now what I want to just kind of show
you is how does thinking develop over
time in this field and so like what I've
shown you is very simple stuff just
evolving rules like heavy in roles
sensitization roles at synapses so where
do you go with this like what actually
would be like new ideas that would
progress forward so it can do more and
evolve more sophisticated systems
learning systems and one important thing
that that I think andreas auto geo
helped to popularize within neural
evolution you didn't invent this idea
but he popularized it within this area
is neural modulated plasticity and the
idea here is that you have something
called a neuromodulator a neuron which
like is highlighted here in blue which
is special it's a special kind of an
and basically what it does it modulates
the level of plasticity of other neurons
so this this neuron is in effect telling
this neuron okay your incoming
connections are either more or less
plastic depending on the level of output
that's coming out of me and this can be
formalized here but I won't go through
the equations but it's pretty
hopefully intuitive that this can
modulate the plasticity here and this is
really nice because this is basically
like knowing when to change one of the
sort of frustrating things about
synaptic plasticity just continues
happening if you don't if you don't have
any way to stop it throughout forever
but but in reality like sometimes you
get into a situation and you just wanna
lock it in like okay that's where the
food is in the maze let's remember that
let's stop being plastic and
neuromodulation can kind of lock in
memories and do things like that and
then it can also unlock them like if
it's a we found out that the world
changed and say okay we need to be
plastic again be flexible and then start
changing our view and so it sort of
moves to our l-like capability so if you
think about it neuromodulation can be
thought of almost like as a proxy for a
reward like something good happens and
then we need to modulate our plasticity
either up modulate or maybe units down
modulated depending on the situation in
reaction to the reward so it's kind of
interesting it we're using low-level
mechanisms here to walk gradually
towards a more reinforcement learning
like scenario and so in that spirit like
what was popular in this area was to
evolve what we called tea mazes and
these were like rats animes that would
be looking for some some food and if it
goes to the wrong part of the maze it
would it would basically get a very low
reward but if I could go to the right
part and get the food to get a high
reward and the idea was it should we
should evolve a brain that can explore
the maze but if it finds the food then
it should in any other subsequent trial
just go straight to the food and not do
any more exploring so it needs to
remember where it found it and the only
mechanism that it has available is the
synaptic plasticity and we could also
once again alternatively try recurrence
alone we don't have to use plasticity in
theory but it was found that plasticity
makes it much easier to evolve networks
that solve this task effectively and you
can create increasingly complicated tea
mazes like this is another picture of
this kind of a thing to show that like
as they get more and more complicated it
becomes more and more important to have
plasticity to be able to perform a task
like this and so the neuromodulation is
the lock-in so like when the when the
agent finds the reward then the the
neuromodulation will sort of lock in the
memory of where that reward was and then
it can go back to that place in the
future and so this is actually it was
interesting to me to see
Petera Beals you know yesterday where he
showed something very similar to these
tea mazes but it was done through
reinforcement learners kind of meta
learning through reinforcement learning
I believe he'll be speaking later at the
symposium and it's interesting that in
the history of this area this is a very
common thing to do to kind of evolve
these guys to run through mazes and
learn how to learn inside of mazes using
plasticity so there's certainly some
analogy to recent work in reinforcement
learning so just giving you a little
sense of some other ideas I think
they're interesting Andreya sitaji of I
mentioned before he came and visited my
lab in around 2012 and he had this idea
for a new kind of plasticity which he
called reconfigure and saturate that we
worked on there and the idea here was
that let's use neural noise as the
exploit as an exploration mechanism and
then use wait saturation as a way of
getting stability and so like modulation
can lead to saturation in a wait and
that will allow us to effectively lock
in but so like this is a this is a
neuromodulatory pattern this isn't any
particular interesting task but just
shows when neuromodulation is high we
like lock in the weight of a connection
at some level but as when
neuromodulation is turned off the
weights kind of gradually sink back to a
random fluctuation and so this is a form
of exploration and so in this case noise
is driving exploration so the idea is
here to move towards a situation where
we could just kind of learn through
expert active exploration in a domain
with reinforcement learning like signals
and this is the domain where this was
tested where this agent was running
around gathering food particles that
could either be poison or not but
whether type A or type B was poison
would switch in the middle and that's
where the learning curve just dies over
here but then it quickly picks up again
because the reconfigure in saturating
'iron sinks back to a level of just
weight fluctuation so it's basically
exploring new policies until it starts
to then lock in again on an effective
policy after the switch so basically
showed that this is something that can
learn a task on the fly there's the idea
of using indirect encoding to encode
plus CC rules this is basically an idea
in their evolution of using one network
to encode the weights of another it was
recently used in gradient descent also
through things called DPP NS and hyper
Nets some-some terms you may have heard
but originated with hyper need and the
idea is that if we can use one network
to encode a pattern of weights in other
words this network outputs a weight for
this connection and all the other
connections in a network then we can
output a pattern of learning rules so
that would allow us to basically paint a
pattern of rules over a huge network so
like imagine that every light level here
is a different rule as opposed to a
different weight and this is a pattern
output by one of these pattern producing
networks then we would create sort of
brain like structures with huge
complicated but but principled and
regular patterns of rules and each rule
could be very complex of a very complex
kind of function surface for how it act
acts and we can then evolve very complex
learning systems now the last thing I
wanted to mention to you is something
that we're going to present tomorrow at
the workshop on metal learning and and
you can see it at a poster spotlight if
you do happen to be interested and it's
sort of a natural progression which we
see often which is this idea that hey
well you're evolving all this stuff like
all these learning rules what if we just
tried to pass a gradient back through it
and so this is about back propagated
plasticity so it's still the meta
learning but we substitute back
propagation for evolution and this is
work I did with my colleague Thomas
Makoni
and also Jeff Clun and uber AI labs and
the idea is that we could adjust the
plasticity parameters by gradient
descent between lifetimes and therefore
we can actually take advantage of
gradient information in order to get the
right rules and this allowed us to get
plastic networks with millions of
parameters and they actually became very
good and we tried image reconstruction
and what was interesting I think was
that we've learned that these plastic
networks a much better than other kinds
of recurrent networks in this particular
domain so like if you look at these
learning curves you could see a regular
recurrent Network can't even learn an LS
TM does eventually learn but much slower
than the blue curve which is this
plastic recurrent Network where the
plasticity rules are evolved which
learns almost immediately to do the task
and
perfectly and so what the task was like
image reconstruction which means we
would show it during the learning phase
the meta learning phase is to learn how
to do this but we'd show it a sequence
of images and then show one that was
incomplete and would have to reconstruct
that image and what was interesting is
the coefficients of plasticity which are
shown here which is very hard to see but
they actually have structure so it's not
like just a uniform set of the same rule
everywhere which you might guess it was
actually structured specifically
tailored to this problem to be good at
it and we tried versions of the network
that were not that were uniform and that
weren't learned through through
plasticity or where the little plastic
rules weren't learned through meta
learning and it was significantly worse
so basically you need this structure to
get the best performance and we were
able to get it so I think it's very
intriguing to think about the idea that
plasticity inside of gradient descent
which is very little explored if at all
is really probably viable for certain
kinds of tasks and maybe actually the
best approach even over and above what
you can do with just recurrent or
state-of-the-art types of recurrent
structures
so in conclusion there's a wide scope
for creativity in this area endless
kinds of plasticity can be evolved
there's a lot of biological inspiration
and domain-specific learning mechanisms
are much less studied the domain general
and so this could be important for
certain domains like learning to walk
quickly on new terrain like you don't
need all of the apparatus of learning to
just get better at doing some one
specific thing thats related to your
general expertise anyway and finally I
think it's nice to see cross-pollination
between different areas I think it's
very natural often that things that
originated in evolution where you have a
sandbox which is in some ways much less
constrained because you don't have to be
differentiable to eventually cross
pollinate into some things like more
conventional deep learning once we get a
handle on how these kinds of mechanisms
work so thanks and if you're interested
in more you can find me at these places
Thanks
we have three minutes for questions
that's a question
so have you compared I guess newer net
architectural with plasticity with
different internet that doesn't have
plasticity but the activities may
actually depend on I'll say you know
other new URLs because there seems to be
a clueless there because you can always
kind of simulates like a weight that has
zero weights with like low activity at
that particular URL so actually I don't
know if I completely followed like what
you want to compare like it was a neural
network with plasticity with a neural
network that does what exactly so
essentially you can think of like the
plasticity as essentially modulating the
the activations of the neural Nets rate
say in that record setting so for
example at different time steps you
would have different activations at
different units and that's what
basically correspond to kind of
modulating yeah like whether the net
itself is like actually very like in
terms of connections I mean like you can
always think of you know something where
the weight connections can't change as
you know something that's fully
connected but you're basically changing
the ways themselves but I mean if you
haven't done the comparison that's fine
no no I don't think I have done it but I
I think or I don't I'm not aware of
anyone doing that we're looking into
that
but I think it would probably be useful
for me to talk more with you to
understand what how to do that
comparison right away</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>