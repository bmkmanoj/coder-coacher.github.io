<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning for Robotics - Prof. Pieter Abbeel - NIPS 2017 Keynote | Coder Coacher - Coaching Coders</title><meta content="Deep Learning for Robotics - Prof. Pieter Abbeel - NIPS 2017 Keynote - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning for Robotics - Prof. Pieter Abbeel - NIPS 2017 Keynote</b></h2><h5 class="post__date">2017-12-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TyOooJC_bLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I think it's a good idea people will
keep walking in anyway I've seen in
previous days hello everybody welcome to
the afternoon session it's my great
pleasure
- my phone stays ok it's my great
pleasure to introduce Peter bill Peters
a professor of Electrical Engineering
and computer science at UC Berkeley his
research focuses on robotics machine
learning and control and his PhD work
with Standring at Stanford is
beautifully combined these three areas
they showed how to you know a
remote-control helicopter could be could
autonomously perform
it's an incredible acrobatic stunts you
know just through apprenticeship
learning since moving to Berkeley he's
been a leader in applying deep learning
methods to problems in reinforcement
learning and robotics one project
appears to developed a robot that could
pick up crumpled items of laundry and
fold them neatly and it's got a you know
huge amount of media attention and he's
also been involved with several startups
grade scope is a company he founded that
uses machine learning techniques to help
with marking and grading homework
assignments something that a lot of us
might find useful
he recently spent a very productive 18
months at open AI and but now has a new
venture that aims to bring machine
learning reinforcement techniques to you
know bring to bear on robotic automation
to try and improve the wave
manufacturing happens throughout the
world
so please welcome Peter
thank you and thanks Rob for the very
kind introduction so let's dive right in
this guy well click see so what are we
watching here this is the pr1 robot
doing a lot of the chores that we wish
to robots might be doing for us but
there's a catch and the catch is that
this robot is being teleoperated Erik
burger one of the students who built
this robot is sitting inside a hardness
and orchestrating every detail of the
motions that this robot is making so
it's more time consuming to do things
this way time to do them with your own
hands but for us as AI researchers
engineers this is really really nice but
what it shows is that the mechanical
engineering is there we just need to now
provide the artificial intelligence to
make this a reality
now when I say just the artificial
intelligence that don't mean to say it's
gonna be very quick or easy necessarily
I think there's a lot of challenges
ahead of us and that's part of what's so
exciting about this and in this talk I'd
like to share with you what I think are
some of the important challenges we need
to solve to get robots into our homes
and possibly beyond a first big
challenge is that I think we'll need
much faster and with that I mean much
more data efficient reinforcement
learning let's first make sure we're all
on the same page and everybody here
works in reinforcement learning what's
reinforcement learning you have an agent
the agent gets to see the state of the
environment or maybe just a sense your
observation of the environment based on
that takes an action an environment
changes state that might be a reward
associated with that and this process
repeats over and over the goal for the
agent is to maximize the reward for
example if the agent was controlling a
self-driving car maybe it was a positive
reward for reaching the destination a
negative reward for not being at the
destination yet and very negative for
any bad outcomes like an accident
before someone is actually quite
different from supervised learning for
example as the agent takes actions there
is no supervision as to whether these
actions were right or wrong it's only
implicitly through the reward that you
achieve in maybe a final state
you get to that you understand whether
the combination of actions that you took
was good or bad assigning back credit to
each of those actions the credit
assignment problem is a really hard
problem that's where you get your signal
from in reinforcement learning another
challenges because the feedback loop
things could destabilize you can end up
with your system destroying itself while
it's learning and then there's the
exploration challenge which is doing
things you've never done before this is
the only way to improve yourself based
on you're able to do before despite
these challenges of the last few years
especially there's been a lot of
progress in reinforcement learning
in fact reinforcement learning has
enabled agents that learn to play Atari
games from their own trial and error
going all the way from raw pixel values
to joystick actions it's enabled
learning to play classical board games
it's enabled to play some of the current
state-of-the-art video games like dota
1v1 and it's enabled to learn to control
continuous systems like this simulated
robot here and what you see here is a
robot where you see the learning in
action when it starts out the neural net
is initialized randomly then over time
it gets better and better and better at
solving the task what does that mean
here solving a task means optimizing
reward there what is higher the further
this robot goes to the right and lower
to higher the impact with the ground and
just from those simple specifications
it's able to invent something like
locomotion from scratch we've seen
success on real robots this robot is
trying to stack the Lego block onto the
corner spot and as she learns to do this
starting with random initialization
things out over time how to solve this
task
ignosi have reinforcement learning take
on control problems that are far less
intuitive to humans for example this is
the superball robot a robot built by
nasa meant for planetary exploration and
we see here is a robot the robot is
controlling the length of the cables by
pulling the cables or extending them the
center of mass will shift and the robot
rolls over so clearly some mastery has
been achieved in a wide range of
environments in fact often superhuman
level mastery but how good is the
learning itself for example dqn took
about 40 days if you were to run at real
whereas humans can learn the same thing
in about two hours in Josh Tenenbaums
lab they did a more careful study on
this and what they found is that humans
after 15 minutes tend to outperform d DQ
n afters trained for 115 hours so this
is a very big gap in learning efficiency
so how we're going to bridge this gap as
a starting observation let's look at the
current state of yard all roads T RP o
DQ n a through c d d PG PP o rainbow and
so forth those are fully general
reinforcement learning algorithms that
means for any environment that you can
mathematically define these algorithms
will be applicable now in practice the
environments you see in the real world
are only a tiny tiny subset of all
environments you can define
mathematically for example what you see
in the real world will all satisfy our
universes physics that's just one of
many many ways you could define dynamics
mathematically so many other options
available so question is can we develop
fast reinforcement learning algorithms
that take advantage of this for that
let's revisit the agent architecture so
we have an agent interacting with the
environment underneath typically we have
a reinforcement learning algorithm and a
policy or q function it's equally
applicable to Q functions as the
policies I'm just being concrete here
and putting a policy down so the
reinforcement algorithm when faced with
an environment
viii will train the policy to become
good that is master what needs to be
done in environment a when facing
environment B the reinforcement item can
be reused to train a new policy for
environment B and so forth and
traditional reinforcement learning
research human experts develop the
reinforcement learning algorithm and
then rely on the reinforcement learning
algorithm to train the policy but
they're both inside the edges and after
many years of human development of
algorithms it says she's still the case
that none of these algorithms are as
good as human learners maybe we should I
should take a step back and let the
system learn more that is why not let
the system learn the entire agent both
the algorithm and the policy rather than
only letting it learn the policy so when
I'll use the term meta learning in this
presentation without riff
for to is a approach where the learning
algorithm itself is being learned that's
what the meta refers to so metal
reinforcement learning means learning -
reinforcement learn what can we hope for
from the intuition we've gotten so far
it's the case that if we want something
to learn faster promise to understand
our world better have seen many
situations in our worlds is that when it
sees a new situation it can pick up on
it more quickly so maybe this meta
learning would be faced with many
environments a B and so forth somehow do
something with them and then output a
faster reinforcement learning agent that
when faced with a new environment F or G
or H would quickly adapt to that new
environment and do the right thing now
how to formalize this this is what we
want but we still need to somehow have a
way to to do it here's one way to
formalize this what we're after what
we're after is a agent franchised by a
vector theta theta is some numbers let's
say it could be parameters in a neural
net it could be an encoding of what some
program looks like anything real we try
to find a theta that is such that it
maximizes this objective which is
unexpected if you sample an environment
and they get to act capital K times in
that environment this agent will
optimize reward over those capital K
episodes it gets pictorial way that
looks like if capital K were equal to
two this agent would be dropped in an
environment gets to act for two episodes
I think it's dropped in a new
environment gets to act for two episodes
again and this repeats over and over and
over and what we want is not an agent
that masters one of these environments
but an agent that is generally
applicable to any new environment you're
going to drop it into and quickly learn
how to collect rewards in that new
environment so a training time what that
means is we'll sample a bunch of
environments the training environments
and optimize this objective over the
training environments we still need to
decide how we're going to represent this
agent it can be a lot of things and you
should probably make some choices of
your own in your future work and have a
better choice than we've made so far but
let's look at what we've done so what we
did is we put a recurrent neural net
inside that agent why a recurrent neural
net is a generic computation
architecture as
George has compute and we want to be
very open-ended allow it to learn
anything that might be useful to be an
RL agent if we look a little more deeply
what this means is that the weight of
this recurrent neural net will
correspond to a combination of the
reinforcement learning algorithm the
agent is using and as well as the prior
the agent has from interacting with past
environments and then the activations as
its acting a new environment will
correspond to what it's fine-tuned in
that new environment in terms of policy
how do we find this theta if you look at
this objective this is actually still a
reinforcement learning objective so we
can actually train to find the RL agent
theta by running reinforcement learning
on this meta objective so we'll
bootstrap from existing slow
reinforcement learning algorithms to
learning
hopefully faster reinforcement learning
agent to evaluate this one place to
start could be multi round bandits and
multi bandits you are faced with a bunch
of bandits each bandit you can choose at
any given time and each band will have a
probability of payoff and you don't know
the probabilities ahead of time so your
job is to this to figure out which ones
have higher probabilities of payoff and
then pull those more often than the ones
that have low probability of payoff
this is a classical exploration
exploitation type of problem in its
simplest form in this setting
researchers have come up with provably
optimal algorithms provably
asymptotically optimal algorithms to
solve this problem so we're going to
compare with those this will give us a
metric of how good an agent are we able
to learn compared to the optimal ones
for this kind of environment it turns
out that our l squared is competitive
with the asymptotically optimal
algorithm get us indices this is pretty
interesting because that means that we
can do as well as the asymptotically
optimal by just learning the algorithm
rather than designing it here's another
environment here the tasks distribution
ease will always be controlling a
cheetah robot but it'll vary what the
desired speed in Direction is so we hope
for here is as a as the result of meta
training an agent that when dropped in
this new environment
learns to control this cheetah robot
almost instantly at the desired speed
and indeed this what's happening here
what you're seeing here is the forever
tasks now run at speed 0.5 is the task
then now the task is run at speed 1.0
what we're watching is the very first
rollout of this agent in this
environment under this task
specification a training time had seen
other task specifications that I have to
control the cheetah for the test time it
gets new ones and is able to adapt
instantly we can do the same thing for
an Throwbot when you see here is task is
run forward we see the first episode is
the first episode now for the task of
running backward within the first
episode it masters the task again first
episode masses the task of running at
speed zero now 0.5 because this is a
somewhat narrow task distribution so you
might expect this is gonna work
reasonably well because it can focus on
a very narrow task distribution here's
something wider that also involves
vision here our agent has to navigate
and maze it's never been in before
it just gets monocular images it's
supposed to navigate this maze to go to
a destination it doesn't know where that
destination is we can see the
destination because we have a map that's
just for our purposes to watch the agent
act but the agent doesn't have that map
so we're hoping for is that if this
agent is really well trained for this
you can drop this agent into a new maze
and in that new maze it'll instantly
explore the maze in a very effective way
and then go to the target and then if it
gets dropped in that maze again it'll
quickly go to the target because it
remembers what it's seen before this is
random exploration doesn't do much good
here is the faster i'll agent that has
been trained on many many mazes in the
past now gets dropped into a new maze
again does not have access to this map
just gives the monocular images and
somehow it has figured out a strategy to
adapt to this situation and when it gets
dropped in the same environment again it
instantly goes to the goal it doesn't
work all the time in that these are the
meta learning curves about 2/3 of the
time meta learning succeeds about 1/3 of
the time it doesn't succeed why might
meta learning not succeed now she's two
reasons they might not succeed
one reason is overfitting you overfeed
to the major currently faced with and
you learn to solve that one instead of
learning to generically solve mazes
that's always the case in learning you
need to avoid overfitting K'NEX also
under fit here in my view that you just
don't get enough signal to even get any
rewards and be able to learn this
one way to propagate more signal is to
maybe put a slightly different structure
underneath
so one thing we looked at is to put
underneath instead of a recurrent neural
net which might have a little trouble
trouble over long horizon to propagate
signal underneath put a wave net like
architecture but of course then we lose
some details and we add on to that
attention so we can also look at the
details from the past this architecture
called snail can then be swapped in for
the RNN and you can run the exact same
kind of process as we did before so in
South it actually gets more signal is
able to do a little better on the
bandits problem than RL squared and it's
also able to solve bigger mazes here's
an example of such a big maze you see
the agent the trained agent here
starting off took a battering
enforcement but it doesn't know that
because it doesn't have the map it just
gets the first-person imagery it very
efficiently explores this maze doesn't
waste any time doing things that are
unnecessary realize at the end of the
maze there is nothing more to be done on
that side starts working its way back
and what we see here is that it's a very
efficient Explorer of mazes it's never
seen before and again this is all from
first-person vision one more unlucky
turn but then final thing it explores is
actually gets to the destination again
this amaze is not seen at training time
what can even put more structure
underneath if we'd like in mammal our
starting observation is computer vision
practice so what do people often do in
computer vision this is beautiful data
set called image net you train on image
net have your neural net trained on
image net and then you have an actual
task you want to solve and you fine-tune
and the actual task actually works quite
well so some questions from there are
how to generalize this to behavioral
learning rather than image recognition
and instead of just training on
something and then hoping for the best
that fine-tuning will do the job can we
end-to-end learn to be ready for
fine-tuning when the real new task
arrives so the key idea we're pursuing
is can we do end-to-end learning of a
parameter vector theta that is a good
initialization for fine tuning for many
new tasks
so at this time what we're going to do
is fine tuning what does that mean we
have a pre trained parameter vector
theta we'll get a new task we evaluate
performs on a new task computer gradient
do an update and get a fine-tuned
parameter vector theta Prime now the
hope is that this parameter vector theta
prime is a good one for this new task if
that's the case we were successful so
how do we train for this turns out we
can train this end to end a training
time with a distribution over tasks and
we tried to find a parameter vector
theta that no matter what task you
sample and from the task is sample a few
training example a few validation
examples you take a grade in based on
the training examples and after that
gradient the fine-tuning step it should
do well on the validation examples if
that's true that parameter vector theta
is really ready to be fine-tuned from
okay so pictorially what this looks like
is that there are many many tasks they
all live in some parameter vector space
theta there's the solutions to those
tasks and what we want is to find this
parameter vector theta that is somewhere
between the solutions to many tasks and
ready with one gradient step to jump
onto a solution to a new task turns out
I'm not going to show that the same kind
of videos again but it gets competitive
performance on the banners problem
cheetah and that we already watched such
as some interesting theory to mammal has
recently been shown by Chelsea Fitness
circa 11 to be fully general what's
meant with that is that even though it
seems like it's law constrained because
it relies on fine-tuning as the
underlying learning rule whereas our l
squared and snail can rely on anything a
recurrent neural net or a wave that can
represent this relies on a gradient step
but it's actually possible to prove that
it doesn't lose any generality by
relying on this gradient step really
cool so Madeleine is not only done for
reinforcement learning
sascha been on quite actively for
optimization and I'll share the slides
after the talk so you can go read a lot
of these references if you'd like it's
done quite a bit for classification
interesting fact is that while mammal
and still were not designed for
classification they also work for
classification and on release of each of
the papers they were as a state of the
art on also meta learning for
classification
not just on reinforcement learning exact
same approach you can apply this to
generative
models and actually there's also quite a
bit of work in reinforcement learning
and the later references we'll touch
upon in the remainder of this talk so
there's not just faster reinforcement
learning that I think we'll need to
solve the AI robotics puzzle we also
need to learn to reason over longer
horizons than canonical our algorithms
will do for us so why would we want
hierarchy well imagine you have a home
robot and maybe you say well it only
needs to do ten tasks today maybe some
dishes and cleaning some cooking 10
total 10 decisions to be made but the
truth is those 10 decisions need to be
incarnated in something below it maybe
moving somewhere grabbing something
placing it somewhere and repeat that a
few times so really then it's a thousand
time steps the robot house in its day
but actually going somewhere require as
many footsteps and so maybe the robot
really has a hundred thousand time steps
in its day and footsteps require
commands to be sent to motors and that
might be needed at 100 Hertz so actually
really we have about 10 million time
steps in the robots day that's a very
long horizon compared to what we've seen
in pretty much any reinforcement
learning results so far to go for 10
million times to learn something over
the horizon very big open challenge of
course people have been making progress
on this challenge but I think it's fair
to say that this is far from solved
there's a lot of opportunity to to make
progress here here are some examples of
some older and recent progress on this
problem and they all center around
defining temporal abstractions or even
learning temporal abstraction models to
make this more tractable dangle I want
to highlight here is that actually we
can formulate this as a meta learning
problem the meta learning problem is as
follows the agent has to solve for a
distribution of related long horizon
tasks with the goal of learning new
tasks in the distribution quickly if
that's how our objective then hierarchy
should kind of fall out now it might be
a lot to hope for that I'll just fall
out we'll give it a little more
structure so we'll have a master policy
which acts infrequently theta and the
sub policy is Phi 1 Phi 2 Phi 3 and so
forth which get to act on full control
rate the master policy
sighs at any given time which of the sub
policies is active and the meta learning
objective here is can we somehow find
sub policies Phi 1 Phi 2 Phi 3 such that
if the master policy has access to those
and gets dropped in a new environment
it's never seen before relying on those
sub policies is a very effective way to
master those new environments so this is
what meta learning looked like before
now we want this hierarchical structure
we changed it a little bit we want this
template to be incorporated so we're
training for is Phi Phi's the
parameterization of the sub policies we
want to find sub policies such that if
we randomly initialize the master policy
with some theta 0 and then this master
policy gets to act in a new randomly
sampled environment M for capital K
episodes it should collect high reward
now the master policy is not just a
policy oh I should be a reinforcement
learning agent that will improve over
time what it's doing so you want these
sub policies to be such that an agent
acting on top of it benefits from them
and can learn quickly the agent on top
of it theta you might may be trained in
the future with some meta learning like
we've seen with our l-squared snail
mammal and so forth and the experiments
I'm going to show to you the RL agent is
just running PPO here's an example you
could test this on moving bandits it's a
bandit problem but the bandits are not
just available to you to choose from you
have to walk to them run to them - be
able to pull them so what you'd hope for
in this scenario is that a sub policy
would correspond to moving to the green
object another sub policy would
correspond to move into the blue object
and indeed that's actually what emerges
we don't have to hard code this into the
system we just let it discover two sub
policies that make it effective to
explore this environment when you
encounter a new incarnation of this
environment and that those are the sub
policies that are learned here's another
example let's say your aunt needs to
navigate a maze very low level control
needs to happen as well as high level
decision-making about where to go you'd
hope that at the low level you invent
locomotion gates at the higher level you
invent something about how to navigate
and explore mazes in fact that's indeed
what happens and the beauty here is that
we don't explicitly train for any of
these gates these are the gates that
emerge
as gates that are helpful to help the
high-level policy to quickly learn to
collect reward in a new maze reinforce
winning is great but there's actually
other things we want from our robots we
want to be able to task them tell them
what to do and one way to tell them what
to do is to maybe teach them by giving
examples it's been a lot of success with
imitation learning in robotics in fact
there's been success in helicopter
flight legged locomotion predicting
where taxes might be going learning to
tie knots and placing dishes into a dish
rack and many many more here's the
typical paradigm you collect many
demonstrations then you train something
from those demonstrations to match in
some sense what's happening in those
demonstrations and then after that you
deploy to learn policy and hopefully it
does well and I've actually often it
does want to assemble a chair you get
demonstrations for assembling a chair
you learn a policy and you deploy you
want to assemble a table same thing and
so forth now that knots a great thing
about this is that any time there's a
new task you start from scratch you need
a lot of new demonstrations humans are
not like that for a human you can give
one demonstration of a task when they
get it maybe not when there is zero
arrow but once they're a little older
and they've seen the world and
experienced what tasks can be like then
one demonstration is enough because they
can tie it into what they've seen in the
past so we'd like the same thing here
once you've seen many demonstrations can
you extract the essence of what a
demonstration is so one demonstration is
enough to learn something new so with
one here is a one shop imitator to be
learned something that can take in one
demonstration and that's enough to do
the right thing based on having seen
many things in the past of course but
then it's going to learn something new
in the future for example demonstration
of task F or G and learn it from just
one demonstration how do we train
something like this well here's one way
to do it
the red box here is the one shop
imitator neural net so that's what we're
training the way we're training it is by
saying we'll have two demos of each task
in our meta training distribution demo
one of tasks a demo two of tasks
the one-shot imitator gets to look at
the entire demo one parse it in whatever
way wants maybe run a
currently rolling it over to make sense
of it then he gets to see one frame from
demo two of the same task it also knows
what the person did in that situation
and has to predict the corresponding
mode motor torque so this is a
supervised learning problem to train a
one-shot imitator if you train it with
enough tasks you'd hope that it would
generically learn the notion of one-shot
imitation we tested this on block
stacking especially a hard task we're
doing low-level control here over very
long horizon it stacks seven blocks
there's many many low-level control
steps you need to go through to do this
the task distribution here is random set
of blocks in front of you in random
locations one demonstration of how they
should be stacked maybe a.m. to be Co
Ltd tail to eyes something like that and
that one demonstration you need to learn
what to do with the same set of blocks
in a new initial configuration what
we're watching here is on the left of
demonstration and on the right the
one-shot imitators policy and we see
socially doing the right thing it's
picking up the right blocks placing them
up to the right target blocks the
highlights at the bottom that you see
those are the attention models so the
bottom right shows you that right now
the launch of the material policy is
paying attention to blocks F and G all
four heads of attention are in F and G
on the left the horizontal axis is time
so you can see what times the marshal
imitator is paying attention to in the
demonstration too soft attention so
effective it looks at everything but we
highlight where it pays the most
attention to and so we see that just
from one demonstrations able to learn to
stack blocks this here was done in
states based so we represented the state
of the blocks we also looked at this
doing this from raw pixels so here we
did it with mammal so a different way of
training again mammal is where you have
a pre trained parameter vector theta
that's a neural net that goes in this
case from pixels to actions and you get
a few examples for a new task in this
case you'd behaviorally clone train on
those few examples and it should then be
ready be fine-tuned to solve that task
itself we actually have different
objects at meta training time that I've
met a testing time so when we show a
demonstration to the robot that
demonstration is using object
it's never seen before on the left is a
demonstration placing the object into
the white bowl okay it sees that
demonstration once and then it learns to
do the right thing what we're seeing
here is the robots view the robots is
taking in raw pixel values and deciding
what to do with the end effector all the
way a tropic CIL's to motor commands if
now the demonstrator demonstrates
something else placing it on the yellow
placemat it learns to do that just from
one demonstration okay so we've covered
some reinforcement learning some
imitation learning it's another thing
that's going to be really important for
long robots in the real world which is
lifelong learning again many challenges
there
what is lifelong learning well let's
first take a look at what we do right
now in machine learning
typically step one is your run machine
learning step two you deploy very
canonical way to put machine learning
into practice so all the learning
happens ahead of time before the
deployment but in the real world often
things are non stationary and what you
learn from past day that might not work
anymore later
so really you'd want something that can
handle those ever-changing situations
you want something that still learns
during deployment that spirit is a
lifelong learning spirit it's actually
been quite a bit of work on this and how
to get systems to continue to learn and
the angle I want to look at here is the
continuous adaptation of behavior angle
so where you look at can we train an
agent to be good at dealing with
non-stationary environments so when we
did meta RL so far we trained an agent
to be good at adapting to a new
environment but now I want an agent to
be good to adapt to a changing
environment a constantly changing
environment so again we can phrases in a
meta learning way what we're trying to
find is an agent parameters by a vector
theta that is such that if it faces a
sequence of tasks that it is good at
solving that sequence of tasks and those
tasks might have some dynamic pattern to
them such that that meta training time
you can start understanding what
typically happens as things evolve and
anticipate some things that'll happen
later what are some examples sources of
now
stationerity that we can infuse
obviously in the real world there's a
lot of non-stationarity but it's
difficult to do machine learning
research in the real world so what can
we do in simulation maybe one source
could be changing the dynamics something
we looked at in the paper but I'm not
highlighting on these slides here
another thing you can look at that makes
a little easier on the designer of the
environments it's actually competitive
environments so what this means is that
you're in an environment where there's
other agents these other agents are
trying to beat you to it and this other
agents are running reinforcement
learning so they're constantly changing
trying to become better at beating you
it's not the only way you can do well in
this kind of environment is by yourself
continuing to adapt actually hopefully
adapt more quickly than the other agents
so we can beat them to it it's a very
natural way to infuse more and more
variation just to longer you run things
we evaluated in the context of Robo sumo
wrestling in Robo sumo wrestling you're
supposed to either push the opponent off
the tatami or are you supposed to flip
them all around to their back if you do
that you win if the other robot does it
to you
you lose okay
so let's watch some robot sumo what
should we pay attention to here
initially the green one the one six
legged bug robot is winning most of the
time it's actually been trained to be a
little better than the other one but as
they play each other the green one is
just running standard reinforcement
learning and the red one the four-legged
one is running what has been trained
through meta learning to be a fast
adapter in non stationary environments
and we see that that allows the red
agent the ant robot to actually start
dominating over time and so we see here
the benefit of being ready to adapt
quickly in this kind of adversarial
environments in fact we can highlight
that even more by looking at a
population study we let a bunch of robot
spiders bugs and ants lose in this
environment where you get let loose what
it means is that you get randomly drawn
against another opponent you play four
hundred times Deusen during those
hundred times you're both probably
adapting your strategy to be better in
the next round against that same
opponent after hundred times we evaluate
who won the most that one survives the
other ones gone and we repeat
what we see here is that the mental
learners the ones who have been pre
trained to be good at adapting dominate
this kind of process one other thing
that I think will be really important to
become better at if you want to deploy
robots in the real world is to leverage
simulation simulation is really helpful
it's less expensive is faster more
scalable it's less dangerous it's easier
to get labels because they're built into
your simulator but the challenge of
course is how to get useful signal out
of your simulator for things you want to
later do in the real world well one
approach is to just build really
realistic simulators you do that you're
all set and a lot of great efforts have
happened in that direction a slight flip
side of this is that you need to build a
really good simulator which is not easy
to do and that simulator might be
somewhat computationally expensive
because let's say you want to simulate
the entire world or something that's
going to be one expensive computation to
run something more intermediary would be
to say I'm going to have simulation that
is not necessarily perfectly reflecting
the real world but I'm also having some
real-world data I'm going to find a way
to line them up implicitly or explicitly
maybe through the main confusion or
domain adaptation type techniques and
that might allow you to learn more
quickly from a smaller amount of real
world data thanks to the simulator data
that is associated with it and that's
actually being quite successful the
direction I want to highlight then which
i think is it's kind of surprising how
all this has been working
something then kind of the success was
really surfacing over the past year only
its domain randomization the idea here
is that if the model sees enough
simulated variation even if none of it
is realistic like you see here on the
left none of that looks like a real
image of a robot in front of a table
with blocks they're all clearly rendered
images even if none of that is realistic
maybe there's enough variation in
lighting texture viewpoint occluders
what you learn in that simulator could
directly carry over to the real world
looking at this after seeing some
results from first-day sadeghi and
Sergei 11 who looked at this in the
context of training a quadcopter purely
in simulation to then be able to
navigate reliably in hallways in the
real world it's really surprising to me
that this worked so we started looking
at this than in the context of robotic
manipulation can we train purely in
simulation and train a system that just
some simulation can estimate reliably
the pose of a block of interest and then
pick it up and in fact this was possible
so the neural nets controlling this
robot got no real-world training data
only simulated brainerd images that are
not that realistic I was able to in the
real world identify the block that he
was supposed to identify now it's
supposed to find the hexagonal block and
it actually finds it never having used
the training image from the real world
what's maybe even more surprising is
that we didn't even need any pre
training so it would compare this with a
few pre trained on imagenet and then
training simulation versus you just
train in simulation and after about
8,000 simulated examples performance is
a tie between the two ways of doing it
now simulation can hopefully get us far
but it's still the case that likely will
need real-world data and so I think one
important thing we need to continue to
face is that we want to extract as much
possible signal from our real-world data
so maybe some of you I guess the way
nips is growing only half of you were
last year at nips and might have seen
Yamla Coons keynote and this keynote
featured a cake okay
so this cake has been reused a few times
since and what is this cake representing
so this cake is a chocolate cake with a
cherry on top we all know the cherry is
a special part now Yama Cohen said the
cherry is reinforcement learning and it
was a whole explanation about this and
that the cherries reinforcement learning
what does that mean is this tiny low
thing on top it's really important it's
really cool you really want it but it
doesn't give you a whole lot of signal
so maybe you need to do some other
learning too frosting higher volume than
the cherry corresponds to supervised
learning you get more bits for example
and supervised learning than you do get
in reinforcement learning and then of
course there is de big mass the cake
itself and that's representing
unsupervised learning anytime you see a
video the next frame you could try to
predict them that's a lot of bits of
information you're trying to predict now
of course in reinforcement learning it's
not the case that people only look at
the cherry in fact there's a lot of work
where people look at the entire cake and
train reinforcement learning agents to
take signal from both theory reward and
auxilary signals for example is enable
to solve in complicated mazes like the
deep mind lab on mazes so what happens
here underneath is that you set up a
neural net that has multiple heads one
of the heads is still the policy output
or the cue function output but then
other heads correspond to maybe next
frame prediction things like that
auxiliary losses and this allows you to
learn good representations at the core
of your neural net that can be used for
both the artillery loss prediction and
for the policy output I think is very
promising we need to continue pushing
this very hard what I want to highlight
here though is something a little
different is that we don't need to
restrict ourself to this kind of kick
how about this kind of kick so this cake
has a lot of cherries and I would prefer
to eat a cake with a lot of cherries
because I like reinforcement learning so
the question now is it's easy to put up
a picture of a cake with more cherries
and even make one maybe I don't know
I've never made one but I think it's not
too hard the question is can we actually
tie some reinforcement lighting ideas
into it rather than just show a picture
one way to get there is an idea called
hindsight experience replay especially a
paper at the main conference this year
builds on a lot of prior work in similar
directions
of trying to infuse more signal into
your reinforced learning Asian but
signal that's more directly related to
reward rather than just auxiliary losses
and our work here builds very directly
on the universal value functions from
Tom Shaw and collaborators presented at
ICML 2015 so what is this einside
experienced replay or this cake with
many many carries the main idea is that
we want to get reward signal from
anything the agent does not just from
success but reinforcement learning tends
to give reward Lenoir success are bad
failure only not from other things so
we're going to do is we're going to
somehow after the fact assume that
whatever the agent did is success as
we're discussing this at open er the way
what you'd like to put it was you know
your agent goes is supposed to get ice
cream for you but it ends up in the
pizza place it brings you pizza you
could say is your reward nothing to be
learned that's classical RL with one
cherry or you could put all the cherries
on the cake and say hey this agent
should have learned now about how to get
pizza for you that's what we're gonna
try to do what you need is a different
representation you can't use a classical
cue function we just stayed in action
you need a goal explicitly in your cue
function because once you have that you
can learn against multiple goals at the
same time and you can in hindsight
infuse goals that you achieved even
though they were not really your goal
when you were acting so we still have a
replay buffer collecting experience in
that replay buffer standard cue learning
with experience replay would replay that
experience by doing stochastic gradient
descent on this loss over here often the
reward will be 0 will be 0 signal in it
what we can do now is whatever happen we
call our goal in hindsight we replay off
policy clearly but Q&amp;amp;E works well off
policy no problem there so you replay
off policy infuse a new reward a new
goal associated with that reward and get
signal from everything the agent has
done once we do that we can get some
pretty complicated things to work with
pure model free reinforcement learning
no auxilary losses except for this
hindsight experience replay idea so on
the left here you see what happens if
you use
hindsight experience replay after 20 a
box is a 7 percent success rate 30 a box
30% success rate and on the right you
see the exact same algorithm exact same
thing is happening on the right except
when it replays experience it does not
hindsight swap in the more signal full
reward or gulf same thing is true for a
task like sliding by after the fact
infusing goals and rewards that
associated with what you achieved rather
than what you were trying to achieve you
get a lot more signal and can learn
pretty complicated skills in a
reasonably small amount of time
comparing the learning curves we see
that indeed also over many runs this way
of learning is quite a bit faster than
if you don't do the hindsight experience
replay one thing to keep in mind here is
that you need an off policy method for
this idea to apply very directly because
you're infusing off policy information
in your replay so one thing I've hope I
hope I've done in the 40 minutes so far
is to tell you a little bit about what
is so exciting about working in AI for
robotics and how there are many many
challenges ahead of ourselves and it's a
good time to be working in this space
there's many more challenges on the ones
I have touched upon so it's an even more
exciting time than I alluded to but now
for the last couple minutes what I want
to do is take a little step back on what
I discussed and see we're at a higher
level maybe things are headed so one
thing you might have noticed that a
recurring theme in the talk was meta
learning meta learning this meta
learning that was often the solution to
the challenge not the solutions wrong
with wooden and it's some progress on
these challenges the beauty of meta
learning is that it enables discovering
algorithms that are powered by data and
experience rather than just human
ingenuity and I'm inclined to think that
the more data you can put into your
arguments the more experience you can
put into your algorithms at some point
you're going to be putting in more
experience on any single human has ever
experienced themselves and some
principle your algorithm should be able
to be better than what any human can
design or may be all you must combine
can design
and in fact of course it will acquire
more compute but the truth is that as
machine learning is becoming more and
more monetarily valuable we also see
more and more advances in computed to
machine learning companies like Nvidia
Intel Google graph core Cerebrus and so
forth are going to give us more and more
compute in the next few years which I
think will empower even more so the meta
learning approaches to solving these
kinds of problems thank you
hey what a wonderful talk alright so any
questions please pumps the microphones
hey well well people are doing that and
maybe I'll just ask one initially so you
know one high level thing of course
admits I guess is dealing with
uncertainty in the world
okay so Rob's question was about how to
deal with uncertainty in this kind of
settings I did not include it in this
talk per se but I think that also ties
very closing to things like safe
learning and combining planning and
learning so whenever there is
uncertainty ideally your neural net
would be aware of this kind of
uncertainty and would account for that
in its planning learning to maybe make
safer decisions that you could make
otherwise so one angle of safety I don't
even have a slide somewhere here not
sure if I can find it quickly one angle
of safety is how do you safely learn how
do you not destroy the system during
learning and that's where I think
Bayesian approaches become become very
valuable how do you understand the
amount of a certain that you still have
so you can maybe fall back on a safer
policy and just what's mean optimal
another angle for safety that's I think
is very important to investigate is how
to continue learning once you already
have 99.9999 percent success rate
because that might not be enough to be
better than a human driver let's say and
so how do you keep getting signal once
that starts happening very important
problem another really important problem
there is once you build really good
attic contraptions how do you make sure
they align their value system with our
value system because at some point they
might be smarter than us and it might be
important that you know they actually
care about what we care about and how to
infuse that kind of caring our value
system is another big safety challenge
so yeah a lot of challenges they're very
important to think about hi Peter thanks
for an awesome talk I just had a
question regarding lifelong learning so
and I'd like to know some some of your
views on how one specifies change
set up for better evaluation of
algorithms yeah so that I would say both
in meta learning for RL and in lifelong
learning I would say one of the biggest
maybe the biggest challenge is to setup
good evaluation environments for these
approaches it's clear you wanted them
the real world it's also not clear how
to do research in the real world that's
effective and fast and how to set up
good evaluation environments is really
really hard so that's where our thinking
has been leaning in the direction of
competitive environments a little bit
because that lets changing environments
emerge but that's just one direction and
sumo wrestling is obviously still quite
a constrained environment and not as
rich as you would want it to be but
maybe if you build richer environments
rely on something like minecraft where
there is more opportunity to invent
interesting strategies maybe even more
interesting non-stationary effects will
emerge but see that's a very open
problem to even define the right way to
evaluate these approaches thank you okay
may I add something sure yeah yeah so I
do see that a multi a multi agent setup
is one really good way to specify change
and I guess there are ways of
understanding complexity of these
systems from some mathematical
perspective as well which could help
with evaluating and telling them against
the how complex system more emergent
system is I don't know if I made any
sense there I'm not I wasn't sure that
was a question yeah it was just a
comment okay just okay thank you thank
you
oh hi Peter one question is for adhesion
if we want to contribute to the study of
metal learning what has a key problems
which should look into
- machine learning in general or salsa
method learning Oh metal learning so I
would say each of the sub disciplines of
metal learning has challenges that is
meta learning for optimization for
supervised learning for unsupervised
learning and for reinforcement learning
then within those I would say one of the
big challenges to even understand what
environments you should training so
that's a big challenge what are the data
sets or the environments to work with
another big challenge is designing new
architectures so the architectures you
see here are just a few examples of
architectures but what we've seen in
disciplines like computer vision is done
new architectures emerged regularly that
I perform past architectures and it's
quite likely the same will be true for
meta learning that the architectures
have shown to you here are by no means
that once people will be using two years
from now maybe not even a year or a half
year from now there's a lot of room for
designing new architectures especially I
think architectures that have more
explicit memory aspects to them we
explicitly store information retrieve
information do things like that then
algorithmically some of the challenges
tie in to overfitting which you need to
be careful about because you have a
distribution over tasks you don't want
to fit to the one task you're currently
training on too much and then under
fitting which is when you have a
distribution over tasks how do you make
sure you extract the essence of that
distribution over tasks rather than
getting no signal at all about the
essence and so there's a lot of
challenges in that regard one place you
might also want to look is rocky Duong
third person on the picture board here
is posting its thesis next week and the
last chapter of the thesis has a bunch
of suggestions on interesting future
work directions within meta learning
there might also be a good resource to
look at we're gonna have to sell
questions here and move on to session
is this gonna be a short break to allow
people to move to the other hole if they
want to attend the other session</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>