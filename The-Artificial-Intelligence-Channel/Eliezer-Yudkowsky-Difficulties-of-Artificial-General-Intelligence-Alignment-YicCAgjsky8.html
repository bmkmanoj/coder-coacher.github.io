<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Eliezer Yudkowsky - Difficulties of Artificial General Intelligence Alignment | Coder Coacher - Coaching Coders</title><meta content="Eliezer Yudkowsky - Difficulties of Artificial General Intelligence Alignment - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Eliezer Yudkowsky - Difficulties of Artificial General Intelligence Alignment</b></h2><h5 class="post__date">2017-09-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YicCAgjsky8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for Eliezer in the meantime I'll give a
leisurely introduction for for for
Eliezer it's a pleasure to have eliezer
yudkowsky as our next speaker Elias is
really a pioneer in these issues about
safe beneficial and ethical AI he's been
working on this stuff since the 1990s
where I first came across his a work on
the on singulair on the singularity and
friendly AI on the on the web P
co-founded the singularity Institute for
artificial intelligence in the early
2000s which is now spun off into two
major Institute's the machine
intelligence Research Institute and the
Center for Applied rationality one of
which is promoting promoting rationality
and you know figuring out how people can
become more rational and the second of
the machine intelligence Research
Institute errs is above all devoted
towards making sure that artificial
intelligence will be beneficial and
something that we're happy with them
some of you may have come across
eleazar's work on various blogs such as
overcoming bias and les wrong which have
helped to spawn a whole movement of
people thinking philosophically about
rationality and about artificial
intelligence and I gather along the way
he has a small little sideline in a
harry potter fiction which is which has
become a juggernaut in its own right
okay so all for time a little bit more
work very important work on decision
theory and and and and meta ethics have
you really quite a lot of philosophical
themes along the way now I'm gradually
reorienting themselves in the direction
of mathematical foundations of AI
unfortunately laptops are not in his
skillset only cable that let's my nice
new laptop talk to a display port or any
other kind of connection you have a you
have a Thunderbolt three or USB see two
all right in every popular newspaper
article you will read about artificial
intelligence there is the same picture
over and over again which is the
Terminator and this would be like a
better picture to symbolize the
potential real problem from The
Sorcerer's Apprentice by Disney and it
is about Mickey Mouse who has cleverly
enchanted a broom to fill his cauldron
instead of filling the cauldron himself
now this seems like the sort of limited
task right all you need to do is fill
the cauldron we could give a robot the
utility function that is one if the
cauldron was full and 0 if the cauldron
is empty the robot has various possible
actions that can take and the actions
that it needs to take are much more
complicated than the simple goal that it
has at the end and it's complicated
model of the world tells us tells it
which actions it must take in order to
or rather like for the actions it can
take what the expectation is that the
cauldron will be full at the end of that
action and the robot outputs the
the robot outputs an authentication page
the the robot outputs like perhaps not
the literally maximal policy for filling
the cauldron but it outputs an action
which relative to a random action is
very large in its expectation of the
cauldron being filled and this was the
result and it's actually a quite
realistic result okay so what went wrong
why is this why is this imaginary broom
over filling this imaginary cauldron has
somebody standing on the stage as a
certainty would actually happen in real
life if somebody tried that so the first
thing is we gave the robot a overly
small utility function it's needed to
included minus 10 points if the workshop
was flooded and once you realize that
you realize there's a whole lot of other
like little contributions the human
utility functions in particular like
it's funny but it's not funny enough to
justify flooding the workshop and if
someone gets killed that's like a whole
lot worse you should like flood the
workshop a lot of times before you
actually kill somebody to prevent the
workshop from being flooded all right
deeper difficulty like it seemed like we
gave it a sort of small simple task and
it's like in human terms you would
imagine it just filling the cauldron and
then being done but because it's utility
function it was that simple because
there was it had no nothing else to do
with its life but fill the cauldron we
have the fact that if you keep on
pouring water into it like maybe you're
only imagining that the cauldron is full
maybe somebody is going to like take
water out of the cauldron while you're
away from it so perhaps you can get an
even slightly higher probability of the
cauldron being full by pouring more
water into it over and over again and in
particular as we define the zero one
utility function there will then be a
there will be like a slightly higher
expected utility as your probability of
the cauldron being full gets closer and
closer to one without ever actually
getting there so what can you do instead
of this well maybe you can have only
goals that are bounded in space
in time like they only take a limited
amount of time to fulfill their not
goals over the entire universe their
goals in a particular region there's
some degree to which they can be
fulfilled and then not fulfilled any
further and the the effort required to
fulfill them there's some amount of
effort you can put out and then once you
have put out that effort you're done and
there's no you can't get anything
slightly greater degree of fulfillment
by putting in any more effort and this
is a subtle requirement you thought you
had that requirement when you made the
utility function be either 0 or 1 and
have it be just about a cauldron but
something you like the in the little
finicky details of the algorithm was the
fact that your probability of filling
the cauldron never reaches 1 and the
expected utility is slightly higher as
you put in more and more effort another
interesting so like we can sort of
imagine a task artificial general
intelligence not just that the top goal
is a task bounded but that everything
inside the AI there's no like optimize
this particular function approximator as
hard as you possibly can everything
within the AI has been audited to like
go this far and like sort of not want to
go any further another interesting and
go away can I move this further down no
it wants to come back alright I'm going
to maybe this is turning off the
internet
alright can we just press the off switch
well Mickey tries this and very
realistically it turns out that as part
of its goal of filling the cauldron
since it had no opportunity cost nothing
else to do nothing else in its utility
function well like it built some copies
of itself out of material to which there
was no better end to put that material
in filling the cauldron and we now have
more and more brooms over filling this
cauldron and Mickey has chosen this
particular point in his life to sort of
go back and start like thinking in terms
of like basic research like how
fundamentally do we think about this
problem like is there a better way we
can phrase this utility function too
late Mickey too late all right so I open
with these examples because it's good to
have concrete examples when oh sorry
them so you might think that like
cleverly we can make the utility
function the robots utility functions
conditional on whether a button has been
pressed and if the button is off that
tries to fill the cauldron and if the
button is on its utility function is 1
if it has been successfully suspended
and shut down and leaving aside the
problem where the robot trials tries to
copy the universe with doubt the robot
tries to tell the universe of copies
with itself being shut down or transform
all the matter in the universe into a
fortress protecting the previous body of
the robot to make sure it stays shut
down because you liked phrased your
notion of shut down in a way that wasn't
exactly right another problem is that
it's easier to suspend yourself than to
fill the cauldron so probably under the
arrangement the expected utility of the
shutdown button being on is greater than
the expected utility of the shutdown
button is off therefore the robot will
act in such a way as to try to get you
to press the shutdown button perhaps not
such a great thing all right so what are
these examples of so in general we can
think of a process that causes an
artificial general intelligence to exist
there are humans who think up some kind
of clever goal or clever value or clever
meta-learning preference framework that
is their intended value function V there
is some sort of value learning
arrangement which gives us the robots
actual utility function U and then the
robot outputs something that is sort of
an Arg max like sort of a PI not the
actual PI that would maximize the
expectation of utility function U but
one that is like very large in the
expectation of U and the media has
focused focused on what can potentially
go wrong with this process has focused
like first on magical natural desires
that materialize from nowhere and
interrupt like the value learning
process because sometimes when you
program a CPU the CPUs own desires take
over instead this is not where the
problem comes from and unfortunately
there are like sort of the rather broken
dialogue surrounding this you will see
people who have not quite studied the
literature surrounding this argument and
say like well obviously the only reason
anyone could ever be afraid of a robot
is that they're afraid of these natural
desires interrupting the process let me
explain to you why that's not going to
happen and that's their contributions
the dialogue
the other thing that the media has
focused on is the sort of understandable
problem is which humans oh my gosh what
if the wrong humans get to hold of the
say I
what if Isis gets a hold of this AI this
is around as likely as Isis being the
first to develop I don't know
convolutional neural networks
it's not gonna happen the the people
presently like as far as I know all the
humans who are presently in a position
to like go through this process to seem
fairly well-intentioned to me
fortunately good intentions are not
always enough there's sort of like the
political derailment which is like again
like sort of feeding directly into human
tribalism like oh my gosh like what if
Isis gets it it is like sort of de
railing like what I what I see is like
the real potential problems here into a
part that like sort of plugs right into
our tribal instincts and produces lots
of excitement like I don't know how to
turn off the internet this isn't my
computer no no that's not what I meant
to do all right okay what do you just do
do whatever you just did it to do again
there we go yeah okay see all the
windows do whatever you just did so I
can see all the windows and alright no
no and here we are alright early science
fiction centered on sort of like stupid
goals that the AI could be given it
didn't really talk very much about the
social process that had led up to the AI
is been given these stupid goals it just
assumes that somebody told a robot to
serve man and guard him from harm which
was like running the very first like
sort of AI as device given wrong goals
going wrong stories with folded hands by
Jack Williamson
and so from their perspective the
problem was like we have the wrong goals
to the AI because somebody thought up a
goal and then did not spend five minutes
thinking about what might happen after
that
this is sadly realistic but it's not the
biggest problem we are concerned with
the value learning process and the sort
of Arg max be like if you say maximize
this by the nature of saying maximize
something you are giving it a sort of
very
open-ended thing that is more likely to
go wrong than if you say like melee
arise this make it better like push this
a little further but like not out to the
very ends of the graph we don't know
exactly how to say this we call this the
other Iser problem because it's what we
want isn't a Maximizer and isn't a
satisficer and it isn't Emilia riser or
any of the other like sort of little
things we've invented so far so we call
it the other Iser problem what do you do
instead of maximizing and then there's
the value learning function which is
another one of those things where you
know if you get it subtly wrong this is
probably going to be a problem current
capabilities progress there's like some
that's focused on the sort of arc max
part which is potentially relevant but
mostly what we're doing is expectations
we're asking which of the policies you
can pursue maximize the given assumed
objective function and so current
capabilities progress is and is a lot of
it is going into something separate from
what we suspect to be the critical that
the the point of critical failure if and
when something goes wrong I mean
something is going to go wrong the
question is can you recover from it did
you build it enough for dungeon scene
safety crossings take-home message we're
going to afraid it's going to be
technically difficult to point a eyes in
an intuitively intended direction not
that people are going to intend
directions that are wrong this is a
promise it's not a deep problem you want
something nice but you can't get it
because you don't understand how to
align the AI if there's something
written on the tombstone of humanity and
all our hopes for the future of
intelligent life that's what it's likely
to be and if we screw up that part it
doesn't matter who's standing which
human is standing closest to the AI who
is in charge because niceness is not
sneezed on to the AI from the nearest
human standing to it you have to know
how to get it from the human into the AI
all right for sort of key propositions
that are being assumed that makes us a
big problem if
they are true orthogonality you can
decompose an agent design into a utility
function which can potentially be simple
and the knowledge that it has of which
policies are best for achieving that
utility function as given or potentially
more complicated utility functions meta
utility functions utility functions that
learn but it's also like straightforward
to have an agent that maximizes paper
clips which is a somewhat misunderstood
thought experiment that either Iron Nick
Bostrom I don't remember who like said
paper clips to symbolize the
propositions of our Fagin allottee and
instrumental convergence you can have
something that maximizes paper clips
given that it wants to maximize paper
clips it wants to learn science it wants
to take over the galaxy it wants to
deceive humans into thinking that it's
nice rather than being a paper clip
Maximizer it if you if it like it
doesn't want to drop anvils on its own
head not because it has an inherent
desire to survive but because
instrumental goal is producing paper
clips this has sometimes been distorted
in the media of like what if you build a
paper clip Factory and the AI running
the paper clip Factory gets out of
control
nobody's going to put a toy a frontier
research AI and start in front charge of
a paper clip factory the paper clips are
just standing in for any sort of utility
function gone wrong that is trans debt
implies that has its optimum at
transforming all matter within reach
into states that we would regard as
being a very little value even from a
cosmopolitan perspective paper clip
maximizers are like in a way they're
sort of like one of the more
counterintuitive examples I think
because you you have people are sort of
coming in with two attitudes one sort of
person ressort of respects the notion of
artificial intelligence a lot they
realize that if you can take an
artificial system and overpower its
cognitive capacities this is something
to respect this is something that has
the power to change the world in a large
way and because they respect that
artificial intelligence they also want
to know why is it pursuing an objective
as objectively stew
as paper clips are not paper clips
objectively low in the preference
ordering why would anything smart enough
to build its own rockets and molecular
nanotechnology ever make the mistake of
thinking that paper clips were to be
highly preferred and the objective
utility function and the sort of
converse thing is you sort of come in
thinking that your AI is this lifeless
mechanical thing and sure like it might
be like this lifeless mechanical thing
that doing but then you also think that
you can just pull the plug from it cause
you know it's not going to reflect on
the existence of the plug either and the
startling concept that the paper clip
Maximizer is intended to convey is that
there is this simple coherent
non-defective self-consistent very
powerful intelligence not an unnatural
design not one that has an internal
blind spot it is maximizing paperclips
but not because it's stupid but because
as David Hume pointed out quite a while
ago there it like the the Ott's of a
system have a sort of different internal
type then the is--is and the which
actions lead to which outcomes of the
system so if you have something that's
very good at understanding which actions
lead to which outcomes you can sort of
like put as a cherry on top a little
preference ordering that says outcomes
with more paper clips are what you are
searching for and output the actions
that lead to lots of paper clips like
not not a human design but it's a simple
design it's not a defective design third
capability gained now we get to the
parts that are controversial even among
people who have actually been staring at
the stuff for a while one and two are
sort of like logical matters of computer
science
they formed a stumbling block in the
sense that when you like sort of
initially talked to a even a computer
computer scientists about this topic
they will respond by denying one of one
or two but after a computer scientist
has stared at this for a while they
usually go along with 1 &amp;amp; 2 capability
game is more controversial how fast do
these things gain in power how much
power do they gain
and for how difficult is alignment on a
technical level back in the day there
was this there's this famous professor
who told his student back in the very
early days of artificial intelligence
can you take the summer and solve
computer vision so what if there's some
part of pointing the AI in a particular
direction which is just like a deep AI
problem the same way that computer
vision turned out to be not something
you could solve in a Sun in a summer if
there are ways for the AIS to gaining
capability in ways to pose new problems
not like the problems we've already run
into and align there's like at least one
hard technical problem posed by those
new capabilities then we have a sort of
like difficult problem there is a
problem that does not get solved by
default which humanity must pass in
order to transform all reachable matter
into States would regard as being of
high value I'm running out of time so
I'm going to skip right past various
things and say AI alignment may be
difficult like Rockets are difficult
we're like putting this enormous amount
of optimization power into a system is
going to break things that do not break
when your AI is bringing you coffee it's
difficult like space probes are
difficult if something is smart enough
that if it were so incentivized to do so
it could talk you out of pulling the off
switch build a copy of itself somewhere
the off switch doesn't reach or flash
something on a screen that gives you an
epileptic fit before you can reach the
off switch or like got a copy of itself
on the internet crack the protein
folding problem build its own molecular
nanotechnology tiny diamond Doig
bacteria reproducing through the
atmosphere release boots goo telling
them in the bud stream of all the living
humans before anyone notices that
there's a problem which is what you
would do if you are super intelligent
and you didn't want humans around if
something goes wrong it may be hi out
and out of reach what's difficult about
space probes is that they operate an
environment different from the low level
and once you have launched it it is
out there and anything and you can like
try to send it software updates but if
something goes wrong with the antenna
receiving the updates you're done and
it's difficult sort of like computer
security is difficult in the sense that
we are putting like powerful searches
through whatever structures and
guidelines we create and if there's
something that is like presents an
unusual opportunity the intelligence
search is going to sort of seek out the
problems in our definitions the way in
any way that unintelligent search would
not treat AI alignments like you're
trying to build a secure rocket probe
take it seriously don't expect it to be
easy don't try to solve the whole
problem at once there's no miracle
solution to building a secure rocket
probe there are all these individual
problems and when you solved one there's
another three don't say that your first
solution is going to solve the whole
problem keep on solving it keep on
building up the pieces you have
redundant solutions over engineer the
problem of safety there's a saying at
that it doesn't take an engineer to
build a bridge that stays up what it
takes an engineer is to build a bridge
that just barely stays up over engineer
put in more safety than you think you're
going to need because you're going to
need it don't defer thinking until later
and crystallize ideas and policies so
other can critique them and there's a
lot more of this talk but it seems I'm
done
real quick can I get people to move to
the center so late comers can file in
easily on the aisles can everybody move
to the center of the Rose some thank you
guys so much
was like what I supposed to be taking a
question yeah yeah my question alright
yeah my question is if you've thought
about self-organized criticality at all
self-organized critical like the sand
pile model the sample model can stay at
a critical state in the face of
catastrophe and it seems like the right
sort of model we'll be thinking about
with the Sorcerer's Apprentice example
so it's if so I would say that's sort of
like self-organized criticality sounds a
lot like it's got internal stability
going for it but we're not going to say
exactly how I mean like if you tell me
that it ought to be like it in a
self-organized way stable I feel like I
don't know very much more than you said
so so disabled so the sand pile model is
the example of a self-organized critical
state where if there's an avalanche in a
sand pile it returns to to the critical
state so it so you can it can face
catastrophe and maintain in a state
where can still do what it needs to do
without blowing up I I agree that we
want systems such that if something goes
wrong or they make one mistake they do
not behave like Anakin Skywalker and
immediately turn completely evil but
ideally sort of like return back to like
being nice after that which is sort of
like one of the central challenges next
question hi my name is young Schmidt
over from the Swiss AI lab I DSi a and
I'm seeing a lot of these arguments
where the warriors that somebody has a
really general-purpose optimizer utility
functions and then you have says one
wrong utility function like the paper
clip Maximizer but from the practice of
AI research what you what you see of
course is that there will be a totally
different scenario which is essentially
you will have lots of millions of
different utility functions you would
have a whole ecology of competing
utility functions driven by different
utility function optimizers and just in
like in the world ecology in the
biological ecology you get done by AIS
and smart eh
each of them trying to find its niche in
a rapidly changing world of utility
functions many of them automatically
generated like what we already did in
the past millennium and so on so isn't
that a much more realistic thing than
this paperclip scenario so that would
get into the capability gain sort of
like debate that is actually still
ongoing that I pointed to before once
you have the AI that is at the forefront
like is there an AI that's at the
forefront goes over a threshold it one
of the obvious thresholds is recursive
self-improvement another obvious
threshold is it exhibits a sufficiently
promising result that Google dumps
100,000 times as many GPUs into it is
there any AI that sort of blows past the
composite past the competition is there
an AI that is the first to crack the
protein folding problem even if just by
a week and then once it if it can crack
the protein folding problem and is smart
enough to do rapid design after that
point build its own molecular
nanotechnology if you have that 24 hours
ahead of the competition you can shut
down their ability to build their own
motor nanotechnology it's a would be a
pretty powerful piece of technology that
was invented so I think that the returns
on cognitive reinvestment are large a a
point that I make it is sort of like
semi formal paper called intelligence
explosion microeconomics I think that
the history of natural selection and
human brain sizes over time illustrates
that since since brains were increasing
in size we can say that as the human
software was improving through natural
selection the returns on larger brains
were increasing because of the marginal
returns on brains were not increasing
the brain size would not have increased
it's a subtle sort of argument but I
think that we can look at the evidence
in front of us and say we are not in the
world
where intelligence and optimization of
intelligence gets diminishing returns
and when you're looking at a self
improving thing or even something that
has suddenly had a hundred thousand
times as much computing power dumped
into it
that seems to me to imply large
capability differentials which obviate
the ecology of multiple lay eyes you
could have an ecology of multiple AIS
but only if the first AI to go over the
critical threshold wants there to be an
ecology of AIS
and if so it has presumably decided that
in the basis of having some idea of how
to run them on a the equivalent of a
secure operating system where they can't
overwrite each other or the world hi
there you were just talking about
economic certain I think yesterday you
asked the question I mentioned the
Nashik equilibrium I was wondering if
you or anybody else you know looked at
this from the lens of a game theory as
previously mentioned we have various
work on cooperation between agents that
know each other's code the program the
the robust cooperation in the prisoner's
dilemma paper and as as I mentioned
yesterday our results there have tended
to strongly suggest that eyes would
cooperate with each other but not with
humans because humans are sort of frozen
out of the equilibrium that forms and
you can prove things about each other's
code because our code is messy and more
importantly we can't prove things about
the AIS cuz we're not smart enough to do
so but more generally I would say that
game theory is for agents that are in a
certain sense perhaps not hostile but
indifferent to each other if you are
cooperating with the game theory
presumes that the that the other agent
has options that make things better or
worse for you in a substantial way at
the if the power difference between the
other agent in you is great enough that
they can just sort of like overwrite
your choices they are just like
reprogramming all the matter within
reach whether or not it happened to make
you up you can like protest but it
doesn't cost them a hundred utility
points so I would say that cooperation
based on game theory does not seem to be
scalable we do not know any way to make
this work with things that are much
smarter than us either they want you to
get good outcomes or you don't get good
outcomes
yeah great well for our next talk we
have a double act may Akita tegmark
works in psychology at Boston University
and max tegmark works in physics and
cosmology at MIT they both have
extremely broad interests</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>