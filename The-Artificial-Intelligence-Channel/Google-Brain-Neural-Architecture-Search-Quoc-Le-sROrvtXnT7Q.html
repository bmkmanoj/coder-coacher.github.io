<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google Brain - Neural Architecture Search - Quoc Le | Coder Coacher - Coaching Coders</title><meta content="Google Brain - Neural Architecture Search - Quoc Le - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google Brain - Neural Architecture Search - Quoc Le</b></h2><h5 class="post__date">2018-03-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sROrvtXnT7Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hmm we're gonna start our next session
and the first speaker I'm very pleased
to introduce Kwok lei who is a
researcher at Google brain and who you
may not know was named in 2014 one of
the top innovators under 35 by MIT tech
review thank you for the introduction my
name is Kwok Leigh I'm a member of the
Google brain team in a Mountain View in
this talk I'm gonna tell you a little
bit about my recent work with some of my
colleagues in using reinforcement
learning to design neural network
architectures so one of the most
important result in the last few years
in deep learning in my opinion it is the
use of convolutional neural networks for
images and this started in 2012 open
maybe a little bit earlier to and
especially in when they use it for image
net competition and started in 2012 with
Alex net and all the way to now we will
have better better neural networks and
if you see the figure on the left you
see that the progress of act and the top
one accuracy of human design neural
networks over time and this progress or
has always has been always about how to
design better new neural networks and
better training schemes for neural net
for convolutional neural networks and
you can go from 55 percent to more than
80 percent which is better architectures
and if you look closely into Inception
v4 which is the network about 80 percent
you will see the network can become
quite complex
so the inception v4 is composed of a
bunch of modules called inception
modules and in the inception module we
see a lot munch of average pooling
convolution and
so it can be quite complex so when I
thought about this I said if this is the
future of computer vision there would be
no chance that I can contribute
contribute to this research area because
it's so complex it takes so many years
of human expertise to design good
architecture like this so we just
recently started using more automated
method to design neuro architectures and
the key idea is that the piie activation
is that even though car and neural
architectures can be complex they can be
thought of as programs or like
configuration strings so let's suppose
that I have this configuration drink or
filter with five filter height of three
and number of filters of 24 I can pair
up reconstruct like one layer in the
neural net so the idea is to use a 9n to
write out this configuration string and
and then when you write out that
configuration string there you need a
passer to pass that configuration string
to construct a neural network and then
you take that configuration neuro
canoodle network and train it to the end
and then validate it on a validation set
and then you get a reward signal and
then you use the reward signal to
feedback into the controller so that the
controller can improve over time so the
picture of the method would look like
this so we have two system one is
seventh controller which proposes
machine learning models and machine
modern male model will be trained and
then evaluated some of them would be
good and some of them would be very bad
and the good ones who will give positive
feedback to the controller and the bad
one will be negative feedback to the
controller so that over time the
controller will sample better and better
architectures so in our framework the
controller again is like an it's an a
recurrent neural network and in
particular we use
the long short-term memory network and
the way it works is for following so we
divide the network into a bunch of
layers so we have layer n - one layer
and and then layer an n plus 1 and in
every layer you have some particular
conferee configuration the the neural
net has to specify for example it has
dissipation specify the filter high
field with stripe stripe width and etc
and for and then the way we trained it
is basically using reinforce or other
policy gradient methods and first of all
we have the first we have the reward is
basically the accuracy of the
architecture on the head our validation
set and then the parameter of the
controller will be optimized using
reinforce and then we can use reinforce
to expand the rule and compute the
gradient and train the controller and
enough so the first experiment we
conducted was on optimizing
convolutional nets for C 410 and we we
have to put it the knower of the field
with filter height strive with strata
and number of filters per layer for
example filter height we can select the
softmax can select between one three
five seven and then filter with the sub
max can select between one three five
seven and strike I strive with and etc
for example if the controller for this
particular layer right now the string
three seven one two and thirty six what
that means is the filter height is three
and filter width is 7 strike height is
one and stripe width is tool and the
number of filters is 36 that's basically
what the controller should write and
then it will concatenate these strings
for longer network so that you end up
with a bigger neural network and we
conducted a massive experiment on this
so will you
the would use 800 GPUs concurrently at
one time and the each trial network was
strained like 50 epochs and we'd read
for the total number of models that we
trained was 12,000 try models and in our
system we had because we run 400 cheaper
phones were cheap 800 GPUs concurrently
we break it down into multiple replica
replica and then the replica will
communicate to a parameter server photo
controller so that the controller can I
synchronously update the parameter so
that it can sample better architectures
over time so when we first applied this
to sefa 10 we end up ended up with
accuracy of three point six five
accuracy as our error rate on the on the
data set and dizzy basically on par or
slightly better than most of the
architectures that human invented and
the architecture that the network the
controller basically end up predicting
was this kind of network architecture so
I didn't talk about too much about the
skip connection but in our controller
you can also have a node to predict what
layer before and that it can connect the
input to so it can choose not to connect
to any input or any layer before or it
could predict one layer and the
controller basically selects many skip
connections and we also ran a control
experiment where we actually manually
connect our layer to all layers or other
layers and the results are actually less
good compared to this model so the
controller actually truly optimized for
skip connection and size of filters and
so on so one thing that I really very
interested in is not only doing on C far
but also on
and omission that is because imagenet is
considered to be one of the holy grail
in computer vision I would say and when
we try did the back back of envelope
calculation when we try the method on
image net we found out that is too
expensive it will take months to get the
result and so we we came up with a
different scheme to reduce the cost so
instead of running neural architecture
search to find the configuration for the
entire neural network we find a layer
and then once we have the layer we can
actually replicate it many times on a
bigger data set so here's the way it
works so on the left you see an
architecture that we manually
constructed for FIFA 10 and it is in
this network we have to cut of sales
normal cell and reduction cell so normal
cell is like a normal layer and
reduction cell is like a pool in layer
and then we want to run this
architecture search on sefa 10 not set
and once we found a good sell on a good
normal cell and reduction sale on sefa
10 we will replicate that many times on
image net basically the number of n here
1 again increase and we're gonna end up
with a bigger neural network and then we
are going to execute it on a mission net
so that way we don't have to spend a lot
of time inventing architectures directly
on image net so the when we ran this
method on sefa 10 we discover the
following two cells so the first cell is
the normal cell that have taken H I
minus 1 and H I and basically have a
bunch of combination method to arrive as
H I plus 1 so you can think of it this
is like an inception cell but it's a
little bit more complicated and it's
something that I would not
be able to figure it out myself and then
another cell that it this is basically
the pooling layer the pooling layer is
basically taking H I minus 1 and H I and
then also perform a bunch of combination
to give rise to H I plus 1 so these two
cells have been searched by a neural
architecture search and once we're done
with that we're gonna stack the stairs
many many many times so you construct a
much deeper neural network on imagenet
and train it to the end with you know 4
before the entire week now one nice
thing about this is that we can decide
how big the the network on imagenet we
can use we can stock up for example if n
equal to 3 we have a very small image in
that model and if n equal to a hundred
you end up with a huge deep model and
that's a flexibility that we can play
with and ok so in summary when we
applied it to C for 10 with the best
Network when N equals to 7 it is n equal
to 7 using the cut out method we
achieved 2.4 percent error rate on C for
10 and I think this is the best accuracy
published up today I think there must be
some other equivalent accuracy somewhere
on some other paper but they aren't per
patient is it probably the latest best
so far again when we apply the layer on
image net we can play around with how
many times we can stack up the neural
network so folks for example if we stack
up and equal to 4 you have a very small
neural network and on this axis on the
ax
this I plot the size of the model in
terms of number of operations so smaller
is better and on the y axis I plot the
top one accuracy so higher is better so
the best is trying to go all the way to
the top left corner of this figure and
as you can see the network that we found
actually better than the skyline of the
the models that invented my human expert
and including like the s in s but is
basically the best model people found so
far and I think that is the in terms of
accuracy we are on par but in terms of
size we are a lot smaller and similarly
in terms of number of parameters we also
a lot smaller than the stay of the
neural networks that human found we also
transfer the architectures that we found
to the data set called the cocoa data
set this is another holy grail data set
in computer vision for object detection
so I at the time when we applied it the
day of the art is around 39 percent
using a complicated loss function and we
didn't use it but the NASA net model
that we found was able to achieve 4
percent better than the state of the art
and 4 percent better in mean average
precision is a huge improvement in
accuracy of this model so we also open
sourced the National model so you can
actually download the model and use the
pre trained wait for your problem and
replicate some of the result in the
paper and yeah I I can stop there and
take questions
thank you amazing work
I was wondering how can we further
reduce the search space because 800 gpus
is a bit too much do you have any
thoughts on the future work how can we
further take it down so that we don't
all have to use 800 GPUs okay so I think
they've been active research on how on
trying to reduce the computational
complexity of this problem this approach
and I've seen Frank cutter like the
previous speaker he was talking about
using learning curve prediction that's
one area that like very promising
because if you predict what model is
doing badly you can view it off very
quickly I think Kevin Murphy and friends
are looking into some way to do
progressive architecture search so
basically the instead of like doing
massive architecture search right away
you kind of do curriculum training I
think this is also you know Barrett is
also a co-author of this paper and I
think that just can be quite promising
the other idea that a lot of people been
around and is the idea of using
multitask if you have a lot of data sets
right as humans we don't ever want to
start from scratch every time we have a
new problem we basically use some
knowledge in previous tasks and I think
I think like Risto could have talked
about it like in early talked so some of
those are some combination of those
ideas that can actually get you further
reduce computational complexity of this
approach yeah thank you very much for
your talk I hope coach and as you know
that over here in nips again we have on
symposium focusing on interpret
interpret ability this kind of approach
you know for example as you've shown
that we can reduce one person
of the test error but as you shown the
example of cells the normal cell and
another cell we could not interpret them
so what is your take on that I mean in
opinion although he can reduce the test
error but it's quite hard to interpret
such kind of models so which direction
would you like to you know okay so maybe
we can have like an objective fun like
another objective function in the
require function for interpreter T and
maybe the best way to do this is
connected to the NIP system so that
every time you come up with new model
you can submit a paper to nips actually
like in reality we've been thinking
about this problem a lot we don't have
good answers I think they are as complex
as any other deep learning systems
unfortunately but it's an area that
we've been spending time thinking about
so so one way to do it is to regularize
the grammar
so that makes sure that the grammar is
something that for example ResNet is an
idea that people like a lot it's because
not only because it works well but it
has some regular structure in the
grammar so maybe we can in a like and
for some grammar yeah thanks thank you
very much for amazing research
unfortunately not all of us can repeat
your experiments to computational
limitations my question is what how you
deal with other hyper parameters that
are not handled by the recurrent Arenado
so you have to predict them their
configuration but what do do is learning
great sizes okay so the question is how
do you handle how do we to handle the
high parameters in the system so it
turns out that it's quite easy to make
this how system not work the idea here
is to make that system work and one
thing that we a few things that I we we
do and we actually wrote it in the paper
too is the number one is that we don't
yeah we don't predict the learning right
so the learning right
if you if you were to allow the sister
the controller to predict the learning
rate it would predict the smallest low
rate possible it will predict the
smallest learning rate possible so that
within if you train it in a short time
the smaller this learning rate will
actually win and we also make sure that
all the models in the system actually
have very similar them upfront
parameters so that otherwise there were
the better the reward function would
prefer the networks that is the smallest
because you're gonna be trained faster
so those in the reinforced so the other
thing that we did was we copy some of
the most more the best stay of the ark
rested training recipes in imagenet or C
410 so for example later augmentation we
copy like the white ResNet recipe the we
have this cosine learning rate by frank
and colleagues
so we basically like copy a bunch of
recipes that exist in we didn't do
anything fancy there presumably if you
want to like take the controller and
predict those hyper parameters it's
possible to but the point that I'm
trying to make is except exist in hyper
parameters and then just invent new
architectures yeah so my apologies that
we're kind of running over time so sorry
about that hopefully we can catch you
after</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>