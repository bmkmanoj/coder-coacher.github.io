<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How to Build Safe Artificial Intelligence - Prof. Stuart Russell &amp; Prof. Peter Stone | Coder Coacher - Coaching Coders</title><meta content="How to Build Safe Artificial Intelligence - Prof. Stuart Russell &amp; Prof. Peter Stone - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How to Build Safe Artificial Intelligence - Prof. Stuart Russell &amp; Prof. Peter Stone</b></h2><h5 class="post__date">2018-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/X8Z1ASAYH5Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is science Friday I'm Ira Flatow
now for a topic we've been talking about
whoa since the dawn of Science Friday 25
years ago artificial intelligence the
field is on the move this year with some
striking examples of what the future
could hold we have a robot surgeon
beating a human at stitching a pigs
intestines self-driving cars are now
entering Ober calls in Pittsburgh and as
robots get smarter they learn more about
how we work start doing more things for
themselves we've heard warnings from
futurists like Elon Musk and Stephen
Hawking that super intelligence could
escape our control how do we ensure that
AI developments are as beneficial as
possible well a new center launched by
unit a University of California at
Berkeley proposes to focus on ensuring
AI is compatible with our needs while
keeping robots under our control even as
they get more advanced Berkeley computer
science professor Stuart Russell is
leading the center welcome back dr.
Russell thank you for having me and
based at Stanford another group is
studying the achievements and potential
of artificial intelligence for the next
hundred years it's if the AI 100
projects first report which is now out
and it looks at the potential properly
channeled AI could have for the year
2030 the lead author of that report dr.
Peter stone is a computer science
professor at the University of Texas at
Austin welcome dr. stone
thank you and if you'd like to talk
about how do we make most of a eyes how
do we take advantage of it the eyes
potential give us a tweet our tweet is
at SCI friken the tweet is there how do
we make the most of any eyes potential
tweet us at SCI Frey dr. Izadi define
human compatibility AI so what we really
want to do is make sure that AI systems
are provably beneficial specifically to
humans and so I need to unpack that a
little bit if you don't mind okay right
so first of all I think we all
understand that AI is about making
machines intelligence and that's
basically a good thing
dumb machines are not
that useful and we already see many of
these benefits that you mentioned and as
we approach human-level AI the ability
of machines to solve a wide range of
problems and learn all kinds of new
subjects then I think that will really
represent a dramatic change in the
progress of history because your history
is really the result of our minds
operating in the world and if we have
access to substantially greater
intelligence that we can use as a tool
then that's going to change the
direction of history and hopefully for
the better so but why do we need a
center for human compatibility can we
just say that AI is already human
compatible it does stuff for us so yes
so far we've focused on very restricted
tasks like playing chess but if you're
garry kasparov right was was it a good
thing that deep blue beat you right from
Gary's point of view probably not
and maybe that illustrates that well
they're good at sweeping the floors too
we got those robots that's smooth floors
and they're good at driving us around
until they crash into the side of a
truck and kill the dragon mm-hmm so the
real issue I think if you ask if you ask
an AI researcher what we mean by AI not
just the delay version making machines
intelligent but the technical version it
means making machines that can optimally
achieve objectives and then you say well
what objectives and the AI a researcher
will say well that's not my job that's
your job to figure out what your
objective is and put it into the machine
and as long as we're putting in small
simple objectives like win the game of
chess or drive to the airport there
isn't too much of an issue but if you
put in objectives like make my company
as profitable as possible or end hunger
for humans well there are different ways
of doing that we can end hunger by
ending humans if you ask King Midas
he'll tell you no it's very easy for
humans to miss specify the objective he
said I want everything I touch to turn
to gold and he got his food and his
drink turned to gold and he was very
unhappy and he died miserable so the
real issue is that AI is only human
compatible if the objectives
are consistent with what humans really
want it's too late when we put in an
objective like end human hunger to find
out that the machine is busily
exterminating humans so there won't be
hungry anymore so what we're trying to
do is is change the definition of AI so
that getting the right objectives
actually becomes part of the job of the
field and it isn't just left to the user
to specify Peter maybe you can help us
hone in on some of the objectives that
you see what are we looking at as you as
you look past in future but a AI and
human compatibility what are the some of
the biggest areas of opportunity right
now yeah that's a great question and
that was one of the the main focus areas
for the for the AI 100 report that we we
came up with we were given the charge to
think ahead to the year 2030 and come up
with the likely influences of artificial
intelligence technologies on a typical
North American city by the year 2030
and before I go into the areas that we
that we focused on I do think it's
important to to say that in contrast to
what Professor Russell was just saying
when one of the leaping off points for
for the report was to observe that at
least to the end in the present day
there is no sort of one artificial
intelligence it's another a AI is not
one thing rather it's a collection of
technologies that require special focus
and research towards every every
application so there isn't there doesn't
exist right now and we don't anticipate
in the foreseeable future at least in
the near future a machine that could be
given any goal but rather there are you
know technologies that are focused on
particular areas and so the areas we
looked at in our reports were starting
by the one that's probably front and
center and everyone's attention right
now is transportation it's clear that
that there's progress in autonomous
vehicles and in other areas of
transportation where AI technologies
have a huge potential to make an impact
in the relatively near term but there's
also great opportunities in health
holman service robots education public
safety and security
employment in the workplace and and
entertainment and in each of these
there's there's great potential for AI
technologies to improve our lives as
Professor Russell was saying and and
also but there's also barriers and both
technical challenges in each of these
you need each of these domains that are
sort of unique to each domain and also
social and political barriers that need
to be overcome if they're gonna achieve
their full potential is that why you
we should bring social scientists in to
this conversation about deciding where I
should be going yeah I think I mean it's
this is a conversation that needs to
happen among technologists but also
policy makers and politicians social
scientists economists and on our on our
panel we had people through representing
all of those those areas because you
know I think I think for example there's
there is a lot of concern I think
legitimate concern that that AI
technologies will affect the the
employment landscape and the the job
landscape and probably not by replacing
lots of jobs in the near term but rather
changing the changing the nature of
certain jobs for instance physicians
well certainly not in the near term be
entirely be replaced by AI technologies
but there will be tools and new tools
and skills that build that they'll have
at their fingertips and be able to use
but they'll need to be able to will need
to be able to make sure that there's
enough transparency that the physicians
and the patients are able to trust the
recommendations that are made by these
technologies and when it comes to the
economy you know that there's there's a
lot of potential for greater
productivity as a whole by society but
it there there is also you know danger
that there could be even greater
concentration of wealth in a smaller
number of people's hands and if we you
know we need to start the conversation
now about how to sort of make sure that
that the the wealth is created by AI
technologies
is treated fairly and equitably in fact
I have uh there are some tweets coming
in about that Mohammed author writes
most programmers are still wealthy white
men how do we make AI better for all
people stories you got any suggestion
maybe curricula change well I think
there is a very big issue with the
demographics of Silicon Valley
I wouldn't say white I would say white
and Asian but you know I think the
technology is spreading throughout the
world and different populations are
going to be adopting it and becoming
expert in it but I do see a lot of
people saying well you know it's great
that robots robots are gonna do all the
work because then we can have a
guaranteed basic income and anyone who
doesn't want to work doesn't have to
work to me this is just a recipe for
disaster because what you end up with is
a society of a thin layer of extremely
rich owners of Technology
a layer of people who serve their
personal needs and then the rest the
population is fed and how isn't
entertained and pretty much left to
their own devices and doesn't have a
real function in society that's not a
vision of the future that I for one
would welcome at all so we really have
to think very hard about what the future
shape of an economy is going to be when
the material needs of the population can
largely be met by fully automated
systems what's what's the best way of
teaching a robot there are a lot of
different ways Peter for example works
in an area called reinforcement learning
which means that essentially you you
give the robot a virtual lump of sugar
when it succeeds in or at least
partially succeeds in a task and you you
know you wrap it with rapid on the
knuckles when it doesn't do it right
this is reward and Punishment and that
process can be very effective in getting
a robot to to learn various tasks
assuming that the human knows when to
supply the sugar and when to supply the
the punishment Peter do you agree with
that
I mean what about having robots watch
what we do and that means that yeah
that's so there's there's many different
ways of communicating to the teaching
signal so one is by just a reward signal
as Stuart was talking about there is
also an area of research called learning
from demonstration where we have the
robot watches examples of successful
behaviors and this is an active research
area as well there's also so you know
there's there's a channel of reward
signal there's a channel of
demonstrations there can also be a
device watching you're watching a robot
or a program do a task and provide it
more of a richer feedback than just good
good robot or bad robot but more and you
know next time you're in that situation
try you know try doing X instead of Y
I'm Ira Flatow this is science Friday
from PRI Public Radio International here
in broadcasting from KQED in San
Francisco with a Stuart Russell of the
UC Berkeley and Peter stone of a
University of Texas at Austin do we want
to teach robots morality and whose
morality do we teach it yes and in some
sense
everyone's so one of the objectives of
our Center is actually to to get robots
to have the right objective so they
don't go off and do things that make us
unhappy after the fact
and there's a there's a process which is
related to the idea of learning from
demonstrations that Peter just mentioned
it's called inverse reinforcement
learning it's sort of the opposite of
the reward and punishments idea and so
what that means is you observe a human's
behavior and you try to figure out what
objectives the human is trying to
achieve so for example if if I get up in
the morning and I struggle downstairs in
my house off clothes and I I press
buttons on a funny-looking machine I'm
it makes grinding and hissing and
steaming noises and then this brown
liquid comes out and then I feel happy
right the robot should be able to figure
out that my objective in all this is to
get a cup of coffee and then perhaps the
next morning it will make the coffee for
me and bring it to
so that's a very simple example we would
like this process to to extend so that
eventually machines understand the the
full spectrum of human objectives so
that for example when we ask it to end
hunger it doesn't go off and exterminate
everyone so they stop being hungry well
we don't as you were saying before we
don't expect the machine that makes this
coffee to be the same machine that's
gonna take on hunger for the whole world
so I think it's it's a spectrum one of
the kinds of systems that people are
very interested in building right now
you know within the next five years I
think we'll see lots of products and
we're already seeing partial products is
the digital personal assistant so that's
an AI system that looks after your day
to day life your calendar you know books
your travel you know some executives
have very well-trained and expensive
assistants who do all this stuff for
them but in fact we could make this
available to pretty much every human
being on the planet for you know 99 or
29 cents a month or whatever so imagine
what that system is going to do so you
want to go to a meeting and you say okay
I want to just booked me into the
closest hotel so does it booked you into
the closest hotel we're the only room
available is the $2,000 a night
Presidential Suite
or does it put you in you know the
Hampton Inn down the road for 115 well
that depends on your preferences if
there's anything wrong with you you
might have one preference or the other
and out of the box the system doesn't
know much about you and your preferences
so one of the main things these systems
will have to do is to learn the
Preferences of the individual in order
to make good decisions on their behalf
and also to make decisions that don't
negatively affect other people so you
might say I am very jealous of my
calendar I don't like too many people
making appointments but you know then
I'm a professor that means there's a
long line of students you can't get
their form signed can't get into classes
and so on so you have to take into
account preferences of other people it
starts to get very complicated and those
are the kinds of problems that we will
see in the near term let alone the
problems of exterminating human race by
accident Peter did the AAI 100 report
warns us against unnecessary fear and
mission of AI honey you know with with
with futurists like Elon Musk and
Stephen Hawking warning us have not to
trust AI how do we trust AI how do we
get to trusting yeah I mean that's you
know there's there various ways to
instill trust
there's one is transparency so you know
making sure that people understand what
how something is programmed and and what
the if it's trained or if there's
learning involved what data it's it's
trained on going back to the the point
on morality and this is one of the
reasons that the Center at Berkeley that
Stewart's leading is very timely is that
there's you know as autonomous cars come
into to existence decisions that never
had to be made explicitly are going to
have to be made explicitly and you know
that's that's where the morality comes
in people talk a lot about the trolley
problem if a car has to make a choice
between you know saving it's the
passenger inside the car versus
pedestrian is outside the car
that's never been a something that we've
had to explicitly decide about but with
a anima scar that somebody's going to
need to program that morality and we
need to decide as a society you know
what are the what are the right
decisions well but another way for I got
it I got I got a break my my my
programming inside of me says I got to
say goodbye so I want to thank both of
you it's a great store we'll get back to
this dr. Stuart Russell computer science
professor University of California
Berkeley Peter stone computer science
professor UC Texas at Austin</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>