<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Geometric Deep Learning on Graphs and Manifolds - #NIPS2017 | Coder Coacher - Coaching Coders</title><meta content="Geometric Deep Learning on Graphs and Manifolds - #NIPS2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Geometric Deep Learning on Graphs and Manifolds - #NIPS2017</b></h2><h5 class="post__date">2017-12-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LvmjbXZyoP0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to the afternoon
tutorial session people are still
filtering in so please sit down quickly
the session is entitled geometric deep
learning on graphs and manifolds and
it's going to be presented by Michael
Bronstein
Joanne Bruner and Arthur Allen and I'll
just hand it over right away and thanks
for being here okay hello me okay hello
welcome everyone to our tutorial so we
are going to talk during the next two
hours about geometric deep learning
we're on graphs and manifolds so what is
the metric deep learning so it starts as
many deep learning stories with very
good numerical result and as you all of
us here all know I don't need to
convince you we have seen many success
stories many successful applications of
deep learning on all sorts of tasks but
all involving data that might be text
speech or images or videos and in many
of these instances there's one common
denominator most that in a sense is
responsible for many of these successes
that is these are architecture that we
call and we refer as CN n the
convolutional neural network so but in a
world there are things beyond images
speech or text for example you might be
interested in trying to leverage some
information on your social network or
you might be working on biostatistics or
computer graphics etc and so in this in
these domains the questions that we are
going to motivate the motivate this
tutorial are the following so what makes
images stage and text what kind of
geometric structure makes convolutional
neural networks successful in these
domains and how can we if you understand
some intrinsic or some Universal
properties on the that makes complete
neural networks successful how can we
leverage this information on domains
that are non Euclidean
so what is the non-euclidean domain so
in this tutorial we are going to focus
and we are going to keep in mind these
two very specific very simple setups one
is a manifold that you can imagine as a
you know as a shape in a in a
3-dimensional space that has let's say a
two dimension of the polity or a general
graph and we'll see many examples in the
tutorial of many situations where these
two data structures arise so what are
the different tasks that are we are
going to be considering and that are
treated in our framework so the first
one is to make this distinction between
what we call the main structure and data
another main so the main structure might
be assuming like your favorite social
network so in a domain structure problem
you might be interested in using the
graph or the the basically the
connectivity as the input itself and you
might want to be inferring properties of
this connectivity structure as opposed
to the setting where you might have a
domain like the same neural network but
now I want top of this domain you might
be observing or measuring signals that
for example the age or the preferences
of users etc etcetera etcetera so you
might be interested in understanding or
processing functions on this domain we
might also think about different and the
set up where the domain is either fixed
for example in a social network again
you might have the same social network
at different functions that you observe
on this network or you might have
problems where the domain itself is
going to be changing from one instance
to the other for example if you want to
be able to do shape correspondence you
have to use every every new data point
is going to correspond to a new graph
when you manifold we also are going to
be making a difference between the set
ups where the graph or the manifold
structure is known is measured by the
sensing process and also problems where
this is a geometric structure that we
presume is there is not observed for
example you might imagine having a for
example a point cloud of particles in
particle physics and this particles are
measured in some space and you know that
they might be interacting in a certain
way
but you don't know a priori what is the
geometrical structure on this data what
are the tasks that we might be
interested in solving so there there can
be touched that similarly as in images
image specification that take as input
whole graph or a functional graph and
try to output some categorical variable
for example the type of the molecule or
you know whether this molecule is from
category A or B we might also be
interested in problems where every node
or every edge in this graph has some
label that we want to infer for example
we might be interested in solving
community detection problems inferring
the voting preferences of people
similarly as in computer vision we find
these examples as for example in image
segmentation right where the goal is to
every pixel in the input will be
associated to label okay so what is with
this motivation what are we going to
describe are what I got we are going to
splain this tutorial so first the story
starts with convolutional neural
networks in their natural domain which
is the occlusion input and then we are
going to try to see slowly how by
introducing a little bit of graph theory
and a little bit of Fourier analysis we
can generalize this construction to
different domains and we will see to
mostly to different route for that one
is based on the spectral domain and the
other one are spatial domain now we are
going to conclude the tutorial with
hopefully broad range of applications
that I hope will illustrate why this why
this story is interesting for you ok so
as I said I mean with this audience I
don't need to spend a precious time
describing what convolutional neural
networks are so I thought that maybe we
can just start by describing all the
good things that they have so as you
know convolutional neural networks they
are convolutional right and
convolutional here is motivated or it is
justified by the fact that the images
the inputs that we need to treat have
some statistical property that is
translation invariant in statistics this
is called like stationarity the other
interesting and key property that we
will see later why is it so important is
this idea that a completion on your
network as we have heard many times
before has this ability to break the
learning problem in two different scales
right so basically the role of the first
layer is just to figure out a way to map
image like images that have a certain
size to images that are slower that are
that are of lower resolution so this
idea that the learning can be broken
into different steps that corresponds to
this notion that some of the tests that
we care about they can be learned by
learning in step by step like by this
notion of compositionality another
another property that is very important
and we'll see also later is that not
only these business models are
convolutional many of the in many
successful instances these filters are
localized into space and and the the
intuition behind why why the filters
have to be localized in space can also
be understood from that from the
geometrical point of view this idea that
if I applied my filters to an input that
is slightly deformed many of the filter
responses are going to be stable if the
filters are local and the first and the
third property together also give us
this bonus that the number of parameters
that we need to learn for layer is
constant right it's a constant that does
not depend on this on the dimensionality
of the input another interesting and key
property of convolutional neural
networks is that they can be computed
quite efficiently especially now with
the GPU architectures we just be a
linear time right and I basically grew
grouping all these properties together
what it means is that because we are
breaking the estimation in different
scales and every estimation step every
layer just we just pay a constant price
of parameters what it means is that the
overall learning complexity of this
model scales very nicely with the
dimension right so it's not exposed to
the course of the nationalities of other
models and so what is
the geometric what what are the the
assumptions underlying mathematical
structures that are underpinning this
construction so we can define
convolutional networks in in in either
in inclusion domain or in a grid in a
discretized version in on a grid and
these domains they have to three crucial
properties that make this construction
possible so the first one is as you may
know that this domain this grid you can
translate it and you get basically the
same Cup by the grid has this built-in
operator there is the translation and
there it has another built in structure
that is the idea that I can take I can
make take my grid and I can make it
twice this course why they can make it
twice as small and I get essentially the
same copy of the grid so this idea that
that you have this very nice domain with
these two properties also plays very
well with what I said before right that
that with this structure matched with
the right statistical properties of
images make the whole story for very
well together and so the road map that
we're going to follow is to extend this
construction to non to more general
geometries by trying to replace or
trying to come up with appropriate
definition of these two operations right
what does it mean to filter and what
does it mean to pull in more general
domains and as I said there are two
different routes to address this
question the first one is to work in
what we call the spectral domain and we
are going to see what this means
precisely and the other route is to work
on what we call the spatial domain okay
so let's go now nanak Lydian so as i
said let's keep this as two prototypes
of the
images that we are going to the the
input that we're going to process so
manifolds and graphs and so that the
challenges that we need to address for
this construction are basically we have
to assume that the same stationarity
properties are going to hold hopefully
hold in the tails of
using these nice models for and what we
need to solve as I said before is how
does it mean what does it mean to filter
so that we can leverage this weight
sharing of convolutional neural networks
across different parts of the domain and
how can we define the compositionality
how can we pull the information in this
domain alright so in one minute what are
the minimum machinery that we need to
know and to learn before we can do that
so what is a graph so a graph in our
context we are is for us it's just going
to be a set of nodes vertices from 1 to
n that are connected with some grid like
some edge structure right so this X
structure can be basically relying at
vertices one foot to the other and we
are going to assume here that the graph
is undirected which means that there's
no preferred direction ality in the
edges and that every Edge has a positive
weight is decided with that so I said
before in a graph we can define
functions so there's here like a little
example that shows a simple function on
this graph that at this point you can
just think about this function as a
vector right it's just a vector of n
numbers that encode the value of the
function at every node and this and this
function that are actually you can you
can we can think about them as being
elements of a Hilbert space in that case
a very simple he was faced with a
standard dot product and this function
in this functional space as I said we
are going to be exploiting or using some
geometry so in order to define geometry
you need to be able to measure the
smoothness of function of the regularity
of things right so how can we measure
regularity of functions like the one you
see here so this is achieved with a very
popular famous historic thing operator
that is called a laplacian okay so the
laplacian is just an operator that is
linear that is going to map a given
function f into a the laplacian of f
which is essentially the difference
between the function and its local
average okay so
just you look at the function you take a
weighted average at every node using the
weights of the neighbors and then you
remove the average all right and so in
in matrix form you can look at this
laplacian as the difference between two
simple matrices what we call the degree
matrix that is diagonal and the weight
matrix that is the input like it's how
we encode the graph that case is
symmetric because the graph is
undirected okay so as I said this is a
tool that we want to use to measure the
regularity of functions so how can we
compute how can we measure the
smoothness of a function so this is
something that we do with what we call
the dirichlet energy which is
essentially the weighted average the
weighted so we look at the differences
between the function and it's at every
node and its neighbors and we look at
the square different and we weight by
the weighted thumb according to the to
the weight of every age and so this is a
can be written as a function of the
laplacian as a quadratic form is like f
transpose the laplacian times F so this
is our tool to measure the smoothness so
this construction it turns out that it
we can perfectly mimic it we can follow
it exactly by replacing the graph with a
Romanian manifold so what is a Romanian
manifold so the intuition is a manifold
is just going to be like a twisted
Lydian plane that it's living somewhere
in a higher dimensional idiom space so
technically formally what what it is
it's a topological space where every
point has a kind of a copy of an equal
ideon space that we call the tangent
plane and in this tangent plane we have
a metric like basically we have a dot
product there is a characterized by like
a met by a metric tensor that is itself
continuous and again similar as before
we can define in this manifold either we
can define a scalar field which are just
functions defined on the manifold and we
can define also vector fields which are
functions on the tangent bundle and both
the both of these functional spaces they
come with their nice
well behaved metrics that make them into
hilbert spaces and so as before we can
associate we can measure smoothness in
this world it is manifold by using the
the the laplacian in that case is called
the laplace Beltrami operator which is
constructed a little bit more using a
diversions on gradient but this doesn't
really matter so it has again the same
properties as before the symmetric self
adjoint operator and you can think about
this laplacian as being basically the
continuous limit of discretized a
manifold that is appropriately
discretized using meshes so again we
have the gray decline energy as before
okay so once we have this directed
energy we can use these this energy to
basically try to think about the
functions and organize function in terms
of their smoothness why did we it'll be
good to understand what are the classes
of functions that are very smooth what
are the classes of function that are
less and less smooth and so the the
right way to to measure this is to
consider basically a basis like a linear
representation of functions where every
every basis element encodes a certain
class of smoothness right and so one we
can encode these we can measure this by
solving the following problem it's a
constrained optimization problem right
we're going to look for what is the
function that is the most smooth so that
minimizes the directed energy that is
the unit norm that would be the first
set up the first line the second line is
okay what is the next basis element that
is orthogonal to the first one there is
as much as possible and so it's not
surprise that this problem can be
written in a much more compact and maybe
familiar form as anagen bubbly problem
right you want to find a basis that
minimizes the the trace norm of that
basically that that corresponds to the
egg and decomposition of the laplacian
okay so the reply genican vectors are
the solutions they encode this they give
us the basically the dictionary to
understand smoothness in graphs for
manifolds okay
what does this egg in the competition
give us on the one hand we have these
eigen eigen vectors that are orthogonal
so you need Norman and de correlate like
orthogonal one to each other and then we
have associated a family of eigen values
that are non-negative because the
laplacian is positive semi-definite that
encode basically that the smoothness
class maybe every one of these basis
elements and so probably you are
familiar you try to solve this problem
in a nucleon domain you will get
something that it looks very familiar
right so here you get the sine and
cosines right which are as you might
imagine the basically the solutions or
they encode they measure smoothness in a
global form as the laplacian conveys so
if we do the same thing on a general
graph we get that sort of functions
right that code that basically encode
different smoothness so as we increase
the size of the the index of the eigen
vector we get things that are more and
more oscillating and the same thing on a
manifold okay so as you might have
suspected at least part of the atoms
have suspect that this looks very
familiar this look like free right so
indeed Fourier is intimately related to
the solution of the direct light energy
and so just to refresh a little bit the
refresh the calculus the harmonic
analysis concept so what is the Fourier
transform what is that for analysis is a
way to think about the function f as the
sum of sines and cosines and so what are
the coefficient that I need to X to u
that every frequency they are precisely
the as you go that the competition and
the basis so this is just taking a
vector like looking thinking about the
function as an infinite dimensional
vector and expressing this vector in the
Fourier basis and so it turns out that
one way where we can intrinsically
define these Fourier basis is again from
the laplacian it was it before the
solutions
this wave equation through so basically
looking at functions that are
eigenvectors of the of the laplacian if
there's no surprise the atoms of the
Fourier representation so now that we
have this this way to characterize
Fourier as a solutions taken the
composition of laplacian and we have a
laplacian in these domains we can
perfectly and we can we can take the
concept and transfer it to the graph and
manifolds so this gives us very
automatically a representation of
functions in graphs and manifolds in
terms of their Fourier coefficients so
how we do that we take a function f and
we project it over the basis that the
unit is the laplacian and this gives us
coefficients f @k
that allow us to reconstruct and these
are precisely the eigenfunctions of the
laplacian okay so here you see again
another example where is you know that
what what does this is the composition
look like okay so in getting closer to
going getting closer to our objective of
mapping these things to convolution in
the networks you might also remember and
suspect that this this base is the
composition is a representation in terms
of for M has must have something to do
with a convolution with filtering so
indeed let's just verify that this is
the case if you're--if just refreshing
the definition the convolution is just
you can at the completion between a
function and a filter it's a linear
combination of the function with the
translated versions of the filter and
because by definition we have the
convolution commutes with translation
right basically translating before after
the convolution gives the same result
and so because of essentially because of
this property what we have is that the
Fourier transform that this convolution
operation is expressed as a diagonal
operation in the free domain so what
does it mean it means that if I look at
the Fourier transform of the convolution
between these two functions this is just
a point wise operation in the frequency
domain
and in some in many exam in many
situations this is in fact the the way
convolutions are computed right because
we can leverage the fact that we can
compete for a transference very
efficiently in a new collision domain so
in in discrete in the discrete case what
does this look like so as I said the
convolution is just a linear operation
so you can just look at the convolution
as a as a matrix vector multiplication
and so this matrix has a very special
structure right and this is called a
circulant structure because every row of
this matrix is just a shifted version of
each other
and so because of this special structure
of this matrix this matrix is the
diagonal in the Fourier basis what does
it mean it means that we can write it as
the Fourier transform times the diagonal
matrix of G hat in the diagonal times
the the Fourier transform inverse and so
again we verify that this is a
equivalent to writing the combo the
convolution as a sum as an operation
that is diagonal the free domain and so
before a similarly as before we can take
this analogy this this convolutional
theorem as a road map as a guide to
think about convolution in a more
general domain right because what we
have what we have just said that we have
expressed the convolution as a very
specific operation in the spectrum of
the Laplace write something that is
diagonal in this in the intel applies in
the free domain envelop in the the
eigenvectors of the laplace so we can
use this as a definition as an as a as a
tool to think about convolutions in this
channel domain so what we are going to
do is that we are going to take the
Fourier coefficients of the filter the
Fourier coefficient of our function and
then just multiply each frequency
independently and then use the inverse
Fourier transform again right so in
matrix vector notation this corresponds
to basically before right so taking the
function f going in the free domain
multiplying every frequency
independently and then going back so
this looks all nice and simple
course it's not as nice as the reality
is not as nice as in the occluding
domain right because this dysfunction
this matrix G now it's not as beautiful
as in the clear domain because it
doesn't have this circle and structure
right as and and this is something that
connects to what I said at the beginning
right that in general graft doesn't mean
anything to translate right there's the
domain is not the same that does not
have the operation of translation in
general but it's only in very specific
domain very specific graphs that this
operation makes sense and the other
thing that is sorry and the other thing
of course that I had should mention is
that the coefficients of this filter of
course they depend on the basis right
they depend on on every every graph will
have a different set of coefficients and
so this representation what I have to
say is that it's it comes with some with
some limitations that we are going to
see just right in a second how these
things can be overcome but I just wanted
to illustrate this with a very simple
example so this is a simple function
over a domain and we can ask we can try
to understand how this the competitive
this spectral the composition varies or
is stable as we make little changes in
the domain right and as the horse is
going to move let's see how things are
going to behave and so here we have a
special like a very specific you know
filter in this domain so as now we are
going to change the domain as you see
here the notion of the filter that we
defined in our initial domain as we
change the HPD from the horse completely
change the sense right so here what what
we are saying is that this is like a an
important limitation of thinking about
convolution in the Fourier domain we
thought localizing in space that needs
to be addressed
and so this is another example of a
filter high frequency filter that we see
that how it's not stable as we deform
the domain so with that now we are going
to see how this technology can be used
to learn and I will let Michael now
continue so thank you
thank you very much Ron so thank you for
this introduction so now we know what is
meant by Fourier transform on a non
Euclidean domain whether it's many fold
in the graph let's now dive deeper into
how to use this construction to do
convolutional neural networks and
basically we need to basically link box
to construct the CN n we need a
convolution right or some kind of notion
of filtering on our domain and we also
need pooling so this is what I am going
to describe in the next about twenty or
thirty minutes okay so the very simple
ideas we've seen before basically we can
just apply the filtering as diagonal
operation in the frequency domain so
that's exactly what we see here
basically we are given some signal a
function f right we compute its Fourier
transform and then multiply by a learn
about diagonal matrix of weights
basically these are the spectral
coefficients of the filter then we
compute the inverse Fourier transform
and this is what replaces the standard
Euclidean convolution on the graph or
any manifold and we can do with this
convolution the usual things that we do
in convolutional neural networks right
we can apply non-linearity for example
half rectifiers that are very popular we
can we have multiple layers of such
operations and so on and so forth so
maybe this very simple idea yet comes
with several severe disadvantages first
of all what which one already mentioned
that the filter the filtering operation
is domain dependent so if we have
especially strong non isometric
deformations of the domain the result
will change quite dramatically
the second problem which is different
from the classical case in this
formulation the number of parameters
basically the number of spectral
coefficients of the filter is order of n
basically it depends linearly on the
size of the input and this is of course
very different from the order one that
we wanted to achieve in analogy to the
classical cnn's the other thing is that
unlike the Euclidian case where we have
efficient ways of computing the fast
Fourier transform with n log n
operations usually on general graphs
which takes order of N squared so the
computation of the Fourier transform is
in
patient on jail graphs we don't usually
have F of T on non-euclidean domains and
the third thing one of the actually
powerful features of convolutional
neural networks is that the filters that
are used in CN enter localized and here
because we're basically we are given a
freedom to choose any spectral
multiplier nothing in the world
guarantees that in the spatial domain
such filters will be local okay
so let's address these problems starting
from the last one and going to the first
one and basically the property of the
Fourier transform in the en domain that
also generalizes to the non Euclidean
setting is what sometimes is called in
signal processing literature the
finishing moments basically if we look
at the moment of the function in the
spatial domain it translates to the
integral over a high order derivative of
its Fourier transform and basically if
we want localization in a spatial domain
we want high order moments to be small
or even vanish it means that higher
order derivatives of the Fourier
transform must also vanished meaning
that the Fourier transform must be
smooth it was a small function where we
understand frequency as the eigenvalue
okay and basically the idea here is that
we can parameterize the filter as a
smooth spectral transfer function
basically we if again we regard the
eigenvalues of the operation as
frequencies we want to apply to them
some smooth function and the application
of this function to the operation is
understood in the sense that we apply we
perform the eigen decomposition of the
operation and apply this function
element-wise so the two is eigen eigen
values so basically we have here a
diagonal matrix of tau the spectral
transfer function apply to the
individual working values okay and here
we can promote rise this function by a
small number of parameters will see
particular examples for polynomials and
the number of parameters can be
basically two under our control we can
make it independent on the input size
will be order of fun okay and let me
show you here is an example of frequency
response that is not very smooth and you
can see that the result is very
delocalized in the spatial domain and
you can also see that it's unstable it's
similar to the example that Ron
showed before this is an example of a
smoother filter that is way better
localized in space and also it turns out
to be much more stable so in general
local filters when they're computer and
in the spectral domain tend to be much
stable and then then global okay so as I
mentioned we can basically it's our
choice to to define this most transfer
function a particularly convenient
choice is a polynomial or to be more
precise a chebyshev polynomial basically
that's an orthogonal familiar of
polynomials and we can apply when we
apply to the laplacian we can basically
write it as sum of powers of the
laplacian as a result we get first of
all fixed number of parameters per layer
right these are coefficients of the
polynomials
basically they parameterize or a
spectral transfer function the filters
are also localized and they are
guaranteed to have our hopes support and
that's because the operation is a local
operator basically application of a
laplacian to a function on the graph
effects only its direct neighbors so if
you apply an earth power of the
operation we affect only neighbors of
neighbors removed our x right so
basically we get guaranteed local
filters we also don't don't need to
explicitly compute the eigenvectors and
eigenvalues of the operation basically
we can apply this function to the
operation itself by just taking powers
of the operation so there is no
expensive i give me composition of the
operation so there is no expensive
explicit computation of the Fourier
transform and if our graph is sparsely
connected meaning that the operation
contains only order of n non zeros in
each row and column basically the cost
of this operation is order of n ok so
let me show you an example what you see
here is example of a graph basically
it's citation Network each vertex
represents a paper and edges represent
citations so we assume it to be
undirected for simplicity and each paper
is described by some feature vector
representing its content and the task
here is to classify each of the papers
into some field where it was published
so this is a classical data set that is
used in network science and in graph
theory
and some of the state-of-the-art methods
the results are summarized here so this
is called the core data set and the best
methods achieve something like 75
percent of accuracy and these methods do
not use any context information so they
consider each vertex individually if you
apply convolutional neural at work on
graph you get significant improvement in
performance above 80 percent so this is
just maybe a toy example of an
application of the graph convolutional
neural networks stone some interesting
data set so we talked about convolution
let's say a few words about pooling so
basically in the Euclidian case pooling
corresponds to changing the resolution
of our accrete right basically we we
forgot usually downsample an image by a
certain factor and take some operation
like Maxim so the same thing can be done
on Kraft in this case the reduction of
the graph resolution can be done by a
graph coarsening process for example
when a pair of vertices is collapsed
into a single vertex and we can take an
operation like maximum or average on
this pair so we an operator that
transfers data functions from the final
graph to a coarser one and basically if
we arrange the vertices in a particular
order in a tree structure this operation
can be done as efficiently as the
Euclidean counterpart okay so to
summarize the limitations of spectral
methods of course their advantages are
very clear because they have direct
interpretation using the standard
classical signal processing intuition
but there's several limitations that
they come with is first of all as we
already mentioned and shown several
examples it's poor generalization across
different domains especially when the
domains are strongly anti symmetric
unless the kernels are very localized so
I should mention that there is a recent
work that tries to somehow remedy this
problem using the idea of transforming
networks applied in the spectral domain
basically trying to align the basis the
way that their Fourier coefficients in
different bases speak in the same
language so
say another problem is that basically in
the Euclidian case the laplacian is
rotational invariant so basically the
kernels that we get with this operation
are symmetric they look like this so
that would be for example polynomial
kernel when the operation is taking from
the inclusion grid and there are ways of
constructing anisotropic kernels but
this is more difficult to do on general
graphs you can do it on many phones when
you have some underlying low dimensional
structure basically the tangent space so
we'll mention it in the following but on
general graphs this is quite a big
challenge and finally the underlying
assumption was that the operation matrix
is a symmetric matrix this is what
allowed us to have orthogonal
eigenvectors and real eigenvalues so
basically dealing with directed graphs
with spectral methods is challenging
there are some generalizations of these
constructions but it's not
straightforward
so the frameworks that will be presented
next approach this construction from a
different perspective basically using
spatial domain formulation of filtering
okay and I will pass here the stage to
Arthur we will talk about the graph
neural networks
okay so we talked about the kind of
spectral the free a side constructions
and now we're going to try and talk
about the special side constructions
these in some sense are simpler to write
down they require less theory to write
down but that theory is it's useful for
understanding even the kind of special
side instructions the other so the very
last few constructions that were talked
about actually can transfer between
graphs but it takes a little bit of work
to make that happen in some sense the
spatial the nice thing about the special
constructions is that they are automatic
it's it's automatic to feed it a graph
so the special side constructions are
designed to input a graph and a function
from the nodes to our gate so the these
kinds of instructions are actually quite
oh they're older than a lot of the stuff
we've talked about but people had
forgotten as has happened so gory
gardenia and Araceli had the graph
neural network construction in 2005 it's
actually a very general paper that
foresees a lot of later things that we
decided were important okay so it turns
out that in order to understand what to
do with the graph on the space side the
best thing to do is understand what or
the simplest thing to do is to
understand what I should do to input a
set the thing about graphs is that they
are permutation invariant and the thing
about graphs is that they can be
different sizes and those once you have
kind of an understanding of how to deal
with those two things you can actually
do you can do all the things you want to
do so sets happen to be invariant
permutations and sets you know if you
want to have a neural network that eats
a set it has to be able to do dynamic
resizing so let's tackle this first okay
so the simplest thing if someone says
okay give me some operator that if you
give me a
of vectors it's permutation invariant
with respect to the set of vectors and
it is it's invariant with respect to the
size of the set is taking a mean so
taking a mean is a really nice thing to
do if people who do you know bags of
things are kind of the classical
approach to if you have a problem the
very first thing you should do is take a
bag of things because it's often really
good of course taking a sum is not the
only thing you can do you can take
products more recently people have
started doing attention attention it is
a thing which is dynamic really
dynamically resizable and invariant of
permutations in the things that follow
will talk a little bit about what we're
going to talk more about sums and a
little bit about multiplicative things
about geometric means attention less so
and what it turns out is you can think
of actually it turns out that attention
or multi-hop attention is another way of
processing sets or processing graphs
even but it's a sequential way of doing
it
multi-hop attention looks at a piece of
the graph or a piece of the set and then
does a little bit of processing looks at
a different piece does a little bit of
processing and so on and so forth
whereas graph neural networks and graph
convolutional networks work in a
distributed or parallel fashion and
we'll see how this looks so actually we
could have a complete branch of this
talk where we talk about multi-hop
attention as graph neural networks it
feels a little different and we won't we
won't do that but actually multi-hop
attention does work on graphs as well
and works on sets as well but okay so
we're gonna find a a graph an N type
approach or a mean based approach that
works on sets so how does it work let's
suppose someone gives you a set of M
vectors what we're gonna do is we're
gonna pick two matrices we'll call them
H and C and H is the matrix that you're
going to apply to yourself and C is the
matrix you're going to apply to
everybody else so let's go through the
in this slide if you'll notice there's a
bunch of superscripts so each vector H
has a superscript that's going to be the
superscript tells you what layer of the
network you're in so for example H super
knot is the set of vectors in the input
to the network there's a bunch of
subscripts here the subscripts
correspond to which vector you're
talking about in the set so h1 naught is
the input is the first element of the
set and it's at the input level h-11
h-12 / one would be the first element of
the set after one application of the
network or in the second layer so how
can we process a set like this well we
do some nonlinear function of the H
matrix applying to yourself and the C
matrix applying to everybody else now
what is everybody else yeah you can see
the hand everybody else is the mean of
everybody that's not you so again we
know a mean is permutation invariant and
we know a mean is scale-free so that's
great this thing you you do something to
yourself and you do something to
everybody else and because you just work
on these two classes you're a
permutation invariant and scale-free and
so okay these equations look a little
bit forbidding let's look at it is a
picture I know some people like it much
better to a picture so you input
yourself and everybody else and what is
everybody else everybody else that's
this red stuff is the sum of everybody
that's not you or the mean sorry not the
sum of everybody that's not you and you
have yourself and you feed that into
some small sum some neural network and
that gives you an output which is the
kind of what sits over the set the
vector that sits over that set element
at the next level let's look at this
same thing we're gonna this is again
exactly the same thing but now written
in matrix form so we've seen this three
times I really want to get this idea
across because this is the idea for a
graph on n here it was kind of
element by element here it was in a
picture and here it is in matrix
notation if we set C bar to be the
matrix C and again super I refers to
which layers so C bar is just C time
divided by the number of elements in the
set then if we write what we had before
we just have a plain multi-layer neural
network but with a matrix that's has
this form on the diagonal it has these
sorry it's block diagonal with these
h-blocks on the diagonal and it's on the
off diagonal has these C bar blocks and
when you apply this thing you apply it
to the concatenation of all the all the
vectors and you can see from the shape
of this matrix its permutation variant
and you can see from this that it is
scale invariant and so you have a really
cool object here already like
independent of graphs or geometry or
anything else you have an object that
you have a kind of construction that you
can parameterize something that each
sets and it eats sets of vectors and
processes them as sets of vectors and as
we talked about before it does it in
parallel so here it does all these it
does all the operations in parallel and
then it it takes the mean of everybody
everybody communicates with each other
and so on and so forth um so let's look
at see what this looks like um is this
video gonna play okay I'm gonna give up
on the videos oh something happened okay
I'll give up on the videos so what you
would see if if this thing was working
um what you would see in the video is a
bunch of cars in an intersection going
through and again the cool thing about
this sort of construction is that it
doesn't care how many objects are in
your set so you can train one neural
network and it dynamically resize it on
input so at moment one you can have a
swarm of objects in a moment one
somebody leaves the swarm and somebody
else comes in and somebody else leaves
and somebody else comes in and in all
times you're still processing that
object with the same neural network
okay so that's what we do with sets the
graph neural network is the same thing
we don't change a single thing except
now instead of taking a mean over
everybody you just take a mean over your
neighbors so here again we're going to
have a function H not from the vertices
of the graph to our D and we're going to
perform the updates just like before the
H at the next layer at the position J is
given by some function of the current in
state at that location and everybody
else's activation and everybody else's
activation is now instead of just gonna
be a mean it's gonna be a mean weighted
by the graph weights okay and here here
I'm just for simplicity going to assume
W is a row normalized matrix just for
simplicity I don't want to write too
much stuff so the only difference
between the thing before that we did on
sets and the thing that we're doing now
on graphs is that you take the means
over your neighbors right and this is
the simplest form of a graph NN and it's
actually reasonably effective already
for for lots of fun things that was
supposed to be another video which I
can't show you it's it's the graph and
inversion of this okay so now let's go
back to what those guys were saying
before about the spectral networks so
what we know from the spectral theorem
is that taking powers of the laplacian
or in this case powers of the diffusion
operator this is this w is the same
thing you can you can take a function of
the spectrum or you can take a function
of the actual operator it doesn't matter
that's what the spectral theorem says so
it turns out that if instead of just
having F of W times H here you have F of
W times H and then so this was just H so
W to the 0 power of H is just H and then
you have W 1 well this is the thing
had before and then you have you can put
higher powers into you recover something
that looks very much like the spectral
construction that we had before in fact
is exactly the same thing as the
spectral construction that we had before
it should you've built it on the space
side and a lot of the things other
things that were talked about before oh
when you do this kind of small
polynomial thing with that that has
that's localized in frequency you only
have a fixed number of hops well here
you can see it explicitly if we build
this construction on the space side we
only have that many hops so when we do
it this way again you can really see oh
the graph is dynamic and test time
because the output of a layer is a
function of the graph and of the end of
the input right and so this is nice it
really you can eat the graph a test time
okay there's one thing I want to say
about that so because this is input
taken because is implemented spatially
we can take the graph as an input but
actually there's a bit of a lie and
there has to be because the pictures
they showed you before the videos they
showed you before said oh you can't
actually take the graph of as an input
because the thing is not stable with
respect to perturbations of the graph
and what happens is the higher the power
used here the bigger the R is the more
powerful the method is the more rich
kinds of filters you can build but the
richer there's a trade-off the richer
I'm allowed to make my filter class the
more dependent I am of the exact graph
structure I have and for example if
let's let's imagine a really simple
thing let's imagine a chain of vertices
a really simple graph if I connect the
last two the topology completely changes
and if I look at the weight matrix which
is just one pop I can't see that change
in topology at all but if I look at W to
the you know number of nodes all of a
sudden I can see that change of topology
as I go around a small change in through
ology I just couldn't make one edge
all of a sudden W to the fifty power if
I had 50 nodes means something
completely different and so you have
this trade-off between having very rich
filter
as you increase the power here and
having filters being able to honestly
eat the graph at test time okay so this
lets I'm just gonna say a few things
this was kind of the most simple
construction here you can do all sorts
of bells and whistles to to kind of make
it more flexible so for example you can
decorate the edges so you don't have to
have just one edge type you can have
lots of edges and otherwise think about
that is having multiple graphs and so
your update equations would look like
this you can decorate the vertices so if
I want to say something if everything up
to now has been a completely permutation
variant but maybe I want to give some
information about some of the vertices I
can tell you that this vertex is of type
a and this other vertex is of type B
right so I can mask out some of the
relationships um again this is this is
an easy thing to do now if you really
want to push this all the way so here we
have kind of vertex decoration you can
push it even further you can make the
graph weights be in something be
something which is learn about so you
can make the graph weights be a function
of the current hidden States so a really
simple thing is to do you know oh that
should be a minus whatever but you can
you can do a really simple thing where
you kind of have some sort of say local
gating let's say well if my hidden
states are very close then I'm going to
I'm gonna compare them when I take the
mean otherwise I'm not this construction
is essentially what is called an
interaction network or on neural message
passing and it's actually super powerful
I think actually this starts to be
somewhat different than a comm net this
starts to feel different at least
normally when we build commnets we don't
have multiplicative interactions our
comments are usually of the form filter
non-linearity perhaps some pooling
filter and non-linearity perhaps and
pooling filter non-linearity so on and
so forth we don't usually in our comm
Nets have multiple multiplicative
interactions or
or attention or gating things like this
you could do it and then it would look
like this so this is in some sense once
you start to get to this interaction
network business it starts to look less
like a common net on a graph and starts
to look a little bit different we could
build calm that's on you know on the
grid like that but we don't usually okay
and there's all sorts of other things
you can do you can have even even though
you don't have kind of this attention
time multiple cat interaction you can
have multiplicative interactions on the
way your own response and everybody else
response comes in you can have W that
depends on layers you can have all sorts
of skipped connections and all the
normal things but the point is the space
side constructions allow you to do all
the things that you're used to and work
pretty well um so I want to talk a
little bit more about something that
Michael mentioned at the end of his
section which is well what is it what if
we take this construction this GNN
construction that we that we've just
described and say well let's let's say
what does that look like back on living
bread this is everything we said worked
on the graph what if we go back to the
grid what does it look like well if the
graph is a regular lattice then as
Michael good you can only get isotropic
filters I mean if you think about it
everything here let me go back even with
high powers everything here was just
powers of an isotropic matrix right on
the grid so everything is going to
continue to be isotropic so if you have
a single isotropic weight matrix then a
graph and n gives you radial Carnales
and that's it and if you think about it
that's a pretty huge change if you if
you have a common attack on that can
have edge detectors and a comb that can
have you know all sorts of things that
really detect orientation so why is it
that this GN n construction which looks
kind of similar in a lot of ways why is
it so much less expressive and of course
the answer is there's no notion of up
and down on a graph there's this is to
some extent if you if you want an
intrinsic solution there's there's no
way you could on a graph get edge
detectors because there's no notion of
up and down so up and down on the grid
come from the group structure the the
you know one of the generators of the
group tells you which way up negative of
that generator is down the one of the
other generator of the group tells you
which way is right and the negative of
that is left right that's that's a
that's a because the grid has this
special structure that graph doesn't
have more than that it's actually
implicit in the ordering of the it's
implicit in the coordinates of the
representation of the filters so because
at every location on the graph I sorry
every location on the grid if I look at
the way the filter touches it
this always touches the top left this
touches the middle left this comes to
the top right this touches the left this
stuff's in the middle and so on and so
forth that order is the same order
everywhere on the graph and again this
is in some sense something that comes
from the group structure of the grid but
that doesn't make sense on the graph by
the way so one thing which is really
cool and funny is that this goes
backwards actually as well if someone
just gives you a pile of patches from
images and doesn't tell you that they're
patches from images and in fact they
even permute all the indices but they
don't tell you and they just give you
this pile of things you can immediately
recover the correct ordering of the
indices and it's exactly again because
of this permutation and because of this
translation invariance that images have
and because local there's local
correlation between pixels so if I were
just to take the principal components of
a set of patches the first principal
component well it'll just be the DC but
the second principal component will be
either the variation and left and right
or up and down and the third will be you
know up and down or left and right so
you can recover this this goes both ways
but this doesn't happen on a regular on
a non regular graph it doesn't happen on
just some graph that someone's given you
the the fact that you have the notion of
orientation was really special on the
grid and you can't have it on a graph
without some without adding some extra
knowledge to it but you can get edge
filters back if you allow decorations so
a few slides back we talked about
we talked about vertex decoration so if
you do vertex decoration then you can
get that's a simple way of getting edge
filters back there are other ways as
well you can use edge decorations you
can use vertex decorations you can use
other things as well and in general if
you have extra information beyond the
graph you can actually give this to the
car the the construction okay
so I'll stop now actually we'll take and
take a break and do questions so before
before everybody wanders off I do have
an announcement and that is that there
was a change to the schedule it's online
but I don't think it's in the program
book which is that the opening remarks
are going to start at 5 o'clock right
here where you are now so be sure to be
here at 5 o'clock so we're gonna have
questions now so if everybody who wants
to ask a question can line up at one of
the mics there's two mics by either of
the projectors so please do line up now
and while people who want to ask
questions are coming up I'm gonna ask
the first question and that is there was
kind of a let's say I mentioned at the
very beginning about how you can
construct a graph or learner graph so if
you think about like a spectral
clustering very frequently what you'll
take is a Euclidian data or some other
sort of data that you can embed in a
metric space but then you construct a
graph on that data and then you do some
processing with that so I'm wondering if
you have any thoughts or comments about
when you might want to construct a graph
when it would be useful to use a graph
technique and when you would say no you
need to do some other type of thing so
when you would want to construct a graph
from data points versus when you might
and then apply some sort of
techniques such as a graph convolutional
network or whatever yeah so that's a
very interesting question
so yeah I did so if you if you have
super vision from the spectral
clustering let's say that you know the
labels or how you would like ideally
your data to be separated and you have
some labels on how you know who which
are the clusters then you can indeed
like make make this make this model
learner ball such that several layers of
rock emotions or network are going to be
predict I mean I'm going to produce
clusters that match what you observe so
in that respect yet I mean there are
ways there are some recent works that
try to actually you know unroll spectral
clustering as if it was a network and
then do back propagation on it and in in
and directly learning the graph such
that the resulting clusters are matching
your data I don't know if that was the
question yeah all right so it seems like
we have more people on on the stage left
side so let's start there hey I think
question regarding craft convolutional
neural networks so you mentioned the
early slides that you can use it for
molecular classification or image
classification so I have a question
regarding molecular classification which
you have graphs of different sizes and
you have different laplacian or
adjacency matrices and the assumption in
the cross spectral convolution you'd
grab collusion had read is that you have
the same Fourier basis right but for
molecule classification your grass will
have different for your basis so how do
you do a molecular classification in
that case I have to apologize but we
didn't understand anything because of
the echo so can you repeat very slowly
so my straight question is how do you go
for molecular classification in which
graphs are of different sizes and like
how do you apply or spectrograph
convolution neural networks for
molecular classification are you getting
let me see if I understand so the
question is how do you apply graph
compression neural networks to molecular
classification where every
molecule has different sides exactly I
mean they might have different agency
matrices they might have different notes
so can you apply it standardly or how's
it yeah so the construction the
construction the entire last bit that I
was talking about all those
constructions are completely happy to
have every input have a different graph
structure that's in some sense the way
they're designed that is in spatial
domain right not in spectral domain so
but the thing is if you think about if
you think about for example the chap net
that he was talking about or any if you
kind of use higher powers of the weight
operator the higher the power the more
you can think of it as a spectral a
spectral construction even though we did
it in the spatial domain it's acting on
the spectrum in a way and basically if
use very high powers you can't expect
such a thing to work so at some point at
some point if the graphs are different
enough you're never going to be able to
do this you're never going to be able to
compare them with a fully spectral
construction if the graphs are
reasonably similar then you can use more
you know chiron higher powers you can
use more and more of the spectrum sorry
you can use you know lower lower
frequencies you can use less and less
localized things but for graphs that are
if you want one model to work on very
different graphs it's essentially gonna
have to be unless you know something
else unless you know something else
special about it it's gonna have to be
localized and then you should build it
in space Thanks so maybe maybe we can go
to the other side now hi thanks for the
presentations very interesting I have
two questions one is what about directed
graphs
it seems that construction could be
extended easily if you change the
neighborhood operation and another
question is about efficiency comparison
so you mentioned that if we label if you
take a regular lattice and label
horizontal and vertical dimensions we
can recover spatial to malucia
filters in image recognition what will
be the efficiency comparison in the
implementation if we train a graph by
its setup versus traditional value from
them so the record graphs will the
spectral methods as we mentioned at the
beginning basically they are not not
made for directed graphs but the graph
neural networks can deal with directed
graphs as well so basically if that
would be a short answer to your question
so you can deal with directed graphs
with a graph neural networks the second
question about the efficiency so for
example if you take the spectral methods
the efficiency basically the cost of
computing the filters is linear in the
input size under the assumption of
sparse curve and what is also important
is that the laplacian operator is local
operator and you can easily at least in
principle paralyze it's basically each
vertex to compute the operation of the
function that active vertex you just
need to talk to your neighbors so
basically we can compute the operation
at each vertex individually you can do
it in a distributed manner there are
actually called computational frameworks
that work with this kind of this kind of
data with graph structure data did you
have any practical comparisons trying to
do one way and another just so I don't
think that basically the thing the
classical convolutional neural networks
and the graph and the graph
convolutional sarcoma comparable because
obviously the domain of application is
very different you would probably not
apply a graph convolutional neural
networks to images where it's a regular
grid structure and we can do the
convolution operations in a way that is
extremely well optimized with current
hardware and software but the classical
sentence cannot work on graphs and
basically the special methods that we
discussed in this tutorial are actually
built for this purpose in terms of the
order of complexities it's linear so
that's basically that's the same thing
what is the constant of course it will
probably be higher thanks
so I have a question so how do you
define spatial pooling on rahmani-r
manifolds it seems like that kind of
Scott's swept underneath the rug and
there's a lot of variance in the way you
might choose that operation can you
recognize how do you define spatial
pooling in the continuous case on a
Romani manifold so like on the horse I
mean in general if you have any multi
scale clustering of your space whether
your space is a continuous space or a
discrete space you can now define
pooling right so there are plenty of
methods for taking a manifold if
especially if it's represented as a mesh
or something like that and cutting it
into pieces at various levels of
coarseness and for example especially as
long as those are nested you have a very
classical notion of pooling even if
they're not nested you have a kind of
very reasonable notion of pooling right
you just need chunks of the space that
are local right but that's then scale
dependent on the chunks of the space so
you might be losing out in certain
topological features so now you're you
basically chuck it down to making a mesh
of the right size on the manifold then
treating it like a graph essentially I
see so well okay so when we
and I'm gonna give a very annoying
answer and I apologize for this but
eventually something's gonna have to go
on the computer right if you give me a
if you give me a if you give me a
description of your manifold as the
level set of some function I could
actually write down for you the I'm
pulling the locations as that function
of the 0 set of something right
I could write I could give it back to
you in the same way in which case I
wouldn't have this thing but usually
it's very rare that I mean to the extent
you represent a manifold as a level set
of something or something like that then
okay I can give you the pools in the
same in the same form quite happily all
right yeah I have a question about the
spatial convolution methods with respect
to when we do have data that's in some
sort of metrics basically we talked
about like especially molecular data so
when we take when we take an average
over all of our neighbors then we lose a
lot of like structural and like sort of
spatial information right when we are in
some sort of like like we do have some
sort of three-dimensional space so how
can we get around that problem so
basically if I understand the question
correctly basically in case of molecules
you're basically there is a lot of
information that is carried by the
intrinsic structure of the graph right
basically the metric structure they
let's say the the length of or the
weight on the edges but also on the
configuration of points in 3d space so
in in chemistry at least I'm definitely
not an expert but if I remember
something from my school chemistry like
of isomers basically of molecules that
have different configuration of atoms in
space but basically the way that they
are connected the chemicals what
chemical bonds are exactly the same so
in this case I think inevitably if you
want to take into account this
information you need to incorporate some
notion of extrinsic information
basically they for example the XYZ
positions of of the of the vertices in
space basically using only intrinsic
information maybe in this case gives you
too much environments but yes so what if
we want to like have rotational
invariance though with respect to our
molecule and so so the intrinsic
information
basically fuse intrinsic structure of
the graph and this is basically all the
methods that we describe so far you get
rotation variance automatically but
basically you also lose basically you
also get invariance to rotations maybe
some parts of the graph so for example
if you look at a molecule of sugar then
maybe you can rotate some of the parts
of the molecule and the structure of the
graph will still be the same but the
chemical properties will be different so
current methods that look at only
intrinsic information lose this kind of
characterization of say of the molecule
so you need to account for extrinsic
information as well thank you hi I'd
like to ask about Auto encoding with
graph convolutions so have you could you
briefly describe what a deconvolution
and then an uncool Ian would look like
yes so essentially if you know how to do
these operations in an image then we
have a dictionary where you can
translate so if that the question is how
do you do and pulling well there's a
there's always a mapping from the node
at a certain resolution to the macro
node right the pulling in a graph just
amounts to coming up with associations
between nodes right that define the
larger node so as long as you remember
these associations on your work on your
way upwards you can uncool by maybe
copying the hidden state of the previous
resolution up to the final resolution
and then iterate there or you can do
even more fancy things like linear
interpolation if you have some way like
from Pomona of an embedding okay thank
you and I should mention that there are
some recent papers that really develop
these ideas now so I clear submissions
this year there's a couple of papers
that work in that direction hi thanks
for a great talk I have a problem about
the have a problem I have a question
about the Reverend hover of the graph
convolution
and the problem is is as you mentioned
the craft collusion is isotropic so is
there a way to define convolutional
graph that naturally reduce to
traditional convolution Union run green
image yes that's a very good question so
the answer is that yes you can recover
orientations in enos in this formulation
by just feeding the network a little bit
of extrinsic information so think about
the graph convolution neural network
where the input to the graph are the
coordinates of the grid so basically
every node you would fit maybe a pair of
numbers that correspond to the
coordinates in the original domain and
the network then can can basically you
are giving the network and embedding a
global embedding that has global
orientation so then the network can
figure out how to use this information
may be to learn oriented filters and so
if you do these experiments on a
Seafarer anemones you end up basically
recovering orientation by just fitting
in like input coordinates through the
network to this question so there are
also some recent works that maybe this
is what you meant the basically they try
to pull back a convolution from some
domain with shift invariant structures
so for example if you think of of
manifolds with with with genus 0
basically that topologically equivalent
to s here you can you can map them into
into subspace this view itself for
example and you can pull back the
convert convolution from there yeah
thank you and also have a fall out
question about this so in a talk we
mentioned there are two way to model the
graph Kalusha one a dispatcher domain
why the spatial domain so what is the
main advantage of doing this in the
spatial domain because I think the
general definition of the spectral
convolution focused on the chebbi Nets
can also be seeing as a form of spatial
convolution
so if you have a single graph that
you're gonna use for all time right so
and in some sense I don't think there's
ever a good reason to do a pure spectral
implementation Yeah right but if you
have kind of a single graph that you're
gonna use for all your problems all your
all your data lie is functions from a
single graph then higher powers and
essentially that means that essentially
you're doing something on the spectral
side even if you implemented it a
different way that gives you a richer
class of filters so the more of the the
kind of higher powers of the weights you
can use the more rich your filters are
the trade-off comes when you have to
decide how how much of a new graph am I
gonna have to eat at test time but if
I'm gonna have one graph for all time
then it's okay to use very high powers
of the graph even if you implement it on
the spatial side yeah you're gonna use
essentially a spectral construction okay
yes thank you I'm afraid we're gonna
have to cut off the last question or if
you can come in the subsequent question
session we might have a chance for it
but we really need to start with the
second half of the tutorial sorry okay
so with that let's start with the second
half Thanks
and someone from sorry can someone from
the a/v team help with the projector
okay thank you okay
so thank you very much for the questions
let's continue now so basically we are
now talking about again spatial domain
methods for convolutional neural
networks but now from a slightly
different perspective basically what I
represented the graph neural networks
were a very generic method and this is
actually related to one of the question
that was asked during during the break
so when you have some restriction of
your problem for example when will be
considering a manifold and not a general
graph then you also have more structure
so in one manifold was mentioned in the
introduction you also have a low
dimensional space at each point the tank
in space and basically we can work in
this tank in space you can create
meaningful so to say charting structures
and that's exactly what I'm going to
describe in this part of the of this
tutorial so basically if we look at
different ways of defining convolution
basically we can distinguish between the
spatial and the spectral domain methods
right and we've seen these classical
definition of a convolution integral in
the spatial domain and the the spectral
domain definition using the notion of
Fourier transform so the easiest way of
generalizing convolution was just to
take what is a property in the occlusion
case what is called the convolution
theorem and make it into a definition
right we can define convolutional graphs
or manifolds as product of the
respective Fourier transforms right and
we define in turn the Fourier transform
as projection on the orthogonal eigen
basis of the laplacian so now we need to
fill in the missing piece here basically
what could be
so the way that we can think of
convolution or correlation for the
purpose of the discussion doesn't really
matter in the classical Euclidean case
on an image basically you can think of
it as the following operation you'll
take a page of pixel let's say square
block of pixels around a point in an
image you multiply it by some template
you sum up the results and you move to
the next position right basically you
slide the window across your image so we
can try to do the same thing on a
manifold but the problem is that we
don't have shift invariance so when we
move from one point to another point
their very way we extract this page
might change right because they because
of the nonlinear structure because of
the curvature of the manifold so a way
of thinking of these pages is actually
we want at each point to attach some
local system of coordinates in which we
can parameterize these pages locally so
again it's not a global system of
coordinates we still cannot do vector
space operations with points on many
phones we cannot add or subtract two
points on our manifold or an aircraft
but locally around a point we might have
this possibility okay so on manifold is
actually very meaningful what kind of
coordinates we can construct for example
we can resort to local geodesic polar
system of coordinates so if I think it's
nicely illustrated in this image so I
take a point here on the surface of the
main point X okay and they can measure
geodesic radius from this point so some
points in the neighborhood this will be
my radial coordinate I can also measure
the angle with respect to some to some
orientation to some chosen direction and
this will be my polar coordinate so I
will denote them by Rho and theta and we
can define a set of weighting functions
in this system of coordinates so
basically what I want to do in image is
basically to define to define a page
basically I want a weighting function
that selects each pixel in my in my page
right so I would like something similar
on a manifold doesn't necessarily need
to be a delta function that selects a
single pixel it could be some something
smaller okay and then we can define
spatial convolution as basically taking
this
what we call page operator so basically
computing these local weighted averages
of our function around a point and this
will give us basically J dimensional set
of numbers right that we can call with
page by analogy to an image we
multiplied by the coefficients of our
filter right our spatial template that
we denote by G and basically this is the
definition of convolution in the spatial
domain right so again here because we
work on a manifold we have this
meaningful notion of local coordinates
in particular we consider the spatial
weighting functions as gaussians with
some parameters mu and Sigma write the
mean and the covariance and the
covariance matrix we can actually write
the page operator in this way and if we
exchange the the order of summation and
integration what is written here is a
Gaussian mixture right so basically we
can apply a Gaussian mixture to our to
our function and this describes the
special definition of convolution okay
and we call this mixture model networks
or monette for short and if you look at
the structure of these weighting
functions that that define our page
operator we can actually come up with
different constructions so actually
previous models that try to do
convolutional neural networks on
manifolds use the fixed system of
weights for example the geodetic CNN's
use some some weights that were defined
some kind of radial and angular pinning
I need a tropic convolutional neural
networks that used an isotropic heat
kernels of the operation used another
set of weights and actually you can you
can learn the parameters of these guy
oceans you can introduce them as extra
variables in your in your optimization
problem when you do the training and we
can come up with basically an optimal an
optimal page operator okay so you can
also apply this idea to general graphs
now on general graphs you usually don't
have a meaningful local system
coordinate so we can use some graph
theoretical features such as geodesic
distance degree and so on but these
choices will be to some extent arbitrary
you can seal of course defining the
system of coordinates or better say
pseudo coordinate you can define some
weighting functions but basically they
will not have this nice low dimensional
interpretation as you have a manifold
okay now you can actually show that with
this construction you can generalize a
lot of previous approaches of course
including the classical convolutional
neural networks but also many other
convolutional neural network
architectures that were proposed for
angular meshes and geo graphs what is
also nice you can see the the spectral
methods in particular the chebyshev
Network as a particular case of this
page operator so if you remember in the
in the spectral neural network we had
some parametric function of the
operation applied to our signal f right
in the spatial filter we have a page
operator that is applied to our function
right and in particular in the in the
spectral filter we considered
polynomials of the operation and you can
easily see that basically these powers
of the operation can be considered can
be seen as a special selection of the
weighting functions in the construction
of our page operator so basically you
can obtain the spectral construction as
a particular setting of this of this
approach ok so let me now give you some
examples of how these methods can be
applied to applications taken from
computer graphics and 3d computer vision
and in particular I would like to talk
about actually one of the fundamental
problems in these fields namely the
problem of shape correspondence so
basically this problem underlies a lot
of applications whether it's for example
texture mapping or applications like for
example 3d avatars and and so on and so
forth so basically the our target is
given a pair of shapes we want to
associate different points on these
shapes to tell whether they match or not
can we want this to be invariant of
course to deformations right so we want
no matter in which pose the shape
appears we want still to be able to find
the correspondence so this actually this
is a motivation why we want to use in
the convolutional neural network
architectures basically we want by
construction to make the architecture to
be invariance though to these set of
degrees of freedom in this way for
example we don't need a lot of data that
were all the possible deformations
appear because by designing intrinsic
architecture
we'll go get these environments
automatically and of course we also want
to be able to deal with missing parts
normal symmetric deformations and
geometric and topological types of noise
okay so there are several ways you can
approach the correspondence problem one
may be a little bit naive but the most
straightforward just learn some local
officials right usually in computer
vision at least maybe a decade from ago
people worked very hard to construct
some axiomatic features some handcrafted
descriptors so the same effort existed
in computer graphics and geometry
processing literature basically you can
replace these handcrafted descriptors
with something that is learned in a
specific way and in particular if we are
given a training set of shapes with
known correspondence between points we
can basically apply the intrinsic CN n
to some geometric information on the
manifold and we can produce a new set of
features and we want that between
corresponding points these features will
be similar and between non corresponding
points these features will be as
dissimilar as possible okay so we use
the standard Siamese architecture for
this purpose a different approach to
modeling correspondence is a labeling
problem and this is particularly good
when you're considering a specific class
of slaves for example human shapes you
can choose one of the shapes as your
label space and then basically the
correspondence will label each vertex on
a query shape as a probability
distribution in your label space as
basically denoting the probability of
point x corresponding to point y right
and if you have the ground rules
correspondence you can actually measure
some distance between the contras
distribution and the distribution that
is produced by the network minimizing
the standard cross entropy or logistic
regression function okay and the way
that we evaluate the quality
the correspondence is again looking at
the correspondence that is produced by
the network comparing it to the ground
rules and at each point measuring the
distance the geodesic distance on the on
the reference shape that basically
measures how far we landed from the
desired correspondence okay and we can
look at the percentage of
correspondences that fall within certain
geodetic era radios and basically that's
the what is called in computer graphics
community the Princeton correspondence
benchmark basically these curves plots
the percentages of correspondences
versus the threshold the error radius
and the higher the curve is the better
so you can see that for example when
using an intrinsic architecture like the
mixture model networking get
significantly better performance than
standard methods that do not use
learning such as blended intrinsic Maps
so maybe a different way to visualize it
we can show the point-wise
correspondence distortion basically the
deviation from the crowd rose at each
point here you can see what the blended
Maps produced so this was one of the
state-of-the-art methods in computer
graphics that actually doesn't rely on
learning it produces nicely looking
correspondence maps but if you look at
the distortion it can be pretty large so
here it's calibrated in in absolute
value absolute units 15 centimeters is
about the size of your palm right so
it's pretty big error this is what we
get with you day 16 and so it's
significantly better basically colder
colors closer to white represents more
errors but there are still some spools
points where the correspondence error is
large this is what is produced by a
nootropic CNN and this is what is
produced by bonnet which produces very
nice correspondences maybe another way
of seeing it is by applying these
correspondences to texture mapping so
you can see that the texture mapping is
almost perfect there are almost no
noticeable distortions of this texture
so this is another example of some more
challenging cases where you have
actually both missing parts and
topological noise and you can see still
that the this framework produces very
nicely looking correspondences with
small with small error
okay so what is the problem in general
of this classification based approaches
that actually doesn't distinguish
between the point that deviates
significantly from the ground rules or
it deviates just slightly so basically a
correspondence that that fall lens here
or lens here were the green one of
course is more distance from the ground
Rose will be considered the same
basically will be considered as
incorrect classification so a better way
is actually to consider the probability
distribution itself and use it as a
weight to weight the geodetic distance
by this by this probability distribution
so in this way we can account for what
we call the soft correspondence error
now the second problem with this with
this approach is that actually we don't
impose any structure on the
correspondence during the inference time
of course during training we can't
analyze for four correspondences being
distant from the ground force but
nothing guarantees that when we apply
our learned model to a new set of data
we we obtain that two nearby points will
be mapped or two nearby points on the on
the reference shape on the target
manifold so in computer vision basically
this kind of problems is usually
referred to as structured out what our
structured prediction so we want some
intrinsic experience of structure
prediction and the way of doing it is to
restore the resorting to what is called
functional map so I will not go into
details basically the idea is to the
following as before is in the siamese
metric learning we produce some some
features at each point of the of the of
the manifold that now B instead of
comparing these features will compute
their Fourier coefficients so basically
we will project them on the laplacian
eigenbasis on each of the shapes
separately will have two different paces
and then using these fourier
coefficients we will compute a matrix
basically linear operator that
translates one set of real coefficients
to from from one basis to another basis
right because we know that these pages
usually behave differently thought this
is releases was actually one of the
reasons why spectral learning methods do
not generalize well across non isometric
domains so basically these metrics
exactly tells us how to translate one
set of code
another one and it actually has a
closed-form expression as the
pseudo-inverse of this feature vector so
basically we need here to back propagate
through this pseudo-inverse once we
computed this spectral representation of
the correspondence we go back to the
spatial domain and basically you can
interpret it as a probability
distribution if you take the absolute
value and normalize it to sum to one
then you get basically this
interpretation that Delta function
corresponds to some blob some
distribution on the target shape and
basically we use these the probability
distribution in our cost function ok
this produces significantly better
results than all the previous models so
this is the first benchmark on which the
performance almost reaches 100% so this
is currently state-of-the-art in these
its particular type of applications okay
so let me show you another type of
applications and this is related to
matrix completion and recommender
systems so this will likely involve
products of graphs or multiple domains
ok and I guess you all know about the
Netflix problem so this is a problem of
recommending movies on Netflix so
Netflix has tens of millions of users
and probably hundreds of thousands of
titles and the users can give different
scores to the movies depending on
whether they like them or not so you end
up with a huge matrix that is only very
sparsely sampled basically every person
even if he or she spends all their life
time watching movies on Netflix probably
they will see just a small fraction of
what is available and basically they
wants to fill in all the missing
elements and the classical approach is
minimization of the rank of the matrix
basically try to fit to the data some
low dimensional structure that explained
explains this data that is consistent
with the observations because
minimization of the rank is is an
np-hard problem it is usually
approximated by basically by convex
boxing what is called a nuclear normal
the problem with this formulation is
though is that it doesn't involve any
structures or if
shuffle the columns and row of the
matrix you will get the same rank for
example if one of the columns is
entirely missing then you can fill it in
infinitely many ways so a different
approach would be to incorporate some
information it can come actually from
the data itself or it can come from some
side information that allows us to
define affinity of different users or of
different items in the form of the graph
so basically we can say think of social
network and you can come up with a model
that tells you that friends will usually
share their tastes right so we can talk
about notions of smoothness of these
matrix column wise or row wise with
respect to these graphs right and you
can basically replace one low
dimensional model basically the low rank
model you can replace it with a
different low dimensional model that is
understood that the rows or the columns
of this matrix can be expressed with a
small number of laplacian eigenvectors
right basically it's a low-pass filter
in a sense so you can do it for using
only one graph or the users you can also
use it a graph for the items so
basically now we have two graphs on
which this matrix leaves and of course
it is possible also to factor the matrix
into column and row factors and
represent it with smaller number of
variables so I will not go into the
details basically these are some
standard techniques so address these
matrix completion problems okay so
basically if you think of this model
where we have this notion of smoothness
of applied to a matrix basically it's a
low pass filter right and of course
nothing in the world tells us that
that's necessarily the best filter so we
somehow as we've seen in the previous in
the previous approaches we wanted to
learn optimal filters that describe our
data in the best way and now the
difference from the previous cases that
we have actually two graphs right and
the analogy would be a two dimensional
Fourier transform so think of how you
compute Fourier transform of an image
usually you would apply free transform
to the columns and you apply the free
transform to the two rows and basically
this combination of two free transforms
gives you a two dimensional for it
Orphanage you can also write it as a
Kronecker or tensor product okay if you
pass your image into basically if you
stuck it into column or row stack so the
same thing can be applied in this
setting instead of basically row wise
and column wise Fourier transforms now
we have Fourier transforms on the
respective graphs of users and items and
the Fourier transforms are as previously
understood in the sense of projections
of all the respective application
eigenfunctions so basically we can
define the multigraph Fourier transform
in this way basically we multiply our
data the matrix acts by the location
eigenvectors from the left and from the
right and we can define multigraph
spectral convolution exactly as we've
seen before basically as element-wise
product in this two-dimensional free
domain okay and as before we don't
necessarily need to explicitly compute
the eigen vectors of these equal
operations we can privatize our filters
as polynomial functions of frequencies
now this will be p variate polynomials
they will depend on two frequencies the
eigenvalues of the role operation and
the eigenvalues of the column location
more coefficients now in these
polynomials but the idea is exactly the
same so again if you think of the
spectral transfer functions of such
filters you can think of Fourier
transform of an image and now we have
two frequencies all these filters will
be two-dimensional so overall basically
this construction of multigraph CNN
allows us to extract spatial features
from the from this matrix and we can
come up with basically with a sequence
of updates that change the score matrix
based on these features for example
involving a recurrent neural network so
I will not go into details you can
actually see the you can see a posture
that will be presented in on Tuesday
that describes in details this approach
let me just show you how it works
basically this is a synthetic example we
have matrix that is sparsely sampled all
the blue values are zeroes and basically
this is a learnable
fusion that diffuses these elements and
missing elements of the matrix here you
see that the root mean square error is
pretty large the score here ranges from
0 to 5 so this is more or less meaning
that the scores are been in class so
this is our initialization after five
instances of this of these iterations we
get significantly lower root mean square
error this is what happens after eight
instances and this is what happens after
10 instances so we get almost perfect
reconstruction so this is of course in
that if example we don't have any noise
but it just shows that the model works
and these are some real data sets that
are typically used in in this community
of a recommender system and matrix
completion and again we get some good
results with these models on these data
sets okay so I will now pass the stage
to Joe and we will talk about
applications in particle physics and
chemistry and we'll conclude this
tutorial with some future directions and
concluding remarks so here yes so we are
going to conclude this tutorial with
just a final batch of applications and I
just wanted to yeah so let's first
discuss about the applications in in
chemistry so so in quantum chemistry
that there's many in many areas where
this thing could be potentially use but
in particular that the general setup is
that for a given molecule were
interested in predicting a bunch of
different things right that might be of
interest for describing the properties
the macro properties of the molecule
like the energy or the vibrations on the
other electron energy cat for example
and so typically this this problem is
addressed with something called a
density functional theory which is
and complicated and sophisticated
technology from quantum mechanics that
deserve the Nobel Prize and so that the
question of the order the motivation
here is to what extent one can define
models that are data-driven that somehow
provide an alternative to this density
functional theory that is a perhaps more
efficient like less computationally
expensive and potentially also more more
precise to some of the some of the
problems and so this is a actually an
area that kind of sparked and motivated
a lot of research in graph neural
networks and so I will just wanted here
to mention some of the the works that
addressed this problem and at the same
time developed important updates on the
basic way that design and architecture
of these models so just to mention a few
so there was this first work by a lead
to Bernard and his group where they they
used essentially they use the model that
was a GNN like an altar described but
where depending on the degree of every
node you would use maybe a different set
of parameters then there's a family of
works that introduced is this notion
this idea that also we describe the
interaction and that altar also
mentioned were a very natural way to
enhance and to make these models more
powerful it's not not only to propagate
information across the nodes through
some fixed adjacency matrix but also to
learn how to modify these adjacency
matrix I in a data-driven way and so
this was done by these series of papers
and leading to the the recent paper by
Google brain where they coined this name
de niro message-passing for chemistry
and all this all this works what in many
of the data sets that they are tested on
they reach what's called the chemical
precision which means that statistically
speaking they are they make relative to
the density functional theory is of the
same order as the error that the thanks
functional theory does with respect to
the ground truth so so now these opens
the question the door to maybe the need
to develop maybe larger data set or more
precise data sets
- maybe verify whether these models can
not only match the DFT but also improve
in some of the in some respects and so
ok so this is for the quantum chemistry
and I just wanted to give you some
background so let me talk to you a
little bit more about another set up
where these things used sorry about the
picture that is kind of slanted but so
this is supposed to be the alleged
scenes of the Large Hadron Collider in
CERN and in this big big experiment
there's in particular one that we care
about that is called the Atlas which is
a detector for that was one of the
responsibles of the Higgs boson some
years ago and here we are interested in
answering specific questions on very
basically particle at the fundamental
level and so that's the kind of picture
that you can that we can we can think
about this experiment is that we have
some maybe some theory right we have a
model for like a four fundamental
particles how they interact and these
particles when there's a when there's a
collision
they are predicted to behave in a
certain way to produce certain events
and these events as you might expect
they propagate right they create even
more and more events that make modeling
process modeling more and more
intractable and so the this is the like
the theory but then the the thing that
is actually detected by physicists by
explain by experiments it comes from
these I mean this complicated machinery
but it is that the the particles that
crash they they trouble several
different layers of detector each of
them of very different technology etc
but at the end of the day the goal is
that given some event that corresponds
to
that can be described as a set of
different for momentum particle
particles x1 to xn the the experiment
has designed a simulation that depending
on whether we are to expect a phenomena
for example here that W boson should
behave a certain way and if it's not it
should behave in a different way right
like what we call the background signal
and so this this problem has already
been addressed using a deep learning
data-driven models mostly using two
paradigms one is to think about what the
detector C is trying to map it to an
image to by by by interpolating the
input for momenta into the appropriate
image on a regular grid the other other
approaches are based on the exploiting
the structure the tree structure of the
tree to use like recurrent neural
networks models so here shifted so so
here what what what the detector sees is
more naturally this four momenta can be
embedded can be seen into a like a
native four dimensional space as a point
cloud right and so here in this in this
point cloud what we do is that we we
construct a graph neural network where
the adjacency is learnt and here what we
observe is that this model currently is
obtaining the best efficiency the best
trade-off between detection and
efficiency as previous approaches and if
you are interested and you want to know
more about this particular application
there's a there's a talk in the in the
in the nips workshop in machine learning
for physical sciences where and Isaac
Arian here the student at NYU he's going
to explain this this thing in more
detail okay and so yeah and these are
the the other directions were this
applications of message passing networks
to
physics these are the sort of questions
that currently we are trying to address
so in particular one of the important
limitations of this model is that if you
want to learn a priori how every
particle actually particle these scales
quadratically with the number of
particles which makes it very in track
when it makes it intractable very fast
so one of the one of the important
questions here is how to devise
mechanisms to scale these things to
larger domains okay and this is the talk
if you want more details about the what
is this work okay and now I'll just
conclude with a last application where
we can apply these these models of a
crab neural networks so this is the
what's called the quadratic assignment
problem so it again it's a very
classical problem in operations research
and in commentarial optimization and
it's this goal of trying to match a
collection of resources like you know a
bunch of factories to a collection of
like destination weather like maybe
locations and so what you want to try to
optimize is a trade-off between how much
do I need to communicate between
different factories and how far away are
the different sites in which these
factories can be can be put right and so
in mathematical terms you can you can
formalize this problem as trying to find
the best matching between the two graphs
that optimizes I kind of had I would say
like a correlation or like that you want
to try to to permeate the nodes of one
graph such that the permitted graph
looks as close as possible to the second
graph as measured by this cost function
so because we are optimizing over the
space of permutations this is not
surprisingly a very hard problem to
solve so it's actually np-hard and you
can prove that you cannot approximate it
with polynomial time and it includes
that the Traveling cinnamons problem as
a particular case and so typically the
way what people try to do is to relax
the constraint of permutation
matrix into us into a constraint set
that is a bit more tractable for example
by using a spectral approach or by using
semi definite programs so how can we use
graphical networks to address this
problem so for this we need to generate
data right we if we want to use like a
data-driven approach we have to generate
data so the way this thing with we
consider to do is to first look at this
problem in like specific families of
random graphs that capture some of the
difficulties of solving this problem so
what the way it works is that we we
sample I graph from one like a random
family for example Altos rainy or like a
random regular graph w1 and then we
generate the second adjacency matrix as
being a generic like a random
permutation of the first one plus some
noise and the noise is just taken to be
our display and so as you can expect the
the goal of recovering this permutation
is going to depend on the
signal-to-noise ratio right the amount
of noise that we include into the system
so what we are trying to do is we we map
this quadratic as a quadratic assignment
problem into a linear assignment problem
by taking the graph G and embedding each
of these graphs with the same is reka
texture that is going to produce for
every node so a bunch of features and
now once we are in this feature space we
can just solve the linear assignment
problem and so we we can do that by
essentially unrolling the similar
algorithm that what it tries to do is
that transforms the gram matrix of the
features into a permutation matrix by
applying a softmax that every along rows
and columns so this architecture now can
be trained and to end on this random
graph family and what we what we observe
what we obtain is preliminary but
encouraging results so on the under are
those rainy model what we obtain
something is a bit on the x-axis you
have the amount of noise and on the
y-axis you have the accuracy or how much
break how much how well are you doing at
recovering this problem and so what you
can see is that in both cases that in
red you have the data-driven model does
basically as good as or much better than
the standard ways to relax this problem
but not only that but at that speed that
is very interesting right so here all
the operations that we are considering
there's nothing that scales worse than
an N squared so this is encouraging
results and there's a lot of things to
understand yet why this thing works well
but as Michael mentioned some
applications to shape correspondence
shape correspondent is clearly an
instance of an assignment problem that
is quadratic right you want to match
basically two manifolds one to each
other so an obvious thing that we are
currently exploring is how to use this
this architecture for the shape
correspondence and now I want to just
briefly mention about some of the other
feature directions that involve graph
neural networks in some form so one
interesting and obvious one is to do a
generative model right so we have seen
very successful generative models for
images in the last year's so one
question is well what is the equivalent
of a generative model for graphs so here
there's this these are pictures from
anonymous submission from my clear
may-maybe the Alpha is among the
audience it's anonymous I don't know so
here are some molecules generated by a
variation auto encoder which is a
trained on by using encoders and
decoders being rough neural networks
there's other examples of synthetic
synthesis with applications to graphics
so on on the first the first example you
have this problem of
inferring a shape from a from a point
cloud that can be partial using also
again ideas from version alot encoders
and on the bottom you have an example of
temporal prediction and there are
nonlinear dynamics right where where
here the we just throw shapes on the
floor and we ask the model to try to
predict the dead in the dynamics and so
so these are examples and applications
that naturally they what you would like
to do here is to exploit the intrinsic
geometry of the object right by using
the appropriate rational networks so
this is just wrapping up the the second
part so and maybe the also wrapping up
the tutorial so what we have tried to
convey here is that many of the reasons
why convolutional neural networks work
well they expand beyond the occlusion
domains right so in particular this idea
that you can share parameters across the
domain and you can use you can learn
step by step by using local models is a
property that not only can be used in
general domains but we have reasons to
believe that there are many tasks that
catch the word this inductive bias is
the right one and so we have seen two
different approaches to accomplish to
achieve this good trade-off between a
locality which sharing and complexity
one of them is using that basically the
spectral properties of the graph the
other one is directly working on the
spatial domain and as Arthur mentioned
these transfer architectures that we are
that we that we define they are also
very related to attention mechanisms
that have been so successful specific
especially in the domain of natural
language processing and so as I said one
of the things that we still don't know
how to do or that are completely open is
how to scale these models so that so
that they can process maybe
systems on domains that have large
number of node maybe millions and we are
also interested and if you if you have
questions about this we can of course
very happy to discuss offline of where
can these models be applied because once
you start having a neural network that
can understand well how to process like
a set of inputs and learns about how
these things should be related to each
other we believe that there are many
areas where this thing can be
potentially useful and so with that let
me see if it is yeah so just ending with
a advertisement if you liked this
tutorial there's a an upcoming workshop
at the item the Institute of pure and
applied mathematics here in los angeles
in a couple of months where we are going
to have a week-long
like a week-long sessions around
basically methods and applications of
these ideas and here's some bible
ography everything is going to be put
online and with that we just thank you
very much for your attention and open
for questions now
yeah so we have plenty of time for
questions if there are questions please
can you line up by the projectors again
and I guess if Mac swelling is ready
then we can start already I thank you
for that wonderful tutorial so I have a
question so I'm in group convolutional
neural networks you also they're also
driven it's also generalization of
convolutions and it's also driven by
beautiful harmonic analysis so do you
see whether what you presented is maybe
a uniform unifying framework or do you
see relations between the two yeah
that's a very good question so it's it's
true that if you can think about a group
as a very nice-looking symmetric
manifold right it's a manifold that
comes with extra with the bundles right
it comes with operations that are akin
to the orientation that Arthur was
saying so I think that I think about the
group convolution networks that exploit
this proof structure as being a kind of
a generalization that really stays in
this in this case where you do have
orientation right and the question is
how do you exploit it and and it's yeah
and actually this is a there are
probably examples on domains where if
you know that there's this group
structure you should use it so in a
sense what we presented here is a is a
set up that is a bit weaker because it's
it doesn't exploit this proof structure
but on the other hand it's a bit more
general right because you can apply it
even if you don't have that group
structure right so so you you don't even
have that you know like the manifold at
every point the neighbourhoods look very
different right and so but it's a it's
and actually would be fascinating and
interesting to maybe think about
combining the two right how could this
thing could be combined that's a very
good point thank you all right and on
the other side thank you so much for
your presentation
just one question with regard to the
mixture neural network that you
explained earlier so as far as I
understood the patch operator that you
learn has a fixed size whereas you know
on graph we can have you know different
size of neighborhoods so how does that
deal with you know that because you have
a Jade the number of kernels that you
learn sure so the size basically the
size of the page is in these cases
understood as the number of the
weighting functions the way that
basically the weighting functions are
something that is local so basically the
function defined on the mini phone that
it it basically it has a non zero it has
support only around the vertex so
basically this construction can eat
neighborhoods with any number of
vertices round around the vertex
basically the use of certain number of
weighting functions is basically is what
defines the dimension of your and one
you know the difference between the
traditional convolutional neural network
and this type of neural network and you
mentioned you are also trying to learn
the patch operator on the top of the
filter itself so does that bring any
specific benefit to the table well so
you probably can do the same thing for
images I'm not sure it is it is of any
benefit probably again because images
everything is so well and the students
all so optimized but we've seen that
basically it gives an advantage because
rather than for example we don't know
exactly what's the size of the page that
you want that we want to use so in the
in this way basically when your allow to
optimize also for the positions and the
and the orientations of the of the
Gaussian kernels that constitute the
weighting functions of the page you
basically this way you can also control
the size of the page right you learn the
optimal size so maybe these ideas could
also be used in images again we know we
never never tested it thank you
thanks so much for the great talk um I
have a question about these methods and
how they generalize for manifolds with
genus greater than zero earlier you
referred to the deformation
correspondence case for genus zero but
I'd love to know more about jeana's
greater than zero
sure so the genus actually with this
definition of convolution if you if
we're talking about spatial convolution
basically all the operations are local
so there are some subtleties so for
example if you construct the patient
using geodesic system of coordinates
basically the radius of the page must be
must be smaller than the what is called
the injectivity radius of the manifold
so basically the injectivity radios
guarantees that this local neighborhood
has has the polity of a disc but
ignoring for a second leasing it can it
can work so maybe when you have some
topological noise some points where we
have short cuts so maybe at this point
the construction of the page will not be
completely valid but basically there may
be the output of the network will be
noisy but otherwise it's a local thing
so it's it works it works on different
topologies it doesn't need to be G no
zero most of our examples that I showed
most of the human shapes there are at
you know zero but some other examples
for example we have missing parts or
connectivity noise they have arbitrary
nose thanks hi had a question about the
application so the indicates of the
point point correspondence from a point
cloud through the human body model so
you've shown examples where there's a
significant amount of missing data but
have you tried it out with examples
where there's some external object in
the point cloud say say for example the
person is holding a mug or a pole is in
front of the person so you're talking
about about the setting of clutter right
so you're not only competing parts but
also some some clutter so well I don't
know it will be slightly longer answer
so we showed you the application that
they showed the best-performing
correspondence is the fun
non-network basically where the
functional maps are incorporated there's
a differentiable layer in the core in
the in the architecture and the
correspondence is learned end-to-end so
basically we have a generalization of
functional maps to the partial setting
that is also capable of dealing with
clutter we still haven't done it
basically we haven't incorporated in in
an end-to-end learning system but it
should be rather straightforward and
very similar to the function lab Network
to do it so probably that will be the
answer thank you okay I'm not sure if
there are more questions but if anyone
thinks of any question I'll ask one just
now I had a question actually from the
first half of the talk you were talking
about how to do basically pooling layers
for graphs now if things happen in in
graph neural networks in the same way
that they happen and many other types of
neural networks there's gonna be a lot
of hacking and a lot of let's say
engineering driven network design are
there things that we need to be
concerned about about let's say
stability or convergence to something
sensible some sort of sensible function
like what are the conditions on pooling
that are actually important to make sure
you learn something that's going to
really be like a general thing
sorry I'm not 100% sure I understand
what you mean by stable so I I mean in
general when we do pooling on a graph
what it means is you have to have some
sort of clustering structure right you
have that sum and I guess what you're
asking is how can I make sure that the
pools are saying this is what you're not
asking yeah yeah how can you make sure
that when you perform that pooling it's
not that you have a very minor
perturbation of your data or or some
structure of your graph and then all of
a sudden you get a very different
structure I see okay so um in some sense
you cannot guarantee this for most I
mean this is the problem with clustering
or clustering in general right but there
are several things you can do and one of
the kind of most effective I think
things you can do is what's called a
cycle spinning in the wavelet or you
know kind of harmonic analysis community
and that's you don't cluster once or you
don't pull once so what we're used to
doing in commnets is we have a grid and
we have pools right and those pools are
defined once and for all you have one
set of pools
what is a saner thing to do in kind of
dangerous situations where the structure
might you know a little wiggle might
change the pools is you you recompute a
pooling structure of 15 times and you
average over you your average outputs
over those and this tends to be a very
kind of robust defying think it's
actually done in Euclidean cases as well
to deal with edge effects and things
like that and wavelet transforms but
okay so then the cost so that are an
additional constant 15 times something
but it's not so bad
okay well it seems like the questions
are done so let's think the speakers
again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>