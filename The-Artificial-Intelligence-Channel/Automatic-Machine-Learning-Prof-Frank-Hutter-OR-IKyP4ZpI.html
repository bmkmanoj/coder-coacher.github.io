<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automatic Machine Learning - Prof. Frank Hutter | Coder Coacher - Coaching Coders</title><meta content="Automatic Machine Learning - Prof. Frank Hutter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Automatic Machine Learning - Prof. Frank Hutter</b></h2><h5 class="post__date">2018-04-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OR-IKyP4ZpI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the next speaker is Frank cutter his
professor in Department of Computer
Science in the University of Freiburg
Germany Frank is very well known for his
work in Bayesian optimization and last
year your team won the competition of
Otto a male right yeah that's the
amazing achievement and Frank is here to
tell us more about his work on Bayesian
optimization and automatic machine
learning Otto ml thank you very much for
this nice introduction and for the
invitation
okay so I'll be talking about Otto ml
and how to speed it up to motivate Otto
ml we've already heard some in the
beginning from quark but let's look at
how um the current deep learning
practice happens for when when a machine
learning expert wants to fit a model on
a new data set so first they need to
decide which architecture to use and set
some hyper parameters and then then they
can run that deep neural network but
typically the very first time they run
it something comes out that they're not
quite happy with maybe learning diverged
or some other hyperparameters weren't
set quite right so they do need to
iterate and and this becomes an
iterative manual process of actually
looking at the architecture xand hyper
parameters and this process also depends
on a lot on how good this machine
learning expert actually is how much
expertise they have and how much
experience they have built up over years
in this art of deep learning that has
also been called alchemy earlier this
week and what we're trying to do is to
move from this this art more towards a
principled engineering science that lets
us define when to use which sorts of
methods and that can also lead to in the
end true end-to-end learning
by which I mean the data goes in you
press a button and then the machine
learns because in the end it's called
machine learning and not human expert
learns about how are the other machine
learning method works so this this
entrant learning can of course be
implemented by some sort of metal level
learning and optimization that learns
about this learning box and optimizes
his learning process and basically as
escort set automates the machine
learning expert one way of treating Auto
evolving it is as a blackbox
optimization problem so that what just
replaces learning box by a black box and
in goes some hyper parameter vector
lambda and this this lambda can be an
object that's actually structured that
can have all kinds of yeah it can be in
your network structure so it there can
be a lot of structure in there it
doesn't have to just be continuous hyper
parameters and outcomes and f of lambda
that that's a performance of this neural
network and then we can use our favorite
blackbox optimization methods and today
we happen to talk about evolutionary
methods and about reinforcement learning
and about basing optimization and and
all of these could be used in order to
optimize this black box function I'm
actually very pragmatic about which
method to use I've used all of them but
I guess today I'm here representing the
faction of phasing optimization so I do
want to take one minute to clear up on
misunderstanding about Bayesian
optimization which kind of do - well
happened after a blog post from Ben
Rusch last year within which he talked
about a really nice method I'm hyper
band but as part of that blog post he
also mentioned that sometimes based
optimization is not actually more than
twice as fast as random search and
that's that's absolutely true I was not
surprised by that somehow a lot of
people were sometimes Bayesian
optimization is not more than twice as
fast and better than random search
sometimes actually in the beginning of
the search when when you know nothing
based optimization is not faster than
random search at all it needs to explore
the space first and kind of figure out
where to go downhill and then then learn
something about the response surface and
then figures out
how to get to the global optimum and
then start speeding up a lot so as time
goes by typically we see that based
optimization does a lot better than
random search and this can also um yeah
the Equator and that dramatic so
sometimes that we really see orders of
magnitude speed ups over based
optimization over random search
note that this is actually not yeah only
the case for based optimization here's a
figure that the Quattro recognizes from
his paper and your architecture sort of
a reinforcement learning and we see that
up to a thousand two hundred function
evaluations of reinforcement learning
was not better than random search here
but with larger budgets of course a is
greater and greater improvements and
that that's what you would want from any
any reasonable optimizer so this is a
positive result and nobody would go and
say look at the first 1200 function
evaluations all reinforcement learning
is not better than random search it
would be the wrong conclusion to draw
from this figure that's pretty much all
I want to say about Bayesian
optimization versus reinforcement
learning versus evolutionary methods in
the rest of the talk I will be using
based on optimization but most of where
I'm going in this talk is actually very
much in line with Andres post on hyper
band that we should not necessarily
limit ourselves to this black box view
because this black box view is very
limiting and a lot of people invest in
optimization actually do use this black
box view and kind of the first sentence
in the papers let F be a black box
function and well yes it can be a black
box function but it can be more than a
black box function and if we treat hyper
parameter optimization of deep neural
networks as a black box and while a
single function evaluation will just
take weeks and that's a little slow of
course we can paralyze this also based
optimization but it's just not the most
efficient use of time okay so I do want
to talk about very briefly three auto ml
systems and while I talk about them I'm
give some hints on 6 ways of how to go
beyond like Mars optimize
and I'll denote these by these little
symbols so the benchmark here is the
Sauter melt challenge that I mentioned
this was a very large-scale challenge
one and a half years organized by child
learn and code Ella
Isabelle goo young there were two tracks
code submissions and the CAG like human
track and the code submissions are the
most interesting part there you really
needed true end-to-end learning so you
submitted a system that reads some
training data builds a model and
predicts on test data all end to end
without any human in the loop there were
25 data sets and one thing in the
challenge was they were already feature
rised so deep learning did not play a
very large role and because a lot of
datasets out in the real world are
already feature rised what what we did
actually back then first for Auto ml was
to parameterize the framework Weka which
is a machine learning toolbox in Java
that's probably not very well known in
this community but it's heavily used in
the life sciences for example in Weka
has something like 30,000 citations so
it implements a lot of different machine
learning classifiers and each of them
have a lot of hyper parameters as omble
methods and if you parameterize it you
end up with since unite 786 hyper
parameters and what we did back then is
simply see what happens if we just
optimize cross-validation performs by a
based optimization here the the first
speed up trick comes in
this is cross-validation performance we
know that cross-validation performance
is an average of the individual
performance of the fault if we have 10
folds and on the first fold our
parameter configuration is already very
poor then it doesn't make a lot of sense
to evaluate the other 9 folds but we
would rather want to move on and save
time so you basically have a little mini
bandit for every configuration and if we
exploit that then we can actually get
5volts beat ups for a 10-fold
cross-validation one one nice thing
working an auto ml that allows us to
actually put software online that a lot
of people can directly use that don't
maybe know machine learning or that
haven't taken a lot of machine learning
courses that are not experts but they
can just use it out of the box
I'm the second system is auto risk
alluring so that's actually fairly
similar but it at parameterize a second
learn a toolbox
one other speed-up trick we use here is
well through meta learning or reasoning
across datasets given a new data set we
looked at how similar our previous data
sets we've seen before and what we're
good algorithms and hyper parameter
settings for those and run those first
in order to kind of bootstrap and get us
a very nice initialization for based on
optimization and that that really does
dramatically speed up the search also
when you run based optimization you get
all these different function evaluations
you have all these different models and
it would be kind of sad to only return a
point estimate in the end it's not very
basing you would rather want to return
some sort of ensemble and you can build
these assembles basically for free and
in particular for small data sets that
actually helps you a lot with robustness
I'm authorized to learn as mention did
when the some competition it's really
ready for primetime there was the
automatic sister station a variant of
that I am James Lloyd was in the
competition for the first two phases and
was really head-to-head so that's great
and then they dropped out so then after
that it was really easy so always to
learn is a plugin estimator a plugin
replacement for us I could learn
estimators and you can use it in four
lines of code and basically here you
just put it in your extra inviter Ain
this line here will run as long as you
tell it to run maybe they and then you
have a predictor and this is an open
source project so if you feel inspired
please contribute that would be very
nice I'm the third system is auto net so
we parameterize deep neural networks the
network architectures and hyper
parameters for the auto ml challenge
that's the only instance iation I have
time to talk about right now and there
we had feature-wise data so we had
really kind of boring networks I'm just
up to five layers fully connected only
29 hyper parameters and we optimized it
on less than a day in for cheap use and
an actually alternate did win several
datasets against human experts in this
in this human track and to the best of
my knowledge that's actually the first
automated deep learning system to win a
machine learning competition data set
against human experts but auto net is
still kind of working a lot on on the
blackboard so kind of evaluating
cross-validation performance or is still
relatively slow and for large data sets
that that wouldn't really am scale up so
for large data sets what you can do is
to work on cheap approximations of this
black box so I'm brittle eyes by this
here we see a response surface of an SVM
fit on em nest this is the whole MLS
data set 50,000 data points and this is
400 data points and we see that the
response services actually look
qualitative really quite similar but we
can evaluate this cheap black box here
about ten thousand times faster than
this black box so we can do a lot of our
work on the cheap black box and then
slowly work our way up in this case we
use base and optimization to have
another dimension in there which is the
size of the data set and got up to
10,000 up to 1000 for speed ups for
large data sets for confidence we only
got up to ten fold speed ups because
they only scale linearly in the number
of data points another way to
approximate the black box is to closer
to the microphone I I already get a
feedback so I don't I don't think that's
a good idea
do you hear me
are getting approved for you I hear a
really bad echo so I will try it's quite
irritating so another way to approximate
the black box is to work with less
epochs
so if we have hyper parameter settings
that are poor then then sometimes you
get learning curves like this and of
course we want to cut these off so we
can use a model to predict when when a
learning curve will not be very good and
cut it off and then have kind of a model
based early termination criterion like
this that they're qualitatively and
quantitatively lead some Speedos
um I promise to talk a little bit about
hyper band so this this is a way to
actually combine these this different
ways of thinking about approximate like
boxes so it has this notion of a budget
and it starts with a small budget the
budget could be number of function
evaluations or it could be a subset of
the data or a number of eport and it
starts with a lot of configurations for
a small budget and then iteratively
selects a half at the top half of these
configurations and gives it double the
budget and that that of course puts a
lot of lets us explore a lot of
configurations for very cheap hyper band
basically is a wrapper around success of
having that that starts as there is
different on trade-offs of the number of
configurations and the budgets and what
we see in practice for have event is it
because it spends a lot of effort on
these approximate like boxes it's very
good in terms of anytime performance so
here comparing two random search in the
beginning we actually get a speed-up of
twenty fold and in the end while it's
only a three fold speed up because well
also hyper band does evaluate on the
full black box at some point and then at
some point the speed ups are not that
large anymore if you remember from the
beginning based on optimization is the
reverse in the beginning we don't have a
speed-up of a random search because we
start evaluating with a full black box
but over time as we explore the space we
can actually basically follow gradients
and get
good evaluations quickly so the two are
very complementary and it made a lot of
sense to combine the best of both worlds
so that's what we did and actually works
on paper here so this is also very new
not from yesterday night but from two
weeks ago and get strong anytime and
final performance okay um yeah we can
also parallel Isis I'm this paralyzed
beautifully so it's also a myth that
based optimization doesn't paralyze one
has to work a little bit harder and it
doesn't first paralyze perfectly but but
almost perfectly
so where's eight workers we get seven
point five fold speed ups here we use
this on being a research lab we don't
have 800 GPUs but so we try to tune
cnn's on a budget so we said okay the
the maximum budget is you can use two
hours on a Titan next and then we ran
this procedure Bob for twelve hours and
ten GPUs and the end result actually had
a test error of four percent it renders
a gain with a budget of three hours for
Titan X and then got down to 3.5% so
within three hours you can get to a
three point five percent and you use a
total budget of twelve hours on 10 GPU
is so that's an equivalent of about
forty full function evaluations and
you're trying to push this further to to
the point where you might only need an
equivalent of five function evaluations
or so okay so the the sixth and final
way to go beyond blackbox optimization
is to adapt your hyper parameters and
your architecture online during the
search so to go beyond starting from
scratch with every single hyper
parameter configuration
I'm the way you can do this in an
architecture space is by exploiting
network morphisms which work as follows
so you have a current model with some
partially trained weights we you know
the architecture here by these four four
bars and we for example make one layer
wider in this child here we make we put
an additional layer in here we may be at
the skip connection and
network morphisms allow us to get the
same function as before I'm so queen we
instantiate the new weights here in
order to base on to keep the same
function so for example it's just 98
mapping for for a new layer and then in
the next step you can train each of
these child networks for a little bit
and we found a very quick cosine
annealing - I'm already yield to large
improvement sometimes and then look at
the performance of each of the
individual models pick the best one and
then iterate this process and of course
you could pick a couple of best ones and
and have a beam search like this yeah
and so yeah then iterate that and
iteratively you make layers wider or add
a layer or add skip connections etc and
by not optimizing each of these networks
from scratch but by exploiting this
partial or partially pre-trained
networks we can actually do architecture
search on a single GPU and half a day
that gets us at least five point seven
percent on C far and and of course this
this process here paralyzes beautifully
so um if we add more resources to this
I'm actually quite confident that this
could lead to to a new state-of-the-art
performance in the future we're not
there yet but III think this is actually
a really exciting area to look at ok was
that let me conclude
so based optimization enables true and
trans learning um I shot these three
different auto email systems and we can
get large speed ups by going beyond just
black box optimization and based
optimization is not limited to black box
optimization we have links to code
online and thank you for your attention
so you briefly talked about using I
guess function evaluations from previous
data says as a way of work sorry so I'm
wondering how much that depends on the
similarity of the previous data sets to
the current day the same okay I'm very
good question it it definitely depends
on the similarity of previous data sets
to the current ones so what we do is
actually um in this particular setting
we we actually looked at a lot of
different data sets that we had run on
in the past and had a similarity metric
between data sets using meta features
and work what started from the from the
data sets that berm was what started
from the configuration that were best on
the data sets that were closest you can
also just start from configurations that
are good on average and that also
already works quite well so good great
I'm from our labs so one of the
challenges in all these technologies is
that you often when you tune your models
you end up you may all fit to your
training data how what are your
experience in that and how you propose
to solve those issues right so so if you
have small data sets then then
overfitting is definitely a problem we
will just have a very large design space
here and we search that automatically so
we have cross-validation of course to
guard a little bit against overfitting
but but that only goes goes certain
distance what we definitely did find is
that based on optimization over if it's
less than something like grid search
because grid search has no regular riser
in there so based optimization evaluates
a lot more models in areas where other
points were working well so you get an
implicit regularization in there
whereas in grid search you just take
take the best of all these different
samples but still you know if you have a
hundred data points I would say do not
use Auto scikit-learn it
it's just not going to work very well on
that so it's not a silver bullet I don't
want to come across it thank you I
should notice if you have only a hundred
data points you can still use Autodesk
you learn with some limitations so you
can say only use an Lda or only use a
very limited part of it and and that's
in the API so but but if you have really
little data and you're worried about
overfitting then that is something that
should be done
thank you speak of a thank you speakers
for the great talks I think we're gonna
take a break and then we're going to
resume at 4:30</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>