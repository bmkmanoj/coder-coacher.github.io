<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reverse Engineering the Mind - Prof. James DiCarlo, MIT Department of Brain and Cognitive Sciences | Coder Coacher - Coaching Coders</title><meta content="Reverse Engineering the Mind - Prof. James DiCarlo, MIT Department of Brain and Cognitive Sciences - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reverse Engineering the Mind - Prof. James DiCarlo, MIT Department of Brain and Cognitive Sciences</b></h2><h5 class="post__date">2018-03-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3djQSX1FJ9I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay folks so I now get to tell you a
bit about I heard the question of
computational modeling and that is
essentially a big part of this talk here
so I'm going to try to tell you about
another aspect of things going on in the
department another one of these impact
areas so you'll have a look at half hour
for me in questions and then we'll get a
break guys everybody hanging in there
are we okay
okay so so um I titled the name of this
talk reverse engineering of the mind and
I'm going to try to use my time here to
tell you a bit of history that's
happened around modeling its
relationship to the brain and through
work in my own lab particularly and then
I'll try to highlight at the end how we
think there's a broader possibility set
here going on within the department and
give you a sense of where that is
heading in the department so this is a
reminder of what I told you in my
introduction that we think of the brain
is some kind of computing device just
not like any that we've built before and
it's this way of thinking about the
brain as a complex information processor
and trying to build things in
engineering terms or describe it in
engineering terms to really say we
understand it not just measure the
phenomenology but to understand these
interactions in a way that we could
simulate them or know how to intervene
in one level to cause changes in another
that's really what these efforts are
after is building deep understanding of
a complex system using models that are
likely to be very complex as well again
not like any system that we've built
before but amazing in its power again
three pounds just 20 watts those are
amazing capabilities that somehow we
have the opportunity to try to discover
how it is that it actually gives rise to
everything that's us so I want to
highlight I showed you this slide
earlier that our mission is here reverse
engineer the mechanisms of the mind and
you heard about from Bob on the impact
on disorders of the mind so a deep
understanding of genetic molecular and
structural underpinnings of brain
diseases is needed to develop preventive
measures and treatments and you heard
that I think very well in Bob's talk and
I mentioned these other impact areas in
the department you heard some about
tools from Bob and you'll hear more
about this and the Lightning talks I
want to focus your attention up here in
this intelligence and cognitive
computing impact area one of the things
we like to say is that the science of
biological intelligence
understanding how the brain gives rise
to intelligent behavior is we think the
foundation of next generation artificial
intelligence systems to work alongside
of us now I don't want you to think that
this is really just a one-way street in
fact um what what really in my mind is
happening is that this is a very
fruitful interactive street at the
moment and I think that's going to
continue for a very long time because
the frontier of our field really now is
building neural network models at scales
building models of these different
levels of phenomenology to begin to
understand again how one gives rise to
the other going beyond describing the
phenomenology about building models once
you're able to build models they have
immediate translational impact into
potential AI systems similarly
hypotheses are things that are built
from the AI community that are in neural
network framing our hypotheses about
things that might be going on within the
brain and so this is a very interactive
two-way street and there's not a wall
between here and this is a very exciting
area for the department right now and
broadly that interaction fueling what I
call understanding accelerates all of
these other things because ultimately as
the question came up a moment ago even
these things here to understand its
orders of the mind we're going to
benefit greatly if we can understand the
computations that are giving rise to say
social behavior or hallucinations and
schizophrenia those are the things that
we don't understand at a mechanistic at
a kind of model-based sense so I want to
focus you here but remind you that it's
driving all of our mission so here's a
kind of general framing that the problem
of biological intelligence the
intelligence that we all think we share
how the brain generates it and how it
could be replicated in machines which
you would loosely call artificial
intelligence is arguably the most
important open problem in science today
there's there's two forms of this why I
think this is the most important open
problem one is the one I just described
for you that it fuels the rest of our
understanding of how the brain works in
model-based sense there's a longer-term
perspective that my colleague Tommy
Poggio likes to point out that if we can
build intelligent systems then that's
sort of the problem to end all problems
because those problems that system then
becomes does a lot of the work that we
are trying to do here on the ground
that's a very long-term view
in the more shorter-term view what we're
after is to try to build models that can
actually interface with our
understanding of how the brain works and
again these things I think are both
synergistic along the same larger
pathway and the way to think about this
is that you know computer you generally
think of technology of intelligence like
AI technology is being driven somehow
out of computer science but the story I
want you to understand is that a lot of
the recent advances in technology of
intelligence have come from a deeper
understanding of science of parts of
intelligence through work in cognitive
science and neuroscience and I'm gonna
try to illustrate that for you using
some of my own work and others in the
department in a moment progress is
resulting from this convergence our
department we think has a unique role to
play in this progress why do we have a
unique role well as I've mentioned in
the introduction we are anchored in
behavioral perspective and ultimately
intelligence is about the ability to
successfully behave in the world it's
not all about just the mechanisms of the
brain but it's how they give rise to
functionally appropriate behavior and
we're at MIT this is the other reason
that we think we're unique although
there are many neuroscience departments
or cognitive science departments in the
world
at MIT we take a particular perspective
and that we want to build it models and
engineering and computer science terms
and that imbues much of our research
that is the answers that we expect from
our work is not just phenomenology but
answers in the language of engineering
and computing models that can be built
to be predictive so I just to focus you
on intelligence so I'm talking about I'm
talking about AI and intelligence very
broadly and I'll sort of segue here to
say intelligence I might you might think
of as really the sort of foundation of
the mind it's sort of one of the things
that unifies or what we call the mind it
has many facets so one of the things
that many of the facets are listed here
and this is not all of them so how we
visually comprehend the world how we do
audio and with speech comprehension and
processing how we can navigate and route
plan how we can make decisions how we
can model other minds which is the basis
of social behavior and being able to do
these things from limited data to learn
from just a little bit of data that
humans seem to do these are the kind of
things that that our biological system
our brain does quite well yet have been
challenging for artificial systems to do
so I'm not going to talk about all these
I'll highlight some of them at that
at the end but what I want to tell you
now is a story of some recent success
story about just one of this facet so
I'm telling this story to give you a
sense of what's happened in last decade
or so so that you can understand how we
imagine things growing forward in the
future so this story for me begins with
work on visual processing so human minds
have strong perception when you look at
this scene you're able to digest and
compute a lot of things in this seam
that are really this is just a pattern
of little light pixels being red green
or blue but you're able to infer a lot
of latent content that this scene for
instance knowing that this is a car and
this is a person and this is a draw a
path but you might not want to go that
way you might want to be over here these
are the kind of things that you can
instantly computer seemingly instantly
compute that we'd like to understand how
you're able to do that and so what my
lab focused on was a more reduced
problem not the whole problem but just
trying to understand object detection
and categorization as I alluded to
earlier just being able to say this is a
car this is a person this is a building
this is a tree um that problem was the
problem that I came here to work on when
I started my lab at MIT and one of the
things that we knew that brain and
cognitive science had already told us is
that your brain doesn't really process
that whole image at once now you may all
have the sense somehow that image comes
in and you know everything about it
but in fact you have high acuity where
you're looking so the center of gaze
this is what we think of as a central 10
degrees if you hold your two hands out
at arm's length that's about your Center
10 10 degree visual field where you have
high acuity processing now you don't
just of course look at just this one
spot but you make movements to sample
the scene you're not aware of your dude
doing this Bob mentioned monkeys making
eye movements this is something that
primates like us do all the time several
times a second we don't notice that
we're doing it so for instance you might
have looked right here when I flashed up
this slide but you quickly made saccade
unbeknownst to yourself or you didn't
really feel them but you're sampling the
scene like something that might look
like this so you might dwell here you
move quickly here you don't really see
when you're moving you see when you're
stopped so you kind of stick here for a
couple hundred milliseconds and you
sample around now what this does is it
brings to what you see in the world
something that looks like this so you
might see a series of snapshots like
that when you stamp
the path now what I wanted you to notice
did you notice you can recognize an
object or something in each and one
those images did everybody feel that let
me try it again so you can see like a
person or a sign or even though I didn't
now I took it out of context right I'm
just showing you these flash glimpses
right and this highlights something that
we call core object perception that you
can do a lot with just a central 10
degrees in just a couple hundred
milliseconds which is what 0.2 seconds
so it's sort of literally like a blink
of an eye so just a very fractional part
of a second which is what I'm showing
you there and um what we have been
focused on this started in and when I
came here to MIT was to try to
reverse-engineer that problem so not
reverse-engineer the entire mind not to
reverse-engineer all of intelligence but
to reverse-engineer that particular
problem and try to build models around
that as a stepping stone to these more
challenging problems so what you're
going to do any kind of reverse
engineering the way I think about this
and the way I think many of us in the
department think about reverse
engineering is that you first want to
especially look at where the currently
engineered systems fall down or where do
they fall short relative to biological
brains which and then if you know that
you also want to ask well what if I can
measure parts of the brain various
different things behavior blood flow
genes neural activity
what should I maybe measure to inform on
how to build an engineering system to do
better and then you want to actually try
to use those measurements to constrain
what it's essentially forward
engineering or hypothesis building
building models constrained by this kind
of knowledge and then this is the loop
that you follow for whatever problem
that you're focused on and I'll try to
show you how we followed this loop for
this problem of core object perception
as an example of how we in the
department are using this philosophy to
drive on other problems so let's talk
about where to currently engineering
systems fall down relative human
relative on this particular problem of
core perception now this actually I even
though I said I wasn't going to talk
about the last decade this goes back
several decades to her own Molly
Potter's work here in the department
who's recently retired but Molly showed
one of the things she showed is the
amazing power of humans to do what I
just showed each of you and she studied
this in great detail your ability to
recognize objects even with brief
glimpses and one of the things that
Molly's known for these are called rapid
serial visual present
patience again this is very analogous to
what I just showed you I hope that you
noticed you can recognize one or more
objects that each and every one of those
images even though they're coming by
very very quickly right and I didn't
give you cues of what kinds of things
that you'd see yet you're still able to
do it very easily everybody likes to
laugh at yoga I don't know why anyway so
one of the things that we figured out
and that Molly Anders helped to point
figure out is that you know the brain
the your ability to do this here oh I'm
sorry I forgot to have a demo let me try
it again this is actually kind of
testing that we do in the lab for both
monkeys and humans just to give you a
feel so you can kind of try to maybe
shout out what you see it's a little bit
fun so I'm not gonna tell you the object
categories but let's see if you can do
it alright was kind of weird though it
was like floating in air right yet you
still could say it was a car that was a
hard one anybody see it was a plane yeah
lots of cars okay so you get you know
those are just I don't know my grad
student gave me a few images and while I
go in those I'm stuck with them but you
get you get the idea that you can easily
be able to say hey that's a car and what
we do each of you just did immediately
but machines found challenging when we
started this work is that you're dealing
with lots of images of cars so you never
see the exact image of a car twice you
see the cars under different viewpoints
different changes in its position size
and pose and the illumination on the car
you see change different it might be
different types of cars you still want
to call a car different kinds of
backgrounds some of it potentially
occluding if somehow you look through
all this and still say it's a car and
way back when we started now this is
actually data from 2009 so is even later
you could show easily that humans could
beat current computer vision systems at
being able to to say that it was a car
not when things were very standardized
if you always see a car from a side view
for instance in the center and it's a
certain type of car than machines
actually couldn't beat humans but humans
were able to generalize to these high
uncertainties that you just did that say
I don't know how this car is gonna look
but I can still to tell that it's a car
that's called high view uncertainty and
that's what we're showing here
quantified here's a bunch of human
observers here was state-of-the-art
computer vision performance around 2009
on these tasks and it was like doing
well here but quickly was falling apart
at these kind of high variation
conditions so so that was kind of known
to
be part of the problem but then how do
we look back to the brain to say how
does the brain help help us to
understand how to build a better system
to solve these problems well first we
have to know where to look in the brain
and here work from like Nancy kanwisher
is at one of the faculty members in our
department she and others help to
highlight figure out using tools like
brain imaging in humans first and later
with colleagues like my colleague Doris
Hsiao being able to show where these
where these kind of where the parts of
the brain are that are involved in
processing in this case it's processing
faces in particular but other parts of
the brain involved in processing more
general objects including things like
bodies or places these were the kind of
things that Nancy and others helped to
point us to the brain regions that are
involved so this kind of work
highlighting the brain regions gives us
kind of a kind of guideposts up don't
look over the whole brain but there's
certain regions of the brain that are
involved and then now this is me when I
had more hair and less gray and I came
here in 2002 as an assistant professor
and what I came here to do was to try to
say well we know about these brain
regions that I'll show you in a minute
could we study the neurons in more
detail to get some clues as to how they
actually drive this kind of behavior so
I was trying to go down here given past
knowledge what we could look at at this
deeper this sort of more fine-grained
spatial level to understand a higher
spatial and temporal resolution what was
going on so of course we all want to
understand this brain this is the this
is the human brain that I introduced you
to that we of course we all want to
understand but my lab chose to work on
this brain this is the rhesus monkey not
shown exactly to scale and part of the
reason we chose to work on the monkey is
even though there was at work in humans
we still knew much more about the
homologous areas in the monkey the
series of visual areas that processes to
object recognition were known to here
and there called the ventral visual
stream there's a series of areas called
v1 v2 v4 and I T in the cortical part of
the brain and I'll show you those again
in a minute so we knew where the
computations were we just didn't know
how they were working we also so we knew
the system anatomy of course great we
kind of knew who's connected to who
grossly not at fine detail but grossly
what areas are connected to which and we
knew that these areas were involved in
connection to areas of the brain that
are implicated in decision and memory
the decision in action and to memory so
we had kind of a rough lay of the land
and we had models based on prior work of
elementary computations going on within
local cortical regions each of these
cortical regions but we didn't know how
the system work together as a whole and
because this is a non-human primate and
not a human though we could access it at
much higher spatial and temporal
resolution and you can use some of the
tools that you heard about from Bob's be
able to manipulate neural activity for
instance using optogenetics which didn't
exist when we started but now is
possible for us to be able to move the
neurons around in a monkey that's not
yet possible in a human so this is all
the reasons we use this animal model
these are many of them now you might say
well why aren't you using a mouse and
Bob's already sort of alluded to that a
monkey has much different behavior it's
much more like our own than a mouse I
mean I can illustrate that a bit for you
here this is a non-human primate working
in one of its working in the lab on one
of we what's like a video game for it
that gets a juice reward for doing the
task that you just did
basically clicking a button to start an
image when he sees him and she then
makes a choice about which object that
he seemd that's what he's pressing with
a touchpad here okay so the monkeys love
to do this kind of behavior they're very
visual creatures they do very well at it
in fact it's not that you do they do
well at it if you compare monkeys and
humans I guess you guys already know the
punchline yeah they're almost identical
in their performance patterns okay so um
you might be like wow it's a monkey how
does a monkey can do that well I mean
really what they're doing is identifying
shapes I did monkey I'm pretty sure
doesn't know that it could get in and
drive the car away or it doesn't know
what a gun does yet it's able to
discriminate the kind of images that you
saw in two different categories based on
just a little bit of training it takes
about a day for an animal to learn a new
object and then it behaves at
essentially near human levels and that's
what's shown here the pattern of colors
meets even humans find something's hard
to discriminate than others and that
makes sense you know you know car or
cars and trucks are very similar to each
other and you make more errors and when
I show you an image of a car or truck
then they say was it a car or a zebra
and that's what you're seeing in these
colors here but so you see the patterns
of errors and the absolute values are
very very similar in both humans and
monkeys in fact when we did this study
humans and non-human primates were
almost essentially indistinguishable for
these kind of core object
categorizations cats and so what that
mean
if we could understand the circuits of
this task then we would have essentially
we think a very good model of what's
going on in each of our brains at least
around this aspect of intelligence so
now as I said we can access this system
the ventral visual stream it's a series
of these cortical areas again v1 v2 v4
ty T and as engineers like think of this
is like tissue that had to be curled up
and folded up inside the skull here I'm
laying them out to you and there's sort
of millions of neurons that are
simulator sketched schematically along
here just to give you a sense of this
and the rough lay of the land Anatomy
that I told you have to sort of forward
connections you have backward
connections we have inter cortical
connections of this type there's other
connections in the brain some of which I
mentioned earlier that aren't on this
slide but these are the kind of dominant
connections through these cortical areas
and this together is called again the
ventral visual stream so what happens
when an image comes into this system it
kind of it hits the back of your retina
it's transformed in the neural activity
that's what's called the retinal
ganglion cell layers so essentially a
nice photographic copy something a nice
camera would do and then it's it's now
kind of modeled as an image across a
population of neurons in the back of
your eye and then it's transformed into
the middle of your brain and the LGN and
then through successive population
patterns of activity along the stream
roughly in it this may where you have
successive lags of about 10 milliseconds
of processing at each level so it takes
about a hundred milliseconds
most of the lags are upfront here on the
camera capture takes about 100
milliseconds to get a new population
pattern of activity in I T cortex here
this place I'll say more about in a
minute i T stands for in for temporal
cortex and what I want you to have know
is that each image produces a new
population pattern that I schematized
here with these red dots so so this
image might show this is just a
schematic might produce this population
pattern of activity this is not a
photograph it's not just copying a
photograph here it's some sort of new
pattern of neural activity that that is
evoked by this image and a new image
evokes a new population pattern and when
you are watching that RSVP of molly's
work that I showed you earlier we have
something clicking along and you're
homolog of i.t a population pattern at a
lag of about a hundred milliseconds so
you're following your brain is following
along and producing a new pattern of
activity now why am I tell you about
this these patterns turned out to be
remarkably powerful and I'll say more
about that
minute but these could have turned out
to be the solution to how the brain
solves the problem the information about
the content of the image are actually
easy to find and these population
patterns but very difficult to find in
these popular these early population
patterns so but what we started this
work when my lab began we didn't really
have an engineering specification of
this transformation that I've
schematized for you we kind of again
knew the lay of the land and some
phenomenology we don't have a model of
what was going on and we also didn't
know the thing I just told you we didn't
know how the aero neural activity here
related to the animals perception and
its behavior and so that's that was kind
that was about where we started about 15
years ago and to summarize that back to
this big picture even though we had kind
of started to work on studying these
neurons and we weren't the first lab in
here other labs are doing this as well
we didn't really have a link you know
what was missing here is some models
that can intervene between the
phenomenology and this behavior at this
higher level so we didn't have that but
that was our goal is to build all that
so when we started we would record from
neurons and i.t cortex and I'm showing
you this slide just so you can get a
sense of what neurons look like when you
listen to them the neurons communicate
with other neurons through what are
called action potentials or spikes and
those are shown here for these are three
example recordings out of IT critics so
these are three different neurons and
responding to five different images here
and you see the little spikes that
they're sending out in response to this
image these each row is a repeated
presentation all I want you to see is
different sites like different images so
this site likes these and not so much
they see more spikes less spikes and you
also see that different sites it's not
like all the sites are doing the same
thing and if you can imagine entire
population that's how you imagine this
picture where each image gives rise to
it's entirely new set of neural activity
when it's when it's presented and this
is showing you kind of the core data so
I'm not going to take you through the
details of all the analysis that went on
but after a lot of work and collecting
these kind of data we and when I say we
I mean my colleagues in particular Tommy
Poggio my collaborator in the department
when we started this work we discovered
how the brain codes information about
objects in AI T so we were able to build
models that could take us from these
population vectors with a simple linear
transformation to get to variables of
the mind be able to predict what the
animal was going to say for each for
each image whether it would say it's a
car
face and to also model that the pattern
of errors that the animal produces and
this work still is ongoing today in
terms of working out the details of how
what intervenes between this population
and the final behavior
but at that point we had realized that
actually most of the action had been
solved at this point around this problem
and that it turned us from not so much
understanding the downstream mechanisms
which again we continued to work on but
to these sort of more upstream
mechanisms that I've skipped over but
after we understood that IT population
activity could explain the animal's
behavior what became mysterious is that
these neural responses even though we
can measure them we didn't have models
again that would take us from any image
and predict the neural responses of i.t
so these were very mysterious responses
people had recorded them before they
knew they were mysterious we now linked
them to the behavior tightly but we
needed to now understand how do you go
from the image to these so it sort of
put that in perspective the way we think
about this is the brain's solution to
intelligent sensing on this core object
perception task is conveyed in this
population activity somehow up here but
again we didn't have any specification
or engineering model of how you go from
an image to that we just could observe
that and say oh it's easy to build a
model from here to behavior but building
model from there to there was hard okay
so that was where we were and now I
don't want you to know all the details
of this I don't want you to take worry
about all these words here all I want
you to know from this is that there were
a lot of data and I don't mean to imply
my lab by far wasn't the only lab
contributing many labs were contributing
data as to the kind of things that were
going on both in anatomy and physiology
along these different levels of visual
processing and some of the kind of kind
of things that we had learned from brain
science are listed in instead of Oh
summary form here and so we knew all
that and that those kind of constraints
were being used in parallel even before
I started this is way back in 1980 to
begin to build models that were modeled
off with a structure that had been
learned about the anatomy so these were
early models of what might be going
along here this was an early model by
Fukushima called the Neo cognate Ron
then later my colleague Tommy Poggio
here in the department built a series of
models called the H max class of models
that advanced over those models and its
ability to predict what was going on
then my lab picked up the ball
started building even more on advanced
models and then in 2012 a postdoc in a
grad student in my lab built this model
that we called HMO which again you can
see all these models they look the same
they have these kind of repeating
structures at different levels and they
actually share a lot of details and I'd
be happy to tell you the stories of what
was evolving along those models but just
take in mind that there's a model here
that's an approximation of what's going
on that we called HMO and what was so
striking to us is that this model when
we built it it's high levels if I could
know if I could back up and go for it
again it's high levels right up here
these kind of high intermediate level so
behavior happens out here in the model
these are simulated neurons in the model
here if we looked at simulated neurons
at these levels up here they started to
explain what we were observing an IT
cortex so just to give you a feel for
that these are images along here this
black is the risk this is the response
of neurons to different images and you
see it going up and down so this IT
neuron likes different images kind of
like that example I show you for just
five images here there's thousands of
images being collected here's a few
examples shown here and what I want you
to see is the red is the prediction of a
model of what this neuron should be
doing for all these images and these are
predictions the model didn't get to see
these images it had to predict what the
IT neuron would do so these are not fits
to the data you can see how good this is
it's not perfect we can quantify that
here but it was a huge advance over
things that we had previously had her
ability to explain these neurons it's
not just neurons like this neuron tended
to respond more to faces but not all
faces this neuron kind of likes chairs
but again you shouldn't think of these
neurons as being categorical they
support categorical perception but
they're not themselves categorical and
you can see that you know even though
the neurons have words that are hard to
put in human terms the models are able
to predict their responses quite well so
the summary of all this I already said
this these are not fits you should not
think oh we're just taking a bunch of
parameters and fitting data these are
predictions of a model that was built
and fit on come other completely other
data to be happy to tell you about this
but this neural network model what we
had now we thought was a model that
captured a lot of the processing it
seemed to be working the way the brain
was working because it gave the same
phenomenology that we were observing
that was previously not understood so
this was a big event
it's for us that was happening around
2012 within the lab and then what
happened is right at 2012 this group
came out this is um Jeff Hinton's group
at Toronto came up with this model which
was an extension of these class of
models that you had already been seeing
and there's a whole story about how they
came up with this model but this model
was entered into the image net challenge
that had previously been dominated by
completely other forms of models this
model one the image net challenged by a
longshot in 2012 so suddenly these class
of models called deep neural networks
that that's what they're now called
essentially took over the computer
vision community in terms of wow this is
way better than anything we've been
using before and that became the
dominant model of field in computer
vision and other things well beyond that
point in fact in 2012 this was a New
York Times article sciency promised a
deep learning programs they were talking
about these kind of deep convolutional
neural networks that I just showed you
that Jeff Hinton's group and others had
built based on theories about how the
brain recognizes patterns that I gave
you a lot of the background as to where
that class of models came from and so
what we had was in 2012 a breakthrough a
biologically constrained AI that people
get excited about now all of you
probably can't walk around without
hearing about deep learning so what deep
learning is referring to is essentially
ways to learn on these kind of deep
models that I just showed you here and
then I'd be happy to talk more about
this in questions that there's many
engineering details behind those models
but the big picture to give you back is
that this wasn't just a one-way street
it wasn't just biology helping to shape
a class of models it ultimately became
the leading models in this area of AI it
was also that that these models as I
already told you helped to inform what's
going on in the brains to remember that
two-way street I said at the beginning
so now we had you know in 2013 we
published those results that I showed
you with our HMO model and then that we
found that these models actually were
able to explain the neurons quite well
just as I showed you a few slides ago so
Dan yay means my postdoc and I showed
this and that now we actually had models
that could begin to explain the things
that were previously mysterious and
interestingly even some of the more
advanced computer vision models that we
didn't build but they were built in the
same flavor were able to predict our
neurons even better than some of the
models that we had built so this line of
work continues today but there's this
exciting interplay but
Queen models in both directions here so
just to give you a summary we were
missing this link Tommy Poggio in our
department helped kind of build the
foundation of this and then geoff hinton
in Yamla coup and these were two of them
now that kind of they're known as the
folks that have really pushed this deep
neural network revolution they were
actually psychologists and neural
network people they stuck with the brain
constraints and always believed this
would work and again through the
combination these kind of efforts we now
had this this this bridge between
systems of neurons and brain regions all
the way up on behavior and cognition
because now we had models that could
link across these different levels and
so the summary of all this I'd like to
leave you with is that we saw a dramatic
advance and visual AI in this narrow
domain so far but we also saw a dramatic
advance that our understanding of one
facet of biological intelligence so
these kind of things are inner playing
with each other as I tried to describe
for you a minute ago they play off of
each other so where are we now so what's
the what's the future look like so this
is like this tells you something about
kind of well there's been these great
great throughs in visual processing
you've heard about many of them in the
field you can read about them in the
newspaper and so here's there's a recent
article from New York Times recent
advances while impressive have been
mainly an image recognition the next
frontier researchers agrees as general
visual knowledge remember I said visual
comprehension the development hours that
cannot you understand not just objects
which is what I was showing you but also
actions and behaviors computing
intelligence often seems to mimic human
intelligence researchers offer two
analogies to describing the promising
path ahead analogies to a child and a
brain so far I've been child referring
to the development process and brain
referring to measuring the the
mechanisms of the brain and the adult
state much of which is what I've showed
you in my talk busy as faculty are
actually pursuing both of these I showed
you my own work and I'll try to
highlight a bit of the other work here
in a minute so I want to kind of point
out for you though we're really at the
beginning so far in my lab in other
words we've only modeled this sort of
feed-forward part of the brain that's
what we modeled so far that I've been
describing as these models we we but now
we can still see even though you know
despite the New York Times saying oh
that problem is solved the problem is
far from solved we can still see the
brain is way better than artificial
systems on many of these images that's
what's shown here in red
us testing many images against
state-of-the-art models even these
models built off of this feed-forward
approximation and the difference we
think is that the models don't have
these other parts of the brain that we
we have a lot of evidence to suggest
that kind of feedback processes are
involved in the performance gains that
the model the humans have and monkeys
have over these kind of models so that's
where my work is heading now as to even
more advanced models based again
following the brain continuing to push
that line of work so but I want to say a
few words
I've told you a lot about visual
comprehension I'd like to say a few
words about some of these other things
to give you a feel of what else the
department is doing in this area so
let's talk about audio and visual speech
cardio and speech comprehension this is
Josh McDermitt josh is gonna run a
lightening session later this afternoon
so you'll get to hear more about this
but Josh's group is really leading an
effort on trying to understand how the
brain processes auditory signals this is
an example of models that they are
building they look very similar to the
models that I showed you and that's
because cortex around auditory
processing looks very similar to cortex
around visual processing it's just the
details vary in interesting ways
so the same family of architectures is
being used these are deep convolutional
neural nets that are approximating
what's going on and what Josh what josh
has showed in his group is that they can
train models to do answer tasks like
which word did you hear which music were
you listening to and those models
provide an understanding of the activity
patterns observed that were previously
phenomenological in human brains
measured with fMRI and I think Josh and
his student Alex kelvil tell you more
about that in the lightning talks so
this is now trying to push the same
ideas into auditory processing and this
is just one aspect of Josh's work that
the department is that is excited about
I want to mention I've already mentioned
several times my colleague Tommy Poggio
who has a theorist and a model builder
who's good for the foundation of many of
this much of this work Tommy will point
out that even though we have these deep
neural network models we still don't
deeply understand them at a theoretical
level his group has done a lot of work
and is continuing work to understand how
these models are able to perform so well
why deep networks perform better than
shallow networks and and that I could
tell you stories about that if you're
interested why they're able to
generalize so well it's very surprising
these models have many parameters yet
they're still able to learn from still a
large amount of training data but then
they
still generalize in a way to surprising
given previous ideas from Statistics and
how you need to train models and that's
something that Tommy's lab is actively
working on I want to end with this slide
here from Josh Tenenbaums group so Josh
Tenenbaum is a professor in our
department of professor of cognitive
science
josh has done a tremendous amount of
work to move the fields of AI and
machine intelligence forward here's an
example of Josh's work on this was
published in science about a year and a
half ago where they built a system where
they were able to have a system learn
from just a few training examples
remember I mentioned earlier it's hard
for humans too hard for machines to
learn from a few training examples that
sometimes called one-shot learning or
small number of examples learning yeah
Josh was able to build a system that
could do this within a limited domain
not just learn from a few examples but
to generate other examples from that
learning to be able to construct new
characters from new character sets that
they had learned from a few examples
construct them in a way that would fool
humans so that other humans couldn't
tell whether Josh's system or another
human had generated those characters so
it had kind of passed one of the Turing
tests that we put up against models to
say how are they doing if you're
interested in this work I'm sure I'd be
happy to say more Josh can tell you more
about that jut the frontier that Josh
and his lab are pushing on is to
understand I mentioned the kids earlier
how even children have an intuitive
sense of physics not just that there are
blocks here but what way they're likely
to fall down what's stable what's stable
what's not stable and even more deeply
how they do sort of theory of mind and
in kind of what we call common sense and
being helpful or not and the being able
to build artificial systems that can
kind of interact with agents and know
what it means to be helpful is one of
the hardest problems and artificial
intelligence and it's one of the
problems that Josh's lab is working on I
want us to show you a video this is from
Wernicke and Tomasello just watch this
for a minute that's an 18 month old
so even many think you may hear AI is
gonna take over AI doesn't do anything
like that yet that's an 18 month old
right so one of the goals of Josh in his
lab and is to understand how what's
going on inside that 18 months old brain
in terms of engineering terms that you
could imagine having a flexible
intelligent cooperative robot just as
that 18 month old is able to do so there
are amazing things that kids do that we
can study that we do study about how
they do it and then Josh and his team
try to build models of how they might be
executing that and those models we think
can be the foundation not just of
today's AI around things like vision and
so forth that I showed you but about the
next generation of AI so that's a really
a quite exciting frontier for us I hope
you can see that there's a group of
people that all working around these
kind of areas in fact in the brain and
cognitive sciences department we are
organized in something called a center
for brains minds and machines that's led
by Tommy Poggio this is an NSF funded
Center to fuel work in this general area
it's one of the most interesting areas
of our students and our grad students
coming to work within our department
it's one of the brands that we have here
in the department it's funded by the NSF
at least for another five years and what
its future is going to look like from
that is something that we're hopeful on
but we're not sure yet we have
partnerships through this Center it's
not just in our department of course as
I mentioned earlier especially computer
science artificial intelligence Media
Lab and engineering that form the kind
of interactions that the center is the
nucleus for within the department so um
with that I would like to thank you and
I think we may have time for a few
questions thank you
yes so I think we have about five
minutes for questions yes so I'm pleased
for very strategy games so I'm not sure
which IBM system you're referring to
exactly but oh so that both has the name
deep deep so you're right you're talking
about the original so iBM has been a
real pioneer and kind of driving AI
vision so you probably when I was a grad
student deep you know won the chess
challenge in 1998 that was when I was a
grassed and then won jeopardy and uh I
should know 2008 Watson right so so
Watson has a lot of the you know Watson
my understanding again I don't know
everything about what goes inside IBM
but one of the things they did was they
set up challenge problems and that was
really drove a lot of the research let's
solve Geoffrey let's do this right and
then a lot of engineers came and applied
a lot of different techniques to make
those systems work and my understanding
is you know Watson has some of the same
things that you saw from these visual
processing in fact some of those same
ideas of deep neural nets are used for
other things like natural language
processing and so forth right so this is
these kind of core ideas about deep
neural networks are imbued without
throughout that system as well as other
ideas from other aspects of AI that
folks like Josh works on as well so
there's a mixture of things within those
systems right and so you know none of
these things and this is kind of less
than I think you should take it's sort
of yes I said these novels have been
known before it's not like there's one
piece of Secrets awesome you just do
this and suddenly the world breaks open
really they're kind of combinations of
getting all the bits in sort of a
reasonable place oh that's true we see
for the brain models and that those
models are slowly started to get better
and better from a combination of many
different things that had to come
together and I think that's also how you
know maybe that's a little depressing
because we'd like to think there was a
sue if I just like to use the word
discover like we discovered how to do
great AI but I think it's more of an
incremental sticking to building models
making them match what you see in the
brain that you gradually and gradually
get closer to something that performs
better that's what I think it will look
like I find that quite exciting but I
don't expect there wasn't one big
breakthrough there were many things that
came together to make these systems run
so I hope that partially answers your
question at least I think I saw one in
the back first and then maybe I'll go
here yeah from you know recognizing what
brain areas were important for
functional imaging and then you know
taking apart the neural circuitry there
and understanding at a biological level
and then building you know computational
models is super exciting but I'm curious
how well do you think that will
generalize to untangling systems like as
you mentioned you know language
processing or memory or decision-making
and what will the big roadblocks be
right no it's a great question and I
think one of the reasons this can work
well is because as I mentioned we had
more access to this system because we
have things like monkey models for
things like language that's not going to
apply in such a direct way so we hope
though and so you know you might imagine
one way to go is if we can develop
enough tools and you mentioned Bob's
talking about Edie Boyd and others
they're always trying to build new tools
that someday we could actually make
these measurements in human brains and
then the same process would apply but
our tools are what's limiting us a bit
as the more human-like we expect these
things to be the more we push the limits
of what we can get when I remember I
mentioned early on constraint data of
what kind of constraint it we can get
some from the humans but it tends to be
a much weaker constraint data than we
can get from an animal model so but
there are many things that animals still
do social behaviors Bob mentioned is
sort of many things that are still
relevant that you know even you know
that first of all I don't want to say we
can't make progress even with just
behavioral data from humans and the
measurements we can get and building
models and that's like the work that
Josh and others are doing you can do
that but this kind of idea of going even
deeper to measure mechanisms we can do
that especially in non-human primates
for areas like social behavior and
others and even to the genetic level as
Josh as Bob mentioned we want me to have
these knockout models that that he
mentioned with Gua pings work that we
can study those those kind of properties
but
I you know what we hope as scientists as
their principals even though I don't
think you know a monkey's gonna have all
the social intelligence that we have
there's something about its social
behavior that we can still learn with
that root that we then generalize up to
a human in a way that's not we can't yet
see but another way to put this is we
don't even understand how a mouse works
or how a monkey works so so that there's
lots of progress to be made in that
domain but there's also progress from
the top of just building models with the
human constraints that we have to try to
see if we can build better systems and
to be perfectly honest I don't know
whether you know AI next day I will come
out of brain and cognitive sciences I'd
like to believe that that is we're gonna
contribute as we have in the past but I
do know that work on AI is important to
the department and that there's this
two-way street but those are hypotheses
of how the brain might be working and
again we're trying to contribute in one
direction but we know in the other
direction it fuels all of our other
missions that you heard about so I'm
hopeful that we will continue to impact
but I certainly know that that tight
interplay is critical to our overall
mission so we kind of try to approach
them both together I'm sorry that was a
long-winded answer but I hope it kind of
covered the ground yeah I think I saw
one here and
there were swearing other than stood a
great information to try and make sense
of it but something happened where
people really they only went three
literacy and then suddenly people
realize oh maybe we go eight layers deep
it's powder but ten years ago nobody
went deep because it seemed to do
anything so was there anything so what
the so okay so there's a couple
questions buried in there so what one of
the things that accelerated this
networks was just the ability of
computing power available to actually
train the networks okay so that that's
one of the answers and now people are
actually running these now we're trying
to go deeper deeper because they sort of
thought oh I go deeper it gets better
that turns out kind of it's sort of
tapped out right so there's something
still missing I sort of alluded to
something in our own lovest like there
are mostly feed-forward they don't have
any feedback processes right so so
there's kind of qualitative things that
are missing so engineers are running
with the obvious thing like let's make
it deeper but that's that's largely
tapped out in fact I think I actually
put here's a slide if you want this is
kind of like these are models of
different depths of neural networks and
this is their ability to predict those
responses I showed you and their ability
to perform on recognition tasks and even
though they're able to I'll skip that
part even though they're able to drive
forward the performance a bit by going
deeper it turns out that they're
starting to explain the brain worse
right so they're sort of I think of this
as they're deviating from the anatomy
more in search of performance which
they're gaining but now they're not
coming back to this question driving our
understanding of the brain which is kind
of what's on this axis right so so again
you know we like to think as we
understand this more we can kind of
drive this back up but you know which
who's helping who is often hard to feed
here right so really what you have is a
community of people that are interested
in both questions and it's hard to
predict which one will drive which but
working in that space is what's most
exciting that's why I mentioned the
convergence between these fields so so
there's many things that throw those
miles and nobody can point them saying
oh it didn't need brain science oh it
didn't need engineering it needed all of
those things right so that's my you know
it's not one thing I have a nice review
on this if you'd like to read that
that's a deep learning that you
described here you threw up all the
pictures of the cars and so forth
and the deep learning is about
classifying information yes as opposed
to that's where that's where it was
originally but you can do many more
things with these networks as well well
so a simple one is just being able to
locate the location of an object or
report its pose and its other latent
variables like so those are kind of
other layin contents beyond the category
of the object in the image and so what's
happening now is people are using these
networks has kind of a front-end to
other kinds of types of models and and
Josh Tenenbaum what happy to many of
Josh's work is now benefiting from that
front end in some sense you can think of
this as the ventral stream is like being
an advanced retina like oh here's this
greater processing and now you can do
more cognition now you have good visual
representations right now whether they
should stick with the convolutional
flavor and the same style that's unclear
the brain starts to change its you know
depending what circuits of the brain you
think are involved those are some of the
forward going questions I don't think
you just keep going with those same
ideas so that was my point about the
brain we've only modeled one aspect of
the brain okay so yeah so maybe we have
a chance he'll me when we're out of time
I don't know well 11:00 okay thanks
Elizabeth so we have a few more minutes
all sorry I got two over here maybe I
think I had you first yes sir
that's a really great question
yeah I kind of skipped over that because
that's certified so we don't yet so I
didn't one of things I didn't tell you
about those models is the models produce
population patterns but the models don't
say oh this isn't around twelve go look
at neuron twelve in the monkey the
models produce a population of like a
feature set we call it right it's like a
basis of the world and then what we do
is we take the models features and we
have the brains features and we say is
there a linear map between them
okay and that's how we ask when I did
those predictions I hid this step from
you that we actually had to do a linear
map to explain that because I didn't
want to bore you with the details but so
then that raises the question is like is
your brain in my brain and I T are we
linear maps of each other are we copies
of each other nobody really knows the
answer to that question yet it's
something we're starting to look at in
the monkeys we know we're pretty sure
we're probably going to be you know
partially at least a linear map but how
much of a linear map and how much we
share and how what our differences are
become interesting too like how do you
see differently than I see and and
that's my belief is that we probably are
some and common but not completely in
common and quantifying that is something
we're trying to do with the non-human
primates in part to know whether the
models are doing the best they possibly
could I can't expect the model to
predict all monkeys it should be like
within the space of linear map bubble
monkeys or within the space of linear
malleable humans so that's why that's an
important question to us but it really
is an open one at the moment but one
that we're able to attack and we just
don't know yet the answer I think I had
maybe one more that there
are you - sure yourselves that we
endeavor to build models to predict the
behavior of our brain
he is accurately representing what our
brains are doing rather than just making
our parallel system yeah that's a that's
another great question so I kind of
skipped through that I did show a few
slides that we don't what you could say
what aspect of the brain should we even
bother to model like cuz we could take
it down and model all the levels of
detail what I tried to say in my talk is
that we had already known that when we
looked at ite
cortex patterns and we read them out in
a particular way we could predict
whether the animal would say I saw a dog
I saw a car so that's a functionalist
approach to explaining the system right
I don't need to know I can't read the
animals mind in this sort of deep kind
of philosophical sense but I can't say
oh the animal thinks it's a car it's
about based on its reporting pattern so
we built up models of how you what
aspects of the neural activity give rise
to those accurately predict that and
then we said now that we know that let's
explain that no more than that that's
what we want to try to model so that's
how we try to attempt to not go off and
just look and model everything but just
model the aspects that are tied to
behavior and that's kind of that's what
I try to say at the beginning like I
start with the goals and what do you
think the interesting behavioral
questions are work your way into the
system find the levels that are mapping
to that and then make see if you can
build models of that not everything but
just focus on those aspects so that's
what we did here and again we haven't
modeled everything I don't want you to
walk away saying oh we understand
everything there's been a lot of
advances but there's a lot more work to
do exactly along lines of the last two
questions that you you gentlemen both
just asked maybe time for one more yes
dyslexic monkeys basically if you found
some monkeys that did poorly and leg
recognition tasks and if you gave them
that adaptation test that showed the
difference yeah I think so
so as by yeah III think these are I'm an
you saw my talk I'm an MD PhD right so I
hope that I work will somehow influence
the human condition but we start we just
didn't know anything about how the
system worked at all and when I started
this work I and I said well and we
didn't have the tools to kind of ask
those kind of questions he may we can
only use we don't have large numbers of
animals we can devote to these studies
but now you hear there's this effort
that Bob mentioned the marmoset effort
where we can have many more animals and
can control their genetics right that
glopping Fang is leading so I hope is
that there's a enough of the primatology
community around here that many of us
can start working on those kind of
species where those questions will be
easier to ask about individual
differences things like dyslexia and how
what their genetic ties might be and so
we're we kind of forming the behavioral
foundation some of my one of my postdocs
is actually training marmoset Sonic's
kind of seemed as you saw that would set
us up to be in the position to answer
those kind of questions but it we're
probably gonna happen better in the
marmosets because of the genetic control
there is genetic control possible in the
rhesus monkey but it's much more
challenging certainly in the u.s. some
of it's going on in China but what we're
hoping that again the bridges be built
humans I mentioned humans will walk it
into rhesus monkeys and then maybe we
step it down to marmosets and try to
keep maintaining those and kind of
keeping those connections is how we know
anything we learn at the lower levels of
the system actually flies all the way
back up to the humans for a condition
like dyslexia and that is kind of the
essence of the department and I hope you
saw in my work how we reflect one part
of that bridge but I need my colleagues
to build all those other bridges I think
with that maybe we should take a break
and I thank you for your time
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>