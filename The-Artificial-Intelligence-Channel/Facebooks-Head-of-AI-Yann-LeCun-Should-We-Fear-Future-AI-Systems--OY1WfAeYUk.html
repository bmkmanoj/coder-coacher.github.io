<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Facebook's Head of AI, Yann LeCun - Should We Fear Future AI Systems? | Coder Coacher - Coaching Coders</title><meta content="Facebook's Head of AI, Yann LeCun - Should We Fear Future AI Systems? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Facebook's Head of AI, Yann LeCun - Should We Fear Future AI Systems?</b></h2><h5 class="post__date">2017-09-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-OY1WfAeYUk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay well our next speaker is someone
who's very familiar to many of us here
at NYU Yann Lacan has been a pioneer in
AI research for many years
you know since back in the nineteen
eighties doing important research on
machine learning in particular neural
networks and has been a major force in
the more recent rebranding of neural
network research under the label of of
deep learning which has now begun pretty
well to take over the the world
Nick yarn has been at NYU since 2003 and
computer science and headed up the was
founding director of the Center for data
science which started about four years
ago here since since two or three years
ago we've been sharing him a little bit
with with Facebook where he's heading up
the the AI group there and most recently
has been a major player and staffing the
partnership on AI about which we might I
hope hear a little bit today but yawns
topic today is should we fear future AI
systems
please welcome you on the clock
thank you David
all right
so you think I make this work
all right
okay
so yeah I'm sure I'm sure David wanted
to wanted me to talk about perhaps
short-term impact or short-term
questions or some running ethics of a
like that I'm I'm actually gonna talk
about the long-term stuff so should we
fear you Jerry I systems so sort of
various pictures of what you know how a
is supposed to behave in in science
fiction from you know c-3po which
basically does what you tell him
or you can turn it off if you're not
happy to have 9000 in 2001 a Space
Odyssey and there is you know some
collision sometimes of those two worlds
where people who expect robots to work
like c-3po are get really annoyed when
it worked like Hal 9000 and that doesn't
is the worst garage door opener ever yes
indeed so I have a terrible confession
to make actually came to work on machine
learning switch philosophy it was
actually or I should say you know theory
of my linguistics and things like this
so this is wonderful book that I
stumbled on when I was an engineering
student undergrad in 1979-1980
which was a debate between Noam Chomsky
and Ron Piaget and they brought there
both of their teams to debate the
nature-nurture
questions and I was you know reading
things from the side of Chomsky not
necessarily by by by him but by you know
people can rooting for him and I thought
I thought it was kind of preposterous to
think that intelligence could appear
with with very little learning on the
other hand on the rampage aside there
was an article a transcript of a talk by
a Seymour Papert who's mathematician at
MIT who had worked on the perceptron and
this is the first time I encountered the
the sort of the possibility of a running
machine and so I started digging the
literature from the 50s and can discover
the perceptron which is an analog
computer at the time and believe it or
not the model of learning that we use
today in 95% that would say of all the
applications of machine learning and AI
that you hear about
are based on more or less the this this
very simple model of the perceptron or
things that are derived from it
essentially so it's the basic idea of it
is supervised running where if you want
to teach a machine to say classify
images of cars from planes you collect
thousands of images of cars on planes
and you run them from the machine and
whatever whatever the machine makes a
mistake you adjust its adjustable
parameters which I symbolized here by
knobs so that the the answer you get
gets closer to the answer you want so
it's kind of like showing a picture book
to a small child where you know you show
a picture of an elephant you say so now
I want to et cetera and the child of
course after just a few examples which
object can can figure out what the
category is with those supervised
running machine you need thousands of
examples of each category the first time
you train them the second time you train
them they've pretty much figured out
what an object is and you only get a few
examples of each example but it's
supervised running so it requires human
annotation human supervision essentially
and it limits the power of the of those
machines in the sense that they can only
do things that for which we have a very
clear input-output relationship that we
can we can produce so this applies to
not just image recognition but language
translation classification of topic in
in text summarization all the tasks of
AI systems that you see ranking search
etc so that's AI today it's really not
AI it's supervised running it's
glorified curve feeding you could say so
what has changed over the last few years
is not that model but it's the
architecture the internal architecture
of the AI systems which now use D
pruning so whereas the perceptron was
composed of essentially a chunk that was
sort of hand designed or classical
pattern recognition system were composed
of a chunk that was hand designed that
we call the feature extractor followed
by a classifier which actually embeds
the the running abilities a deep
learning system is basically a cascade
or a collection of learning modules that
are all trained simultaneously to to to
perform the task
that allows that that is qualitatively
different in the sense that now the
machines not does not just learn to
classify things but also longs to
represent things so it Lorenz internal
representations of the world this is of
course inspired by biology very mildly
inspired by neuroscience not in the
details of it just in the kind of
underlying general principles but really
not in the in the details so the kind of
inspiration that we get here is similar
to what you know early pioneers of
aviation got inspired by by birds but of
course that planes are very different
from birds even though they use the same
principle to fight so that resulted over
the last few years I mean the techniques
that are used in the eye systems deep
browning and things like that today are
are pretty old in their basic principle
about 30 years or so but what has
happened in the last years is that the
combination of the availability of large
data sets and powerful machines has
allowed us to solve new problems that
really weren't solvable before and so we
can build those very large convolutional
nets or recurrent Nets or or you know
any one of a large collection of
different architectures of neural nets
and get them to recognize images build
construct internal representations of
images in a hierarchical manner so this
is idea that in fact Piaget expresses a
lot in the book I mentioned earlier that
learning proceeds in stages and you sort
of learn low-level concepts on top of
which you build higher-level concepts
more abstract concept and this idea of
deep learning is very much rooted in
this idea that you know the way we
represent the world is hierarchical with
more more abstract concept built on top
of K no more elementary ones in fact the
recent versions of those deep neural
Nets have anywhere between 50 and 150
layers it's become this there's been a
huge inflation in the number of layers
in those in those systems and it's also
been an inflation in the size of those
systems in the sense that the number of
adjustable parameters that we have in
those in those models in you know is
close to a billion or at least 700
million the amount of computation that
takes place to Rack
as a single image for example is
anywhere between one and 10 billion
sometimes even bigger so we're getting
to the point where those those networks
are kind of the same size as what we
encounter in biology not the human brain
we're very far from this but sort of you
know small brains of small animals if
you want small mammals and you know what
those techniques we can get we can do
things we can do before in terms of you
know detecting recognizing objects okay
you know building systems that can drive
cars around and which are based on these
conditional Nets techniques I was I was
mentioning earlier and there is a you
know a huge industry around this now
where you know companies are trying to
kind of get machines to drive in sales
that ask a whole host of short-term
ethical questions which I'm not going to
discuss so basically perception is not
solved but but it sort of works now we
can we can build perception systems that
work pretty well in some instances have
as good or better performance as the
human visual system for for locating
identifying objects so here's an example
of recent results for a system
constructed at Facebook that can you
know detect and label and outline
objects and images including when they
are extremely blurry or ambiguous and
you know detect Berkeley or even count
sheets
so that's perception okay we know how to
do perception now and this was a very
challenging task for classical AI
because classically I you know try to
focus on sort of higher level functions
of human human reasoning logic and
things like this you know without
realizing that most people actually in
the world have no idea how to do an
object but you know including people you
might not want to vote for and and so it
turns out logic is relatively simple to
to mechanize the problem with logic is
to write down the rules that are
necessary for for logical deduction it
turns out most of human knowledge cannot
be reduced to a number of rules so
perception for example is very difficult
to reduce two rules which is why it took
so many decades and it took techniques
like deep running to to be able to do
perception basically that takes the
human engineer out of the loop with deep
running the the design of the system is
very basic and it's the data and the
training that really builds the machine
and those running algorithms are better
at designing vision systems then human
engineers are at doing it which is sort
of humbling for a engineer like me but
that's reality so what people are
interested in now is using the power of
the planning methods to enable machines
to do reasoning remember things turns
out neural nets are kind of bad at
remembering things so you have to do
things like augment among them with a
sort of episodic memory similar to what
the hippocampus does in the mammalian
brain and so there's got to be the work
on this which is reading to kind of a
sub area of AI that I think we could
call differentiable programming so it's
the idea that you can build essentially
a computer that has a memory and has or
call it a knowledgebase perhaps and they
kind of a a computation engine if you
want that operates on this memory and
that plays the role of inference engine
if you want to not
but here instead of manipulating symbols
with logic we manipulate vectors with
algebra so when we replace symbols and
logic with vectors and algebra what we
get is computation that's smooth and
differentiable which means is compatible
with the learning algorithms that we use
which are essentially numerical so
there's a lot of interesting work on
this which I'm not going to go into but
to focus really on the obstacles to AI
so the state of the art is that you know
we know how to do supervised running we
know how to do reinforced micron in
which I talked about in a minute but it
is one piece of running we don't know
how to do that's called unsupervised
running and the bad news is that that's
what most of human learning is about so
yeah
the AI system really is a combination of
perception predictive modeling so a
predictive modeling is the ability to
essentially visualize or predict what's
going to happen in the world just
because the world is being the world or
because of our actions so imagining
what's going to happen as we take a
sequence of actions is what allows us to
plan ahead and the ability to predict to
some extent I think is the essence of
intelligence so we need predictive
models we need systems that can predict
how the world evolves their environment
remember what happened and is able to
reason and plan in this world and we
don't really know how to do this with
machines basically what we're asking
here is the old question that unsolved
problem in the eye how do we get
machines to acquire a common sense so
there is a famous set of problems in AI
called Winograd schemas and you have an
example of it here my colleague in the
computer science department Ernie Davis
has a whole list of all those sentences
which are ambiguous so if I said the
trophy doesn't fit in the suitcase
because it's too large you know that the
it refers to the trophy if I said the
trophy doesn't fit in the suitcase
because it's too small you know that it
refers to the suitcase now and the only
way you can lift this ambiguity for this
reference this pronoun reference is is
because you know how the world works
you know what those objects are supposed
to do and and and they're they're the
geometry and all that stuff
you know how do we get machines to
require this kind of stuff we're not
going to type rules into into a machines
for that if I say David picks up this
bag and left the room there is you know
a lot of facts that you can infer that
have to happen because you know how the
world works probably has to stand up
extend his arm pick up its bag walk it's
not going to fly it's not going to you
know fly through the wall it's probably
going to go through the door etcetera so
you know the constraints of the physical
world that's what allows you to fill in
the blanks of all the missing
information that can be derived from
just those few words that's common sense
and we we just don't have any techniques
right now that allow the machine to
learn this kind of common sense so my
hypothesis is that and this is in my
opinion the biggest obstacle to
significant progress in AI and we're one
of the mountains that separates us from
the current state of the eye which is
very primitive and you know general
intelligence if you want or super
intelligence or whatever you want to
call it so there are three types of
learning reinforcement learning so this
is sort of like you know training a
circus animal or a pet you wait for the
animal to do the right action and then
you give a reward and that's very
inefficient in the real world if it's
done in isolation supervised running I
talked about already this is what we
know how to do now but unsupervised
learning which is the ability of animals
and humans and hopefully machines in the
future to learn how the world works by
observation is something we don't know
how to do so the the joke I've been
making for a few months is if
intelligence is a cake the bulk of the
cake is unsupervised running the icing
on the cake is supervised running and
the cherry on the cake is reinforcement
running
the problem is we know how to make the
icing on the cherry but we have no idea
how to make the cake and so I I sort of
sympathize with my colleagues in the
physics department who are sort of the
embarrassing situation to have to tell
the world you know we we know what about
5% of the mass of the universe is and
the rest of 95 percent we have no idea
what it is same thing here so let's
imagine what the architecture of the
future AI system will be like
and this is where questions of the
design of those machines and how we can
control them will will emerge so an
attachment machine will be composed of
really two modules I separated them but
it really part of the same thing an
agent that generates actions that
influence the world and then the world
gives it percepts or observations from
which the agent can sort of infer the
state of the world if you want and the
agent is trying to optimize an objective
that objective is sort of an immutable
module inside of it that determines its
morality if you want so it determines
what what the agent lives to do and so
the goal of the agent is to basically
keep itself happy which means minimize
the output of the objective over the
long run and so what it has to do is
figure out what sequence of actions will
produce the proper percepts in the world
that will put it in the state that will
make its objective minimize which is you
know it will maximize its happiness if
you want so basically what will drive
the machine to do something or not do
something is the design of this
objective as well as how good it is at
optimizing it and then how cooperative
the world is obviously so what does the
agent have to do to be able to act
intelligently internal to the agent
there has to be a couple modules that
basically allow the agent to imagine or
plan what a sequence of actions is going
to cause in the world okay so internally
to the agent it has to be some sort of
world simulator that allows the you know
a model of the world that allows the
agent to figure out what's going to
happen when I take this action all these
sequence of actions or what's going to
happen if the you know I just watch in
the world does does what it wants so
it's the kind of model of the world I
think it's an essential piece of
intelligence and that's why I'm saying
what I said before the ability to
predict is the essence of intelligence
internally there is an actor that
generates action proposals that you know
we can run through the world simulator
which is kind of the way of imagining
what's going to happen when we take an
action but there has to be also a critic
this is a
a term that's used in the context of
reinforcement learning it's basically a
module that attempts to predict what the
value of the objective is going to be
for a particular sequence of events and
this is where the machine can design
substitute objectives for the real
objective so if the real objective is
let's imagine a situation with animals
or humans what the real objectives
really survive and reproduce
there are several actives to that which
are you know some some some are
hardwired like you know feeding but some
are not necessarily hardwired like you
know for humans it's know we're
hardwired that we should you know go to
school or make money or things like that
but a lot of people can build this as a
substitute objective function we again
have examples you know maximizing money
in the political world today but you
know they have kind of substitute
objective functions that they seem to
want to to optimize as a way to optimize
the ultimate one so those are kind of
things are developed on the way to
learning or producing optimal behaviors
so without be too technical there there
will be internal to the system again the
the wall simulator running estimation of
the world the actor producing a proposed
sequence of action and then the critic
kind of figuring out is that going to be
good for me and the better our world
simulator is the the more accurate the
prediction of actions we can we can do
so one of the big questions that we
don't know how to solve with the AI
system is how do we build predictive
models of the world the rest we pretty
much have a good handle on we know how
to build actors The Critic maybe there
is some difficulty but but we don't know
how to build a world simulator and the
reason we don't know how to build it let
me skip ahead actually is because the
world has the bad idea of being not
entirely predictable so so here it's a
very simple example let's say that we
want to teach a machine to
predict the state of the world in a half
a second and we're training it on very
simple videos where we show it a few
frames of a video and then we stop the
video and we ask it what is it we're
going to look like half a second from
now a fraction of a second from now so
here are two little videos where I put a
pen on the table and I let it go and it
falls and machine maybe has been trained
on on thousands of those small videos
and happens to predict the the video at
the bottom left where the pen Falls to
the left and the back so this is the
prediction of a generator essentially a
model of the world that from the past
predicts the future and perhaps some for
some you know source of random variable
as well but let's say in this particular
video the world actually disagrees and
produces the the one on the right where
the pen Falls to the back and to the
right and so it's the the prediction of
the model was qualitatively wrong it's
it's the result is different but
qualitatively right in the sense that he
predicted the the pen was going to fall
and you know maybe not in the right
direction because that's was essentially
unpredictable given the limits of the
perception of the system here and so how
do we train the machine to in situations
like this and this is something that
humans animals deal with every day where
the prediction is fuzzy is uncertain and
we want to train the machine the the - -
we want to tell the Machine ok you know
you're very wrong but really you get it
right qualitatively so essentially the
the possible futures are represented by
this sort of ribbon here the surface of
possible futures and the real future is
one point and that's on that ribbon and
the prediction of the system is another
one in that ribbon and we'd like to tell
the machine if you want that ribbon
you're right doesn't matter that you are
actually technically wrong you are sort
of right and so that's kind of a you
know non formal explanation of the
program of running under predicting
under a certainty really which is what
the sort of mathematical practical and
algorithmic problem we have to solve to
to be able to do and supervised running
so if you ask me the question two years
ago how are we going to solve this
problem or three years ago
yeah you know I mean there is tons of
ideas but none of them work a lot of
people have been working on this for a
long time Geoffrey Hinton and myself you
should enjoy a bunch of other people who
don't supervise running but none of them
really worked that well at least not to
the extent that the same extent as
animals and humans can learn foreign
models this is an idea that came out two
years ago by a gentleman called Ian
Goodfellow who actually was a PhD
student with your cheb NGO at the time
adversarial training I'm not gonna go
into the details of how how this works
but basically it involves training two
different learning machines two
different neural nets against each other
so one is a predictor and another one is
an Assessor or called sometimes the
discriminator which basically tries to
figure out if the the prediction is
observing comes from the real world real
data or comes from the generator and the
generator the generator tries to fool it
it tries to generate points that look as
much as possible as the real world so
that the discriminator can tell the
difference and so it's kind of
adversarial training and it's a very
very CUTE idea a lot of people are
experimenting with this these days for
things like video prediction again I
can't explain exactly how it works but
people have trained those things not to
do video prediction but to do just image
generation so that the input to the
system is a bunch of random vectors and
you run them through what amounts to
accomplish on that backwards which I'm
not going to explain how it works but it
basically produces images and if you
train this on bedrooms and then you run
a random vector in the system and have
it generate an image it will generate
the image of a bedroom and those are
non-existing bedrooms generated by the
system and they have what it takes you
know whatever bed the windows they have
dressers they have you know lighting or
various coins you can have them
generating manga characters and
interpolate between them and you can
even do arithmetic so on the left you
see an image of a man with glasses and
what you can do is compute the input to
the the generator that will produce this
image okay which is a vector you can
also compute a vector that will generate
a man with without glasses and the
vector that will generate a woman with a
glasses and if you take the first vector
subtract the second vector add
third vector and then generate an image
you get a woman with glasses how amazing
is that so that means in this kind of
abstract space of vectors from which you
can generate images you can do kind of
relatively simple arithmetic that
basically embed relationships you know
the man with glasses is to the man
without glasses as the woman with the
letters is to the woman without glasses
essentially that's what it means various
versions of this so if you're in the
back and you look at those images you
might think what those images are as
pictures you know nice pictures from you
know various sources if you're on the
front and you get those you can tell
that those are actually not objects so
those are images generated by one of
those Jayanti models and they capture
the kind of statistics of images without
actually capturing the notion of real
objects so those things don't so this
thing has been trained on thousands with
almost millions of images actually -
from various categories about a thousand
different categories and when you run a
random vector through the system to
generate an image it generates images
that look like objects but are not
objects it's sort of abstract painting
but it looks like real real thing if you
turn it on dogs
you know it generates like Salvador Dali
kind of dogs like soft dogs you know so
it's got the details right but not kind
of the big picture and so that you know
it's because the abstraction level at
which those things are generated is a
little too low
it hasn't really abstracted the notion
of dog really it has abstract a kind of
the statistics of dogs if you want so it
works also for video prediction these
are a few snippets of videos where the
first four frames are observed and then
the last two frames which are so called
in in red are predicted by a video
prediction system and if you are not
careful if you train with sort of a
classical algorithm that doesn't use
this adversarial training you get this
very blurry prediction at the top here
the system can do nothing but predict an
average of all the possible futures and
that ends up being a blurry image that's
kind of the average of all the possible
things that can happen but with this
adversarial training you get fairly
sharp predictions which may be wrong but
they look plausible so other examples
where here it's funny because we we take
a photo that we we take a camera that we
turn around and and the system has to
predict us to basically invent what this
apartment is going to look like that
it's never seen before and so you know
it sort of continues the bouquets and
the shape of the couch and all that
stuff yeah so what will future AI
systems be like okay so this it's
difficult for us humans to imagine an
intelligent entity that doesn't have the
drives of and the failings of human
nature so as humans you know we have sub
preservation instinct we have the desire
of access to resources we don't starve
you know some of us not necessarily the
smartest have desire of access to power
to control other people so that's not
necessarily correlated with intelligence
by the way so we we have this instinct
of associating all of those traits to to
intelligence but in fact there are as
Nick said earlier they are orthogonal so
the the kind of drives you you you can
have and the
sort of moral fiber you can have is
orthogonal to your level of intelligence
you could have intelligent machines that
are not that don't have all the failings
of humans so one way to make sure that
AI systems will have sort of a moral
fiber if you want or or basic drives
that are aligned with with human human
morality essentially is to design this
objective that I was talking about
earlier to be to do to beget a machine
to do the right thing but also to be
immutable so that the machine cannot
modify it and that's basically the way
we're built those busy drives we can't
really modify them we can build on top
of them but we can't really modify them
so that would be one way of preventing
servants a way I have kind of good
designs for those objectives and and
safeguards built into those objectives
but what can possibly go wrong
well you know we can wrongly design the
objective we can build incompetent
agents or we can make them live in the
world that's trolling them and turning
them into bad robots and that happens in
fact that's probably something that has
consequences in the short term so there
is you know the famous example of the
the tie dialogue but that Microsoft
deployed recently that got trolled
immediately and they had to shut down in
24 hours so then there is the question
okay so if you know what if someone
designs any AI system to be purposely
nefarious and and sort of releases it
releases it to the world and then it
becomes an AI cyber war basically
becomes you know a question of is my AI
stronger than you are AI and here is an
interesting thing I think defensive AI
and AI war will win because if you have
two AI systems with the same amount of
resources one is a general AI and
therefore potentially dangerous the one
is a very very narrow AI and its only
purpose is to destroy the first one the
second one will win the someway the you
know maybe a the carrier virus can kill
you doesn't have to be that smart but it
has to be specialized and so there is
protection against against you know
rogue a
basically it's specialized AI working
for us
you know there's questions about the
economic consequences of a I wish I'm
gonna skip but so when another thing I
think that's important for for people
people in society and parents in this
room to realize is that the emergence of
human level AI will not be an event it
will be progressive over several decades
it will it will take multiple decades
people disagree on how many it will take
it it's definitely not going to happen
in the next ten years twenty years some
people ask questions the century perhaps
the problem is as always in the eye
we see the obstacles we see what
obstacle in front of us in my opinion
unsupervised running is one of them we
see we see this big mountain we have to
climb we don't see all the mountains
behind it and so it makes us a little
optimistic because we think that by the
time we're past this first mountain and
we'll have sold the problem the history
of AI has been a repetition of finding
new mountains behind the one you just
you just passed the other thing is that
no entity in the world no lab no company
is significantly ahead of any other you
know in certain areas maybe one can
produce something three months before
another but it's immediately reproduced
so it's not like there is some secret
lab somewhere in Alaska or whatever
place where some genius is building a
human destroying a is just can't
possibly happen it can't even possibly
happen in ads like like deepmind Google
Facebook IBM Microsoft etc because all
the research is done in the open mostly
if it's not done in the open it becomes
open really quickly because you know
people change companies and you know
things like that so it won't be kind of
a secret conspiracy if you want and the
the good ideas come from academia anyway
so
last question this is my that's right
will a I fear disconnection so this is a
theme in you know that that was
pervasive in some of the questions we
had here and also in science fiction the
in 2001 Space Odyssey where Hal 9000
gets disconnected is very scared it kind
of blocks the astronaut from coming back
into the spaceship because it is afraid
of getting disconnected so well
really Isis don't be afraid of being
disconnected and I don't think so
because it's not necessary that will
have they will have self-preservation
instinct again we have to build that
into them for for them to be worried
about it and so we can design those
objective functions so that it's not a
problem for them to be disconnected the
second one is you know will it be
morally wrong to disconnect them if they
are intelligent the way it's you know
morally wrong to disconnect a person and
the thing is it may not be morally wrong
because you know we can we can save
their their memory on a hard drive and
we store them whenever we want first of
all so they're not a unique piece of
thing that will disappear if we turn
them off like like like what happens
with humans and second they may not care
for the reason above and so if they
don't care and if we can revive them
then it probably isn't morally wrong to
turn them off if we want to thank you
um you you started off scoffing at
traditional AI you know people don't
understand logic and but then you you
mentioned that sort at the beginning of
your talk that deep learning networks of
this sort you you you work with don't
actually do memory very well and then
you proposed a memory module doesn't
that suggest that there might be a room
for traditional AI somewhere in the
middle between a deep learning network
for perception and a deep learning
network for action so I just wonder if
you could speak to that right so I it
was not my intention to scoff at
traditional AI because I think you know
I think it has a new problem and we
played an important role I think what if
there is something that's wrong with it
is the the fact that the form in which
is formulated in terms of you know logic
and symbols makes it very difficult to
be compatible with with learning or at
least the learning techniques that we
understand today which are based on
gradients and numerical optimization and
things like this which are you know very
non discrete so logic is discrete and
what we know how to the kind of machine
we know how to train our kind of
continuous if you want and so I think
the the ideal combination between the
two which is what those memory networks
and differential computation systems are
is basically how can we implement the
type of operations that classically AI
systems with you know knowledge bases
and rules can can do but make them
differentiable so that they can be
compatible with deep learning and we can
train them to do complex operation so
but you know I'm agreeing to the implied
suggestion you were you're making I
think excellent talk so part of
intelligence is being able to describe
things and you know me I come from
machine learning prediction is our bread
and butter but part of intelligence is
this
being able to describe or explain things
where does that fit within your
worldview of AI so I think that's part
of human intelligence so you know what
when essential characteristic of humans
as social animals is the is language but
a lot of animals are pretty smart and
can't really describe things very well
at least we don't have the feeling that
they can maybe they can write so okay so
there are two things they are right I
mean certainly some animals are trained
some a lot of mammals in particular are
you know get a lot of input from their
parents to to learn don't survive so
many males don't so you know octopus are
pretty smart and the the mother dies
when the babies are born and so they
never trained they're not social animals
and pretty smart they can solve problems
they can open jars to get crabs out of
em and you know so there are different
forms of intelligence some that require
interaction with with tears some that
may not just interaction with the world
and the necessity for explanation or
description is something that comes with
language and social animals but there is
plenty of animals also who are very
smart and not particularly social that
go hutongs so so I think again it's one
of those examples where we think the
human characteristics are necessary for
intelligence but really there aren't
they your language is not necessary for
intelligence perhaps I mean that's a
very controversial statement in this
world maybe I should I should have
brought a yeah okay so actually I have
two questions so one is we're training
all these recognition networks to
recognize sheep cars you know Bugattis
those are discrete things if if we're
hoping that these deep learning networks
will sort of grow into a whole agent why
what and they can't handle discrete you
know classical logical concepts and
objects then that that just seems like a
complete mismatch
the second question is on this issue of
self preservation see we don't have to
build self preservation into the robot
but the robot if it's half halfway
intelligent knows that it can't
for example fetch the coffee if it's
dead so we don't have to build self
preservation in if it's going to fetch
the coffee and there's a big hole in the
floor it will go around the hole so it
doesn't fall into it so it for all
practical purposes have a
self-preservation instinct if someone
tries to switch it off and it has the
goal of fetching the coffee it will
prevent the perfect person from
switching it off yeah but won't have
this huge surge of adrenaline when he
feels threatened it doesn't matter right
what matters is what does it do well it
doesn't I don't care whether it feels
threatened or whether it worries about
death what I care about is does it
prevent me from switching it off does it
mow down pedestrians who get in the way
of between it and the coffee the things
that matter so I guess I mean that's a
very important question and it and it's
essentially embedded in this how the
question of how you design this
objective right so in the objective
there are very kind of low-level primary
objectives like you know build paper
clips or whatever but you know we can
have other terms in the objective
function that says you know don't turn
the whole universe into paper clips like
you know limit the amount of resources
you use you know you know golden rule
like like like like objectives in in
them so relatively simple things that
can be implemented that well kind of
make themselves safe but it's all a
question of how do we design this
objective I don't think they are I don't
think those are like fundamental
questions of oh my god things will go
bad if you know by default I mean it's
hard enough to build a machine that does
anything so you know like building a
machine that's going to destroy humanity
by accident I find that as an engineer I
find that extremely unlikely but but it
all comes down to designing those
objective functions so the way you do
this is you know you build a machine
with your key function you test it
thoroughly without actually giving it
real power right and and you
what kind of safeguards if you really
scared about it I mean that's the way we
build airplanes that's the way we build
just about anything that we use power
stations occasionally happen twice
okay now we've just got time for a brief
panel discussion might first see if any
of the the speakers have questions for
each other design objectives to
representing the morals or ethics of the</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>