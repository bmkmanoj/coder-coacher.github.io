<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Rights of Artificial Intelligences | Coder Coacher - Coaching Coders</title><meta content="The Rights of Artificial Intelligences - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Rights of Artificial Intelligences</b></h2><h5 class="post__date">2017-09-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/54-FI4qpwa8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the the next speaker is Eric
schwitzgebel he doesn't seem to have
slides is that possible it's okay oh yes
so Eric is professor of philosophy at UC
Riverside but most philosophers will
know him as the author of the most
widely read and I think by far the best
philosophy of mind blog the splintered
mind where many really extremely
interesting topics have come up that it
resulted in papers by Eric such as a
paper on why people used to dream in
black and white but now seem to dream in
Technicolor
his main interests are philosophy of
psychology philosophy of mind moral
psychology he's also interested in
classical Chinese philosophy mara is a
grad student of philosophy at Riverside
her interests are an ethics mind and
their intersections in moral psychology
and the the title is the rights of
artificial intelligence says thank you
so hypothetically we might someday
create entities with human grade
artificial intelligence and by human
grade artificial intelligence which from
now and we'll just refer to as AI for
short we mean AI that's both cognitively
and emotionally similar to human beings
that is AI who have both human-like
theoretical and practical intelligence
as well as a human-like capacity for joy
and suffering so today we're going to
present a positive argument that human
grade AI would deserve moral
considerations similar to what we owe
natural human beings and perhaps even
more moral consideration than we
normally owe to human strangers and
we're going to suggest three imperatives
for ethical AI design and discuss a
puzzle case we find interesting
so here's our argument for AI rights
which we call the no relevant difference
argument premise 1 if entity a deserves
some particular degree of moral
consideration and entity B doesn't does
not deserve that same degree of moral
consideration there must be some
relevant difference between the two
entities that grounds this difference in
moral status right and for premise 2
there are possible a eyes who do not
differ in any such relevant respects
from human beings a conclusion therefore
there are possible a eyes you deserve a
degree of moral consideration similar to
that of human beings and the conclusion
here is intentionally modest we're only
saying that there are possible a eyes
who deserve equal moral consideration
we're not committing to technological
optimism about actually creating such a
eyes in the foreseeable future nor do we
commit to any particular architecture
that they would take and then if desired
if you want to you can make the argument
bolder you can strengthen it in either
dimension the first or second premise so
premise 2 relies on what counts as
possible a is receiving equal moral
consideration so we might strengthen it
by more narrowly specifying what types
of entities would deserve moral
consideration for example somebody who's
an enthusiast about simulated realities
might argue that there's possible a eyes
who live entirely in simulations and new
nonetheless deserve equal moral
consideration and then premise one
relies on there being a relevant
difference and you might make the
argument bolder by narrowing what types
of differences are relevant differences
for example you might argue that
material constitution is never relevant
unless there's going to be some
psychological or social difference that
goes along with that material difference
and for example the fact that some
possible a is our virtual may not count
as a relevant difference from humans to
diminish their moral status so this no
relevant difference argument suggests a
kind of test of moral status which we
call a different
test the difference test is a type of
moral argumentative challenge if you're
going to regard one entity as deserving
greater moral consideration than another
then you ought to be able to point to a
relevant difference between these
entities that justifies that
differential treatment and an inability
to provide such justification opens you
up to suspicions of chauvinism or bias
so there are possible objections to even
the modest on strengthened version of
our no relevant difference argument and
here are three objections to the second
premise in our brief replies so is this
objection from psychological difference
according to this objection all possible
a eyes would be importantly
psychologically different from natural
human beings in a way that would prevent
them from deserving equal moral status
for example they might inevitably lack
freewill or consciousness or human-like
creative insight and our reply well the
objection overestimates the difference
between us and possible future a eyes
who might be designed along very
different lines and current a eyes so
future AI systems might involve
artificially grown biological or semi
biological systems chaotic systems
evolve systems
artificial brains and so on and even
researchers such as John Searle
and Roger Penrose who are famously
skeptical about conscious AI designed
using architectures that are currently
popular even they allow that future very
different architectures might generate
artificial beings who have as much or as
little freewill consciousness and
creativity as natural human beings have
so for the next objection this objection
from duplicate II so the objection might
be something like a eyes would be
duplicate belen a way that natural human
beings aren't so that destroying an AI
is less of a tragedy than destroying a
human being and our reply to the
objection is maybe duplicate e does
reduce this moral worth of something but
there are possible AI is so complex that
they would be no more duplicate than a
human being
or it might also be someday possible to
duplicate natural human beings in which
case that argument would lose its force
and then finally in some cases duplicate
bility you could think of it as
increasing the moral worth of something
for example if one entity is a planned
template for making ten thousand more
than destroying that one might be more
of a tragedy than destroying another
entity that can't be duplicated and then
the objection from other nests so
according to this objection since we're
human beings we have a special duty to
the human species thus there always be a
difference between us humans and then AI
and reply reflection on hypothetical
cases reveals this to be kind of
obnoxious if intended as a general
principle so you can suppose you can
imagine that there was somebody
psychologically similar to us in every
way blamelessly thrown put into our
society from infancy and an ignorant of
its status as an AI and maybe unknown to
everyone around including herself this
being had been artificially grown in a
vat rather than from human DNA and is so
because of that not a member of our
biological species or maybe has an
artificially constructed brain but one
that works just like ours with the same
kinds of thoughts and feelings and
conscious experience so it would be a
cruel mistake to demote such a being
from full moral consider ability simply
upon discovering this artificial nature
or origin and then here's a fourth
objection this objection from
existential debt so suppose you build a
fully human grade intelligent robot
fully conscious fully capable of human
levels of joy and suffering full of
long-term hopes for its future and
suppose now that this human grade robot
cost you $10 a month to maintain you
maintain it for a couple of years but
then after a while you decide you'd
rather spend the $10 a month on a
magazine subscription so you decide to
dismantle and delete the robot learning
of your plan however the robot complain
hey I'm just a being as worthy of
continued existence as you are you can't
just kill me for the sake of a magazine
subscription and suppose you reply you
ingrate
you owe your very life to me you should
be thankful just for the time I've given
you I owe you nothing if I choose to
spend my money differently it's my money
to spend so the core idea of this
objection from existential debt is that
artificial intelligence because it's
made by us
owes its existence to us and thus it can
be terminated or subjugated at our
pleasure without moral wrongdoing as
wrong as long as its existence overall
has been worthwhile and now consider
this possible argument in defense of
eating humanely raised me and see like
how it might be different so a steer
let's suppose leads a happy life grazing
on lush hills it wouldn't have existed
at all if the rancher hadn't been
planning to kill it for me
its death for me it appears to be a
condition of its existence so seen as a
package deal the rancher having brought
it into existence and then killed it is
overall morally acceptable but then or
imagine a religious person dying of
cancer who doesn't believe in an
afterlife
she might console herself by thanking
over all her life has been good she's
grateful so God has given her nothing to
resent and so analogously the RV
argument might go
your robots continuation at your
pleasure was a condition of its very
existence why it came into existence so
it has nothing to resent so we're not
sure how this argument works for
non-human animals raised for me but we
reject this argument from this objection
from existential debt for human grade AI
we think the case is closer to this
clearly morally odious case which Eric
will continue with all right so suppose
that Anna and Vijay decide to get
pregnant and have a child the child
lives happily for his first eight years
on his ninth birthday Anna and Vijay
decided they'd prefer not to pay any
further expenses for the child so that
they can have a boat instead no one can
easily be found to care for the child so
they kill him painlessly
but it's okay they argued just like the
steer just like the robot they wouldn't
have had the child let's suppose had
they known they'd be on the hook for
supporting it all the way up till its
18th birthday the child support at their
pleasure was a condition of its very
existence over all its life has been
worthwhile it has nothing to resent so
we hope that you'll agree that their
thinking isn't a good example of moral
thinking their decision to have a child
carries with it a responsibility for the
child it's not a decision to be made
lightly and then undone although in some
sense the child owes its existence to
Anna and Vijay that's not a callable
debt that can be vacated by this then
discontinuing the child's existence so
our thought is that for an important
range of possible AIS the situation
would be similar right if we bring into
existence genuinely conscious human
grade artificial intelligence fully
capable of joy and suffering with a full
human range of theoretical and practical
intelligent intelligence with
expectations of future life then we make
a moral decision approximately as
significant and irrevocable as the
decision to have a child so we're
inclined in other words to flip the
objection from existential debt on its
head if we intentionally bring a human
grade AI into existence we put ourselves
into a social relationship that carries
responsibility for the AI as welfare we
take upon ourselves the burden of
supporting it or at least sending it out
into the world with a fair shot at a
satisfactory existence
so artificial beings if they're
psychologically similar to natural human
beings in consciousness creativity
emotionality self-conception morality
rationality fragility all that stuff
they warrant substantial moral
consideration in virtue of that fact
alone
if furthermore we are also responsible
for their existence and their features
then they have a moral claim on us that
otherwise similar human strangers would
not have to the same degree all right so
that's our primary argument for the
moral status of a is now we're going to
suggest three imperatives of ethical AI
design and then we'll conclude with a
puzzle case all right so the
epistemology of consciousness is quite a
tangle especially epistemology of the
consciousness or not of hypothetical
future AIS or I suppose that you create
a sophisticated machine that's capable
of some sophisticated and intelligent
seeming behavior is it conscious does it
have a real stream of experience as a
capable of genuine joy and suffering on
almost every view of morality these
facts about it would be morally relevant
to its moral status so if it's hard to
know what kind of conscious experience
it would or would not have then it would
be hard to know what our moral
obligations to that AI would be right so
that kind of difficult epistemology then
invites three related imperatives of AI
design that we want to suggest one is if
we are going to continue to make
intelligent and sophisticated a eyes
then we should simultaneously prioritize
research into the nature of
consciousness and the conditions under
which we could reasonably expect that
genuine joy and suffering would emerge
from complex systems otherwise
might accidentally create conscious
beings without realizing that we're
doing so or alternatively we might think
we've made conscious beings without
actually having done so
and either possibility invites moral
catastrophe we should also consider a
second principle which we call the
design policy of the excluded middle
according to this policy we should only
design artificial intelligences whose
moral status is clear one way or another
right we should either design AI that's
simple enough that we know that it
doesn't merit serious moral concern and
can be deleted at will or alternatively
we should create AI that whose moral
status we know deserves respect and then
give it its due violating the excluded
middle policy means creating a AI whose
moral status is unclear this would then
create an unfortunate dilemma right so
the dilemma would be either you give
this dubious AI full moral rights you
think it might deserve or you give it
diminished rights if you give it the
full rights and it doesn't in fact have
consciousness enough to merit or
sophistication enough to merit the level
of moral concern that you've give it
then you might be sacrificing real human
interests for something that doesn't
have interests worth that sacrifice but
on the flipside if you give the AI
diminished rights when in fact it might
really deserve full rights then you risk
perpetrating slavery and murder without
realizing that you're doing so here's a
third design policy we call this the
emotional alignment design policy the
idea here is that you want to design
artificial intelligence to a provoked an
appropriate range of emotional responses
from ordinary users neither too much nor
too little
now suppose just suppose the researchers
eventually figure out tricky questions
about what types of AI do and do not
deserve serious moral consider
raishin we shouldn't create an AI that
deserves serious moral consideration and
then put it into a bland box with a poor
user interface that tempts people to
disregard its status conversely we
shouldn't create AI that generates a
powerful impression in ordinary users of
deserving serious moral consideration if
that AI is not really sophisticated
enough to deserve that moral
consideration so for example suppose you
create an AI without consciousness but
which ordinary users react to as though
it's really conscious really deserves
consideration and now there's an
accident in which both that AI and
normal natural human are both in mortal
danger right you wouldn't want an
ordinary user to rush to save the
beloved but unsophisticated non
conscious AI and let the ordinary human
die right so our emotional responses to
AI should be neither too high nor too
low now in Douglas Adams's novel the
restaurant at the end of the universe
there's this scene which this
protagonist Arthur Dent is given a
chance to meet the meet he's an uplifted
intelligent steer ambles up to him in a
posh restaurant and starts talking how
about how delicious he is and what cart
the custom would be that would that
would be the tenderest right and the
idea is Arthur's gonna pick out a slice
of this steer and then the steers gonna
go in the back room and shoot himself
for Arthur's dinner Arthur naturally
enough finds this prospect unappetizing
or here's another example consider a
human grade Android that's sent on a
dangerous scientific mission alongside
several natural humans to be capable of
astute scientific and practical
reasoning and smooth social interaction
this AI is given a normal complement of
human emotions except for one difference
it radically des prioritizes the
continuation of its own existence
compared to the continuation of the
other humans
and the completion of the scientific
mission without qualms or resentment
this a I would sacrifice itself if that
meant a 5% increase in the likelihood of
one human surviving or if it meant that
the data collected at the end of the
mission would be higher quality we might
imagine both Arthur's meat and the
scientific Android are pre-designed
to want nothing more than to commit
suicide on our behalf if that's what it
takes to meet their goals right the
steer desperately wants to kill itself
to be someone's dinner the scientific
Android would desperately wants to kill
itself to avoid a small risk to natural
humans or the or the scientific mission
so we think two interesting ethical
questions arise from this one is is it
ethical to design genuinely conscious
human grade-a eyes in this way so that
they want to commit suicide on our
behalf and the other is if they are
somehow created should we attempt to
prevent their suicide as we would
normally attempt to present prevent the
suicide of a human being so we're not
sure how to answer those questions we're
gonna live time we want to leave time
for a discussion so we're just putting
these out there as worth some thought
but we do want to conclude with one
other thought which is a possible fourth
ethical design principle we think that
in the case of the steer what makes us
recoil maybe is that the steer doesn't
seem to properly value its own life
there's something wrong it seems and
it's wanting to commit suicide
simply to become someone else's dinner
it's not as clear to us that there's
wrong with some something else something
similarly wrong with a scientific
Android case but maybe so either way
we're thinking of maybe we should
consider this fourth design principle
which we call the self respect design
principle and that would be if human
grade-a eyes are sometime someday
created they should be created with an
appropriate appreciation of their own
value and moral status thank you
so looking at this were you saying here
at the last slide shouldn't the obvious
ethical question be the opposite is it
moral to create an Android or create any
AI that actually values user only
systems it would seem to be more moral
to design something that releases
Android that can be happy and then
successfully terminated when animation
is over that would allow I mean it's
just I guess it's just testing this
intuition like with a human being you
would say no you know you need to value
your own existence you don't want to
destroy yourself and that and then why I
don't know why you might say okay they
don't have this desire to live but they
have it in their interest to live so
then why would the AI you can't you say
the same thing is it because they don't
belong to a species where everybody you
know you think everybody should do that
or that seems to be the standard it just
seems difficult to say like why you
would say it in one case with your
fellow human but not say it in the other
case if they're essentially the same
except for that difference and it could
be an artificially intelligent creature
that's organic I mean it might just look
like a human have a brain very similar
to a human just be artificially built so
I don't know like that intuition might
then if it seems that human like it
might you know might be more sympathetic
to it so there are two reasons why we
don't kill people generally right one is
you know we attempt for moral reasons to
minimize human suffering but the other
one is also because there's not other
moral reason which is that we kind of
you know increase the entropy of the
universe if we delete a complex organism
basically
all the information that you know was
created in the brain of the being of
that of that person disappears when that
person dies it may not be the case for
four androids or robots or AI systems in
a sense that they may have multiple
bodies with one mind that survives the
death of the body so in that case the
the moral consequence of you know
perhaps they will still have suffering
when one of their bodies dies but the
increased entropy is limited to their
physical body not to their mind which
will which will survive and so so the
the cost of the the physical deletion as
opposed to the deletion of the entire
mind is considerably less so what you
know it's a different situation as as
humans yeah so I would agree with that
so I would say if the AI continues as a
mind when its body is destroyed and then
can be really that wouldn't actually be
killing the AI yeah so that wouldn't
then be a case of murder that would be
maybe an inconvenience or something yeah
hi
loved your talk so I'm wondering if
there are cases where our intuitions
will fail but there are still moral
imperatives in particular the carrying
capacity of the universe for artificial
Minds is vast almost beyond imagining
and someone could argue that there is a
moral imperative to allow this process
of expansion to unfold in this process
we might be faced with different kinds
of mind crime where our intuitions would
tell us that it's unacceptable but the
potential payoff might be worth it
overall this seems like a very thorny
issue and I was wondering what your
thoughts were thank you yes so I think
that our moral intuitions were built up
out of materials in our evolutionary
history and our
social cultural history and our
developmental history and they're tuned
to a certain limited range of
circumstances in which most of the
beings who we interact with who are
similar to us in one dimension of
complexity are also similar to us in
other dimensions in complexity there's
certain Maxima to the possibility of joy
and the rationality of the beings around
us right so there's a very limited range
of environments in which our moral
intuitions were cultivated once we break
out of those environments it's natural
suspect that our moral intuitions will
start to break down but the problem is
in my view we don't have a good
epistemology for figuring out what the
moral truth is once our intuitions start
to break down right so this puts us in a
difficult position to see through those
issues to paraphrase Jeremy Bentham I
think he said doesn't matter whether or
not it has Walia but whether or not it
can suffer and another famous person
said no no brain no pain which I agree
with but I don't think it works in the
reverse no pain no brain I mean you can
have you know pain without it you can
have a bring without pain how much
weight would you put on the ability to
suffer the capacity for suffering as
opposed to just consciousness or
internal experience and if so why not
design a machine that doesn't have a
somatosensory cortex or limbic system so
in this paper and in general we're
trying to be pretty broad about what the
basis for moral patience your moral
considering bility would be so we're
kind of just trying to load everything
in right consciousness and suffering and
rationality and all that kind of stuff
and say let's assume that you've got all
that stuff now if you then say okay well
which is the really important stuff in
there that's a question that I don't
think we're Mara and I are prepared to
answer right now that's not part of our
project at the at this moment hi thank
you for that talk my name is Tracy Lao
and sorry and yeah I wanted to I noticed
you were asking
consciousness and about that's one of
the prerequisites for moral
consideration and I know
Christof Koch who's one of the you know
world experts in consciousness believes
it's a continuum and so animals do have
consciousness to varying degrees given
that as a human species we've completely
failed in terms of coming up with you
know solutions like cultured meat I know
that the funding for that is very poor
right now we haven't developed
technologies to prevent us inflicting a
lot of suffering on billions of animals
given this well in my opinion a massive
failure what are your fears about how
and I think the work we're doing is
incredibly important but the fear that
they're not going to be protected
because they may not be in a position of
power and if we look on how animals are
treated it's well you know I think it's
a failure and is are we just going to
bring that forward into our treatment of
a eyes I mean I guess that's why what
like a paper like this would try to set
out maybe is some anticipation like
instead of having some crisis or you
enslave a bunch of robots or you have a
bunch of androids and they slowly maybe
after several versions finally get to a
state where they would have as much
status as an animal let's say I guess
you'd want to be prepared for that
because even I mean I think human like
human grade AI is interesting but also
it could be like the case that you would
want to give him as much as animal
rights like earlier on or something like
that so it's good to anticipate that
definitely yeah and I think um and I
would also affirm your point about the
power relationship so currently the
power relationship is that we're the
designers and so we're the bosses and
the legal framework is their property
and stuff like that right so if you just
kind of assume that as the kind of
social starting point then it seems like
would be natural that if AI were created
it would be in some sort of subservient
which might be problematic okay so our
next speakers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>