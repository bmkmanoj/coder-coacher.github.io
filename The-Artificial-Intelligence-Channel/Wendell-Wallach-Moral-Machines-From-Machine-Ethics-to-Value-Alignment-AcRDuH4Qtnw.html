<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wendell Wallach - Moral Machines: From Machine Ethics to Value Alignment | Coder Coacher - Coaching Coders</title><meta content="Wendell Wallach - Moral Machines: From Machine Ethics to Value Alignment - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wendell Wallach - Moral Machines: From Machine Ethics to Value Alignment</b></h2><h5 class="post__date">2017-09-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AcRDuH4Qtnw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we're we're basically going to
power on through there's not a there's
not a coffee break but well Wendell sets
up feel free to take out you take a
minute or so to stretch in place mini
break we figure if people start leaving
this place we'll never get them back in
stick around we're starting in one
minute
okay well so now where this is working
okay okay so now we're getting to the
fourth and fifth talks in our in this
session of both going to be by
philosophers reflecting on these issues
Wendell Bullock has been a really a
pioneer in thinking about ethical issues
about artificial intelligence he's
written a couple of classic books in the
area moral machines is really I think
become the classic book on machine
ethics the project of trying to program
or build ethical principles and ethical
behaviors into machines more recently
he's also written a book called the
dangerous master on the way in which
take we can keep technology beneficial
and under our control he's based at the
Yale University in interdisciplinary
Center for Bioethics where he's been
centrally involved with many projects
and the foundations of technology and
especially artificial intelligence he's
heading up a number of programs in that
area now and today he's gonna talk to us
about moral machines from machine ethics
to value alignment so please welcome
Wendell and I close enough great well
thank you Dave those of you know my work
know I focused primarily on near-term
rather than the longer-term ethical and
governance challenges posed by emerging
technologies as far as the longer-term
concerns such as super intelligence I'm
your friendly skeptic I'm friendly that
the can do engineering spirit that says
remarkable things are coming in in the
future
I'm skeptical as to whether we know
enough about intelligence to know
whether it's reproducible by
computational means and while my
skepticism hasn't totally disappeared I
am truly heartened and appreciate the
work being done by people like Stuart
and eleazar focusing on the control
problem reorienting the trajectory of
out of
intelligence so that we can be working
on both the long-term concerns but
through approaches that are actually
going to help us deal with the near-term
ethical considerations also but I just
sometimes become concerned that we are
overlooking the socio technological
challenges that are rising these are not
just autonomous systems or building but
they're part of the whole human project
the part of our socio technological
world I sometimes think we overlook the
extent to which what we're creating may
really differ from anything we have seen
before hand so perhaps the Machine human
comparisons aren't as good as we'd like
them to be in fact we may be engaging in
our own form of self-deception and it
might be helpful if we came up with a
totally new anta logical category for
what we're creating here most of what
we'll what we see tends to be joint
cognitive systems rather than purely
autonomous systems and one of the major
concerns that comes up when you're
talking about joint cognitive systems is
this problem of coordination and
coordination failures and coordination
failures can lead to disasters and
disasters have a lot to do with helping
shape the trajectory of how technologies
are get get developed for example we
have no idea what the public acceptance
or how the public is going to react when
we have the first self-driving car that
kills a pedestrian think of this for
example we're really engaged in an
experiment with self-driving cars and
pedestrians our human subjects in these
experiments but they aren't giving any
kind of it can inform consent and there
may be a strong reaction there may be a
mild reaction it's not clear at the
moment but there are other things that
are going to affect the way in which
this technology develops if we don't get
a ban on lethal autonomous weapons then
all bets are off as far as our ability
to develop AI in a truly beneficial
robust safe and controllable manner and
then there's technological unemployment
that was John Maynard Keynes's term for
the 200 year old Luddite concerned that
each new technology will rob more jobs
than it creates hasn't
and for 200 years each new technology
creates more jobs but perhaps we're
starting to witness for the first time
the downward pressure that automation is
placing on jobs and wage growth this is
not an AI problem per se but if our
socio-political systems don't address
this then we may get reactions against
the technology which will stop
development but could radically slow
development or what will be acceptable
and what won't be acceptable but I want
to focus upon the prospects for
implementing moral decision-making
faculties and computers and robots that
was a dream that first appeared to to
science-fiction writers most notably
Isaac Asimov who with his stories on on
the laws for robots the first one of
which was round about in 1942 he changed
the whole trajectory of robot fiction up
to that time we had only robots that
turned bad suddenly we had robots that
could be good over the first decade of
this 21st century this idea not the laws
of robots per se but the question of
whether we could implement moral
decision-making faculties and computers
and robots it captured an array of
different scholars philosophers legal
theorists there were a few computer
scientists in there game theorist
psychologists and they slowly gave birth
to a new field of research now when
Colin and Alan and I set out to first
map and lay foundations for that new
field of inquiry which which we covered
in our book moral machine teaching
robots right from wrong it had many
different names from the more
philosophical side it was called machine
morality machine ethics computational
ethics artificial morality and then we
had Eliezer and his colleagues who were
tackling it more directly from
specifically whether we could control
super intelligence which was or which
was a different orient somewhat
different orientation at that time but
the field didn't settle on one term per
se but I think the basic idea is that if
design AI systems that are sensitive to
moral considerations and factor those
considerations into their choices and
actions new markets for their adoption
will be opened up on the other hand if
we fail to adequately accommodate human
laws and values there's going to be
demands for regulations that limit their
use initially this field of research was
one part moral philosophy one part
applied ethics one part moral psychology
and another part I would say computer
science mathematics more hard sciences
would get involved in other words the
hard sciences played a very small role
in the research that was actually going
on but if you approach this from the
perspective of a moral philosopher and
and it's been interesting that in I
talked so far words like morality and
ethics have not appeared very often in
fact the I only counted it twice from
from from Max and he used the term
grammar school morality was that who was
that the phrase that we had I didn't I
didn't hear those words from either
Stuart or or Eliezer but when you
approach it from the perspective of
Memorial philosopher the question is
well what role should epical theory
should moral philosophy play in the
design architecture and the control
architecture of of a computational
system and you might turn to two broad
approaches one is the top-down approach
and top-down approach there is a
philosophical definition at the top for
those of you who like that kind of thing
but top-down really refers to capturing
any moral theory or principle within the
system so we're talking about the Ten
Commandments we're talking about Kant's
categorical imperative utilitarianism
and as a cosmos lost her robots which
started out as three but eventually
became four so it's really could one of
the languages of ethics be instantiated
computationally bottom-up approaches
refer much more to what happens in the
moral development the moral education of
a
could you take it through a process
where it learned about morality
regardless of whether it it was directed
toward any higher order ethical theory
and it takes inspiration from learning
from evolutionary psychology we have
game theory comes into play there
genetic algorithms all kinds of
different techniques learning algorithm
might be appropriated for a learning
computer but there was a third area that
came up when we looked at this and and
and much of the research that's going on
has looked at different approaches and
whether they really are computationally
tractable at not or not but there's a
third distinction that really has to be
made here and that's that humans are
evolved systems we evolved from a
biochemical platform in our higher order
faculties they emerged from the
instinctual emotional brain computers
are logical platforms from the get-go
now this might give computers certain
advantages they're natural-born Stoics
they can do broad ranges of calculation
we humans our rationality is bounded
furthermore these computers have an
absence of emotional biases they have an
absence of base motivations and they
have and they are not likely to have
emotional hijackings on the other hand I
grew up in the age when stoicism was
what was valued in moral decision-making
we now live in the age of moral
intelligence where we really reflect on
both the beneficial and dysfunctional
aspects of of emotions and that raises a
question of whether artificial moral
agents are gonna require some form of an
effective intelligence will they need
emotions of their own and will it be a
satisfactory if they just have cognitive
emotions weights that are programmed
into them or will they have to have
somatic emotions will they have to be
able to feel and what if they have just
simple faculties such as the ability to
read the emotional expressions on human
beings and we're gonna feel comfortable
with that I don't mind if a robot knows
that I'm smiling but I don't
know what the goals of the robot may be
and I may be very uncomfortable if it
knows when I'm vulnerable but I think
one of the most important areas in all
of this and perhaps where we made a
significant contribution which may not
be as apparent today as it was back in
2008 when our book was published is that
there are many capabilities beyond moral
reasoning that go into moral
decision-making and that was a period
when there was getting to be a lot of
fresh research just beginning in
different areas of human moral
psychology and people were beginning to
focus on emotion sociability embodiment
theory of mind
empathy consciousness understanding what
these different capabilities play in our
in our decision-making capabilities
other than emotion which were often
overlooked in the history of moral
philosophy and that created a whole
subfield of looking at specific
capabilities when they would be needed
for moral decision-making and what role
they played certainly they played roles
of access to new forms of information
that might not be there otherwise but
what role does consciousness what
functional role does consciousness play
in moral decision-making and there is a
paper that we produced on that
particular question but when you look at
this whole challenge more broadly there
are two really hard problems here one is
whatever goal norm rules principles of
procedures you select how do you
implement them the other is what I refer
to as framing problems and framing
problems as how does the system
recognize it in an ethically significant
situation how does it discern essential
from in essential information
how does it estimate the sufficiency of
the information it has and I won't go
through all of these but these are you
know different kinds of things that are
framing problems that are very difficult
to bring in so that gives you a very
quick synopsis of what went on in the
first decade and I heard about values
alignment for the first time in a speech
that Stuart gave two years ago and I
immediately recognized it as a bottom-up
approach to machine ethics but it was
clear that the engineer
and the ethicist didn't necessarily know
that background or that thinking at all
and what was exciting for me is that
this was starting to be picked up more
as an engineering as a computer science
as an AI challenge and wasn't just the
reflections of us philosophers and
cognitive scientists and so forth
looking at what the challenges might be
but it was interesting that the emphasis
was on the word values here and I came
to understand that engineers were very
uncomfortable with words like ethics and
I think that was actually illustrated in
some of our earlier earlier talks they
perhaps see morality is nothing more
than politics by other means or perhaps
they're very cognizant of the failure of
ethics or the constant ethical debates
about which ethical Theory
consequentialism or deontology or virtue
theory should be superior and which form
of those should be superior the problem
is that ethical theories didn't have not
led to clear action procedures either
for humans or fir or for computational
systems there's no moral algorithms and
I but I also think that what's going on
here is something similar to what
happened at the beginning of the
Enlightenment at the beginning of the
Enlightenment we all acquired a cell for
the first time up to that time
individuals had souls they didn't have
cells and this word self slowly came
into being to substitute for the word
soul as the individual because it didn't
carry the theological baggage that the
word soul did so perhaps we're trying to
expunge ourselves of some of the the
dross that maybe ethics brings with it
but I'm concerned about whether we are
bringing in new biases and that they'll
have their own their own losses so I'm
particularly concerned about what might
be lost as we take on a broader and
broader technocratic explanations for
human faculties I for example have
meditated for forty seven years now I've
got tens of thousands of hours in in
doing that now perhaps I'm just stupid
and I've wasted a significant portion of
my life I don't think so I think what
that is all about is about subtle
sensibilities that science doesn't
really know how to approach yet here's a
quote I sort of like from Steven
Hawkings he said the greatest enemy of
knowledge is not ignorance it is the
illusion of knowledge
now when engineers approach the
challenge of implementing moral
decision-making faculties they usually
think of ethics in terms of a utility
function I think we saw that quite
effectively with eleazar's conversation
or they look at it in terms of
constraints on behavior and this is all
this is all very helpful and good but I
think we need to be aware what the
problematics are there for example and
this goes back into all kinds of
reflections around utilitarianism
consequentialism what is being maximized
what does that utility function really
stand for what are you trying to
maximize is it pleasure as was brought
up in our last talk is it look good and
what is the good is it human welfare and
how do we define human welfare now the
previous century was beset by political
systems for whom the greatest good for
the greatest number was an aphorism that
they live by and they had no compunction
about killing tens and even hundreds of
thousands of people in in pursuit of
maximizing the greatest good for the
greatest number this has always been a
weakness of utilitarian approaches to
ethics and it is perhaps not an accident
that John Stuart Mill really one of the
two founding fathers along with Jeremy
Bentham of what we know as utilitarian
theory also wrote a book called on
Liberty which in some senses compensates
for I think the weaknesses within
utilitarianism by itself so the
importance is that utilitarianism if
left just to itself can run roughshod
over
human rights and human rights are
essentially deontological but
nevertheless utilitarianism is very
attractive from a scientific perspective
because it says just calculate look at
the various consequences of your action
and add up the benefits and and losses
that you get and then select the one
that gives you the most the most
benefits the problem is our information
tends to be inadequate tends to be
inaccurate and consequences can't fully
be determined so very few people
actually go through some form of
utilitarian process utilitarian analysis
though there are strong advocates for
that and I even advocate for that in
various conne in certain contexts so
here's two of our main advocates we have
Peter Singer and we have Josh Greene now
let me take you back to something that
came up yesterday a little bit to talk
about another problematic with
utilitarian determinations of the
appropriate action and that's the
application of trolley car problems to
the driverless cars so I think we all
know what that's about that's largely
about should the car kill a number of
pedestrians or individuals and some
forum and others should drive off the
bridge and kill the passengers of the
car and in science magazine there were
two articles about this in June one of
those articles almost a piece of public
opinion research that found that the
preponderance of people wanted a
utilitarian calculation they said kill
them kill the least number of people but
they were asked another question and
surprise surprise most of them said I
would not buy a car that would kill me
and my family now there was a companion
piece to that by Josh green and Josh
green said well let's build moral
algorithms into these cars now he could
be excused for such naivete because he'd
read moral machines in fact he'd read it
twice
and even though I have advocated for the
implementation of moral decision-making
faculties in computers and robots this
is not a morally decidable challenge
there is no right answer are we going to
for example implement moral algorithms
into cars that might kill the occupants
but it stops millions of people from
buying those cars which means to stop
one once in a trillion accident and save
a couple lives we may lose thousands of
people who die because they because of
human error
you know this we've got a long-term
utilitarian calculation their short-term
can utilitarian calculation it's not
decided by moral theory it's the the
need for the establishment of a new norm
and we probably need multi-stakeholder
our committees to engage in that process
now there's also been a lot of
discussion that's going on over the last
few hundred years about whether you can
turn utility utility into rules and
duties or rules and duties into utility
in other words can most of our rules and
utility or least the ones we want to
carry into the future can they be
described in utilitarian terms and from
the deontological side it's often but
said well the greatest good for the
greatest number is one of the prime
official duties that we have but it is
not the determinant of all actions it
just plays off against other prima facia
duties that we have from the side of
utilitarianism there's been concerned
about how we can contract this whole
process because utility analysis can be
really time-consuming can take a lot of
energy and can we contract it down to
simple heuristics so can we have rule
utilitarianism which is in a sense a
compromise where you turn you a utility
analysis into a bunch of of rules
heuristics that you can make quick
decisions but all of us understand there
are some limitations of we're making
quick decisions are good in where they
override other concerns
and as we have become aware there are
cognitive biases there all kinds of
cognitive biases that we humans indulge
and we are subservient to bad reasoning
in in areas that Dan Kahneman has has
helped elucidate for us all and I guess
all I want to say is sort of bringing up
my concluding section is that we need to
start thinking about what are the
cognitive biases of Engineers and the
ways they are going to approach this
challenge and I am not bringing this
these critiques up as a naysayer
I am totally supportive of any approach
to implement moral decision-making
faculties in our computers and robots to
see how far we can get
but the biases are that ethics is merely
a constraint problem and it's not merely
a constraint problem or the
utilitarianism a consequentialism is the
way we should go in all matters no it's
the correct way of going in some matters
but there are areas of concern such as
medical ethics where it becomes
problematic and that the stoicism is a
good thing that robots have I think a
lot of people understand know the
stoicism may not be adequate for some of
the kinds of moral intelligence we like
and there's a tendency to confuse right
and wrong with human moral philosophy
ecology in other words the way we make
this moral decisions isn't also has
flaws within it also has cognitive
biases within it and there will be a
tendency to ride roughshod over the is
art distinction as Eliezer did when he
brought up David Hume in in his talk or
the naturalistic fallacy I'm not going
to get into those at all because way
beyond the scope of this particular talk
but all I'm trying to say is let's keep
the biases in mind we are still engaged
in an experiment here we're engaged in
an inquiry as to how far we can get in
creating intelligent systems through
computational means we have no idea we
have all kinds of theories and all kinds
of conclusions about where this is going
now this is still an experiment and a
lot of these theories may turn out to be
wrong and if if there happened to be
clear limits in our
to develop artificial moral agents or
manage robots then it's incumbent upon
us to recognize those limits so that we
can turn our attention away from a false
reliance on autonomous systems and
toward more human intervention in the
decision-making processes of computers
and robots thank you very much hi Henry
Kong here
yes sir was talking about how one
approach to making machines ethical is
to program them to be sort of more like
us or learn from how how people are or
any more but you know I see a conflict
here and you're saying another approach
is to sort of teach them not how we are
but how we want to be in terms of things
like consequentialism the ontology but
then you know I whose the ontology and
whose consequentialism I look around the
room and I see a certain demographic for
example the white male affluent Western
and if these are the people who are
going to be creating these machines then
that creates biases of its own and so
you know you have people who well
Jonathan Haidt calls them weird ethics
so we're gonna program weird ethics into
into the into the into the systems so
there's a conflict between do we want
the machines to be more like us or more
like the people we want to be but then
who are the people we want to be so I
don't think the problem here is that we
know what the machine should be but
there is probably gonna we're probably
going to create standards for them and
if they don't at least live up to good
human values we were gonna be very
uncomfortable with them I mean it would
be great if they could do what your
children do they bring you an ethical
dilemma that that you have no answer for
but you're just thrilled that they
recognize that that ethical dilemma even
exists
Windell okay okay good but there's
there's a lot more to this and there's
other approaches to this problem there's
a the third big tent in ethical theory
is virtue theory and virtue theory says
it's not the consequences of your action
or follow the rules the good which is a
determinant of what's right true and
just but it says it's the actions of
what virtuous people do and it's about
the cultivation of that kind of
character which which would be great if
we do within machines we can start to
think about it now that we have some
more effective learning algorithms and
we used to have but that's kind of an
interesting approach because these
bottom-up approaches tend to be much
more flexible much more adaptive than
the top-down which you know are very
hard to fit in all actions within one
category or not so we're gonna want this
kind of adaptive learning that's that
Stewart outlined but we may also need to
subject it to some kind of top-down
evaluation as to what it's whether it's
appropriate or not he alluded to the
problems of whose behavior are they
observing who are they learning from are
we gonna be able to keep them isolated
from from the bad actors could you
imagine a robot that was just sitting
there watching American news for the
last six months trying to learn what
appropriate values were to be an
American citizen hi my name is Chris
Hatter Columbia University alumnus so my
question is in the president philosophy
engineering dialectic that does exist in
part in this room and in other spaces
what do you see as the big problems in
addition to the ones you've already
enumerated which are the the human
biases well the biggest problem is we
all live in our silos you know and we
have very little appreciation for the
perspectives of other scholars and and
my work right now partially in thanks to
a grant that came from the future of
life Institute
in the Stewart and I have been running
some workshops around has been focused
more on silo busting bringing different
kinds of thinkers together and
brainstorming over these ideas and
getting insights from each other it's
not just that we live in silos but our
institutions don't reward people for
transdisciplinary thinking they all say
we need more interdisciplinary ins but
very few people get rewarded within the
present structure of academia for for
interdisciplinary work and it often
happens only within from older
established scholars it would be nice if
the young scholars who want to jump into
these and more transdisciplinary
challenges we would get rewarded for
though that we'd have more of these
Institute's that really want to draw
upon a transdisciplinary perspective hi
thanks for that this is mostly just a
request for clarification
at one point you distinguished between I
think you said merely cognitive motions
versus real effect you're wondering
which of those you might need for moral
decision-making can you just say a
little bit more about what you mean by
each of those notions sure well all of
the super rational faculties are
faculties beyond reasoning that I had on
that slide there are attempts to
simulate them at least in in cognitive
in computer science in one form or
another so we have effective computing
we have machine consciousness we have
all these fields going on but when
people talk about effective computing
they're often talking about putting
weights on the relationship between
different persons so if it comes to
killing somebody maybe you could put a
heavyweight that says no you know so
it's really a mathematically it's a
mathematical weight on a judgment that
represents an emotion and that's what I
mean by cognitive emotions versus
somatic emotions which are what we feel
now there is a view of life but not only
consciousness
but all of what we are is this vast
infrastructure of somatic emotions of
somatic markers and a lot of what our
moral education is about is
strengthening weakening those markers
and furthermore that our emotions are
very much about maintaining us as
integrated beings as integrated entities
and that that is a fundamental grounding
for our ethics that's way beyond the
scope of anything that we're talking
about in AI at this point and in that is
perhaps beyond what most of us want
these machines to understand in terms of
practical ethics and not doing harm in
some of the obvious contexts so though
they will move through but it may be
something fundamentally that colors what
it means to be an ethical agent what it
means to be an ethical being why we
evolved as ethical beings because we
needed throughout evolution to maintain
that integrity and unfortunately too
often we fall fall to what I will call
the the ethics heuristic which is ethics
is what is my opinion and my opinion
should be treated as a categorical
imperative and if nothing else I'm just
trying to say to here there's perhaps a
little bit more going on in ethics and
whether or not it's going to yield us
algorithms simple action procedures
thank you very much
okay the last talk in this session was
by another philosopher Steve Peterson is
at Niagara University in New York
Steve's background is in is in
epistemology he's worked in a whole lot
of error of fields over the years he's
done some really interesting work on
algorithmic metaphysics and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>