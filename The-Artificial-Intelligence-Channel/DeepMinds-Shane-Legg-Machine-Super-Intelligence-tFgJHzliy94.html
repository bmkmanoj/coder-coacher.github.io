<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind's Shane Legg - Machine Super Intelligence | Coder Coacher - Coaching Coders</title><meta content="DeepMind's Shane Legg - Machine Super Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind's Shane Legg - Machine Super Intelligence</b></h2><h5 class="post__date">2017-08-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tFgJHzliy94" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll be involved in trying to build
thinking machines and stuff for a while
my first started doing machine learning
at the University of Waikato New Zealand
using the week of work bench
so if you're doing your machine learning
you're Manstein at this it's pretty neat
it's a free software and it's got all
the classification and the classifiers
it's all already know Java and you can
this victory is plug and play with the
more ones visualize it so it's premium
so I started on there and then I went to
number of years later so we were being
good for which you you probably know and
I started working in New York on his
project intelligences which later became
with mine and one of the things that
struck me when I was working there as we
were meant to be with them on the thing
machine if I lean around I talk to
different people in the offices what are
we trying to achieve what is
intelligence they're all give me
different answers and not only will they
give me different answers but some of
the answers but if you contradict each
other I mean they would actually be
inconsistent with each other that's one
thing and in any project where you can't
actually agree on where you're trying to
get to the different people working on
it
I think and so this got me interested in
trying to try to understand well what is
intelligence
can you get it try to find them
precisely we we want to get to everyone
we want to get to we might better come
up with some plans on how we might
trying to get there
so what I've found in general since then
is that if you talk to different people
and artificial intelligence you get
roughly three categories I think of
answers about what is it when you're
asking the waters intelligent
intelligence one group of people
basically give me answers along this
line they say well I'm not really
working on intelligence I do speech
recognition or I do I build classifiers
all right doot-doot-doot whatever and I
want reassure what intelligences now I
respect these people because I think
they're almost I think they they
genuinely don't no one comes and says
they maybe they got in
the feel to start with because they want
to do something like this and and then
they started building classifiers they
do and they make and they do define
rigorously what is they're trying to do
there there are different measures of
performance of categorizations on and
you think yeah so it's quite a concrete
thing but I sort of drifted away they
don't long ago trying to build a real
intelligence thing they don't you should
be sure what it is then there's another
group of people and I think it's summed
up by something like this quick
interactions intelligence is the same as
we're actually pornography I kind of
find it but I like it when I see it
um and I think this group is a
problematic group basically their
attitude seems to be yeah I don't know
what intelligence is I don't know where
I'm trying to get to but I think when I
arrive I'll not and so let's start
writing some code and building some
system and trying to make something that
does some stuff and we think
intelligence has something to do with
this and something to do with that so
we'll throw a good this and this ro bit
of that in and we sort of meander around
in different directions and so um I I
think that's really not a very
productive
well yeah you discover some things long
way but if you really if you're really
serious about getting somewhere
it's it's good to think seriously about
where that is you want to get to start
with before you start trying to hit off
in different directions and you're not
even sure when you arrived here we think
you've arrived or whatever and at least
all kinds of problems one of the
problems that you've probably heard
about is this shifting goalpost people
will say ok playing chess well you know
that was going to require real
intelligence then it's sort of solved in
a brute force way like oh maybe it
doesn't really require intelligence
after all I'm playing God that's going
to require intelligence in the last few
years has been an enormous amount of
progress in going playing systems they
can now compete with the low level
professional players and so people say
thing or maybe go isn't really
you know I think that's a symptom of
this problem of what actually having a
fear idea where we want to get to that
thinking that well when we arrive
eventually if there are random sort of
search where love will not get there
so I'm on the third group and it's I
think it's summarized by by this sort of
quote basically what we want to do is if
we're serious about the AI and AI we're
going to think seriously about what this
thing is what it is what it isn't and
try to make it a bit more concrete but
it would even just make it more explicit
exactly what the different opinions are
inside and almost nobody does this
almost nobody does this I I published a
paper I put together the the biggest
collection of definitions of
intelligence that as far as I know if
anyone's gonna put together I also put
together the only which i think is
remarkable the only review of measures
of machine intelligence do you think
about that this is you know this whole
field artificial intelligence nobody has
ever done a review of measures of
machine intelligence as far as I do you
mean please tell me if somebody else has
done it for it so I don't know I've
nobody's ever done there that's
ridiculous if you want to be a science
you need to define things and you've
gotta meet your things otherwise you
don't even know if you're making
progress
you don't know you try to get to so if
enemies looking for a topic for the
thesis that they're writing it's a
difficult thing to work on because you
tend to get ignored the people in the
area a lot of them either this group
they're like oh yeah we're not too
worried about what intelligence is but
at least they're honest they know
they're not trying to do well then this
group is like oh we don't really care
about funny tell them since we just
we're gonna know when we get here and so
you generally ignored so my review of
meters of machine intelligence as far as
I know has never been signed by any
people the people is not interested in
this and I think that's fundamentally
wrong if you want to be a scientist so
but there are a few people with write
papers that say hey we need to find it
and this is this is one example here
right so what is intelligence
now I'm gonna present this a little bit
backwards in a way I'm gonna proceed my
results based and they're not gonna
present some of the supply evidence for
it and the reason for that is that it's
easier to judge some of the supporting
evidence if you already know where I had
rocketed need it that way so so what I
did as I said before as I I put put
together this big collection of
definitions of intelligence from
organizations from psychologists from
artificial intelligence researchers and
I went looking through all these
definitions looking at the analyses and
it seemed a bit discouraging to start
with but actually as I as I pro priests
I found that there were clusters of
definitions and some of these groups
were much larger than others and within
the group often there'd be the same
ideas expressed again and again either a
different language or different levels
of abstraction and they were actually a
lot of similarities particularly within
what I think is the main kind of cluster
and so what I tried to do based on my
review of all these definitions is put
together a very general and simple but
also fairly mainstream view on what
intelligence is and use this as my
informal definition to start with okay
so the first thing is that intelligence
is a property of some active agent the a
you have you have you have something and
enter
the world some sort of environmental
song and when you're talking about
intelligence you're talking about a
property office that's agent and that's
fairly universal across definitions the
agent has to interact with an
environment it doesn't really make sense
to have think of the intelligence of a
system that doesn't has no external
interaction and and the the students
often a lot of definitions as you'll see
as well it is to do with the dynamics of
this interaction and what what goes on
their intelligence as a matter of degree
we often when we speak and we say
something is intelligent or isn't
intelligent but really it's it becomes
really clear that it's a matter of
degree there's something some more
intelligence is some kind of scale
they're another property which is
miscellaneous obvious is that the
intelligences are related to the agent
success and achieving goals there's
nothing the agent is just sort of
drifting around and doing whatever but
there's some tangible scenes of success
or failure and intelligence is to do
with the aegis ability to make decisions
to lead to achieving some sort of goals
on and this also comes through in
particular definitions missing
and finally the environment is not fully
known to the agent so the agent has to
be adaptable it has to learn it has to
adapt it has to adjust itself it has to
come up with invent new strategies has
to be creative it has to have all these
things in order to be able to deal with
the fact that it doesn't know what the
environment is to start with it doesn't
know what problems that could come up
against and so it has to be able to deal
with a wide range of I would I call
environments that they're actually right
around in to the problems and situations
that may encounter
it's operation so my informal definition
then is intelligence measures an agent's
ability to achieve a wide range of range
of goals in a wide range of environments
okay so it's a very general ability to
achieve stuff to make decisions based on
their experience in order to achieve the
types of things you want to achieve and
this is a I believe this captures the
essence and an abstract level a lot of
the definitions we'll see I'll show you
a sample in a minute and it's also
something that you can try to formalize
and I'll do that later we can formalize
what an agent is we can formalize more
goals environments are we can formalize
what wide range of environments are and
so on yes so it's it's it's an ability
on average okay so if you generally file
at every problem you encounter because
you're unable to make good decisions
okay so this is a this is a slightly
there's a point that trips up people we
coming from a reinforcement learning
background we use slightly unusual
terminology here and when I you you're
under this here you run for this problem
of okay this is a few things going on
one is that our goals are actually going
to be part of our environment and
usually that makes it mathematically
easier
with everything because we want to
consider this whole range of
environments and all the different
problems you may want to achieve in the
room but that sort of makes sense from a
reinforcement learning sort of
standpoint but actually if you you know
if you physically build a robot or
something then the robot is going to
have to interpret its environment and
it's going to have its goal sort of
inside itself right and it's going to
actually decide what constitutes success
or failure these sorts of things now you
can have you could have a model where
the agent is able to have access to its
own goals and define its own goals you
need to be quite careful there because
you can have things come unstuck quite
easily so you can say well how I mean
you can give the sort of Zen Buddhist
kind of situation where well how am I
going to be really successful I know I
just define my ultimate goal is doing
nothing Wow I'm incredibly successful so
if you're able to just arbitrarily
define your own goals then you can you
know so when we we wouldn't want to our
measure the intelligence an agent what
we actually need to do is we need to
constrain it with some goal so you'll be
given an IQ test or you'll be giving
some problem to solve and what we want
is we want you to optimize towards that
goal so we can measure your performance
if we can't do that we basically can't
measure how evil you are because you
could always what about the intelligence
of the ghosts
well we're going to consider all the
gauls sir
we're gonna do the whole space or and
I'm going to define that a little bit
more precise and soon so it's a very
very general thing we're going to not
going to exclude anything more or less
I'm gonna gloss over a few the detail I
should also say that at some points here
I'm gonna say things which are not quite
technically true because the full
technical details are actually get quite
complicated so I want to say things like
probability distribution over sequences
and really I'm talking about a law semi
computable semi major defined over some
of the seats on alright so I just want
to I just want to say that up front
somewhere that you know it's not that
I'm in order to get the idea across I
need a sort of glossing over a few
things and they're not in and there are
all in the books you can read all the
kingdom of details and but for a
presentation you just get lost in all
kinds of details that don't really
matter
well waters yeah what is perform well I
mean you have some environment yeah okay
sorry
now let's have a look oh why yes a
little bit more if you want more ask me
and I'll I can talk for ages about like
I put you the Franklin's and everything
whatever you want okay so first of all
just get this in mind before we start
looking at a few definitions I just want
to sort of support the fact that this is
more of this inaudible off the wall
definition bias David IP is intelligence
mean she's an agent's ability to
achievable achieve goals of wide range
of environments but things twice the
goals things raised and Varmus general
sort of ability to achieve
okay so here's everyone some definitions
from psychologists which we use with
intelligence to mean the ability of an
organism to solve problems so you've got
a few things coming out here you've got
the organism so you've got some kind of
agent and you've got solving new
problems and so it doesn't know
presumably doesn't know what the
problems are in advance if the new
problems right so it has to be able to
deal with some wide range of
possibilities or some reasonably wide
range of possible
and then by the fact that it's got the
ability to solver this there has to be
some notion of what success is in the
situation right thank you
intelligence is part of the internal
environment that shows through the
interface between person and external
environment as a function of cognitive
tasks commands and so here we see the
important point that you you have an
agent in an environment and it's at the
interface of the interaction between the
two that we can actually meet and you
and you're saying that we need to have
an AI agent an environment is a broken
up before it's all and you've got some
sort of path to the miles which are
called in the nature so these are again
the problems the person possesses
intelligence insofar as he has learned
or can learn to adjust himself to his
environment so again you've got the
notion that's a matter of degree here
because these are all of course
definitions for people and I'm not
really so interested in definitions for
people I really wanted a definition for
machines whenever they say people I want
to sort of put agent and then learn to
adjust themselves to a virus there's
some notion of adaption here to some
environment but has to learn and adapt
to it then it doesn't fully know what
else to speak okay um it's a global
concept that involves an individual's
ability to act purposefully think
rationally and deal effectively with the
environment so again you've got this
interaction with the environment you go
through the notion of dealing
effectively so there's some idea of
success not failure the environment will
speak some kind of our goal here the
cluster of cognitive abilities that lead
to successful adaption to a wide range
of environments wide range of
environments
excellent and now here they talk about
it's a little bit different here so this
is an idea this is an example of how and
why definition
a little more abstract than some other
definitions and they talk about a
cluster of cognitive abilities now I
don't view intelligence as a cluster of
cognitive abilities my review I view it
as the result of some cognitive
abilities and the result is your ability
to successfully do stuff right and so
you might get planning you might have
the ability to categorize you might have
the ability to recognize recognize will
protect or do all sorts of things humans
may have some of these beliefs there may
be cognitive abilities that humans don't
even have the machines in the future
will have and so intelligence is not
actually these cognitive abilities it's
actually the result for youth abilities
see what I mean so my son seems my
definition is a bit more abstract okay
but in the lace it's is quite similar in
flavor and it's got the wide range of
environments which is cool so I now
switch over to some AI researcher
definitions Kalman systems are expected
to work work well so we've got the we've
got some concept of 60s here and many
environments so here we've got our wide
range of environments and then the
property intelligent allows them to
maximize probability of success there's
some there's some notion of some goals
or something some seen some with I
succeed or fail when I speak to that
goal so you go coming out again doing
well in a broad range of tasks is
imperative intelligence right take
appropriately in an uncertain
environment again it doesn't know
exactly
is we appropriate actions that increases
the probability of success it's
excessive the achievement of behavioral
sub-goals that supports us an ultimate
goal similar thing again mean system
that generates a definite behavior to
meet goals or a wide range and a range
of environments if you intelligent so
hopefully you believe me now that while
you may not agree with my particular
definition of intelligence at least it's
not completely off-base with what a lot
of people are saying about this
it's an agent's ability to achieve goals
or the wide range of environments okay
so if we have at least an informal
concept of intelligence now we've got
some idea we want to get to this sort of
clarify data back a bit and I'm gonna
clarify this more later no we don't have
any practical machine that actually
satisfies this definition so what are we
gonna do well we could start trying to
build a machine that can do that and
that's really hard one possibility is to
try to theoretically study the problem
and say okay we're going to normal
computational cost and we're gonna try
to come up with a theoretical machine
which was intelligent now a lot of
people object with us a lot people say
ah but if you're ignoring computational
cost you're basically throwing the you
you're throwing out the essence of the
problem and yeah I I accept that the the
computational cost is obviously very
very important here but nevertheless
prior to this work by hooter nobody here
they eventually eventually even define
such a system ignoring computational
cost and maybe and I think the answer is
there to some extent we have succeeded
maybe if you do study such a system and
you formally define it as something
you're no longer hand waving hands
around you knows your proofs talked
about this you can learn something about
the problem and then once you have a
theoretical machine and and it meets
your criteria for intelligence
you can then think about okay we
actually had something very explicit now
we have
equations how can we try to approximate
this in an intractable way and I think
that's possibly progress and history
will tell me to not be progressed a lot
but at least we've formalized that we've
actually seen explicitly what we mean to
you and so there's some people are never
going to convince about this some people
really think that if you're doing this
theory stuff with your boring gut
computable cost in your way to find it
so anyway this is what we're going to do
we get a new computational cost and then
we're going to try to see if we can come
up with a theoretical solution that's
wrong okay so we're going to begin with
inductive inference here we have a nice
sequence for you one three five seven
what comes next
no we have a vote for nine oh I hope
most of you speak that's like the
grecian why do you think it's not one
reason you might think it's line well
the standard sort of explanation is that
you've used principle called Occam's
razor and you've looked at the sequence
and perhaps you recognize that there are
other possible explanations for the
sequence but the simplest explanation is
that you start with one you said two
each time and the simplest explanation
is consistent with what you've seen
generally seems to be the most likely
thing right and this is the
philosophical principle of Occam's razor
and you've all used it and in fact an
intelligence tests they expect you to
use all tenderizer don't give you
sequences and you could invent some
reason why it's the mother answer some
complicated explanation but they design
it so that there is a significantly
simpler explanation which which will
give you the quote unquote right answer
and certain intelligence tests they
actually expect you to use all Kenter
answer and we're going to return to this
later it's not usually in a definition
intelligence but I argue that if you
really want to fool lies that you
actually need it and that's why Tunes
open intelligence this so yeah it's a
what nine because you've used Occam's
razor and the simplest explanation is to
n minus one okay now you're all wrong
the answer is actually fifty city and
the reason why is the distinction
generated by the sequence here this this
is the generator okay no you can't be
expected to get that right but the
intelligent thing to do is to use all
kids razor and most likely it was
probably going to be this but you're not
cool you don't actually know and so this
highlights a secret important principle
and that's that there's actually in
enormous space of possibilities there's
an enormous space of different
explanations for things that you've seen
and you can't throw any of them out if I
consistent with what you've seen if
they're inconsistent with what you've
seen them sure they can't be right
there's some point you've seen is
impossible under this this this is a
hypothesis about the world but you can't
draw white ones they are consistent with
what you've seen even if they seem quite
unlikely so this is another important
philosophical principle now it's quite
obvious so we want to have a very wide
range of possible when we doing
inductive interest you want to start
with a very wide range of possible
explanations we don't want to throw out
anything that's inconsistent with what
we've seen and we want to use walk
handraiser in judging which which are
the most likely things given what we've
seen okay so the third the second
principle I came by I explained what is
the interest principle of multiple
explanations and that's keep all
hypotheses that are consistent with the
data and if we're really serious about
all hypotheses there should be an
enormous space okay but if you keep up a
lot of hypotheses that are consistent
with a data you know why we
the possibilities all kinds of crazy
explanations so how you going to make
sense of this how you're going to make
any useful predictions well you use all
kinds razor and then among the
hypotheses consistent with the data the
simplest is the most likely okay and
then if you want to formalize some of
this process a bit more we come into
Bayes rule because the first in my
regions here and essentially what this
is saying here is that the probability
of some hypothesis about the world given
the data you've observed is equal to the
probability of that data given the
hypothesis times the prior probability
of the hypothesis and then there's some
normalize attempt so we just ignored us
down here and so this is useful because
it's not too hard often to figure out
how likely is it that I've seen
something if this is actually what's
really going on in the world okay and if
you can work this out and you have some
idea of how likely this hypothesis was
to start with then you can figure out
how likely your hypothesis is about the
world and so you've done an inductive
inference you've gone from your data
what you've seen to state and a
distribution now over different
statements about the world now the
really problematic thing here lists this
part here often isn't too bad one of the
really problematic things is where does
this thing come from this is the
probability of some explanation of the
world before you've seen any data for
how likely are different things before
you've seen any data and essentially
Occam's razor tells you something about
this did I use that among all the
hypotheses consistent with the data well
here it's any data is nothing I think a
conditional the simplest is the most
likely so if we were to build a good
prior here and we believe in Occam's
razor what we want to do is we want to
have the prior probability of different
explanations about the world in
proportion to how complex they are
several explanations are more likely
more complex explanations
at least likely okay so in terms of
Bayes rule if a currency is that the
probability is our prior probability
should be above zero for all H so we're
considering to start with this huge
range of hypotheses and then H bonds are
some very large seed we don't want to
exclude anything before you win them
again and then I can say is that the
probability that I pollicis depends on
the complexity of H so what we need to
do if we want to really formalize this
is we want to say well this very large
set is and we want to come up with some
measure of the complexity of the
hypotheses right and then we can
actually formally we can formalize both
of these these philosophical principles
and come up with some sort of inductive
learning system that does that ok so
what can you use the something called
polar gorram complexity and turns out a
comb-over completes in this case is a
little bit little bit involved it's
describing my thesis takes about a page
but the intuition is pretty simple and
that that's sufficient for us
essentially the chroma bar of complexity
of something is the length of its
shortest description and the intuition
is quite simple so let's say you have a
sequence of a trillion zeros all right a
very short description of that is a
trillion zeroes right and you can
describe it in terms of a computer
program for I equals 1 to a trillion
print 0 and so it has a very short
program and that program is an effective
description it's a computable
description and so what we if you want
to describe something as much much more
complex say or even a little bit more
come when someone went PI the program is
quite a bit bigger to generate the
infinite sequence of digits of pi and
then there are there are much much more
complicated things where they're only
very light program to describe them so
the intuition is that simple things are
things that you can describe very
compactly
they have short descriptions complex
things only have long descriptions they
don't have some descriptions that's your
intuition ok
between complexity is all to do with the
description how how short is the
description of defender and so here we
say that the combo complexity of some
there's going to be a probability
distribution over sequences there's a
length of the short program and so to go
back to our example here we have the
sequence before to the minus 1 this is
what you all suspected music guitar
music it's razor it has a virtual
program so there's low complexity the
Sierras are going to have a longer
program it's good hands higher
complexity that's the intuition so if
you believe in all kinds raise up this
high complexity low probability
oh sorry low complexity high probability
high complexity local
ok right so this is this is the
universal prior probability of in
hypothesis what we do is the
distribution is we have a the length of
a program that is computed by a
universal Turing machine which takes
bits as input and so with each bit from
probability hearts all right for the
length of the program this is why we get
to the power of negative so that's just
1/2 and then it just it just multiplies
up so the intuition is that the more
complex the explanation about the world
is the lower the probability and vice
versa ok so this your intuition and that
that is off hands razor divided and we
take the space of all the hypotheses
here to be essentially all distributions
and sequences this is an enormous space
we actually even doing more debt on that
you know we're getting today so this
prior speaks of a curious rule because
well firstly this is an enormous space
here it's all the different all the
different distributions yes
so yeah there is some you have to
normalize ability it depends how you
define some things it if you use soap or
prefix prefix-free universal Turing
machines in the prefix free property on
the links and all the program means
means that you can boundless using craft
and equality and then okay so yeah there
is respective occurs for all because any
anything that's going to get a home
anything else with a finite that's
goodness is computable distribution it's
gonna be fine out here so here's some
possibility so that satisfied and a
formalized Occam's razor we've got the
complexity is proportional inversely
proportional to the probability okay so
therefore wise we've come up with a
prior that captures the appearance rule
and offends razor now as a little bit of
technicality here if you want to
actually predict over sequences we need
to actually consider all the different
hypotheses and then the probability of
the sequence you've observed so far and
then then this little Mexican will walk
okay so if you use the probability
theory this is this is an expected thing
to do right if you consider all the
different it's possible explanations for
what you've seen you have to wait
according to how probable they think
when you've seen this is you know
hopefully you think that installations
if you don't follow that go so here's
they are see simple it's Marcus's
favorite Greek leafy and this is gonna
this plays an enormous role this
distribution has some incredible
properties and very very special
properties now what we're going to do is
we're going to try to predict sequences
using this C distribution okay and this
is called solomonoff reduction and so
what happens is we have some sequence
amigo we've seen all these digits coming
along and it's coming from some
distribution that we don't know that's
the whole point we don't know what's
generating the thing we're trying to
infer what
and so the notation here is that we have
Omega that's 1 2 n so that's the digit
first digit up to the nth digit that's
what we've observed time in and we're
trying to predict what the next the next
business sequence okay and so the way to
do that is just basic probability theory
we just take the conditional probability
given given what we observed probability
that makes a bit of sense 0 we just said
it we just take that just an appropriate
probability conditional problem right so
it's the probability of what we observed
with 0 divided or Molloy's run properly
what we do simple as that
how well does this work turns out it
works ridiculously well just insanely
great this is really insanely great for
any really any unknown computable
distribution and this could be anything
this can be quantum mechanics is a
computable theory right it's Turing
computable safety update equations all
these things that we set relative
Newton's laws of physics you can put
this in and in this point any computable
distribution over anything you observe
so this is some massive space this is
absolutely gigantic right and for any of
these any of these possible hypotheses
about the world you don't even know what
they are start with the expected total
error but I won't define exactly what
that means but it's basically the the
deviation between the expected deviation
between if you actually knew the creep
down what this is if you're using this
as a predictor and you can of course
over the infinitely Internet's over all
the predictions for the rest of infinity
it's bounded by a constant there's a
constant amount of error you're going to
make no matter what it is that's
generating the distribution and you look
at this constant that is the length of
the shortest description of the actual
generating mechanism this thing is
learning almost as fast as if you just
told it the answer to start with it's
ridiculous
there so if you're if you read about
this one of the things people
about this the coma bar of complexity
depends a bit on the choice of reference
machine it depends on the language
you're using but it's bounded by a
constant there and so even if you change
the language around you're just putting
a constant number of bits in here maybe
a thousand bits or something to go
between different turing machines and so
if you actually plug this in to say
video feed and it takes a thousand bits
more to converge come on
this converges for anything you see in
the cache they catch it only works in
theory because speed is not computable
if it was computable we're probably
trying to predict the stock market and
becoming fabulously wealthy well okay so
when you say then what you're saying is
that essentially the complexity of
something is not just a function of its
lien for this a function of this
computation time and that is that's a
perfectly valid way to view complexity
and it's not the way comma graph
completes the abuser commonground
completes of these years I mean for
example a little also quantum mechanics
right you can write them down some
things are quite simple right in CERN
all came to raise the seats maybe that's
a good explanation on the other hand
actually computing what's going on and a
complicated system using them is
incredibly intractable using a classic
so as fear enough you you know I I agree
with you you may want to use different
notions of simplicity then just it's I
think it's you can quite recently I did
a different sign way
now what is the function that generated
it right above it very different right
one thing and one would take a lot more
computation of time yeah which is a very
very short for figuring out the program
is sure so that's it's a perfectly
reasonable perspective to take and it's
an alternative perspective I think the
trick here is that you're using
something that's not computable so it
does all the computations in parallel
every possible fractal
yeah the graphic what fun in a sense
that's already another than it is every
time time looking at all the implausible
things and so it's not going to take any
more than the number of steps that it
needs to get the definition anyway just
because it's looking at everything and
it's actually if you if you are
concerned about the compute time is
being part of the definition of
complexity they actually kind of helps
you in a way because it means they're
working out the complexity of something
is now bounded in time because you're
not interested in all the explanations
that take forever to run the bad news is
if you contrived formalizing a
definition of complexity like Homer of
complexity with time in there it gets a
bit difficult your relative problems
leaving complexity is one and we're
gonna run to Henderson
okay so yeah catch not forgettable and
this is why we're ignore compute compute
time but we had this optimal predictor
okay okay so some people say well
someone in action has nothing to do with
reality
well if you the approach we take here is
that you define something in theory and
then you look at how you can approximate
it right so we've got a concrete target
have approximated
and it turns out the slow-mo production
that it's this ultimate prediction
scheme and if you break it in certain
ways to make it a bit more tractable you
actually get a hold on statuses
things come again so we get map
estimators in your mail estimation and
then a description wind makes an entropy
so one view of solomonoff induction is
that it's sort of a model of ideal
inductive inference when you're ignoring
computation cost and then in practice
what we have to do is we have to try to
approximate really the on each point I
want to make here is that even though
we're ignoring computational cost it's
not completely disconnected from the
world of statistics it's it's actually
you can do equations make some
simplifications okay and if you want to
know more about that that's every
Thursday and the kholghoor publicly in
general and say okay so what are done so
far is we've come up with this perfect
predictor okay Vivek there it will
predict that but we will move them there
that definition of intelligence required
an agent which interacted with an
environment right so the engine can't
just sit there and observe the world and
continually figure out what's going on
predict what's going on it has to
actually make actions in order to
achieve some goal design and so what we
need more than just prediction yeah we
need to have an active agent and so it's
the other way of doing this sort of
thing in in the area is called
reinforcement so basically we have an
agent we have an environment
the agent can perform actions that
affect the environment and then there's
rewards come back from the environment
or yeah more realistically if the agent
has some some parts so part of that
which it can modify which decides
whether like what is a reward or not but
theoretically this is a easier nice
thing I work with and then we have these
other things with just observations so
that's to some patients what's going on
the handler
and this is standard setup reinforcement
learning okay
and so the agents goal essentially it's
its internal goal is to maximize another
little widget
okay now this produces some interaction
history over time the ancient action the
environment returns of observation of
reward the cycle continues and so I
formalize this as far as they the aides
of the action so as the observations are
the rewards goes along over time up to
say time T and then because I'm going to
have to deal with some complicated
combinations of sequence little sign I
need a little bit of notation here so
what I do is I stick stick these three
symbols together and then index them 1 2
T so this whole sequence here is a RR
all of them under people ok ok so we
take an agent to be a probability
measure over actions conditional in
history so basically the agent has
everything that's observed so far all
the all the actions are taken all the
observations all rewards and then
there's some probability distribution
over what its action is going to be ok
so this is a very general agent if it's
deterministic only one of these things
will have a probability of 1 everything
else will be 0 and this is a more
compact notation here it's just the same
thing so this is a very gentle agent
we're nice unit is computable it could
be working by magic so long as it
defines a distribution it can be
stochastic because it's a distribution
so it's a very very general class and
it's conditional the entire history so
we're not making any markov assumptions
or anything here this is very rich in
and we want to keep it that way
the environment we just recently do the
same thing but it's the other side of
the interaction so we have a
distribution over what the next
observation would are given the entire
history so far ok and the agents goal is
to maximize total expected reward and so
this is the V function here so this is
the interaction between agent I
environment moon and the expected amount
of award
gates is the expectation okay so this is
a very very general framework let me
emphasize that you can put just about
anything you think of it yet you know
playing chess you could you're one
player you've got another player the
other players environment your actions
are moving your different pieces the you
observe the moves of the of the other
player and maybe you get a reward for
winning the game otherwise you don't get
anything
another possibility is you might get
rewards as you take pieces if you take
the opponent's queen you get a positive
you lose your queen you get a negative
reward you if you're if you lose the
whole game you get a big negative reward
okay going through a maze answering
questions an IQ test you have down to
the questions you observe the observes
the test you get rewards for doing well
passing the Turing test you he reward if
you pass we have to interact with you
know somebody on the other end you
trying to figure out with your machine
or a human riding an award-winning
book you do anything you think up you
put under this conference so it's a very
very general and so we have to think
about what is our opponent likely to do
we have to consider in the space of all
the different things the opponent might
do and predict what it's likely to do or
not do and so if if I was playing chess
against Garry Kasparov Garry Kasparov
would probably make moves that were not
optimal if he was playing a grandmaster
because he knows that I'm a terrible
chess player he can make a move and
assume that if there's a slightly risky
move for him I will miss some subtle
reply to it and then he can crush me
right but he wouldn't do that move with
a grandmaster because he know I was the
Grand Master we'll see what's coming up
and so how do you play the game depends
on who you're playing against if you
really want to play optimal
yeah if he wants to he wants to destroy
me quickly if he wants to be really sure
to destroy me
inhumane you can see that what's going
on here is that you have to consider
water boobs then assuming that any of
those moves are taken you then consider
one of the likely things your opponent
is going to do any for each of those
possibilities you have to then say well
now it's my turn
what am I going to do then you look at
the best move and then you consider what
a symphonia gonna do and you have this
big tree of possibilities and you sort
of mentally go through this and you give
me the point Cheers
so we have maybe something like this in
their present situation when you take
the poured with the Queen in your check
we lose our queen so this this tip here
is what we do this next step and there's
next level as what our poni does and
then the next step of what we do and it
interacts all the way through okay and
so we can assume we're free to choose
our own boobs so we can assume when
we're choosing our own booze will choose
the best move
as far as we can tell so for this deep
and for this depth our out turns we can
we can choose our own moves so this week
we'd have maximum we take the Beast when
we're looking at these other steps here
we have to consider how likely it is
that our ponent is going to do different
things in response okay and so we have
to we have to know the probability of
the agent doing yeah environment doing
different plants we have to consider the
consequences on them and as we go
through this tree their student may be
rewards as we go through so say going
down here Lucas a queen with it's pretty
bad we take a night baby who returned so
that sort of pile of corn sex it's still
pretty bad we get down to a chick made
here and that's that's really this the
ultimate and so we have to optimize over
this whole space of possibilities so how
do we do that we have this big nasty
equation and this big nasty equation is
reason this is we're only one step away
from the
actually agent noh feel with me we're
getting there this big nasty equation
just is what I just said up here
in someone's so we have as I see if we
have our own actions these are the A's
here and we make some ways over these
because we can choose our own actions so
we make the best action we can make so
that's a maximization and then what we
have to do is we have to consider we
have the sequence of our action the
environment our action environment our
action and so on and so on so we have
the maximization over our current action
of time T then we have we have to take
the sum over all the different things
the environment can do is there are
observations rewards and we have to
arrange them according to the
probability this is the environment here
the probability that the environment
actually does this particular thing so
that's the probability of the other
player taking your queen in you do
something selling well the probability
give you a lot of the other player is
ignoring that so we have to wait all
these possibilities and so we go through
this tree our action we take the best
action we can we've got all the
different possibilities that the agent
will reply with at time T then we have
our our action at time T plus 1 our next
action and we have the environment and
so on and so on and so on
and we have to wait all this problem all
these according to how likely we think
the environment is to do all these
different things in response to their
moves and then we have here all the
different rewards so actually taking
interpretation of all the rewards this
is a probability this is what we're
taking expectation of we have to have
this whole tree of possibilities
maximizing our actions taking the
expectation with respect the
distribution of the rewards for the
ancient outside so this is really just a
formalization of this and you can see
the tree structure coming here they
alternate our move environment our move
environment a balloon environment we're
trying to come up with the maximum
amount of rewards and this is a model of
how the opponent behaves ok so it looks
pretty scary it's actually conceptually
not that
thank you there's a big big problem with
this in general if we would have be able
to remember our definition of
intelligence as the ability to perform
well in a wide range of environments
now this performs well this performs
optimally when we know what our
environment is right so that's not good
enough
we don't know general know what our
environment is if we know what our
varmint is we can converse computer and
come up with a solution so that's one
problem that we don't know what our
environment is the other problem is that
this is of course very very difficult
computer it's a neat eventually large
tree and we're taking all the way out to
infinity but we're ignoring
computational cost we'll get back to
this data so theoretically anyway if we
ignore the computational cost of this
huge tree as we look through ideally all
possibilities in the tree um we don't
know what the environment is but we get
the lumber off conduction this here is
actually just a prediction of what the
environment is going to do right we've
stolen off production which converges
insanely well for anything
so what we can do is we could say well
we don't know the 'true environment how
about we drop them so warming off
predictor in there right so we replace
the Terr environment with see that blues
market is like here and this is what we
get this is the same equation and now
we're going to see in here so what it's
doing is we don't know exactly what the
environment is going to do we use so
long run off induction to predict what
it's going to do to all the different
futures okay and that is a o HC you
manage to be with me that's the basic
model of general superintelligence
ignoring computation and you can prove
that it converges to optimal behavior in
any environment where this is possible
for a general agent and there are some
environments where it is impossible so
for example let's say you have an
environment you have two doors they're
unmarked you go through one boy you can
never come back
or both the words you can
come back to me either of them one
thanks we - given one tension to help
what do you do you have to go through
one of them no matter how smart you are
you council was born
you just have to kiss right so you can't
actually have the optimal behavior in
this environment at least you know in
advance which board you have to take
because there's nothing you can do to
find out about it so we have to have
this sort of caveat yet where it's then
any environment where this is possible
for a general agent and turns out there
a medium bar that's good news so one of
the things I did what this was to try to
spell out or they drew out how the
unfamiliar results by the way this great
walkability repellents great well it's a
whole bunch of different things so one
of things I try to do my thesis is to
spell it out more exactly it's a thing
that it's not like a binary thing like
if you give where you died you can
actually do a feasible nasal on your
environment
if you have if you have one chance in
one world we have four goes through the
door so you're getting big that's the
kind of beat you have better repeatable
measure so you can actually got yeah
kind of
it gets a little bit subtle so it's
basically
Reviver people it's technically you've
got it sits in a going to compartment
and so if you're in libraries a go to
sort of basically means what are the
mistakes you make you can do something
that sort of get you back to before you
make these things so that's that's an
important concept community so and you
can see that this sort of it looks like
it basically satisfies our own formal
definition of intelligence we want to be
able to achieve a wide range of goals
and librarians requirements and this
converges the optimal in any place where
it's possible for ginger so it satisfies
that definition of intelligence in
theory so I wanted to sort of flesh this
out a bit
I won't tell you details of who did this
how I did this um but basically I it's
in the chapter I formally define all
these different things are we have
Markov chain to go to give me peace this
is a voting thing you're sort of
alluding to there Markov chains
bernoulli schemes classification
problems banded problems
bla bla bla bla computable sequence
prediction function there's a whole
bunch of stuff and this is this is a
taxonomy so as you go down this becomes
more and more specific some of the
things out the top here are just too
general but basically below some level
here everything in the space that's
possible under the definitions like get
for a general agent to convince optimal
and we know ahc converges in any of
these situations and these are whole
classes of problems isn't a specific
font is entire domains problems so I
would argue them if you exceed the
definition of intelligence the ability
to a well of wider in environments I
actually performed optimal in all these
environments
it turns satisfied
okay now the computability problem
look at you and it's it is bad for two
reasons it seems like one
this is just all computable rule it's
pretty bad and the other thing is that
this here is exponential it's the whole
tree I drew earlier on right it grows
exponentially so you've got two poles to
deal with one is you need to replace the
long run off conduction with something
as if you're a real predictor 2 you're
going to be exponentially large surge
now people um find it's all these kinds
of problems people trying to deal with
these suits prices when they break she's
playing program didn't go to playing
programs this right they try to deal
with this kind of and people try to
build predictors as well well there's
like Javanese and he has a background in
making she's playing programs and go
playing programs be quite successful at
it and he's now been supervised by
Marcus we - among others in Australia
and he did obvious thing you took the
speak to make Street laughing yeah
and he used multicolored research like
they use in Ghana and he took this alone
off predicta they replaced it with the
clear choice with context rewriting and
you have to try to complexity weighting
as well
will the complexity of the moral because
we won the small camps raiser to come in
right and you can you can rewrite this
equation in a slightly different way and
then if you actually rewrite how what
this is it almost looks no identical
it's all true search means you don't do
everything that you do a random
selection of dad comes in you think
that's estimate and you so you take
these samples for your tree and you
didn't try to put confidence intervals
and they're different points on the tree
of how good or bad things are and you
then adjust which parts you're going to
look at
and says a whole bunch of work has been
done to there in chess playing program
and don't playing for Amazon to try to
intelligently search through these
possibilities and so you can you we can
use existing technology and we can come
up with this morning Callaway XC this is
a reflex tear paper and you give
something that actually does things so
you can do simple prediction problems
well that's not really surprising it's
got a predictive built into it it can
move my tic-tac-toe I can play Paper
Scissors rock okay I'm going to find its
way through mazes where it could own see
locally so I can't see the whole maze
and it says so corridor it doesn't do it
has to actually walk down the corridor a
mentally count how far it's gone in
order to be able to know what your
decision to make at some point it
doesn't actually know that absolute
location we're going to reinforcement
learning environment so when it gets to
the end of the maze against reward then
it's like a little maze of measures yeah
it gets it it gets to multiple
experiences so and so the technique to
talk is to play the game with all times
various types of tiger games so these
are games where you're in front of a
door and you can listen and you can then
hear or the tiger sometimes and it's
probabilistic so you don't you know once
you hear it you're not completely sure
where it came from that gives you a bit
of information and so you have to figure
out how long to wait and how many times
to listen before you make the decision
which door to go through you don't get
eaten by the tiger so there's a slightly
subtle sense of problems we have to sort
of optimize multiple different things
and computer games you can learn to play
pac-man
this is really for deterministic systems
no and I came back to the stock market
it's not for deterministic systems these
are all probability distributions so
Paper Scissors rock is not deterministic
you're gonna prediction problems where
some stochastic sequence with other
biases okay it's all one generalize over
distributions no where are these
particular ones no something we'll
probably please the partially observable
yeah so this is a pom VP so it's not
assuming that Khan and the general model
by RC doesn't like people asking how it
came to its decision unlike the grandmas
why did you come solution well it
doesn't
yeah it doesn't have any language where
you can internally look at it the model
you can internally say okay here's the
tree that you looked at these are the
these are the probabilities you think
different things are going to happen say
you when the Chiefs example and so you
think that the apply the player is
really good and so they won't miss this
this obvious move and so this is a
combination of moves that leads to a
good outcome so you can and speak
sure so you can are well if the reward
is not this refinery probabilistic
distribution is article yeah it would
optimize the average so I didn't make
you do this work so I don't actually
notice try that
we're with the presidents of Lima
there's been previous working with I
exceed and you can show that it was kind
enough to see okay so only small
community is concentrated on general
intelligence nobody has tried to make a
thinking machine in the in t chip cheese
that's the key thing bottom line is we
haven't really made that much better
quote from Marvin Minsky now the thing
here is that this iooks multicolor
galaxy lumens to play all these games
you don't you don't hey we don't have
team different Monte Carlo AI exceeds
that we play with different games with
you can make up a game of this sort of
complexity just invent one and give it
to it come back the next morning in this
room while your game right so it's quite
general it has a number of other very
interesting properties one of them is
that it's so-called embarrassingly
parallel because just using Monte Carlo
tree search you can farm this out across
tons of students and break up the the
tree sits right so if you get a
supercomputer or you're going to access
to Google's plus four or something like
this you could farm this hour pretty
efficiently and it would do massive soon
spices right so it's quite easy to to
make this code work in parallel Thanks
currently going on oh I should also say
about teaching my chest my currently
trying to get to my chickens and then if
I can get chickens working they're gonna
try cheese so I don't know if Marvin
Minsky within that at least that's what
they're doing they're actually making
system and trying to teach a chick
that's for teaching cheese it starts
with just a general compressor and you
can be inside the wall okay so you can
like say with brain machines and so yeah
it's embarrassingly parallel you
paralyze across a mix of hardware in the
future when this zilean the course of
use it's gonna work
it's an anytime algorithm and what that
means is that you can run it for a fixed
amount of time you've got three seekers
and it will then return the best answer
has managed to come up within three
seconds and you see this in garden
playing which is playing the older
person has this property and that's a
really nice property to a scale and
system up and you can tell it that it
has to run within a time constraint and
it just wants the best it can within
that constraint if you give it more time
obviously if you're searching there just
adaptively building as it goes and then
you just come off you say climbs up and
it just returns that these funds were
here
the point is it returns a sensible on
some given time constraints is the price
of some algorithms you're gonna run the
algorithm for a minute and a half before
it returns and outside if you need to
answer in half a minute you haven't gone
outside yeah
and another cool thing is that it's
using this Monte Carlo sorry it's using
this context tree waiting and you can
play around with Reese
the convict straight wedding is Loomis
limited things into a couple hierarchies
of sequences in the closet
things like that you can play around
with the priesthood and other things
than the more logical rules on there so
you can say something about your
environment you put a few logical rules
them you just play it straight in and
suddenly it starts with a lot better
because know there's a huge space of
possibilities it doesn't worry about so
those are you there and so some of the
work they're doing the moment is to
enhance the compressor you get trouble
kinds of machine learning algorithms and
they're to find patterns and all kind of
stuff to make a system so some people
will freaked out when they saw us
there's some people very worried about
super-intelligent machines destroying
the world and there were things in the
future um you you don't need to worry
about this fifthly Joel was quite aware
of these problems and he's interested in
the work but singularity Institute in
the weakest links he knows about these
problems and he considers them to be a
girl and the other thing is that while
it seemed out of increasing up if you
really think about it come up actually
it's very general but it's something
like paint believable intelligence it
finds its way around a mice eating
that's right
so at the moment there's no there's no
danger this and the models are
well-known and well-understood the
compressors are well understood there's
nothing too crazy so there's no need to
panic it would it would be sort of us
light years away from being out to say
understand its own source code I mean
there's no chance of an ant
writing yourself improving anytime soon
yeah so you don't even worry about this
at least not for decades anyways here
and in fact there's a reason businesses
I don't think this is the way that's
going to drive a just maybe I just want
to check my intuitions
down the road it wouldn't be very hard
presumably if you've got some of an
array of these things running bots on a
high level that gives it directives like
understand your own source code or do it
or do anything this well I mean you know
how do you a code understand your
although okay work out how to work out
what the a the ANSI algorithm is or you
know or anything you don't we don't want
to know how to do right it wouldn't be
very hard to have a high level but yeah
this is basically yeah yeah well you can
do that in a much simple way just have a
search over all programs that keep
running them and divided up is parallel
if I might even the search type argument
and you can it you can run the evolution
you can do lots of things which in
theory can lead to these things that are
practice that's so computable
intractable that just doesn't happen and
so I think this falls into this camera
we would need much smarter compresses is
just using a compressor from teks
completion
it's a tapes compressor with search
strength all upfront about this
basically what it is it's I think it's
ability to achieve a wide range of those
environments and things like to meet
unlv reduction are very important parts
are being able to do that the
destination kept my whole concept what
about morale differences yeah presence
is a set of rules which is great enough
dimension that say it has it can
interpret and say this is an action I
should
should wait to do more it says no reason
you can put that in at the moment it's
just yeah so yeah so what progress
things and later cuz we're going through
an era okay so I actually don't think
this is gonna really drive stuff
forwards really soon I think there's
actually some another area with
okay can we formalize this I'll try to
go through its recently quickly
basically when we've defined a I see
we've actually defined it in the parts
that we need we have a new machinery
that we need to formalize what this is
because we have to find an agent which
refined succeed goal we've defined a
wide range of environments we've got a
little bits and so you're just basically
a simple all the pieces together from
the AIT machinery it naturally comes
together that you have a definition of
intelligence the only thing is missing
is that this is nothing about all kinds
razor in we know we want to meet you the
entire general ability to achieve goals
or wide range environments we need some
we're waiting these environments and
essentially that's what Occam's razor
tells us it sees that I prioritize some
polar environments are more likely and
so if we want to teach the idea in such
a way that using our controls and
magazines we need to wipe the
environments and so if you put a little
together we've got out you know agents
are we've got our wide range of
environments here which is will out
computable distributions we've got Alex
all kinds razor here that's the
complexity of the environment waiting
and then we've got the expected success
on the agent in this environment and we
just consider all possible environments
so this is the wide range of
environments this here is the success
the the goal is sort of a plus
within the environment and then we just
we just take the whole we just look at
the expected performance so it contains
everything isn't intelligence well if
this argue to seek and before that
there's formalizes the definition I have
and so if you accept the important
definition right here this is a formal
characterization of it each of the
symbols corresponds to part of the
important definition plus Occam's razor
so I think it's you can argue that in a
formal and a very very very general
sense because that's what I want a lot
of very general concept of intelligence
yeah it could apply to machines anything
it's a super intelligent in future
whatever it formalizes a lot of these
ideas about knowledge of psychology it's
very general it says nothing about the
internal workings of the agent the agent
can use magic to come up with its
distribution of relations that's fine so
we're not making any assumptions about
what's going on inside it doesn't have
to be computable in fact a oh I see is a
computable we can talk about the
intelligence agency so that's it's very
fortunate with um intelligent agents
with apply universal intelligence very
powerful turns out that that it seems
this intelligence feature is the dual of
a I see IO XE is a good agent this is
the Misha that has the same structure
where Eric sees the mixing language okay
and so we know theoretically theme that
if something has that very high
universal intelligence it's going to be
optimal in this message space with
bottom insist on so having the very high
universal intelligence
theoretically really means something in
terms of empowerment and it's also
practically meaningful if you get a
system that was able to achieve a wide
range of goals and wide range of
environments and conventional for do its
thing
and it's really quite a significant
practical thing right so as well there's
not meaningless in reality
if you have a high vision it's more
power and from eccentric there's a big
problem with the Turing test you have to
be you have to convince some judges that
that you're heavier you're human right
and so a key part of passing the Turing
test is being human life you have to
make spelling mistakes you have to not
be able to multiply to twenty digit
numbers in under a second
right you have to you have to do all
these sorts of things so it's really a
test of humanists and the idea is then
if you're sufficiently human-like while
intelligence must have come along with
it but I would suspect the next bottle
Tommy compositor in theory superior
human intelligence because you're able
to fake being something of a completely
different nature so convincingly things
of humans kind of different this says
nothing about humanity it's all the
finer formats and finally it's formally
defined even if you don't like my
machine intelligence you don't have to
agree with my equation at least I've
seen detective what I mean there isn't
any ambiguity Mia I'm not saying all
intelligence is the ability to get
conscious interaction and a creative way
or something like this where I'm really
just moving the problem defining
intelligence then to all these other
words that I have in the final
I've actually given you an equation not
see precisely what are these past op so
even if you disagree with me you should
disagree with me in a very specific way
because I've seen exactly what I mean
okay how do you makes you have you
practically apply they tell this vision
yeah okay so it misses again we work in
theory it's not a computable media okay
so again what we have to do is we have
approximation but finally I would say
that okay because the media considers
the spice of all environments which are
computable attributable distributions
it's an amazing amazing spice so if
somebody comes up to me and says ah your
definition of intelligence it it's not a
right given issue because you haven't
taken account off insert your favorite
thing consciousness creativity later
okay let's take this I created I would
then answer well it's creativity useful
for being able to succeed in some
environments now there are some problems
that require creativity to be able to
that helps you being creative and if the
answer is yes then creativity is already
being measured by this because you're
using your creativity to be able to work
better in these environments that
require creativity with cell phones so
it's already taken to universe
we're now consciousness the
consciousness is important my question
is does it help you do stuff
consciousness if it does it's a revision
if it's not being needed it means that
in non computable distribution over any
possible universe can consciousness help
you do anything right in which case I
don't really care well in terms of
intelligence I don't care
philosophically I might hear about
but it's not actually helping me any
system achieving and that's what I worry
about you okay so can this be my
practical lease work has been doubled us
but there is there are a few things
going on I had some attention to do some
things in this area but I'm just kind of
being distracted so Matt Mahoney has
been essentially sampling from see what
he does is he generates random Turing
machines with some time bound on them
and gets him to spit out a whole lot of
stuff and these the stuff that comes out
has a essentially a kind of universal
distribution and then he uses it to
benchmark compressors and as you might
expect they're really good compressors
and we're elemental benchmarks work well
on against a universal distribution as
well this guy here this was back in 2001
ago he had some some more ideas about
intelligence and what he does he put
together a definition of intelligence
using Kolmogorov complexity and then he
approximated using some cool Nathan
complexity which takes time from the
time into account so it's not just
description we this time like what you
were and then is computable and he
didn't manage to come up with the
sequence he I needed sequences he didn't
do full interaction systems like oh my
beautiful so so good more limited lenses
and sure enough he came up with
sequences and you had to then predict
what's coming needs and I'm like usually
an intelligence test you get all these
different sequences you have to project
and some are hotter than others where do
they come from well you know people
think that this thing here we actually
have numbers we can say this sequence
has complexity 9 right let's stick with
this complexity 14 and sure enough as
this number goes up it gets much harder
to predict sequences and what he did is
he took a whole bunch of these get into
more students and getting IQ tests and
sure enough the IQ test and their
ability to deal with complexity
correlated
it's not a surprising really so this is
actually a computable but only in the
sequence prediction sort of and finally
being good soul is about to publish a
paper where he takes this universal
intelligence staff and then tries to
approximate in different ways and do
freeze things it's not published yet I
you seem familiar on one of the guys
here in society there is so there are
some things happening here but still
quite you okay
now do I think that all this universal
theoretical stuff is going to drive ie I
think it's good I think it's a big
improvement but Isis peaked think some
of the key things I actually got to come
from your site and if you look at Monte
Carlo IRC really if you want to make
that work well a lot of it comes down to
building meter compressors it's ok it's
good it's better at predicting what's
gonna be water what's happening to each
other and so why you have now decompose
the problem into a Mon Calas research
and having very good compressors you
know you still haven't that's a
beginning but this is all the world of
problems we solved how do you deal with
all sorts of Copeland sequences and
stuff you need to build very very path
with the graces to do these things so a
heaven it isn't 20-foot so notice all
from there there's an enormous amount of
research to be done with a prism and
actually really doing so I suspect that
where it's where we're going to get a
lot of things from a lot of monochrome
is theoretical neuroscience and this is
the reason why I have been actually been
working on complexities of things
recently and I'll come to London to work
with the Gatsby unit and you know this
College London and against the unit's
interesting place because they do
theoretical and computational
neuroscience and machine learning so
they're actually working at the
interface of two different areas one
there's all the theoretical machine
learning stuff from artificial
intelligence and the other side is the
computational theory of New York and the
finding the points of which these things
are crossing over and I think that's
really interesting I think there's a lot
a lot of things been contacting them and
so oh my giving that the brain is not a
black box usually I don't have such a
gory picture of a brain but it's
Halloween the cerebellum here you go so
cerebellum down here you got cerebral
cortex a little brain stem right so when
we look at a brain and you start looking
at some theory and here a science you
actually start to see a basic
architecture for an hour it's really
neat so I'm going to try to explain in
the minutes that remain a little bit of
this so this is the cerebral cortex this
is part of the stuff under here there's
a whole bunch of stuff underneath this
and there's actually whole bunches so
this is are the outside view of the
brain the brain stem coming up here and
there's a whole bunch of stuff
underneath this subcortical in that's
important too but we'll start off with
this here this is a very interesting
system one of the first interesting
things about it is that it's basically a
sheet and it's got these folds in it
because it's being crumpled up to fit
inside the skull there's actually about
the size of a dinner napkin right and
it's such a useful thing that the brain
seems have had to evolutionary scrunch
it all up and fit inside your skull
because it's of such a value of what's
been worn
and this sheet acts as sort of like a
computer to the brand and it has an a
fairly consistent structure it's gone we
can create count six to twelve layers
and it's pretty similar across the whole
the whole from from from the back of the
front of the brain up and down it's
pretty similar you find some what kinds
of neurons and similar layers right
across the whole thing
okay no the way it's split up is you
have basically this is the back of your
brain this is the front of your brain
the backside here is this some of a
central sulcus down the middle here
which is a kind of a valley now this
whole blue side the backside here is the
perception side so basically what that
does is you have information coming in
from the world and it processes that
information so here we've got areas
seventeen that's the primary visual
cortex so you've got from your eyes you
can you go down to optic nerve you go to
its mcquewick Alateen they get projected
up to here and here you've basically
just got a map of what you're seeing
okay so this is where the visual
information comes in yeah forty-one here
is the primary auditory so your Sounders
coming into here and then along here
you've got touch that's and prepositions
like angles and rejoins so this is all
your touch stuff this is sound
this is vision and then what happened is
that in each of the same envision it's a
little bit more complicated you guys
you've got mobile pathways and stuff
going on but basically what happens is
that you get different areas going up
and it goes from very very simple like a
map of what you're seeing to finding
more and more abstract features of of
the world and so it builds a hierarchy
of abstraction from the most basic
features like ages orientations moving
ages areas of life and darkens
and it builds up more and more abstract
features until you get up to this
lighter area here this is association
cortex we are it represents things like
Bill Clinton and you can find Euron's up
here which will respond simply adjust
the Bill Clinton you can have a picture
with a whole bunch of people and then
your own fly's wings go over there bill
is gonna be a cartoon not a real picture
you can be young give me all sorts of
different things and so up here you have
representations which are very abstract
here it's not so abstract same thing
goes on and sound same thing goes on
motor on this slide here it's the lowest
level to touch stop as you move away
from it it's much more abstract things
to do with weather deceiving now we go
the other side we've got the same basic
idea going on but now it's an executive
system is the action system so along
here we have all your ability to drive
muscles so this is all the muscle
driving to move your fingers and speak
and all these things and then as we as
we move away from this it becomes more
and more abstract
so it starts having plans of action and
then whole sequences of action all the
way up to sort of a long term conceptual
thinking about and so you've got this
beautiful architecture you've got this
hierarchy from low-level perception all
work to more abstract sort of perception
and then on the other side you've got
the executive side the action side of
things from very low level things of
your ear everything
yes yes yes they do so you have actually
a sort of a linear map of your body like
I remember exactly oh yeah your hands
are down here up here your face and your
tiny video on the other side it matches
up now what happens is if you look at
the connectivity you feels really good
there's no better image they actually
connect at about the same level and then
this holds up the hierarchy as well so
you've got an area up here Broca's area
which is to do with the generation of
sentences and meeting and synthesis and
then you run a video theory which is the
corresponding part over here which is
the recognition of the meaning of
synthesis so you've got the generation
side and you get a recognition slider
and if you damage one of these you can
lose the specific ability so if you
damage to this area you can lose the
ability to recognize the meaning of
sentences but you can still construct
meaningful sentences because you've got
the generation side and vice versa and
not surprisingly these two areas can end
up with each other this area doesn't
connect down to this area it wouldn't
seem so what we have as we basically
take this that we can remap that out
into some hierarchy so we had were your
senses here your the information coming
in here becomes more and more abstract
as you go up and also the the senses
become integrated so you have things up
here which represent things to do with
touch and sound and then on the other
side you have the same thing but you
have the executive side that's right and
that's what drives your actions it's on
and so you like for muscle firings down
here to do things all the way up to long
term conceptual planning and what you
find is that these actually interact
with each other on similar levels of
abstraction okay
I don't know it's some cases like an
estimated for vision because you can
figure out how long it takes and it's
it's a she's very sweet it's something
that's from vision talking yeah so it
sounded because they're always short
it's not an exact number by any means
yeah we didn't get the nice little
little societÃ  in there deviations
basically you can present an image down
here and you can see how fast it
recognizes Bill Clinton up here and you
know how fast the information propagates
through neuron so calculate right I need
to get moving so these questions might
have to wait for pop okay so this is
quite an interesting structure it looks
like something you might actually design
yeah there's actually more going on so I
still get under here you've got this all
the subcortical stuff now this here is
basically just a model of what's going
on in the world and it's the information
doesn't just go off by the way it comes
back down so you can actually see
something in the expecting at a
conceptual level to see something in a
commune interact with each other and the
same here the information goes up and
down because it had very very simple
properties okay
so underneath this we've got another
system down here which has all kinds of
things in it and some of the things that
it does is essentially like the
reinforcement green color system and so
what happens is you did an abstract
label you could say okay we have an
environment the information comes up
here it gets processed from symbol from
raw representation of the very abstract
conceptual representation and the other
side we have their executive process
which generates the actions back out to
an environment we have our low level
stuff we're very consumers over here and
then we have a little woman system on
top of us which which sort of is the
goals and directs and modulates okay now
how is this we're going to all begin
with us how does the system that's us
listen let me know about it is that
we've actually been to a lot about it
we're making a lot of progress at the
moment about this the last team last
five years huge amount of progress and
it's really amazing what we're
discovering we're discovering that the
brain is using a whole bunch of
algorithms that we already knew about
algorithms already discovered in machine
learning of course it's not using some
of the algorithms like there's not going
to do so at least we know which is
really optimal you need a big matrix you
invert it it's pretty hard to do a
neural computation it works really well
so we can we get better algorithms of
what the brain is using and so I seem to
know the client speaking but yeah
standard algorithms that we know about
the brain is actually using example
rhythms it's really agreeable
so one algorithm is table difference
meaning now it's going to put an
equation up but I figured we probably
eight forty by this point so so the idea
is actually very simple what you do is
you remember the V function we had
before that's the expected reward okay
so what you do is you have a model the
new system what is the expected reward
in this state of the world what's the
expected reward on what do I expect in
the future how we have good is this and
then what happens is that you experience
what comes next and what you do is you
can hear what you're expected with what
actually happened you look at the
difference and then you update your
moral based on the difference so if you
expect it to get a big high out of
something and it turned out to be not so
great you're going to update your model
so in the future in that situation
you're not going to expect as much as
you previously do and you incremental e
update you more and you can adapt the
other way as well hope you go looking in
the brain this is indeed the music
systems dopamine as it seems to be a key
thing in this whole process so the
dopaminergic neurons in
vitia ventral yep
and so what happens okay to start with
we we this in the beginning we should we
have a stimulus this is this is the old
stuff I'm showing you like some pretty
original things there's much they
actually must be a recordings it's okay
we show a stimulus at this point the
monkey two guys are okay
and then we give it a reward at this
point and just after a reward is a big
jump and the activities in your eyes and
so what happened is that it didn't
expect anything great to happen and all
of a sudden something did and so there's
a difference between its expectation and
what actually happened there's a big
positive difference I didn't expect that
anything cool was going to happen after
I saw this wow that's really good I
didn't expect chocolate bars to taste so
good I gave each other by well that's
really good so you can update your model
so that next time you in chocolate bar
you see a chocolate bar you speak it's
gonna be tasty right and so what happens
out for a while is that you show the
stimulus then is an increase here
because it's expecting the future
deficit another reward when you actually
give it the chocolate bar we're taking
juice that's what I use use monkeys the
lava juice there's no spiking you blow
right because this updated it's more
there's no era there's no difference you
see see the algorithm working and then
you can do this you can do the same
thing um you can so you can show the you
can show the there's stimulus that you
get a spite here if you present us then
we lost is no there's no nothing happens
because it's as expected if you don't
give it the reward suddenly this these
drop down and then actually this one
like a negative right it's a negative
prediction and so it then has to update
its model the other way so this is the
old stuff and there's been a ton of
stuff following this and it's and it's
basically what we're finding is the
brain is using a whole bunch a over in
this we
no it's really incredible so there's
tons of stuff well money goodish I'm not
in the expended I'm just going to show
you a few bits of it okay so this is a
bit problematic representing a negative
sign in a spike in your language right
so this is a bit difficult well turns
out that there's a park over beyond
which is computing the negative of this
that makes it much easier so if you were
actually designing this system you will
need you'd probably put this on yourself
does the brain use bottle base the
bottle free reinforcement learning this
is a big split within the area some
methods like I'm not gonna go into some
pieces have an internal model of the
world and then they can do much more
powerful things but it's a lot more
computationally expensive other methods
are several more free they're much more
computationally cheap but they're more
limited in what they can do so you could
trade off between the power of what
you're able to do and how much
computation you have to do now what's
the brain to some people in the field
you know and machine learning do these
some people do these was the branding it
does both it does both and it switches
and knows when to switch when it can use
the cheap system the use of the cheap
system when that's not good enough it
switches over to the more expensive
system and you can actually identify
which parts are doing computing there's
different systems if you have a mouse or
whatever it's damaged in one of these
parts you can show those performance
words back to the beautiful forwards of
these two algorithms where they don't so
it's pretty clear excusing both but
pipes but talks that we've actually
developed long before we knew this was
going on the brain it already figured
that out and why don't use both types
and just intelligent any switch now the
trick that we use and practice that we
have suit our rewards so from formative
cues and things like that we don't know
if it's going to associated with any
reward in the future but it could be
useful as there's a general heuristic we
might want to
we're doing that turns out the brains
using that trick too it's stealing all
our tricks how does it deal with coal
convicts table sequences so the way we
do this and reinforcement learning is we
use something with hierarchical
reinforcements meeting and I'm looking
explain at all but this is a typical
sort of hierarchical reinforcement
learning model it's going with different
bits in there this is the era of the
rewards it's a careful difference so
from actor critic model blah blah blah I
explained it but these are all the parts
of the brain seem to be doing this
now I presented this to a group of
neuroscientists some of it quite expert
in this including neuroscientist from
other parts of UCL and some visitors
from America who work in this soft and
you know what they told me I gave an
hour-long talk just on this it's not my
work it's as I'll summarizing the work
of discussing and what they keep saying
was they didn't say this is wrong they
see it uh-huh
what we know over here or over here or
Weaver there's if we don't more than
this the brain is not just doing this
but it also does this endless endless
endless and there's the business so this
is actually just a simplified model of
what we know and these these are our
boxes we were you know we don't know
what's going on yeah these are
algorithms we read you about we already
coded these you can download a code that
does these types of we're actually
starting to understand higher hierarchy
they reinforce what was going on the
brain it's really quite amazing and this
was progressing very quickly his own
cousin you stopped disease you need you
multiply mice that can um you can switch
off different parts of the system and
precisely identify though yeah I'm also
number neurons in the area will we have
a good understanding of our Alice's and
the brain Bob for 2020 right the answer
I gave is typically oh we should
understand before the fact we have a
pretty good outline or even right this
part of the brain really is a long
progress
they're really starting to get a grip on
and might speak with in well with the
teen years they should have it by 2020
we shouldn't really be in it right so
that's the reprints from any part now
the big problem in reinforcement
learning is that if you register a robot
and it's in the world and it can move
any arm and any direction all these
things there's a gigantic space of
possibilities and it's also looking at
this pixel view of the world that it's
so what you need to go to do is you need
to go to abstract the world out in order
to make it manageable to deal with the
complexity so what you need is you need
some sort of a hierarchical abstract
representation of the world firstly for
your actions and secondly for your
perceptions automatable hey what's the
brain going it's appears to be doing
just this and if you look at the
connections you have those dopaminergic
and all this stuff in the basal ganglia
connecting output into the cortex so
we're just dying to see some like an eye
design here this is what the brain is
doing sorry do we have anything that
does this type of purchasing well would
actually be making more progress in
doing that kind of thing I don't know if
we're gonna have enough progress in the
next ten years and I think one of their
big promising areas love the work done
by a previous director that gets the
univer a mad moment Geoffrey Hinton um
when someone could restricted Boltzmann
machines and they have all kinds of
properties of Asia very much like this
hierarchical system we see right so you
can train deep networks
previously in Jerome into a lot of
problems training these nitwits we need
to figure out how to do it
these are ones you can train these
nitwits the hierarchies and you see
increasing abstraction as you go up the
lions just like we see in cortex it has
a local learning rule meaning that when
you adjust the weights in the system we
don't need to know about stock all over
the system
you just need to have a look at the
information passing through we you are
and then there's exactly what you have
reserve Sciences they can't do some glue
all fancy computation they have to count
like to pass along where they are well
it's me so you need a local luminol
these have local minerals
it's a generative model that can both
recognize and generate behavior and
that's exactly what we see in this
system it's able to do multiple
constraint satisfaction and filling in
so it can observe a whole bunch of
things and then you can ask get to
generate something which is none of the
things that seem that's a bit like from
culmination it's very very flexible in
this way um it can be my temple ruins
sequences of time it can even be
implemented with a spike of your own
work that we don't usually do this
because more efficient to do it was like
different way but it can't actually be
implemented that a wall that's more like
real brick so I'm not saying it's the
same as Cortese who deeds is a kind of
complicated thing in this system but it
appears to be in the same general class
of algorithms that's going on and it has
many of the similar properties and if
we're trying to build an AI that's not
necessarily we have to exactly represent
cool takes is that we need something
that provides an time a function to our
system and this seems to provide a
similar talk so there's a couple of
videos maybe I can show you the videos
later ok ok so I have actual videos
which I can show you afterwards where
the system is generated you can get it
to imagine things and you can actually
see it imagining things which is really
neat you can get it to learn walking and
so you see the system walking and then
just start the unit observes walking and
then it's just you can generate it they
never walk around your screen in a
natural way and then you can get it to
move between different styles of walking
so connect can move between walking sexy
to work walking and
like John Wayne cowboy style a little
and then you can ask it to do both at
the same time and in Asia comes up with
some combination of them it's a very
very powerful janitor poem and it's the
same type of system which also works for
recognition you can get your equines
characters and their do image
recognition and all kinds of different
things okay sorry
so if we could public unit level
intelligence I I think we can almost see
me scaled up to understand a machine
that has well above human level
intelligence we'll probably be able to
understand so in disorder decided we
have no idea how to do this we really
are in the dark here there is an
institute singularity proper
intelligence studying this problem one
of the pool balls has developed some
cool friendly AI which is a very
valuable safety for humanity no matter
how super challenging now they don't
really know how to do that this is we
can see this it looks like it's probably
gonna make some consoles play some
serious risks here we don't know how
system the supply whispers sorry this is
what Halloween's memory for you I think
in the early 2020s we should have either
floppy snaps that is not radical you can
build a desktop machine today with a few
of the latest play and greatest cards
and get into your floors right in ten
years we should be at petaflops and
think even if we slow down our current
progress we didn't even hit box isn't
somewhat conservative
oh I'm Peter thousand tiny mrs. Foster
yeah so this is one and then 15 zeros
after it
calculations is easier okay so um how
are we gonna hit powerful now we're
those with deep the linguist I think
we're already getting there I haven't
show you the videos you can see the
mouth words they do some doesn't pretty
fancy stuff it looks like it's in the
right sort of spice of algorithms and
answer what's with the properties right
and this is the career where I say this
is a key thing to reduce down the spice
and possibilities with reinforcement we
need to make well so I put a question
mark here I don't know how it's gonna
progress are we going to be at a point
in between the early 2020s where we can
basically provide the same type of
functionality the context provides for
an AI system we've made a lot of
progress in the last ten years so I
think we looks like we're heading in
that direction
furthermore if you've got desktop
machines with a petaflop
you can make much bigger networks but
more importantly you can keep a lot more
ideas so all the students and I know I'm
talking about desktop machines because
all your great student stuff get these
finishes they don't get super keys
these are these are the guys nation
drive along private schools they can run
a thousand times as many experiments
where they can run a hundred times me
experimenting a hundred things that are
a hundred times as big and so that
really speeds up research we don't wait
a week to see the results anymore
you can run ten different variations any
manner and out so you can try out closed
things that's I think this is a to going
to speed up this quite a lot and we've
already made just the last five years
they've been long progress so I think
it's possible but I got a question mark
on there possible that we'll have it's a
really good temporal deep belief
networks to the next Anita also brain
reinforcement learning that's going to
be no really
we're great progress there with people I
speak to think it's going to be pretty
long asleep and we even they're better
algorithms some
the brain needs to use it because we can
just take a matrix in leadership and
with brain flows it pretty hard
so we creation took workforce with all
these things and do them in a really
good way so we can possibly probably do
it better than others and this was the
early 2020s and this I think is the
basic architecture for an AI you've got
the performance to do it you've got the
powerful algorithm to generate the
abstractions and you have the underlying
reinforce when you drive home now
I think the consequence of this's will
be many groups in the early 2020s we're
gonna brain like a a gir which it's
artificial general intelligence I think
it's just a natural consequence of
reasons if any of these groups has any
significant success they're going to be
trying these things out supercomputers
prefer this is an inevitable and they
should be exaflop supercomputers so this
is one with 18 zeros after it and this
is a conservative prediction if you look
at the supercomputer guys and what
they're predicting and say top 500 org
they are predicting an exaflop
by 2019 we're already we're already at
two petaflops team petaflops is coming
in a year and just like six months after
that is twenty petaflops and IBM is
already talking to two groups about how
to build the first exhibition
they're already discussing it with them
based on the 20 people right so this is
the idea that we're gonna he'd be here
by the early 2020s that takes your
rocket light okay so I think that this
is very reasonable
this is very reasonable if these things
are true then this is very reasonable
this is certainly very reasonable that
it's very reasonable this is a question
mark what we figure out the algorithms
to get the pretty powerful I don't know
that's a speaking mind but we're not
gonna have a practical theory of
Friendly AI now I've spoken to a bunch
of people including
my commands to the president of Sinclair
Institute none of them that I'll be able
to think that they will have a practical
theory of friendly artificial
intelligence in about ten years I know
of why when I talk to them they say well
we've gotta be it's gonna take longer
than that it's not going to be 2020s
before things start getting crazy
oh my go in here we are starting to
understand I'm not saying we understand
corner court how the brain works because
there will be a lot of things we won't
understand elevators but where others
we're going to look at the brain we're
actually getting an architecture for the
system we know all these different bits
how I connect up the basic algorithms
that are you've been used and in some
cases quite explicit than the old ones
over yet so we keep a blueprint for
building an artificial general
intelligence and it's emergent quite
quickly and we're going to have the
computer pelvic drive with them so
that's my Halloween scenario that in the
early 2020s we're back we're gonna have
the hardware you didn't have the exit
clock to good kids
we're gonna possibly have the deep leave
me with this is the questionable but
this is the bit that could ruin make a
ruin the whole thing I've been going to
get local this is a hot area of the
money by the way a lot of people coming
into this we just had a big workshop
with all 12 people in the world it
against the Union lot of things happen
again this this is this little guys
gonna be so
so if this will takes off we can have
people with computers and we know crazy
what can you do you can't slow down this
nothing I mean this is a massive global
I don't think you can really speed that
much it's really hard when I say that
you know they're not going to develop
this this is not a criticism of the
irritability or they're a team or
anything like this
it's really really hard we have no idea
how to solve this problem
I don't know if we can really speed up I
mean really and if you handle
alternatives by Halloween scenario they
tell the world</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>