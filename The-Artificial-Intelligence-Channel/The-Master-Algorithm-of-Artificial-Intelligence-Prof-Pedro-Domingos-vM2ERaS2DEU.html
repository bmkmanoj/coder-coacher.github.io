<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Master Algorithm of Artificial Intelligence - Prof. Pedro Domingos | Coder Coacher - Coaching Coders</title><meta content="The Master Algorithm of Artificial Intelligence - Prof. Pedro Domingos - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Master Algorithm of Artificial Intelligence - Prof. Pedro Domingos</b></h2><h5 class="post__date">2017-11-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vM2ERaS2DEU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">can everybody hear me where does
knowledge come from until recently it
came from just three sources
the first one is evolution that's the
knowledge that's included in your genes
the second one is experience knowledge
it's encoded in your neurons the third
one is culture the knowledge that you
acquire by you know reading books
talking whether people and so on but in
just the last few decades a new source
of knowledge has appeared on the planet
and that's computers and notice that
each of these sources of knowledge was a
major you know development in in the
history of life on Earth
evolution is life itself the experience
is what distinguishes you know mammals
from insects and cultures is what makes
you know humans are unique and and so
successful and you know the appearance
of computers as a source of knowledge I
believe is going to be every bit as
momentous as the previous ones notice
also that each of these sources is works
orders of many toots faster than the
previous ones and generates orders of
magnitude more knowledge and computers
are going to be no exception in fact Yan
Lacan who is the director AI research at
Facebook says that most of the knowledge
in the world in the future is going to
be extracted by machines and will reside
in machines so so how do a computer
discover new knowledge well there's
there's five main ways one is by filling
in gaps in the existing knowledge a
little bit like scientists at work
another one is by emulating the brain a
very popular one these days another one
is to simulate evolution then you can
discover your knowledge by
systematically reducing your uncertainty
and finally you can discover your
knowledge by reasoning by analogy by
noticing similarities between the old
and the new and the field of computer
science that deals with discovering your
knowledge is called machine learning
it's machines learning it's it's the
field that I work in and one of the
things that makes it not not just very
important is this but also you know very
fun and very fascinating is that every
one of these paradigms of
machine learning has you know its roots
in a different field of knowledge and
and it also has you know what I call its
master algorithm and master algorithm is
is an algorithm that you can use in
principle to discover any knowledge from
data you take the same algorithm you
gave it you give it data from a
different problem and it learns to do
different things which is very different
from having to program computers which
is what the old way of doing things was
so the first tribe that we're going to
look at is the Symbolists data wants who
you know discovering your knowledge by
trying to fill in gaps in your existing
knowledge they have their roots in in in
logic in philosophy and in computer
science and their master algorithm is
inverse deduction and we're going to see
the very shortly what that is then
there's the connectionists whose idea is
to reverse-engineer the brain and
therefore their words are mainly
neuroscience there a master algorithm is
backpropagation the evolutionary to have
their origins of course in evolutionary
biology and their master algorithm is
genetic programming patients come from
statistics and and what they do is
essentially probabilistic inference and
then finally the analogy actors they
actually have their roots in many
different fields but the main one is
probably psychology there's a lot of
evidence that human beings reason by
analogy and and they're their master
algorithm is kernel machines so let's
start with the symbol lists here are
some of the most prominent simulus in
the world Tom Mitchell at Carnegie
Mellon Steve Martin in the UK Ross
Quinlan in Australia and and the the
basic idea of the symbol lists is
actually very simple but also very
insightful it's this idea that learning
is induction right and the induction is
discovering general rules from specific
facts which is the opposite of deduction
which is going from Swiss from from from
which is applying those general rules to
infer specific facts so in a way
induction and deduction are the inverses
of each other
in the same way
that say addition and subtraction are
inverses so for example addition gives
us the answer to the question if I have
two plus two what do I get well for
subtraction is the answer to the inverse
question what do I need to do to what do
I need to add to 2 in order to get 4 and
the answer is 2 well interest so the
idea of induction as the inverse of
deduction works in the same way so for
example deduction is something like if I
know that Socrates is human and I know
that humans are mortal what can I in her
well I can infer that Socrates is mortal
now induction is the inverse of that is
saying like if I know that Socrates is
human what else do I need to know in
order to infer that his mortal and the
answer of course is that humans are
mortal and once I once I know that once
I've discovered that rule I can add it
to my knowledge base and then in the
future I can combine it in you know many
different ways at different rules to
answer questions that maybe I didn't
even think of before and as I mentioned
this is a little bit like scientists at
work and in fact one of the most
interesting applications of this type of
machine learning so far is the biologist
that you see in this picture the
biologist in this picture is actually
not the guy in the lab coat the guy and
the lab code is is actually machine
learning researcher by the name of Russ
King the biologist is there is that
machine there it's a complete robot
scientist that starts with basic
knowledge of molecular biology DNA
proteins and so on and then it looks at
data it formulates hypotheses to explain
that data using inverse deduction it
designs and carries out the experiments
to test those hypotheses using you know
DNA microarrays and so on it which is
what you see there and so on and that
robot is called Eve and two years ago it
discovered a new malaria drug and once
you have a robot like that there's
nothing keeping you from making a
million so now we can make progress in
science much much faster than than we
could before now the connection
they're skeptical of this approach they
say it's too clean it's too logical it's
too abstract to high-level the best way
to learn is to emulate the best learning
system in the universe namely the one
inside your skull so their idea is to
reverse-engineer the brain to do
learning that's inspired by our
knowledge of how the brain works the
best the leader of the other
connectionists is geoff hinton he
started off as a psychologist these days
is more of a computer scientist and he
splits his time between Google and the
University of Toronto and he believes
that the way our brains work the way our
brains learn can be captured by a single
algorithm and he's been trying to
discover that algorithm for years now in
fact he tells the story of coming home
one day from work very excited saying
yeah I did it I figured out how the
brain works and his daughter replied oh
dad not again
but you know so his quest has had its
ups and downs but it's starting to pay
off in particular this algorithm that he
Co invented called back propagation is
taking the world by storm it's used for
just about everything these days the
cell phone in your pocket probably uses
your deep learning in order to you know
to understand what you say to do speech
recognition for example you know search
results ad placement in image
recognition all this is being done using
the algorithms that that he and others
invented like for example people like
yin lik and yoshua bengio so so what's
the idea here
we know that your brain is made of
neurons it's a big network of neurons so
what we're going to do is we're going to
build a mathematical model of how a
neuron works and then we're going to
connect those neurons in the network and
and and try to make it learn the same
way that your brain works and the key
idea here is that all the knowledge that
you have everything that you know
everything that you've ever learned is
encoded in the connections between your
neurons which is why this school is
called connectionism so we're going to
do the same thing here so here's a
picture of a neuron you know there's the
cell body there's the a neuron looks
like a little bit like a microscopic
tree there's a trunk called the axon the
the branch is called dendrites the roots
are also called dendrites and then the
roots of one neuron the bread the
branches of our neuron connects with the
roots of others and and the electric
discharges flow across so let an
electric discharge across the axon is
called an action potential and if the
charge coming into a neuron exceeds a
certain threshold then the neuron fires
this is actually what's happening in
your brain right now
there's the symphony of electric
discharges going on and the key thing is
that you know one urine can be more or
less effective at making another one
higher some connections are stronger and
you learn by making those connections
stronger or weaker okay so we build a
mathematical model of that it's straight
forth you know there's you notice how
there's a correspondence between the
parts of this cell and and that parts of
this diagram you know here's here's my
cell body
you know the inputs come in there could
be pixels for example in an image that
you're looking at or there could be
other neurons each one of those gets
multiplied by the way that's what we're
going to learn and then if that's some
and then they all get added up and if
that exceeds the threshold and then
you're on fire so for example if you're
looking at a cat and you do recognize a
cat then then the neuron then then
you're on fire now the interesting
question is is what we do when we have a
big network of these all connected
together if you think about it you know
think of one little neuron in the middle
of your brain how is that neuron
responsible for the mistakes or that are
the good calls that you make all right
this is often called a credit assignment
problem or maybe it should be called the
blame assignment problem because it's
the question of who do you know what to
change when something is going wrong
okay if it should be firing because it's
seeing a cat but but it's not firing
then you know how do I need to change
the weights and that's where this very
famous algorithm called back propagation
comes in what proper propagation does is
essentially just looking at each one of
those weights in turn you know starting
you know at the output and going all the
way back to the input and saying if I
change this if I increase this weight a
little bit does the output become closer
to what it should be or farther and then
it changes all of the all of the weights
and then it looks at the next example
another cat or maybe another dog and and
and keeps going that way and as I
mentioned this this type of learning is
used for just
about everything these days but but
perhaps the most famous example is one
that was on the front page of the New
York Times a couple of years ago and it
became known as the Google cat Network
in fact this this network can recognize
all sorts of things besides cats but the
journalists picked up on on the cats
because this this network was learned
from YouTube videos right what they had
was the network watching hours and hours
and hours of YouTube videos so maybe it
should have been called the couch potato
network but but they call it the cat
Network because I don't know if you know
this but people love to upload videos of
their cats so there's more footage of
cats on YouTube than anything else so
the single thing that this did net were
the best was was recognized cats but it
recognized a lot of other things as well
and and and you've been using it when
you use Google probably that without
knowing it now the evolutionary's they
say well sure backdrop is nice for you
know fine-tuning the connections between
your neurons but it was evolution that
actually produced your brain in the
first place not to mention all of life
on Earth so maybe the best way to learn
is to simulate evolution on the computer
except that instead of evolving you know
plants and animals were going to evolve
programs so the first person to run with
this idea was was John Holland and then
you know more recently people like like
you know you know john 'cousin and how
flips on they have done a lot of work in
this field so here's the idea
we know roughly how evolution works so
we're just gonna make the computer do
the same we're going to have a
population of programs each of which has
it has a genome which of course is not
going to be DNA based message is going
to be bits right so in a computer genome
is really a bunch of bits the bits
define a program and then their program
runs on the test and gets a score for
how well it does
called the fitness you know by analogy
with biology and then the fittest
programs actually get to mate you
actually generate children programs by
crossover between the genomes of two
parent programs and then they get
randomly mutated and then you have a new
generation of programs and the amazing
thing is that starting from random bit
strings the
Canaries have been able to develop
things like radios and amplifiers better
in many cases than the ones that were
designed by human engineers in fact that
you know these albums have actually they
have a lot of patents literally they've
won patents for the things that that
they produced
now John Cruz's idea was to say well if
what we're trying to do is evolve
programs you know bit strings are a
little bit too low-level
let's actually just evolve the actual
program trees right a program is just a
tree of subroutine calls all the way
down to simple things like additions and
multiplications so let's actually have
trees representing their programs and
then we pick a random node in each of
the tree to do the crossover and then we
swap the sub trees and evolve new
programs that way so for example here if
you pick this point as the crossover
point one of the trees that you get is
the one in white and that tree is
actually one of Kepler's laws that that
tree represents one of Kepler's laws
which is the one that gives the the
duration of a program of a planet's year
as a function of its distance from the
Sun okay and you know this is very easy
for these genetic algorithms to to
evolve and you know they do they do much
more complicated things as well for
example these days people like hard
Lipson are actually busy evolving not
just programs anymore but real hardware
robots so this spider that you see here
is a natural robotic spider from Howard
Lipson slab and the way it works is they
start out with random parts of piles in
simulation once the robot in simulation
is is competent enough it actually gets
3d printed and then the robots start
moving around flying you know they've
also evolved dragonflies or you know
dragonfly like robots for example and
then in each generation the fittest
robots get to program the 3d printers to
produce the next generation you know
this is happening right now it's very
exciting maybe also a little bit scary
right so if Terminator ever comes to
past maybe this is this is how it's
going to come about now the patient's
they are known in machine learning as
the most fanatical of the five
cribes and the reason for that is that
they used to be a persecuted minority in
statistics but and you know so they had
to become very you know fanatical in
order to survive and it's a good thing
they did because they have a lot to
contribute so here are some of the more
famous ones David Eckerman who the Pearl
Mike Mike Jordan and the Bayesian
learning is all about something called
Bayes theorem Bayes theorem is one
simple rule for updating your beliefs
with new evidence and and Asians love
base theorem so much that there was this
vision machine learning stuff that
actually had a neon sign a huge neon
sign of Bayes theorem made and hung
outside their office for everybody to
see so so what does Bayes theorem say
Bayes theorem actually is just the
following idea I'm never sure of my
knowledge when it's learned from data
right I'm always uncertain so I have to
quantify that uncertainty using
probability and then what I do is I
start out with my prior probability that
each hypothesis is true that's the prior
right there and then what happens is
that I see as I see evidence the
hypotheses that are consistently the
evidence become more likely and the ones
that inconsistent become less likely
that part is called a likelihood and the
product of the two is the posterior
which is how much I believe in each
hypothesis after I have seen the
evidence and hopefully as I see more and
more evidence some hypotheses eventually
become you know hopefully eventually one
hypothesis becomes the clear winner but
that's not necessarily the case and
there's also something called the
marginal which just ensures that the
problem is adapt to one and we didn't
worry too much about and again Bayesian
learning is used for all sorts of things
these days your first self-driving car
will probably have you know a Bayesian
learning system inside it and one
application that we're all familiar with
is spam filters so in spam fill in spam
filtering the the the two hypotheses
that you're looking at is this email is
spam versus this email is good and then
the evidence is the content of the email
so for example if the email contains the
words free in all capitals that makes it
more likely to be spam if he contains
the word viagra that makes it even more
likely to be spam
and if it contains free viagra for
explanation marks it's almost certainty
be spam on the other hand if it contains
dynamic of your you know best friend on
the signature line that makes it less
likely to be spam and these days people
use all sorts of different algorithms to
do spam filtering but the first one and
still of the best and most widely used
is Bayesian learning
now finally the analogize errs this is
actually a lot of people you know like
that I've met that they have read a book
they said like 'hey-- this is what I do
I reason like this I'm an analogized so
so what did the analogy is do do you do
the analogize is is that everything that
you do everything that you learn is
reasoning by analogy and you know the
most probably the most famous analogizes
is vladimir vapnik another one is peter
hart douglas hofstadter which you may
know as the author of gÃ¶del Escher Bach
and other books he actually coined the
term analogize and his most recent book
is 500 pages arguing that everything
everything about learning everything
about intelligence everything we do is
just reasoning by analogy so what's the
idea here it's actually an incredibly
simple yet extremely powerful idea let
me illustrate it by means of a puzzle
suppose I gave you a piece of paper with
will that's that's the map and what I've
marked on that piece of paper is the
locations of the cities in this one
country you know I'm calling it positon
because it's the process and another one
which are the the you know you know the
the minus signs are the cities in
megapolis so in sorry in in Eglin okay
and and and what I ask you is knowing
just where the cities are tell me where
the border between the countries is and
now of course you can't do that because
the city don't determine the border but
you can roughly guess where the border
might be and the simplest analogy based
algorithm is called nearest neighbor
just uses the following heuristic it
says a point on the map is in possessed
and if it's closer to a positive city
went to any negative one and vice versa
so for example when this type of
learning is used for medical diagnosis
and
it's very good at what happens is like
I'm a doctor I see a new patient what I
do is I look in my files for the patient
with the most similar symptoms and then
I assume that the diagnosis is the same
and this may seem very simple-minded but
there's actually a theorem that proves
that if you give this how with them
enough data it can learn you know
basically anything now there's a couple
of limitations of this one is that that
boundaries a little bit jagged is
probably the rule owners probably bit
smoother and also we're remembering here
a lot of these examples all of these
cities they're actually not necessary if
for example this city disappeared its
neighborhood would be absorbed by these
are the cities and the boundary would
actually stay the same so I could
actually throw this away and they're
away and nothing would change which here
it doesn't seem like a big deal but on
you know in this there's a big data that
could be a huge amount of memory and a
huge amount of time to try them out so
support vector machines or kernel
machines this is vladimir vapnik the
invention this is one of when the most
powerful kinds of machine learning there
is what they do is they solve both of
these problems the support vectors are
basically the examples that you need to
remember because they support the
frontier if you take one of those out
then the definition of the other of the
concept changes and the way they do this
is by you know trying to draw a line
that has all the positives on one side
for example all the cats on one side and
all the negatives for example the non
cats on the other side
what's the while keeping as far away as
possible from each of them trying to you
know maximize what's called your margin
of safety imagine that these were land
mines and you need to walk between them
right you would give them a wide berth
and that's actually what what this type
of learning this and analogy based
learning is with one of the oldest and
most were these types of machine
learning but one application is they
said it's very important and we're all
familiar with is recommender systems all
right these are the systems that every
e-commerce you know site worth its salt
uses to suggest products you might want
to buy based on the products that you've
bought before and a famous example of
this of course is Netflix three-quarters
of the movies that people watch on
Netflix come from the recommender system
important assist to their business and
on Amazon a third of what people buy
comes from that the recommender system
so how does this work well the idea is
very simple is how can I predict whether
you will like a particular movie or not
so that I can write recommend it to you
what I do is I look for people who have
similar tastes to yours and the way I
figure out whether they have similar
tastes of yours is of course that
they've given similar ratings so if
someone give high ratings to the movies
that you give high ratings and low
ratings limits that you give the Rings
and now there's a new movie that they
just gave five stars - but you haven't
seen yet then I guess that maybe you
like that movie and I recommend it to
you so to summarize we've seen that
there are five main schools of starting
machine learning what I call the five
tribes of machine learning each one of
them has a problem that that it's really
focused on over over over the decades
and has solved very well for for
Symbolists it's the problem of
discovering rules that you can compose
in arbitrary ways and their solution is
invested duction for the connection is
it the credit assignment problem and
their solution is backdrop for the
evolutionary is what they can do is
discover structure right the
evolutionary is just tweak the weights
on an existing structure but the
evolutionary's can actually come up with
that structure using genetic programming
for the patient's the crucial problem is
uncertainty how do you deal with
uncertainty and their answer - there's
probabilistic inference and finally the
analogizes by reasoning by similarity
they can learn from many fewer examples
than anybody else
for example from just - you can actually
form a frontier or you can generalize
you know across very different things
like generalizing from the solar system
to the atom which is how Niels Bohr came
up with the first quantum mechanical
theory by saying well the nucleus is
like the Sun and you know the electrons
are like the planets and so on but
really so this is the state of the art
today but I would say that what we
really need is the single algorithm that
solves all of these problems
precisely because each of these problems
is real no single approach actually has
everything that we need and a bunch of
us have have been working on this for a
while we've made a lot of progress we're
getting close to what you might call it
and unified theory of machine learning
in the same way that the standard model
is a grand unified theory of physics and
you know the central dogma is a grand
unified theory of biology and and so on
but I think we still have a ways to go
and and my guess is that we're going to
need ideas that are beyond well what
I've described here and in a way
paradoxically it's perhaps people who
are not in the field who will most be
able to have those ideas because again
when you're in a field you start to
thinking along the lines of that
paradigm and often you don't see the
other things that you could be doing in
fact one of my goals in writing this
book was to get other people outside
machine learning to start thinking about
these problems and maybe have ideas so
so if you do have an idea please let me
know so I can publish it so let me just
conclude by mentioning some of the
things so we've already seen some of the
major applications of machine learning
today from recommender systems to spam
filtering you know to speech recognition
and image recognition and so on but let
me mention some of the things that we
are working on in the research labs and
that I think will be possible once we
have you know this the Select the next
stage of machine learning one of them is
his home robots right we would all like
to have robots that you know cook do the
dishes make the beds you know maybe even
you know look after the children and and
there's this I think we don't have this
yet because it can't be done without
machine learning and it's beyond what
today's machine learning Albums can do
but it won't be beyond you know for
example this you know unified learning
realm that I just talked about another
one is something that all the major tech
companies are working on which is what I
would really like to do when I'm on the
web is not type keywords and yet back
pages is to ask questions and get back
answers but in order to do that I need
to represent the knowledge you know
that's basically in text and web pages
in a way that the computers can
understand and then deal with the fact
that it's very messy and incomplete and
contradictory and again I think this
this is something that that that we need
this the next generation of machine
learning to do here's another one
perhaps the most important one and
that's curing cancer medical diagnosis
is actually one of the killer apps of
learning I can take you know a database
of a thousand patients and run a very
simple learning algorithm on it for 10
seconds and it learns to diagnose for
example lung cancer or breast cancer
from from x-rays better than people who
spent years and years in Mexico or you
know if I want to recommend the drug
right it's a little bit like
recommending you know a movie you know
or or or a book but the problem with
cancer that makes it much harder is that
cancer is not one disease
everybody's cancer is different and the
same cancer mutates as you go along so
it's very unlikely that they will ever
be a single drug that cures cancer the
cure for cancer is is a learning a
machine learning system that looks at
the genome of the patient the mutations
in the tumor the patient's medical
history and so on and recommends the
specific drug that will kill that cancer
without harming the patient's healthy
cells and for that we need to build
models of how the cell works how gene
regulation works and you know there's a
lot of people working on this today and
and and I think we will get there you
know but but it's you know it won't
happen overnight
and finally apropos of recommender
systems what we have today is each
company has it's different recommender
system based on the data from your data
has access to it has a very narrow
picture of you so in our Netflix you
know predicts your movie taste based on
your movie ratings Amazon predicts what
you want to buy best one you've done
there you know Facebook has a model of
you to predict what updates to show you
you know Twitter has one per tweets and
so on and so forth but this is really
not very satisfying what I really like
to have as a as a consumer is a 360
degree recommender system that learns
from all the data that I've ever
generated and then helps me with you
know it assists me with all the
decisions that I have to make it every
stage of my life you know not just
finding things to buy but finding jobs
deciding where to go to college finding
mates right it auto you know dating is
actually one of the biggest applications
of machine learning these days the third
of all marriages start on on the
internet and the matchmakers are machine
learning
so they're actually children a wife
today that wouldn't have been born if
not for machine learning okay but the
truth is is a very hard problem you
can't really just print you can't
predict whether people are compelled
from just their profiles but if you have
access to all their data if you have a
really complete picture of them then you
can do a much better job not just for
dating but you know but but for all
sorts of things so what about all of
this in the book that you have on your
table I hope that you enjoy it and I'll
take questions so the first question is
always difficult especially for
evolutionary book makers of algorithms
so let's go straight to the second
question just wait for the mic and if
you could say who you are and where
you're from
hi I'm Peter mr. Chris with jll
excellent presentation Pedro in terms of
machine learning what is your forecast
in terms of workforce algorithmic
replacement of human beings lots of
discussions from Brookings MIT and
elsewhere and what are your thoughts
there yeah I mean this is a huge debate
that's going on right now right it's
like you know what is machine learning
going to do to the world of work and a
lot of people are concerned that machine
learning is going to lead to massive job
losses right there's been these
predictions that you know half of all
jobs will disappear in the next ten
years or such I think I mean there's a
lot to say but let me just mention a
couple of key points some jobs I think
are going to disappear like for example
I think truck drivers have a reason to
be worried and you know car driver is
the most frequent profession in thirty
of the u.s. 50 states however I think
what's need to happen with with with
most jobs is not that they're going to
disappear is that the way they are done
will change it's really not i mean the
question i think we should all be asking
ourselves is what in my job can be
automated and what can I do on top of
that once it's automated
I can probably automate the boring parts
and actually then get on to doing the
things
I wish like I had the time to do so in a
way the way to keep your job safe is to
automate it yourself and and you know
here's I think a very good example of
this computer chess right so you know at
one point you know deep blue beat
Kasparov and now computers are the best
chess players in the world right
actually wrong the best chess players in
the world they are not computers they
are teams of humans and computers a team
of a human and a computer can actually
beat a computer precisely because humans
and computers have different strengths
and weaknesses so this I think is the
way to think about it it's like
automation is like it's like having a
horse right if you have a horse you
don't trying to outrun them you ride the
horse and as a result you go farther so
I think this is the way that we should
be thinking about how machine learning
is going to impact force okay thank you
time for one more question
I won't ask if you're a thank you
sadly Lord with Herman Miller I've been
reading a lot lately about how
algorithms are not without by Hadley
Hadley Lord Herman Miller thank you I've
been reading a lot lately about how
algorithms are not without bias so it's
been talked about specifically around
Facebook in the way that they're using
those algorithms to recommend things
based off of your activity that might be
the most important to you but that those
leave out things that maybe are
important for you to know that maybe
they wouldn't predict based off your
past behavior as you start thinking
about these different algorithms and
creating of a master algorithm does that
kind of increase a specific point of
view or bias towards a certain solution
versus right now if there's different
types of algorithms did those algorithms
then have different types of bias so
that you're getting a little bit more of
a broader bias I guess you could say yes
I think it's very important to make a
distinction between ordinary program
algorithms that somebody wrote down and
learning our wheels a program out with
is basically just what you told the
computer to do so it inherits all of
your biases learning Gong is actually a
very different beast in the sense that
the learning algorithm itself the biases
it has are just very general things like
for example reason by analogy like I
described here it actually doesn't say
anything in critical about any specific
problem there only happens when you
apply their algorithm to data so in a
way in machine learning the biggest
source of biases are not the algorithm
itself it's the data that you train the
algorithm on and it's how the data
scientist use the algorithm so you know
we need to one of the important things
that I think needs to happen and that's
part of why we all need to be are
machine learning is that we need to be
able to scrutinize what these algorithms
are giving us in order that we can trust
them and we can say why did you
recommend this to me or actually that
was a mistake
you should have recommended that or you
should be able to tell them this is the
kind of thing that I want you to do for
me
what happens today is that the learning
algorithms they're running under the
hood you actually don't it's a complete
black box to us and what they're
actually doing is they are serving the
goals of the company that produced the
other like you know Google is trying to
mix em eyes the the click-through rate
on their apps and Facebook is trying to
you know keep you you know it is trying
to be sticky so I think we are you know
it's like driving the car right
only engineers and mechanics need to
understand you know how the engine works
but everybody needs to know where the
steering wheel the pedals are we all
need to know where the steering wheel
and the pedals of the learning
algorithms are and they have the
equivalent of that it's just that most
people aren't aware of that today
Pedro thank you very much I've got this
irresistible vision of corporate robots
in 40 years time having to in attend an
unconscious bias workshop that may
happen I think a wonderful start to the
conference you have opened up a world of
thinking from symbolism to analogy
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>