<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Francesca Rossi - Ethical Embodied Decision Making | Coder Coacher - Coaching Coders</title><meta content="Prof. Francesca Rossi - Ethical Embodied Decision Making - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Francesca Rossi - Ethical Embodied Decision Making</b></h2><h5 class="post__date">2017-09-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sHPkLYaSt7c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I know I missed some people but there'll
be a panel at the end so you can ask you
a question then we have professor Fang
Cheska Rossi she teaches at university
of padova and is a computer scientist
she's the president of the international
joint conference on artificial
intelligence and his associate
editor-in-chief of the Journal of
artificial intelligence research and
today she's going to talk about ethical
embodied decision-making
okay so yeah so the point that I want to
make with this talk is that in order to
help people make a very good decision on
close to optimality we need to have both
these features we need to help them in
an embodied environment and I will
explain a little bit what I mean by that
and we also need to make sure that the
process of decision making and also the
result of the decision making process is
going to follow some ethical principles
so in order to talk about this I will
start from a document that has been
published very recently two days ago
from the White House it's a document
that describes the strategic plan for AI
and it contains the description of the
goal of the strategic plan and the goal
is to produce AI which is beneficial to
society while minimizing the negative
effect and I think that this is the goal
of many of the events that we have all
been participating and many of the
initiatives included including the in
partnership initiative that I am part of
that was mentioned before and that I
will come back to it later on so I think
we can all share that goal and to
achieve that goal this document
describes the seven lines of seven
Direction several strategies one is
related especially to a funding agents
like a government should be which is to
have long term investments in AI
research because we need to understand
not just the short term implications and
results that we can get but we also need
to think as many people have mentioned
today we also need to think at the
impact in the long trail another one is
the issue of human eye collaboration so
it has to do you know with the fact that
AI systems will work with us together
with us in the environment in which we
function and I will get back to that
the third one is to try to understand
also the ethical legal and societal
implication of the use of a UN system
including how to make them aligned in
terms of values and ethics to our values
human values and ethical principles then
you know the fourth one has to do with
building AI systems that are safe and
secure in the sense of that they can be
reliable in our eyes they can be
dependable we can have trust into this
AI systems because otherwise we would
not be able to relate well in to them if
we not trust them and the fifth one has
to do with you know doing things in the
open a lot of many successful AI
approach is especially those based on
machine learning we know that our depend
a lot of the data that is used to train
those systems so it's important as these
data sets are public and available and
known also to build that trust that I
mentioned before the sixth one has to do
where you know evaluate how we are going
to evaluate a yeah and assess the real
you know capabilities of an AI system
and the seventh one of course is the
implication on to the workforce and in
this talk I will mainly touch upon these
three three strategies in the middle so
I will describe and stress a lot the
issue of human eye collaboration more
than the autonomous AI which of course
has a lot of value but in this talk I
will mostly talk about human AI
collaboration the ethical and legal and
societal implication of AI and how to
embed these ethical principles or
questions about how to embed these
ethical principle I don't have many
answers ok into this AI systems that
collaborate with yuppies and then how to
build trust into these systems so let's
start with human area collaboration so
the idea here is that the focus is on
systems that are now going to
place our intelligence but our going to
actually augment our intelligence so in
terms of decision-making which is the
area that have been work for many years
in the past the idea is not to make
decisions instead of humans making
decisions but to help humans make better
decisions and these can be done when a
single individual has to make a decision
we are going to help that single
individual to make a better decision
whatever it is that is doing whether in
his private life of professional life or
also helping a group of people to make
decision which is a bit more complex
task and of course in order to do that
you have to put together a system which
is not just a human it's not just an AI
system what is a human and the AI system
for a group of humans and the AI systems
together and as you see here that's a
typical example that is given that in
many scenarios for example in chess the
people say that most of the time you
know now it's recognized that humans
plus machines planes just play better
than the human alone on the order or the
AI system alone but also in more you
know useful applications like diagnosing
cancer it has been seen that the error
which can be small in both the human
alone or the AI system alone but if you
put them together it becomes even
smaller
this means that human the machines are
really complementary in their
limitations so they can help each other
in really achieving a better performance
in terms of for example or diagnosing
cancer in this way so I really think
there is a lot of interest of energy
from society a lot of beneficial
implication in trying to focus not only
on this but trying to you know
understand how to make in the best way
these human machine collaborative
systems and of course to have a natural
and effective interaction of course we
need to understand what it means you
know - for this human and machine to
work together and we need to understand
how to
model the human how to elicit is
preferences is opinions it's desires
what is God the goal that it has to do
we need to be able to have awareness
from the eye system of the context in
which the human is functioning and the
context not just in a static way but
also over time remembering what was done
in the past
and how decisions were made and what the
implications of the decision
consequences being so that we can
improve over time we we want system that
are proactive proactively providing
information to the human and not just on
request because they can help much
better in that way they can alert when
things are not going well for example on
the ethical side they can alert humans
and help them being for example more
ethical now I'm anticipating going to to
the ethical side but of course humans
you know that we have all our ethical
principles we may have all our ethical
principle but sometimes we're a bit
confused and we don't follow them just
because you know things are rewarded or
read the sign in a different way so
definitely there is a way for a human
area collaboration here and also they
have to be able to sense our emotions
our in a way of moods you know our way
of functioning that particular day in
that particular context so they have to
have some sort of emotional intelligence
and then you have to be able to interact
with us in a very natural way so
real-time dialogue interaction natural
language approach processing and so on
so this is the desiderata thinking of a
human area I collaborative system and I
claim that this disease data can be of
course we can try to implement this
desiderata into an app into our phone or
into a system that we can open laptop
and use it but actually I think the
future is not of people opening a laptop
and using a tool or people using an app
in only my phone either people walking
around in whatever environment they are
and that environment recognized
damn remembers their preferences and
opinions and observes over the humans
though sees if he needs help in whatever
task is approaching in that that
particular context and provides
information I'll help this women make
decisions so I think the best way to
help people make better decision is
within embodied environment and the
environment of course the fact that you
have an embodied environment is that the
environment is always with you wherever
you go the environment follows and of
course it is related to the internet or
things think where everything will be
linked to each other so you are not
going to close a little laptop somewhere
and then open it somewhere else
and then you have to say oh by the way
these things happen in the mean time and
then you need to remember this because
this is going to affect my next decision
but you know the information will always
be there video and there are also going
to be situations like a surgery room or
a cockpit of a plane in an emergency
situation so you humans are already in
embodied environment all the time during
their private or professional life so it
would be not reasonable to not exploit
this or embody the vironment that are
already there especially when you need
the real-time decision in a very fast
time and with you know critical results
and consequences so the environment
should be able to see and listen to the
human you know so perception
capabilities remember you know every
memory remember all the past actions and
decisions collect the human preferences
past in a dynamic way and possibly not
just monotonic and accumulating but
resolving conflicts because human can
change their preferences of opinion over
time and then exploit I think that the
embodied features on the environment
could really be exploited to have a
better natural interaction for example
we know that dialogue natural language
dialogue between a human and a system is
not really at a very advanced
lever question answering is but dialogue
is not really we cannot really dialogue
meaning you know several question one
after the other without having to repeat
the contest from scratch all the time so
the dialogue capabilities I think would
be much supported and helped and improve
the by having an embodied environment
because for example if this room where
you know an embodied environment where
you know with cameras they can see where
I am what I say where I look at where I
point then some pronoun resolution for
example that are not easy that would be
easier by just looking at where I
pointed where I look at so the
information in this inverted environment
where humans will have will be helped in
making decision will flow in two
different direction from the environment
which is going to provide you know to
provide data in the right format for
human to understand is going to provide
suggestions or possible decision is
going to provide capabilities for
resolving conflicts if it's a group
decision making like a hiding committee
so flows of information will go from the
environment to the humans but also
information will go from the humans to
the environment by providing you know
with my gesture with what I say it will
collect my preferences and will be used
during the decision that I have to make
now today but also tomorrow because will
be remembered over time so now let's go
to the other the other objective that I
use which is about ethics and of course
we don't want just humans to make better
decision we're better means more
efficient or more operationally more
close to optimality but we want it to be
decision that are going to be aligned
with our ethical principles and and when
I when I say ethical principles so it's
a new it's very vague word and probably
I use it in the wrong way because I'm
not a philosopher but I mean a very
why the set of things going from the
ethical principles that philosophers
talk about to moral values and so on but
also to social norms to some behavioral
rules that we usually follow in a
certain context or even professional
codes laws
traditions cultural based behaviors that
we have you know maybe you know and and
and so I mean many different things that
then will be part it will be specific
than will be specialized and in
particular context is and when I'm doing
one particular task and so it's
important of course that these
principles are not just stuck into the
system at the end when the system of
decision support system is already
constructed but they should be there
since the beginning of the design phase
so there is you know engineering part
that of AI safety that should be really
part of the whole process from the
initial design of the decision super
system to the end when the thing is
actually deployed to the real world we
know that is not that clear yet how to
embed ethical principles into these
systems of course it would be very nice
if we could write some rules
well-defined rules and then once we have
rules as was mentioned before of course
we can computationally write these rules
in a way that a program can you know
this language can describe and then we
can implement them and so on and then
the system can follow the rules but most
of the time is not like that first of
all because roots can may change and
also because we don't have rules but as
mentioned before we have mostly
principles like take a doctor a doctor
has to follow the Hippocratic oath when
he makes decisions the Democratic God is
very vague many things have not said and
so we don't have a set of rules that the
doctor has to obey every single minute
of his professional life it is
of course possible then to say okay I
don't have rules so then let's look at
the doctor let's look at the way it
works let's look at the way he makes
decision over time and then I will learn
more or less these principles and rules
by observing the doctor you know like an
inverse reinforcement learning idea and
this is also you know a possible
approach but I mostly think that the two
approaches should be combined because
sometimes you have some rules maybe with
a lot of holes here and there and then
you can fill the gaps in that but I
think that this is not enough and what
is needed is also something that we we
don't know how to describe which is
common sense and as also Ian mentioned
before you know we do many things and we
predict many things of behavior of other
people because we have you know we know
how the world works we have common sense
reasoning inside the during our you know
ear several years of life we have built
this common sense reasoning inside us
and nobody gave us some rules but we
just build them by observing others by
looking at the consequences of our
actions and by doing many many methods
to acquire this common sense and we
still don't know how to put machines
about to put it into machines so two
different doctors but following the epic
rat accord principles they maybe behave
in very different ways given the same
patient with the same history just
because you know they have maybe
different ways of implementing the
Hippocratic oath into their real life
you know professional decisions it may
also depend on where they are maybe a
doctor in Europe who behave different
than a doctor in in the United States
you know so there are many different
dimensions along which that make this
task complex but mostly in this AI human
collaborative system that I'm talking
about this ethical you know we need to
make sure that really there is value
alignment and the reason why we want to
do that is because we want to trust
these systems if we don't trust
this system and we not trust them at the
right level of trust then we are going
to have a very you know non optimal
results we don't want to over trust the
system of course because otherwise we
are going to expect some good behavior
when the system does not have the
capability to perform that way
but we don't want even to under trust
the system because otherwise we won't be
able to exploit the full capabilities of
what the system can give us in terms of
helping us make decision so building
trust is very important and it's not
something again that is static even
between humans you don't build trust
just the first time you meet somebody
you build trust over time by looking and
how that person behaves whether it's
reliable whether you can explain why is
doing something and why it's not doing
something and in this case if it's not
making the seizure
suppose the DI system is not make
anywhere just suggest indecision it has
to tell me why are you suggesting that
therapy would that patient and not
another one that I would suggest maybe
you have read more you know text you
know in scientific articles then you
tell me which which is the article that
is going to you know explain why you're
doing that and so the role of
explanation is really very important and
possibly over time when we interact with
this AI system over time possibly the
explanations will be less and less
important because once you build trust
then you understand that the system
behaves according to certain principles
and possibly the beginning you may want
a lot of explanations and then over time
we will we will just trust by requiring
very little information behind the
decision that is suggesting you to do
there is a lot of emphasis on
explanations especially on domains where
domains where you have some liability
like in the healthcare domain over there
is liability meaning that you know the
system the system human plus machine or
the human if is the one is going to be
liable as to explain why it did
something and as you know in Europe they
recently was a regulation that said that
by 2018 every
system that is going to make decision
who's that significantly affect the life
of some human it has to provide
explanations why that decision was made
of course explanation can be come in
everything and nothing you know you want
explanation that are understandable to
the target users if it has to explain to
the patient if you are in a healthcare
domain is one thing it has to explain to
the doctor is another thing if it has to
explain to the person would develop the
system is another thing so of course its
clinician may mean that different things
but the Gili nothing this is true for
some people think that this business of
building trust and explanation is mostly
to do with autonomous systems but I
really think it has a lot to do also and
especially with human machine
collaboration because in that symbiosis
of human and machines is where you know
you want a very effective with seamless
teamwork and without trusting each other
you will not be able to get that so the
ethical embodied decision-making putting
all these ingredients together is what I
think is the vision for the future the
intersection of these three areas
decision-making ai ethics and embodied
cognitive systems but in fact you need
research in any of the two sub areas of
two things alone so you need at research
on ethics in decision systems which
values you represent the values for
example in this area we have I've worked
with other people in this room and also
many others have worked on this in
knowledge representation decision-making
is mostly based on preferences of
individuals and groups so we want to
understand whether these models of
preferences the people have developed in
AI for many years they can be also
adapted and reused
also for expressing ethical theories
because ethical theories after all they
are defining more or less the same way
the preference er the funny which is a
partial or a total order over the
possible decision that one can make and
we also have to work on of course
embodied decision-making
so interaction in an embodied
environment and I didn't write that but
possibly there is also work to do in AI
ethics and the body cognitive system
without even thinking this is you're
making but in general I think is a very
multifaceted approach where besides the
technological part and the AI experts
part we need to talk also in a very
multidisciplinary environment I will
skip the part on the preferences and
more details but I can give the slides
if it's useful and then I will just
finish with some of the initiatives that
are ongoing the movie related to this
event and also to what we have been
talking today so this ethical how
embedding ethical principles in decision
support systems is also a subject of a
project that I am doing together with
other people
funded by the future of life Institute
and of course in that project we focus
on collective decision-making so helping
people make decision and we now focus
that much on embodied so we are working
in the top part of that picture that I
showed you but there are really a lot of
interesting issues and ideas there so
it's really an area a lot of scientific
advance and very promising the second
one is an I Triple E Global Initiative I
Triple E you know it's a very large
Association of Engineers and developers
all over the world and they are really
interesting into the issues that we are
interested in today all of us and they
are planning to you know think about
standards for designing systems that
have built that process ethical values
and so on another thing is the company
that I joined IBM I joined IBM one year
ago and I brought this interest into
ethics to into the company and I think
that the company supported that very
much of course iBM is not that it was
not doing things ethically before me
joining it has done things you know in a
responsible way for a hundred and ten
years but I think that now things are
more explicitly handled with various
initiatives white
papers and so on and and what happened
sorry
the last thing is the partnership on AI
so the partnership Eliza mentioned
before is an initiative that was
launched two weeks ago maybe with five
companies that put together their
interest into developing and deploying a
guy in the most beneficial way for
people and society that's the name
partnership of AI to benefit people and
society and the founders are five
companies but actually it is going to be
open to everybody non corporate
environments of any kind as scientific
associations professional associations
other companies that actually don't do a
guy but deploy to the real world for
example or even nonprofit organizations
and many many other stakeholders
everything that comes to mind is invited
to come and to discuss these issues of
what it means to deploy a in the real
world to develop it in the ethical way
and also to make sure that it behaves
ethically when it's deployed in the real
world
thank you very much for your talk my
name is Carla crime I am from Stanford I
think I think there might be a little
bit of a tension in what you're talking
about with respect to the embodied
environment and I'm gonna try and put
this all in one question although it's
sort of two questions
so in this embodied environment it seems
to me so I wanted to get your input on
whether you think there is something
lost in this kind of an environment
because you might think that humans are
sort of evolved that we have these
adaptive abilities to exist in a natural
environment and that when we transition
into this embodied environment that
there's maybe either a sense of
authenticity that is lost or some sort
of aesthetic value that is lost and and
the conflict I see with that is so now
I'm transitioning the second part of my
question your discussion of how we ought
to embed sort of not only moral
principles but even social norms in our
AI I think necessarily something like
the embodied environment violate social
norms because we don't accept that when
people stare at us 24/7 and deduce by
our little twitches in our faces what it
is we're thinking about so I think I
think there's a tension there that on
one side it sounds like you want to
maintain kind of a regular human social
norm environment but on the other side
this is a very artificial you know it's
in the name environment thank you I
understand yeah okay so well the fact is
that tools for the first question I
think I mean most of the time we are
inside an environment so I don't feel
this environment artificial with the
room where we are right now I don't feel
it as artificial because most of the
time I mean not in a room like that but
I rather in my office or in a meeting
room with other people or in a
conference room so
not suggesting to put people in a
special in a very strange environment
which is all dark and there I don't know
they have goggles to look at each other
or something but I envision in a
scenario where people are in their own
natural environment natural however
natural you feel is this conference room
and the room is a kid with possibly as
the least invasive things as possible to
actually support those people towards
individuals to do whatever task they are
doing in that environment already so
pilots are already in the cop cockpit of
the plane I'm not building another
environment for them and that's my whole
point I mean I don't want them to open a
laptop when there is a crisis on the
plane and they need to understand what
they do an emergency landing I want them
to the environment to help them so I
think this is actually more natural than
just you know having to you know use a
tool which is separated from the
environment so I don't see that as
unnatural because I want the normal
environment to support this
decision-making system and the second
part of the question which has to do
with violating social norms in a
conflict with social norms again I think
that you know when I think that there
could be that tension if the environment
becomes too invasive in order to help
more the decision process so I think
that yes there could be a way to you
know to draw a line to when the
environment is a support to the
decision-making and when the environment
is not I mean the support you know is
becoming too much and then the people
should be to be able to you know to use
their social norms including how they
interact naturally among them hi first I
just want to quickly just that point
games are about suspension of human
and it happens all the time and so
creating a game environment in the
embodied environment will I think
bypassed that problem and there's no
question of new social norms being
needed which is about human evolution so
the question I have for you is I heard a
focus
there's self human self to machine human
few to machine human many to machine
dialogue and that's the subject of your
focus now my question is is the
following also a subject of your focus
and your research many to many human
dialogue many to self or many to few
human dialogue where the technology the
embodied environment is facilitating the
existing human dialogue rather than the
dialogue being between the Machine and
the human yeah that's a very interesting
question so I think there is a role for
machines to help people resolve
conflicts possible conflicts in
interacting among them as I was
mentioning there is a role for machines
to alert humans or groups of humans when
there is some deviation with respect to
norms of laws or or ethical principles
or professional codes so in the same way
I think there could be a role in helping
them resolve you know interaction
glitches Francis came from Harvard
University I thought it would might be
useful for you to consider the an issue
that some philosophers have focused on
namely to what extent can one trust and
rely on the opinion of a so-called
expert when one makes a decision though
one doesn't understand and agree with
the expert so certainly when you hire an
accountant or a plumber okay you after a
while you see that their results have
been good you don't know what most about
it you don't understand why they're
doing this right now but you
with their view but some people have
argued that in the area of ethics it
would be wrong morally wrong to trust
and rely on what has even been an
dependable trustworthy ethical system in
the past if you still do not agree with
that entity they provide you with an
explanation you don't see why they think
that is an explanation some people have
argued that in the area of ethics in
particular unlike other areas of
knowledge it would be morally wrong to
do what you in good conscience do not
think is the right thing to do
and I'm just wondering if that will
affect the degree to which we can rely
on let us say the ethical expertise so
to speak of an AI system there is a
whole literature on this and I just
helpfully I hope it will be constructive
yes of course I agree that you know in
what's important is that the system
helping you make a decision
maybe alerts you that that decision
involves some ethical dilemma and it
tells you you know how it explains you
why is suggesting you to make a certain
decision because it's following some
ethical principles you may not agree and
the important thing that you are aware
the line of reasoning that that that AI
system is is using in order to suggest
you that decision but the final
decision-maker
is the human and the human is supported
in the in his decision even if the
decision is different from what is
suggested by the AI sister but the AI
system has helped the human at least to
to to describe to structure the space of
the possible decision and then to
suggest something according to the
ethical principle that the AI system s
but maybe it's not the same as it maybe
not the same social norms or same of the
doctor but the doctor at least as I
think even in that case it has been
helped in making a decision even if the
decision is different from what is
suggested so yeah I mean it's very
interesting and I would like to see
moral conduct</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>