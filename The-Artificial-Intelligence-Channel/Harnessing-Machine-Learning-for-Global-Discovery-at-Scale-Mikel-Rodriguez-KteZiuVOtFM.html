<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Harnessing Machine Learning for Global Discovery at Scale - Mikel Rodriguez | Coder Coacher - Coaching Coders</title><meta content="Harnessing Machine Learning for Global Discovery at Scale - Mikel Rodriguez - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Harnessing Machine Learning for Global Discovery at Scale - Mikel Rodriguez</b></h2><h5 class="post__date">2017-12-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KteZiuVOtFM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">here for the mic on first of all thank
you very much for having me and for the
organizers of the workshop it's a
pleasure to be here a lot of familiar
faces I thought you know for the past 15
years my main research passion and
interest has been computer vision so I
thought I spend some time sharing with
you some thoughts about how can we
harness machine learning in particular
computer vision to a label enable us to
do global discovery at scale we're in
the situation in dr. honey just touched
upon this right now where you know we
have this explosion an amount of data
and we're also in a situation now where
we're you know conflicts can essentially
arise at any moment and with that much
warning so this has really challenged us
to how do we think about collecting
intelligence and at the same time this
is another thing that's happening right
now we have I think that Joe Mundy
touched upon this last time you know
we're also in a very exciting moment you
know there's a lot of challenges
associated with being able to to collect
intelligence at a global scale but it's
also a very exciting moment because of
this kind of emergence of commercial
space and I wanted to touch upon a
little bit but one of the key things
that we keep thinking about is you know
we were in the situation where you know
our adversaries are continuously now
escalating conflicts and these kind of
subtle decentralized ways that are
usually only only perceptible really
when we look at things that are much
larger global and temporal scale so
that's where I really feel that is a
great insertion point for things like
computer vision machine learning when we
really need to scale out and focus
attention of individuals so as Joe Mande
talked about yesterday we're also in
this very exciting time where now we
went from the situation where we had
essentially one company in the
commercial space that was imaging about
5 million square miles a day so now
we're in a situation where we have tens
of companies that are you know imaging
200 million square kilometers a day and
you know there's companies like planet
that are looking at imaging the world
once a day which is incredible and it's
not just the number of pixels we're also
seeing a lot of diversity in the
modalities so this is really exciting
because you have things like a
Anthony hooks yesterday showed some
video from Sky box and this is
fascinating the first time I started
working with this is like wow we have
commercial just video from space we also
have you know other kind of sensors also
Joe mentioned synthetic aperture radar
so these are it's it's exciting to see
these developments we've seen this
explosion and in diversity and in the
spectral space as well now at the same
time of course we have just the prep you
know they also massive number of
uncooperative in non-traditional sources
that are just out there I'm still
stunned to think that's four hundred
hours of video are uploaded to YouTube
every minute I know so it's just it's
stunning to me and a lot of these things
have things like geo tags and so so the
question is how can we harness these no
uncooperative non-traditional sources to
help us focus our attention also focus
are kind of more exquisite
government-owned assets that are usually
just so distraught views how can we use
these uncooperative non-traditional
sources that are out in the open to help
us focus our attention so that's kind of
what I wanted to explore a little bit
today know we we've talked about this a
little bit yesterday I think in every
presentation a little bit but you know
as a computer vision community up until
recently we were in trouble we really
didn't have much to offer we've been
working on this problem for quite a bit
and but but we didn't have something
that was very robust for say object
detection and you know this is
surprising sometimes because you would
think that we were sitting on this
amount of data in in in the sense visual
data can be thought of as the biggest of
big data is right you have the four
hundred hours of video that we just
talked about we have you know the 300
million photos that are posted on
Facebook each day in fact Cisco has a
study that shows that close to 90
percent of all net traffic is soon gonna
be visual so you think hey can't you
just brute force this problem you have
all this data so what's what's why are
you complaining so a colleague Pietro
Perona likes to call this mass of images
and videos that we have out there
it's just digital dark matter right it's
just sitting there no one had even seen
most of it no one knew how to index it
or effectively reason about it in any
way so so that was that was the
situation we found ourselves in and of
course the problem really resides and
I mentioned this you know much more
eloquently I could put it but
essentially it was really a problem of
representation you take an image such as
the one you see over here it's made up
of pixels right these lifeless
meaningless numbers so as he said is we
essentially weren't this kind of
boutique you know business where we were
doing all our features and our different
classifiers you know I and we were all
in this business so it's it's this is
where we're trying to go from these low
level numbers to meaning and that in
this event was always the challenge now
again as people mentioned yesterday
these things started to change a little
bit this is a video here of young
lagoons showing one of his early you
know convolutional neural nets that
essentially is doing you know digit
recognition and what was interesting
about this is that it's an end-to-end
system right that's not doing the
feature engineering it just goes from
you know the data all the way to the
labels so the story's been told a
million times so I don't want to dwell
on it too much but you know yawn is now
you know a big guy and deep learning him
I'm honored that we have Rob Fergus here
as well another you know eminence in
this area so these people have had a
tremendous influence on the field but
you know what I would a lot of us did
not see this coming
this this revolution and it's and I was
really I was very interested in hearing
Rama's reflections about how how do we
as researchers react to that and what
would is our our way of embracing that
you know it to me it's funny because
before I before my current position I
was actually in a lab that you know John
would visit us all the summers and he
would every single day you know tell us
you need to start looking into these
deep learning approaches and here he is
this is a classic picture here's
convincing everyone trying to do this
deep learning stuff and I'm all the way
off to the right over here this back
when I had here completely ignoring him
and perhaps having a little too much fun
in the French countryside drinking port
or something like that but so the point
I want to make is that you know I did
not see this the the you know the
influence of this the set of deep
learning approaches come even when I was
right in the middle of it you know so I
was listening to ROM mind doing SPMS
just getting here but
I think it's okay that's right that's
that in fact that's the one of the
biggest things that most people do but
um so at any rate it really didn't it
didn't hit me until I was with the rest
of the people and seeing geez well my
gosh we see this big jump in our and our
ability to recognize objects when you
know this method was used for the first
time and one of the images met
challenges and of course Anthony I think
talked about this really nicely he added
another part to the stool of why now why
is this happening now
you know people usually cover the yes
there was the massive amount of label
data there's 50,000 people it took to
create image net I was one of those just
as a graduate student labeling things
and contributing so that was one piece
of the puzzle the other one was compute
right well now we have these GPUs that
are you know relatively cheap and able
to do you know amazing computations and
I like that you had and at the end and
yesterday I think we need to acknowledge
more often is just the open nature of
our community so I don't have that as I
didn't updated my slide after talking to
you but I think that's a great and great
thing to acknowledge so that this
created a Cambrian explosion you know so
it's you see all these different methods
and algorithms they're all doing using
similar DNA of approaches but they're
doing everything from you know coral
reef classification overhead
classification you know surveillance as
part of the as part of the Maven program
we were recently going out to the valley
and just talking to all these different
you know companies and it's amazing the
under the hood they're all using you
know the same kind of set of approaches
very similar at least so it's it's an
it's amazing to see that so there was a
diaspore of these ideas that kind of
started out in academia and that quickly
went into industry and you know National
Labs so what's exciting is to see okay
what can we do with this now for you
know global discovery at scale so if you
can do things like object classification
at a reasonable accuracy we could we
could really imagine now beginning to
harness these sources that are fairly
noisy and and and uncooperative so you
know things like social media which I'm
excited to hear it will be a couple
talks from right after me if you're
talking about the the pixels themselves
you could really start to focus
attention rights and
now this is you know yet another pipe of
data and as dr. honey said we need to be
careful if we're introducing yet another
problem actually so you really need to
look what can we do with this there
would be a value and what we're seeing
is that you can yes you can take that
you know this very noisy source and you
can start finding objects and events of
interest and I think Joe Mande said
something interesting was also resonated
very strongly if you take these
detection zin
individually it could be dangerous
sometimes because you know it's going to
make mistakes so it's interesting to
start aggregating these results to do
things like indicators and warning to
give us kind of a global global coverage
and start understanding the patterns in
some of those detection x' so you're
seeing over here is just
of different military targets on social
media and you know using exactly what
Rama just mentioned you know sometimes
you could bootstrap your method using is
this is a you know classic transfer
learning bootstrap using imagenet and
then you have some you know maybe subset
of labels that are are not as large as
imagenet but then you could do things
like aggregate these detection that
might be slightly noisy and provide you
some insights from those from those kind
of large scale data sets now the the
other thing is we're seeing us a lot of
commercial companies are providing
things that are of great value right so
orbital insight is you know doing things
like urban development where the roads
where the vehicles we have to think
about what are the things that they're
not providing or the sensitive analytics
that we need to invest in as a community
that are not going to be things that we
could just buy right so there's gonna
there's some interesting discussion
there what are the the particular you
know layers that we could probably only
provide ourselves because there's no
economic incentive to do so
there's a lot of amazing work in doing
things like semantic segmentation where
they're using things like space net the
equivalent of image net but for the
overhead but we need more data for this
and it's labeled and this is an area we
would really need we need to invest more
in so you look at all this stuff and you
think gosh this looks pretty good or can
we all go home now I mean it's I mean is
it all just a good story we were all
recently capr and and you could get the
impression that life is pretty good in
deep land right you've got all these
students that are doing they're you know
they're classifiers training they're
they're deep learning systems and then
going off and selling the startup and
making millions but there's still some
hard challenges that I think we really
need to address in particular to our
community over here I think there's some
things that that really require some
some some dedicated research one one of
them is the fact that you know the world
has this kind of heavy tail distribution
right so you have these few categories
that are very common and if you notice
you know like you know the Google object
recognition app is really great at these
type of things you know the car chair
you know monitor but there's only really
a few of these that where we have that
you know abundant number of examples a
lot of
we're interested in things that are only
have a few handful of examples these
rare targets or maybe some things you've
never even seen ever so so those are the
type of challenges and again I think
Anthony said this really nicely
yesterday was that you know we should
focus on the things and not everyone
else is already funding so so this is
one area where I think there is a
promising work in fuchsia and zero shot
learning but we need more so just get to
illustrate this example there's some
amazing work out there we'll take an
image and it'll basically spit out a
sentence a full sentence description and
you give it images like this from the
internet it's like oh my gosh this is
looks like it's a solved problem you
know it says the car parked next to the
road okay I'll give it that it seems
pretty pretty close when you realize
that when you just type car into Google
this is the most common thing you know
cars next to you know basically parked
on the sidewalk so really these these
approaches are great for the most common
of scenes as soon as you give it
something a little bit strange you get
some you know pretty out of the ordinary
results or predictions which is you're
saying down here same goes for this one
over here for things that it hasn't seen
very often they degrade pretty not very
gracefully but um so so how do we deal
with these instances where we have a few
examples on hand again as I mentioned
there's the community is already
starting to look into this there's work
and you know a few shot and zero shot
learning but but this is an area that
I'm hoping that we could support some
more I won't blow this too much this is
everyone probably in the room knows the
challenges I mean this is the Holy Grail
trying to how can we leverage on
labelled data but I just wanted to point
out that there is some incredibly very
very creative work that people have the
community there's a lot of folks are
probably familiar with the kind of the
generative adversarial approaches what
they're saying okay I don't want to just
learn how to classify the data I
actually want to be able to understand
how to from scratch essentially say
paint me an image of a tank and really
truly understand what that means
people mentioned synthetic data
yesterday I think that's another
interesting intersection how do we
actually mix the work from kind of some
of the generator approaches synthetic
data and maybe handful of real examples
to to create a strong classifier there's
also this is I wanted to point out this
is to me a very interesting work that's
been happening where
I think we as we could really leverage
some of these ideas that idea the
concept is and we're just going through
this right now in the end the Maven
program we're spending a considerable
amount of money to label data and we an
image net I said we had about a 50,000
people working to label data so we can't
really expect every new problem that we
have in I see then we're going to go off
and have 50,000 people annotate data for
us so we need to get creative about how
do we use our current datasets that
actually might have hidden signal
supervisory signals that we can actually
use to train train algorithms let me
indicate what I mean by that there's
work for example that says I could
potentially use a movement of the camera
for example if I know some of those
parameters as the label itself so a lot
of our sensors we have a lot of metadata
that could potentially be used as is a
surrogate for not necessarily having to
have 50,000 people label this we can
actually potentially already leverage
some of the metadata in our datasets to
to bootstrap some of these approaches so
that's something that I think could be
very interesting for us to look at the
other thing that I think has also been
mentioned is we have this you know
incredible diversity in terms of our the
sensors that we have that doesn't have
that much attention in this a CPR
community there was this year a really
great workshop at CPR about a computer
vision beyond the visible spectrum but I
think there could be a lot more work
that could be done in this area Anthony
mentioned this yesterday I think there's
an impression that you see these great
results from ground level and then you
simply doesn't work usually for overhead
let alone for spectral so again there
was a pretty great workshop as part of
this year CPR so another challenge that
we face is deployment so we you know we
especially in DoD I see you can't expect
to have especially at the edge you can't
expect to have you know a supercomputer
or you know dgx one where you could just
drop your deep learning system so we
need to get creative about you know we
have this basically this deep learning
crank that people keep using right and
the great thing is that it's very useful
for a lot of things you could say okay
pass an image is it a you know
a car or tank you could even use things
for you know semantic segmentation or
even for overhead but you know they're
all using similar kind of convolutional
based approaches and the good news is
that these things are getting better and
better but the bad news is that they're
also very expensive in terms of
computational you know so you know this
is about you know twenty billion flops
of just for a forward pass to in to make
an inference on one image so you know
these things are expensive not just in
terms of dollars but also in terms of
just memory in power so there's a lot of
interesting work that's been happening
and how do we how do we optimize this to
be able to deploy these systems at the
edge so there's work and having to do I
know you can't read this here but this
is essentially there's a there's a
there's a lot of interesting work in
having to say there's a lot of
redundancy maybe we could compress some
of these methods there's a lot of work
and having to do how can we improve the
forward pass and so there's a lot of
good research that I think we could
start incorporating and the bottom line
is that they really are starting to find
a way to reduce the both the the compute
and in the power constraints for some of
these approaches what that allows us to
do then is that you can start actually
deploying these object recognition
systems and fairly you know cheap
hardware this is this is me just showing
you can actually just as a proof of
concept you could put this on a
Raspberry Pi you know so now you know my
family which is very patient at me
cleaning you know experiments it this is
you'll notice now I'm able to you know
detect objects on this very pretty much
cheap hardware and where it gets
interesting is when you start
incorporating this as part of platforms
right so this here is just basically a
little scary but it's running just on a
DJI drone and it's just doing
essentially object detection and you can
actually integrate that into the PID
loop of the of the you know the control
of the actual platform so now it could
say I'm gonna anytime I see a silver
sedan I'll follow it or any other and
this is just running you know now you I
wish I could have brought it here we can
do an experiment but so the other thing
you could actually do so you could take
it even smaller this is actually what
the drone sees these are the different
convolutional layers and you're seeing
the different kind of the output of the
other detection
that's kind of the what's behind the
scenes you can actually you know really
miniaturize these things and this is
where it gets you know very interesting
you could start it's a balance between
how much recognition power do I need and
how much how much power am i drawing
from the device so really reducing the
Swap has been an interesting area of
research the other area that I think we
would be very interesting to put
additional resources in is that okay
you're able to tell me that there's a
car in this scene but I think Anthony
mentioned this again also we need more
work in complex activities and events
this is something that's gonna we need
more data when you data we need
additional research in this area because
it's it's not gonna work to just pass it
through a convolutional neural net and
just say vehicle and be done with it I
think one of the things you know as a
computer vision person I got excited at
one point when I you know I there was a
bunch of analysts say we're working with
him he said here's here's some video and
here's all these bounding boxes we
basically created another problem right
so we really need to that doesn't solve
anything
so how do we actually start modeling
some of these complex events and
activities and then we also if we're
going to be using these things at scale
we need to think about assurance and and
and and what are the the things that
essentially for the longest time nothing
works so we really didn't think about
security so now that these things are
actually beginning to get robust we need
to start thinking about counter I I
seriously and there is some there's
obviously a community that's been
looking at this and I'm hoping we can
embrace the research they've been doing
here I show just here's a particular
example one of the recent excuse me one
of the recent papers is a universal
adversary or perturbations and
essentially you could essentially think
about it as an Instagram filter on an
image that now at this point makes it
not being able to recognize by a by a
computer vision algorithm you could even
print that example and what you're
seeing on the right is completely wrong
labels that the system is now fooled by
by that simple kind of Instagram filter
so you know I think I think a ramen shop
mentioned yesterday how do we think
about noise and how do we in the
training process this is important for
us to embrace and really think about in
from a theoretical level because there's
all kinds of things you can achieve with
these adversarial approaches you could
do everything from just simple Mis
classification you can also do things
like
extract the training data for example so
even extract the model so we need to
think about this carefully from an
operational security but also just from
an insurance perspective so in
conclusion you know I think it's an
incredibly exciting time to be doing
work in this area I think we have some
very unique interesting research
challenges so I'm hoping we could find a
way to harness the you know creativity
and the work of the broader AI community
and how do we get them to help us with
our problems there's some particular
things that I think that we you know
should maybe a faster and focus on
things like you know rare targets are
unique modalities so I'm interested to
hear your thoughts on some of this thank
you very much for your time
thank you if only I'd listened to Jana
kun as much as you listen to Joe and
Anton yesterday now reason why computer
vision researchers were not into deep
networks in the late 90s is because of
the example right I thought about it a
lot M NIST is not something that Russian
people were interested in we were just
going into 3d at that time recovering 3d
representing 3d and so on so OCR was
seen as a solved problem in pattern
recognition document I you so if only
image net we're available at that time
and one of Yan students had done what
Alex did several several decades later I
think you know it would have been except
so there were that was one of the issues
right where the application is and
whether we think that's not going to
work for us at that time we could not
have done that
so that's one thing anyway nice talk it
was a good summary and I'm impressed how
much you remember of all the talks
yesterday I don't remember most of it
any other questions to please
there's no TV how much of some of these
new architectures are being used in
image processing whether I mean a lot of
the architectures that are being used
for image processing are based on one
normal architectures you know with this
non online architecture such as
neuromorphic computing this huge
benefits not just in terms of energy
but also in terms of acceleration for
image processing how much of that is
being exploited these days there's first
of all from there's there's a lot of
interesting work in other architectures
neuromorphic is you know one of the ones
that is in the timeline but but you know
I like the phrase people are gonna using
yesterday is about you know we're all
participating in this great deep
learning hackathon and there are what I
mentioned mostly today where is what
works the most which is the
convolutional neural net a deep
evolution neural net approaches but
there's also other you know very exotic
architectures that are that people are
employing but from the hardware side
yeah there's also you know embedded GPUs
that are you know looking at
particularly to constrain like power
power environments but you know and
there's the and there's a whole Wild
West of the neuromorphic stuff which is
yet to to really materialize but it's
exciting to follow
well yeah it's gonna be I I think it's
the idea right now it's in an
interesting time because yeah this is
kind of working but but we need to go
back to some of these basics and say
well if we're really optimizing for some
of these things how would we actually
build this and so it's yep Anthony hugs
really a follow-up comment maybe on that
question in the talks so far we haven't
really touched upon the contributions of
deep learning and cnn's to image
processing as opposed to say computer
vision but there are tremendous
contributions and utility for for cnn's
deep networks in this area they've made
huge strides in in things like super
resolution and all kinds of image
sharpening image enhancement image
transformations and this is because the
first few layers of the networks are
learning the signal processing basis
just like we used to hand design and
Gomorrha filter bank that was hand
designed is now automatically learned by
these networks and it can be learned for
your problem so it's not necessarily a
Gabor filter bank it's an optimized
filter bank that's going to address what
you're trying to do so for this
community in the IC if you look at space
imaging you always want more resolution
there's really good ways to do super
resolution now that are much better
using deep networks than before there's
much better ways to do various forms of
managed cleanup colorization was one of
them you cited chef roses work at
berkeley he's working with us
my company KITT we're on the media
forensics program at DARPA to look at
this colorization problem and use that
to detect that image has been
manipulated at the signal level and they
had some really nice work to detect
image cropping it was an image part of a
crop or not and you can do this with a
neural network it turns out quite
accurately we don't really know how it
works we don't even know what it's
actually doing this neural network but
we think it's looking at the slight
chromatic aberration that comes from the
misalignment of pixels as you move away
from the center of rejection of an image
you get a slight distortion in the RGB
alignment of pixels across an image and
this is regardless of the content of the
image and what it's showing and so the
CNN appears to be learning this these
these tiny chromatic aberrations of
we're in an image the crop was taken
from a co-op as a window of an image so
it's amazing what these things can learn
even at that signal level and they're
still using standard architectures to do
that in this community has a number of
specific problems in that area that we
could potentially invest in so my
comments are motivated from the security
side a very interesting talk and the one
chart that we resonated with me was the
chart that you talked about the long
tails for images now if you saw I don't
know if you can bring that chart back
but one of the things that that I
connect there from the security
perspective is you know how you go in
two different dimensions so if you go in
the horizontal dimension you talked
about how the IC community has very
specialized images that's really also
the the point that came up yesterday
that in the security space we have very
little ground truth what's the next
piece of malware going to look like what
it's going to do and so very curious to
know what kind of approaches you can
take when the number of images or the
ground truth is
small and that and and and and you know
what is how how does deep learning
evolved I mean so one of the approaches
we are taking is we can't profile the
ground truth so we have to profile what
is normal activity and then flag what's
anomalous now that may work for some
time but how do we push that forward the
other part if you go in the other
dimension which is the vertical
dimension of your graph you know you're
showing the accuracy rates and so on
but these accuracy rates are against
certain parameters for image recognition
which are essentially very brittle and
where you have to grow that is you know
exactly as you said you know move the
car up it's no longer part of the road
malware you make slight mutations slight
variance and now in fact two weeks ago
at DEFCON which is suppose you guys know
it's one of the premier security
practitioners get together white hats
black hats and so on they're trying to
use neural networks to actually train
mal to to synthesize malware and evolve
it in a way that would evade current
antivirus engines and that's part of AI
and counter AI so so that's so we have
to think a little bit I understand
there's a huge revolution with image
recognition and so on but I think we
have to abstract that away and say you
know ok what does this mean for the more
general problems around security and and
I think that that shot was very
appropriate for that so so I'm going to
end with one question which is counter
AI I mean I know you touched upon that
and count reassurance
can you talk in exactly I mean what what
are you seeing is happening in this
space because what's clear is
adversaries are using AI techniques any
AI tools for synthesizing new variants
of malware that's going to evade
security controls and we can't put it--
are like restrictions on AI everybody
can use you know
you know either tensorflow or a cafe or
and siano and so on so all that stuff is
out there addresses are using it so
what's happening encounter AI is that
something you could talk to it is an
area where there's tremendous amount of
work being done so so I completely
sympathize with your comments but there
is both on the offensive side defensive
side this is you know obviously a
cat-and-mouse game but but there is some
strong work that's coming out where
people are saying well in our training
we really need to pay attention to the
fact that people are going to be trying
to mess with this right so there is a
from that from the defensive side
there's some really good work and
adversarial training but there's also
it's tremendous it's I think it's
important for us to recognize what's
possible I mean it's it's really
shocking to see that if you have a model
out there and you've just you've trained
it on some data it's very easy for
people to extrapolate the data that you
train it on what is the model itself so
being clear about that as a community of
what's possible I think will help us
will help us kind of an operational
security side of things and also I think
there are some areas that are there's
not as much work because there's no
economic incentives necessarily per se
to work in that area right so so we need
to recognize those pockets and those
gaps and really and-and-and-and-and fund
research in those areas so so there's
there is already a community that's been
looking at it significantly but but I
think there are some gaps that we can
address
Pelayo fernandez is so a question is
that how useful might be synthetic data
such as gamers are realistically
creating the world around us and that
might lend itself to helping cnn's
and deep learning activities and
algorithms learn better varieties of
targets as opposed to having to worry
about having a bunch of labelers and I'm
not sure how valid that approach might
be and what the strengths and weaknesses
are of using synthetic data from my
gaming's so yeah there's a long history
in using or at least trying to use
synthetic data in computer vision right
so it's always been a balance between
how do you make something so it's not a
toy within that world but I think it's
there's some promising work
that's being done right now where you
want to essentially you know open the
eye does this all the time with it so
there's a lot of the folks in the
computer vision community is essentially
you could bootstrap your approaches by
having you know a big synthetic world
and Anthony mentioned this a little bit
they're using gear cig you could have
some do this similar for overhead so
there is this is this is this is
something that's emerging but it's a
tricky balance because at least from the
work that we've done
you can't over rely on that there's
something you know about the world it's
just very hard to capture in some of
these these simulations so it's a tricky
balance between how do we incorporate
that to alleviate some of the labeling
efforts but but it's something that I
don't know if we've actually cracked yet
yeah all right just to follow up on that
real quick I think the combination of
synthetic data plus adversarial networks
could be really powerful because ever
sterile networks are really they're
typically used to generate data in a way
that then can't be fooled by by one of
these surreal attacks and you can do
that with synthetic data generation you
could generate synthetic data and then
add noise or other things to make it
look realistic and you could learn how
to make it look realistic automatically
so that it now can't be fooled or
distinguished from the real data so
actually we got some funding now to try
this in the next few months using using
deer sick as the generator so that's
going to be quite interesting and
exciting and and back to the gaming
community it turns out that the gaming
community has I'm not a gamer so excuse
me if I'm wrong on this is probably some
serious gamers in the room but they've
created all these 3d models of Russian
military equipment hundreds and hundreds
of these models and they're openly
available 3d models with material
attribution and so on for rendering and
so that's what the deer's to people use
so they have these great models to
render to render all this Russian
military stuff that we might see in
satellite imagery and watch Chinese
stuff too but it's really the Russian
stuff that apparently is really popular
Anthony DNA be fake models
best advisor microphone sorry
my experience at NIST we've run a lot of
evaluations and gather lots of data
my experience with data is especially in
a environment and where you're going to
be using deep learning that's the
synthetic data has been generated by an
algorithm and what your systems are
going to learn is what the algorithm was
that generated that data and to the
extent that your data actually matches
the real world that will be okay but in
my experience it just doesn't right
there are always things about the real
world and the problem that you're trying
to solve in that real world that your
synthetic data is not going to capture
because it's not real okay any other
questions comments yeah please Kathy
Kathy McEwan Columbia University curious
we always come back to this question and
various programs that I've been in with
i ARP and DARPA which is the
communication or interaction with the
intelligence community and what kind how
we can be informed by analysts what
their current practices are and I'm
wondering here with data and
particularly as we go into a multi-modal
stage where we're going to look at
multimodal data whether there is the the
captioning one is a good example and
when you have the picture of the tank
and we have very little of this and sure
that there's a lot of this and I don't
know you know what is classified and
could be released but might there be
cases where there are both images with
accompanying text which might be reports
which I think that would be great data
to use to begin to talk about you know
action between the text and the image
for generating captions or generating
descriptions so forth so there was a
good study that came out of DNI that a
major issue study that I think addressed
exactly that there were we have on
different networks yeah is it exactly as
you said you know descriptions in and of
an image and why not harness that to
power some of these approaches and
unfortunately it's one of the things
that we've had a challenge is some of
these things are all just in different
silos of data that don't talk to each
other and we don't usually think of
harnessing those say analyst reports to
drive you know an AI system so I think
that your comments it's well taken I
think that's something that we should
start thinking about and then the other
challenge is how do we you know some of
these things are sensitive so how do we
still tap into the broader we don't want
to do this in isolation there's some
amazing ideas out in the academic
community in industry so how do we still
provide datasets that are relevant and
it can help us out as a challenge that I
yeah ramaa chilapa the data issues that
thing improving I don't think we can get
the restricted data because universities
won't even touch ITAR data but if you
look at one of the projects we are
involved in and the one that Anthony and
I will be involved in this from IR Pavan
it's a Chinese program and you know a
lot of data from the internet for faith
but it's a very specific problem face
for action recognition also I think the
data set is getting better but some
sensitive data I just won't be available
you know I mean that's always been a
challenge even when all three of us were
involved in a project known as radius in
the early 90s we were working with model
board imagery very very synthetic like a
little town you know with buildings and
little toy cars and so on because
initially major he could not be released
and off-site and so I think synthetic
data as it's false but as long as we
understand where the limitations are and
not assume that you know I think we are
now been afraid where we can't assume
anything is real in some sense right
because there's so much flexibility for
everybody else to go on and
change things so we have to be careful I
know we're getting close on time here
but yesterday Travis and the Maven
presentation mentioned this Trevor
treasure trove of data that that the DoD
and certainly the IC is sitting on so
part of that data is really what Kathy
alluded to for at least the full motion
video case or predator reaper type of
video or there's cameras on UAVs there's
always a group of people watching that
video in real-time and logging about it
they're logging in a few different ways
there's what's called call-out texts and
there's called
there's chat some of this is less
structured some of it's more structured
but it's always happening and it's it's
slightly stored in different silos even
within the enterprise of a given UAV
system but but it is there it has that
there's the call-out stuff is a little
more structured and it's been to me more
of a permanent record and then the chat
stuff looks like the text we saw
yesterday you know it's just text that's
got a bunch of abbreviations it has its
own language issues it's like its own
dialect effectively but that data is
there and it sits there for literally
hundreds of thousands of hours of UAV
video wouldn't that be awesome to take
this data and try to use it because it's
now video and captioning it's it's it's
not all the things that are happening
it's the ones of intelligence interests
a lot of stuff there was anything going
on but it's there and there's a huge
amount of it and it could be leveraged
in principle so that's mavens problem
right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>