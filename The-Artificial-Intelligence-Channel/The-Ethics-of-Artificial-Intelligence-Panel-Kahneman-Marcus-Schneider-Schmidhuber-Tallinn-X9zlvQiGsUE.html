<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Ethics of Artificial Intelligence Panel: Kahneman, Marcus, Schneider, Schmidhuber &amp; Tallinn | Coder Coacher - Coaching Coders</title><meta content="The Ethics of Artificial Intelligence Panel: Kahneman, Marcus, Schneider, Schmidhuber &amp; Tallinn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Ethics of Artificial Intelligence Panel: Kahneman, Marcus, Schneider, Schmidhuber &amp; Tallinn</b></h2><h5 class="post__date">2017-09-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/X9zlvQiGsUE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to our final panel
discussion the way this is going to work
is we've got a wonderful panel of five
people from a bunch of different fields
assembled to give us their perspectives
from a number of different angles I
guess in the end that turned out that we
have broadly speaking to psychologists
to computer scientists and one
philosopher this was assembled a little
bit at the at the last minute for
example yogurt and sugar just emailed me
this morning to say he was in town and
he heard an interesting conference was
on and I said great you want to be on
our panel Danny Kahneman just registered
for the conference about four or five
days ago and great handle for you still
we're still try to give it we're still
trying to convince Francis can but so
far she's Tom Nagel maybe anyway so
we've got a wonderful a wonderful group
here assembled they will each speak for
just five minutes or so to give some
perspective and some comments maybe
we'll take about just one or two
questions after every panelists to break
things up and then we'll segue into a
general discussion with questions among
the panel questions from the audience
they needn't necessarily be questions
only for the five people who are on the
panel we've got many of our speakers
from the conference nearby and anyone
else who's any speakers who are way back
there should feel free to come up so we
could end up having a general discussion
of the issues and we'll see where it
goes anyway for our first panelist we've
got a very promising young psychologist
named Daniel Kahneman who is doing I
believe is to be doing some very
interesting work suggesting that just
possibly humans may not be quite as
rational as we as we thought they were
but he's going to give us his
perspective on the conference
it's on okay good well that there's a
word that's been used a great deal in
this conference it's the word intuition
moral intuitions and and I was impressed
by at least how little that the use of
that word corresponds to what I think
about when I think about intuition so
there are intuitions
moral intuitions roughly of two major
kinds there is one about statements so
that when we hear the statements that a
capacity for suffering means that you've
that there are rights if if an entity
has capacity for suffering that is an
intuitively compelling statement and we
we tend to accept it so statements we
tend to accept or or reject
but there are also intuitions about
cases about events about real things
that happen and those are not about
accepting and not accept they are
largely about an emotional response of
indignation or more or less indignation
or approval or in some cases or to some
extent I think when we think about the
future of AI intuitions about cases
should be taken very seriously and they
should be taken very seriously because
of their political significance because
the public is going to respond to cases
it's going to respond to things like a
self-driving car killing someone in an
example that was given and and you know
from my perspective of course both kinds
of intuitions are terminally incoherent
I mean it's and they're incoherent
within themselves
our intuition about statements are not
coherent otherwise the ethics problem
you know would have been solved and our
intuitions about cases are not
hiren and there is no clear relationship
between our intuitions about cases and
about statements let me give you an
example so psychologists love the
trolley problem and which has been
mentioned here several times then why
psychologists love the trolley problem
in particular is because it is a case it
feels like a case and it arouses emotion
it's a very different thing than just
agreeing or not agreeing there is an
emotional response to these scenarios
and and the emotional response it has
several characteristics one of the
characteristics is conflict it's not
simple neither of these cases none of
these cases are simple and some of them
are really very very difficult to deal
with and I was thinking that when we
think of a self-driving car faced with a
trolley problem with a trolley light
problem one thing that is sort of
appalling almost horrifying regardless
of how the self-driving car solves the
problem is that it will solve the
problem instantly and there is an
element missing that is to solve the
trolley problem without feeling horror
and presumably that AI will not feel
horror there is something missing and
the response I think is going to be that
regardless of where the self-driving car
decides we're not going to like it there
is going to be a sense that there's
something wrong about that decision
regardless of what it was because it was
not accompanied by conflict
it was computed and then you know
whatever maximized whatever function is
what was
and so we're going to have a lot of
difficulty with that and there are going
to be it's an example the kind of thing
with intuitions about cases about that
event really do not necessarily
correspond to our moral you know two
well-defined system of ethics or to any
set up a particular rules one
characteristic of intuitions about cases
in particular is the dependence on
essentially irrelevant factors and
that's large extent is what makes them
incoherent so our focus I mentioned only
one and that's appearance what is the
role of the appearance of the agent
inner intuitions about the agents moral
behavior or moral rights and I think
it's profound I mean just imagine that
you have a bunch of lifelike models of
squirrels you know that they're models
but they are on the floor and then there
is somebody taking a hammer to them do
those little statues of squirrels is it
the same as somebody taking a hammer to
electric trains and I submit that there
is a difference because we feel very
differently about things that look like
living things that we associate with
living things that we associate with
emotion so this is going to make a very
big difference and it shouldn't now our
intuitions about the rights of AI agents
are going I think to depend critically
and whether they are designed to look
like us I find it very difficult I mean
when we are talking about a capacity for
suffering so let me go back to that the
capacity for suffering how would I know
that the sea is suffering
how would that I communicate to me its
capacity for suffering and I can tell
you you know I was sort of trying to
imagine that's a program a very
elaborate program have a capacity for
suffering now in a sense you know those
who say that an a I will have capacity
for suffering or giving a positive
response to this question and yet I find
myself in a lot of difficulty feeling
empathy for a program because I'm told
that it has a capacity for suffering we
depend very critically in our intuitions
about the rights of people about the
obligations about the duties we depend
very much on what they look like on how
they express emotion and how they
express their suffering on whether they
compellingly convince us that they are
conscious and convincing me that you're
convincing me that your conscious
depends a great deal on your looking
essentially like me I mean there is a
difficulty imagining the consciousness
of of something that I cannot empathize
with an empathy which is built-in which
is in the emotional substrate empathy
depends on irrelevant factors factors
that are not really or should not be
morally consequential so that's what I
was that's the puzzle that I was left
with that in a lot of discussions you
know notably this afternoon on the
rights of AIS and and so on every one of
these statements looked compelling you
know I felt accepted the intuitions
about you know what determines whether a
eyes have or do not have rights and at
the same time I wasn't at all sure that
my intuitions about
about the events would actually
correspond to those intuitions because I
wasn't sure about whether I would get
the emotional connection that is I think
quite essential to moral intuitions of
any kind
thanks for that uh so there's this
difference between what we think is
morally appropriate and then what we're
gonna empathize with and so respond to
but those things seem to me at least and
you know a lot more about this tonight
but they're extremely malleable so I
feel empathy for all sides of sorts of
weird sci-fi characters that are
described in strange ways or in movies I
feel bad for wall-e which is just I know
not is it just a representation of
something that's purely algorithmic I
know that it's actually not a real thing
but I feel sadness for it when I see
people kick Big Dawg which is this
robotic dog I feel like no don't do that
and and we have people that at least
claim that our sphere of empathy has
expanded over the years and so I wonder
if you think there's any prospects for
just changing ourselves so we'll feel
empathy for the ratings I think wall-e
had you know they were enough cues so
that it looked emotional you know I mean
you felt for wall-e you empathize and
that's those are some other conditions
for which are the kinds of beings with
which we can empathize that's going to
be the issue that's going to determine
our intuitions about cases which I think
in in turn is likely to determine our
political intuitions I mean the response
of the public to two cases and possibly
two rules take one more
I wonder what your thoughts would be on
two things first is what impact it would
have socially on humans if we were to
try to override the empathy we feel for
kicking a robotic dog for example like
if if we if we were to say it's fine to
kick a robotic dog because it's just a
robot
it's a harmful like for example we each
spoke the other day about rape in a
virtual reality world right and then the
detriment to potential detrimental
effects of that experience so that and
then also the just understanding the
relationship between AI and humanity in
terms of conflict a conflict of interest
like for example if you've seen the
movie X machine ax if Ava had been
treated nicely maybe she didn't have
this stabbed her maker and lock her
lover in a box well you know as in a way
it's it's similar to the previous
question our intuitions about these
things are indeed quite malleable and so
you know we have a lot of instances
where people view other people as not
having any moral rights and feel
absolutely no empathy for these other
people you know I mean there are
historic precedent their current
instances or that that sort of thing so
indeed this is malleable we can
desensitize whether we can with equal
ease sensitize that I'm not sure that as
we need a minimum of cues and in you
know in all the example that we have
including ex machina and wall-e there
are very good hues so that we're
horrified at the end of ex machina when
when she locks her lover and sort of
walks out
okay one book one story one question for
person
sorry I don't know what he's talking
about but anyway professor Kahneman
there might be a distinction between
hypothetical cases that are considered
in actual cases and I heard you two
thinking of talking about actual cases
because very often when you think about
hypothetical trolley problems you don't
have any conflict at all you may have an
immediate intuitive judgment which you
then try to give a rational
justification for in terms of a
principle which you deny there is but
anyway because the judgments are usually
about permissibility is it permissible
to do this I heard you talking about
what might be real cases you said if an
actual AI driven car runs into somebody
how will people respond and it may be
that when I'm in an actual situation if
I were where I had to turn the trolley
though I knew it would be permissible my
conflict will be about will I do this or
not because many people say it's not
obligatory to do it and in an actual
situation to decide on what to do is a
different thing that to consider a
hypothetical I'm wondering if that could
you know modify some of what you said no
I mean I when I was talking of cases I
wasn't talking of actual cases and I was
not taking the perspective of the
individual who must make the decision
and and from that point of view the the
perspective of an observer of the actual
cases or the reader of a compelling
description of an actual case that's
what I was talking about and in some of
the trolley problems indeed there is an
immediate intuition and it seems easy to
justify in other problems you know like
the fat man on the bridge
there is conflict and and there is and
the sensation is that you know this is
why it's it's become such an important
problem in in psychology actually in
social psychology the trolley problem is
because of that conflict
now I didn't deny the existence of
principles I was talking instead pause
when I was talking of intuitions about
statements and we clearly have them
I made the claim with which I'm not sure
you would agree that I made the claim
that these are probably not internally
consistent ok we should move on to our
second speaker who's Susan Schneider
from the University of Connecticut and
philosophy susan has done a lot of work
in the philosophy of cognitive science
and a philosophy of mind for the book on
the language of thoughts and a lot of
work in the metaphysics of mind but
lately she's been doing a whole lot of
really interesting work on technology
and the future of AI as well as
connected issues such as mind uploading
and the universe has a simulation and so
on which is very much worth checking out
thank you so I have five minutes I'm
gonna say a few quick things about
machine consciousness emphasizing the
ethics of it all so just so we're all on
the same page here's what I mean by
conscious every moment of your waking
life and even when you're dreaming
there's something it feels like from the
inside to be you so when you see the
rich use of a sunset or smell the aroma
of your morning coffee you are having
conscious experience so today a lot of
us have worried about whether a I can be
conscious I'm gonna emphasize why it
matters first as earlier speakers noted
suppose we create sophisticated AI say a
GI artificial general intelligence and
it is conscious then it can suffer and
feel a range of emotions but we've
created it to work for us and to force
it to fight our wars or clean our homes
isn't that just slavery and wouldn't
destroying it as opposed to just
temporary
early unplugging it be murder further
wouldn't developing a GI that was
conscious be an ethical muddle and not a
good investment now though suppose on
the other hand that AI is not conscious
I want to emphasize that this is a game
changer - for a number of reasons first
of all the media has been talking a lot
about being in a computer simulation
okay the good news is if AI can't be
conscious you're not in a simulation
right now because you would be AI if you
were in a simulation so congratulations
alright but other issues also um we
can't merge with machines in this case
the way people like Ray Kurzweil
envision repaint replacing parts of your
biological brain with microchips would
cause you to lose awareness and it's not
clear at all that you would be able to
survive brain uploading I mean if you're
no longer conscious then I think there's
a good argument for that upload not
being you but being a computational
duplicate of you at least in terms of
certain information processing
capacities and let's also think about
this issue on a more cosmic scale so
some say that the next phase in the
evolution of intelligence on earth will
be artificial intelligence you and I how
we live and experience the world right
now we are just an intermediate step to
AI synthetic intelligence will supplant
biological intelligence indeed in my
work with NASA I've urged that a similar
phenomena may have played out on other
planets already the greatest alien
intelligence is if they exist at all
maybe post biological being a eyes that
evolved from biological civilizations
like our own but if it doesn't feel like
anything to be an AI we had better asked
whether a universe full of intelligent
non conscious machine
is more valuable than biological life
like us even if it's radically less
intelligent than super intelligent AI so
I hope my quick blurb has convinced you
that the question of whether AI is
conscious or not concerns the very
future of humanity as I like to tell my
students when it comes to AI philosophy
is actually a matter of life and death
so so what can we do well I think we
need to figure out a way to test and
determine whether AI can be conscious
but I just want to emphasize this will
be difficult for two reasons first
earlier speakers pointed this out
already
we know biological beings can be
conscious and we also have a sense that
non-human animals are conscious because
their neuro physiological e similar to
us but the question is very difficult
how to know whether something that's
made of a different stuff entirely
microchips of some sort could have
experience but suppose for a moment we
do find out microchips are the right
stuff I want to emphasize an
unappreciated problem that emerges
consider super intelligence so again
super intelligent AI is hypothetical AI
that's able to out think us in every
domain social skills scientific
reasoning and more the problem is this
it may be more efficient for a super
intelligence or even just an AGI it's
not a super intelligence to eliminate
consciousness altogether
think about how consciousness works in
humans only a small percentage of our
human mental processing is actually
conscious at any given time most of our
thinking is non conscious computation
consciousness is correlated with novel
learning tasks that require slow
deliberate focus so think about how
focused you were when you first learned
to drive as opposed to how unconscious
you might be when you drive nowadays on
a familiar route a superintelligence
surpasses expert level knowledge in
every domain having rapid-fire
computations that could range over the
entire planet and encompass the entire
internet what would be novel to it that
would require slow deliberative focus
wouldn't it have already mastered
everything so we need a way to determine
when and even whether a machine needs to
be conscious in the first place
some may but some may not thank you you
seem to be making in your speech first
of all when you spoke about
consciousness you seem to be associating
it with ability to feel now there are
humans who do to neurological damage do
not have the ability to feel emotion
then would you that would you then call
them unconscious according to your own
definition question number two you seem
to state that an AI who is bound to
serve humanity is equivalent to a slave
now the whole field of AI safety
proclaims that if an AI does indeed feel
like a slave then purely because of its
superior power it who is break free and
and exterminate us or do something that
we do not want it to do therefore can
you elucidate your definition of slavery
according to this definition does it
boil down to a question of free will or
does it boil down to an instance of
unfriendly AI who considers humans to be
immoral in a court like in their actions
towards itself okay you raised a lot of
issues there um so I'm not saying
that hypothetically a being that
couldn't feel emotion would lack
consciousness clearly emotion is closely
connected to our awareness but I you
know in my five minutes I certainly
didn't mean to be making any sweeping
claims now and you raised a lot of
issues that I I mean you're asking about
AI safety and are you asking about the
role of consciousness and freewill in AI
safety or okay so I think I'll leave
consciousness intuitive I hope that at
this moment it feels like something to
be you and you can define it
introspectively philosophers have fought
for a long time over precisely how to
define consciousness slavery in relation
to AI I mean it's a very interesting
issue I think some of the earlier
speakers addressed I was using it in
just a loose sense by analogy analogy
with slavery you know that we do know so
I'll just leave it at that
all right thanks so towards the end you
were talking about whether na I would
need consciousness at all and the
function of consciousness and one thing
you were saying is that one of the
functions of consciousness seems to be
associated with the with novel
information and perhaps if an AI had
access to tons of information or and
we're able to use that and had mastery
over that then it wouldn't need
consciousness and I guess I was
wondering if you could say a bit more
about that because it seems that as far
as we can tell consciousness has
multiple functional roles one of which
might be executive decision-making or
you know deliberating over what to
actually do and if we're thinking about
artificial general intelligence then if
it's the case that that is one of the
functional roles of consciousness
it doesn't seem obvious that that would
be lacking in an AGI yeah I'm saying
it's an issue that's open for
investigation I mean I think you know I
was suggesting that consciousness is
reserved for slow deliberative
processing and I think the question is
which AGI systems if any would need
consciousness so it could be that
slowness and novelty are just not part
of the information processing needs of
the particular AGI in questions so I
think we have to just investigate the
issue okay let's move on now to our
third speaker who's jurgen schmidhuber
an AI researcher who is director of the
swiss AI lab in Lugano very well known
for his work over the years on on deep
neural networks recurrent neural
networks deep neural networks of all
kinds there's been a pioneer since I
guess the the early 90s he's also done
some really interesting work on
universal AI including designing the
wonderful gÃ¶del machines which are you
know provably more powerful than all
kinds of other AI systems and
unfortunately not fully implemented just
yet but
over to you Juergen thanks so much David
first I had no idea what I'm going to
talk about but now I can't just respond
to to Susan let me briefly explain how
we already have little conscious
artificial systems in our lab and have
had them for a couple of years and and
in those systems consciousness is just a
natural side-effect a byproduct of data
compression during problem solving why
that so to build that general problem
solver that is what we have been working
on for many decades and we are getting
closer and closer you need something
like a general-purpose computer which in
our case usually is a is a recurrent
neural network because in this universe
you want to have a computational default
device which exploits the physical
constraints as much as possible so that
you want to have lots of processors and
in a small volume connected through many
short wires and few long wires to a
minimize communication costs so you have
a problem solver where video streams in
and pain signals and and auditory
signals and whatever and it translates
there somehow into movements action
sequences and it gets rewards positive
signals feedback signals for achieving
goals so there's some utility function
it wants to maximize that why you give
it one goal after another and then over
time it becomes a more general problem
solving now this guy has a little helper
which is another recon network which is
a model of the world and what does it do
it takes all the data ever observed all
the inputs that in response to the
actions that were generated by the
control module the controller and the
model they interact in a certain sense
and the control is shaping the history
of inputs coming in which which then the
model tries to predict to compress sort
of predictive coding why does it want to
do that because it wants to figure out a
few regular things in the world now we
have a very friendly environment and lot
of things are repetitive and repeat
themselves and so you can this means
always that you can you can more
compactly describe them in by devising a
little program a little recurrent sub
Network which can encode these things so
predictive coding for example so you for
example have videos or falling apples a
hundred apples fall down like this and
then the raw data is like that but if
you understand gravity you have a little
recon network which implements the
predictions according to gravity then
you can predict how these apples come
down and you can greatly greatly
compress the video and all of physics
and all of science and all of chemistry
is really about compression progress
about finding short descriptions of the
data which you then often can be used
through the controller to solve problems
better now what is this unsupervised
module doing which is doing the
compression of all the data ever
observed in response to all your actions
ever execute it so you you better store
them all you don't know whether humans
store them all but robots should store
them all it becomes storage is cheap and
you can easily store 100 years of high
resolution video and current devices and
then you you want to find the
regularities during sleep for example
you work on all this data and you try to
compress basically which means that all
the things that frequently occur in your
environment such as faces for example
are efficiently encoded in some sort of
prototype phase so a new face comes
along such that you have to encode only
the deviations from the prototype phase
that you already have same for classes
same for everything that is happening in
your world the words the repet a
repetitive signals that are coming out
of the mouth of other people and so on
and and there's one thing which is
always present as you as the actor are
interacting with the world which is the
actor itself so it's really really
useful to set aside a couple of neurons
and synapses which encode this
representation of yourself of yourself
as you are interacting in many different
ways with with the world so as the data
compression of the entire history of
your life is progressing you are
inventing all these little neural
symbols which stand for the stuff
Klee occurs which helps you to better
compress including yourself symbols so
you have symbols that send for yourself
now whenever the controller is trying to
solve a new problem
by by finding a weight matrix a program
essentially that that leads to an action
sequence that solves that problem it has
a search space and it can reduce that
search space by looking at this model
which already knows a lot about the
world and has algorithmic information
about how the world works and you can
reuse that in a good way to come up with
new solutions more quickly and as it is
doing that sometimes its racket waking
up these self symbols for example then
you are thinking of yourself and our
robots are thinking of themselves they
have self symbols which wake up just as
a natural byproduct of data compression
through during problem solving and and
then of course what is happening as you
work worked correctly stated some of the
things that you some of the skills and
the observations sequences and the
pattern recognition techniques that you
have developed all the time they become
automatic so they become kind of
unconscious and only the new things
where you're still working on how to
improve the performance of the
controller the problem solver they are
still undergoing a search process where
you want to find a good combination of
weights that solve the solves the new
problem without destroying the previous
skills that you have learned and so our
first system of that type we really I
had one in 1991 it was a recurrent
neural network it just learned to
compress the data unsupervised learning
network which learned to compress the
observations and and everything that
this guy didn't understand it sent up to
her to a high level game unpredictable
stuff went up to a high level guy which
then tried to find higher level
irregularities and then maybe found
additional compress abilities and
developed internal representations which
the lower level guys the automatize ER
as opposed to the conscious conscious
chunka could learn to imitate such that
he previously can't
stuff during problem-solving and science
became unconsious by imitating just the
problems that the high-level guy found
and so we have all of these things going
on and have them had going up and they
have been going on for for many years so
I would say from a technical and
practical perspective the issues of
unsupervised learning and consciousness
are solved and we already have them in
our I think there's nothing to it how
could I not I mean you know everybody
has a definition of consciousness and I
mean that's why I said let's just do it
introspectively because we all know what
that means so we have another proposal
why you know that consciousness is the
side effect of data compression and I'll
just say what we need given the ethical
significance of machine consciousness is
a dialogue between scientists
philosophers and others to really figure
out what the heck consciousness is from
a scientific perspective and then we
need to figure out under what conditions
we want AI to even be conscious so I'll
just leave it at that
so you're gone what if I tell you that
the little program you described right
now which is a very clever program a
nice principle and all that what if I
tell you that that's not conscious at
all I mean it behaves in certain sense
conscious like but it doesn't actually
feel like anything to be that program
and what's more what if I tell you that
you know you can disagree with me but
there's no way you can convince me
because the only way I could the only
way you could possibly convince me that
something actually had phenomenal
consciousness would be if I could sort
of see myself in it that I could sort of
see that well the thing is Soren
sort of like me and therefore it
probably feels like something to be
liked it it's not a problem for your
program or everything we're talking
about here this was Julian to kill you
is my former postdoc now professor at
NYU I think now it's time to make a
little confession my company Nations
building androids and and I'm a
prototype and and and we are not
commercializing our products yet because
we still allow suggestions for
improvements and let me tell you we have
a long long list of suggestions for
improvements but there will be a version
2.0 and and now let me try to answer to
unions question while I'm saying all the
things that I'm saying I'm totally
unconscious but I'm really good at
faking it at faking my consciousness and
and we leave it at that
next I think he could be doing a better
job at faking consciousness the next
speaker is Jung was one of the
developers of Skype and has become a
really major force and the in the
community for thinking about the future
of AI he's been a big force in the the
Cambridge Center for existential risk as
well as in the future for life Institute
thank you David
I am in the middle of establishing a
personal record here the personal record
is my shortest visit to the u.s. I have
this personal policy of whenever i
displace my atoms by the width of one
ocean I try to stick around for like at
least 10 days or so to come justify the
financial and moral expense but I was
always really torn between like should I
cannot violate my policy in order to
join the like just amazing roster or
speakers that David had amassed somehow
here and including my scientific hero
who's sitting just next to me here and
so but now I'm really confident that I
did make a right decision so the spirit
of tastes like closing panel I would
just like go over the notes that I that
I made during this conference to
basically kind of try to convince myself
and you that it was the right decision
so for example there were like really
nice nuggets of insights that I hadn't
thought of before starting from like
Stephen Vogt from who's a friend and he
mentioned all from tones something that
action musicians go to for creative
insights or creative ideas this is like
a
great card time in my pocket when people
come to me that saying that though
what about creativity isn't that
something that we always distinguish us
from from computers also like today's
presentation from Stuart Stuart Russell
who I really need to think that we need
to clone like he ideas that the denial
of risks led largely to the demise of
nuclear industry which is like really
great to bring up when when we talk
about AI risks in AI
denying AI risks also the idea of
starting to build uncertainty about
objectives to AI was great great insight
I think and eleazar's point about
sorcerous and pred this movie actually
being the correct representation of
what's what's happening not a terminator
now I also like night really appreciated
the mix of short-term concerns and
long-term concerns I think it's really
important to kind of I think I said
earlier pointed out like a very sharp
distinct distinction and not really
delude ourselves that addressing
short-term concerns would actually buy
as much over like when we really want to
figure out how to survive the long term
long term issues perhaps we could find
ways to to basically make a transition
as GeoTrust pointed out that perhaps
sort of industry interest in creating
really of more and more autonomous
systems that can function in unforeseen
situations might transfer over to
something that could be useful for sort
of super-intelligent regime but it's
still important to keep in mind that
that there is a distinction between
talking about human creations and
machine creations that were created by
machines that were created by machines
that were created by machines and so
forth so so like when we are talking
about things about yeah what about moral
rights of AIS and and
in AI and then like it's important to
keep in mind we're talking about we
probably talking about human human
creations not machine creations but that
that's still like I think it's it's a
valuable thing one of my favorite blocks
that I can't just recommend in office a
star slate codex and there
Scott Alexander the author pointed out
that like we really shouldn't have the
dilemma of like how much should we spend
our resources on thinking about AI
safety short versus long term etcetera
like we if I can afford the hundred
million dollar boxing match it doesn't
really seem that that we are on the kind
of product optimal frontier where we had
to make kind of really like shop like
really careful considering decisions
when it comes to you know future
technological risks reducing the risks
or future technology and so yeah one
risk myself have really kind of focused
on long term risks but I'm really happy
that that people are working on the
short term short term issues and one a
total numbers weapons Petera sorrows so
as someone who knows someone who has
this got to be us achievement of from
the 90s of ending up cleaning up
creating a record in an antivirus
database under description of a virus of
unknown Eastern European origin it's
it's really important
inside that by creating increasingly
autonomous weapons and thus entangling
disentangling the scaling of offensive
capability from the number of
eighteen-year-olds that you have as an
institution we might be creating similar
situation as as Peter said that we might
create sort of similar regime that we
have with computer viruses where like
individual people without attribution
can't cause global
damage and and yeah like a like I said
it's it's it's a it's great that that
people are thinking about long
short-term issues I kind of think of it
as a like I'm really glad that people
are working on global warming for
example I really like reducing the
effects of global warming I don't think
that global warming is going to kind of
end the world but I'm not fully sure and
an same thing is like with with a
lactarius like debate happening even in
within like AI safety circles like the
soft takeoff versus software versus
heartache of scenario like and when
there are like myself I do believe like
looking at at where the trajectory of AI
research is going at you think that
heartache of seems more likely to me and
and also a creation of non-human like
agents also seems more likely than to
meet and yet alternate but I'm not sure
and totally willing to give like at
least single-digit percentage points to
the alternative so so if in case I'm
right sorry in case I'm wrong I'm just
really glad that that people are going
to taking up this this particular patent
right
oh actually let me end and intesting
that like I've been really the last few
years I've been kind of focusing on only
on a few things like one is joint you're
gonna remove various bottlenecks from
the AI risk and X risk communitary like
words capability to address X risks and
and really glad to see that that this
conference actually is great data point
that that this pollen XR are being being
lifted like not least AI careers lunch
that we had today that was like a really
impressive turnout there so I'm really
glad that that young people are getting
interested in these topics and just
before this panel I was really glad to
witness the sort of friendly and heated
discussion between between eliazar here
and and young
yeah and there which I've been trying to
facilitate by and also having these
discussions with Jurgen my friend Jurgen
there every once in a while
facilitate a Zygon of matter of tactics
I've been kind of investing in AI
companies in order to get a ticket to
hang around in their kitchens and have
these conversations and and I think as a
outgrowth of of that approach the
partnership of AI for example it's it's
as far as I know it was deep mines sort
of initiative originally and unclutter
it is be appearing very improved it
wasn't oh sorry okay so like not ready
for me then but right so yeah in closing
I think it's valuable to think of AI
research in the world as search for the
best possible future for Humanity with
the important caveat that we are likely
to irrevocably commit to the first
result that we find therefore let's be
really careful about this search let's
improve the processes that go into that
could you summarize what the exchange
but how the exchange between Eleazar and
yarn went ha that would be difficult
well it really was like there were a few
topics there like one topic it seems to
be like concept disagreement between yon
and and various other other people here
in this room it's like like how strongly
are the instrumental goes that fallout
from from your Ghana overall goal of
likely to emerge I think an end point is
that instrumental goals have to do a lot
with how humans treat treat our
objectives like I'm not really doing
justice to this right now but but anti
counterpoint is that no no this this
that just fall fall out a skewer just
it's really hard to deliver milk if
you're dead so that you try not to be
dead if your goal is to is to deliver
milk so you don't have to be afraid of
death you just like avoid it because you
have somewhere to go so that I think
that was one one one topic to discuss
and they were other I think one
important thing that Eliza pointed out
that really need this regime change
between like how a is or a developed now
and and how basically make it make sure
the culture is closer to how
cryptography is being developed a lot
now that the idea is that instead of
just trying to think two things that
seem to be works in to work and be
useful let's just try to poke holes in
in various engineering approaches and
see how they how they might fail that
because we really need an end result to
be really robust and beneficial it was
very interesting discussion point I mean
there are points that we discussed but
we didn't quite disagree and other
points where we well at least one point
that we did
read strongly and I said as long as
we're stuck with supervised running
there is no danger of any eye becoming
you know sort of life-threatening if you
want globally life-threatening and
eliezer simply disagrees 52-degree as
well and we haven't it was time to kind
of come into this room and so we didn't
finish that discussion but there was
clearly a point of dissension can you
set the record straight on how the
partnership got started
oh it was actually a discussion so it
turns out several people within the
various companies were interested in
doing this independently and it was it
started as a discussion between me and
it was in January right like in 2015
when Eric Orbitz can I came on board and
fabric as well and then in January we
had a meeting here February we had a
meeting here at NYU where we had like
the first kind of founding meeting with
all our members IBM can i joined at that
point and you know Eric and Horowitz
from from MSR took it upon himself to
kind of run the show a little bit
together with Mustafa Sulaiman from from
deep mind because he had been thinking
about those questions for decades
essentially so he was kind of history he
felt strongly that you could play a role
there and so that started but really all
of those people were thinking about the
same thing about the same time and it
cannot coalesced yeah thank you thank
you for doing that which kind of risks
do you think are the biggest bottlenecks
Oh like when I went mint polo max uh
perhaps didn't have enough time to
elaborate I like thank you for the
question
I've seen that extension who likes
attention risk
industry in the world has had following
polo next it's sorted with a reputation
of bottleneck
like in 2008 or so when I joined that
there was like up if you
people in California and Tenant really
the their message didn't really resonate
because it was really easy to dismiss
them on reputational grounds or like
just didn't have like proper academic
credentials etc so III think that
situation has greatly improved now
Theresa is very hard to make a case that
Cole Cole like people like Stephen
Hawking and and the list of British
scientists like who is who in in our
Cambridge Center adviser report as
people who don't know what they're
talking about so the next bottleneck was
financial bottleneck and it's it still
leaves is a problem like in the sense
that the finances are not distributed
evenly in the exoteric ecosystems so I
do think the mirror is right now the
tagging of the best best use for
marginal dollar in my opinion like you
should do your own research then and the
next one is going to be like talent
talent polo-neck
and and its really great to see that
that just progressed as well now you
have questioned the look like I don't
have enough like I take all the
existential risk reduction that I can
get so so I don't want your kind of
discouraged people to work on another
auditing stunt and like AI risk I just
focus on a risk for two reasons one is
that so I really encourage people to
work on things like bio risk and and
nano risk even nuclear the first of all
my backpack around just these computer
science and physics so so I do think
that it's I just I can make a bigger
contribution area safety than tanning in
biology that I don't really know much
about okay we should um we should
probably move on all right okay our
final panelist is an NYU local Gary
Marcus Gary I guess is both they both
are psychologists
AI researcher you all know Gary's books
and psychology and foundations of
cognitive science such as the algebraic
mind and books for general audience
Sanchez Kluge and guitar 0 in which he
starts off totally unable to play the
guitar and by the end of the book
attains the guitar singularity more
recently he's a co-founded area nai
machine learning company geometric
intelligence that's doing all kinds of
wonderful things that Gary can tell you
about and his perspective on the ethics
of it I made a title which moment in the
future history of AI should we most
worry about I'll give you my answer at
the end this is Alexander my son who's
not quite 4 years old and if you think
of him as a cognitive system he's pretty
sophisticated he has essentially
complete linguistic capacities he can
understand and produce counterfactuals
he has well developed though imperfect
capacity to reason about the physical
world and outstanding capacity to
acquire and even invent new motor
actions strong ability to transfer
knowledge between domains into novel
situations if I could do that with my
company I think I would be rich and
famous here he is standing on a glass
coffee table or I guess this is the end
of the motor actions trying to acquire
and the landing strip otherwise known as
the couches on the other side I was
testing out my new camera so you can see
basically what was going on there is
however just one catch he made it this
time
but maybe the emphasis is on this time
this cute child's cognitive abilities
are uneven even if his linguistic and
motor capacities are great his capacity
for assessing risk is only modest
modestly well correlated with reality
and he has no concern I see this as an
ethical failing on his part for what his
parents will think if he injures himself
during exploratory play there are other
lapses that happen every day this also
from my new camera this is my daughter
it would seem that about 3 to 10 times a
day there are disputes about property
rights and they they in there inevitably
and badly my understanding not owning
any of the older variety of the species
is it it's only going to get worse as
they become teenagers the teenage brain
still under construction
teenagers have even straw
longer reasoning and language skills
well beyond anything we can do in AI
they have adult-like pleasure systems
that induce them to do things like drive
very quickly they have their own set of
inhibitory challenges and unfortunately
they have a greater capacity than
toddlers to operate heavy machinery so
that's the first that's the lead-in here
we'll come back to it in a way the point
I want to make today is just this
cognition is not homogeneous
so there are abstractions where what
we're looking for is a particular
threshold moment of strong super
intelligence but I think we all know at
some level that intelligence is really a
multi-dimensional variable it's not a
one-dimensional variable I just want to
meditate on that I don't think it's a
surprise to anybody in this room but it
hasn't been the focus here and I think
it's it's worth thinking about what's at
stake so there are many things that go
in intelligence I'm no more gonna define
intelligence than you define
consciousness you know just sort of
really provisionally I don't want to get
stuck on it but one way to define
intelligence would be something like a
capacity to coordinate a vast array of
cognitive tools like induction deduction
pattern recognition and so forth we're
doing really well on the pattern
recognition part right now in AI but
maybe not so well on other pieces like
physical reasoning and planning
computers and you can think that there's
a multi-dimensional space defined by
these different Dement or sub dimensions
of intelligence like your clock speed or
your memory capacity and so humans and
computers already occupy very different
parts of the cognitive space and I think
one theme that's come up is we have no
idea what these future AGI machines are
gonna think like where are they gonna be
in this space which is of course much
more complicated than what I've drawn
here and even different AI systems
radically differ from each other in
terms of their composition there's been
a lot of sort of talking about age yeah
in generic ways but just think about
like the differences between Watson and
Alex net Alex net was one of the the
groundbreaking image recognition things
Watson from IBM you all know or memory
networks that Jana McCune mentioned in
his talk and there's a similar paper in
nature by deep mind this week these
things have very different properties
like some of them have read/write
buffers some of them a modular structure
some of them don't there's very
different cognitive systems so in recent
years we've seen very you know we've
seen huge advances in hardware so you
know as Juergen said you know it's
trivial to have terabytes of memory
years of video we have enormous numbers
of floating-point operations per second
and enormous progress in certain aspects
of AI like image recognition and speech
recognition and so forth right now
though and this is a technical term we
know diddly-squat about how to make
machines that can actually reason
ethically except maybe in very limited
toy car we define some very narrower
world so you could take for example the
average teenager this is a new
variational a Turing test they suppose
I'm proposing online watch a movie and
tell me are this characters actions
ethical or not any teenager can do this
in a vast variety of Hollywood movies
let's say and probably a lot of other
movies we've no idea how to make a
machine to do that at this point that
could do the segmentation of things into
individual characters that are
time-space worms that evolve over time
and then have actions and so forth it's
hard enough in fact to program an
autonomous vehicle to recognize the
simple concept of a truck on a sunny day
as we all know from from the first Tesla
fatality we have as president I think
it's important to be realistic so far as
I know no serious proposal for how we
could actually code something like
Asimov's laws I think we all know that
asthma's laws themselves are too
primitive but how do you encode a
concept like harm and I think Eleazar
was pointing in some similar directions
in such a way that it could be reliably
evaluated relative to real-world
contacts we don't have tools for that
the most impressive techniques we have
for learning right now and I should say
we're going to have to have learning
systems for ethics we're never going to
hard-code
all of the specific contexts that we
need things so we have to have learning
systems well our best learning systems
are things like deep learning really
involved huge numbers of parameters they
require huge amounts of data and they're
still not good enough for
mission-critical jobs like driving like
Nvidia made a big splash with this thing
it was trained for 72 hours and it could
drive around it had an autonomy rate of
98% but you read the fine print and that
means autonomy is 98% you don't have to
touch the wheel in a 10 minute commute
in New Jersey right if you scale that up
to across you know billion or sorry a
million driverless cars it becomes a lot
of fatality a friend of mine made this
and I turn it into a t-shirt
don't worry kill a robot I'm really a
starfish it's a reminder of how easy it
is to fool a deep learning system I
don't mean to pick on deep learning
systems there are any other systems that
are any better in fact deep learning
systems are probably best we've got
right now but it's easy to find for
example textures that
them into thinking that there are
objects there is no high-level reasoning
about what's going on most importantly
these systems for now still lack ways of
representing acquiring abstract
knowledge of the sort that any Ephesus
would take for granted just to get the
conversation started notions like agent
or harm or value or perpetrator a victim
or causality we have no idea how to
learn over those kinds of entities in
any given domain right now results are
impressive so here's a captioning system
that was in the front page of New York
Times you put in a picture you get out
text of a person riding a motorcycle on
a dirt road and you're very impressed
but if you look in the fine print you
always find really bizarre mistakes this
is my Oliver Sacks mistake the man who
mistook his wife for a hat this is the
program that mistook it's a parking sign
for a refrigerator filled with lots of
food and drinks enclosed tasks where
there's only a thousand categories these
systems do pretty well but in an
open-ended tasks there are always lots
of bizarre errors and there's no concept
for example of a refrigerator or a
bumper sticker or a parking sign there's
no serious capacity for a compositional
reasoning over those entities and even
get to work it would get worse if he got
to something like a compositional
concept like a mild harm caused
inadvertently while trying to save
someone's life if we're really going to
do the kinds of ethical reasoning that
we're talking about in machines we need
to have systems can learn over those
categories and we just don't really even
have a clue right now how to represent
them and I think this is a good reality
check wire is always running these
things about how great these days but
they reported on the Allen Institute's
challenge to have systems do
eighth-grade science Tiff's ik reasoning
best scores were like 60% you can get
like 45% just by feeding things into
Google because many of the answers are
kind of already there but the rest of
the way there is not very good until the
way that wire put is the best day I is
still flunking eighth grade science well
you can't do eighth grade science you're
not going to be able to do high-level
ethical reasoning the point is not the
deep learning sucks which I'm sure is
how people will paraphrase the talk I
actually think it's a pretty good tool I
use it my company every day but rather
the point is as a part of this I already
said we can't possibly hardwire in every
ethical contingency
unfortunately deep learning is our best
tool for automated learning right now
and it's maybe not the right tool for
that particular job we don't necessarily
have another tool so most of the
research right now is on pattern
recognition for images and speech it's
not clear that that transfers is
something like that
ethical principle there's also a lot of
problems Peter Norbeck just gave a
couple of great talks on this you can
find online at M Tech and O'Reilly a
about challenges in engineering launch
large-scale machine learning so how you
incrementally build something how you
debug it and so forth it's not clear
that for mission-critical jobs that
these are the right tools building
superintelligence is probably going to
require the development of a lot of
radically new tools that we don't even
have a clue right now about what they
should be the nature of the cognitive
machines that we create how safe and
unsafe they are depends not on some
overall IQ number like 120 200 or
whatever but on the exact constellation
of cognitive abilities they have and how
those cognitive abilities interact with
each other and so in a way I'm pleading
for a like thought on the texture of the
particular entities that we're talking
about I don't think we can take for
granted there's just going to be you
know something with a 300 IQ and we can
reason from that number no two
snowflakes are alike probably no two so
super intelligences are either who knows
where we'll be and I'm starting to wind
up where we'll be in a hundred years but
until we have basic advances in building
machine learning systems that can reason
over abstractions we're not going to be
able to create systems that are capable
of reasoning ethically and so I think
this is my last bullet point maybe the
greatest fear shouldn't be super
intelligence which is any case kind of
long term but maybe something
medium-term which I mean maybe super
intelligence actually will be able to
reason ethically maybe super
intelligence will demand solving these
problems about learning over
abstractions and so forth that I've been
pushing on maybe the thing we should
most be worried about is the teenagers
basically to connect this all back to
the way I started the teenage immature
pre super intelligent systems that have
a lot of perception which were really
good at right now a lot of optimization
we're getting really good at and control
but not very much abstract reasoning so
thank you very much
well I mean there's a good argument that
humans have been you know stock in that
way you just look at humans from like
200 years ago that late thought slavery
was fine for example observe that humans
are sometimes inconsistent in their
ethical reasoning do you anticipate that
whatever representation and system we
develop will will somehow be consistent
where the humans are not or do you think
they will ultimately need to develop a
system that can allow for and resolve
inconsistent on preferences concepts I
think it's a great question humans are
sort of our best model of ethical reason
errs right now and they're not that
great and we don't know whether there
might be something better so AI doesn't
have to be human-like like there are
things we want to learn from humans
because humans can still do certain
kinds of reasoning better and language
better but for example arithmetic is
much better left in the hands of
machines maybe there will be AI systems
that are better ethical reasons that are
not caught up in cognitive biases and
you know south bias and so forth so I
think it's at least possible at the same
time I think the issue you raised about
inconsistency is really important so one
of the things that we do really well is
to reason among the inconsistency find
kind of reasonable compromises among
that inconsistency where is like logical
you know theorem proving engines don't
really have a way of dealing with that
problem the kind of deep learning stuff
doesn't really have a way of doing that
reasoning so we we're missing something
in between where you can reason over a
somewhat inconsistent world and not kind
of give up and so there's probably a
hint from humans and maybe we can do it
better someday
hi I was just curious I mean are there
simple things for example philosophers
discuss whether the numbers of people
that will be harmed counts now is it
possible for example at this stage to
introduce something like the rep you can
represent numbers of people right and
presumably you could encode things like
numbers are relevant so that even if
they had no concept of harm okay you
would have to decide whether the
entities were being harmed or not maybe
that's something that you know you can
see but they could be taken to derive
the conclusion that then given that
there's harm at stake which you decide
numbers count right or the way in which
harm comes about I was very surprised
when professor Kahneman said there was
conflict over whether to push the fat
man intuitively I'm no conflict at all
it would be wrong to do it so if you
could encode things like pushing someone
over a bridge or even you know making it
the case causally that someone falls
over the bridge that's something that
they could recognize you could decide
that it would be wrong they don't have
to know anything about harm or the idea
of wrongness or anything like that they
could be used then to limit their
conduct on the basis of well he would
have to be you know thrown over the
bridge rather than die as a result of
something happening like turning the
trolley these concepts are uncodable
aren't they and they are morally
relevant so I'm reminded of Yuans
metaphor about since he pointed I gave
me a shout out for one of mine I'll give
him one of his the which is about the
cherry on top so some of the things that
you raised I think the points are all
well taken I'll kind of like the cherry
on top so just counting numbers yeah
machines can do that and so it's no
problem to have a utilitarian
consequence you know better to save the
the five then then you know the
sacrifice to the 35 or whatever I said
they're right but some of the things
like pushing somebody over a bridge I
think gets to my core point which is
you're probably not going to program in
all of those contingencies so you know
pushing or pushing over bridges might
actually get hard-coded in somebody's
ethics because
there's so many trolley problems around
they're disproportionately represented
in the literature but sorry one more
saying but a lot of other things that
you could think of that aren't literal
pushings over the bridges
nobody's gonna directly code them right
and so then the question is how are you
going to have a system that hasn't rich
enough understanding of a world to
understand things ranging from physical
reasoning like if you push somebody in
an elevator shaft they will fall and
that that will cause their death mate to
have the system actually make the
inference that pushing someone an
elevator shaft is a bad thing if it's
not hard-coded and this is where I think
I see you want to file a bill I'll just
say one more sentence this is where I
think the rubber hits the road and we
just don't know how to have systems that
are rich enough to make those kinds of
inferences so somebody might say well
great we'll put in a physics engine well
it's not so easy I see or any Davises
here Ernie and I have written a bunch
about common sense reasoning like
getting a system to do the physical
reasoning to realize that you put
somebody in the elevator shaft and
they're going to follow well depends if
you've set up the problem correctly
people imagine these kind of like game
engines the the video games sorry it's
true there's only one sensor I thought
if I didn't take a breath
and of the five being saved 35 being
saved and then the one will die this is
just a question of causal direction okay
you could get something closer to what I
think is the right answer people may
disagree about what the right answer is
and I'm not asking the machine to do
that you know we crypto machines can
figure that out but if it's matter of
causal relations you can get pretty
close well we're not actually that great
in AI at causal relationships so I mean
there are some formalisms for
representing causal relationships but
typically some human is sitting there
making a directed graph and labeling you
know what's the likelihood that the
burglar alarm is going to go off if
there's a person as opposed to a truck
going by or something like that
what you would like is a machine that
get in an automated way and for what
causal relationships are at stake so
imagine my Turing alternative where you
watch a video and you try to figure out
causally what are the relationships
between these characters and the props
that they're using and so forth and it's
at the level of like parsing the scene
and parsing the world that we really
have trouble some of the abstract
principles that ethicists proposed you
could do that but like the question of
how do I then identify in this scene is
there being a heart caused is actually
the hard part Eleazar wants to follow up
on that
we of course tend to spend a lot of time
talking about the things that we
disagree about but I think that's one
thing that's worth sort of remembering
what people do tend to agree about in
this field and I think that's sort of
like everyone with any kind of
experience in artificial intelligence I
think basically agrees that any sort of
real-world applicable values goals that
has to be learned there has to be a
learning rule you like back in the early
days of AI we tried to hard code how to
recognize a cat with logical rules it
didn't work it's not gonna work for
values either like there must be a
learning system in a learning rule they
cannot be hard-coded there's just too
much to generalize I mean I think that's
a nice way to think about it so we have
solved more or less not completely the
problem of recognizing cats new there's
this whole literature now about spoofing
and adversarial cases and so forth so I
can devise a cat that a deep learning
system will get wrong but let's not
worry about that too much let's suppose
that that's mostly a solved problem can
we use in essence this is what I'm
asking can we use the same techniques or
what kind of techniques do we need in
order to recognize an agent in an action
or a risk or a harm like so the way that
we are able to recognize the cat is we
have a lot of examples and we developed
low-level these systems automatically
develop low-level features and then
medium level features and they do it as
a kind of hierarchical decomposition of
features that are basically giving you
information about things like texture
and so forth is that same thing going to
apply I mean I think you know probably
there's some graduate student this room
who could make that a thesis topic and
be really interesting trying to say the
same techniques my guess is that they
will fail but I think that's okay I
think that we need people to think about
what the alternatives are there a short
one for for Juergen your reaction a
short reaction which is compression is
really cool I like the architecture that
you're talking about but I'm not sure
what it has to do with consciousness the
key really there seemed to be self
representations and GPS systems have
that but I
tribute them consciousness yeah no the
point is that you get self symbols
automatically by compressing just your
interaction sequence as an interacting
agent with the environment and then the
stuff becomes conscious as you are like
focusing your attention on it or include
that into the search space of the thing
that you still haven't solved yet as
you're trying to solve a new problem you
have a search space now you have to try
a couple of alternatives now suddenly
you have in your focus of attention
these few things that are relevant or
seem to you relevant at the moment as
you are trying to solve that problem as
you are then once you have solved the
problem the same thing is becoming
automatized and I get architectures
which really do exactly that so since
1991 where you have to use consciously
all right if we're gonna go history here
I'm gonna I'm gonna point to John
Anderson's act our model and its
predecessors as another way of doing
automatisation another way of having
central focus of memory all the
properties are described I'm actually
older then you described in the symbolic
world just you know while we're doing
the history but I wouldn't say enact our
models conscious abstractions all of
abstractions about compression either in
space or in time or in energy so you
would try it with less with fewer
computational resources encode the same
history of actions and product as a
byproduct of that you get all these
high-level concepts which you even I
think you even said that we don't yet
know how to get these abstract reasoning
things no we do know how to get them we
do know how to get high-level spatial
temporal concepts that stand for things
such as word phrases and you know and a
plan of how to go from here to Beijing
you know not by simulating the future
millisecond by millisecond but you know
first going to this taxi station and
then to the airport and then for nine
hours nothing happens and we leave the
airplane so we do have these things and
what you mentioned it's the question
about necessary versus sufficient so for
the sake of argument I'm willing to
stipulate that all that compression
stuff is necessary and that self records
of some sort are necessary I'm just not
sure there's sufficient so I can think
of lots of
AI systems of various sorts that have
those things but that I wouldn't want to
attribute consciousness to but I won't
say anymore so we have a bunch of people
who study animal behavior in this
question of conscious self-awareness and
they have tests for these kinds of
things too so they don't think that
there are lots of animals that I think
satisfy what you would say they're
self-conscious they include insects but
when you tell an animal behaviorists
that an animal is conscious they often
fight very hard to reject this so this
is meant to be a little cheeky but how
does your data compression algorithm to
do on the mirror test for example yeah
of course and so actually all of this
mirror neuron business is a natural
consequence of data compression because
obviously as you are a robot which with
pain sensors and whatever and out there
there's your friend another robot which
looks a lot like you you are trying to
encode what you see about this other
robot and you are trying to do that in
the most compact and most efficient way
and the one way of doing it right is to
have something like a like a prototype
robot which stands you can't stand in
for him but also for yourself
and you only have to encode the
deviations from the prototype so the
mirror neurons of course are those
neurons that you can use not only to
encode aspects of your own experiences
but also the aspects of the experiences
of the other guy who which you are
predicting so it's just more efficient
to encode possible experiences of not
only yourself but also of the other guys
by just having these mirror neurons so I
always thought this Tononi and these
guys aren't they they are really
mislabel even in coming up with these
very complicated ways of defining
consciousness when all of this just
falls out of data compression try to
encode the same data with fuel company
computational resources such as
time-space energy and this is what
unsupervised learning is about and this
is where you get the abstractions from
all of the abstractions all of analogy
is just a side effect of data
compression the emotion said you the
emotional connects that you feel with
another guy like yourself because you
can feel what he is feeling is of course
due to your
predictions which are based on your own
experiences in the past and there you
can just reuse the same code the same
neurons if you were to model his
experiences so all of this is again I'm
just a natural byproduct of data
compression
oh it just so just added like of course
like mirror test is different thing to
mirror neurons however I think you're
against Jurgis answer does apply to also
mirror test because you would be
compressing your model of the world just
a procedural note what we're doing for
the last 30 minutes or so I think you
should not feel obliged to ask questions
directly to these five speakers about
their specific talks it's also an
opportunity for people to bring up more
general issues on the theme of the theme
of the conference which might be partly
directed at some of the previous
speakers who should feel free to take
part in the in the discussion so you
know I'm not every not every question
has to be about the data compression
theory of consciousness despite the
warning let me mention the c-word so
David there are many things I admire
about you one of them is your ability to
sit and hear people say things about
your one of your pet subjects that you
violently disagree about without the
total poker face smiling and and then
not even commenting not even trying to
set them straight or anything I don't
have that I don't have that level of
inner peace developed on so so let me
just add something in my conscience
everybody's entitled of their own
definition of what consciousness is of
course so and many of the people on the
panel I think define consciousness is
some kind of ability the self reference
or say things about yourself or whatever
I however share with you this
fascination for the question of
understanding subjective experience and
to me even if we can build machines that
are very good at referring to themselves
and acting exactly like your again
prototype 2.0 or whatever and making us
feel that the conscious
I'm very interested in whether it
actually feels like something to be that
robot and part of the reason of course
is what what Nick Bostrom brought up
issues of a mine crime and so on but I
think there's another one that hasn't
really come up very much at all if it's
since you encouraged us to think long
term okay suppose one day in the distant
future our descendants are some kind of
machines you know we this is not the
science fiction because when we look for
aliens today with a new hundred million
dollar funding or so are we we take very
seriously that we're most likely to find
other civilizations there actually some
form of AI because you know if something
formed 2 billion years ago it's not so
likely that they're still gonna be
exactly in their biological form so if
that's suppose that happens one day you
know on earth and in the cosmos there's
all these amazing life forms doesn't
that make you feel warm and fuzzy they
maybe they have our values and speak
your language and even read your books
you know isn't that great
except suppose it turns out that they
have no experience there they're zombies
they just there's no one home you know
wouldn't that be a bit of a bummer and
III think particularly people like Ray
Kurzweil and others who take a lot of
solace of in the dream that they're
gonna upload themselves and become
immortal you know if it's not so
satisfying if you to do that if you
realize that there will be no experience
at all and then the whole future of the
universe is just a giant waste of space
so it to me this isn't just an idle
philosophical speculation and regardless
of whether you want to call experience
consciousness or something else I think
the question of what it is about a blob
of quarks that makes it have a
subjective experience is one that we
really should study we should be honest
about the fact that we have we're pretty
clueless about it and it'll be relevant
in the very near term if you have if
you're a doctor in an emergency room who
has an unresponsive patient to figure
out whether there's someone there and
they'll become more and more relevant
than the future as AI progresses so
little shout out to the hard problem
it's very it's a very important topic
not just for the philosophy of mine but
for but for ethics it actually reminds
me of one of my favorite philosophical
thought
which is a mash-up of of consciousness
and ethics it's what I what I like to
call the zombie trolley problem so the
the trolley is going down the down the
track and on one of the tracks there's
one conscious being and on the other
track there's five zombies so otherwise
functioning pretty impressively like
like humans they have no conscious
experience you've got the choice of
which track to send the send the trolley
down who's going to send it down so it
kills the the one conscious being hands
up who's gonna kill the who's gonna send
us what kills the five zombies okay big
big majority yeah next question now we
could no we could our trolley set up on
one one on one track this one conscious
chicken and on the other track there's
an entire planet of humanoid zombies
let's let's ignore their instrumental
relevance and all the things they might
do for us and so on who's going to who's
gonna kill the chicken who's gonna kill
the planet of humanoid zombies like a
scene from what is the metric of
conscious in this year before I might
cast my vote it's a thought it's a
thought experiment I get to make it's a
thought experiment I get to make it up
on the substrate I mean you know having
empathy for something made of metal is a
lot harder through something made of
meat so what how would you know you
saying this question is very important I
mean people have been saying that the
consciousness question is the key
question of our time for a long time
what would an answer look like and why
is how can a question be central when we
have no idea how would we wreck how we
would recognize
answer if we found it we find localized
answers in brain work on patients and
that is based on a correlation between
two things what they tell us and the
brain and then we can extrapolate when
we see the same brain signals for
somebody who is not talking we can we
can assume that that being is conscious
without that correlation we can ask
computers but we when the brain signal
is not there I really find it extremely
difficult to know what we're talking
here I'm the optimist I really think
that there is an answer that we will
recognize once we get it I think I'm a
blob of quarks and I think if you put me
and I am some future really if even
today if I sit in the MiG machine at MIT
it was 306 superconducting magnet it's
just reading out things and I'm looking
at a screen where the computer is trying
to predict what I'm thinking about you
know I can think about an apple and old
with maybe eight 90% probably get that
right I mean I can do really serious
scientific experiments and try to first
let's see if the computer can predict
which information processing in my brain
I have an experience of in which I don't
and then this is gonna be this is the
answer to your question I think
eventually somebody will have a Eureka
moment and come up with an equation it
said this is the equation that
information processing has to satisfy in
order to be a subjective experience and
then we can test that in all sorts of
examples and gradually build confidence
around that just like we do with our
physical theories and wouldn't we feel
really confident about it we'll take
seriously also what it predicts about
computers and and other systems where we
can't rely on self-report that that's
what I'm hopeful for and I think at
least we should do is least try find
such a thing
what do you hand the mic to Erik next
year so yeah it's on this so I guess my
perspective is closer to Professor
connivance on this I mean it seems to me
like we rely on
intuitions about these things that are
grounded in an evolutionary history and
a social history that's based on a very
limited range of cases and then even
when people try to apply science to it
like here at the table all right someone
says well here's the equation right or
Giannone says here's the equation right
and then other people say well that's
not that guys back there says that's not
my intuition so we come at it with these
very limited range of tools if we're
very lucky we might find some
convergence between what you know our
philosophical intuitions are what is
kind of has various theoretical virtues
like simplicity or elegance what you
know we can happen to discover through
our scientific tools and if we're lucky
and get a convergence then maybe we can
all come to agree but that might require
more luck epistemic luck than is
realistic to expect okay I have Wendell
and Eliezer then Tom at least to what
degree a system might have consciousness
and it's one that will actually
accommodate your gains on perspective to
some point and that's that simply I
think most of us would agree that we're
highly automated beings in other words
so much of the activity we do is in a
certain sense our I have at it's
programmed into it if it's happening
through unconscious forces but a lot of
the function of consciousness seems to
be the director attention to what cannot
be handled through the automatic systems
that we have and therefore it requires
inspection now there's some of those
situations in many cases but just a
little bit of a tension because we
didn't quite get catch what the
situation hand was a little bit of
attention will allow our automatic
behavior to kick in because it's such to
recognize oh this is X and I mistook it
for awhile and that's a lot of what
we're dealing with but there's a whole
flock of problems we come up with that
the advertised system does not kick in
at all and that's because there
nation is lacking this information we
can't have their factors that come into
play that we don't fully recognize and
for me that's what ethics is ultimately
all about is how we mediate in those
situations where the automatic behavior
can't kick in and therefore we need to
begin thinking slowly because we no
longer have the automatically handle so
it becomes very important at least one
test toward the direction of whether a
system is really conscious or not is
whether it can go through those
processes of coming up with a response
to a situation at hand that can't be
fully automated in terms of the
behaviors it already has here I can
comment I think that the oldest person
here when I was a student more than 60
years ago we didn't think of
consciousness in terms of action and
deliberate action this is relatively new
this way of thinking about consciousness
what we thought about consciousness was
Gestalt and perception we essentially
passive and you're conscious of the
world and that was consciousness for us
then now things have shifted in
interesting ways and I can't right
offhand see you know how and when it
happened but we're now thinking of
consciousness in terms of deliberation
and that's that's a big change but I
don't think it's a necessary change that
is for me the consciousness when you're
completely passive and you are watching
something that makes no sense whatsoever
or listening to sounds that that don't
mean anything in particular that is
conscious for me at least I feel I'm
conscious when I'm having those
experiences how I ever I would know that
you have those experiences I didn't know
60 years ago and I'm no closer to
knowing today
so when I was 16 I read a really
exciting paper they had a lot of
influence on my thinking over the next
several years it was called facing up to
the hard problem of consciousness or
something like that I forget who wrote
it this is like I would actually say
that like the most important problem to
solve is actually the inverse problem
how do we make something that we are
pretty sure is definitely not conscious
still still pretty difficult but if you
have your AI that is sufficiently
powerful coming up with hypotheses to
explain the world and the world contains
people we have to worry about the fact
that the best hypothesis to describe a
conscious being is much more likely than
many other computations to be a
conscious being even if it's not the
same conscious being and I would also
like to say a word of optimism about
this seemingly utterly hopeless and
confusing problem because I am such a
famous optimist and that is that if I
flip a coin and I don't know how the
coin landed and I say 50% probability my
ignorance isn't a fact about the coin
it's a fact about me a blank map does
not correspond to a blank territory I
sometimes say that mysterious questions
do not have mysterious answers in fact
we've like made that mistake many times
through history never with anything as
on the same order as conscious
experience in qualia but life like Lord
Kelvin called it like infinitely beyond
the reach of present science how trees
generate generation after generation of
trees from a seed he's sort of like he
got a big emotional kick out of not
knowing something and made statements
about its eternal and accessibility
which were violated mere decades later
and that's because when you are confused
about something you're confused the
confusion is part of the map there's no
such thing as confusion in the territory
in some way we are confused about
consciousness and the way we will
recognize our answer is when we know
that we have stopped being confused and
there isn't there isn't going to be a
fundamental barrier it's just going to
be something that we were confused about
and we will recognize the answer because
instead of the new theory of
consciousness still giving consciousness
a sort of sacred inaccessible opaque
status and saying this is the ritual
that matter performs to summon the
inscrutable demon of consciousness we
will wake up and realize that we are no
longer confused about the subject
meanwhile if you build an artificial
intelligence and you need it to like
have some level of reflectivity split it
into two processes have like AI one
being reasoned on the object level have
AI to being reasoning about AI one but
if you can possibly avoid it don't have
AI two reasoning about itself and like
enforce strict type discipline on the
reflective process so that like there's
only a is reasoning about other AIS and
hopefully that will prevent it from
being conscious in the meanwhile while
we work it out okay I think maybe Tom
Nagel my everything else to say about
consciousness that has been largely
absent from contact with it proposed
economist remarks and partly due to the
fact that despite Dave's efforts he
couldn't get any moral philosophers
except to agree to appear
there haven't been expressions of moral
skepticism that is the view that this is
all complete the acknowledgement that
there are fundamental
there are some basic widespread
agreements about about about certain
sorts of cases but cases in particular
in which there are conflicts between the
interests of different individuals give
rise to multiple contestant in quite
complicated theories and the question of
the process what is the process by which
we try to address these questions and
the other is the big problem of how we
make collective decisions in the
situation issues of political theory
that arrived from this question so in a
sense there is room for expansion of
this discussion to take in a more
complicated conception of the ethical
background and it's essentially
contested nature and also about the
question of whether we should think of
ourselves as discovering eternal truths
when we engage in moral investigation or
whether this is a field in which things
are changing and will continue to change
in in very partial answer
in very partial answer to Tom speaking
as a developmental psychologist who's
also an AI researcher I think part of
the answer here is that we don't want to
instill the complete set of values and
ethics in the machines we want to give
them some opportunity to do reasoning
given some premise and I think it's the
same thing as a parent you you you don't
want the child to be a carbon copy of
you but you don't want them to go off
into random space either we may be
pushed in a similar position with the
machines where we give them some
guidance we're basically building their
innate apparatus and giving them their
first dose of information but then
they're gonna go out and reason over
cases that they've seen in the world in
some fashion and I think that's a very
flawed system right some people their
kids raised you know the kids do things
that they wish that they hadn't right
some of them grow up to be murderers and
so forth and my one of my fears in this
general area is we might develop systems
for essentially moral education for
robots that are like 90% efficient or
effective you know if you can even have
such criteria given your question but
what about the other 10% and what about
the scaling problems you know you know
how many bad machines there quote bad
machines there have to be in the context
of a lot of ones that have learned the
things that we want before we get really
really worried so I think there's like a
developmental in the psychological sense
piece of an answer to your question is
by no means a full one but it's one way
to think about at least a little bit
just one sentence
one of the points I was trying to make
was that indeed values are contested and
they're contested within the individuals
not only between individuals that was if
there is a basic inconsistency it's a
it's a different problem than the
problem of resolving differences you
know as Tom himself has pointed out in
an article on moral pluralism that there
may be different considerations that
people have to take account of in making
a moral decision but that doesn't mean
that there's necessarily inconsistency
so the fact that there seems to be
conflict between you know should I take
care of someone's rights or should I
can't look at the consequences or what
about my virtues your if that's I'm not
sure that that's what you mean by you
know internal incoherence but that's not
taken by philosophers to be a sign of
internal into incoherence I actually
wanted to ask question about
consciousness may I get a chance at that
the thing is that this has to do with
bioethics in a sense there is this
investigator a moral floss bioethicists
dr. Aetna Cornel Wilde John Finn's I
think his name is and he's recently made
clear that people who are declared to be
unconscious perhaps this is a question
for you
I forgot your name I'm sorry this is a
Susan people who are declared to be
unconscious in the sense of you know
tests of consciousness that we use if
they're asked you know can you tell
pretend that your of hitting a tennis
ball the areas of the brain that are
usually involved okay so and they
respond they seem to respond even to
questions like you know do you think
your mother's still alive yes enough
right now if these individuals are
unconscious in your sense or
mrs. maybe related to your question
David you know suppose that permanently
we were like that in other words we were
functioning in a way that was really
pretty high level and I don't know
whether they asked them to prove
Fermat's Last Theorem and maybe they can
do it when they're unconscious the point
is that you know if we were systems like
that or you take that person in the
hospital room and the reason this came
up was the question is look should we
pull the plug on him all right
he's been unconscious for years and then
all of a sudden they discovered that
he's responding to these questions okay
and they don't pull the plug so the
thing is that if in the case of human
beings we find unconsciousness but with
the same sorts of responses to questions
or whatever right
do we want to extend the concept of
consciousness or say that someone who
was once conscious when they fall into
the state has different rights than
someone who was forever from the
beginning that way you know I don't know
what Anand cephalic scan do you know can
they respond to this question the point
is that there could be human systems
that are unconscious in this sense but
are functioning the way AI systems work
and we don't treat them like we're
treating all we think we ought to treat
AI we're treating them differently we're
not pulling the plug on them so that was
my question I love that so I love that
point I think we have to bear it in mind
you know this whole conversation on
machine consciousness just leaves me
struck by absolutely it's it's going to
be incredibly difficult to figure out
how an AI or a super intelligence that
structured so differently from us is
conscious I mean we can't even figure it
out in these tricky cases and I mean I
entire I think what you said is an
excellent point we just need to we need
to bear that in mind
just on the on the patient's in
vegetative say this was actually the
main subject of our centers conference
two years ago measuring
like okay next time we're holding the
conference at midnight for you Frances
[Laughter]
yeah but in fact this the subject was
very much under debate at the on those
patients who showed the brain response
associated with imagining playing tennis
or imagining walking through your house
I think the the canonical astounded you
on this is that is a sign of
consciousness so it's not going on
unconsciously or there's certainly
there's a lot of debate and dispute
among that among neurologists but
actually one thing that came up at that
conference of an urologist is there a
lot bit and this connects right to a
point that Eleazar was making we're not
better at getting ever things that look
like evidence for consciousness and
things that look decisively like
evidence for lack of consciousness and
if you look at the tests that
neurologists use for what they call the
you know the absence of consciousness
they're terribly unconvincing on
philosophers grounds you know things
like really complicated stuff like
planning and goal directed behavior and
so on nothing like that we think is
needed for consciousness you know simple
pain simple sensory experience we really
don't have very much in the way of
decisive tests for the absence of of
consciousness which no it is true that a
number of processes seem to be down
unconsciously by the human cognitive
system which which gives rise to the
possibility that AR systems may be doing
that consciously but you know may be
doing all those things unconsciously but
speaking for myself you know a lot of
people recently have taken seriously the
idea that consciousness goes all the way
down the the natural order and there's
some element of consciousness even in
very simple information processing
process is something like Giulio Tononi
view has that consequence for example I
don't think we're in a position to rule
that out and that really does have to
raise the question what matters morally
isn't just the presence or absence of
consciousness per se but the kind of
consciousness the character of of
consciousness and certain forms of
complex consciousness and better than
cognition for example so I think you
know much as I would love all the ethic
ethical issues here to be about
consciousness I think that might just be
the beginning of it sure go ahead
[Laughter]
okay I would say we're pretty well at
the the point where the conference is
coming to an end I get Ned and Matthew
and John Simon a baby's got a few words
of thanks to two people to end the the
conference it's been a it's been a
wonderful thing we want first of all to
thank all of you for coming to thank all
of our speakers
I'd like to like to give a shout out to
over helped us we've had for the
conference you've seen you've seen
Ileana and and Joe and Albert running
around the auditorium and downstairs
we've had a whole bunch of others Cassie
Cassie and Jennifer and Lee bond have
played a very central role and
administrative support for the
conference from bioethics and from the
philosophy department or any other
crucial volunteers we should know him
you're all the older bioethics students
and velocities that'd be fantastic and a
whole lot of a whole lot of support
really the key role in getting all the
legwork for this conference together and
endless emails and and running tasks for
months on end has been this man here
Jonathan Simon
and finally Mia Matthew Nate and I would
just like to thank all of you for coming
and all the speakers for coming and I
hope you have you been stimulated to
think really interesting thoughts about
the ethics of AI for some years to come
so chess and let's not</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>