<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Yoshua Bengio - Deep learning &amp; Backprop in the Brain | Coder Coacher - Coaching Coders</title><meta content="Prof. Yoshua Bengio - Deep learning &amp; Backprop in the Brain - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Yoshua Bengio - Deep learning &amp; Backprop in the Brain</b></h2><h5 class="post__date">2017-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FhRW77rZUS8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right our next speaker is yoshua
bengio again someone who I don't think I
really need to introduce but since I'm
here and I'm supposed to I guess I will
do it he got his PhD in computer science
from McGill he's professor of computer
science and operations research at the
University of Montreal and a Canada
chair Canada Research Chair on
statistical learning algorithms he's the
author of over 200 publications with
over 75 thousand citations co-creator
and general chair of the international
conference on learning and
representations
he was since he's been inspired by and
trained again by the likes of Geoff
Hinton Yamla Kuhn and Michael Jordan
he's been sexual to the development and
the explosion of work on and the success
of deep learning his paper with Jana
McCune on deep learning has been cited
over 9,000 times he's known for his
contributions such as curricular
learning and general adversarial
networks he's been instrumental in
bringing connectionism from principles
and parlor tricks to practice from toy
models of semantics and family relations
to cars that are starting to drive
themselves but I think what is most
impressive at least to me is that he's
done all of this while remaining within
the folds of academia and I don't think
I have to sort of elaborate on that
point I really greatly admire him for
that I think he stands as a model for
the kind of scientist I'd like to see of
the future as I believe that academia is
still and will continue to be the best
place for real deep learning so with
that all right well thank you very much
thanks for the organizers to have
brought me here and a witness something
that we don't see very often a meeting
of the minds from these different
communities so I'm really really glad
that glad that this is happening for
those who are interested in learning
more about deep learning with my
co-authors I wrote a book called deep
learning that came out last December and
that's meant to help people with
some undergrad math and computer science
get into the field or I understand it or
use it and apparently it's is selling a
lot I want to thank a number of people
but we have a little bug here so I'm
gonna tell you later about some work
that I've been doing with several
collaborators in the last couple of
years trying to bring back prop closer
to the brain so in particular Benjamin
say Walter San Juan Sacramento Ashiya
Fischer Tom Amina dong-hyun Lee Oakes
Alexa billion nuke and Jonathan Venus
who's here and of course I want to thank
Jeff Hinton who's been really an
inspiration for much of that work and a
lot of other work I've done so before I
start let me let me say something
important regarding the kind of work I
want to do and and how I think it
relates to the goals of this conference
so we're trying to understand the brain
all of us here I think and there are
many ways in which we can relate to this
question of understanding and and the
way that I like is you know when I
understand something I can I can explain
it someone else in a in a compact way
right so that's that's what I mean by
understanding that's the kind of
understanding that we have in physics
with fairly simple physical laws and of
course if you consider physics the full
description of you know how things are
happening in the world is incredibly
more complex than the physical laws that
underpin it and the thing that I'm
interested in is really discovering
these kinds of of compact descriptions
that help us understand human and
machine and animal intelligence so in
machine learning we can bring something
to this exploration basically we're
looking at such principles and
testing them by running experiments with
machines not with people or animals
trying to see if those principles
embedded into algorithms and and
specific software implementations can
solve difficult tasks can learn complex
tasks the thing that I'm gonna tell you
a little bit about later is also
important if we want to see if those
principles relate to how humans become
intelligent we need to validate those
kinds of ideas with neuroscience and
biology and I think you know it'd be
nice if we had more of that of course
okay let me spend a little time to talk
about neural nets and what brought me to
this field in around 1985 which was
connectionism because a lot of what I'm
doing and a lot of what's the success of
today is deep learning owes to the ideas
that came out in those days so I've
tried actually to do a one sentence
description of the main lessons that I
drew from connectionism from the 80s so
it's about iteratively learning
distributed representations through a
composition of newly inspired simple
operations so these are what the neurons
the abstract neurons are supposed to be
doing towards a justifiable training
objective this is something that I
really think is quite important that the
whole system is optimized respect to
something such that the learner is
forced to capture the relevant
statistical structure of the data so
that's connecting to maybe higher-level
goals about understanding the world and
probability as an important tool to do
that now let me say a few words about
one of the things that really makes
neural nets work and that is distribute
representations and and depth so the
multiple levels there is now
a number of papers several from my group
which produce mathematical and empirical
evidence that these two aspects can
provide an exponential advantage in
terms of ability to generalize our
ability to represent functions compactly
compared to machine learning approaches
or even you know other kinds of
approaches that don't exploit these
these kinds of assumptions because
basically one thing that is important to
understand about machine learning is
that you can't get generalization if you
don't make any assumptions about your
data and and humans are learning
machines and so they're making
assumptions about the world and these
assumptions somehow are embedded in you
know in our genetic material and and the
construction of our brain even before we
were born so a lot of what we're trying
to do in machine learning is come up
with a way to formalize those
assumptions maybe it's in the form of
architectures maybe it's in the form of
the training framework it can come in
different forms it doesn't have to be
always very explicit but at the end of
the day this is you know what
distinguishes a successful argument from
another and of course also the
computational aspects that that we just
heard about of limited resources is
something that that comes into them
matter now in particular here I want to
mention something interesting which is
that if you think about these these deep
neural Nets and and the kinds of things
you're computing if you if you cut
through the middle and you look at some
hidden layer and you think about what
each neuron is computing is a feature
what you get with that representation is
a description of the input space which
is very very rich that's that's the key
with distributer presentations that they
can represent an exponentially large
space of configurations of the inputs
but what makes it possible to generalize
very well is that you can learn from
about this exponentially large space of
possible configurations with a number of
parameters and there's a number of
training examples which doesn't scale
exponentially but
scales linearly with the number of these
features and and that's one thing that
distinguishes neural nets and especially
the deep ones from say nonparametric
more classical nonparametric methods one
example of this actually that has been
seen practically I mean in experiments
is from the work of Carabas group where
they've tried to characterize what a
deep convolutional net trained on
recognizing places had learned in it's
hidden units and you find units that
specialize on you know have discovered
the notion of people or lighting or
tables at least as far as their visual
appearance is concerned even though
these concepts were never shown or in
terms of as labels by the system so
somehow these these features actually
correspond to meaningful concepts at
least at some level so one question i
asked i get asked a lot is so what's new
with neural nets with you know deep
learning and in this century and of
course one important aspect is we can we
have tricks now to train these networks
even though they're deeper than what we
were able to do in the 90s and it turns
out that one of the tricks and they're
few people know about this actually came
from work we did inspired by biology so
my students a big Lauro wanted to
explore non-linearity for neural nets
which was closer to biology and of
course these kinds of rectifying
non-linearity had been used for a long
time by biologists but it turned out
that when you you stick them in these
deep multi-layer networks
it just makes training much easier and
they have now become a standard that
really you know there are some
variations now but it's really important
another example of inspiration that came
probably from biology I assume because
it came from Geoff Hinton is this
injection of a noise inside the network
what we call dropout which has some
similarity to spiking and helps to
regularize and make these networks more
robust another thing we're which has
seen a lot of progress in this century
compared to the previous one is
unsupervised learning Jeff I'm sorry
yan-lo command talked a lot about this
and he introduced Gans the giant Avadh
virtual network switch which on which
there's a lot of work recently so I'm
not gonna go through this but you know
if you have the idea that neural nets
are just pattern classification machines
you know things have changed and they
can generate these kinds of images so
they can you know creatively produce a
diversity of images sounds text and so
on that's quite impressive although I
would say as John was saying we're far
from having nailed that particular
problem um another thing I want to
mention that's connected to one's
provides learning and I put the slides
because I feel like in talks that I
heard here this notion wasn't
necessarily compatible with with what I
heard the notion of disentangling
factors a variation something I've been
discussing a lot in in the last decade
when we were trying to learn
representations so what is our you know
new on that learning and it's hidden
layers or what it should be learning and
and what kind of representations you
know our brain learning you know there's
the question of what makes a good
representation and of course for many
decades we've we've known that good
representation is useful representations
would be ones that somehow would be
invariant to the things we don't care
about and only sensitive to the things
we care about but but the problem is if
you consider like a learning agent like
a human or an animal or a robot that's
gonna have a very long life it's hard to
know ahead of time you know what the
future will bring us and so if you're
not sure what the future task will be
then you should you know as a safeguard
instead of throwing away a lot of
information that currently you don't
need you
try to be a little more conservative and
just separate the different aspects of
the world some of which may be relevant
now and others might be relevant
relevant later so this is the idea that
we don't want to necessarily compress
information and eliminate what we
consider now to be noise but rather to
separate the different aspects that
explain what we're saying the causal
factors ideally and if you're able to do
that then you can you can really
considerably reduce or even eliminate
that what's called the curse of
dimensionality that makes sistex
difficult in high dimensions and it
really makes it easy for a learner what
once it has discovered those factors to
to learn a new task and and you know to
essentially just select the factors that
are important and even do things that
think that seem crazy like learning from
zero examples but I won't go and explain
that
another thing that's changed is that's
related to the idea their neural nets
are not anymore just processing vectors
to do classification is the introduction
of attention mechanisms in neural nets
this is really really something that's
changing how we do machine learning
these days so attention is in principle
very simple it's about gating some
computation and and maybe selecting some
assets some computation to focus on on
aspects of after data or of the elements
in our representations and we introduced
this in the context of trying to crack
the problem of machine translation using
real nuts a few years ago and we
introduced a particular form of
attention which we call a content-based
soft attention which has turned out to
work really really well in which the
translation a neural net will generate
one word at a time in the output
language say it's going to produce one
word in English one after the other but
as it does that sequentially it's
allowed to look in the input sentence or
its representation as semantic
representation
has been extracted by some other
recurrent net and to focus its attention
on one or a few words and this ability
to just you know eliminate from the the
process saying most of the input sort of
just makes it much easier to do some
tasks and it was night and day when we
introduced this thing into the systems
and you know in in a few months we went
from really bad systems to
state-of-the-art systems and a year and
a half later Google put that in their
Google Translate and of course there's a
lot of other complex engineering tricks
and and other nice things that have been
added but but now if you look at how
machine translation was a couple of
years ago with engrams
compared to sort of how humans translate
according to human evaluations the
current neural nets have more than half
sort of the distance between the old
systems and human translation so this is
something pretty important and of course
it's it's allowing neural nets now to
process not just images and fixed size
vectors but any any data structure
anything that can be represented by some
graph by some list by some tree in any
kind of data structure set you know we
can we can use these things as input we
can produce these things as output using
attention mechanisms and and this kind
of techniques has also been used and I
think this is also a very important
change from what we've seen in the past
to augment these neural nets with a
memory so in other words you can think
of it like we we have a piece of network
which is like cortex like which you can
think maybe as a controller and and
something that represents information
and and combine that with some things
they may be hope hippocampus like that's
gonna allow you to do one-shot storage
and represent memory that can be stored
quickly and recut and and and then we
can use attention to decide what to
write where and what to read where I
mean where to read and and again we can
learn you know where to put attention
given the context so depending on the
state of the system those networks can
to attend to what's most relevant to the
current state of the system and and once
you see this you can also see how this
can be used to reason at least in the
new on that sense so we're not talking
about reasoning in the classical AI
logical modus ponens sense
we're just saying what we mean by
reasoning here is that the network will
sequentially combine pieces of
information and through learning it
figures out you know what's the right
way of combining these pieces of
information so depending on the tasks it
will learn to do modus ponens or it will
learn to do other kinds of reasoning
that we know exist and humans do for
example and this has been used for
question answering tasks as an example
all right so to put what I'm talking
about it in the perspective that Josh
Tenenbaum introduced at the beginning of
this conference neural nets are
somewhere in between brain
implementations and and cognition and
there is a lot that could be said about
how you know they can move up or move
down depending on the kind of
investigations we're looking at some of
the things I've talked about how
attention can you know help us do data
structures to learn to access memory to
reason I haven't talked much about
language and semantics but it's also
highly related at the end of this
presentation I'll say a few words about
a current project which involves
representation and the notions of
objects and causality which I think
relate a lot to cognition and but for
the most part I'm gonna tell you about
some work that I've been doing about
that's moving down closer to biology
about how backprop could be a
potentially implemented by brains so so
back prop Blake Richards said it also I
don't need to repeat these things he
gave a really great introduction to this
subject and has amazing new ideas that I
just died to read the paper for
understanding better but so some of the
key points to
to get from that is the thing that
really works in the applications of deep
learning is this idea of end to end
training where we we have a big
complicated system with many modules
with attention with memory with
different pieces that talk to each other
but they're all trained together with
respect to this one common objective
sometimes there's a game theoretical
thing like in Ganz where you have
different modules have different
objectives but even in that case we can
you know have an interpretation that it
sort of makes sense mathematically to do
that so and even in that case we have
back prop being used to do credit
assignment so to propagate information
about how things to change in various
places in the system problem is oh and
by the way and back prop is not just
used for supervised learning it's used
for enforcement learning is used for
unsupervised learning during the models
so it's everywhere right now in deep
learning problem is you know how could
it possibly be implemented by brains in
some of the questions like you know how
would be the equivalent of the
backpropagation equations be implemented
and one thing that I'll mention is
there's one issue which is if you look
at those equations there are actually
linear propagation so of course there's
non-linearity involved but in terms of
the errors the errors that the next
layer are a linear function of the error
of the layer downstream and it's not
clear how neurons could do that linear
con that kind of linear computation and
whether there should be like a separate
Network doing this and and as Blake was
saying you know how is it gonna
synchronize its weights to match the the
main network so that there's all kinds
of questions that are open so here I
just listed a few papers and current
work in that line of thought that I'm
involved in and I'm not going to go
through all of these things but I'm
gonna you know sample some of the ideas
in in some several of these papers so
this is just for reference so one of the
first papers we did was trying to
connect
sdp to something a little bit more
amenable to the kinds of things we do
with neural nets so it turns out that if
you have weight changes that are
proportional to a presynaptic times the
postsynaptic term where so the role role
here is a firing rate and s is the the
voltage potential or the equivalent and
and so if we take the rate of change on
the postsynaptic side times the firing
rate on the on the presynaptic side we
we get weight updates that end up
looking like the s GDP the signature of
s GDP if of course you you you put in
spikes on top of these rights so that's
that's one there was one starting point
with which stimulated the next piece of
work which was about I I think one of
the key ideas here which is that if you
if you have a network that has both feet
forward and feedback connections and and
here I'm going to assume that the
feed-forward and the feedback
connections are symmetric so it's it's a
recurrent net and and you let it settle
to a fixed point and later I'll tell you
that we can get rid of that assumption
so if you clamp the input is gonna
produce some outputs and now there's
something interesting that happens if
you nudge the outputs a little bit in
the direction corresponding to a better
answer in other words you're you you now
know the right answer and instead of
clamping it on the outputs you're just
pushing a little bit the outputs towards
the right answer so this creates a
perturbation to the fixed point and then
if you study how that perturbation
propagates down the layers it it it
looks like backdrop it there's some
scaling issues but it has essentially
the same properties as backdrop so this
was a kind of starting point and when we
did this paper we considered that this
these fixed point
computation we could think of inference
in fact if instead of just running the
deterministic dynamics we consider
adding noise to this then this
approximates a particular form of Monte
Carlo Markov chain and the important
thing to have in mind here also in in
the later work is the way we conceive
the dynamics at least as a convenient
way to think about that it corresponds
to minimizing some energy function and
so here the the state of the system is
just evolving by gradient descent on the
energy plus some noise so so you can
study both deterministic systems and
then we're interested in those fixed
points corresponding to where the energy
derivative with respect to the state is
0 so as dot here is a temporal
derivative so fixed point means DSD T
equals 0 but we also want that to
correspond to a minimum of some energy
function and and you can you can
generalize these kinds of notions to
stochastic units and some of the
theorems we've worked out have the
stochastic version where now you don't
actually get to a minimum but you get to
a stationary distribution which just
corresponds to the Boltzmann
distribution of this energy function
which just means that if you wait long
enough then the state distribution is as
a probability proportional to e to the
minus energy which is just a stochastic
version of the deterministic thing ok so
so one way to think about this nudging
idea that got us some mileage inand the
latest paper we published on this which
we call equilibrium propagation is that
we're gonna change the energy function
so instead of clamping like we do in in
both machines or contrasted Haven
learning instead of clamping the outputs
and having these two modes or two phases
we are still gonna have these two phases
but in in the phase where we get to see
the answer we're just gonna change the
energy function so that we're gonna pay
a small price
beta times the cost of our prediction
compared to whatever the target was so
that's the the beta here is a small
scalar small positive scalar in the
nudging phase but it would be zero in in
the prediction phase so you could
imagine we have some network with
recurrence and then the outside world
you know imposes some constraints or
nudging when the information is revealed
so so we came up with this theorem
mostly the work of Benjamin SLA that
ends up telling us how we should change
the weights so worried about the
notation gonna explain this but but
essentially we end up with an equation
that tells us what's the true gradient
of the cost function the thing that
we're trying to minimize which may be
it's like prediction error but it could
be any objective function that for which
we we know how to compute derivatives
with respect to the output and and we
end up with an equation that says well
we should consider small perturbations
small beta small nudging and so in the
limit of these small nudging we're gonna
just compare the derivative of the
energy with respect to the parameters
which are what we call sufficient
statistics so the same kind of thing we
see in both machines in in both
situations one corresponding the fixed
point achieved when the network is
allowed to relax and produce its answer
and one when we also impose a little bit
of nudging towards the right answer so
we have these two forms and we take the
difference between those two and that
gives us the gradient and there is a
stochastic version of this
alright so that's interesting but there
may still many things that are not
biologically plausible in this
formulation first of all how does that
relate to things like a CDP well if you
choose a particular energy function
which looks a lot like the hub feel
energy function then you end up with
weight updates which are essentially the
same as what I had before except they
are symmetrized so
if you had a way to make the
feed-forward and the feedback weights
agree with each other then you would get
these kinds of updates and you're able
to train neural Nets these kinds of
neural nets with feedback connections
using this this rule the the one most
bothering problem with this particular
approach is that we have to really wait
to get close to the fixed point so it
might take like hundreds of time steps
of simulations here before we are
allowing the network to do an update of
the weights otherwise the gradient is
not correct and it doesn't learn well
and and and also that this depends on
how many layers you have and somehow for
longer for deeper networks it gets it
takes more and more time to you know
enough precision in in converging to the
fixed point so so these are sort of
nagging problems that we're trying to
fix now and I'll tell you about in a few
minutes
another issue so related to this issue
it takes a lot of time to converge to a
fixed point we investigated a particular
kind of energy function which I call the
feed-forward energy function and it's
not really a new kind of energy function
or something something that people have
studied in different guises so it's just
an energy function that says that what
we really want to have at the end of the
day is the result of a feed-forward
computation right so if you if you
minimize this well the minimum is when
the state at level K is weighted some of
the firing rates corresponding to level
K minus 1 so that's just the usual
feed-forward computation in other words
if we had an implementation that was
just doing feed-forward computation in
one pass it would achieve the fixed
point at least in the prediction place
now there's there are some issues with
so that's really nice because we get you
know convergence immediate convergence
and and also that allows us to connect
with traditional neural nets and so you
maybe you can ask me well so what
happened with the feedback connections
well they're still there but they're
sort of implicit in the energy function
so in in the previous energy functions I
showed you we had W IJ and w GI we had
feed-forward weights and feedback
weights and they had to be symmetric and
so on now what we have is that if you
take the derivative of the energy
function you have two kinds of terms and
they correspond to the feed-forward
computation and the feedback computation
and the feed-forward part of the energy
function we can just recognize when we
set that to zero we get we get the
normal feed-forward equations and and
for the second term you notice that if
the first term is zero in other words if
we if we if we have set the activity
according to the feed-forward
computation the second term vanishes so
in the feed-forward situation the
feedback will somehow magically cancel
if we were to follow that derivative but
of course the question is how would a
biological implementation do those
computations that the feed-forward thing
is a usual neural computation it's not
clear how to do the other one there so
that's one thing I'm gonna be telling
you about yes yeah as I said the that
energy function yeah leads to one pass
convergence at prediction time and if
there is no nudging then then the the
the second term the feedback term
cancels and and then there's something
even more magical that happens which is
that in the the gradient formula from
equilibrium propagation we have these
two terms one one which is in the
prediction phase and one which is in the
nudging phase and it turns out that
with these kinds of energy functions in
the prediction phase with beta equals
zero the first term cancels become zero
and so the the only thing that remains
is the in the sufficient statistic that
we need for our update is the second
term which comes in the nudging phase
which essentially if we're going to see
corresponds to implementing back prop so
so what this is saying is maybe we don't
need to have two phases maybe we can be
having a system that's continuously
making predictions and getting feedback
at the same time and we don't need to
have this alternation between predict
get the answer update predict get the
answer update which is a traditional
view that we have with with back prop
and that we had in the first formulation
right so let's let's look at this a
little bit in in a bit more detail if we
are near the fixed point solution so
beta is small and we look at the
equations for for the derivative of the
energy that tells us in which direction
the the neurons are moving so what we're
seeing is that we have a term that's
pushing each neuron to satisfy the
bottom up input so it's just saying you
know the the the previous layer is
telling this guy what to do with the
usual feed-forward kind of computation
and what this guy is doing is saying aha
well you should change the layer K so
that if if the the guy at the next layer
has a discrepancy between its state and
and what you're sending it then you'd
like to help it reduce that discrepancy
by moving in this direction all right
and then if you're an output unit you
also get to be nudged towards whatever
the outside world wants you
to do okay so if we if we set this equal
to zero at the point where beta is small
then we we see that these these errors
the discrepancy between the the the
state and the bottom-up input into the
the neuron these errors actually
correspond to back propagated errors so
if you look at this formula and you
remember something from back prop it's
exactly the formula for back prop in
other words there is a way to find the
solution to the nudged phase which
involves a single downward pass which
only propagates these perturbations and
and it would compute exactly the same
thing as back prop okay now there's a
problem here which is that in this phase
where we're just propagating those
perturbations from layer k plus 1 to
layer K we're doing this computation
which is multiplying this error that we
have this perturbation that we have this
discrepancy between the bottom-up and
top-down really at the next layer and
then multiplying by the non-linearity of
the derivative the donor on and
multiplying by the transpose of the
feed-forward weights and so there are
several issues here one is how do we get
the transpose of the feed-forward
weights and the other is this whole
thing is linear in the perturbations at
the next level so how are we going to
implement this kind of equation which is
linear computation so one idea that we
are working on and seems to to give
interesting results is to use lateral
connections to set up a system that will
allow to implement this kind of
equations because yeah we we know sorry
we know how to implement that part of
the updates but it's not clear you know
what's the neural implementation of this
thing so how do we do that so so the
idea that we're exploring is that these
ease these errors which correspond to
the back propagate
errors they are going to correspond to
the difference between bottom-up and
top-down inputs in pyramidal cells and
so the bottom-up we know is just a
weighted sum of inputs from the previous
layers so what about the top down
well top down is actually going to be
the combination of two things is going
to be the combination of actual feedback
from the next layer and lateral
connections from the same layer through
inter neurons so I'm going to show you a
picture here that's better
so you have these pyramidal cells they
have bottom-up input and that's gumming
coming in to the basal dendrites and
they have these apical dendrites that
Blake told you about and then we're
gonna have two kinds of inputs into the
those dendrites the direct feedback from
the next layer and and through what I
call mirror inter neurons maybe March
energy cells feedback from other units
and the same unit but you know I mean
other units on the same layer to the
same place and and so what we're gonna
be doing is we're gonna be thinking of
training those Landfill connections so
that they are canceling the feedback
coming from above so the network is
trying to figure out what the feedback
from the next layer is gonna be and
learning to cancel it why would we want
to do that because the only thing that
this layer can predict about the layer
above is sort of the normal behavior
that doesn't depend on the outside
injection of of errors so it's gonna try
to predict the the behavior of the
downstream units in the situation where
beta equals zero where there is no error
there is no external signal that comes
in and so when we combine the the signal
from above with the lateral connections
if there was no nudging then this would
be zero and there would be no
contribution to the learning here but if
there is a an error higher up then it's
gonna show up here and potentially
produce a signal that can
drive the learning here so that's that's
what we've been experimenting with and
uh and we can train em lists on the
two-layer network using this kind of
thing but now let me make other
connections so there is an interesting
connection between this and some of the
ideas that Geoff Hinton proposed in 2007
that Blake already referred to that
relate those those errors that are being
propagated to temporal derivatives of
the of the activity of the neurons so
now so it turns out that if we start
from the the condition where there is no
nudging and we consider how these these
errors are going to change over time
well they will be proportional to those
those propagated gradients that we'd
like to see so that the the error
information is really encoded in the
temporal derivative of the activity of
the neurons and the other thing of
course is you know if you were to follow
the backrub rule what what kind of
update do we want well we want something
that's very similar to what I showed you
before except we're we instead of the
temporal derivative of the postsynaptic
we have this error but that errors is
the temporal derivative right so this
whole thing is the same thing as we had
before so again this is something that
would be naturally implemented by a CDP
okay let me say a few words about the
wait symmetry which I sort of mentioned
as an issue
so again Blakeman I told about this
already and and and he told you about
feedback alignment experiments
suggesting that well maybe you don't
need to have exact symmetry in fact you
can fix the weights I you know I I have
some doubts about whether this
completely solves the problem I think
you you still want those weights to be
approximately close to the transpose or
functionally close to two that
computation but there is something I
think that can save the day here
and we'll have later uses which is a set
of both experimental results with
simulations with the noisy encoders and
a set of theoretical results from the
group of Aurora which both suggest that
if you if you consider two consecutive
layers and you think about the feed
forward computation and the feedback
computation is together forming an no.2
encoder and that auto encoder in our
case has also noise injected in it
then if you if you try to optimize the
weights so that you minimize
reconstruction error you will often end
up with those two set of weights
converging to the transpose of each
other all right so I think this this is
something that needs more both vertical
and experimental work but there is
evidence on both sides so for example
the theory requires sparsity which is
something reasonable for brains and and
potentially other assumptions that need
to be verified experimentally but there
there's this paper we put out seven
years ago which shows something like
this happening with Widodo encoders so
that's so I think there's some hope on
this issue of symmetry of the weights
and finally there's another thing I told
you about which was that well we don't
really want when we we have these
dynamical systems we don't really want
to wait for a fixed point before we do
another we'd really like to have a
system that can make updates all the
time and and so we have a new
formulation and this is work with Walter
Sen and experiments are being done by
genres and Venus was here in which
instead of thinking of the state of the
system as the activity of the neurons we
think of the state of the system as
jointly the activity and their temporal
derivative so the velocity something
more similar to what we find in in
physics then actually finding the
solution of de D this extended state
equals zero becomes very easy and if
you just write this equation you end up
with a relationship between the state
and its derivative which defines the
dynamics in other words you can give me
any dynamics which is whatever your
network does and I can I can come up
with an energy function that is such
that so long as the network computes
those dynamics you you now are always on
this particular manifold of the state
and its derivative that satisfies this
equation and so we can actually apply
the equilibrium propagation equations
that tell us how to update the weights
all the time we don't have to wait for
convergence and we tried this and it
works and it's is very stable and it
seems to be robust to the depth so we
need to do more experiments with that
but the figure shows that well the
errors go goes down in some of the
experiments we're running pretty quickly
and and also that you know you can
change the inputs and the nudging
targets on the fly and and and you can
update all the time and the size of the
I Bates updates might not be the same
all the time these are the size of the
updates but learning works ok so so
there's a lot more work to be done we we
want to we want you to test those kinds
of theories on real brains and and I'm
not an expert enough in in biology and
neuroscience to tell you what the right
experiments should be but there are some
pretty obvious things that you know
should be true for example well testing
the idea that we are doing something
like gradient descent could be observed
by checking whether after the updates of
the way it so the next trial the the
some key neurons that was important in
the the behavior should improve in the
sense that it's it's now doing something
that that makes the output closer to the
right answer and if we're able to
identify such neurons then we should be
able to see that not only after the next
trial you're getting a sort of more
appropriate activity for this neuron but
in addition in the few say 100
millisecond or something on that order
of magnitude after there's been a
surprise and learning you know due to
nudging the
this neurons activity should also
improve in other words this is due to
the weights changing and this is due to
the activity changing so the the the
nudging has an effect that all the
neurons should be moving towards quote
unquote better configuration that would
give rise to lower error and this
potentially could be something we would
observe if we can make these
measurements this this theory about
having feedback and natural connections
cancelling each other each other again
is something that can probably be tested
for example if there is no nudging there
is no surprise then somehow the
contribution from the apical dendrite
should should be zero and if you're able
to stimulate those inter neurons either
to increase their activity or decrease
it then you should be able to see you
know increase or decrease LTP
corresponding to that and I'm sure many
more experiments can be devised all
right so to conclude that part bad prop
has been really the workhorse of the
amazing successes of deep learning and
would be great to see if brains can have
a functionally equivalent implementation
of something that does credit assignment
according to similar principles and it
would help us understand how brains can
learn more complex tasks may be
different from the typical tasks that
people in psychology and neuroscience
are testing but more like the tasks that
machine learning people are testing I
don't have a lot of time so let me just
move to the last little topic my last
slide to talk about something now going
in direction of cognition so I've talked
mostly about neural nets making contact
with brain sciences so there's also a
lot of work in in our field trying to
make neural nets do more sort of
high-level cognition types of
computation and in in my group the way
that we're thinking about this is
representations because someone said
representation learning is the heart of
what deep learning is about it is how we
called our conference I see
our international conference on learning
representations and I don't think that
right now we have really satisfying ways
of forcing a deep network to learn
representations that are more abstract
in other words that will generalize
better and and furthermore that capture
causality in other words how things in
the world are really related to each
other and and so let me tell you a
little bit about a paper that we put out
on archive recently that attempts to go
in that direction so so the objective is
to use agent learning so some kind of
reinforcement learning in an environment
to help discover better representations
that capture something about causality
that has to do with maybe affordances
like how agents can influence the world
and and and how to use that influence to
name things in the world in the sense of
setting up appropriate representations
and so so to to explain that idea let me
use ooh let me use a prop
okay so here's an object here the
control mouse or whatever the pointer I
just made up a policy to control it I
can move it in space and I can do it
independently of other factors that
explain the world run right I could
choose to instead of move my phone and
not move the mouse so what what's
happening here why am I telling you this
because it would be very convenient if
my brain had not only the ability to
produce this policy that controls this
this particular factor that explained
things around me but also to represent
that and and I think that we get a
important clue about good
representations that some of the factors
that hopefully our brain represents
fairly explicitly correspond to aspects
of the world that we can control
independently in other words that we can
change I can change the position of this
bottle and not move other things this is
very important because if I'm going to
be able to control the world later and I
know you know these building blocks
these factors that allow me to control
separate aspects of the world
it becomes much easier to come up with
policies that combine those building
blocks those options as they're
sometimes called in order to come up
with complex policies also those factors
are likely to correspond to good
explanations about the world positions
of objects in images of course is
something important but instead of
hardwiring that notion as we normally do
with supervised learning what I'm
talking about is an objective function
which of which we you know we've tried a
few that is saying let's train a system
that jointly learns policies a family of
policies and and and and neurons sort of
feature detectors at some level in the
network such that for each feature
corresponds a policy and the policy has
the property that it changes that
feature and tries to change as little as
possible the other features right so it
seems to be like a chicken and egg thing
because one depends on the other but
that's not unusual and you know nest
right like each layer depends on the
other isn't we can we can deal with that
and ganz is even worse and so that's
that's interesting because it maybe
helps to separate different aspects of
the world so disentangle them and it
also connects to higher-level notions
that people care about in cognitive
science like objects and attributes and
agents so so once we start talking about
attributes talking about agents is not
very far I mean I'm sorry about objects
is not very far I mean and John an
object is just a collection of
attributes that somehow have something
to do with each other for example when I
move this thing I'm controlling a bunch
of attributes together and and somehow
you know if I can control it maybe I can
more easily change some of the
attributes so there's there's a tight
connection between those attributes in
the sense that the way I can control
them is is is tightly linked and then
the notion of agent comes because in in
my initial explanation I said well those
factors correspond to aspects of the
world that I can control independently
that I can control so I'm an agent right
I'm an entity that can control things in
the world but I could also have factors
in my mind
that concerns aspect of the world that I
can't control but that you can control
and I'd like to model that right so now
I have to think about other agents and
what they could do and even if I don't
have access to some of those factors
directly by you know imagining those
those agents doing things and executing
policies I can build a better model of
the world which of course children you
know use all the time to control us
right all right so so I think this opens
up a bunch of other interesting
high-level questions and connects also
to natural language because I believe
that natural language gives other clues
about these high level causal factors
why is it that we use those words why is
the way we use those concepts in natural
language they are the things that
typically we can we can manipulate that
we can we can act upon and and so if we
want to train machines to understand the
world I'm sure that it'll be easier if
we talk to them that's basically what
I'm gonna close on thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>