<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reproducibility in Deep Reinforcement Learning - Prof. Pineau - NIPS2017 | Coder Coacher - Coaching Coders</title><meta content="Reproducibility in Deep Reinforcement Learning - Prof. Pineau - NIPS2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reproducibility in Deep Reinforcement Learning - Prof. Pineau - NIPS2017</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TAMer41J038" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our next presenter is Shu el pino
Shabazz professor at McGill heads up
fare Montreal sure Alice research has
been in planning learning and decision
making
also in mobile robotics human robot
interaction dialogue and adaptive
treatment design and to this she'll tell
us about reproducibility and
reinforcement learning please join me in
welcoming Joelle thanks very much it's a
pleasure to be here I have started being
very interested in recent months around
about this question of how do we set
about developing the standards that we
need such that the research that we're
producing is reproducible in a way that
we can actually build on this and
hopefully progress more efficiently in
the spirit of nips workshops of ancient
days I would say most of the work that
I'm presenting today is unpublished some
of it has appeared on archive a lot of
it is a very fresh off the press and so
it's a new set of slides a new set of
results I'm very much welcoming the
opportunity to discuss this after the
presentation today digging into the
question a little bit further it's
useful at times to actually think of
what we mean by reproducibility in
research so I went digging for various
notions of reproducibility in science
more broadly than in machine learning
strictly and found this one proposed by
the NSF in 2015 whereby reproducibility
refers to the ability of a researcher to
duplicate the results of a prior study
using the same materials as we're used
by the original investigator
reproducibility is a minimum necessary
condition for finding to be believable
and informative until this quest for
reproducibility and the awareness of the
importance of reproducibility research
isn't something that is unique to
machine learning in 2016 the journal
Nature ran a survey of 1,500 scientists
across several fields asking them
whether they thought there was a
reproducibility crisis in their areas of
science
in a significant number of them 52% said
that yes there was a significant crisis
another number 38% said there was a
slight crisis you know 3% said no not
much of a crisis and a number of them
didn't quite seem to know what was going
on a second set of questions was asked
following this this question was have
you ever failed to reproduce an
experiment and they asked them both did
you fail to reproduce one of your own
experiments or did you fail to reproduce
someone else's experiment and so the
results quite quite telling
it seems the chemists are having the
hardest time at this with a large number
of them almost 90% of the cases it had
happened I'm not saying 90% of the
experiments but in 90% of the for the
chemists they had had some case where
they were unable to reproduce someone
else's experiment and it's small on my
screen 60% or so reported having been
unable to produce reproduce one of their
own experiments they didn't break out to
the data for computer scientists we fall
in the other category which may be doing
a little bit better than the chemists
but probably not as well as we would
hope to do so I had the opportunity last
weekend to be at a workshop sponsored by
CFR and I had 20 or so of our colleagues
are captured in a room and I asked them
the question in an anonymous manner how
many of you think that there may be a
reproducibility crisis in machine
learning and this is the CFR program on
learning and machines and brains and out
of these 20 participants 45% of them
indicated that they thought there was a
slight crisis 35 percent thought there
was a significant crisis and again about
10 percent each so there was no crisis
or weren't quite sure what was going on
so it seems we're not quite immune to
this phenomenon I went further and asked
them the question have you failed to
reproduce an experiment and here it was
very interesting we see much more
differentiation between whether had
failed to reproduce someone else's
experiment
about 80% of them reporting that that
had happened to them in terms of whether
they fail to reproduce one of their own
experiment they were down to about 5%
and so that may be symptomatic I would
not go into very deep hypothesis about
why there's such a difference in our
field versus another I will be maybe you
know optimistic and say that I'm
compared to these other scientists we
actually are in a much better position
to do reproducibility in that for the
most part we are running code on
computers and therefore it should be a
lot easier in terms of an experimental
setup than some of the other fields such
as chemistry and so on I started digging
a little bit more to figure out what
might be the criteria for
reproducibility and I came across this
quote that I found quite telling
especially on the eve of nibs said the
following an article about computational
science in a scientific publication is
not the scholarship itself it is merely
the advertising of the scholarship the
actual scholarship is the complete
software development environment and the
complete set of instructions which
generated the figures and so I take this
as inspiration for a field as we move
forward
my particular expertise is actually in
reinforcement learning and this interest
in reproducibility came really from my
students in many ways I saw them
struggle over and over again to build on
results that were in the literature and
we decided to document that struggle in
a bit of a more formal way and so most
of the results I present today that
apply to reinforcement learning come
from the work of Peter Henderson
riadi slam graduate students at McGill
University and the results are presented
in a paper that's gonna be appearing at
triple AI this winter and it is an
archive already backing up from the work
that they did in the particular
experiment that they did I want to sort
of consider a little bit more carefully
what it means for reproducibility in
reinforcement learning I did say we had
an edge compared to these other
scientists
in terms of being able to run an
algorithm on a computer and so in that
sense we should be in a better position
to reproduce a lot of results but it
turns out it's not that easy there's a
lot of other information that often
comes into play and so David gave us a
beautiful description of the work that
was done on alphago and it would be a
little bit simplistic to say that to
reproduce alphago all we would need it
need is necessarily the code and the
machine in in effect you know there's a
lot of infrastructure that goes into
producing a system of that magnitude
there's the expertise of course from the
experts and there's of course the
ability to play the game against human
experts which may not be accessible to
everyone and so there's a lot of pieces
of the scientific inquiry that come into
play if we really want to be able to do
reproduction of some work and I would
say one of the things that I think we
have to be careful of as a field is to
set us up in a way that we can do this
as much as possible in that in some
cases some of the results that we're
seeing are absolutely impressive
magnificent results but we have to
figure out also how to have a way to
have access to the blueprints such that
we actually know what we are building on
as a science and so in some ways you
know I'm taking alphago as an example
but it is but one of the cases where I
think this reflection applies and we
have a part of the blueprints we have
some of the papers that go into quite a
bit of detail into how the method was
conceived of on the algorithmic level
there's quite a lot of information but
there may still be a lot of the
information that is missing to allow
people to reproduce the work in
particular the access to the data itself
the access to the expert which may be
hard for some of us to contact the
access to the code the specific hyper
parameter setting the computer the
magnitude of computational
infrastructure that it might be
necessary for some of these results and
in particular the evaluation setup
and so as much as I think we have to be
in of some of the monumental progress
that has been achieved in reinforcement
learning I think we also have to be
sensitive to the younger population and
avoid putting them in front of these
monuments of research and asking them to
produce research that is in some sense
you know a little bit more difficult if
we don't have all the ingredients that
we need to enable reproducibility and so
I'm hoping that as we move forward as a
community as the field we have a little
bit more of these Eureka moments where
students are able to very easily and
successfully not just students everyone
the community but replicate these
results in lower the bar to that level
of reproducibility there's been a lot of
efforts I haven't I'm not by any means
the only one who has been talking about
this issue in the community has done a
lot to improve our ability to do
reproducible research in reinforcement
learning in particularly in the last few
years the availability of the arcade
learning environment in many ways I
think was one of the reasons for the big
progress in reinforcement learning that
we saw in the last few years the fact
that this was an suite of environment
very rich but that anyone because
essentially have access to have you in
use and build on in their own research
and the availability of the magico set
of tasks was also very useful and in
some sense I think the four that I Ellis
trait here also the robot soccer
platform and the elf multi-agent
platform that was presented earlier
today at nips are very interesting
because they have different facets of
the problem so in some cases there's
very rich
observational domains and some cases we
have continuous control casts in other
cases we have actually physical systems
in the last case we actually have the
ability to test out multi-agent systems
and so by having a variety of platforms
on which we can do research in which we
can share between labs we have really
made a lot of progress in terms of
enabling the reproducibility of research
in this particular area in the results
that I'm gonna present in the the second
half of the the discussion today I'm
actually going to be building primarily
on tasks I'm having to do with the Mojo
Co environment and so just a little bit
of context in a sense you don't need to
know a lot of reinforcement learning I
hope to appreciate today's talk a little
bit of context in terms of the setup
that we are going to assume for some of
our experiments we are looking at tasks
where that are primarily solved by
policy gradient methods so the Mojo Co
domains have these sort of animated
characters where you have to actually
figure out the right policy to move the
character along and so in these policy
gradient methods typically we're
interested in finding a policy I'm using
PI to denote the policy but policies
parameterize by set of parameters
denoted by theta and what we're
interested in finding is a policy PI
which maximizes the return so ro
indicates the return for a particular
set of parameters theta condition on an
in particular initial state s0 and
because it's a policy gradient method we
actually look at the gradient of the
return with respect to the parameters
and so that gradient is taken over here
and we take into account the the
expected value over here with respect to
the distributions of states and so
there's different variants of that
general idea and I don't necessarily
want to go too much into the specific
variance but it includes variants that
are on policy gradients and other
methods that have a critic and so actor
critic methods that may have some off
policy components and the reason I look
at this particular set up is because
it's a type of method that has been used
with quite a bit of success in recent
years particular in the last two or
three years and we've seen many papers
come out which build on this general
framework I'm not gonna focus in fact so
much on these new results I'm gonna
focus a lot more on the policy gradient
baseline methods which these results
compare to it gets quite interesting to
see in this case how much the results
are replicable in terms of the baseline
results because in a sense these are
reused in many different papers in many
different cases and so the baseline
algorithms I'll be used
for the purposes of today's talk are the
four following algorithms some of which
have already made an appearance in the
session today
TRP o and PPO which is a variant of TRP
o both very popular as baseline methods
when people produce new papers on policy
gradient methods the GDP GI approach
also very popular recently and the actor
critic KT our approach which was I
believe presented at nips this week and
so these are sort of my baseline methods
which I'm interested in comparing just
for the reference but as you we go along
witches which is not gonna matter all
that much to my message so let's
consider applying these four different
algorithms in this case on a particular
task from the Mojo ko simulator this is
the hopper task I believe another half
cheetah task in which we have the
results and based on this I'm not even
telling you which algorithm is which
what we're is observing is that the red
algorithm seems to be really doing much
better than some of the other algorithms
albeit with quite a bit of variance and
so so it's maybe unclear whether it's
actually a very robust methods and so
you may be tempted after seeing results
like this I hope you tempted to look at
another domain or two and see how well
these methods are doing these four
algorithms which are comparing these are
the four algorithms listed here these
four algorithms which are comparing on
some other domains and so you go about
trying another domain you try out in
this case the Harper domain and the
swimmer domain and you get these results
now the cool the colors are consistent
from one algorithm to another it's a
little bit difficult to figure out what
you conclude right in some cases you
know the red seems to be doing a lot
better but then you look at how it's
doing on the other two domain and it's
quite terrible blue seems to be doing a
lot better on the swimmer a lot worse on
half cheetah that's not that surprising
right in some cases you may not expect
an algorithm to do best on all of the
different domains but in this case these
are pretty close domains right they're
not all that different in terms of the
type of structure the observation
spaces the same the action space is
similar not always the same dimension
but similar we went digging in a little
bit deeper than that in fact you know
one of these methods is trp oh I won't
tell you which and we weren't quite sure
we were getting the best result we could
out of our TRP Oh method and so we went
digging for other code in thankfully the
authors of the TRP Oh methods as well as
other groups have made different code
bases available for this so it was
reasonably easy to do and we found three
different code bases for TRP oh and
these are the results we got so so we're
a little bit puzzled in particular with
a little bit concern because different
papers in the literature which we're
using TRP or as a baseline we're using
different one of these code bases it's
not like everyone agrees to use the blue
one and the other two are deprecated we
actually pick those three because they
had been used as baseline reference
algorithms in recent papers so you
thought wow we did the same thing with
DD PG implementations went looking
around three different GD PG
implementations none of them are own
that were released so it's not just one
type of approaches a little bit
difficult to figure out what to do we
went looking a little bit further in and
we started playing for example with some
hyper parameters of these methods and
what we observed that there is in fact a
very intricate interplay between the
hyper parameters that in and of itself
is not completely surprising anyone
who's done deep learning in the last few
years is well aware of this phenomenon
the part that may be a little bit
problematic is the level of motivation
of the experimenter to find the
particular combination that is winning
when dealing with the baseline the level
of motivation seems to always you know
lack a little bit when it comes to
finding the best hyper parameter for the
baselines in this case the type of a
hyper parameters that were changing are
for example the parameters of the policy
network the number of knows number of
layers and so on and so that's one that
people are reasonably used to optimizing
over we looked a little bit more
and looked at a hyper parameter which is
the reward scaling so you may not know
but when you do reinforcement learning
it's often the case that the magnitude
of the reward gets rescaled before
feeding into the algorithm it has become
standard practice and in some cases
there's very good reason to do so
there's papers that demonstrate that
when you do the scaling appropriately
you get much more stable learning again
not a bad thing to do it turns out that
there's a lot of interplay between how
you do your reward scaling and whether
you use layer wise normalization and so
once again if you're not sufficiently
motivated to find the right combination
you may come up with very very different
results or your baseline results it's
not just the hyper parameter
optimization that had us a little bit
concerned we got very interested in how
people measure performance for their
policies in most of the graphs I've
presented so far the way that we've
measured performance is based on the
return averaged over several trials and
we've included a standard deviation over
that over the set of returns but we
found a lot of papers who report the K
best returns and so presumably they have
run a lot of trials and then they found
a small number K being 3 or 5 and
produce the results for that and it's
not always clear how many trials were
run to pick the set of K so we looked at
some of the papers and in this case you
know they were producing results from
maybe the top 5 so that's the K best
trials they I don't know how many they
ran but they report results for the top
5 other of them report results for three
five top two top three and so in this
case I would suspect that perhaps we're
not getting a very full picture of the
performance of the algorithm under
different conditions and in this case
I've blotted off the author name because
quite honestly
I'm I'm not so interested in pointing
figures on specific work because quite
honestly this practice is very
widespread it's not a single group it's
not a single set of results that are
doing this it is so widespread that it's
become the standard way of producing the
results for this area of a research
which I personally find a little bit
concerning and so I think I'm trying to
see as a community whether we want to
continue in this direction or perhaps
realign our practices because we were
picking the top few trials it became
interesting to think about how are we
picking those Kapus ones and so the
students are in one more experiment and
this is a particular cute one they
produce results from two different we'll
call them two different methods the
orange method and the blue method in
this case they did five runs of each
right there has a lot of paper that do
five runs they didn't pick the top five
they just did five of one and five of
the other and they showed me this
results and we looked at them were
thinking yeah orange doing a lot better
than blue on this and then they revealed
that in this case both orange and blue
were five runs of the same T RPO code
with the same hyper parameter
configuration and so just presumably the
choice of this random seed in this case
was the dominating factor explaining the
difference in performance between the
two approaches so I think there's some
things that we can do in particular in
reinforcement learning but also more
broadly in machine learning to improve
our ability to have good results on
which we can build in which we can trust
I have made in this case mostly the
focus on reproducibility in some
discussions the question of whether
there was a difference between
reproducibility and replication came up
I would say I went digging a little bit
for definitions and replicability is
typically assumed to be a stronger
concept than reproducibility in this
case our study is deemed to be
replicable
if we can actually collect the data in
the same way we can perform the same
data analysis and then we get to the
same conclusions and so most of the work
which I presented today the notion of
collecting the data in the same way is
not necessarily problematic as long as
we have all of the information but as we
move away from simulation domain and
into domains where we're actually using
real world data and collecting real
world data then this notion of whether
we can actually replicate the collection
of the data itself becomes an
interesting factor in terms of seeing
what we know about the work as I was
thinking through this work I started
thinking quite actively about what were
the right metrics for reproducibility
what were the ways to measure whether we
were in fact on the right track with
respect to this so I came up with a set
in discussion with many other people
that seem to give me a good target in
terms of reproducibility so the first
few items are things that most people we
think about quite readily it is really
useful if we can have the exact data set
or the exact simulation environment in
which a work was produced that's very
useful information very useful if we can
have the right strain validate and test
partitions especially for work on
supervised learning sometimes this
information is missing from papers it's
very useful of course if we can have the
implementation some executable files
particularly the version number of the
dependencies can be very useful as well
as of course an account of all the hyper
parameters used I would add to that a
specification of the random seasons
particular in cases where a small number
of results have been produced in many
cases it is surprising how there may be
misalignment in cases where we do have
code there may be a misalignment between
the paper and the code and so ideally
from the code we should be able to go
all the way to generating the actual
figures that are in the paper should be
very good alignment about this the
clarity of the code the clarity of the
paper in terms of ease of understanding
language and so on is of course a factor
third set of fact I would say some
details on the computing infrastructure
used it doesn't mean everyone will have
access to that infrastructure but at
least knowing what is that
infrastructure would be helpful some
information about the computation
requirements in terms of time memory the
number the types of machines that were
used and finally some indication on the
reimplementation effort so in the case
where someone's trying to re-implement
the work figuring out can this be done
in a short amount of time do we need a
lot of time what's the expertise level
of the person doing the real
implementation that is necessary to
reproduce the work and finally in some
cases for people who engage in this work
of trying to reproduce previous work
right how many and how complex are your
interactions with the authors until you
are able to reproduce the results that
are actually in the paper and perhaps
this goes a bit beyond reproducibility
but can we actually see if the
conclusions generates to other datasets
beyond those that are strictly in the
paper so I think this gives us you know
maybe a something to shoot for in terms
of what we want to see in our field to
make sure that we are really adhering to
good principles because last weekend I
had my you know captive audience of CFR
learning and brain members I actually
extended the survey a little bit more
ask them a few more questions I asked
them like amongst this whole set of
different things right which ones do you
think are very important for
reproducibility which one are important
which one are not so important so they
was on a scale of 1 to 5 where 5 was the
most important and when's the least
importance and here are the things that
they thought were really important so 16
out of 20 said the account of all hyper
parameters is really important 4 out of
20 said yeah that's important so that's
like all 20 of them were in agreement on
hyper parameters 19 out of 20 thought it
was very important or important to have
the exact data sets in the simulation
environment 18 thought we needed clear
code and paper and 16 thought we needed
train validate and test partitioning
information so these came out as the
most salient features in terms of what
we need in terms of
what else do we need right and nine of
them thought it was very important and
six of them thought it was important
that we had the implementation or the
execution file so that's 15 out of 20
again 16 thought we should have some
alignment between the paper and the co
and 16 thought we should know what's the
reimplementation effort necessary and
then the other factors were deemed to be
less important according to them again
very very small sample size I'm not
really trying to make a statement on the
basis of just that though if the survey
appears at some point a broader
population I encourage you to you know
tell me what you think but some of these
other pieces people thought was less
important in terms of really doing
reproducibility a couple quick notes I'm
not alone in this and there's a lot of
people who have built a lot of
infrastructure to facilitate
reproducibility so I encourage you to
explore some of these infrastructures
for your own research adhere to them if
you can both in terms of as a resource
sharing the results of your research and
also building on the results of other
researchers results about the month and
a half ago I launched an effort called
the iclear 2018 reproducibility
challenge it is a call for the community
to participate in essentially a
crowdsourcing reproducibility effort
whereby people are encouraged to pick a
paper submit into iclear 2018 and try to
reproduce that paper I have tasked the
graduate students in my graduate course
to do so and there's 10 other
institutions across the world that are
doing the same so there's a good set of
students doing that right now and as a
result of this effort I have encouraged
them to post on the open review the
results of their reproducibility studies
with all the challenges and philosophy
may entail how many people here
submitted an IEEE clear paper this year
raise your hands how many of you have
made code available in an anonymous
fashion I see four hands in that whole
big room it is not too late the students
have until December 15 in my course and
but later in some of the other courses
and of course this is just one drop in
an ocean of effort towards doing that so
please try to remember in future efforts
what you can do to help that it's
probably an experiment that we will
repeat in future in the future as well
so just a couple last note right I think
as a community we can do better in terms
of experimental practice in terms of how
we report our results in terms of how we
share our results and so I'm encouraging
everyone to do so if you have some
availability or some of your students do
to get involved in the reproducibility
challenge maybe you want to do like a
48-hour hackathon in your lab on the
heels of nips related to this and I
think you know as a community we should
really commit to reporting accurately on
our methods to releasing code and to
reproducing results because it is so
important to the advancement of our
knowledge this work was made possible of
course by some fantastic collaborators
Peter Henderson Riyadh Islam on the RAL
reproducibility for the iclear
reproducibility I have ego lava shell
rosemary had 9k engine be freed who are
helping me out and many students in the
reasoning and learning lab have
contributed to this to this work thank
you very much
hi thank you for the talk my question is
about open source for its closed source
for different aspects like for example
the simulator I know that mojo code for
example while it's helpful could also be
expensive for site licenses for example
yeah so it's a simulator is available
but it's you have to pay for it I mean
what is the reasonable way to look at
that in your opinion if a simulator is
available and we have to pay with the
accessibility level of that if in some
cases the amount of computation you need
is very very expensive also there's a
challenge there I would say it's a it's
a challenge in which we are not alone if
you look at some other disciplines for
example in physics they have initiatives
that require very very expensive
equipment and one of the ways that they
have put in place to sort of ensure sort
of the sanity checking of their
experiment is to make sure that there
were several groups involved in
conducting and reviewing the experiment
and so in some cases this may be the
kind of work the kind of setup that we
have to explore as a community if we
want to move faster so I've seen a lot
of presentations here that did have a
github link in the slides but that kind
of moves us to the next problem which is
it's not always easy to reproduce the
results just from having the code and
sometimes the amount of time and effort
as an author that it takes to support
someone else using your code can be
quite high so I'm wondering if there's a
way we can sort of incentivize people to
dedicate time to doing that I think
that's definitely true both as an author
and as someone who wants to reproduce
results the cost can be quite high in
many ways that the the reproducibility
challenge and the way we've launched it
with some students right it aligns the
incentive the students are crying
incentivize to do good work because it's
a course project and don't need to be
prompted as much as someone else but I
would say that it still it still remains
a challenge there's I think because what
I've described has a sort of a wider set
of criteria for reproducibility that
includes the ease of reproducibility so
I think it's a factor
perhaps not definitely not the only one
but it's definitely a factor I think
code gives us something extra but that's
not all in many cases we need much more
than that I think I've had a lot of
suggestions since I started talking
about this of people who to help
incentivize things even further to
convince people to do it so I think we
we have some mechanism to explore to do
that thank you for your talk I think
this is a huge problem in our field I
don't have the computational resources
to reproduce many of the big results in
our field so I make up my own way to
scale it down everyone else does and
then we end up with results like you
presented what should we do about this
yeah so in many cases right it may not
be necessary to reproduce everything
well one of the I would say one of the
results that that's interesting is in
many cases even reproducing the baseline
can be really really useful to our
knowledge and that's maybe a really good
step for people who have less access to
resources to just sort of be the
inspector an experimental inspector that
the baselines were well done time
horizon work that's fundamentally non
run not reproducible disappears you know
you might get some citations for a year
or two but as far as advancing
criminology goes if if work is not
reproducible it it goes away have you
like this is one of those questions but
it's actually just me telling the saying
my ideas is it possible to turn that
into an incentive to motivate people to
create work that's more reproducible
yes to create work that's more
reproducible yeah I mean I think I agree
with you that in some cases right that
work just if it goes away and what we'd
like is that it goes away a little
faster right and so I think there's many
things that are in the air to see
whether we can do that whether you know
we have some tract and some conferences
that are dedicated to work that is
reproducible to publish there you both
have to propose a paper and verify one I
think some of these things are being
discussed they take quite a bit of a
change and they take quite a bit of
people infrastructure to put them in in
place so we'll see what happens
but people want to get involved I think
there's a lot we can do we lost our
question on that side so it's really a
question for you I wants to ask because
it seems to me echoing what someone said
earlier that there is actually a cost to
making things as reproducible as we can
I think first of all actually there's a
lot of goodwill and I think that
actually most researchers are trying
their hardest to make their research
reproducible and so I think we should
encourage those people to I think
actually offering good guidelines of how
to make it more reproducible as a great
idea and I think that's a really good
benefit of your talk but the cost is
when you have work which is when you're
asking for people to put code out there
which needs to be made public and usable
and friendly for anyone else to use that
can take a lot of effort and I worry
that will actually push people away from
publishing particularly people are in
industry who have kind of proprietary
code bases and things like this we might
actually have a negative impact on so
for example for alphago you asked about
alphago I think we we tried our hardest
to to make that work reproducible but if
we'd been forced to make the codes
shareable it probably would have taken
us an additional year I would estimate
and so we wouldn't have published and so
then the question is are we pushing
people away from publishing
and I think in reality the work was
reproduced in fact within two months of
the most recent paper there is a
reproduction a bit there's people who've
reproduced it online in various regards
so I think the my question for you is
like where's the trade-off how do you
get the trade-off right between pushing
people to do the right thing but not
pushing them so hard that they feel that
that actually there's a burden on every
paper now which is so great that that
it's actually unrealistic and I think
some of these requirements do start to
border on there may be going that that
way yeah I I agree that there you know
that there's a there's a there's a high
bar in terms of the requirement in terms
of what I'm proposing here I think in
some sense the recipe is to have many
different modes to diffuse knowledge
right and so there may be some venues in
which a very high standard is required
in terms of reproducibility and there
may be other venues where it's a
different standard in terms of what type
of work is accepted and the way it's
disseminated so I'm not arguing that
only one what this this standard needs
to be applied to all work all the time I
just think that work that meets that
standard we have a higher level of
confidence in terms of the knowledge I
think we need to wrap up here for this
session let's thank you all again and
we'll restart at ten past four</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>