<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Jurgen Schmidhuber - Singularity &amp; The New AI | Coder Coacher - Coaching Coders</title><meta content="Prof. Jurgen Schmidhuber - Singularity &amp; The New AI - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Jurgen Schmidhuber - Singularity &amp; The New AI</b></h2><h5 class="post__date">2017-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IJVQaqIuVNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so next let me introduce our keynote
speaker
jurgen schmidhuber he's uh he's the
co-director of the Swiss Institute for
artificial intelligence which is the NCI
DSi and the professor of cognitive
robotics at the Technical University of
Munich and a professor of AI at the
University of Lugano Switzerland he got
his doctoral degree from Technical
University of Munich in 1991 is
habilitation degree in 1993 I admit I
don't know what that is but I guess it's
something impressive second PhD well one
year in the u.s. he has helped transform
HCI into one of the world's top 10 AI
labs it's the smallest of the top ten AI
loves this was ranking according to
Businessweek magazine and a member of
the European Academy of Arts and
Sciences and he's published a couple
hundred papers on pretty much every
topic Under the Sun and that the rate
the rate the reason I invited Jurgen
give me the keynote speakers I really
felt he he combined three things in his
work which is quite rare to be combined
in one guy are really a broad grand
vision of AGI and all its stuff future
prospects and potential grandeur a lot
of rigor in the mathematical
formalization and kind of formal
understanding of what a GI intelligence
intelligence are about and really saw a
practical useful work in the a AGI and
AI algorithm development so being able
to combine the grand vision the
mathematical theory and practical
algorithm development so well in such a
long time and so many papers I think
that that's quite rare there's not many
people you can say that though so I was
very happy he came and agreed to speak
for us
thank you very much Ben and organizers
for this invitation which I consider a
great honor I was talking about three
prisoners that were sentenced to death
one of them French one of them German
one of them British what is your last
wish they asked the French guy he says
one final exquisite French meal with
exquisite French wine what is your last
wish they asked the German guy he said I
want to give a speech what is your last
wish they asked the British guy he says
I want to get shot before the Germans
starts his speech unfortunately for you
guys it's too late now already in the
middle of my little speech they'll
actually been already told you
everything about what AGI and the new
way I could be so I only have to add a
few aspects of the new AI is that I
personally consider important this is
the title of my talk this is my name
this is to help such a man and I have
two affiliations two affiliations one is
suic IDs I a was he it SIA and Lugano
and that we are currently trying to hire
a bunch of postdocs and PhD students for
robot learning machine learning
universal artificial intelligence theory
and things like sounds and the second
affiliation is with the team in combat
lab where we have all kinds of
interesting robots roughly - and what's
the first humanoid with sophisticated
vision such that could walk across an
obstacle so years ago today the Japanese
are leading in that field that is a
robot which is the success of the first
robot that was able to tie knots and
this is the one that we are using for
our students to train students and why
do the slides look the way they do why
because they were all designed according
to recursive applications of the
harmonic proportions and and the reason
why why they are designed this way is
because this reflects something that
will be a topic later in the talk namely
compressibility whenever you have a
simple regular scheme like that then
there is an aspect of what you're seeing
that is highly compressible and
compression has a lot to do with optimal
inductive inference and optimal decision
making and Occam's razor and things like
that and so in that sense the slides
reflect a little bit what
going to talk about now there are many
ways of putting Labor's like old AI and
new AI and stuff like that one when all
kinds of research one of them is this
you can say the old AI is logic
programming and expert systems then the
narrow AI or the standard AI whatever
some people might call it the one which
is now dominating most of the activity
in this field is about embedded agents
that are somehow embedded in a bigger
world they are interacting with robots
for example or internet agents etc and
the way to deal with them is and to make
them learn something about the
environment is based on probability
theory so much of of current AI is
actually just a variant or extension or
epsilon additions to traditional
probability theory in Bayesian analysis
and etc and you have a very successful
reactive learning mechanisms sub
symbolic learning mechanisms I think I
have a pointer somewhere thank you
reactive subsidized learning like for
example support vector machines or
feed-forward neural networks that take
an input pattern and map it to some
output pattern some classifications and
make assumptions such as all these
patterns are statistically independent
and they are used to drive cars like
this one that is a self-driving car an
embedded agent living in the real world
and companies like Google are indeed
making a lot of money with this type of
AI and one way of putting a label on the
new era would be that we are also now
going to be very interested in
nonreactive mappings from sequences two
sequences not just J the stationery and
the parents to output patterns in
methods that learn entire algorithms and
programs and behaviors where you have to
memorize previous events in order to do
the right thing at a later point in time
there's something that the support
vector machines and all these really
successful devices
of recent machine learning cannot do and
well I think it's most important in this
context is that we now for the first
time have an AGI which is math Maggie's
sound there is a mathematically optimal
way of decision making in unknown
environments which can be proven to be
optimal in certain senses and in that
sense we see now we are now in the
middle of seeing AGI becoming a former
science as opposed to a bunch of guy
guys playing with robots now this is one
of the heroes of traditional AI or
standard AI that is an stick months and
Sigma's is the progeny of autonomous
driving in the 1980s he built machines
like that and his team of course there's
a Mercedes van and it has cameras on
board and back then when computers were
a million times slower than today he was
able to drive already on empty streets
in Bavaria and in the end you had huge
computers and and then as the computing
power and the algorithms improved he put
it into a smaller machines like the
Mercedes Benz and then in 1995 that was
15 years ago he already had a car that
was able to go from Munich to Denmark
and back on the Autobahn
in traffic in traffic tracking up to 12
other cars simultaneously using
Activision to do that using common
filters and statistical approaches to
deal with uncertainty in the
measurements etc passing other cars
automatically identifying other cars
passing them with up to 180 km/h so that
is the my hero of autonomous cars and
and then later you also saw other
self-driving cars but you know the the
Doppler grandson
maybe some of you have heard about that
then you didn't have traffic and the
cars were actually driving by GPS so you
had navigation things on board to do a
singing curve you had to follow forward
GPS points and in between there was just
a little bit of revision assignment to
do not lose the road and more challenge
more interesting worse than the
challenge of 2007 where you had to
observe traffic signs and all kinds of
more complex pattern recognition
problems had to be solved and the same
year just 15 minutes from our Institute
we had the European land rover Toronto's
and here we see pictures of cars and
rough terrain showing off their self
their autonomous driving capability is
there well here's another example of a
tradition and a on a standard AI of the
embedded kind with learning machines but
not in the general sense that that I
mentioned few slides over here we see
Alexander he is a postdoc in my lab at
the NC a-- and his main claim to fame is
that he was the leader of the team that
won the RoboCup in the fast league in
2004 and the fast league is probably the
most interesting of these robot leagues
because these guys are so fast that you
don't is that a human expert with the
joystick cannot win against the best
teams cannot win against the best teams
and each of these robots here has four
little wheels and they have a little
learning mechanism on board that makes
them predict what is going to happen if
I send this moment with these four
particular motors each of them is
slightly difference or first there's a
learning phase where the robots learn
what's going to happen and then you can
use these predictions to plan ahead
tick-tick-tick and and in that plan
there's still the option you can modify
the actual sequence that you're going to
execute
in the near future such that a certain
goal is reached without bumping the ball
away and stuff like that but the global
behavior is scheduled and pre-wired and
pre-programmed so that would be an
example of traditional AI well something
like a feed-forward network of support
vector machine why didn't work of model
was something like that is used to train
a little part of the machine such that
it's part of a bigger thing where then
the whole involve is a whole system so
some interesting tasks the future will
be more at least the future of machine
learning will be going towards learning
programs algorithms and machines that
have an internal state that can memorize
previous events in in some sort of
internal memory and can learn which of
these many events that are coming in and
the input stream are important that
which can be ignored
etc and in the end then in the next
slides to show you a few examples of
what can be done today and then finally
I will move on to these universal
methods
you
how many people in this room know what
is a recurrent neural network
okay how many people in this room do not
know what is their week on your network
and there's a quantum superposition week
on your network is a little bit like the
thing that we carry around in our brains
you have all these nodes and they are
nonlinear and what they do is they sum
up the activation of connected friends
where you have a program that is
actually embedded in these synapses in
the veins which are we'll valued numbers
essentially and they define the behavior
of this recurrent system which receives
a stream of inputs and while new inputs
are arriving all inputs are still
somehow circling around inside according
to the rules determined by this parallel
sequential program embodied as a white
matrix and these recurrent networks are
as powerful as any traditional computer
you can implement on such a recurrent
Network any algorithm that you can
implement on this computer here but
unlike with traditional computers what
you can do is in supervised learning you
can you can train the network to become
a better computer because you can
evaluate the difference between what the
network should do and what it really
does and you can differentiate your wish
which is that this thing should be
really doing what I want to do you can
differentiate this wish with respect to
the program running on the on the
computer on the weekend network so you
can generate a gradient in a little
space in the space of possible programs
parallel sequential programs that are
running on theism become networks
see learning algorithms that we are
using today for these methods are not
Universal but already pretty general and
especially useful in quite a few
applications especially in the so-called
long short-term memory
networks Palestina networks you see a
picture of supporter who was the first
author of the paperbacks and in 1997 and
the main advantage of these LSD and
networks I'm not going to go into
details that say overcome problems
associated with the previous with the
older become networks which weren't able
to deal with long time lags between
events in a way that was clearly
superior to stand a feed-forward
networks
so with these Elysee and that suddenly
you could do all kinds of interesting
sequence processing tasks where you had
to memorize things for thousands of time
steps and today they are back then in
the 90s these was just say what just toy
applications and applications but now we
have all kinds of state-of-the-art
applications for example in protein
prediction and certain aspects of speech
recognition where the main competitor is
usually the hidden Markov models how
many people know hidden Markov models
okay
my main competitor usually and
handwriting recognition connected
handwriting recognition which is much
harder than an isolated character
recognition for example here we see a
picture of Alex Grace who used to be a
student at the in Syria and now he's a
postdoc in Munich and the support the
problem is to translate this handwriting
here into letter so that would be a
teeth all ve
UIL ing eto and so on
now there is no teacher who knows which
what is this scribble here or is that he
part of the n-no is that part of the eye
or whatever all the teacher now is on
the training set is a sequence of labels
that would be the target label sequence
that you want to output given the real
valued input that you get as you are
feeding says connected handwriting here
into a recurrent neural network from
here to here that's about 600 time steps
and then actually they are not is not
only one week on networks that is trying
to predict the next letter given the
context the left context that it's all
so far going from here to here for
example but there's another one which
goes the other way round which goes from
the future into the past such that at
each point in my connected handwriting I
get two different outputs one from the
forward network one from the back what
we can reverse Network and you add them
all up and then you get again a gradient
in this space of algorithms defined by
these two network architectures and you
just improve your your program again by
computing the gradient existing year
which is the probability of the output
sequence in the training set of the
output labels and the training set
giving the input sequence and then if
you and it turns out that this works
amazingly well much better than with any
other approach that is no one at the
moment
so for connected handwriting this is
currently state-of-the-art and connected
handwriting is much harder than isolated
digit recognition with isolated digit
recognition support vector machines and
traditional machine learning methods
Lucian neural networks they achieve
performance which is identical to the
one of the best humans now down there
you see a little movie which which
corresponds to a sensitivity analysis of
a networks that has already learned the
behavior the correct translation here
we've got six and time steps and
whenever you see something spiking up it
tells you how important is this event
here while something back here in the
field in the past or here to make the
correct decision at this point where the
network is making decisions so where you
receive the most concentration of Hulu
there is the decision point and here is
where the letters are being generated
often you see that correct decisions
require information that is hundreds of
time steps in the future or in the past
or here key words popped in this is a
picture all just reminds me last time I
was in America this was a couple of
years ago and everything was slightly
different back then
so some things were totally different
for example there was a totally
different president it's the older one
still remember
and back then everybody was afraid of
al-qaeda and one of the ideas was
listening to a billion phone calls and
and check for suspect keywords and this
is what Sankey did here with a large
corpus and again the main competitors
are always hmm in that field and he was
able to to achieve much better
performance we see this double recurrent
networks on keywords watching and see
other approaches now of course this does
not only have military applications but
also commercial applications such as
industrial espionage but today of course
most people are less afraid of al-qaeda
and they are more afraid of their
investment advisers the other day my
investment advisor Tony said I've got
good news and bad news the good news is
the money that you invested it is still
there the bad news is it doesn't belong
to you anymore
now suppose your investment advisor is
contracted with a stock market curve
that looks like that or like fast these
are three hundred time steps it doesn't
really matter it's in milliseconds as a
unit interval or bays and the idea is
you want to predict the future of this
time series and I low and sell high in
the future now I don't I personally
don't know any investment advisor who is
able to predict the future of this time
series but recurrent networks have got
something to offer for everybody and if
we feed our recurrent networks which the
beginning with beginnings of time series
like I said then it's always was a good
eyes they will maybe see that it's a
little black dot which means which
represents the predictions of the
network
they're almost always right on target
sometimes you
little deviations that but actually
there's a variety in this little piece
of data and a few hundred types of the
day of data that can be used to
extrapolate to the future in in the
correct way down here it's always it's
also the case so another interesting
application potentially lucrative
application of beacon neural networks
robotics like this thing here which is
trying to tie knots you have actuators
and you're working a very confined era
in this artificial animal down here
where you have a narrow cavity and then
a surgeon is showing how to tie knots in
this cavity and it's hard also for the
surgeon it takes a couple of minutes and
you do that a couple of times to show
the network what's going on and then you
have a typical state ambiguous and
disambiguation problem because again and
again you get inputs that that are
identical or though they represent a
different part of the trajectory of the
knot-tying trajectory and different
decisions have to be made although the
input is the same so you have to
memorize which kind of input didn't have
before in order to do the disambiguation
and again sees we cannot rise i can do
that i can learn that or more recently
you can even build we can network status
that take the two-dimensional buyers of
images in form of a pre wiring and can
can excellently recognize characters
what perform traditional methods on
certain types of character recognition
and especially promising for movies and
object recognition in movies but if
there's no teacher if there's no teacher
that tells the system on the training
set what should be that these
this is just a reinforcement learning
task where you try to - to solve a
problem like this one this is a picture
of how steno Gomez who got a best paper
award for this here we've got a rocket
which has fins and it's and it's kind of
heavy because the fins say they they
weigh something so if you remove the
fins then the rocket is lighter and can
get higher but it's harder to control
the rocket then and without fins the
rocket does something like this it goes
up and up and at some point the boosters
are miss controlled and the whole thing
explodes but you can devise an
evolutionary learning algorithm whose
only feedback signal is the height of
the Rockets that is being achieved and
if it's better than the best so far then
you get a positive reinforcement for
this network which can be air we can
network down here which is controlling
the Foresters there's no teacher who
knows how to do that so you the only
feedback is at the end of the trajectory
and the question is does it get higher
than the highest so far now if you have
many parameters in your network then the
problem is that two additional
evolutionary algorithms do not work
because they don't really work with
these long genomes but what you can do
is you can have a lot of separate
evolutionary processes running on all
these little neurons here such that each
of them has a population of sub
population just limited to its own
weight vector which greatly limits its
search space and they will get the same
feedback signal and it turns out this
works surprisingly well and outperforms
all other kinds of competing algorithms
oh he has an another application where
you have a recurrent Network that is
steering a robot that has three wheels
the wheels look more than
and we're physically simulation you have
previously friction etc you have three
of these you can networks that are
controlling the wheels and there's a
there are sensors that tell them tell
the robot what's going on is there an
obstacle nearby etc and there's a pole
on top of the on top of the robot and
the goal of the robot is to balance the
pole and after maybe three thousand
iterations it really learns first of all
to bounce the pole but then also not to
run against the wall and the only
feedback is based on trial lengths so
it's the controller better than the best
so far
yes and you have a higher probability of
surviving if you are part of the sub
population of these weekend neurons yeah
so that can be learned now if we take
the same robot and we add a second pole
and stand it on and put it on top of the
first pole after four thousand
iterations what we get is an almost
perfect behavior but now you see in a
moment that is really to polish standing
on top of each other because it's this
one yeah crashes but after five thousand
six thousand iterations it really knows
how to do it and again no teacher
telling the robot what to do and it
turns out that if you put this idea a
little bit further and then you call
evolve all the synapses in the network
then this leads to one on the cerebellum
like control problems leads to to the
best performance on challenging tasks it
turns out that the traditional
reinforcement learning algorithms are no
real competitors anymore and the best
competitors are usually always some sort
of evolutionary method in particular
CM a covariance matrix adaptation
evolution strategy is that is one of the
gold standards and
or maybe Z gold standard in that field
but cosign outperforms it and at least
in this particular application
does anybody know has anybody ever heard
about covariance matrix adaptation okay
ask your neighbor and he'll explain well
here's another application of an object
moving through water and a little little
actuators on top of the object and the
goal is to make them learn to move such
that they create counter turbulence such
that the turbulence that is generated by
the moving object is somehow killed and
there's this less drag and less
resistance and if you have then a
realistic simulation of what's going on
looks like that add a little bit but
then after and if you if you look at the
vortices before learning they look like
that but after planning they become much
smaller you see that and the drag goes
down to about 40 percent of the original
value a couple of years okay again we
did that and west eight of the artifacts
and oh we are training these guys as
these these humanoids to walk not only
on rough dirt on flat terrain like that
but also on rough terrain
and here there's a new type of research
that that is relevant here is closer to
the theory of computation and
algorithmic information sent to the
traditional statistics field and here
are some of the heroes of computation
girdled who started the field of
theoretical computer science essentially
in 1931 we talk about the limits of
computation and and probes during a few
years later we formulated these results
with charge and this guy is interesting
in 1933 he was the one who wrote down
for the first time the axioms of
probability theory and was so displeased
with with these with the basic
ingredients of probability theory that
30 years later he came up with this
alternative definition of complexity and
of information which is today known as
the Kolmogorov complexity the length of
the shortest program that computes
something that is the Kolmogorov
complexity of the something here I have
written the information that is
contained in this object and and in many
ways these notions are closer to what's
going on in these recurrent networks
then what we can learn from traditional
statistics currently we are writing Z
and Boxey we can attract before and now
let me switch to the universal stuff
because yes these we can networks they
are general in the sense that you can't
really run arbitrary rules on them but
the learning methods that we have seen
so far limited because they are greedy
for example gradient descent is actually
a greedy method evolution methods are
greedy methods in many ways and it turns
out that there are best possible ways of
learning programs for
for for computers that is generalities
and to introduce doors let me first
start with my little IQ test are you
still awake
so I'm going to show you a pattern a
series of numbers and and there's a
pattern hidden in these numbers and and
you were supposed to think about a
little bit and and then tell me what is
the next number so here is the pattern
what's the fifth number zero zero zero
me a peon to ten
the answer is 24 because the answer
makes this and number space but in if
you say 34 in an IQ test it could happen
to you that you won't get the full score
because what the IQ tests want from you
is a simple solution they want to want
you to say the solution is 2n as opposed
to this and the reason for that is this
man down here Malcolm said simple
solutions are better simple solutions
are there if you have 10 different
explanations of the data take the
simplest one in Canoga of polarized that
and he said as I already mentioned he
said well simplicity can be measured by
the size of the shortest problem that
computes the things that you want to
describe in that case the polynomial so
to end somehow needs a shorter program
to describes and the land and and
solomonov this man over here there's a
picture I took at each year and 2001 and
he's still a Barbie kicking he was the
one who came up with a fundamental way
as an optimal way of predicting the
future of time series like the ones that
we serve just saw from the previous
observation observation and and because
that's such an important concept how
many people already know how that works
ok so maybe 10 percent and the others
are going to learn it right now it's
such an important concept everybody
every scientists not only every machine
learning person every scientist I think
should know this suppose you want to
predict the next in the next observation
X n plus 1 after you already saw X 1 X 2
X 3 xn etc the next observation will
come with a certain probability
me what's the English name are saying no
okay no so the probability is mules at
xn plus 1 will happen adopt a certain
value given all the previous
observations over know you could date
the next element from the previous ones
not with the true prior which you don't
know with the true probability
distribution you don't know is that one
instead what you say is you generate a
new a priority distribution which is
this mixed prior here which essentially
is all of the computable distributions
taken together each of them gets a
little indexed so mu L is the elf
computable distribution see the elf
computable a distribution that assigns a
value xn plus 1 given the observation X
1 X 2 and so on an exam and each of them
gets a little weight and ideally is this
little weight here is the Kaluga
complexity to the program that computes
a probabilistic distribution in the
exponent in the negative exponent of 2
so you have your faces away there's
something that is you know 1/4 1/8
smaller number and the sum of these
little weights cannot be more than 1 and
so by summing all of these distributions
you get a new distribution which in some
sense encompasses all the other
computable distributions now you predict
with this prior instead of the true
prior which you don't know which is this
one which you don't know and it turns
out this is the best thing you can do in
a rather in a very general sense
so there are theorems that shows that
this is in a certain sense the best
thing you can do and this guy guy yeah
he 16 now 40 years later he put it
together
and used that distribution to build an
optimal Universal decision maker and an
optimal universal AI is Marco's Jota
when he at that time when you were still
postdoc at the its eeeh but then he
became a professor and kambera where he
is still working and in this little
format I just explained what it means
it just says takes the universe's
distribution of the solomonov and and
extended a little bit such that actions
are taking into account so you always
have an action and then an input comes
and you do another action and another
input comes etc and then you try to
maximize you try to select for the
future see action that is the first
action in the sequence of actions that
maximizes fill the mixture predicted
reward in the future so you want to
maximize the expected reward that you
will obtain in the future as a total
cumulative future reward that is your
goal in life you want to achieve that
and what you should do is and there's a
proof that this really is the best way
to do it you you should execute the
action that begins the sequence of
actions that will lead to a maximum
model mix predicted reward although the
mixture there is no universal mixture of
the sonam enough is very different in
general from the true prior which you
don't know from the true way the world
works which is one of the many things
that are there and the big mix so
I would say it's a new AI result you
have a very simple equation and it is
not a huge software package that
describes AI a huge AI system or
something it's just a simple equation
and there's a proof that says this is
the best thing you can do
so that's part one of these results that
show us that AI generally is on its way
of becoming a former science now you may
say it's not computable this thing yeah
because there are so many computable
distributions and and there are in fact
infinitely many and so how do you really
parent but their ways are firm you know
relaxing a little bit or actually making
additional assumptions which are not too
restrictive for example suppose that
it's true but cannot Susan already
predicted in the 1940s namely that the
entire universe that we are living in is
computed on a cellular automaton or some
other sort of computer is a computable
universe there's no physical Evette
evidence against this possible to you if
there is truth and it would be nice to
know that program and of course all the
physicists that's what they want to do
if they want to find the program that
computes the universe if there isn't now
at the moment we don't know this program
but if we if we say if he makes the
assumption just to become to get a
computable way of dealing with this
problem of universal AI if you make the
assumption that whoever's running
universes and whoever is running this
particular universe also has some sort
of resource constraints and wants to
find fast ways of computing universes
and really likes that fast
it's a quickly computable universe is
much better than the others if you
assume that then there is an
he was quickly led to the optimal way of
computing all kinds of things which goes
back to leaven there's an asymptotic
optimal way of computing things
including universes and from that you
can derive the so-called speed Pryor
which essentially assigns unlike
Solano's Pryor which assigns high
probability to things that have a short
description this one assigns high
probability to things that not only have
a short description but also can be
quickly computed and then if you take
this one as a prior it is restricted
enough such that you become suddenly
that you get a computable strategy of
acting in in the world such that your
future reward is maximized another
result which really goes far beyond the
in computable things
Marko's time he came up a while ago with
the a some totally fastest algorithm for
Holly Bell defined problems do you
already know this algorithm how many
people okay so many people in this room
do not know the argument but it's a it's
a very nice and very simple one so any
problems any problem that you can think
of as long as you can formalize it and
write it down formally such that you
have a domain and a domain of problems
and then a solution space for example
you want to solve Traveling Salesman
problems and are many traveling salesman
problems now you get a traveling
salesman problem as an input and there's
a function f that computes the solution
to the travelling salesman problem in
the domain wine then when you do this
super arrhythmia it searches
systemically for proofs until it finds a
problem Q that computer is that provably
computes the function that you want to
solve for all elements in the domain for
all traveling salesman problems in the
domain within a certain time bound which
depends on q and on the instance which
is death and then there's a clever way
of allocating runtime such that you
spend most time on the on on the program
with the best currently proven time
bound you look at this algorithm and
then you can prove this as fast test
algorithm here which does not know the
solution doesn't know anything about
traveling salesman problem will it
compute your charming salesman problem
solution as quickly as the fastest
dropping salesman solve what is over
which you don't know
you don't know it say for a small
constant one point zero zero one can be
a heart rate arbitrarily close to one so
you almost don't lose anything here plus
some constant which depends on F but not
on that problem which depends on the
problem class but not on the problem
instance so
that's see enough computer science
except for the one little drawback then
the proofs archaea corresponds to this
constant down here and it can take a
long time although it's constant the
constant may be so large that it doesn't
really fit in this unit order n to the 3
plus 10 to the 10,000 distilled water
into the 3 n to the third and in the
annotation of c-value computer science
it falls out out of the game picture but
in the real world it doesn't so this
shows that asymptotic optimality can be
dismissed leading this guy over here
maybe he's a some totally optional but
it won't buy him anything
what can be done how much more time do I
have
all right what can be done what one
thing that can be done to overcome the
problems with the good machine with the
red to become the problems with the
constants is the girdle machine and to
construct the green machine we have to
go even further back in time not only to
the sixties when Solomonov in coma cough
did their work but to the thirties when
this man here gorillas found it
essentially if you read in computer
science by showing which are the limits
of computation of math in general what
he did he created a self-referential
formula that speaks about itself and it
claims to be unprovable by an eyeless
make for your improvement that just a
miracle ii generates theorems so the
formula says i am not provable by an
algorithmic theorem for either this
formula is true but it's not provable
then well math itself is as an argument
a flaw in it all right
easily now the girl machine is just how
many people are in love with a girl
machinist then I skip one of us oh I
can't do one we skip it so the girl
machine is just a computer which has a
software on it
an agent controlling program that also
self-referential so we steal that
trigger good and it speaks about itself
and it's ready to rewrite itself in
arbitrary manner as soon as it has found
through an envelope theorem prover proof
that the rewrite is useful according to
utility function that is given by the
user which can be any utility function
not only asymptotic optimality
can be embodied by this utility function
but anything
so again you plug in in the initial
software which I call P you plug in any
utility function as an axiom stored in
this initial program the program
interacts with the world and while it
does that what's the ISM robot like this
one here it also may express a queue and
proof recusal program and proof is a
proof that the queue does something
until it finds a probe of the rewrite
Parton is a software of P through this
program queue that it just found here
implies higher utility then leaving the
initial software as it is now with that
approach see good machine will optimally
speed up itself including the proof
searcher so it's not like with the ACE
entirely optimal general problem solver
which has a hardwired proof searcher
that cannot be changed that cannot be
improved
something instead the proof search
itself can be rewritten replaced by
something else possibly and any of these
self rewrite is necessarily optimal
globally optimal that cannot be any note
local Maxima since the proof itself if
you think about it that's the whole
point of the self referential business
necessarily must have demonstrated the
uselessness of continuing the proof
search for something else but something
even better rewrite
so we are almost there but one important
question I think fabulous some things
before the first earthly right of such a
greater machines let's let's focus on
this question if only see instructions
per second say 10 trillion why 10
trillion because your brain maybe does
something like ten trillion instructions
per second what is the best way of using
these 10 trillion instructions per
second together as close as possible to
the theoretical limits identified and in
the previous part now I don't know how
that is possible but it seems to me that
one one thing that is not entering the
hasn't entered the discussion so far is
that we should further by us our system
towards regularities Saoirse is still
very general but it should be biased
towards regularities in the following
way it should be able the agent the
learning agent should be able to design
its own experiments its own tasks to
figure out which are the interesting
parts of the world where interestingness
essentially corresponds to the things
that are noble but not yet known learn
about regularities in the in the world
which its own limited learning machine
can learn quickly so it should be able
to have this additional freedom of
generating its own class to figure out
which parts of the world contains some
regularity that I don't know yet
and this is something that I'm going to
learn and the hope of course the only
reason why this might make sense is that
the external rewards like filling the
battery by plugging into the wall or
something
that also have a lot of in common with
all these regularities that you find on
the wand it's not if it is true that I
cannot get a lot of reward by doing
something random like standing at a
certain position it's time to pray a
certain random prayer and then the
reward is thrusted upon me if that is
not the case if all the reward sources
in the world really have a lot to do
with the regularities then this is one
way of greatly reducing the search space
by biasing the system towards the
regular positive was and this is
actually a different type of talk a
different talk that I like to give which
well a very simple principle helps me to
explain essential aspects of novelty and
interest in it and also beauty and
discovery in science and curiosity and
art and creativity and it's very simple
I just want to spend one slide on that
especially know when I'm when I'm
talking about first of all the
intelligent agent that wants to use
these limited instructions it's 10
trillion instructions per second for
example in a wise way what should I do
what if it can do this then it should
certainly store its entire life because
the entire life is all the data that you
get from the environment that's the only
thing that it will ever really know
about the environment there's the anchor
of everything that can be inferred now
it turns out that a human brain
apparently is able I did a rough
calculation of storing 100 years of
lifetime at the rate of about 10 to the
5 bits per second so it would be like a
low resolution MPEG movie and technical
systems will soon exceed that volume so
if if that is possible that's just a
hardware problem it's that as possible
then don't throw the data away so that's
a very simple things that you should do
now you shouldn't just store the data
you should try to explain this by
compressing it if there's a real
attitude repetitive regularity in the
data compress it and here my little
definition of beauty of some data point
X at a given time T in the system
lifetime it's essentially the bits that
you need to coat the data X at time T
for example suppose you you will run
around with a prototype face that you
use to encode new faces a new face comes
along which looks very much like the
prototype face that you have in mind
then you just you can encode a new face
by just encoding a few deviations from
the prototype face without without
wasting a lot of storage space now
the other thing is if the face is very
regular then but within the limits of
what is natural then you again need just
a few bits to store this new face and so
in that sense you can argue it's it's a
beautiful face but this is not the point
what you have to do all the time is not
only compress it but continually try to
improve the compressor for example if
your compressors are weak on you and
that's what like that which tries to
predict the next thing from the previous
things the better you can predict the
better you can compress because
everything you can predict you don't
need to store separately so if there's a
week on your networks and it will learn
to become a better processor processor
and predictor over time it has during
sleep all this data available where it
can work and try to compress Bella and
and then what is that the
interestingness of some data well it's
the first derivative of the beauty it's
the first ever derivative of the number
of bits that you need to encode the data
as you need less bits to encode the data
as your explanation of the data improves
see the interestingness of the data is
proportional to the one - the change to
the derivative of the number of bits
that you need and this is your internal
reward that is your happiness that is
your wall effect that you get
as you are explaining something that
seemed unexplainable before but suddenly
you see there's regularity and in that
moment you get the reward and that is
what you need as an additional reward
for the controller for the action
selector the curiosity based action
selector that tries to find data in the
world that is compressible in a new way
that wasn't already known okay that's it
so a discovery is just a large
improvement in compression performance
for example if you are a physicist and
using you watched apples falling down
and some of you see hey if they all is
falling in the same way there's a law
behind that then with that simple law
you can greatly compress your inputs but
if you didn't know that long before and
then suddenly you have a huge
compression improvement or in the arts
in the arts when you have a guy who for
example is drawing a picture of Obama
let's say but just five lines are
necessary and everybody recognized this
is Obama then it means that he has
extracted the essence with a in a very
compact way and the first time you see
that you think that's interesting so
that's why you look sad there's a new
variety that in that sense you didn't
know before and that's in the scientists
and the artists are both doing the same
thing they try to create or find data
that is compressible in a new way that
wasn't only for what one thing one one
take-home message if there's any one
important ingredient of the new
artificial general intelligence in my
point of view is that for the first time
you are you don't have this heuristics
dominated approach anymore but now you
try to create a form of science which is
based on optimality theorems and they
are based on a combination of property
theory and see read the computer science
and juris takes a come and go but
theorems are for it
so that is maybe the main reason why we
are why former Sciences are preferable
and it's it's a long time and can we
also say something about the near future
well this would be the third part of my
little presentation which Ben asked me
to provide and this would be the fun
part but we can skip that a few minutes
so there's a picture Vernor Vinge
in the 80s when I was a young man seems
unbelievable today but yeah I read these
novels of him about the technical
technological singularity so the idea
back then of Vernor Vinge was
accelerating progress gets fast and
faster and at some point becomes
incompressible and you have those things
that he called the technological
singularity and first he published that
in scientific a science-fiction novels
and I read so many science fiction
novels back then and these were one of
the and most of them were really bad but
this was really interesting stuff I
thought and then later I read in a book
like great what smile that actually is
the first guy who came up with this idea
of a converging history that singularity
probably was Danny's la una and and this
guy here a very binge though is probably
a really the first one who elaborated on
that concept in many many ways not only
in science fiction novels but also in
scientific publications now I ask myself
is it really true is there such a
pattern in history that's fits the theme
that fits that idea of a singularity
historic singularity and yes there is
one and it's amazing how is how simple
it is it turns out if you take as a
basic interval a human lifetime which
takes one which takes 80 years then the
ends main chapter of history in the
history books you can just open a
history book
is to 2zn human lifetimes before oh my
god
it's a Singler I think which is for some
reason in 2014 and let me show that to
you
so only guys 2014 many people I'm coming
up with a number like sounds you know
2040 so I take that as well I say one
lifetime is 80 years and it's true that
always there have been lifetimes like
that and the average lifetime of
previous millennia was shorter mostly
because of the high infant mortality so
eighty years all it was a good lifetime
and my arrow bars are not in most cases
they are really much smaller than 10%
now to to the nine lifetimes ago forty
thousand years ago Homo sapiens sapiens
comes into existence and and colonizes
the world and we take half of that and
we hit a neck exactly as far as we know
the invention of long distance weapons
like bow and arrow and the hunting
revolution and we take half of that and
we hit exactly the beginnings of
civilization the invention of
agriculture the first permanent
settlements and and the first walls etc
and we take half of that and it's
exactly the first five civilizations in
Sumeria and Egypt and the most important
invention of recorded history the one
that made recorded history possible
named a new writing 5,000 years ago and
it's not quite clear in Sumeria or in
Egypt and we take half of that
and it's exactly it's a feat of the
ancient world under the grace back then
there was there was this huge Empire the
Persian Empire which in many ways was
much more remarkable than the Roman
Empire but it's not as much covered in
Western history books but it had
probably have almost half of humanity
and in there and probably clearly more
than than half of the world economy and
it existed 2005 and years ago and after
furniture's of the empire exciting
things happened especially in Greece in
Greece that during that time all the
basic
values were created democracy was
invented philosophy as we know it the
first former proofs appear perfect
anatomically perfect sculptures were
made for the first time engines were
created the steam engine is a Greek
invention but was not commercialized
back then harmonic music was invented
the basis of all Western music organized
sports the the Olympics and things like
that and at the same time the Old
Testament was written the basis our see
three Abrahamic religions and at the
same time and the other part of the
world in India
Buddhism was created and Greece was not
the only technically technologically
advanced part of the wild in China the
first calculation tools emerged India
invented the zero so that was an amazing
time back then and we take half of that
and it's exactly the Golden Age of the
of the Islamic world the Islamic Golden
Age and at the same time it's the most
important invention of the past two
thousand years took place namely the
invention of book print in China Omega
minus one thousand three hundred years
and we take half a cent and is exactly
it's exactly the beginning of the of the
New Age well at the at the end of this
almost right now right at this point
here the Mongolian Empire was the
largest empire ever and contained most
of the known landmass and the two major
civilizations of that time were within
its borders and and the Chinese invented
guns and gunpowder and pockets and did
the first explorations so saying hey he
explored from China the east coast of
Africa and Australia and slightly later
fifty years later maybe that Europeans
went the other way around and discovered
the rest of the world or if they said
they discovered the rest of the one you
know like this picture here
this is the 14:20 thence the ship
satsang he used to explore large parts
of the world or one of the ships to you
and then it's in Santa Maria 15 years
later that Coulomb was used to
rediscover America yeah I I can't sing
hey but I don't know when is the real
pronunciation John huh I'm sorry yeah
and then the Western book friend which
some people call the most important
invention of the past 1000 years and the
Scientific Revolution starts and we take
half of that
and exactly the age of enlightenment and
the first steam engine the first
congressional steam engine unlike the
your grief steam engines the new common
engine was really used and and lightness
invented the binary arithmetic sand
build computers like this one and you
wooden eyes physics and this is one of
his telescopes and the fundamental
theorem of calculus was published by
lightness and Newton also said he also
had that and we take half of that now we
are two lifetimes before homeira
and it's exactly what many people call
the Second Industrial Revolution when
ought to invented the gasoline engine
and all these combustion engines the
diesel engine that cetera were created
and entangle and bends came up with a
car there's a replica of the first car
of this reminds me of something this
reminds me of something you know that
Obama is extremely popular in Germany
did you know that when he went to
Germany there were 200,000 people who
cheered him but the other day is that
something shocking
any Germans because he said the car was
invented in America
he didn't say by whom
maybe by Abraham Lincoln but now many
Germans are worried that next time he
will say our mental Lincoln also
invented lederhosen and so but then also
maybe the most important invention of
that time was the germ theory of disease
which saves hundreds of millions of
lives and founded modern medicine cheap
electricity became available modern
chemistry so and many of the big
companies that are still existing today
they were founded right there and the
highest building ever was full of the
Eiffel Tower was right at that time and
we take half of that and you see I'm
always missing there the awful parts of
world history I'm always missing some
things like the Second World War this is
right in the six years oh my god mine is
one lifetime and Sputnik starts the
Space Age and Apollo lands on the moon
and DNA is discovered or gets at least
the Nobel Prize for the discovery
reading and modern pop culture emerges
the computer was invented long before
that but for the first time it became a
commercial important commercially
important thing hydrogen bombs give a
new quality to the concept of war and
again whatever thing people think things
are going forward they build a nice
building ever so that was the World
Trade Center and the the population
explosion of the Train century probably
the most outstanding feature of the 20th
century was at its peak it was triggered
in 1980 by Fritz Harbor who came up with
artificial fertilizers and then within
one century the population went up from
1.6 billion to 6 billion in the end but
the highest derivative that was right on
the 6th years which in many ways defined
the world as we know it today and we
take half of that
this is now of course and again people
are buying the highest building ever
this is I think twice as high as the
World Trade Center they are building
that in Dubai and the world has
transformed again by networks so the
world wide web was created at the
Eastern here we see a picture of the
CERN Tim berners-lee created www.and and
the cell phones of course and and and
the GPC is a transformed industry and
private lives and of course the
mathematical theory of optimal
University my emergences right at this
time and so how is it going to go
continue well many say claims that in in
2020 1/4 lifetime before army gonna see
pcs will match the rock and potential
power of a human brain and then ten
years before army girl who knows what's
going to happen in five years and two
years and then you know obviously it's
converging history's converging you see
here's another of one that shows that
history is converging now it's not based
on human lifetimes anymore but on the
main breakthroughs in computer science
1623 Shikha builds the first computer
two centuries later Babbage comes up
with the with with program control
computers so she cuts computer you
couldn't program you could just do
certain simple arithmetic operations it
didn't work so because maybe is he for
several reasons and then one century
later the first really working computer
was built by SUSE in 1941 right in a
decade and the thirties everything
changed
you just as media infinite invented the
patented the first transistor Google
founded see you and computer science and
together with touring came up with the
Turing machine and by the end of 30 is
the field as we know it today was more
less established and then half century
later 1990s a www was created and within
a few years changed the world again and
of course you you know something about
statistics and machine learning you know
if you've got three data points the new
can extrapolate without hesitation it
was centuries one second one half
century and obviously it's going to
converge again exactly in in 2014 just
for fun
just for fun let's redefine Omega naught
is not 2014 anymore but fifteen fourteen
fifteen fourteen now ninety six years
before Amiga which is now fifteen forty
the most important thing in the
time-life list of the top events of the
previous millennium happened India the
invention of or the reinvention of
footprint by forty point four
forty-eight years later it's a second
most important event in the time-life
list of the most important events of the
past millennium happened Columbus
discovers America he discovers a mic
yeah yeah yeah what and so I'm you're
totally right so maybe that Chinese has
a very different opinion about that and
of course Columbus did not become famous
because he was the the first to discover
America but because he was the last to
discover America and then 24 years later
the third most important event in the
time-life life list of most important
events of the last 1,000 years happen
namely luta started the Reformation now
our Chinese friends will maybe not be
convinced that this was but but do you
see the pattern
96 48 24 it's converging right there in
1514 and that's one of the reasons I
think why we never had a shortage of
prophets claiming that the end is near
you look at the data point you look at
the logarithmic plot so as always some
way are fitting a straight line through
all these data points especially if you
ignore the data point and so you
shouldn't you should take all of these
predictions with a grain of salt and
maybe maybe actually there's a reason
why it looks to every subject as if
history is accelerating dramatically
look at yourself
most of your memory space is allocated
to the most recent events you'll still
know what you did yesterday we do you
know what you did one year ago and
exactly at this point in time so first
you further you go back in time for for
less if the smaller the fraction of your
memory space probably that you allocate
to do this
time and maybe the same is true for
history books most history books today
are written about events that happened a
few decades ago and the bulk of all the
history books is really concentrated on
recent events and not on very distant
events so maybe that's just like an
inverse speed prior where you are
allocating memory in proportion
well exponentially in proportion to the
size of the previous time intervals that
are and enter the distance of these
previous time intervals as you go back
you allocate less and less memory to
these events but even that if that is so
it's still clear that we are living in
exciting times and I think it's a
privilege to be part of that and let me
finish by saying one one more thing
thank you very much for your attention
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>