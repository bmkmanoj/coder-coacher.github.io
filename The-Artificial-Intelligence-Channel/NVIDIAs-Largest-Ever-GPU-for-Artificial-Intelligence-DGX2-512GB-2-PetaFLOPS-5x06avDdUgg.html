<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NVIDIA's Largest Ever GPU for Artificial Intelligence - DGX-2 512GB, 2 PetaFLOPS | Coder Coacher - Coaching Coders</title><meta content="NVIDIA's Largest Ever GPU for Artificial Intelligence - DGX-2 512GB, 2 PetaFLOPS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NVIDIA's Largest Ever GPU for Artificial Intelligence - DGX-2 512GB, 2 PetaFLOPS</b></h2><h5 class="post__date">2018-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5x06avDdUgg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">deep learning deep learning has
completely turbocharged modern AI this
incredible algorithm that could
automatically detect the important
features automatically detect the
important features out of data and from
this date from this algorithm it could
construct hierarchically knowledge
representations and these knowledge
representations if you give it enough
information if enough data it would
become more and more robust and
recognize a larger diversity of that
space become more intelligent deep
learning has revolutionized modern AI
the thing that deep learning needs Bell
is a ton of data and more importantly a
ton of compute and the reason for that
is it just Edward lovely goes through
and tries to figure out what are the
important features using all of these
incredibly revolutionary algorithms our
strategy at Nvidia our strategy at
Nvidia is to advance GPU computing for
deep learning for AI at the speed of
light from the processor development to
the systems the interconnects the way we
construct systems to all of the software
layers that are on top of it making it
available and partnering with cloud
service providers and OMS everywhere
irrespective of what AI framework you
use or what AI network you're trying to
create wherever you are we will support
it and we will support it from end to
end so that the frameworks that you use
to develop the networks and models will
be deployable in large scale and we
would even make it possible for you
access our platform in every possible
way and in any sort of way whether you
would like to build a personal
supercomputer for a few thousand dollars
with our Titan V or to be able to rent
time in the cloud
for a few dollars an hour or to be able
to build your own super computing
cluster or just to buy one from us we're
gonna make it possible for you to do
your work at the speed of light and to
be able to do it anywhere today we're
announcing several big things first we
are doubling our GPU the Volta v100 is
now 32 gigabytes of HBM to memory it's
twice the size of what it used to be now
this is so important and the reason for
that and I'll show you later
the networks are getting larger and
larger and larger and so it doesn't fit
in 16 gigabytes researchers would like
to not worry about memory size while
they're developing their neural networks
and 32 gigabytes gives them just a lot
more space HBM to the fastest memory in
the world connected to the volt of you
100 it'll be available as of now and the
next DG access you buy it'll be
available in the cloud ver very shortly
it isn't complete volume production
everywhere
the neo Volta with 32gig newer networks
are growing and evolving at an
extraordinary rate at a lightning rate
what started out just five years ago
with the Elex net Alex Khrushchev skis
now world famous Alex net was eight
layers deep had millions of parameters
eight layers deep millions of parameters
eight layers deep and millions of
parameters
it's a CNN the CNN was first developed
and and produced or created by young
Laocoon and the the the training
technique is called stochastic gradient
descent by geoff hinton and it was
implemented in alex net for the very
first time and a one the imagenet
competition well five short years later
thousands of species have emerged
thousands of species of artificial
intelligence networks models of all
different types from the cnn's of yan
lacan to the RN ends that was first done
as Schmidt Hoover's lab in in in
Switzerland the generative adversarial
networks of Ian's good fellow the deep
reinforcement learning that was most
recently done by deep mind emesis deep
line and Peter Beale over at Berkeley
all of these different architectures
have hundreds of different
implementations the number of species is
growing so incredibly and there are so
many new ones coming out Hinton just
recently did disclosed the capsule net
so that he could not only recognize
images in its image form but to learn
the geometric form of the images so many
new techniques neural collaborative
filtering ncf which is used for
recommenders what started out just five
years ago as eight layers and a few
million parameters is now hundreds of
layers and billions of parameters
eight layers and hundreds of millions of
parameters to hundreds of layers
billions of parameters neural network
models have increased in complexity not
to mention the number of species the
thousands of species that are now out
there neural network models have now
grown in complexity 500 times in just 5
years 500 times in five years
remember Moore's law would have only
kept up with 10x of that 500 times in
five years so the world wants the
researchers all over the world wants
just a gigantic GPU not a big one a
gigantic one not a huge one a gigantic
one and so ladies and gentlemen today I
would like to announce the world's
largest GPUs this is the world's largest
GPU this is 16 Volta equivalents
connected by 12 brand-new high
throughput switches that the world has
never seen is called MV switch these 16
Tesla V 100 equivalents each with 32
gigabytes creates virtually a 512
gigabyte memory not only that these 512
gigabyte memories the way you address
the memory and the way that every single
GPU could talk to the memory of another
GPU completely using the same memory
model the memory fabrics semantics on
our chip that connects all of the
processors have been extended out of
this chip out of the GPU onto the envy
switches connecting each and every one
of them in total in total 14 terabytes
per second of aggregate bandwidth 14
terabytes so that's what is that let's
think about that 14,000 14,000 gigabytes
and let's say if a high
resolution movie is 10 gigabytes okay
so 14 thousand gigabytes 1440 movies
1440 movies more movies and any human
has ever seen could be transferred
across the switch in one second like
that
ladies and gentlemen fourteen thousand
movies fourteen thousand movies
downloaded in one second dream come true
altogether eighty one thousand nine
hundred of course to pep to pet flops to
petaflop
this GPU is to peda flops I told you
earlier the fastest supercomputer on the
planet is 125 the fastest supercomputer
in America is a hundred peda flops
this is too too pitiful ops of tensor
course for a eye on this GPU the world's
largest GPU let me show it to you no no
no that thing
hang on saying you know guys here's the
thing we're so busy pulling the stuff
together we don't rehearse okay our
rehearsal is basically grip it and rip
it ladies and gentlemen the world's
largest GPU
you didn't you didn't fall for that did
you you didn't fall for that hang on a
second this isn't the world's largest GP
this is the world's largest GP you guys
it's sitting in plain sight hiding in
plain sight right here
Sandee if I could give that to you now I
I understood earlier that that was that
the game plan for me to do this myself
okay it wasn't alright ladies and
gentlemen grip it and rip it this this
ladies and gentlemen is the world's
largest GPU now what you're looking at
is something that's really truly amazing
so let me let me show it to you this way
and I'll come back to that okay this
switch has two billion transistors made
of TSM C's 12 nanometer FinFET this
great process this every one of these
switches and there are twelve of them
every one of these switches has 18
lengths which are eight bits wide 818 MV
lengths 8 bits wide with a surtees
that's moving at 25 gigabits per second
25 gigabits per second on just one
signal there's eight of them per length
there's 18 links on this one chip and
it's bi-directional all together it
creates 7.2 terabits per second or 900
gigabytes per second so 900 gigabytes
per second by the way of bandwidth well
that's a lot of movies too so 7.2
gigabits is 7.2 terabits per second
hundred gigabytes per second goes
through one switch there are twelve
switches twelve switches what that
basically says is that every single GPU
every single GPU can communicate to
every other single GPU at 20 times the
bandwidth of PCI Express
is that amazing every single GPU can
talk to every other GPU and and every
single GPU can talk to every single GPU
in a non-blocking way it's a fabric it's
not a network it's a switch
it's a non-blocking fabric switch with a
memory semantics memory programming
model that's exactly the same as what's
inside our chip and therefore all the
reason all the rights and all the
Atomics just works across this the
bandwidth and the performance and the
latency of this switch is incredibly low
it's incredibly low unlike a network
this is a switch which means every
single GPU can talk to the other GPUs
with extremely low latency you one read
you want to synchronize a parameter it
takes no time 300 gigabytes per second
and all together Nvidia is largest
graphics card this is what it looks like
the largest graphics card
the largest graphics card the world has
ever made the largest graphics card the
world's ever to petaflop s--
512 gigabytes most most most graphics
cards in the world today 2 or 3
gigabytes 2 or 3 gigabytes this is 512
gigabytes 10,000 watts 10,000 Watts only
only ten thousand watts only ten
thousand watts the amount of airflow is
really quite amazing to cool it and it
and we designed this this this porous
this porous fabric in air intake in the
front it looks beautiful but it's
incredibly functional air comes
traveling through it flows through all
of our chips ten thousand watts of
energy power is being thermal managed
three hundred fifty pounds
no human can lift it no three hundred
fifty pounds the first time I saw that I
was thinking I could lift at didn't move
and I'm fairly strong upper body
strength and so 350 pounds what's inside
of this 16 Tesla v1 or 30 gigs with 12
MB switches and it's got this incredible
back blue it's called a playing card it
basically has 200 times it has 200 times
the bandwidth of the highest speed Nick
on the planet so imagine it's like 200
Nick's of connecting the top to the
bottom connecting the top GPU tray to
the bottom GPU tray each Nick call it a
thousand two thousand dollars call it a
thousand dollars two hundred thousand
dollars of Nick's would be required to
connect the top to the bottom it has
that much bandwidth and as that much
bandwidth in order to use networking
cards well we still have to connect
multiple of these and so we have eight
of the world's highest speed NICs from
Mellanox to connect multiple of these
systems together it has the two it has
the two fastest CPUs we can possibly buy
the Xeon Platinum's 30 terabytes of
storage 30 terabytes of storage on the
system because as you know we're gonna
crunch through a lot of data really
really
so all together all together ladies and
gentlemen it looks like this this is the
nvidia d GX - the world's largest GPU
the world's largest adding card -
petaflop s-- 512 gigabytes 350 pounds
and just beautiful this this is
this is what an engineer finds beautiful
you guys this is sexy this is beautiful
this is unbelievable
incredibly beautiful and not to guys did
you guys notice the animation and videos
and videos artists are so spectacular
this entire animation was shot with a
hundred and twenty one hundred and
twelve shots it's photographed a
stop-motion animation this is not 3d you
know that we could do it in 3d but we
chose not to why
because it's more fun you guys want to
see it again
oh stop motion animation guys check it
out
guys I love you guys it's incredible
Tim Burton you got nothing on us look at
that the world's largest GPU 350 pounds
well let's see what it can do ten times
faster than DG x1 does that even make
any sense
ten times faster than DG x1 and so this
is six months ago this is literally six
months ago we showed these numbers and
the entire stack look at the entire
stack there's so much software that's
necessary to build this this is a high
performance computer it's a
supercomputer designed for deep learning
and so look at this entire stack of
software that was optimized as a result
running PI torch the framework and
training fair speak the sequence the
sequence model used for national
language use for a machine translation
literally took fifteen days on a DG x1
which was a world record at the time and
now it takes one and a half days well
the amazing thing is you don't have the
researchers don't have 13 days left to
sit around the researchers are now using
all kinds of techniques to use AI to
create a ice and so the number of
experiments the number of permutations
the exploration space of AI sweeping
through all the different configurations
of architectures in the way that compose
it
the number of layers the training rates
sweeping through all of those hyper
parameters and three sweeping through
all the different architectures at the
same time is going to create just
hundreds of experiments at one time and
so as a result with bigger networks more
data more experimentation dgx to
couldn't have come at a better time and
so it is ten times in six months now the
thing that's really amazing is this how
much should we charge
I mean look it took hundreds of millions
of dollars of engineering it took
hundreds of millions of dollars of
engineering this is the first unit so
this is call it 250 million dollars and
for a just an incredible
friend-to-friend price of one and a half
million dollars it seems like chesaning
it doesn't even seem fair it doesn't
seem fair at all now ladies and
gentlemen three hundred and ninety-nine
thousand dollars
you guys understand they're only two
themes in this in this conference that's
right the more you buy the more you save
okay this front rows got it we're gonna
keep working alright the more you buy
the more you save and so three hundred
and ninety-nine thousand dollars for the
world's most advanced most powerful
computer this is what it replaces three
hundred dual CPU servers easily for
three million dollars easily for three
million dollars easily for three million
dollars but the important thing is a
hundred eighty thousand watts 180 watts
in order to train fair seek and one and
a half days well it only takes one of
these beautiful dgx to just one dgx to
ten thousand watts only ten thousand
watts 118th the power one eight the cost
the more you buy the more you save it's
incredible
you know it's truly amazing how far
we've come in just the last five years
we started talking about deep learning
about six or seven years ago and at the
time the effectiveness wasn't anywhere
near what did this today um five years
later five years later five years later
since Alex net the progress that we've
made is literally incredible and I think
it'd just be kind of fun just to do a
report card see what happened in the
last five years this is what it looks
like this is what Alex Khrushchev gives
Khrushchev ski trained Alex net on to
dgx to GTX to GTX 580 s to GTX 580 s and
it took them six days but those were six
worthwhile days because he became world
famous he won imagenet and kicked off
the deep learning revolution five years
later we could train it in 18 minutes
five years later we could train it in 18
minutes five hundred times faster five
hundred times faster is it not amazing
guys
to be able to do a task 500 times faster
in five years to be able to do any task
500 times faster any task of great
importance 500 times faster or to be
able to do a task of great importance
500 times larger in the same time this
is some kind of a supercharged law that
we're experiencing well one of the
things that that made it possible of
course is what I said earlier this is an
innovation that is not just about a chip
this is animation about the entire stack
everything was touched thousands of
engineers all over our company from VLSI
to process engineering to package
engineering the circuit design
architecture the chip design system
software solvers and libraries and
algorithms and systems all got involved
this is the effort this is the work this
is the body of work of literally every
single employee at Nvidia we are all in
on deep learning and this is the result
and so over the next several years it's
going to be just utterly incredible
because we're just picking up steam this
is some new type of computing the amount
of data is growing exponentially there's
evidence that with GPU computing the
computation is growing exponentially and
as a result deep learning networks and
deep learning models AI models are
growing in capability and effectiveness
at a double exponential at a double
exponential more data more computing
both on an excellent exponential growth
compounding together into some kind of a
double exponential for AI that's one of
the reasons why it's moving so fast
really really exciting times</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>