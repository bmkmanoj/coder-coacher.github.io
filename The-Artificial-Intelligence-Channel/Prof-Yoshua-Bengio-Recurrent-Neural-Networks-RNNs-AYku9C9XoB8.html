<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Yoshua Bengio - Recurrent Neural Networks (RNNs) | Coder Coacher - Coaching Coders</title><meta content="Prof. Yoshua Bengio - Recurrent Neural Networks (RNNs) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Yoshua Bengio - Recurrent Neural Networks (RNNs)</b></h2><h5 class="post__date">2017-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AYku9C9XoB8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we've heard about backdrop and
multi-layer nets there's a special kind
of network which is really central to
dealing with sequences you've all heard
of recurrent networks so I'm gonna try
to go a little bit deeper than the
standard explanations today the heart of
what's going on with recurrent Nexis in
this equation which is really really
simple it's an old equation that they
expand well before recurrent Nets it's
just here a deterministic dynamical
system with a state vector s T which is
obtained through a parameterize function
f theta of the previous state s T minus
1 and the current input XT and we're
gonna have a sequence of these X's and
so if you unfold the computation as we
say you get the kind of graph that you
see on the right where we have a
sequence of inputs and we have a
sequence of states so remember each of
these states in the case of recurrent
Nets is gonna be a real valued vector
and the XS could be anything that you
usually put an input to a neural net the
really important thing is that the same
parameters are used over and over right
so in a normal feed for net you have
different weights at different layers
here you can think of that as a very
deep net they're the same parameters are
used for all the layers and also you
have the input coming in like at each
layer if you if you think of this as a
deep net going to the right the other
interesting thing about this is to
realize that what we are really doing is
computing a variable size function you
know a function with a variable number
of argument so if you look at the state
at time T it really is a function
indexed by T of the first T inputs and
so now what we're building is a
mechanism to map a variable length
sequences and later we can see we can
generalize to variable length data
structures to fix size vectors now those
stick science vectors could be pretty
large
and you'll see for example when we
talked about memory networks and things
sort of modern extensions of recurrent
Nets now the state could be so large
that you can actually store everything
about the past but the original idea is
that that state is a sort of a summary
of everything we've seen before but it's
not necessarily a summary that is like
in compression that keeps all the
information but it's a sort of a smart
summary that's gonna be trained to keep
only the information that we actually
need for further processing so this is a
building block we're going to use that
state to do things to predict what's
gonna happen in the future to take
decisions to do things like translating
generate speech or whatever it is that
we're we're we're doing with sequences
okay so that's the basic building block
and of course we can write it compactly
with this kind of circuit diagram which
is inspired from Electrical Engineering
where we use the little black box to
indicate a delay of one time step
between the update of the state and the
previous state so if we stick on top of
that some outputs that's gonna depend on
the current state then we get a graph
like this which can be used to map a
sequence about inputs to a sequence of
outputs of the same length so these were
the kinds of recurrent Nets I was
playing with when I was doing my PhD in
the late 80s and at that time we didn't
have the kind of modern things that I'll
show you later where we can do things
like map sequences of some length two
sequences of all the lengths but anyways
this is sort of the the first kind of
use that people have done where you
causally process a sequence to generate
or to predict or to transform into
another sequence now there's something
really really important in many
applications about recurrent Nets which
is that we can use their output as an
input for the next time step and when we
start thinking about this we can we can
use recurrent Nets as what we called
Auto associative generative models so we
can use recurrent Nets to generate
sequences and we can use them to
generate sequences in a probabilistic
sense we can represent a probability
distribution over sequences
vectors or sequences of symbols or
verses of anything you want so this is a
really really important advantage of
recurrent Nets and sort of frames them
in a classical Maxim like you'd setting
which which of course is very convenient
so let's look at this more carefully if
you have a sequence X 1 to X T and
you're trying to characterize their
joint probability in other words we're
trying to characterize how the different
elements of the sequence interact with
each other in terms of their statistical
dependencies then we'd like to have some
kind of model of the joint but but the
joint distribution over variable length
sequence is a complicated object there
are many random variables and it's not
clear how we can write a function that
has a variable number of inputs but
remember with recurrent Nets we can deal
with variable length data and in
particular we can compute something like
this joint probability so we're going to
use this trick that I showed you in the
previous slides to represent the joint
and the way we're going to be doing it
is we're gonna decompose the joint as a
product of conditionals this is you know
coming out of simply the definition of
conditional probability right so if if
you know P P of a and B is just P of a
given B times P of B right so if we
apply this recursively and I you know do
it for P of ABC right we get the product
rule and this is exactly what we have
here but replace ABC by x1 x2 x3 x4 okay
so that means we can represent a joint
distribution which otherwise would seem
like a very daunting task because you
have so many variables so many
interactions into a series of
conditional probabilities where on the
left-hand side of each probability we
have only one random variable so
capturing the the joint within that
random variable condition on the past is
much easier than trying to capture the
joint overall so we've sort of broken
down this very hard problem of modeling
a joint distribution into modeling a
bunch of
additionals each of which is much easier
to do alright so practically how does
that relate to a recurrent net well
remember we have this sort of state
update equation and we're going to
produce some outputs but now let's think
of those outputs as telling us about the
probability distribution of the next XT
given the previous ones the previous
ones are summarized by the state so the
state that T summarizes all the previous
X's and now we're going to compute say a
probability that say it's a symbol so
we're going to produce a probability for
each of the possible symbols values for
the next time step and and then we could
sample from that distribution and we got
actually a new symbol and we could stick
it in as the next input or that's how
we're going to be generating a sequence
right we once we are able to compute
those probabilities we can also sample
from them and we can use the sampled
symbol for the next time step as an
input to the next time step and then we
use that to feed the update of the next
state and then for the next state we can
compute a gain the probability for the
next symbol we can sample that and we
just stick it into the next input and so
on and so on and this way we can
generate a sequence so we can do all the
things that we usually care about with
probability models we can sample here
it's a sequence we can compute the
probability of a given sequence and
because those probabilities are fairly
smooth easy to compute functions of the
parameters which are the all the weights
that go in the middle we can compute the
derivative of the log latitude with
respect to parameters and so we can do
maximum like your training so how do we
compute the probability of a sequence
well so if somebody gives us a sequence
now we're not going to be generating the
symbols because we have them but we can
just compute by applying this formula so
we we force those X's to be the observed
values and then we we see what
probabilities we're getting at each time
steps and we take these the
probabilities for the observed thing
right
the actual next symbol in the data and
we just multiply those probabilities
together that's it and once we can
compute those we can of course compute
derivatives we stick that in a automatic
differentiation thing like V&amp;amp;O and tends
to flow and so on
alright any question up to this these
these are really the basics but it's
really important to understand them yes
so question is about early days as I
said I was working on things like this
in late 80s and the main application I
was looking at was speech recognition
there were already some people trying to
use this for financial applications time
series data I remember people working on
other kinds of natural time series like
the the the Suns even your cycle and
things like that but yeah the main I
think the main application was
handwriting and speech in those days
which we're already using these kinds of
recurrent Nets yes yeah
great so the question is is this a
finite state machine no it's an infinite
state machine because the the state is
real valued yes but it's of course very
closely connected yes yes okay so what's
the connection to Bayesian Nets so it's
it's a good question you can actually
think of this picture like a graphical
model where the nodes for the state are
particular in the sense that there are
deterministic notes so you can you know
you can have deterministic nodes in the
graphical model and and in that sense
this looks like the same graphical model
is things like hidden Markov models
except that the state is is continuous
valued and a high dimensional compared
to hmm and much richer because of that
and the other big difference is that
these states are now have a conditional
distribution given the past which is
deterministic which means that inference
when you go from left to right in other
words if I if I know some inputs I can I
can compute the properties of all the
future ones is trivial but inference
going in the other direction in general
is gonna be hard so so it is a
particular kind of directed graphical
model the other thing compared to other
graphical models is that if you were to
try to model the full joint in general
it would be very very expensive and
because we're using this sort of funny
parameterization with this recursive use
of the parameters at each time step we
are able to very compactly represent a
joint distribution which would otherwise
potentially require an exponentially
large number of parameters if you look
at the length of the
sequence so this is this is an
interesting sort of particularly
efficient way of representing a joint
distribution okay now we can play all
kinds of games once we understand this
principle and there's a lot more that's
not covered on that slide for example
well there are easy things we can do
like we can read a sequence and then
predict or calculate some vector right
which which we can use for a
classification or whatever you want so
that's sort of a trivial thing we can
look at the final state after we read a
sequence and then use it as input to
some MLP or whatever to compute
something we care about so that we can
do sequence to vector we can do sequence
to sequence of the same life that's what
I showed in the previous slide we can do
vector to sequence so as I said earlier
we can generate a sequence but there's
it's very easy once you have a journal
of model to make it a conditional
generative model with neural nets we
just stick in extra inputs so we had our
recurrent net that could generate a
sequence of outputs but now we we take
some some vector of features that we
want to condition that probability
distribution over sequences and we just
you know use that extra input that's
going to condition every state or every
parameter whatever you know
parameterizations we care so we can
condition on a vector and of course once
you start thinking a little bit more
about it we can condition on a sequence
so that's a fairly new idea that dates
back to around 2014 when we were trying
to do machine translation and so so now
think of it like the way we thought
about it is we have two recurrent Nets
and we call this model that encoder
decoder recurrent Nets and Google guys
called it the sequence to sequence model
and I guess their choice of phrase stuck
but anyways you take a sequence and you
read it so that's the same thing that I
was telling you about before now you
have a state that summarizes everything
you've read in that sequence now you got
a second recurrent net it could be the
same one actually you're just going to
condition differently and and now that
one is a journal
so it also has outputs and its outputs
you know are used as input for the next
time step when you sample or when you
claim the inputs is just computing the
conditional probability of that sequence
given the input sequence so now you have
two sequences an input sequence and an
output sequence what we're doing is
we're reading the input sequence sort of
compressing it into a fixed sized vector
and then using that to condition the
generation of a next sequence which
could be a totally different kind of
sequence so that you know that made a
lot of sense for machine translation but
we're using this kind of model also to
do things like in dialogue where you
know you read some context and then you
generate an answer so there are many
situations where we we want to read some
sequence and then generate some other
things condition on that so that's the
sort of the sequence the sequence model
now III mentioned that but I would like
to come back to this that there are
these two ways of using a recurrent net
a generative recurrent net right one is
the way that we're actually running
things when we train when we are given a
sequence we call that teacher forcing we
force the the sequence to be the one in
the data and we just compute the
probability for those those elements
given the previous ones and so what we
see here in this picture is the wise
could be given from outside so you know
the data here is XY pairs and the Y's
could be given from outside and then
used as conditioning input to predict
the next probability for the next Y and
that's what we do when we compute
probabilities and this is what we do
when we compute gradients of the light
cue because we are given some training
sequence and we want to know what their
probability is so we can maximize that
probability compute the derivative with
respect to parameters but when we're
going to be using the recurrent net to
generate new sequences we're going to be
using actually a very different schema
where the Y's are not going to be given
by the outside we're only going to be
sampled from the model itself so now we
have a sort of
closed-loop versus open-loop process
these are two different kinds of
dynamics and if they're too far from
each other are we gonna potentially be
in trouble and there's gonna be a
mismatch between the kinds of things
that the network sees the kind of
context that it sees during training and
the kinds of contexts that it sees when
it's actually running free running
generating those contexts itself some
similar issues arise in reinforcement
learning that's for next week but this
this kind of mismatch can be a problem
so I remember when we were starting our
work on speech synthesis with recurrent
Nets that the network would they
wouldn't work very well initially and
although when you train them they would
see speech like things when they were
generating very quickly they went into
some kind of configuration which didn't
look at all like speech and then they
would sort of go off to crazy land and
generate a big bang or go to zero or
something like this so because the the
recurrent net during training has only
seen nice things like speech like
context when you show it something very
different there is no guarantee that it
does the right thing and then if it was
only you know one time step when this
happened the problem is not it feeds on
itself so you've got a kind of
compounding of errors from time step to
time step and then it can go in crazy
land and and do very bad things so this
is a concern in some applications and
and several approaches have been
proposed to deal with that which are
connected to earlier work in structured
output and reinforcement learning like
CERN and dagger so this this is actually
this I'm referring here to a paper by my
brother
Samy at Google and his collaborators at
nips 2015 where they they're trying to
deal with this by injecting noise during
training so that instead of using the
actual symbols that that are in the
training data they randomly flip some
coins so that sometimes so that you know
the inject a bit of noise in the input
so sometimes they actually use the
symbol from the data and sometimes they
they put in something random or some
combination
and you could try to anneal that noise
and all kinds of tricks and this
interesting questions about whether this
is a proper way of estimating
distribution they're all kind of
interesting questions around this but
I'm not going to answer them today but
this yeah yeah this is called curriculum
learning and it could help I don't think
it completely solves the problem though
because the problem is is running off
into configurations that it has never
seen so you need to so so my mental
picture of this is the way that we
really fix this
is you think of the dynamics and you'd
like to carve the dynamics so that when
you are away from kind of dynamics of
the data the the state to state
transition brings you back towards
things that are like the data and one
type of approach that I really like is
based on Ganz that you'll probably hear
about later this week
generative adversarial networks where we
so we had a paper last year called
professor forcing where you but but
there is now several ideas floating of
that nature where you you're trying to
look at the two dynamics and you try to
make sure that one is gonna push towards
looking like the other but yeah I think
it's pretty much still an open problem
how to solve this properly now in
practice the good news and that this was
true in our example I gave up speech
synthesis the good news is that if you
train sufficiently eventually even with
maximum likelihood these kinds of
problems tend to go away so really
well-trained model you know max my code
will reduce probably masse away from the
data and eventually get rid of those
spurious modes so regions where the
model would like to put probability mass
but it shouldn't and so in some cases
you don't need to do anything special
but this trick of injecting noise I
think is a good one
keep in mind as well there was another
question or yes I'll talk about the
vanishing grazing problem I'll talk more
about it
it's an optimization issue yeah
so so now we're going to start talking
more about all kinds of architectural
variance so one kind of variation is to
put in depth right you know that was of
course the the hot topic and it's still
but maybe a few years ago people who are
exploring how can we you know have
deeper X and deeper Y and how do we can
have deeper recurrent Nets and actually
there are many ways you can make them
deeper because if you if you look at the
vanilla architecture which is unfolded
like this where you have X 2 H and then
H to the next H and 2y each of these
arrows is really like a one layer MLP in
the standard vanilla networks but we can
replace each of these arrows by you know
deeper models like we can stick in some
extra hidden units between you know in
the state to state input and state to
state transition or between the state to
output transition
I mean model so that's one simple thing
the other way we can make them deeper is
we can think of the state as having
multiple groups of units corresponding
to multiple levels and that really
corresponds to a sort of constraint on
the architecture is still like a
recurrent net but instead of having
think of this one big vector and this is
the next big vector instead of having
all - all connection where all of the
units here talked to all of the units
everywhere these guys only talked to
those guys and these guys only talked to
those guys except but now we're gonna
have these extra sort of feed-forward
links going from the input to
higher-level so if we can now maybe
think of these different layers in the
heart in the neural net as a kind of
hierarchy just like in regular deep Nets
so that's another way that that's been
explored that that seems to work well
now once you understood this notion of
dealing with verbal sized structures
like sequences it's very easy to extend
this to other data structures and the
first of those that we studied in the
90s actually is the tree structures so
instead of recursing once we can have
you know a two-level weaker I mean to to
input recursion so we can imagine a
state here which depends on two states
rather than just one previous state it
could depend on two states now the
unfolded graph is a tree so so that's
one nice trick another example of this
is a special kind of graph which is not
a tree where you you you have at each
node sort of you have a grid and at each
node of the grid the I J node depends on
say the the neighbor to the left and the
neighbor to the north as inputs and then
you can also have different layers of
this so this is called multi-dimensional
recurrent Nets another another kind of
architecture which is very very popular
these days and highly successful for
example we use it always for machine
translation and and other sequence
processing applications is the
bi-directional recurrent net because one
issue with the standard we can't net
architecture is that it has this sort of
asymmetry in time right we go from the
past of the future and when you're
processing things in a causal way like
you have to answer on the fly based on
the data from the past it makes a lot of
sense but when you're given a whole
sequence well you could process the
sequence left to right right to left or
and many other ways so why not do it
right so that's it you we're gonna have
to we carrot nets one that reads the
sequence from left to right one that
Rize sequence from left from right to
left and then we're gonna use their
state as input to high-level processing
so now at each time step what do I have
I have two parts of the state one part
which summarizes which summarizes the
past and one part oh I guess I'm
supposed to be somewhere else
and one part in which which summarizes
the future so so really what we're doing
here is instead of having say looking at
each input say input word we now have
the word interpreted in the context of
everything before and everything after
and that's that's a really useful
quantity to to process okay other kinds
of games that can be played instead of
having the usual sort of vanilla you own
that architecture like this where you
have an affine transformation of the
input of the previous state we can we
can have multiplicative interactions
between the input side and the previous
state side so this this and then you can
have other variations on that so this
actually increases the expressive power
of recurrent Nets without requiring
deeper computation because actually one
thing I didn't mention is that when you
when you do this kind of thing where now
the state to state transition you know
now you can have interactions with
nonlinear interactions between the
previous state and the current input
going into the next day but the price
for this is that if you look at the
unfolded graph over time the number of
sort of nonlinearities that you have to
traverse has just doubled and as you'll
see because of this vanishing and
exploding gradient problem it can make
training more difficult and that is why
when we wrote this paper actually we
introduced skip connections like these
connections and these connections which
make sure that they are shorter paths
that that through which the gradient can
can flow more easily okay so now let's
talk about vanishing an exploding
gradients so I wrote this paper in 1992
and was published in 94 called learning
long-term dependencies with gradient
descent is difficult with Patricia Marr
and Paolo fresconi
I was at Bell Labs in those days
actually started this during my thesis
and then my postdoc at MIT and 91-92 and
I was trying to use recurrent Nets for
for processing sequences and I realized
that when the sequences got longer it
was harder to train them to capture the
thing so III tried to simplify the
problem and I got to a very simple
problem which was just try to memorize
one bit so I have an input sequence
which has the first symbol being either
high or low and then the rest is noise
and that the end of the sequence the
network has to answer a question was the
first input higher low sounds like a
trivial thing right it just needs to
remember one bit of information and in
fact we know that there's a solution
with a single hidden layer with a single
wait we can't wait that can store that
information we can know what the optimal
solution is for this but when we try to
train with gradient descent with back
prop as the length of the sequences
increases the probability of training
being successful decreased drastically
so we try to understand that and we came
to the following conclusion that if we
want the network to store information
reliably especially in the presence of
noise like in this experiment then the
dynamics of the network need to be
contractive so what does it mean that it
be contracted so contractive dynamics
are such that two points map of the by
this two nearby points are mapped to two
even closer points so that's what
contractive needs and contractive means
that there will be
attractors in the sense that if I were
to run the same dynamics like I would
eventually converge to some attractors
which would be a point attractor like a
fixed point or it could be a whole
region and and we can characterize those
regions where the dynamics are
attractive and we call them basins of
attraction so depending on where you
start in like in my example I could
start with the high value or the low
value you
end up either in this basin of
Attraction or this basin of Attraction
so deep so in other words the state
being on that side means we store the
value zero being on that side means we
store the value one that's like a very
prototypical simple thing of course in
general you can have many more bits but
but understanding the even the one bit
case is really interesting and there's
gonna be this boundary in the in the
state space where if you start on one
side you get into this Basin and you
start on that side you get under this
basin and there are regions around those
basins which are unstable so what what
it means is that if you start here and
you add a bit of noise you may you know
close to the other side so this sort of
a sensitivity to noise because of the
fact that it's not contractive in this
middle region so so it turns out that
the weather the dynamics are contractive
or not can be characterized very simply
by the spectral radius which is the
largest eigenvalue of the matrix of of
of derivatives the Jacobian of the state
to state transition so if you look at
the state to state transition which is a
function and that function as a matrix
the Jacobian matrix of derivatives DST
IDs t minus 1 J so the IJ element and
that matrix has eigenvalues which in
general will be complex but you can look
at their their magnitude and if the
largest eigenvalue is less than 1 then
the whole thing is contractive and if
it's greater than 1 then it it can it
can yield to n stay and stable diverging
trajectory so it's illustrated by the
size of these the size of these balls
getting larger and larger so a small
ball could expand into a larger one into
larger ball in other words if I start
anywhere from the small ball here I get
to a larger one and then anywhere
further from here I can get I mean one
one of those points here gets to one of
those points there so the the size
increases and especially true if I'm
adding noise at each time step but if if
I
in the sort of stable region then I'm
guaranteed to to you to stay within that
that basin of Attraction anyways
so so those eigenvalues being greater
than 1 or less than 1 are really
important and and you can understand
really what is going on with this
vanishing gradient problem by thinking
about these these eigenvalues the
spectral radius and the reason why this
becomes important if you think about the
the derivatives of the loss with respect
to the state at some early time it's
just a product of these Jacobian
matrices that I was telling you about so
you know by the chain rule that's going
to be the derivative the final loss with
respect to the final state times the
derivative of the final state with
respect to the previous state and so on
until we get to the last Jacobian ending
at s T so each of these except the last
one here as a matrix the size of the
state squared and think of these
matrices as numbers right so a single
scalar number if we multiply a bunch of
numbers together what's going to happen
as the length of the sequence increases
well if those numbers are less than 1
we're going to converge to 0
exponentially fast
if those numbers are greater than 1 we
can explode and if you have a mixture
it's of you know numbers that could be
sometimes greater and sometimes less
than 1 well it's a little bit more
complicated but you can think now in
random terms what's going to happen with
the variance of those of those products
and the variance is also going to
explode so I mean in some conditions if
the if the if the individual numbers
have have some properties particular
properties so this was actually this
idea that when we multiply all the
jacobians we could have vanishing
radiance was actually something that
hush rider said hush rider had
discovered in his master's thesis which
was written in German so I wasn't aware
of it I was working on the same thing at
the same time but he put it in as these
and actually it's because of this that
he worked on the LST M which we now use
in fact the the idea of the LSE M was in
his thesis in 1991 it only got to be you
know ironed out in the in the late in
1997 was the first publication but
really it came from his work in 1991 so
why is it a problem that we have this
you this product of jacobians which
could either become really large or
really small well to see the problem in
its full glory we have to look at the
gradient of the loss with respect to the
parameters so there are many ways that
you can post a gradient but I'm going to
use that way to illustrate what happens
so the gradient of the loss at some time
step T with respect to the weights of a
recurrent net you can rewrite by the
chain rule as VC T da so a now is the
state sorry for the change of letters
and that's for any earlier time sum over
all the previous time steps in other
words how the state at time tau
influences the final loss and how the
weights immediately influence the state
at time tau and now the Spheeris factor
we can decompose into DC TD 80 times dat
da da and that's that's a Jacobian
matrix but that's the one which is equal
to the product of Jacobian matrices and
that's you know the number of these
matrices is gonna grow potentially such
that the dot product will will converge
to either something very small or
something very large and that's a
problem because it means the gradients
will become either very large or very
small but there is something more subtle
here and this was the main point of the
94 paper which is that in the conditions
where we have stability in other words
where we can store bits of information
reliably that's when the spectral radius
is less than one and that's when that
product of matrices is going to converge
to something small and then what's going
to be the consequence the consequence is
that the total gradient is going to be a
soma
terms corresponding to the effects of
the parameters on the final loss
associated two different temporal
dependencies so you know when when taw
is just t minus one is the effect of the
previous time step on the next saw the
next time step and if when tau is one
it's the effect of the beginning of the
sequence on on the final time step T so
the sum here is over different temporal
dependencies different different spans
where we're looking at the dependency in
the sequence from here to here or from
here to here or from here to here right
and what this says is that the terms
corresponding to things far in the past
will be exponentially smaller if we are
in the regime where the network can
store things for a long time which is a
regime where we usually want to be in
applications involving long term
appearances okay so this is this is
really important the total gradient is
the sum of terms some of which will
vanish and others will dominate
exponentially the short term effects
will dominate the long term effects will
vanish and so the network will tend to
focus on the short term effects and not
be bothered too much with the long term
effect at least you know initially when
when the network is training its gonna
initially focus on the short term stuff
eventually when it gets a short term
stuff completely right the maybe be some
of those terms that used to be
dominating could be smaller and then it
can focus on the longer term stuff but
it might take a lot of time because
those gradients become very very small
okay yeah and this is this is really
different from the kind of vanishing you
have in deep Nets although there's a
there's a connection
it's different for several reasons but
so one of them is that we have the same
weights at every layer in the case of
recurrent net and that means that the
dynamics the the sequence of jacobians
tend to converge much faster to either
explode or vanish
so think about it like this if I'm gonna
multiply a bunch of numbers together and
they're all the same that's gonna
converge very fast either 0 or infinity
but if I'm allowed to multiply like
small large small are smart there's
gonna be some cancellation effects so
it's not going to converge as quickly to
something bad so that's one way to think
about it in a high dimension is even
more important that effect because you
have different directions and you know
it might explode in some direction and
vanish in other direction and then one
is going to really cancel the other so
that's really a different situation yeah
and also because in in the recurrent
case also the total gradient on the
weights because they're shared weights
is this the sum over the different times
whereas in the in the non-shared case
like the normal feet for net they all
get their separate gradients so they
don't like hurt each other in that way
okay so to summarize if the eigenvalues
of those state-to-state jacobians are
greater than 1 or at least one of them
is greater than 1 you can get gradient
explosion if there are less than 1 for
sure you're gonna get vanishing and if
they're random well the variance can
grow exponentially fast now how do we
deal with this so it turns out that
there's a very simple trick to deal with
the case where the exploding case so
first of all the exploding case isn't
that common in practice because we tend
to want to remember stuff and then
usually the that case the network won't
necessarily go there but if it does you
know by mishap during training you can
use what's called gradient clipping
which I'll explain later
where you just prevent the gradient from
being too large when you update and the
the one is harder to deal with this as
there is the vanishing case so besides
the the great clipping trip which is you
know one line of codes here there are
other tricks so the the grating clipping
trick is there are different variants of
it but the simplest one is just prevent
the norm of the update to be too large
by taking the gradient and then
Thresh holding its norm so if the norm
is greater than some threshold make the
norm equal to the threshold and then
continue with that do the update with
that and and we can also we can win AK
we can actually see that this trick
works in low dimensional cases where we
can visualize things and we find that in
recurrent Nets the the shape of the
objective function isn't the the typical
valley that we think about in in sort of
optimization continuous optimization
problems it's more like things like this
where you have these cliffs
corresponding to corresponding to these
regions where you transition from from
one basin of Attraction to the other and
where the derivatives could be very
large oh yeah well I didn't explain why
why the derivatives can can explode so
that's actually a cool thing when you
look at the basin of Attraction story so
remember we're gonna have these regions
where the rivers small here the
derivatives are gonna be small less than
1 and converging and here as well but in
between they have to be large because
think about it a small perturbation here
can lend you either here or here which
is a huge effect so that means the
derivatives of the those jacobians here
they have they must have huge
derivatives around here right because a
small change to make a big impact so
there are places in the state space
where things are bad the sense that the
gradients will be large and so you know
we we really need those those clipping
things and this is kind of what's
happening here
when you get close to this region you
have a huge derivative so this is the
cost with respect to parameters and the
clipping trick what it's doing is that
when you get close to this large
derivative place if you if you didn't
clip you know the gradient would be so
large that the parameters would go crazy
somewhere and by by bounding them you go
you know away from that cliff but you
don't go too far and you can continue
sort of going down this this slope which
would you know it's a like a dangerous
place to be if you've just
trust the gradient another reason why we
want to clip gradient is that we can't
really trust large gradients remember
that what's the gradient right the
gradient is supposed to tell us if I
make a small change of the parameters
this is about the change in the cost
that I'm gonna get and when you have a
large gradient it's you know something
crazy it can be large everywhere it's
not you can't trust that to make your
step because it's saying oh that I could
like have a huge reduction in the cost
if only I made a small change in the
parameters in that direction of course
it's not true it's true only like
infinitesimally when you are it's like
here when you're sitting here it's
promising you that if you make a small
step you're gonna have you know a huge
drop and it's gonna be true for you know
very small time but then it's not true
anymore
so you basically you can't trust large
gradients when you do gradient descent
okay it turns out that when we were
studying this we proposed something
which is very close to the gru that I'll
tell you about later which is a simpler
version of the lsdm what else
delays so another trick to deal with
long term dependencies that was actually
proposed delays and hierarchies were
proposed in the 90s in addition to the
lsdm as a strategy to deal with long
term dependencies and and and the
thinking is very simple if we can create
shortcuts in the unfolded graph the
number of of these jacobians that we
need to multiply the number of these
nonlinear time steps that are going to
be composed can be made smaller at least
on some paths
so if I have delays like an upper right
what's going on is of course I you know
there's some path that goes through all
the time steps but I have these skipping
connections through time which create
these shortcuts through which the
gradient can flow through you know
longer sequences and similarly in in the
middle figure and more recent models
we're using for dialogue if you have a
graph that has both long paths and short
paths at least grading can flow through
some of the short path so here the way
we create shortcuts is you have
different levels in in you know you have
a deep recurrent net and and some
neurons get updated less frequently or
more slowly with more you know some kind
of inertia so that the derivatives can
flow over longer time steps in the
higher levels and at least some
information gets through and maybe the
the places where you have more frequent
updates will be able to capture the
short-term dependencies and the higher
levels which change less frequently will
capture the longer-term dependencies so
I think this is a good plan and there's
still a lot of research trying to to
make that plan work more recently you
know we worked on something like this
for dialogue where you have two levels
in the recurrent net one that handles
the words in an utterance and the other
one that updates after each utterance
after each sentence and so so this one
can capture longer term dependencies and
those ones can capture the sort of
within utterance dependencies
okay now the lsdm and gr you are part of
a family of recurrent Nets that use
gating units so gating units are really
a magical thing that's everywhere in
many architectures neural net
architectures but especially those that
deal with sequences and so here we're
gonna be using the gating units to
create the kind of hierarchy or and
shortcuts that I was talking about here
so that in some places instead of
updating we're gonna be copying right so
if the state is not updated but is
copied then gradients will just flow
through directly so how do we get that
well we we create a self loop which is
essentially going to be doing a copy or
an additive copy so we have some state
and we're going to
that state is equal to the previous
value times a gate which is hopefully
gonna be one very often plus something
the plus here doesn't hurt us it's okay
the the point is that when you unfold
this there's a direct path when the gate
is won through the past and you can it
can compute gradients to this so what
let's say the gate here is always one
then what's gonna happen is that the
state is gonna be something st is gonna
be st minus one plus xt right so that's
a really simple thing in practice of
course we can have a gate here and and
that gate is going to control whether we
actually copy or and add some some new
stuff or we actually ignore the past so
this is the central element of the last
LST i mean has other gates which are
less important actually which decide
whether we want to kill off that thing
or or whether we want to use the output
for the rest of the network this is the
output gate but more recent word
suggests that these gates are much less
important so the really important thing
is we have these these gates and you
have you find that gate isn't in the gr
u as well which is sort of a lightweight
version of the LS TM there was a
question oh well well I mean it's hard
to get it to be exactly one right so if
it was always one like this the problem
is now well you can see that this sort
of limits the expressive power what can
can be done for example sometimes you
actually need to forget things that's
why they call this gate the forget gate
I mean remember that recurrent net is
trying to summarize things and so this
particular recurrent night with a gate
here can choose
when is it a good time to clear the
state and so and this forget some bit of
information about the past and when we
should just continue adding up stuff
from from the outside will is that
answering your question or you had
something else in mind yeah that's what
it's doing here it's the same it's the
same I mean if the gate is one it's
copying if it's zero is forgetting
slightly less than one well okay so
there are practical questions here so I
mean of course G is not gonna be either
a zero or one it's gonna be the output
of a sigmoid that's gonna be between
zero and one so it's not gonna be
exactly one and I guess that we'd like
to have something that would be one
exactly but it's hard to learn something
like this because we're looking for
continuous quantities and if we're gonna
be airing we want to err on the side of
contractive so less than one and that's
the strategy that's been adopted in
these nets I'm not saying it's the only
way of doing it but it makes a lot of
sense if if so actually that's right so
we we had a paper a couple of years ago
called unitary recurrent Nets where are
we trying to make it make it exactly one
in a complex domain and it kind of works
but it's missing the forgetting part and
so we try or trying to you know put in
some forgetting in other ways but but
yeah you could you could force it to be
one but but in the real case it's sort
of a bit drastic you only get an
addition for example remember look one
interesting thing about this is that if
you have exactly one in this formula
you're just adding things up so it's
accumulating and so it's it doesn't care
about the order in which things were
presented for example so they're they're
issues but but it's I think there's
still a lot to be understood about these
architectures and it's still pretty much
a field of investigation how to design
those architectures let me tell you a
few words about
another way that you can use gating
mechanisms to sort of copy the state in
a way that you can now remember things
from the very far pasts and this is
something you find in your old string
machine to some extent in memory
networks that were introduced about
three years ago and that use a gating
mechanism which has come to be known as
a soft attention so the idea of soft
attention is that we're gonna use a
gating mechanism to decide on which
parts of the state or the input we're
gonna be focusing most so we can
actually take a linear combination of
say the content of a set of cells so we
can have a set of cells that have say
content CI at time T and we're gonna
form
now these gating things so so this is a
vector this is a scalar and now for each
eye each position in my memory we have
all these vectors at each position in
the memory we're gonna decide whether we
actually read something from this and we
can also write there or not so if the
gate is zero then we don't care about
what's going on if it's one then we do
and if we make those G's sum to one then
which is the typical way we're doing
this with the softmax then we're just
gonna have a soft selection of one
position and maybe other competing
positions we're going to take the
content of these and use them for the
next operation so now we can read from
the memory with a sort of soft pointer
that decides where to look and so why
you know why is that interesting and of
course you can do the same thing to
right
vice by doing an update at each position
proportional to those gating x' but I'm
not gonna write the math for that the
interesting so I'm gonna just show that
in in a picture so what is this picture
is supposed to be this is the state of
my memory which is now a bank of these
vectors at each time step and remember a
good way to understand what's going on
in recurrent Nets is to think about the
unfolded graph of computations so that's
what I'm doing here I'm thinking about
the comp the sequence of states now the
state is a complicated thing
oh now the meeting yeah I'm supposed to
do a movie so this state of the sequence
of states now is is partitioned into all
of these vectors and we're gonna have
this attention mechanism to decide where
to read and where to write in a soft way
so it's really reading and writing at
multiple places at the same time but you
know for the sake of mental
visualization we can think of it like
this there'll be some memory cells where
we're neither writing or reading and so
the content is just going to be copied
copied copied copied and that means that
gradients are going to flow through this
directly without any you know the in
that case the Jacobian is exactly 1 when
you're just ignoring those cells
sometimes the content of a cell is going
to be obtained by some transformation of
the content of another cell so we know
writing in some cells and we're gonna be
reading in some cells to you know write
in other cells or updating some cells
and so there's gonna be some complicated
graph of these readings and writings so
now you see what's going on we can store
something in this memory and until we
override it it's gonna just be copied
and so information can stay there and
and have an influence over a very long
time span so this is something really
important that I think we're only
starting to realize the potential of and
of the designing systems with very
long-term dependencies so as I mentioned
the attention mechanism itself does it
is a long history of earlier work but
there is a particular variant of it
using a soft attention that's called a
content-based and address based
attention which has really worked out
well especially in the machine
translation applications so imagine that
we are reading this sequence of words in
French and we're we're gonna be
translating that means we're gonna be
generating a sequence of words in
English and we've mapped the words into
feature vectors say using a by direct
bi-directional recurrent net so now at
each position in the French sentence we
have a vector that tells us about the
meaning in that area maybe the meaning
of that word in its context and we have
this other recurrent net which is going
to generate one word in English at a
time and when it generates the next word
in English would like to use an
attention mechanism so that instead of
looking at the whole sequence as an
aggregate the whole French sequence is
an aggregate it's gonna pay attention to
one or a few places in the input so if
you think about it when you're
translating of course it's not worth
word but it's it's approx word to word
right so there's like gonna be one word
or a couple of words that are really
gonna be dominating the meaning of the
new word that we're gonna be generating
for the translated sentence and so very
often in applications you see this that
in order to produce a next bit of
information we are really focusing on a
few bits of information from you know
the context or the source of the input
and in the case of translation it's very
obvious that this makes a lot of sense
what happens so this is interesting
historical story here we were we had a
paper and we were working with this
sequence of sequence we which we call
the encoder decoder architecture for
mapping a sequence say in French - a
sequence in English but the problem with
it was that was
now remember the architecture I showed
you before you you read the whole
sequence and now you have this vector
which is supposed to summarize
everything safe in the French sentence
and now you're gonna use that to
condition the generation of the English
sentence that works free if the
sequences is short like ten words 20
words but when it gets above 20 or 30
words it's really hard to train those
recurrent Nets to remember that much
stuff in detail it tends to drop off the
details because the summary you know is
this sort of summary is a summary so it
doesn't you know it loses details but so
so try to think of doing machine
translation this way translating a whole
book let's say I'm gonna read I'm gonna
read the deep learning book in English
and then have it all in my head and then
write the French version that's not does
it make any sense no so that's what
we're trying to do here whereas if you
use attention you're gonna say oh I'm
gonna have two pointers I'm gonna have a
pointer to the French version that I'm
creating and I'm moving one sentence at
a time and so on and I have a pointer to
the English version you know keeping
track of where I'm in the English
version mostly the English version is
gonna move you know one step at a time
sometimes we need to look back a little
bit but mostly those two pointers track
each other so having these two pointers
is really the key and that's what that
attention gives us so it's a very
powerful mechanism so thanks to this we
were able to reach the state of the art
in machine translation around 2014-2015
and and thanks to this and a bunch of
other tricks Google just a few months
ago released their new machine
translation system which you can now use
with Google Translate at least for a
number of sentence pairs that has really
amazing performances these numbers are a
bit confusing but but the summary is the
following I'm gonna use the whiteboard
again
so they have evaluated the machine
translation the neural machine
translation systems and compared it with
the traditional Engram based machine
translation by asking humans to rate
translations and so this is sort of the
quality of the translations
according to humans and on that scale
let's say this would be the quality of
the translations reached by humans so
that would be you can see that's the
column human so it's like about five or
something and there is the quality of
the translations that used to be done by
the traditional system which would be
say around two or three and there's the
quality the translations that are done
by this was the the old stuff and that's
the new stuff okay so whatever something
between four and five right say four
point five so it's a huge advance and
you can you can feel it if if you know
if you were used to doing machine
translation using Google Translate using
some of these language pairs and you try
them now
you'll see the difference I mean it's
it's much better okay so so this idea of
attention is used in many other settings
one example is the point two network
that came out just after this machine
translation thing by Alvin Yale and
we've been using this also in
translation so there's a problem in
translation where sometimes in the input
sequence we have words that are proper
nouns or names of rare rare words or
things like this names of places and
they might not be in your vocabulary so
this this sort of created a bit of a
problem
but it can easily be solved using in a
special kind of pointer and attention
mechanism and the idea is that each time
step instead of doing the normal softmax
over all the words in my vocabulary I
sort of extend my vocabulary to include
all the words that are in the input and
the way I can do that is I can say okay
so I have the choice when I do myself
max to either pick one of the words in
the vocabulary and I'm gonna computer
prodigy for each of them or to pick a
word in the input and then the way that
I'm going to choose the word in the
input is kind I'm gonna compute a score
for each word in the input I'm gonna
compute you know for each position now
I'm gonna I'm gonna decide whether it's
the right place to pick the answer for
the next the next word so you know in
between French and English it works
really well we just copy a word an input
into the output at the right time but
you could do more complicated processing
so let's say you're translating from
English to to some other alphabets a
Hindu you need to translate the letters
into a new alphabet and so you can do
that by inserting some special network
that learns to do this is called
transliteration right but the important
thing here is again this attention
mechanism can allow us to do things that
are really really practical in many
applications you can play games other
games with the architecture and try to
also understand theoretically what those
means so we've talked about skip
connections we've talked about
feed-forward connections and we talked
about how we care about the length of
paths so one interesting question is
what kind of architecture would give
rise to what kinds of lengths of paths
so so there's this sort of two effects
that we may be looking for one effect is
we want short paths to be present
because we want gradients to be able to
propagate more easily but the
if we have a deeper network in other
words a deep recurrent net with multiple
layers that's also advantageous because
now we have these feed forward path that
allow us to compute sort of more complex
results but if if we have if we have
connections like this and then
connections back from top to bottom
versus bottom to top actually the impact
is different in terms of the length of
the shortest path and the longest path
and so we did experiments where we
compare the two kinds of architectures
bottom-up versus top-down and and we
find that where one really works better
than the other and and the reason is
that in one case we can actually have
much longer paths which sometimes is
something desirable then then in the
other case so when you have the top down
you can actually have a fairly long path
that goes you know from the previous
time step up up up down up up up down up
up up down and so on so so it's actually
worth thinking about sometimes the
details of the architecture so yeah I
mentioned the unitary recurrent Nets you
can you can play games with the
architecture and various ways like force
those matrices to be orthogonal or to be
unitary you can also create skip
connections that are random so instead
of having these fixed skip connections
you can have various kinds of skip
connections that are stochastically you
know decided a little bit like in
dropout but here you you know you have
the connection or you don't
you can also inject in the recurrent net
architecture kinds of things that we
haven't talked at yet in the summer
school but you know latent variables so
the recurrent Nets don't really have to
be deterministic up to now I've talked
about you know everything being
deterministic but some of those nodes
could be stochastic as well
and the language of variational
auto-encoders and variational methods
could be applied to training those kinds
of recurrent Nets with latent variables
so years ago we had this variational
generative recurrent Nets and there's
been a lot of these kinds of variations
in the literature since then you can for
example we've use them in dialogue to
inject random choices at the high level
of the hierarchy and I'm not gonna
explain the details here because we
haven't talked yet about generative
models okay let me now talk about a few
related architectures so remember I
started the discussion by talking about
the decomposition of the joint into a
conditional as sort of one of the
hallmarks of the probabilistic
interpretation of recurrent Nets but
actually this has been around in other
settings than recurrent Nets and and
other kinds of neural nets in fact first
with sort of linear models this was in
Brandon phrase thesis in 1997 you have
the picture on the right hand side where
we decompose the joint into a product of
conditionals but here there's no
sequence it's just it's just at a pool
of random variables and we're gonna have
say a logistic regression of the I
variable or their teeth variable given
the previous ones and in this way we can
represent the Joint Distribution of a
couple of binary variables or you know
we can generalize that to any kind of
other random variables for which we can
have a a parametric distribution and
we're gonna make the parameters of that
distribution be computed by our network
at each
point in that sequence so one way to
visualize this is is here where we're
gonna have sort of just a unit without
input that's like a bias unit which
computes the quality of the first random
variable and then we're gonna have a
very simple network which predicts the
second one given the first one and then
another network which computes a third
one given the first two ones and so on
right so now we have we're gonna have
this sort of N squared matrix which with
some of the connections being left tough
so that only the past can be used to
predict the future and some in this
arbitrary ordering because now we're not
talking about a sequence we just have a
bunch of bits or a bunch of numbers but
we choose an ordering and then based on
an ordering we say we're gonna predict
the condition probability of the it--one
given i to 1 to I minus 1 right the
previous ones in nips 99 my brother and
I had a paper where we extend this this
idea to nil nets so now instead of
having these linear models we have a
sort of MLP one hidden layer MLP but
with a special structure in the
architecture so that the hidden units I
mean so that the overall graph maintains
this constraint that the output only
depends on the inputs from 1 to I minus
1 and we can do it in various ways but
one way is that we have these groups of
hidden units which only take inputs up
to you know the the 8th input and then
connect to I plus 1 or more so that was
actually our first foray into thinking
about neural Nets as a way to deal with
the curse of dimensionality in trying to
model joint distributions and more
recently hugo whom you heard yesterday
presented in 2011 a variant of this but
which we call auto regressive neural
auto regressive models which he called
the Nate neural auto regressive density
estimator in which there is a
a particular parameter ization which
allows us to compute these these
probabilities much more efficiently I'm
not going to go into the details and it
works quite well in terms of a density
estimator it can be generalized to
variables that of course are not just
binary but any discrete but also
continues there's a lot of variations of
this idea more recently this kind of
models these kinds of autoregressive
models that have been incredibly
successful as giant of models of images
and speech and so you can you can have
things like pixel RN ends which use the
this kind of iron and architecture but
to generate the eye pixel given the
previous ones
they're also versions that have a finite
context which are convolutional and and
these actually give rise to pretty nice
image generation kind of
state-of-the-art image generation okay
so this is my last slide one of the open
problems that I find really
mind-boggling is how would the brain do
so anything like back rub through time
hum with the brain what's the learning
algorithm in the brain that allows us to
learn about sequences and the reason I
find this mind-boggling is that it
doesn't seem very plausible that the
brain would use a strategy anything like
back prop through time so back up to
time is this you know we just unfold the
graph and we wait to the end and then we
compute the gradients as usual with the
chain rule and back prop right what's
the problem with that the problem with
that is that it requires us to store in
memory the whole sequence
hold it there look at the outcome and
then compute gradients in the reverse
time direction but it doesn't feel like
that's you know what's going on in our
brain like we don't redo our life
thousand times to see
we could have done it better right so
maybe you would say oh we don't learn
long-term dependencies that's the length
of our life maybe we do it on a day
basis maybe and so maybe like each night
we replay backwards the things that
we've done during the day as possible
but it doesn't feel really right so
maybe there's something else so let me
say a few words about another strategy
to compute the gradient which was
introduced in the late 80s called the
real-time recurrent learning RT RL but
what it is for people who know about
various forms of of gradient computation
it's just the forward mode the forward
evaluation of the gradient so in other
words as we are moving forward in time
we can compute the derivative of each of
the state variable with respect to each
of the parameters and each time we get a
costs we can use that immediately to
make an update so we can do online
learning we don't need to wait for the
end of the sequence so this looks very
cool right it's the kind of thing we
think the brain might be doing because
we do online learning we you know
something happens and and I don't know
about you but like something bad happens
to me and I somehow immediately try to
associate with the things that might
have done wrong in the past so you know
it sounds like something that could be
done by the brain unfortunately if you
take the equations for this to the
letter you end up with computations that
are not practical that are not
exponential but but polynomial in in a
fairly bad way that wouldn't scale with
something like the size of the brain or
even the size of the kinds of networks
we're trying to train for things like
speech and language so so this is still
an open question like how do we do
something like an online update that's
that's computationally efficient so
there are some there is some work
already in that direction I don't think
it really solves the problem yet but I
think it maybe indicates a good
direction where instead of trying to do
an exact gradient computation maybe
there exists some approximate gradient
estimators that statistically might be
unbiased or even if they're a bit by
maybe that's okay and what would give us
these kinds of online estimators so this
is still an open problem of course all
right so now I'm going to take questions
yes well anytime you can do
bi-directional versus single normal RNN
do bi-directional because you just get
more information so so sometimes you
can't because like in the case of when
you're generating an output sequence you
can't be bi-directional because you're
not supposed to look at the future that
you haven't generated yet it doesn't
make any sense
so it has to go left to right or right
to left but you can do both or at least
not in one pass so there are many places
where you can replace a normal we can't
met or lsdm gru whatever by a
bi-directional version of that that
could be so it's gonna depend on whether
there is a good solution to the problem
that's only causal looking only at the
past or actually it's useful to look
into the future yeah yeah yeah just
concatenate them or you can take the
concatenated into some further
processing like an MLP or whatever no
yeah yeah we don't add them up we
concatenate them exactly yeah
now the iron is not encoding the chain
rule it's trained using radians computed
by the chain rule the the backwards oh
the probabilistic chain rule yeah that's
what I'm saying you can't use the
bi-directional RNN when you're computing
the the probabilistic chain rule yes you
you don't need I mean you don't need and
you can't it doesn't make any sense
right but if your that's when you're
producing an output but when you're
reading an input or any anything you're
reading you could you could process that
as much as you want in any direction in
any way you want that's all you know
free game yes and this side yes please
yes well they're deterministic in their
state but the output is a random
variable that we can compute the
probability and sample from sorry I
didn't hear well so the question is
there theoretically proven advantages of
these variational schemes for our own
ends well we don't need the durational
schemes for the output side so you know
things I've talked about in the
beginning we don't need anything fancy
here we can directly compute the
probabilities the like yields because
these are not latent variables they're
observed so everything is easy simple no
need for a variational anything where
the variational tricks come in is if we
want some of the latent variables some
of the hidden units to be latent
variables if you Hannah if we if we want
to have Leighton
things inside then well one approach is
to have different parts of the RNN
compute posterior distributions for
these so that's like what we call the n
quarter part like an a very small 1/4
and then the rest of the RN n which uses
those things it's like a decoder if you
think about the duration or 1/4 I mean
there's no guarantee that this is gonna
buy us anything but the intuition is
that we would like some of the
randomness when you think about the rnns
adjourned of model like all the auto
regressive models the places where it
injects randomness is the input space is
the data space and one of the
motivations for deep giant of models is
that we'd like to think that a proper a
better way of generating instead of
directly painting the pixels one after
the other is to first think of oh what
is that I want to draw what are the main
objects what are the colors where are
they located and then sort of put all
these details after I've decided the big
picture so we first generate the high
level things and then conditioned on
that we generate lower-level things and
so on and so on so there we need latent
variables if we want to go this route
there's no guarantee that this is better
but intuitively it makes a lot of sense
and this is what has been motivating a
lot of the design of models including
the recurrent Nets with latent variables
yes in the back there
I don't hear you well sorry yes right
unfortunately no this slide is about
temporal dependencies and the
equilibrium propagation is about a
different kind of recurrent Nets which I
haven't talked about which is kind of
recurrent net you'd use to handle
recursive computation for a given input
so the kinds of recurrent Nets have been
talking about here are recurrent Nets
that handle a sequence whereas there's
another kind of recurrent net which is
also something we have in our brain
which is that given like current image
our brain is the dynamical system that
will sort of evolve over time with the
input fixed you know there's gonna be
some dynamics running and in the case of
the equilibrium propagation we actually
let those dynamics converge to a fixed
point so these kinds of networks
recurrent Nets they're called fixed
point recurrent Nets or whatever this is
sort of an orthogonal question so you
have the dynamics given a particular
input and then you have dynamics over
time with the time of the data so these
are like two different things and
equilibria per gate propagation is is
really looking at other dynamics within
one time step in the case of the feed
forward computation that we usually have
there is no dynamics I mean it's just
bang bang bang bang you get the output
but the brain has these feedback
connections and so it's not just bang
bang bang bang we get the output
actually you get that and then things
evolve to maybe a better configuration
which you can think of as a better
interpretation for the current input yes
maybe this is gonna be the last question
we talked about what vanishing gradient
yes so rez lets you skip connection this
is a very I think it's probably inspired
by the things that have been done with
recurrent Nets yes I think you missed
the slide yeah well I had one example
here no that's not it
sorry wrong picture but anyways yes this
was proposed in 1995 or even before to
have I think there's even a paper 1994
to use the skip connections through time
as a way to deal with the long term
dependencies issue and it's you know it
doesn't completely fix it but clearly it
helps so so this idea is really old and
it's you know it's these kinds of ideas
in recurrent eyes is probably influenced
there the resonance if if any I mean all
right so thank you very much and I guess
we can have our break now
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>