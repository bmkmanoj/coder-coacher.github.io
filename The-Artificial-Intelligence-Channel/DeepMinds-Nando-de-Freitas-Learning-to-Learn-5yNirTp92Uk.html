<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind's Nando de Freitas - Learning to Learn | Coder Coacher - Coaching Coders</title><meta content="DeepMind's Nando de Freitas - Learning to Learn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind's Nando de Freitas - Learning to Learn</b></h2><h5 class="post__date">2017-09-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5yNirTp92Uk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you and today is a special day
it's Canada Day and and I'm gonna
indulge and say a few words about that
because on Canada Day we also something
really special happened this year did
you see the news about your Shore how
about a big time
and it's very well-deserved over the
last 15 years or so
Yoshi has been one of the organizers of
these summer schools with yoshua with
young and many others have helped some
that are here c-max and so on
and many people who you know now are
sort of big names in deep learning
whether it's alex grace in Goodfellow or
you name it they were sitting there
where you are and right now learning
about rpms and all these other cool
things that deep learning was about and
that would never have been possible if
it hadn't been for all the hard work
that your show put into this with his
colleagues with Shep and so on so this
was a huge contribution I think to the
world of science and certainly to those
wanting to understand intelligence and
most of those people are actually away
in the United States in Britain so if
you go to deep mind it's full of
Canadians and so is Google and so on
we've given our them our talent we've
given them our startups for cheap we've
given them our IP and it we don't care
because we like to give because we're
Canadian
and we believe in the greater good and
we look to the future and and out of the
and it's paid out because those
organizations are coming back you know
those who have benefited from us are
coming back and helping us here and that
again would not have been possible
without initiatives like what you've
seen you're sure doing here in Montreal
that's been really amazing for the
Canadian economy because it's it's all
fine to serve for our institutions to be
supporting research in other countries
but it's also important for us to have a
sustainable basis here and that's only
the beginning because we've seen a lot
of stuff happening in our in Canada they
also we need to make sure that that's
sustainable for the future we need to
make sure that a community's benefit
from it that we don't just go into a
process of gentrification and I know
this is something that your show worries
a lot about and it's also going to be up
to you the next generation to protect
the same values that we've endorsed here
I've I remember many years ago we had a
dinner and and we were talking about AI
and so on and I was like it's all
hopeless because I have no faith in the
human kind you know I I I went through a
part I went through South American slums
I've seen the worst in people
but you shouldn't buy that he was like
there's good in people and if you
actually look at the global statistics
were much better today than I've ever
been and we just need to keep doing good
and and that's honestly over the last
few years I've known Yoshi has always
lived by that it's not gone into the big
millions and so on that he could have
easily got from corporations and so on
but he lives a simple life and he's
passionate about what he does and he's
passionate about doing good things for
other people whether it's plea for the
environment or for science or for
tolerance tolerance of religion but also
tolerance of atheism and he's always
been a champion for that and and I
really thank you for that because that
voice business
and lastly Syria yesterday I mentioned
that it's not all about competitions and
so on
competitions are important but I agree
that is about more than just
competitions
it's competitions are going to the
extent that they help us advance our
knowledge and the things that we can do
to help people and not just people our
planet in general and and many nowadays
this is obvious 15 years ago this wasn't
obvious 15 years ago good luck getting a
paper published on we're going to solve
intelligence or AI or whatever it is
that people claim they gonna do now and
it was very hard to sort of have those
dreams and to talk freely about those
dreams without being called anata and
and I remember being in one a sipper
meeting and this was a renewal I think
it's 2008 maybe it isn't and we were
talking about what was going to be the
next big project and it had to be
something big something big about AI
that was going to capture the
imagination of people and so on and and
few people had opinions there was we
were not clear I remember I thought I
had a really clever idea I mentioned in
the meeting we should build self-driving
cars and that was obvious at the time
already so it wasn't such a great idea
but I thought you know I quoted some
figures 1.2 million in the people in the
world died because of car accidents and
it's atrocious that we're not stopping
this from happening so this would be a
good thing to do and also it allows us
to extend what we can do with the eye
and I still remember Jeff was standing
that in the corner and then he answered
he looks at me and says why don't people
take public transport
and then he and Yahshua proceeded to
then say no it's not about this
application or that application and it's
okay applications are important and of
course these things are useful because
after all that most of those people who
died in car accidents are the
pedestrians so we should protect them
and a is given us a lot of tools and
we'll continue giving us a lot of calls
to cool tools for healthcare and so on
but then their plea was like we need to
study intelligence that is one of the
greatest questions to actually
understand what makes us what we are and
not just we as individuals but we as
communities how culture how the
intelligence is not something that there
arises isn't a single individual it's
it's a communal thing how how does that
process work whatever all the mysteries
how do we manage to sort of manipulate
objects how do we these things are
beyond what any of our machines are
capable of doing and at that time they
they were very assertive they say we're
not going to do any of these other
things we are going to go for this we
are going to go to understand I think at
the time the brain and and and so be it
that's where it became we decided to
understand the brain and we wrote many
papers on RBMS and we stuck them and we
made them deeper and they didn't work
and then we had many workshops like this
one where we learned about GPUs and some
of the people are in the audience the
the Alex Cruz air skis and so on they
learned about this and and then after
2012 I think more or less 2010 12 with
the successors and speech and image net
the rest was history then the world
realized the cool things that would have
happened lots of cool stuff and so I won
last time let's congratulate Joshua for
the other camera
okay so on to my topic for today so I'm
going to take a slightly different
approach and we've seen some really cool
takes on this thinking about problems a
statistical summation instead of the
optimization that's really important in
science that we got yesterday also about
understanding the neuroscience behind
deep learning and how deep learning can
contribute in your neuroscience and
vice-versa we learned a lot about
language and I'm sure you had I wasn't
here before but I'm sure you learned a
lot of cool stuff about generative
models and so on
today I'm gonna go on about a topic that
is for me arises from these following
questions it arises from trying to
understand what it tastes or machines
can and cannot do yet that we would
think of as intelligent behavior or
intelligent capacity I'm always looking
for what is missing what is that thing
that our machines could do so yes it's
about competition as an benchmarks but
it's not just about doing well at a
competition even though that's important
that improves our methods and algorithms
and so on but it's also about deciding
what is the competition what is the next
challenge how can we keep making a field
go forward and and that's a very hard
question because you need to set a goal
that is reachable if we set a to harder
goal then this community will sort of
fail at it so that actually is one of
the greatest challenges for us as
researchers and it's one of the greatest
challenges for a eyes is deciding what
we're going to learn next
so I'm gonna start with a video of my
daughter Tala she's two now this is when
she was one I wish I had sound let's see
amazing no machine that we've ever
developed a no machine that we have
insight that we think
we'll be able to develop over the next
couple of years is capable of this and
I'm gonna try to break this down into
what toddlers doing here
she's able to reason about objects she
understands where objects are she's able
to reason about geometry how to put
things and together
she obviously standing she knows how to
stand she knows how to move their hands
she needs she knows all about contact
forces and manipulation friction how to
sense multimodal interaction with the
world and that why when she succeeds
that that intrinsic motivation that that
the same motivation that I think the
reason why you guys are here it's like
that that's getting knowledge and we
don't really understand this process
really well yet how it is that we why it
is that we've evolved to seek knowledge
but this is what also drives us to do
what we do another thing I wanted to
show it where does it happen
okay so I'll play one last time she
tastes and that's very interesting that
she tastes the blocks in hours and she's
getting an understanding of what this
thing is about respect there's a few
people here who speak Spanish I was so
there's the word which when I was a kid
with severe yes a and you confuse to the
the past tense of those two verbs of
tasting and knowing and there's a common
thing and that's because and that's true
of other sort of in the European
languages knowledge is to taste so
seeking to sense the world to have a
better understanding of the world is
something that is akin to searching for
knowledge now Tyler was one when she did
all this yeah
turned boy and so one a sauce how was
she able to in that one year learn all
of this stuff and and perhaps she didn't
learn all of the stuff within a year but
perhaps who she was born and she
definitely was born with a brain and
that has a lot of structure that allowed
her to serve get started she was born
with the ability to learn now I'd like
to argue that evolution endowed Tala
with and I'm thinking about evolution is
a learning process that then gave Tala
the ability to learn so there is this
process where learning happens at
multiple time scales and you learn the
ability to continue to learn at a
different time scale and this is not the
only time scales and by the way I
strongly recommend you read papers by
Elizabeth's Pelkey if you're interested
in in this type of thing where she looks
at what are the sort of things that
babies are born with in order to be able
to best understand how they come to
understand what an object is and so on
let's look at a different time scale so
this is an experiment that Harlow did in
1949 and in this experiment
Harlow would show a monkey two objects
to visually distinct and behind those
objects was a reward so in this case the
the monkey chooses the the beer and gets
a reward the next time step the monkey
chooses wine and it doesn't get reward
and then it chooses this other object
again and it gets reward so that's one
episode and then this gets repeated with
two different objects and
the monkey chooses wisely first and gets
a reward it gets a reward but then I
guess it's evening time the monkey
chooses wrongly and doesn't get a reward
another trial happens and the monkey
decides to choose the beer and sort of
makes sense it has to be before but it
doesn't get a reward and then it chooses
the wine and it gets a reward so gee
what do you think will the monkey get a
reward how many people think the monkey
will get a reward next you're the beer
drinker so the rest of how many people
actually think it won't get a reward
okay yeah most of you are awake so in
indeed it won't get a reward and what
what am i what monkey's actually learned
that you just learned three examples
although monkeys usually go through many
many thousands of trials to learn this
is that they learned that the task is
really about you have to identify you
have to once you pick an object that
same object will always give you a
reward
Eirik regardless of the object so the
monkeys actually through many trials
they learned this they learn how to
choose they learn to solve these bandit
problems by by doing multiple trials so
that the time scale of experiments the
monkeys have learned how to resolve a
particular problem when they encounter
two objects they try one if they don't
get reward from their own they know they
have to switch to the other one and
they'll get reward
so I haven't done this experiment myself
but I recommend you look at the paper by
Jane Wang one of my colleagues a deep
mind and I'm pretty sure so they're they
consider and I'm gonna come to this as
well sort of bandit problems where there
is uncertainty and so that the rewards
will be stochastic but really what I'm
trying to sort of say here is learning
to learn happen many scales
okay okay and if you're following what
this crow is doing you're seeing how it
sort of learns to in order to get the
food in the tube it learns to throw
stones in in the tube and this one is
really cool it has to choose between
getting two kinds of food in one case
it's obvious that it can throw stones
and get the stuff in the other case it's
obvious that it can't and so it changes
tasks to do this other thing
so these Caledonian crows are capable of
making tools and and I didn't say using
tools but making tools they learned this
by the age there one and they learned
this from they learned this from there
the other crows so they learn it from
their community so they're capable
whoops
ah how do I stop this day I get rid of
the distracting sound at least they're
capable of learning from their community
how to do things so they learn to
imitate their elder crows and and just
to tell you how remarkable this is
humans learn to make tools at a much
later age so the crows are able to do
this and and this gets passed from
generation to generation so this is not
a again an ability that they will just
learn once but they actually really rely
on this sort of different time scale of
learning which comes through their
culture and here is another problem that
is become very popular machine learning
that it is the problem of few short or
one-shot learning where you're given it
a sample of a class for example like a
an omni cloth digit as Brendan like
dozen but rate Brendan like and Josh
Tenenbaum have a paper about human
intelligence and contrasting community
so for animal intelligence with
artificial intelligence I strongly
recommend you look at it they post some
really good challenges for all of us
there and so one is this that we if we
look at one character we're able to
easily go and draw other characters or
we might see some object and we can sort
of imagine this object in different sort
of poses and slightly different shape
and one of the questions one of the
challenges that posed is how can your
own networks learn just from a few data
because after all when we try our neural
networks we have a lot of data I mean
everyone keeps saying we need bigger
data sets to train the models but we
also need to understand how we could use
neural networks to learn from very few
data to be able to replicate this type
of behavior I would argue that learning
to learn and I will present in fact some
examples by a few phone
spy at Adam Santoro and later by ooga La
Rochelle who actually showed that if you
have a model that is capable of learning
how to use data a test time then it
basically it learns how to give in a
small data set quickly learn to be able
to provide you with an answer now I've
done some work in learning to learn over
the last few years I've been very
excited about this topic because I think
it sort of answer some of the big
questions that we have is you know it's
talking about development in animals
it's talking about how we can learn from
few data I think it is an important
topic is one one that we need to
understand well we do not understand it
well yet and I'm gonna give you some
examples mostly biased to my own work
and they sort of give you some
understanding of this and how we might
be able to do it
but by far the I think this is still an
open problem and there is there aren't
killer applications out there like
commnets and so on but but I think it's
an important problem that we need to
sort of work hard at until we either
find that it's a complete wrong course
of action or until we use this theory to
be able to understand better again
development and in learning with few
date now the way I'm gonna the the
problem that I'm going to look at is how
can a neural network control another
neural network or like in particular how
can a neural network learn be used to
learn another neural network acting say
as an optimizer or in fact it could be
an MCMC sampling zone to be a network
providing samples for another network
there's many variants of this work many
many researchers have worked on this Max
has worked in this Yosha has a beautiful
paper with his brother actually not one
paper he has like four or five papers on
this topic learning to learn where he
was trying to learn
supervised learning rules with other
types of algorithms I think those papers
papers of the time early 90s and should
be revisited because I think it's time
to give it a modern treatment so for
people looking for topics I strongly
recommend you look for the Benjamin
Benjamin ein 2 &amp;amp; 94 on this the problem
comes in many forms there are folks that
use one neural network to generate the
parameters of the architecture for
another network in program induction the
parameters determine the behavior of the
network if the network is a recurrent
Network essentially implementing
programs and so this is often known as
programmable neural networks it has a
long history it's in the paper of
rumelhart
McClellan and Hinton so one page only
but the idea without experiments but the
idea is there and many others have sort
of thought about this problem it's been
reincarnated in recent years as well in
the deep learning community where people
serve again trying to understand how to
one network and set the parameters of
another network and I think there was
someone already mentioned that here
control
how can a network sort of control the
behavior of other networks so then we
get into things like a hierarchical
reinforcement learning or gating that
again is a neural network choosing if
either the activations or choosing the
memory or choosing a bias of another
network and through that being able to
control another network and then of
course that is this question of how do
you learn one algorithm with another
algorithm and there's many variants and
I've called this learning to learn X by
Y where x and y are two references to
variables and you can plug in your
favorite words here so we've done SGD by
SGD you could learn and supervised
learning as Yoshi has done with
supervised techniques and this actually
is very interesting for
for neuroscientists I think so we saw
these talks about neuroscience and how
we're trying to figure out alternative
to tobacco application I think this
could say a lot about that because you
might be able to instead of you hard
coding with that unsupervised learning
algorithm or this you might yeah I
recommend and in fact when I looked at
that you also looked looked at simulated
annealing and so on you basically
through all your your why was everything
you had available to you at the time and
your expertise set of alternatives to
heavier learning I think we need to sort
of revisit that nonetheless with modern
tools because we our Y has expanded and
our X and we know indeed indeed
I'll come to that and so and you could
apply this also to inference as well and
I think there's a few papers have
already looked at this so how do you
learn sampling had you learned for
example line Siobhan Montecarlo
algorithms with another algorithm so
what we did was simple in in a way we
just basically noticed that when you do
when you do have a learning rule
typically you have this update where you
have a network of parameter safe pizza
and your ad you take a step in one way
or the other in the duration of the
gradient and then what people do is they
spend a lot of time trying to come up
with a transformation of these gradients
whether it's rmsprop or Adam until they
get better results on em nest and
eventually they test on bigger data sets
or others in the community do in and
through the community we come up with
better algorithms but the process of
designing such algorithms is sort of
it's it's it's basically feature
engineering just like we with deepness
we like to say let's stop doing the
feature engineering has learn the
features engineering optimizers is still
feature engineering and so perhaps we
could aim to actually learn these autumn
optimizers automatically now so that's a
dream and that was a work that was done
I did mine with my colleagues Marcin who
had who is now at open AI Misha Daniel
who still works the rest are all at it
mind Misha daniil Sergio Matt Hoffman
David file Tom shawl and Brendan
Schilling Forge in machine so we go to
tents the world of tensorflow
and the way this was engineered and not
easily because for any of you who know
tensorflow if you're gonna modify the
learning algorithms there's a lot of
hard work and Matt Hoffman in surgery
actually spent a law long amount of time
working on the software and they
actually put a software online it's one
of the most downloaded pieces of code I
think and I I really recommend it
because they'd a really good software
engineers and they put a lot of effort
into it and so what their modules do is
essentially you have two networks an
optimizer and what we called this new
word that we coined in English the
optimizer
the thing that gets optimized by the
optimizer they're both networks the
optimizer could be say a combat so f
implements a continent and it gets a
loss they supervise loss and then you
have this other network which is say a
recurrent network that has some internal
state and it generates the gradients for
this other thing so it provides this
network with parameters theta with the
update step and then this network itself
is a dynamical process that's unrolling
over time so the new gradients are being
generated by some network and this
network might look at some information
from this other network like gradients
and so on but instead of looking at
gradients you should also consider and
this I'll leave as a homework exercise
to take everything that you just saw
yesterday as alternatives to gradients
and learning rules and plug them in here
and see if you can get this to work so
that's more or less the the idea of the
approach is use a neural network to
adjust the parameters of a neural
network and then you just generate one
big network so the optimizer and the
thing that you're optimizing is a single
network there is no longer the algorithm
in the model it's all a single network
oh battery battery's dying I think
not this one the other one
no what's it
test all right yeah deep learning
consumes a lot of power we need to make
it more sustainable oh where was I so
yeah so the whole thing is just a single
network just like the brain the single
network affected by environmental
influences and it's a dynamical process
surf moving it has dynamics and in fact
the brains with other brains and working
together also dynamic process and so
just as the same way that we look at the
universe as this process that we're on
space and time I think that's a good way
to be thinking about mind and brains
the particular instance that we chose
here was a coordinate wise optimizer we
you sort of take a parameter you pass it
through the corner to think that the
optimize see your return you get a
gradient you plug into an LS DM and it
gives the update the update gets out it
to the previous value of the parameter
and then you go back and you generate a
new parameter and the reason why we
chose to apply just one parameter at a
time in this particular instance is
because we wanted to to be able to learn
to transfer two models of different
sizes and so on to be able to transfer
the optimize so one thing that we were
interested was in you learn disjoint
network together and then you sort of
cut the optimizer and move the optimizer
to optimize other functions and we
wanted to test how far could we push
this and so the first thing we tried was
mm missed like everyone else and we
trained with to a small network with 20
units in the hidden layer and we
observed that the optimizer that we were
learning was doing better there's some
very respectable competitors like Adam
Hermes professed Gd an ester of
adaptive gradient and then we tried it
on a network with more units with 40
units and it still was working so this
was exciting and we were doing this with
100 optimization steps so here we need
to enroll the lsdm for 100 steps when we
do training and then we went to 200
steps because then there is this
question if you were to unroll it
further what would happen and so they
seem to be working still two layers boom
here we were so happy we're like that's
it we're going on for the next big award
and then this happens we went too far
outside the model class for which we had
trained we're doing these transfer
experiments and eventually we figure out
how to break it also good for
experimental methodology don't just go
and publish the results of work and try
to find the ones try to always see where
you break it yeah thank you for the
question yes I forgot so the question
was what did I train it with 10 10 H
units and so it didn't know how to deal
with railway units the paper has more
experiments than we go and study how you
do relative tange and so on
but I'm gonna skip that because I think
we have now different ways of doing this
that I work better and then we did the
same experiment for other datasets so we
didn't try this on safer and in
particular we were looking at transfer
where we're still using the same network
but we sort of learn with five labels
and then we go a test time we try to
learn five different labels or we learn
with two and then try to learn with
eight and again we were doing pretty
well in comparison to these other
competitors and then we tried near a
large so most of you probably know this
near
work where you sort of have a painting
and you have a photo and then the ideas
you want your photo to look like the
painting and so this involves who are
the authors of that paper the knurl
octavius matthias me yeah so I should
have put a citation to their work and so
we're using exactly what they do and but
that involves doing an optimization
process basically in as part of I won't
have time to go into their method but
you do need to require you require to do
some optimization to come up with this
image and here we were able to do that
optimization with these methods and we
were able to give a new contents or new
images and and be able to sort of
generate this for a particular style on
a particular image size and even more
exciting we were able to try completely
different styles and content and sizes
of images and it's the word so we had
learned an optimizer that could transfer
for this particular application and
Annie was doing better than the first
order methods that we tried but there
were some challenges left and we
realized this by doing more experiments
and so we didn't stop there with them so
we got our paper and so on and but we
continued trying to push the work and we
found there were some challenges in
trying to and in particular we wanted to
scale this you know this is only it's so
cute to do different and data sets on
but we want to do like the big things we
were going to do image net we wanted we
want to train you know at the time we
were ambitious we wanted to train the
whole of Google products with this stuff
and you know and you have to have that
sort of ambition because eventually you
sort of hit the bar you have to push as
much it's much further as possible and
in particular with scaling we do realize
that still I think there's one of the
biggest challenges is how we scale our
networks
and what we found was that because we're
using this LS TM optimizers and even
though they share parameters while
updating a single parameter of the
optimized e they have a lot of
activations and because I have a lot of
activations they you need storage in the
GPU for those activations so now all of
a sudden you have to you have a trade of
if you're training a resonant model
because you're running out of memory in
the GPU you need to make the ResNet
model in order to make room for the
optimizer which is also neural network
and so there's a difficult trade-off to
to deal with in practice because we we
want to optimize it to be as big as
possible at least in supervised learning
and control is a different story and
I'll come to that next week and the
other issue is all the optimizing
horizons that I have described so far
they're fairly small because you don't
need to optimize so many steps to do
well on em nice and so on but if you go
to image net you do need to optimize for
many steps and so you need to understand
realization has to happen not just a 200
set but it has to happen to do thousands
of steps and of course there were the
Rev was still and and there's another
challenge with this is these experiments
are really expensive ok we're learning
to learn and the first learning as I
argued before that's evolution evolution
has endowed Tala to learn to put blocks
together a better than any existing
machine but that processor over of
evolution was very expensive and I'm not
sure we can actually sidestep this by
going this approach by going this way
and and I have no answers for this
particular question yet but yah sure
seems to
learning which is paralyzed yes so
that's what I would say is indeed not
only that but we need to understand
evolution properly so for a long time
there's these papers on evolution this
good work very good work out there in
evolution but there's also many papers
where people just run things in parallel
and they call it evolution or they do
you know and and and I can feel like we
that met optimizer we still don't
understand it really well and it
probably would help to understand it
better to at least to be able to invent
this kind of research I mean there might
be recent so so the questions why focus
on read loss and in particulars reasons
for I maybe Bareilles are only used
because the optimization is hard and so
on I still think some people would use
rather because you know they think of
they have some biological considerations
for example as to why a relative might
make sense but also highlighted rellis
here but what I really mean is it has to
work for everything not just relased but
the rel is the one thing that if I've
shown you that it fails but it could
have failed that many other things and
so I'd like to design optimizes that are
capable of transferring and solving many
things and so we go into scaling and
generalizing and I'll go quickly over
this and this was mostly the work of
researchers
in Mountain View in particularly Olga
she cause she worked so hard in this and
to put a lot of effort together with an
intern at Google brain Nehru who is
we've already seen a citation to him
yesterday one of your students I think
as well as Josh's show Dixon who led
this project and so we teamed up with
deep mine teamed up with brain to be
able to sort of address this issue of
scalability and there were many things
we tried in doing this and the work will
be presented at did mine for a new few
who want to see more about it so there
were several things we decided to do
differently so first we decided that
instead of just trade this idea of
training on one data set and still
testing on that data set Josh I had this
idea of let's train on lots of different
data sets let's train on simple black
box optimization functions and random
functions and generate random data and
fit logistic function to it and so on
so a huge range of training sets they
were small he also randomized the length
of the training and so on so the idea is
let's try to sort have a lot of
variation in this dataset
so hopefully the optimizer learns to
deal with all this variation he also
introduced a hierarchical lsdm that it
requires less parameters and less
activation so it's a bit more efficient
and also made a design choice of let's
not just like completely obviate all the
insights that we have gained from the
theory of optimization but let's use
some of these ideas I mean we know that
normalization and so on helps so we
shouldn't ignore it let's just feed in
some of this stuff in and and and then
also you know by training with different
lengths as I mentioned earlier and the
result of doing this was the following
so let's look at the rail or example
here again and and let's say also look
at optimization over many steps and now
we're starting to look at a hundred
thousand steps and once again if we
actually look at this inset here so we
magnify we see that the new sort of
approach is still able to do comparable
to Adam and it's able to actually do
better than rmsprop in this case the old
approach that I've been showing you
before with rail or completely diverges
as we saw earlier but when we train this
with you know confidence on em nest
perceptions on em nest the learned
optimizer still does a very good job and
eventually serve as symptoms but it even
has a faster rate of convergence at the
beginning than optimizes that we used it
regularly yes yeah we do
one of going explained forget to back
drop now I do have some lectures online
so Google deep learning then the de
freitas Oxford truncated back pro so we
are enrolling the LSD err oh I'm not
gonna have time to cover back
propagation through time and truncating
it but basically as you go further back
if you compute the gradients often you
have to stop from going too many steps
and that's pretty much it and there was
one more thing I wanted to say about
this picture
oh I highlights again it's doing well on
this component this was not trained in
your own networks these optimizers were
trained on
random functions logistic functions
functions from optimize the optimization
literature so this is complete transfer
to deep learning from classes of
problems and then all go struggle for a
long time but you managed to eventually
get results for ResNet sand inception
models and it's still this is kind of
where we are right now so the learned
optimizes they still don't do as well as
the competitors but it's quite
remarkable that we are now we are
scaling we're able to run these
optimizes on the sort of
state-of-the-art models and not
necessarily the worse so we have made
progress and I think we're so short of a
few ideas and that hopefully will
eventually take us to actually beat the
other optimizes it might also be very
hard to beat the other optimizes because
the other optimizes are things that
we've been working on for years and
using very smart brains at it but this
certainly I think shows that learning to
learn can scale and can happen
yes yeah good question thank you
hyper parameters so with the training
with L STM's and so on initially during
training you do come up as the usual
hyper parameters for training these
methods and in fact a training time we
use atoms so we sort of learning to
learn by atoms so there's actually three
levels of learning a I guess but once
we've trained the optimizer and we
remove it from the big neural network
and we're using this optimize a neural
network that optimize a neural network
has no hyper parameters so unlike the
other techniques which you have to tune
to the data and you have to figure out
the hyper parameters this one has no
hyper parameters at all so this issue
that we were talking about if doing have
a parameter sweeps and so on that that's
gone
you're definitely right and I'll have a
slide on that and instead of repeating
I'm gonna bring that slide so it's about
being able to do feel short optimization
by thinking of it as in in the process
of learning to learn I've shown you some
well you saw what happened with the
Rallos before
I'm sure this optimizer I'm sure I'm
sure if we push it really hard we
eventually will get failures there's no
doubt of that
so and that I think is true of most of
even though optimizes that we hand
engineer you can construct objective
functions a sort of break are no
continuty and so on and eventually you
can sort of get them to break before the
things that all gain nearer and Josh and
Matt and Sarah have been testing they
seem to be working reasonably well and
so the problems that we've tried are the
same problems that everyone is trying
and in particularly is this type of
thing so one thing I haven't talked
about is RL so actually that could be a
good example where if you if you deal
with a lot of very if there's a lot of
variance we haven't quite addressed that
problem that would be worth studying
yeah so it's we sort of training on a
bunch of types of loss functions and
then we're testing it on the type of
loss functions that arise and when
machine learning like in particular sort
of their type of losses are too deep
that we use in deep learning it's
possible that we you could come up with
other losses with discontinuities or
with a lot of noise where this could
break down but then the solution would
be you would have to train in those
losses as well
yes you could do that homework exercise
I'm gonna give you another one other
example of this and there's been a lot
of mention of hyper parameter
optimization so and I'm assuming that
Michael Osman talked about GPS and based
optimization yeah okay so you kind of
seen what it's like you sort of have
basically it goes like this you have
only two data points and there might be
some true function which you don't know
which is this dashed function here and
then you have to decide where do you
pick the next point to learn the
function so it's problem of active
learning where you're trying to learn
where is the maximum of this function
you're looking for this maximum here or
the arc max here and then what you do is
you do your trade of exploitation
exploration so ideally you would like to
pick something that's near at this point
next because this point had a higher
value but you also need to take into
account that there is your model has
uncertainty and as a consequence of your
model having a certainty there is also
away from the data you always observe
this there's there's uncertainty there
and you to make decisions you to have to
take into account its uncertainty now in
this case this is almost a terminus --tx
away you sample you actually get a
perfect observation of the function but
you might also have noise in the
observations so that's the model
uncertainty that we use your model in
deep learning and this would be the sort
of pace in uncertainty then certain
about your model know your parameters
and you need to trade off so you want to
look in areas where you haven't looked
but you also sort of need to look in
areas that where you know you've done
well and based optimization so has many
ways of trading this off so you might
say pick a next you might have some
function which we call their position
function
which we engineer to trade-off between
the sort of exploitation exploration
this function tells you where to look
next
then we conduct an optimization of this
function to pick the next point and and
I'm assuming Michael would have told you
about probability of improvement
expected improvement entropy search
Thompson sampling the million ways in
which you can do this and so on until
you get to the optimum and this is a
very useful technique for tuning the
hyper parameters of neural networks it
also has some challenges of how to get
this done in practice what we were
looking at this and we're like well can
we just replace that GP process and the
process of having job to Mai's a
surrogate just by using a recurrent net
let the recurrent net learn to pick the
next point and let the recurrent net
learn to do active learning to learn to
the optimization so we decided to do the
following
we sample and here we we knew that GP
sort of work well so let's construct a
very large class with different kernels
of GPS and let's just sample many
functions from the GP so this is sort of
related to the previous question and for
those functions that you sample we we
can run the GP we can run as sort of the
the Beijing optimization and we can also
train an RNN because we you know the
values X and the function values and so
you can have an RNN that has some
internal state looks at the previous
values the previous input and the reward
that you got and decides what the next X
should be and then you evaluate the
function get a loss and after several
steps you aggregate all these losses and
you back propagate and you learn
basically the RN end that is choosing
the next point and so this work was done
by Eugene gent who is PhD of Max Matt
Hoffman Sir Hugo MERS Misha Denis of the
same people that were involved in the
other Pro
check and Tim Lily crap man poor Finnick
who are my colleagues a tip money
working mostly in neuroscience and
Eugene also came up with this idea of
doing this in parallel because when you
optimize neural networks you don't want
to wait too long
for the answer to come back before you
try you'd rather model especially at
Google we he'll launch many of these
jobs in parallel you really can evaluate
the function in many machines and so it
would be nice to be able to take the
previous observations but we also
introduce a variable that indicates
whether they're still resources
available such a binary variable or
whether we there are no resources
available and with this we're able to
actually do this in parallel so as soon
as the machine stops and gives us an
evaluation we're able to incorporate it
into the learning process and we sort of
reuse that machine and one last thing we
did is instead of using a less TMS in
this project we found the the
differential neural computer so what was
used to be called the neural Turing
machines a variant of the neural Turing
machines to work better and so there's
many ways in which you can do this and
all of these different ways will sort of
be using these different acquisition
functions if we use them to learn as
surrogates will be using doing sort of
different trade-offs between exploration
exploitation well the one that we found
our the best was to just use this thing
observation improvement which is a sort
of a sample version of the expected
improvement that's so what I'm showing
is this is a function that we're
optimizing and this is just the
parameters that the optimizer is
training so you sort of seeing a run of
what is trading many things and how it's
sort of converging to the optimal Thanks
so he first tested this on GP function
samples from GP and of course they're
the methods that were designed with GPS
like in particular things like spearmint
which learns the hyper parameters or
integrates out hyper parameters with HMS
with sly sampling one experiment with
fixed hyper parameters we also looked at
things like TPA smack and so on every
phone that we sort of we're doing
comparable to those techniques in
horizons of length 20 sorry left 100 for
all these experiments we assume that the
setup is you're going to have a hundred
steps to reach your decision and so this
is very common like in like in web
applications you can you can show
customers as a hundred steps and after
that it becomes too costly so you have a
finite horizon in which you can try
things to decide what is best to deliver
though the experiment or morning
interestingly if you're doing clinical
trials you can just do so many trials
either because of whatever resources you
might have economic of humanitarian
before you can actually choose the drug
that you're going to use and we then
moved on to try this for the typical
global optimization functions of people
use and then we're starting to see that
these versions of the DN C and green and
red are doing better for the particular
horizon which we train then the tools
that folks use out there and this these
are tools that are very popular they're
well developed packages so this is again
proof that learning to learn can do a
good job and of course the optimizers
here after you learn them they don't
have any hyper parameters so anyone can
use them and they're much faster rnas
are much faster than you having to do an
optimization like when you maximize
expected improvement
yeah so that's a good question so but so
what we did is we we trained in just
many functions of GPS and so on because
it sort of made sense for us and then
really the rest of these lines what they
show is the transfer to different other
problems so so now it this is showing
that it can transfer to these benchmarks
in optimization you know these simple
functions in 2d or these functions in 60
and these are sort of these sort of
optimization problems that have many
minim and so on that's sort of
constructed adversarially by folks who
work in this area and it's still able to
transfer the same this is the same RNN
we're deploying the same RNN for
different problems we learned at once
then we use it for all this
it worked here we tried it in a control
problem it worked well we tried it on
optimizing a neuron machine learning
architectures this is here we follow the
same examples as an experiment paper and
we're again doing better than even
Spearman to some of the tasks just like
of course we have to do better because
you know we're coming after them but we
but nonetheless is showing that learning
you can learn an RNN that is capable of
solving this optimization problems tune
the online in LD a tune SVM's tune
logistic regression and in fact even can
be used to tune resonate and so on so
you could use a recurrent net to do the
the meta tuning of your of your network
here you can see that the Spearman still
does a bit better job in in in trying to
optimize a resonate and of course as I
said in the beginning we still not yet
them but we are very close and we need
to keep working on this and and we're
short of some my ideas and I hope that
it's going to be Udo will go and figure
out some of these ideas if we don't get
to them
mm-hmm
that's a very good question so instead
of using Adam to learn the RNN could I
use learn a network that learns that
network that learns another network that
learns another network that is to be
done
and that's and that's my last slide we
did try here because the DNC itself had
some hyper parameters so we did try to
use this learned optimizer to learn the
hyper parameters of a DNC that would
then be used as an optimize I know you
can try that I don't remember how I lit
it but but actually I think there is
something there
this process of recursion I think it's
not just a joke in it but it's there
might be unlearning to learn with
multiple recursion yeah so I think
there's good computational reasons for
this so if there was a really good paper
that I cleared on using recursion but
that was for the neural programmer
interpreters where instead of training
it with say to do for loops that do
recursion and as a consequence you end
up with much shorter programs and then
you're able to then show things that are
remarkable that a neural network trained
to learn to sort sequences of say length
20 can actually sort sequences of
arbitrary length you can generalize
indefinitely so in a symbolic sense it
has is a capable of doing that sort of
thing and that's only possible through
recursion moreover the authors in that
paper prove they actually prove that it
will generalize for a Sikhism because
the problems are small enough you can
actually do verification so yeah there
is something there that I think is very
important
now let's look at some other what I
think of some other interesting works in
this area one of them is there's this
paper in our chronic by Jane wank that I
already alluded to on learning
reinforcement learning she actually uses
reinforcement learning to learn things
like bandits or in fact you do learn to
do the harlow tasks and learning to
navigate riah hassle and a few of my
other colleagues that if mind also have
a paper where they sort of do something
a navigation that is sort of learning to
learn and that's very similar to a paper
by Berkley opene I called RL square RL
to the power two they also try to do
this sort of thing learning to learn one
interesting paper the sort of was very
popular in social media in song with
this paper of Barrett's often courtly
where what they do is they just they
still have an RNN controller as they
call it generating the structure for
another network and so what they do for
example for confidence is they have an
RNN that chooses the number of filters
to filter hides the filter with strides
and so on and then moves on to the next
layer
and because it's choosing these discrete
variables so this is like a grammar for
generating the networks and so if they
train this with reinforced this'll SDM
will choose the basically the the
architecture of the other neural network
and then you run the other neural
network on data and see how it does and
that's your reward and then you do
repeat this with reinforce this is
extremely expensive
I've I've heard some stories about how
much energy did this experiment consumed
and it's it's portable I the sort of
thing you shouldn't do but it's really
sort of challenging it's challenging how
we might be able to in fact have a
network that completely to learn
a new network okay so you this is all of
you should be worried about this because
this could produce a lot of the job the
intention is to have networks that
invent new networks we're not yet there
but I think it's it's a very interesting
exploration still what it's capable of
doing it's it's nothing but just sort of
it's not yet capable of it the kind of
invention that we humans are capable of
another very cool work and this is
related to the few short learning
question that you asked is this work by
such in Ravi and new Galera shell where
they have a metal urn again and they
have some optimizer which is a confident
in their case which takes some data and
it might take so the data will be say
for example five images with labels and
then there's a test and you want to and
just from two images so when you use a
test time you'll see two images in you
and a generator label so the training
set is only five images so you want to
be able to train so you wanna you want
to have many examples of this type of
problem where you look at five images
and the labels and then given two images
of the tests that you need to generate
the label and so the way they do this is
they feed and here the index is time so
this is capital T iterations X&amp;amp;Y is
basically a thing that looks like this
five images with five labels and you
feed them in and with a setting of the
parameters you get a gradient out and
then this lsdm chooses the next
parameters you're going to go forward
backward on this get a gradient etc and
eventually you get a test and then this
optimizer which he calls M which has
loads at the input the input image X
looks at the parameters that this guy is
provided from running the LST M on this
and then it predicts the label for
these guys in a test time you minimize
the loss so that gets the right labels
at inference time you see five images
and you're able to from those five
images predict what the labels are um I
think this is one of the coolest works
that I've seen in machine learning for a
while because it really you start you
learn how to a test time be able to use
just a few data and capitalize on that
data to provide an answer of course
there are other ways of doing this and
there's I think other ways that are even
more effective than doing this but it's
interesting to see that you can actually
do this and the same framework as what
could be used for many other things as
I've shown in the setup for learning to
learn this one shot our few short thing
is becoming very popular and it's
important so there is this very cool
paper also from Jana and colleagues at
Berkeley and up in AI where they they
take a policy Network PI that produces
actions a but they condition not just on
the current observation of say a set of
locks on a table but they also condition
on a demonstration and so they train
models of this form and what these
models then learn to do is a test time
they can condition on a new data set so
if you see a new demonstration then you
know how to you you learn how to have a
policy that regardless of what
demonstration you're going to see a test
time and you don't know what data
someone's gonna give you you don't know
what someone's gonna try to teach you a
test time but you learn how to react to
that so you prepare yourself for the
lesson and I think that's a very
important direction because it's surf we
start designing models that are not
don't have this sort of fixed
architecture everything is stored into
hyper parameters but you accept that a
test time there's gonna be some data and
you gonna have to know what to do with
that date there's a video of how they do
this
I'm gonna just skip it and just mention
something very quickly and I'm gonna go
back in the last three minutes before I
open the floor for questions and go back
to my daughter Tala and what she was
doing she was experimenting she was
working as a scientist trying to figure
out how to do things she had these
internal rewards and she was able to put
Lego blocks together and she's able to
take them apart and again I emphasize no
machine that we have today or no machine
do we have insight to develop over the
next two years as capable of this
there's all sorts of outstanding
problems that you can sort of think
about when you look at that but all is
not alone this desire for knowledge is
much deeper it's about it's the desire
to sense as well as what's sort of led
to us having eyes and ears and and
constructing machines that enables to do
more sensing like telescopes and radio
telescopes and microscopes and so on
computers for that matter and why we're
building AI trees have this trees have
opsins which are the molecules that seek
light and they use it so that they can
find light and grow beautiful leaves and
provide oxygen for us we have the same
things we have options different
variants of these molecules and this is
how we kind of learn to see and they
gave us an advantage through evolution
through a process that again emphasized
we don't understand well but brought us
where we are now and this quest for
knowledge is one of the most important
ones and that we need to understand to
get at what intelligence really is about
I'd also like to contrast what we do in
deep learning what what I learned when I
was an undergrad in physics we try to
come up with models and physics meanings
in the sciences we're trying to come
up with models that sort of represent
the world and in statistics too that's
kind of what we do and we accept that
all models are wrong some are better
than others and some do good predictions
in a set up but they might not do good
predictions in a different set up we're
trying to come up with explanations
mathematics and Surya gave us a good
example of that yesterday and then we
also try to make predictions but we
don't just predict that a model it's not
just about the model predicting this but
it's the model predicts something and it
has to also tell you how you could
verify my prediction what's the
experiment that you can conduct to
verify that indeed the earth is not the
center of the universe these are
questions that took us a hundred hundred
thousand years or so to figure out and
and importantly you want to be able to
verify your predictions you want to get
to the heart of explanation it's not
about having an image and generating a
set of a descriptive caption that says
what's in the image it's about being
able to explain it's being able to sort
of get to the causal mechanisms that
require understanding it's not just
assigning a label to a set of pixels
it's about being able to reason about
what's happening there in that scene and
so on
and it's only that through that as you
were going to get out of get rid of
these things like adversarial examples
and so on so this kind of led us to
start thinking a lot about how do we
learn to experiment and we've been
taking a few humble steps in that
direction but I think a lot more needs
to be done in this area and I think this
in particular this here does these three
lines which I took from my undergrad
physics book are worth thinking about
more than ever we saw already a creature
experimenting before the crow that
interacts with the environment it
eventually learns to choose something
and then it's sort of it gets a reward
who gets the food or not
so Misha Daniil Pulkit who is an intern
with assertive mind and he's at Berkeley
teachers at O'Mara's Peter Batali and I
decided to serve try to sort of
understand how you could sort of
interact with your environment to learn
about your environment and we decided to
make it interesting so we decided that
we want to address problems where you
really need to understand the physics of
the universe the agent is not completely
disembodied from the world but we're
assuming that the agent is sort of is in
an environment and this environment is
what determines its intelligence and to
a large extent and by the environment I
mean other agents as well and so in
particular we were looking at a setup
where like imagine that the agent cannot
sense let's strip the senses but let's
allow it still interact with the
environment so if I put a thermos here I
don't have one
and I ask you does it have coffee in it
or not you will not be able to answer
that question the only way you can
answer that question is by actually
standing up walking here and picking it
up and then you can tell what if it's
heavy you probably say it has coffee or
you might have to open it and taste it
and check that it's not tea so for a lot
of perception we actually need to act on
the world we need to conduct experiments
and so what one of our interns a joy did
was he had a sort of setup where there
were ping artists of different weight
and this is in mood Joker so this is a
physics engine that has gravity in a
ground floor friction etc and then this
little guy moves around and it has a
hammer it has to learn to apply the
right sorts and how to move and how to
touch these things and then it has to
stop just like the crow and it has to
tell you which one is the heaviest
pinata
so here the the heaviest is the first
one but this is an agent that hasn't
learned so it's not very good if you
trained us with reinforcement learning
then this agent sort of moves around in
this case the yellow one is the heaviest
and just by touching it already realizes
wait this one is the heaviest one and it
tells you that that's the heaviest now
how does it know this it was trained
with many instances so this is again an
is like the Harlow task it was trained
in many instances where there was one
thing that was always somewhat heavier
than the others we did many other
control experiments it's all in the
paper but in this case one of them is
much heavier so it can't see so it's
trying things and then when it gets to
one that is much heavier than the two
other things that it has tried it
already knows because one thing is
always much heavier than the others it
already knows that must be this so it
doesn't need to try the other things
well this is interesting as well as
because we're training this with
reinforcement learning but for you for
those of you who know about bandits what
this guy is doing is solving a bandit
problem it's a known it's a known to her
or him or whatever gender okay yes it's
a known to this creature what what a
bandit problem is all the knows it has
to interact with an environment and it
has to get reward and it tries a few
things and it and there's in practice
weird mapping the other thing is in this
case we didn't have pixels but you could
also try to now in doubt the agent with
pixels and then sort of it learns with
one modality but as a consequence of
learning to interact with the world with
one modality touch and so on it will
then learn to understand wade through
pixels like I can now look at that and I
can predict its weight and all of these
objects I know the what their weight is
from pixels am i there's this weird
objects that I've never picked up but I
can predict all their weight I can
decide whether I can lift it or not and
it can even the project maybe I can lift
it so it's sort of have an idea about it
but I didn't learn this from pixels I
learned just because I interacted with
the world
I should there's some other experiments
but I would like to stop and take some
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>