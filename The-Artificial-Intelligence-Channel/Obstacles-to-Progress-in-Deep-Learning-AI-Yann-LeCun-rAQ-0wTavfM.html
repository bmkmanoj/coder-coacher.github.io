<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Obstacles to Progress in Deep Learning &amp; AI - Yann LeCun | Coder Coacher - Coaching Coders</title><meta content="Obstacles to Progress in Deep Learning &amp; AI - Yann LeCun - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Obstacles to Progress in Deep Learning &amp; AI - Yann LeCun</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rAQ-0wTavfM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you I know it's a pleasure to be
here and we're so I feel attend with
with CSE actually we see I'm sorry you
click on computer engineering department
here and it's always a pleasure to you
know cross the river to come here you
know as as as I remarked the machine
learning and AI is having an increasing
effect on society and very interesting
development over the last few years is
that NYU at all as you know taken a big
role in that in that revolution and has
become kind of a focal point of a lot of
interesting things happening in that in
that area partly because of the Center
for data science you know to which many
members of 10:10 are affiliated and also
because you know there's a lot of people
working on the Browning and sort of the
new the new AI including here tendon and
I've been one example so you know I
think there is a huge amount of
opportunities particularly in the
engineering so my background is actually
Electrical Engineering it's Anita actual
engineering as a as an undergrad my
specialty was actually chip design it's
very odd but that's what that's why
they're here that's why I study at
school and only later kind of shift
towards shifted towards things like
optimal control and then kind of led me
to neural nets and AI so I view myself
as an engineer as well as a scientist
and always like to build things which is
really the purpose of engineering
inventing new techniques and building
things okay so let me start by kind of a
summary comment that I heard from Josh
Tenenbaum who is a neuroscientist or
cognitive scientist at MIT he was a
conference I was at also a few months
ago and he said all those AI systems you
see nowadays none of them is real AI
now what he means by this is not the AI
systems that we have today
can match the ability of biological
systems animals and humans to to Lauren
you know how to do how to survive you'll
deal with variability of data in the
world so we can do a lot with current
techniques but we're very far from
really sort of reaching the goal of
making machines really intelligent and
so the first part of my talk is going to
be a bit of a state of the art and a
little bit of history as well and the
second part is going to be about the
challenges of the future and I think
areas like robotics a big important role
to play there because that's where the
important problems pop up that will come
to this a minute okay
so AI today really is supervised running
what supervised really means is that you
have a decision in school so I use an
analogue synthesizer as a model for
running machine it's a learning machine
really is a parameterized function you
show it an image and you you know run
the calculation through it and it
produces an output if the output is what
you want you don't do anything if the
app is not what you want you tell the
Machine here is what the correct output
is and what it does is it figures out
how to adjust its parameters so that
next time around it seems the same input
the output gets closer to what you want
okay so let's say you want to classify
cars from images or cars from images or
airplanes you know show you a bit of a
car and the machine doesn't say car just
you know address all the knobs you know
which they could be millions so that the
admin gets closer to what you want and
the interesting about interesting thing
about this is that if you build image
change in such a way that it has enough
power if you want the machine will
eventually figure out what the concept
behind a car and an airplane is and and
kind of will be able to classify any car
in any airplane you know including if
there is a lot of variability you know
the example chairs here with there's a
huge amount of variability between the
appearance of chairs and the reason why
that is is because of the essentially
the appearance of deep running over the
last of the popularisation the deep
Ronnie will last 10 years or so even
though work work on this started about
30 years ago and we started more
recently by peeking
ago the the world can I started learning
about this you know about seven years
ago I saw not even that so classically
when you want it to do Karen recognition
you would heat the image to a feature
extractor that would generally be built
by hand and then you would plug a
trainable classifier on top of it so
those kind of roles painted Pig painted
boxes are supposed to mean that they are
adaptable that it can be trained all
right so the time at the top is to get
the classical model of a pattern
recognition and it's used still very
very much in all practical applications
of machine learning today at the bottom
it's kind of the deep learning model and
the idea of deep learning is that you
you build your model as kind of a stack
of adaptive all trainable parameterize
modules and you train the entire system
end to end so as to minimize some
objective function that measures the
discrepancy between what you want and
what you get but the point is the entire
system is trainable and that allows it
to kind of process the data in sort of a
hierarchical way and I'll come I'll come
to why that's a good idea so the next
question you have to ask is what you put
in those boxes and it's deceptively
simple essentially a simple neural net
is a succession of search boxes and are
really two types one type is just a
linear operation right so imagine it's
text an image or an audio signal is
coded into a vector you take this vector
multiply it by matrix you get another
vector possibly of a different size okay
that's a linear operator and the
coefficients in this linear operator are
going to be the knobs that we're going
to adjust by by learning so that
produces an outlet vector you can think
of each component of this vector as
being a weighted sum of the component of
the input vector weighted by the
corresponding row in the matrix by which
you multiply this vector then the second
operation is a point wise non-linearity
and in recent incarnations of neural
Nets very often this point wise
non-linearity if the one shown on the
top the top right is called a value it's
really half wave rectification the nice
thing about floating to engineers is
that in order to explain what half wave
rectification is
not social computer scientists so you
have the cooperation point wise
nonlinearities which are very simple and
then you stack multiple stages of these
pairs of operations linear non linear
linear non linear you can show that with
only two layers of these so linear non
linear linear you can approximate any
function you want as close as you want
as long as the middle layer it's large
enough okay and that's something that
people have relied on for many years
arguing for the fact that you never need
water in two layers because you can
approximate any function you're one of
the two layers that actually is not
entirely true because for most functions
you want to compute the middle layer
might need to be extremely large and so
classical models like or what have
become classical models like sacred
machine support vector machine things
like this are limited to two layers in
fact only the second layer is relative
and that's what limits the power it's
the fact that it can only do basically
two steps of computation and one
nonlinear step computation so it turns
out there's a lot of interesting
functions that require multiple
nonlinear types of computation and
that's why deep learning is really
interesting and it's very difficult to
kind of turn this into a theorem and
sort of have some theory about why you
need multiple layers when it really
makes complete sense
most functions require multiple steps
computer scientists know this right it's
very rare that you can write a program
to compute a function that will only
require two instructions right generally
you have loops you have things like that
right okay so training supervisors
running really comes down to memorizing
an objective function you show an
example to the system
compute the output compare with the
output you want that gives you a value
for the cost function the objective you
want to minimize the error between the
what you want to what you get adjust the
parameters a little bit then show
another sample adjust the parameters a
little bit and show another sample etc
etc and if you do this what you're doing
in effect is what's called a stochastic
gradient optimization algorithm
shown at at the bottom where you
basically replace each parameter by its
own value minus the partial derivative
of the last function you want to
minimize with respect to this particular
parameter multiplied by some constant a
step size and because you do it on the
basis of a single sample that's called
stochastic gradient because you get an
estimate of that the gradient of the
objective really the object you want to
minimizes the average of all the samples
but because you do it one sample at a
time you get a noisy estimate every time
or you do it with a small batch of
samples at a time you get a noisy
estimate so it's called stochastic rate
and for that reason and so pictorially
you can think of the cost function as
being you know some sort of landscape in
a high dimensional space and what this
algorithm does is going to find the
minimum of that you know the valley the
bottom of the valley in that landscape
but canister caste stochastically every
time being everyone kind of a noisy
estimate of the direction of steepest
design so especially my ask is how do
you compute this gradient and it's the
so called back propagation algorithm
which I'm not going to go through
because it would take too long and many
of you I'm sure know about it it's
basically based on chain rule so it says
if you have if you know you know your
nap
your network is basically a graph of
computational nodes and you know how to
compute the output of each of each node
of computational blocks here in this
case you know how to compute the output
of each block as a function of its
inputs and its internal parameters and
it turns out if you know the gradient of
the loss function with respect to the
output of that module
you only need to multiply by the
Jacobian matrix of that module with
respect to its input or with respect to
its parameters to get the gradient
respect to the input and respect to the
parameter so that gives you a suggestion
that you can have some sort of recursive
algorithm that goes backwards you know
through the graph and kind of propagates
gradients this way let's call the back
propagation algorithm it's really just
chain rule but it turns out you know
which you express it graphically you you
express your system as a graph of
networks you don't have to figure out
here to compute the gradient as long as
your system knows how to compute the
gradient for each module you can
assemble very complex networks and not
worry about how to compute the gradient
that will be done automatically by the
software you use so all the plumbing far
more basically what they do for you is
they allow you to build a graph for
those networks and then they
automatically compute the gradient for
you
okay so linear operators are good but if
you have an image an image might have
you know a million pixels and
you know if you multiply individually
million pixels by matrix and the advert
is also a million variables let's say
that's a big matrix right 10 to the 12
terms so what we need to do is it's
actually reduce you know print some
constraints on this matrix so that it
becomes manageable and commercial nets
which are used somewhere universally now
for image recognition as well as for all
kinds of other applications like speech
and even text translation and things
like this are the main idea behind that
is to basically constrain the matrices
so that they're basically took its
matrices and that they can you know have
fewer parameters and reduce the amount
of computation so again the nice thing
about talking to electrical engineers is
that I don't have to explain about
completion is but but the basic idea of
a commercial net is you take an image
and you take a neighborhood of pixels
and you have a set of coefficients you
multiply this little patch of pixels by
and that gives you a weighted son okay
that's one term in in the output so that
gives you one output value and then you
swipe that that little window of pixels
over the entire image and every time you
compute the dot product with the the
coefficients and record results so
that's the discrete 2d completion and
you get the you know if the input is
here at the bottom the bottom right you
see the completion kernels the set of
coefficients and you see the result here
that's called a feature map that you
pass result to a non-linearity say value
or Sigma it or something like that so at
the bottom here on the on the top left
you see the image then you see the for
future maps a lot a result of convolving
the image with four different kernels
and the coefficients of those kernels
all the result of learning right along
with with back propagation all the way
through and then there is a second
operation computing which consists in
aggregating the response of the filter
over a small area by a max or average or
l2 norm or something like this and then
reducing the resolution and the reason
for this is to impose a little bit of
shift and distortion invariants in the
system and this is very much inspired by
biology you know we talked about
completions and invariants and blah blah
but in fact the architecture of this is
completely suggested by
architectural visual context this
classic work in the 60s in fact Nobel
prize-winning work but you're going with
all about the architectural visual
context where what it showed that
neurons in the visual cortex of cats
look at local areas in the visual field
and and are basically repeated all over
the the visual field so that's where the
idea comes from and there were models
that try to emulate this you know back
in the 80s Fukushima is neo Neutron for
example so now scientists call this
simple set of complex cells the the the
completion like operation and the
pooling so here is a commercial net in
action and you know each vertical column
is a is a different layer you see the
input on the on the left and you know if
the input chips by four pixels then the
third layer after pulling shapes by two
pixels because it's example by factor of
two and then you know two layers up it's
shifted by one pixel and then couple
years up and maybe 1/2 clicks or
something like that so as you go up the
layers the amount of distortions caused
by a shift in image or distortion in the
image is reduced and it's easier to
model which is why the systems can are
more amenable to do to do an image
recognition or kind of invariant
somewhat invariant recognition
so let's seems like an obvious concept
but but back in the 90s very few people
were convinced this was this was a good
idea
so for example this is a picture taken
in 2005 which was the dinner
resulting from a bet between larry
jackel sitting on the right I'm glad I'm
a replica with in the back here whom I
think you'll hear talk at this seminar
in a few weeks so it's a former
colleague of mine at Bell Labs and the
bet
naari jackal was the head of the
department at the time and Jacko bet
what fancy dinner that by March 14 2000
is was in 1995 people will understand
quantitatively wide big neural networks
working on large data sets on it so bad
understanding means that it will be
clear conditions and bounds and things
like it's the kind of theoretical
arguments that that Nick likes that be
bets bets when fancy dinner that jackal
is wrong but if that big figures at the
bank indeed and conditions that big
still wins the bet so this was a silly
failed attempt by jackal to convince
that people you should do the theory for
neural Nets he completely failed and and
he also lost the bet because we still
don't quite understand really kind of
the generalization properties of neural
nets I mean we understand they work you
know there's no problem with this but
but not to the same extent as simpler
models just because you know they're a
much more complicated and so we don't
have simple theory for it we have
theories but they're not that useful
there's a second bed that Nick bets one
fancy dinner that by March 14 2005 no
one in his right mind will use neural
nets that are essentially like those
used in 1995 Jack Roberts one fan said
you know that that because one so Jack
who lost the first one that peak lost
the second one and so in 2005 there was
there was a dinner and they shared the
bill and lay up with you and I remember
- it's the on the left here
louboutin I just enjoyed the dinner ok
so you know since the Scotia people here
I see fashion I hear the first row
working robotics here third work a
little talk a little bit about some
products project that we did and this is
a story called going back up you know at
ten years even more the end of the
project was about ten years ago this was
a project called lager running applied
to run robots sponsored by Daka and the
idea was to use machine learning to
drive
robots of the type you see on the top
left off road so the system would should
be able to you know through vision can
you figure out what's an obstacle what's
not and you can do this with stereo
vision for short range but it doesn't
really work with long range because with
stereo vision you can only you know
depending on the baseline of the two
cameras you can only figure out it's
something sticks out of the ground about
ten meters out but not much more than
that
so what you see here in the middle
comment are the result of applying
stereo vision to to this and you know it
starts after 10-15 meters whereas if you
apply the neural net
it's monocular so it works all the way
and the cool thing about this is that
you can train a neural net with the data
produced by the stereo vision system so
you don't have to explicitly supervise
it you basically let the robot run
cutting data from stereo vision and then
use that to train the monocular system
to tell you whether something is an
obstacle or not and so it was a
accomplish on that which you know the
idea of this was that you would apply to
commercial net basically on small
patches over the entire image and would
classify every pixel if you want so it's
a kind of a task in computation that we
call semantic segmentation which
consists in categorizing every pixel in
the image by the category of the object
in belongs to so this was running on
this was a you know robot built by N
Reich at CMU around 2005 and so the
processors were not what they are now
no no T to you or anything so we could
run the neural net to the back one frame
per second so we had a short range
television system for kind of avoiding
your unexpected obstacles and and then
the long-range vision system with we run
at about when one Hertz and so this is
the robot this video is accelerated
twice and these are annoying ki students
making the life of this problem possible
but they are allowed to do that because
they actually wrote the code and this is
kept so many who is now working on
robotics I gogo brain in California and
right Hadsell who leads the robotics
research group at defined in London so
after this we decided that we could do
sanity segmentation for real so instead
of whoops I'm not sure why my slides are
telling it but um anyway so such that we
would work on 70 segmentation for real
and essentially have a system that
labels every pixel in an image
not as to whether it's Orzabal or not
but but with the category of the object
it belongs to if you have an image like
this and you'd like to be able to tell
whether it's the world or a car or
building or the sky or trees or whatever
pedestrians that can be important and so
eventually we we build such a system
it's again a commercial net canna swept
over the entire image you can run this
very efficiently we with claim of rob a
who is now the vp of machine learning
services at Nvidia he implemented an
FPGA board that run a system at about 20
frames per second and so it's kind of a
demo of this this is state of the art at
the time down Washington Square Park
some of you might recognize situations
Square Park so it's making stupid
mistakes like it's gratifying you know
some areas Washington Square Park has
desert or Beach there is no Beach I'm
aware of there but it would certainly
make the campus more attractive so you
know it's not perfect and since then
those technologies based on some
commercial Nets I've made a huge amount
of progress but you know this was 2010
roughly and we could run this at 20
frames per second so I gave our use
talks about this around the time and it
gave some ideas to people working on
start driving cars to say they realize
perhaps they could use this for selling
cars so in particular this the company
called mobile I in Israel that was
working on vision systems for for cars
created by Anan Joshua who's a professor
in computer vision at Hebrew University
and they started playing with commercial
Nets and we're getting much better
results with whatever technique that
were using before and they had a problem
because the chip they had designed
wasn't really designed to run commercial
net so they kind of had to shoehorn
control nets onto the current shape now
they have new chips that differently but
it went it worked well enough that the
the cars that appeared in 2005 it has
won 15 2014 15 actually use their chip
for autonomous driving after that the
divorce was various reasons
that also prompted Nvidia who built the
GPU cars to get into the business of
surviving cars could be realized you
know they could you know build hardware
for this and they also realized that you
couldn't just sell the hardware they had
to build software on top of it in fact
one team that does research on said
wemmick card at Nvidia is because you
know former colleagues of mine from Bell
Labs and worked in hope you can see the
same building where I used to work at
Alette and they are doing pretty amazing
stuff there and Anna's husband actually
works there too okay so what happened in
2012/13 I said people figured out I mean
first of all there was
you know much bigger data sets that we
kept available like imagenet import for
image recognition so image nights data
set with 1.3 million training samples
he's got 1,000 categories of objects you
know the dominant object in the picture
and this is the kind of data where
commercial Nets really shine before that
the data sets were so small that you
know techniques that build more things
by hand were actually more successful
and then commercial Nets we couldn't
really get kind of you know record
leading performance with commercial Nets
before that but locked it with our data
sets that really kind of blew everything
out of the water and the the second
reason was the efficient implementations
are commercial nice on GPUs which
allowed us to build much much bigger
commercial Nets so the work by our
friends at University of Toronto and its
crew jetski yes it's Kevlar and in turn
really kind of opened the eyes of the
computer vision committee on this and so
created a kind of a revolution which is
funny because in 2011 we submitted a
paper on the semantic segmentation to
see DPR and it was rejected mostly
because people had no idea what the
commercial net was the reviewers and
they didn't believe that technology that
never heard of could work so well two
years later to tell maybe three four
years later 2015 14 you cannot get a
paper accepted as a VP or unless you use
conditionals so there's been a big
revolution now what's happened over the
over the last years is that so many
people got interested in this that
they all came up with vertical ideas new
architectural ideas on how you kind of
assemble those functional blocks into in
various ways and the latest
architectures of commercial nets for
image recognition have something like 50
layers or harder layers the uses logical
resonate originally proposed by kami
kami hay from Microsoft Research Asia in
our case work in California and they you
know it kind of this revolution that has
led to incredible improve improvements
in recognition rates so on image night
for example before commercial Nets the
best error rate for one 25 26 percent
this is you can an error when the
correct category is not in the top five
among the 1000 then when you're later
the crews XT paper brought this down to
about 15 when you're later it was around
10 when you're later it was around five
now its beauty and that's super human so
if you ask people to categorize those
images that will make just about as many
mistakes and most of those aren't images
are niggas in it you know in the first
place you don't know what the relevant
object is so one question we might ask
is why is it that makes conditional Nets
and those hierarchy called mentality of
structure is actually good and in my
opinion it's probably because they
reflect the compositionality of the real
world so the real world particularly the
conceptual world is compositional in a
sense that an object is form of parts
and parts are formed of motifs and what
ifs are formed of collection of edges
and so if you have the low level a sort
of oriented edge detector which is what
does completion all Nets learning and
what your brain is doing and what every
actually even handcrafted computer
vision system will do that you can
detect combinations of those edges to
detect you know common shapes like
corner circles grating things like this
and then also example to form parts of
objects so you detect combinations of
those and that's kind of what this
architecture kishan that reflects with a
pulling operation that allows the
position of those little features to
kind of vary a little bit and so you get
this sort of it istick perception and so
you know it's see it sounds like a
conspiracy that the world
it's so designed that it's easily
understandable by architectures that
happen to fit in your brain case which
led us to even with a applied
mathematician at Brown to say the world
is compositional or there is a god this
is kind of a play on a something that
Albert Einstein said that he said the
most incomprehensible over the world is
that it's comprehensible the world could
be completely random and it was nowhere
we could make any sense out of it and so
it seems like a conspiracy that there is
you know some understanding we can have
of it okay so we come to go study or
restart work this is a little bit of a
snapshot and state of the art of what
people can do in key provision and this
is some work on that the clinical care
research in in California in the low
park technique called mask or CNN and
this is again a supersymmetric
segmentation system but it doesn't just
label it kicks off with the category it
labels it with the instance of the
object and it gives you a mask for the
for the object so I'm not going to go
into the details of this architecture
but it's conceptually quite simple you
can do things like this so it well you
know again it's kind of a sliding window
combination that it looks at multiple
scales and it you know it kind of
focuses on different areas and the
output is not just a category for the
for the pixel in question it's also a
mask of the object that it believes it's
looking at okay for every location so
you put this together and you get a you
know a mask for every object in the
picture this will bad the dog at the
bottom the people etc there are
individual people you know the wine
glasses the wine bar all the computers
in the back the table you can count the
sheets etc I mean it works amazingly
well and this is in my opinion really
one of the most kind of impressive
progress of the last few years in
computer vision showing what people can
do with this we can track key points on
on bodies and sort of reconstruct the
the pose so this result this code is
open source by the way
so you go to this URL you can just
download the code play with it you can
you know train it to your own purpose
it's a twenty plus class speaker imply
conditional nets to 3d data this is some
pictures from also symmetric
segmentation competition actually run by
Stanford I believe we're a team from his
work in Peretz actually want a
competition using 3d convolutional nets
and the problem with 3d is that when you
have an accompany the Capon see map or a
punch out of 3d points from lidar for
example or a depth sensor most of the
box holds our ante and so if you run a
counter on it on the entire volume you
can waste your time just multiplying
integrals with you know with odd numbers
so they figured out a way to can just
follow the areas whether it's voxels are
non-empty and and you know they're by
kind of increasing the efficiency and
they just it just to show that compute
on this can be used for something else
than pixels the one of the systems that
Facebook uses for translating languages
from one I wish to another it's actually
accomplished on that it's a gated
conditional net so it's a more
complicated Android commercial Nets the
architecture you show is shown on the
right I'm not going to go through it but
it basically takes every word as a
vector essentially and it learns the
mapping from words to vector as well and
then that goes to accomplish on that and
then this commercial net has some gating
mechanism that can sort of you know
regurgitate words in a different
language in the right order so you can
translate you know English to German you
remember to put the word verb at the end
and stuff like that so it's it's
interesting that something that was
designed for perception and vision can
actually be used for you know producing
sequences and that are not you know
pixels but but you know text has a
common property with images which is
that this kind of local correlations
right the you know the one word can
follow another and there is some
correlation between neighboring words
but far away words are less correlated
so it's kind of like pixels and images
now another things that Facebook and you
know I I wear two hats I have when
you went for the Facebook I should say
one and a half foot at Facebook and went
half witted NYU is is that all the
research we do is is open we publish
everything and we distribute a lot of
Korean open source so the the one that's
probably most useful at Holland is pi
torch which is a framework for deep
running it is very flexible and it would
be nice if my laptop didn't die okay
apologies
all right okay now keep running is this
idea of assembling modules in kind of a
fixed graph but really what people are
moving to it's something some of us
would call the differentiable
programming and the idea there is that
you don't want this graph of operators
to be fixed you want it to be defined by
program and you might want it to be
dependent upon the input you're looking
at so for every new input you're seeing
the architecture of the graph will
change and so what you call
differentiable programming it's because
basically you write a program that
computes the ad words and this program
calls functions that could be the
equivalent of those modules functional
modules I thought I told you about and
there is you know in the background
something that figures out how to
basically back propagate gradients
through your program so that whatever
program you write whatever or I should
say sub gradient for the mathematicians
in the room because a lot of those
functions are not entirely
differentiable so let's call it sub
gradient and be done with that so but
but the idea that you know the program
is data dependent and you know builds
the graph dynamically and you can still
propagate gradient through it so let's
call this differentiable programming you
could think of this as a slight
generalization of deep running the same
way
probabilistic programming is a
generalization of totalistic graphical
models for example and people are
interested in this particularly in the
context of natural language processing
or reasoning because we think that's
going to require for certain types of
reasoning so an example of this is
neural nets that are augmented by
modules that basically act as a scratch
pad memory like a Ram if you want and so
that's a kind of a concept that popped
up in a few years ago with the memory
Network idea by Jason Weston with that
face book stack government in our n ends
of ironical also at Facebook no Turing
machines and differentiable mobile
computers by great at Al from the mind
those ideas can applaud couple more
that's all at same time with days of
each other on archive which is funny and
the meaty the basic idea is that you
have a recurrent neural nets or no net I
can feel it it put its output back to
its input if you want
so we can have some dynamics and it's
you know accessing a memory on the side
so a memory is just a different type of
set of modules that behave in a
particular way and this you know again
several architectures which I'm not
going to go into the details of but just
talking about this idea of
differentiable memory so think of the
circuit of a ram a ram chip okay a ram
chip has an address decoder which
basically compares the input address
with all two to the N possible bit
configurations and activates one line in
the in the chip memory and then what you
read out is the the some the weighted
sum of all the memories except all the
weights are zero except for the one who
is you know is addressed
so the differentiable memory used in
those memory networks is exactly the
same idea the input is a factor you
compute the dot product of this vector
with a bunch of key vectors which are
like the that's like the address decoder
so the dot product gives you you know
how well the input vector matches each
of those keys you get a bunch of numbers
correspond to products you run that
through a softmax which turns this into
a bunch of numbers between 0 and 1 that
sum to 1 it's kind of like a Gibson
tribution the formula is on is on the
left and then you use those coefficients
to you know in a weighted sum of values
which themselves are backwards produced
by the memory so what you get is
weighted sum of the values and the
memory weighted by those coefficients
resulting from the dot product of the
input address with each of the keys so
it's kind of a you know soft associative
memory if you want and the cool thing is
that you can propagate gradient through
that you can learn the keys you can
learn the values etc so you can build
neural net so this is kind of a
recurrent neural net and fold it in time
which accesses this memory three times
in this case if you look at the the
right part of the picture here so you
get for example you want to build a
system to answer questions you put a
question at the input which is really
encoded as a vector and then the system
so computes an address accesses the
memory to see if the answer is there
then gets the guess the answer runs the
recurrent net once or twice then I
to the memory again recurrent net access
once or twice et cetera and I would
allow the system to basically access the
memory multiple times if it needs to
connect multiple fax to answer a
particular question so my colleagues at
Facebook came up with this thing's
called the Babbitt ask you can it's kind
of hard to read but in the middle at the
top there is something a legal story Sam
walks into the kitchen Sam picks up an
apple Sam works into the bedroom Sam
drops the Apple where is the Apple and
the answer is in the bedroom so you can
attract you know what goes on etc they
ought to make simple inference Brian is
a lion Julius is a lion Judas is white
Donald is green Michael is Brian if you
assume all lions are white we're on his
way so we came up with 20 different
types of questions of this type that
people might want to answer and you know
what machines to answer and training a
memory network to answer all those
questions they they can basically solve
20 19 of the 20 types there's another
type of network called entity network
that can solve all twenty that we came
up with more recently but which I'm not
going to it's very similar it's also
kind of a memory augmented and all that
so that's really nice because that's the
idea of kind of marrying reasoning with
with with learning and reasoning is
something traditionally I was very
strong at so traditionally I you know
expert systems rule-based systems this
is basically completely concentrated on
the whole idea of doing reasoning
properly but and there is the problem of
knowledge acquisition how do you get a
machine with enough knowledge to reason
properly and that was always the main
problem the main issue with expert
systems it's how much effort it takes to
actually reduce everything to rules etc
so what what you'd like is machines to
be able to reason but at the same time
learn how to how to do that and learn
facts here's another example also done
at Facebook in Menlo Park by Jason
Johnson who was at the time an intern
and a whole bunch of characters from
Facebook and here the problem is to
answer questions like you know the the
picture at the bottom here is there a
match
you that has the same size as the red
metal object so to be able transfer
questions like this you can I have to
you know count the number of objects of
a particular type detect them first etc
so they came up with this architecture
and people since then have come up with
even better ways of doing this but the
idea is a little similar so imagine you
have a question that the system needs to
answer something like are there more
cubes that yellow things so you take the
image you run a commercial net on it
which you know we're going to train for
this whole system then what you'd like
is you like a little block a little
block one of the green blocks here that
filters colors another one that filters
by shape and the teks cubes the first
one filter is the yellow objects then
there is a block that counts those
objects then it's a block that compared
those two numbers and you get the answer
to your question the problem is it's one
of those examples where this graph of
operators needs to be dynamic no it
needs to be determined by the data and
so what I have here on the left is the
question goes into a recurrent neuron
that that then spits out a specification
for a graph essentially you know it's a
kind of a tree pipe I mean it's sort of
a graph like syntax that specifies a
graph and it's it's kind of hard to back
propagate to this operation more recent
work on this by others by erinkoval at
you know sir material for example I've
done this in a fully differentiable way
so here there is a bit of um people
hocus-pocus you have to do to train this
system end to end but the wonderful
thing is that the only way you have to
train the system is you you give you a
question you give it an image you give
you the answer and it basically figures
out how to kind of organize all this
it's a little bit of prompting at the
beginning to tell it what the graph
looks like for a few examples but it's
able to generalize pretty well so that's
one of those can really eye-opening
examples where you can have essentially
the neural net producing an other neural
net dynamically and then had this neural
net solve a particular problem and it
seems like it sounds intuitive that we
might have something like this in our
head you know ahead right we face a new
situation we kind of configure
or you know reasoning engine to kind of
figure out you know analyze the current
situation let me skip ahead a little bit
when I'm skipping ahead of to apologize
to an abacus it's work that he and I did
together a few years ago but I want to
get to obstacles to AI so as I said
before animals and humans learn much
more efficiency efficiently then than
all the machines that we have and you
know why one may wonder what kind of
learning algorithm does to grow news or
what kind of principle we can a paradigm
you know it may not be just a question
of algorithm and one thing that humans
and animals seem to have is some sort of
common sense ability right the fact that
we kind of know how the world works and
so we can fill in the blanks for a lot
of things we don't have to be told
everything you know right now you're
most of you are seeing my left profile
you can figure out what my right profile
looks like because you know that faces
are mostly symmetric you know you have a
blind spot in your visual appeal where
your optical nerve punches your retina
and you're not even conscious of it
because your brain kind of fills it up
you know we have a very good ability to
predict the future and predict the
consequences of our actions that's what
allows us to plan ahead in fact you
could say because of this that the
essence of intelligence is really the
ability to predict it's the ability to
build models of the world right by
learning observation by interaction so
perhaps what we did our mission is to do
is it's kind of predictive learning it's
it's real I the phrase here is not
exactly it does not exactly reflect what
it means perhaps a better name would be
in putative learning but that sounds a
little pedantic so it's basically the
idea that the system should learn to
predict every variable it doesn't
currently observe from all the variables
that you currently observes so a pretty
good teacher from the from the present
on the past predict all the things you
can't directly observe right now but you
might observe by you know moving your
head so right now some objects are you
know the front of the stage is hidden
behind the lectern but you know I
move and figure out what it looks like
so I can sort of you know try my brain
to predict what it looks like and see
what it is my brain certainly can
predict what the view is gonna is going
to look like if I shave my head 27
minutes to the right and in the process
of learning how to do this
we probably develop the notion of depth
right so that's can be learn a chiral e
by predicting how the world changes when
you move your head or if you look look
at the world with two eyes or so but
even when I would wait close to work and
so you know we we we run this as babies
right we look at the world we can hardly
act on the world when were babies
but we don't a huge amount of stuff
about the world just by observation we
you know we don't object permanence we
learn you know intuitive physics you
know baby one we talked also long things
here is a video one we talk it's playing
it was being played a magic trick put an
object in a cup we move the object
without seeing and then showing the
antique up and is what you look for
lacking let's video me talk they are
both almost as smart as we are you know
if yeah you know which is my lifetime we
could build beliefs that were you know
just a little bit you know smart as dogs
there would be complete success in fact
I think if there were as intelligent as
a house cat I think would be complete
success you know so if you show the the
picture of the top left to baby I stole
it from Emanuel you poo who it's a kind
of scientist in Paris and if you show
the six month baby or you know four
month baby the tsunami on the left here
you crucially look hard off of a support
and it floats in the air of course is a
trick Phoebe said sure that's the way
the world works no problem after six or
eight months they figure out what
gravity is and they look at this fact
it's like the baby on the bottom left
they're completely surprised by this
because the figure that can't possibly
happen every object is not supported to
follow so in fact
emanuelle built this chart of at what
age babies learned so different concepts
like object permanence the fact that
there are animate and inanimate object
that's about two months three months
stability and supportive and five months
gravity inertia conservation momentum
that's about eight month you know things
like that so you know the object would
be things like so basic you know
concepts of a hardware works are learned
in the first few months of life mostly
by observation with very little
interaction with the world and we don't
know how to do this with machines
but that's what we should do so that led
me to this you know there is those three
modes of learning that people use in the
context of machine learning one it's
called reinforcement running so we first
went running is when you get a machine
to do something you don't tell you for
the correct answer is you just tell it
you did good you did bad but you give it
one scalar reward or you know feedback
once in a while and it could be at the
end of a long sequence of actions which
makes the whole thing very difficult the
supervised running where the teller
machine were the correct answer is well
we talked about this and then it's kind
of other modes of running which
generally we can call in supervised
running but really that doesn't detail
what we do but maybe predictive chronic
that what machine learns to predict
every variable it doesn't currently
observe from the ones that does observe
and in that platform there's a huge
amount of feedback that the machine gets
right if I if I put it if I try to
predict what the world looks like when I
move my head and I do move my head the
amount of feedback I get is all the
clicks falls in the world it's not just
one scalar value once in a while it's
all the pixels in the world so it's a
huge amount of information information
yet and that's according to various
people I try things and this is the only
way we can train very very powerful
learning machines to learn anything
about the world is if we give them a
huge amount of feedback every time we
train them so that I'd be to this
slightly obnoxious slide that I'll show
every time now because become the meme
in the machine learning community where
if intelligence is a cake the bulk of
the cake version wise if you want is
unsupervised predictive learning and
then the in terms of the amount of
feedback you get to the machine the you
know icing on the cake is supervised
running and the cherry on the cake is
reinforcement running because you're
only giving a very small amount of
feedback classical reinforcement
learning and that's that's why you know
classical reinforcement learning doesn't
apply to the way our world the world it
works really well for things like glue
and chess and things like this but
probably a world if you want to learn to
drive a car and you have to drop the car
off a cliff fifty thousand times before
the machine learns not to drive the car
off the cliff
you know we seem to be able to do much
better than this because why because we
have a model of the world allows us to
predict what's going to happen if we run
out of a cliff so we don't actually run
off the cliff I don't need to do this to
predict what's gonna happen our model of
the world is good enough to prevent us
from doing this
so what reinforcement learning systems
need to have is a model of the world
they need to learn about of the world
and that's called model-based RL but
model B is running in general it's not
just limited to reinforcement learning
it's it applies to all kinds of things
so when first mattoni works well for
games because games the world is very
simple you can simulate it real quickly
you can you know crash your car 50,000
time doesn't matter and so it works
really well for group of course it's
amazing how it works it works for you
know games like doom it's starting to
work for gaming like Starcraft although
it's still very preliminary but this all
idea that you should have predictive
models is very classical in optimal
control there's a few people here
working on human control and the way to
do an optimal control system is you you
have a model of the thing you're trying
to control that predicts the next stage
of the system from the current state and
the action you're taking or the command
and then you optimize the trajectory
this way this those techniques exist
since the 60s so why can't we do this
with with learning machines basically
having learning machines learn the model
of the world and so clearly the next
revolution in AI will be unsupervised
and the concept for this like on some
earlier choice from Berkeley so you know
will will need AI systems I have you
know in the AI system tries to optimize
some objective does perception it it
acts on the world etc but inside the
agent
you need some sort of world simulator
some way for the machine to predict
what's gonna happen next and I'm just
going to quickly show you one example of
this it's the don't be scared by the
fact that I'm skipping over a bunch of
stuff and and first point point to
towards a problem the problem is that
the the is that the the world is not
entirely predictable so if you have if
your world consists of two variables y1
and y2 your entire world is you know
consists of two pixels and the only
dependency between those two pixels the
data points you observe you know obey
this rule so if I give you the value of
y1 you can to some extent predict the
value of y2 but you know if I give you
the value of I 2 y2 it's probably two
values or maybe not at all for y1 that
are possible and I priority you know I
don't you know you're not going to know
if you observe y1 y2 you're gonna have
to predict the other one from whatever
the one you predict what is the best way
to represent the dependency between y1
and y2 possibly through a contract
function so this is the idea of an
implicit function you don't have a
function that computes y2 for my one and
another one completes what I went from Y
to your function that takes y1 and y2
and tells you whether those two values
are compatible or not so call this a
contrast function an energy function and
negative likelihood whatever you want to
call it but it's a something that will
take root values all the data points and
take higher values outside the data
points and the animation here describes
perhaps with the learning process for
such a function with me right so it's a
function with two inputs Y when y 2 and
it produces a single output which is the
weather y1 y2 have compatible values so
it's easy enough to train a model to
take low energy values for points you
show it but it's hard to actually push
up the value for points you don't
observe and why don't we made a list of
you know seven different methods to do
this which I'm not going to go through
be reassured I'm not I'm only mention
one which is actually not on this slide
called adversarial training and that's
because I think it's one of the most
exciting developments in machine
learning
years adversarial training it allows us
to learn predictive models under
certainty so so here is a situation for
example we have a predictor particular
looks at the past so it looks at the
segmenting video for example it's trying
to predict what's going to happen next
right so it would be a good way for a
machine to run into the physics for
example so let's say I put a pen I hold
a pen on the table and I tell you I'm
going to lift my finger you can predict
that the pen is going to fall but you
can't really predict in which direction
if I if I do it right
so the work really build a search this
time it's that the the predictor is
going to make a prediction and to make a
prediction it's going to have access to
a source of random vectors called Z here
in this case and so it combines x and z
ran this through some neural net and
makes a prediction for what the world is
going to look like half a second in the
future and we can train the system
because we can just let time pass or
observe the result and then you know
train division to produce this problem
of course is that if the machine
predicts that the pen called falls to
the back in to the left and what
actually occurs is the pen falls to the
back and the right machine was not
technically wrong it's quite it you know
quantitatively wrong but you know it
produces the wrong picture but you know
it kind of predicted the right thing so
how do we tell it okay they do it wrong
but really I'm not going to punish you
for it so what you'd like is among the
set of all possible future is
represented by this red ribbon here in
the the space of predictions you'd like
to tell the system if you made a
prediction that is on the red ribbon of
plausible futures I'm not going to push
you even if your prediction is different
from the thing that actually occurred
it's only if you make a prediction
outside of this red ribbon that I'm
gonna punch you okay I'm gonna make you
pay in the objective function now the
thing is you don't know what this red
ribbon looks like so the idea of
adversarial training is you train a
second neural net to learn the location
this red ribbon and it's you know it's
very much like this contrast function
we're changing over earlier and the
predictor is actually used to produce
hypothesis samples that the other one is
going to tell whether the real winner so
that's the idea of a virtual training
the Scotch rice function the the the
non-metal predicts is from contrast
functions called discriminator and we
have a generator that makes prediction
so initially the generator makes bad
prediction and we train the
discriminator to produce a high output
high energy output and then we show the
discriminator and actually occurred in
the world and then we train it to
produce a low output okay so the
discriminator basically learns the
contrast function between things
actually occur and things that for now
don't occur because they predicted by
the generator which initially doesn't do
a good job so continuously the generator
gets the gradient from the discriminator
and so it knows how to change its
prediction so that it's commuter will
think it's real okay so as we train the
disc linear and the screen there learns
this contrast function and as we train
the generator the generator trains to
produce those green dot closer to the
real dots and we want to do a good
predictions that's basically a virtual
training you train to network so it
breaks all the things we do in machine
learning because instead of now
minimizing an objective function we try
to find an actual equilibrium between
two functions one that is used to train
the generator and the other one used to
train the discriminator and you know
that turns out to be a lot more
complicated and minimizing functions so
people are working a lot on trying to
make this process stable essentially but
when you manage to make it work
it works amazingly well those are known
in 15 bedrooms that were produced by a
generator that doesn't actually look at
anything other than random vectors and
is trying to produce images of bedrooms
so those are randomly sample images
bedrooms this is work from a few years
ago so much interest work the other guys
are Google I believe there is a huge
amount of progress there I'm only
showing you kind of early work on this
and these are this is trying on image
net so you can't really actually
recognize the objects these are trained
on dogs you can't you know it's kind of
funny dogs more recent work on as we
show training by various groups across
the world have produced images that are
kind of amazing high resolution
megapixel images of faces that are
incredibly realistic trained on
celebrity so you get an honest
seek celebrity that kind of looks nice
this is work at Nvidia I mean it's a lot
of really good work there in this area
but we are interesting using this for
modeling the world so if you use D
Square to try and accomplish on that
let's say to predict the next friends in
the video you get blurry prediction
because the the system cannot really
predict what you know what is going to
happen in the world and so it produces
the average of all the possible things
and things might happen and that's a
blurry image so that's the you know e
square or you know traditional criteria
produce the kind of result you see at
the top right here
but if you use a bit of adversarial
training you get much sharper
predictions which may or may not be
right but they look reasonable so
multiple animations are six for friends
the first four frames are observed and
the last two frames are predicted
indicated by the red control and little
patience look okay these are video
segments shot in various New York
apartments and as the camera rotates the
system has to interact when the
apartment looks like and so it look you
know it figures out what the bouquets is
supposed to look like as the camera
turns the couch is supposed to continue
you know that there are pictures on
picture frames on the wall things like
that more interesting if you work on
surviving cars you'd like to be able to
predict what the other car is next to
you are going to do before they do it so
that it will allow it allow you to drive
defensively so these are examples again
a few frames are observed and then you
predict three friends in your future
there are space from a I think at six of
the second apart so you predict happy
second and the system predicts that you
know if pedestrians start crossing the
street they're going to keep crossing
the street if the car starts turning
life is gonna keep turning that the
scenery is gonna keep moving so slightly
more interesting with a model that I'm
not going to explain it's a little video
so this is a little game the paper is
gonna be or not on a kite in a couple
days this is little game where you pilot
a spaceship and you're supposed to go to
one of
colored space station and it's you know
a planet with gravity and you know you
don't have enough threats to actually
you know go against the gravity so
sometimes you crash but you have to
figure out you know sighs to figure out
the trajectory and what it does is that
it builds a model of the dynamics by
just learning observing what happens and
you know with sort of various random
actions and then it uses this model to
do planning so it's you know very
similar to what people do in system
identification and optimal control
excited here it's everything is run with
a neural net essentially you reckon on
that and this works much better than so
classical reinforcement learning methods
that require on the order of 4 million
interactions with the environment to get
anywhere and don't actually work really
well
whereas this system you know with
800,000 actually works
ok ok I'm going to stop here and thank
you for your attention and take your
questions
thank you on since we have you here I
was wondering if you could comment on
max pooling operation then the work of
George Hinton in terms of capsule
networks and see if you could comment on
men it's good to use max pooling versus
capsule networks and going back and
forth basically right so there is this
new paper recent paper by Jack Hinton
and a couple of students on something
called casual networks and it's a you
could think of it as a new type of
convolutional net where the pooling is
replaced by another operation that is
kind of more explicit and tries to
essentially directly manipulate the
geometric parameters of parts of objects
into objects so normally in a classical
control net you you do max pooling so
you take the response of the filters
over an area you compute the max and you
can think of this as kind of a switch
that chooses one of the accusations
there are other types of pulling that
people have used
but capsule is a particularly
sophisticated kind so so far it's been
demonstrated that this works really well
on things like M nest when you have a
lot of when you kind of need a lot of
invariance to recognize handwritten
digits it's not been really demonstrated
on large data sets like imagenet or at
least it's not being mastered as being
particularly useful I don't think the
experiments have been done yet it's a
little difficult so it's it's a little
up in the air whether this will really
change the way we do things on whether
this is sort of an interesting idea that
needs more maturing jeff has had ideas
of this type for about 30 years almost
which demo was a student with him back
in the late 80s actually worked on a
model called traffic that had the kind
of similar idea of explicitly
manipulating coordinates of parts of
objects I think their projects to the
constellations or something recognizing
constellation stars and and so you know
it's taken him quite a long time to
figure out how to sort of use this to
practice to something I actually
he had an earlier paper four years ago
on you know some version that wasn't
nearly as good so I think you know it's
gonna take a while before before if you
got this one out okay I don't think we
have time for any more questions but Yan
is gonna spend the entire day with us
okay well thanks Yan for that
enlightening talk and what is arguably
one of the most exciting areas of
research today in artificial
intelligence and its applications also
as a colleague in electrical and
computer engineering you're affiliated
with our department and on behalf of the
Tannen School of Engineering I just have
a couple of other words to say one is I
would really like to thank Anika Rahman
sky and rattle Thompson who's here
somewhere
in the back they put an immense amount
of effort to put the series together and
so I'd like to take you Anna in
particular and right here
and for those students are as to make a
very bad joke I would say if who's
supervised learning we are in charge of
we are you know I hope you take the
right take away that this combination of
mathematical insight data computing
power and good coding skills on your
part can lead to immense benefits so
please keep that in mind and of course
very hard work so as we saw for the
presentation today so again thanks yawn
and thank you very much and we have this
plaque for you
thank you thank you very much
nice talk I'm UCSC targeting down
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>