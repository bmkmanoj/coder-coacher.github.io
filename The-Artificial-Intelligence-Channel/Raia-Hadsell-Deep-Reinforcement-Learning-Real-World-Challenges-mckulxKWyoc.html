<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Raia Hadsell - Deep Reinforcement Learning &amp; Real World Challenges | Coder Coacher - Coaching Coders</title><meta content="Raia Hadsell - Deep Reinforcement Learning &amp; Real World Challenges - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Raia Hadsell - Deep Reinforcement Learning &amp; Real World Challenges</b></h2><h5 class="post__date">2017-09-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mckulxKWyoc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">left welcome right ahead so at this
stage for the for those of you don't
know her she's a research scientist at
deep mind I'm focusing on topics in
lifelong learning deep reinforcement
learning in robotics PVC spent time at
SSRI international at CMU so if that
right she's you're quicker thanks no
podium but I get a clicker great thank
you for having me and did not have far
to go so we're just a couple floors down
in this lovely building and the work
that I do there have been working the
last three years or so with deep mind
and I work in the area of deep
reinforcement learning and specifically
I've started to spend a lot of my time
thinking about how we can take some of
the research that we've done amazing
research in areas like Atari games the
very important world of Atari games or
professional go playing and bring that
towards more real-world problems but
that of course is a challenge and and in
particular my background is in deep
learning and robotics and I would love
it if we could bring the potential of
deep reinforcement learning all the way
to the area of robotics and be able to
train robots in the same way that we can
train Atari agents for instance so let
me start off by giving us all a little
bit of reminder as to a little history
on how we got to where we are now with a
lot of people a lot of technologists all
thinking about deep learning and
artificial intelligence so a few
examples a quick timeline so in 2010 I'd
say that the state-of-the-art method for
taking an audio stream and turning it
into text ie doing speech recognition
was to train an acoustic model train a
phoneme model train a language model put
these together and do some more tuning
maybe and that was our state of the art
for doing audio to text speech
recognition and then deep networks came
and made things better by simply taking
one model and training it basically
end-to-end from the end output that we
want all the way back to
to the raw inputs a couple years later
this was closer to my hearts they've
always done more work in the domain of
computer vision so the sort of
state-of-the-art methods involve taking
raw pixel inputs extracting key points
doing feature computations maybe
applying something like a deformable
part model before ending at the results
that you might want in this area maybe
labels for objects in the image and of
course with image net all of a sudden
and alex nets we ended up with a new
competitor in the area which was deep
nets which surpassed the old results by
double digits in terms of percentage
points in accuracy machine translation
much the same thing happened though
there is I think more more resistance
here because the feeling was that we
needed to have all of the expertise the
domain knowledge that went into this
field that allowed you to do text to
text machine translation but again with
enough data big enough models the right
sort of learning then we end-to-end
learning ended up changing this field
and allowing a better level more
accurate machine translation more
automatically than had been previously
than possible that brings us to what I
would point to as being something that
looks a little bit similar which is the
sort of state of the art of how robotics
is done which is that it starts with
some raw sensors in the world on a robot
you have a perception module you have
maybe a map or a world model you have
some planning you have some control and
then you have the output which is the
actual action to take so this looks a
little bit too similar I think that it
is perhaps a matter of time only before
we find that in act in interactive
environments that we start to see
end-to-end trains neural networks doing
better than this previous state of the
art but robotics is different right so
if we want to train a big network to
predict label
from images then we can have we have big
data sets we have been speakers today
talking about about wonderful datasets
that we can collect but for robotics the
problem is is that the robot changes the
world as it takes actions so you can't
take a static data set and simply learn
actions from it you really have to have
a interactive domain an interactive
learning domain where there's a feed
from sensors and where the actions can
have the possibility of changing what
comes from those sensors in the world
that's a fundamentally different way to
do machine learning than taking a data
set and drawing IAD batches from it and
training a big neural network with lots
of optimization approaches so that
brings us to reinforcement learning
reinforcement learning is one of the
ways one of the best ways that we know
how to learn to produce actions from an
interactive inter interactions with an
environment so just to run through this
cartoon we think of reinforcement
learning as being a relationship between
an agent which can take actions and an
environment which produces observations
the agent is given a goal and there are
observations which come from this
environment to the agent for instance a
image video touch sensors some sort of
input information and there's actions
that the agent can take that could
change that environment and there's a
reward signal and and this is this is
the puzzle this is how we would like to
be able to learn in more in in in
interactive environments and deepmind
came into this space and said well we've
got deep learning we've got
reinforcement learning we put the two
together we put a neural network in the
agent and we do a lot of work on the
algorithms to make them stable and make
them work if what you're training is a
deep neural network and and this has
worked very well and given us a powerful
set of tools and the nice thing about
these tools is that they're quite
general
so all of these ten different atari
games were all trained using exactly the
same neural network with the same
initial weights the same actually this
these were all trained together this is
one multitask network that's playing all
of these ten different games so it's
using the same hyper parameters the same
learning rate same learning algorithm
etc it's one general algorithm to play a
lot of very different games and the
output so the input here would be the
pixels of the the pixels of the video
frames and the output is 18 possible
actions on the joystick for the Atari
emulator now the nice thing is that that
also works for simple robotics tasks so
this is a robotics task where we've got
a simulated Jayco arm with three fingers
and the task here is simply to reach
towards that red ball wherever it
appears and you get a point if you get
close enough to it and you can be sort
of an arranged around it which is why
you end up learning sort of a jittery
policy but the cool thing here is that
this is trained using exactly the same
approach as the previous as the Atari
games the only difference here is that
now the output instead of being the
actions possible on an Atari with an
Atari joystick now we have the desired
joint velocity for each of the joints of
the robot the six six joints plus the
three fingers so that's that's great
that that lets us know that we can do
simple tasks using exactly the same
formulation the problem though there's
still there's still a lot of problems so
the rest of this talk I'm going to go
through three of what I think are the
the biggest challenges that we have of
going from these simple but general
approaches to much bigger problems and
I'll give a challenge and give sort of
one approach that I think could help in
this area so the first one is data
efficiency and this actually is maybe
the whole thing it's all about data
efficiency because really when you're in
the real world then and and you're
dealing with live data
of interactive data then that's always
going to be expensive there's always
going to be a cost associated with that
and for instance this this lovely robot
arm this lovely little simple task took
about 20 hours to Train on a computer
but that was an asynchronous
multi-threaded algorithm so there was
actually 32 different agents training at
the same time we were sweeping over a
bunch of hyper parameters at the same
time and the simulator was capable of
running much faster than reality
so if we had actually done trained this
task this policy on the real Jayco arm
that we have in the lab then even if we
let it train 24 hours you know 24/7
without stopping it would take 55 days
to get to a robust reaching it could
start anywhere and reached any position
55 days is a long time to let a robot
learn how to reach so data fish
efficiency yeah data efficiency is a big
one if you are exploring the world using
sparse rewards and and and and raw
pixels then it can take a very very long
time for learning to get off the ground
so one of the things that we've been
looking at it deep mind is ways of
speeding this up and the one I'm going
to talk about involves learning from
multiple senses or or more generally
learning from multiple different
constraints multiple different
objectives so what do I mean by that let
me let me talk about this paper here the
authors has got a nice long author list
we always do it at deep mind this is a
paper called learning to navigate in
complex environments and the this is
what the the game the task looks like in
a nutshell
so we have amazed this is a top-down
view of the maze that the agencies but
it never sees this view of it it only
sees this fear of it so it's a like it's
a rat running around in enemies and it's
trying to get to this
get here I have I don't have a pointer
I'm there
the little orange goal that's there in
the maze in the second frame and if it
gets to that goal then it will
immediately get 10 points and respawn
somewhere else and so the whole task is
to appear somewhere in the maze figure
out where you are find the goal then you
end up somewhere else if I do I back to
the goal again and again and again for a
fixed number of frames and so you keep
on and and and and there's different
ways in which we can set this up we can
have the maze have a different topology
every time or we can have it be fixed we
can have the goal change location so on
every episode it's in a new place you
have to find it and then refined it
again and this is a hard task because
the rewards are sparse oh there's also
apples those are worth one they're sort
of sprinkled around and this is a hard
task and what we see is that the when we
train an agent even in a state of the
art learning algorithm on this then it
takes a very very long time to get up to
a reasonable level of performance and in
fact it never really gets up to a human
level performance which is what the bar
we usually set so what can we do here
the the the approach that we came up
with was to take a take a fairly
standard approach that we use which is a
convolutional neural network first to
process the input video the input RGB
and then we add a recurrent layer
actually a two to recurrent neural
network layers both lsdm layers and then
we add auxiliary tasks and the point is
here is that it's very hard to learn
from the from the reward signal alone
that's the policy and the value outputs
here of this of this Cartoon Network so
we add these additional tasks it's like
saying to the learning algorithm you
need to find your way to the goal and
you're going to get points for doing
that and eventually you need to learn
how to maximize that reward but you also
should be thinking about how to make
sense of what you see and what you have
learned in every trajectory around this
maze so the additional tasks that we add
one is to predict the depth of every
frame so it's as if I had two sensory
inputs 1 gives me RGB information and
the other one gives me gives me depth
information like a light our signal or a
depth camera or connect information that
sort of thing
and I want to take that RGB and I want
to predict what the depth is how far
away things are and this is a way of
teaching the agent about the geometry of
a scene we also can add loop closure
prediction which would be in this sort
of a maze answering the binary question
binary prediction of have I ever been at
this at this location before in this
episode am I going in circles always a
good thing to know if you're trying to
navigate so how does it how well does
this work so this is I'm just going to
show you a comparison of some of the
different learning curves across the
y-axis we have average reward mean
reward over a set of episodes and over
in a cross the y axis X and across the x
axis is the number is the length of the
the learning process so yes a hundred
million frames seen in terms of learning
and so we see that this is the initial
agent that we trained and this is the
average performance over the top five
different hyper parameters top five
different seeds that we tried and this
is just a feed-forward agent so this is
taking that architecture and saying I'm
just going to train a convolutional
neural network with a fully connected
layer and as you can see it doesn't
really get up to very high it gets up to
maybe almost a hundred points on average
per episode and has this very slow
learning curve
now the next agent that we compare is if
we add the lsdm so now we've added the
memory layer to this
recurrent neural network memory layer
and we see what we often see when we
added an LST homage that you get to
eventually a higher performance but it's
a little bit harder to take off and you
get this sort of inflection point where
you start to finally learn where the
agent starts to learn so that's an
improvement and we know we need memory
for this task so that's an important
thing to add the next agent is if we add
actually that stacked LST and I said
that we had two LST M layers had some
additional inputs if we add that we do
do better but the late the learning is
not terribly stable and these these
learning curves are quite smooth it
already now if we add that first
auxiliary task so we're supporting the
learning now by saying it's an RL task
plus this other supervised task we see a
much earlier take-off in the learning
rate happen but overall it's a more it's
a little bit unstable still but we do
get to a higher point and also have an
earlier takeoff and learning that's
adding that loop closure prediction if
we add the depth prediction we see
something much different now we see very
stable learning happening and we see
that it improves very quickly very
rapidly right after the agent first
first starts to explore this task first
starts to train on this then we see an
immediate takeoff the interesting thing
here is to compare this to if the depth
channel information is so important to
the agent why don't we just give it to
it as an input and we tried that I don't
have that in the slide deck but if you
feed the depth information as an input
as an observation to the agent it
doesn't actually help it helps a little
bit but still the take-off is somewhere
here as opposed to here so we can try
there's two different depth prediction
auxiliary losses that we tried one is on
the LST M and one is on the continent
they both do well the d2 is on the LST M
that does very well and then we can add
them all together which actually takes
off a little bit earlier but doesn't do
quite as well asymptotically and we can
compare this to the human expert and so
actually all of these with axillary
losses eventually
exceed what our human expert can do on
this task so this is a really big
advantage in terms of data efficiency
and that's why I wanted it here although
this this research was a little bit more
about navigation but this is obviously
very important to us and the idea here
is that when you're training a neural
network if you have noisy gradients from
sparse reward you need to also have
something that's stable and and this
helps to simply drive the predictive
capacity of the whole network and
stabilize the learning so adding this
other axillary tasks really helps the
entire learning process including on
those sparse rewards which is the only
thing that we're actually measuring here
is how well we do on those at maximizing
your wood right and here is a video of
the trained agent so first this is an
amazed that is static so on every
episode the it's a big maze but the
where the walls are is fixed we change
how the pattern on the walls look and
the different things here but we you see
the agent zooming through and this shows
the depth prediction so this is actually
it's not even a very fine resolution
it's a very coarse signal here that's
being predicted by the agent but you can
see that it is that it gives some notion
of the geometry of the space here's the
agent moving through the maze and it's
actually trying to predict over time
exactly where it is in some places it
can do a very good job of predicting its
location in some areas it's not as sure
if for instance if it gets into a corner
or right after it responds
all right so on to the next challenge
the next thing I'm going to talk about
is problem complexity and one solution
here is hierarchical reinforcement
learning so I'm going to talk about
something called futile nets that's a
recent paper at ICML by some folks at
deep mind as well so let's talk about I
know I said that Atari games were too
simple - that wasn't what we wanted to
solve but we do want to solve
Montezuma's Revenge Montezuma's
revenge's is awful when I got to when I
first started at deep mind then people
were just saying oh you know this is
this is the worst thing ever and we
shouldn't even try to do research with
montezuma's revenge because it's it's
hopeless
so why is it so hopeless so basically
the reward signal is not just sparse
it's it's really really delayed so when
you have to jump around in this
environment and the very first reward
that you get is when you get the key
over here and to do that you have to
come around here and swing on this and
avoid the the skull and you're not going
to get there by random actions you're
not going to get there by by by just
random exploration and even if you do
manage to get there it becomes pretty
meaningless so it's very hard for the
network to to generalize it's very hard
to get concerning 'fl exploration so it
would and that's all because we're
operating in this space of simple very
primitive actions if we could instead
make the decision of I'm going to try
swinging on that rope or I'm going to
try climbing that ladder and see what
happens but that's not what we do we
usually just decide I'm going to go one
space up or one space down which doesn't
tell us much so that's why Montezuma's
revenge's very hard so and what we would
like is a system where we have perhaps
sub goals where we can break up the
problem and learn some sub policies to
achieve them and instead of expend
instead of exploring over primitive
actions we could explore over the
possible sub goals
but this is a bit of a problem because
how do we decide where the sub goals are
of course we could come in as a human
and say climbing the ladder as a sub
goal swinging on the rope is a sub goal
but that that's not in the spirit of
end-to-end learning you know cannot we
don't want to give that sort of top-down
information to the agent so there's a
bit of a chicken and egg problem there
to doing the learning how do you learn
the sub goals if you can't get anywhere
in the game to begin with so HR L or
higher claw reinforcement learning has
been around for quite a while as being
and the promise here is that if you have
HR L then you can do long-term credit
assignment so you would be able to say I
did this back here and now hundreds of
steps later I died and I'm going to do
the accurate credit assignment that's
really hard to do otherwise but if I'm
thinking about my policy over this long
term temporal span then I can start to
to do that long term credit assignment
and memory I could also do structured
exploration as I said and this will lead
to better generalization better
exploration and also transfer learning
right so if I learn the a particular set
of sub goals in one part of the game
they'll probably apply to a different
part of the game or even to a different
game altogether so that's the promise of
HR L but but there have been very few
approaches that have really managed to
do this there was a paper from 93 by
Peter Dayan and geoff hinton so you know
if if they're authoring something 93 we
should probably all be taking a close
look at it now which is what Sascha and
his colleagues did so the idea here
behind futile Nets is similar to the the
paradigm of a convolutional neural
network you want to have layers where
you have levels of where you have levels
of abstraction and where the but in this
case the temporal resolution changes as
opposed to an a continent where the
spatial resolution changes or the field
of view it's a way of saying that my
policy should
decomposable into more primitive parts
and higher-level parts that are that are
more abstract and we need to be able to
learn these different layers
automatically that's the tricky part and
of course we'd like to do this with
neural networks which is something that
although Peter and Jeff coined the term
futile RL they didn't have a means for
making this work with neural networks
and that's what we've been working on so
at a high level this is futile nets
which we call fun that's the name of the
paper is and the idea is first there is
a confident right so that's going to
extract our visual features from the
input image and then we're going to have
two different networks here almost
entirely decomposed one is going to be
the manager the manager is going to
operate at a low temperature in the form
of just the latent space latent
representation so a set of features and
those goals will be generated and sent
to the other network which you can think
of as the worker the worker receives the
sub goal and chooses an action so has a
policy choses in action and executes
that and the worker is trying to do two
things it's both trying to so it is
trained to match the sub goal so if the
sub goal says get to the ladder then
it's going to try to get to a latent
representation that is as similar as
possible using cosine distance and it's
going to also be trying to maximize
reward in the environment which is
different from the original futile RL
and the manager is trained using a
different policy gradient it it assumes
that the worker is going to succeed it
assumes that the worker is going to try
in good faith to get to the goal that
that the manager sends to the worker
and so given that assumption of that
it's going to try to get to the sub goal
then that frees the manager to optimize
at this higher temporal level at this
higher level of planning there's of
course this is this is feudal that's at
a high level there's a low level just a
lot Messier my husband looked at this
and said you're missing the ground
connection so I'm not going to go
through this I'm going to say take a
look at the paper it's on archive if you
want to know all of the details there's
a lot of great stuff in this paper
actually but the the question for us is
does it work and the answer is that it
does so on the right here we have the
training curve and this shows what
happens if you have an agent that just
has an Alice TM which theoretically
could manage to do the same sorts of
things but it really just doesn't manage
in the exploration space to get to get
off the ground very far these two red
lines indicate the first red line is
getting the key is the points you get
from getting the key and that first
screen and the second red line is the
points that you get when you move to the
second screen so this is the first two
this is where you need to get to before
you can really start to do any learning
in this game so the green curve manages
to get to that second screen but doesn't
get much further even with a lot of
experience whereas the feudal networks
agent managing manages to get up to
almost 2,000 points which is nowhere
near as good as a human expert but is a
lot better than what we had before and
we can do a little bit of analysis as to
what's happening this looks at a
correlation of getting to these sub
goals so these are these are things that
the after training that the manager
thinks that these are useful sub goals
and and the worker knows how to get to
them so the correlation of those two
things and what we see is that we see a
spike when for the wealth the first one
is the start location the second one is
hopping over that first little barrier
there the third one is climbing down the
ladder the fourth one is
I don't know jumping over the school
that's there maybe and then getting to
the key which are all those are those
are approximately the sort of sub goals
that a human I think would also define
so it's nice that it converges to
something that's sensible and this is
the actual policy that's been learned it
always dumps one life to get the grit of
the skull always
and it always dumps one life doing that
but it manages to get to the second
screen with lives despair can go down
the ladder has to get the sword here
come back up the ladder and then yay
kill one of the skulls and get killed by
the other and that's about it that's it
that's 2500 points but I can I can tell
you there was a lot of excitement in the
office when this video got sent around
um so it's it's good to see of course
there's a lot of I think other
interesting ways when when we play the
game you know if my ten-year-old was to
play Montezuma's Revenge
I think that he would realize pretty
quickly that schools are bad and keys
are good swords are good there's a lot
of semantics that we bring to a game
like this which is the other reason why
it's why it's so hard and I think that
there's a lot of interesting work to be
done and you know what if I could learn
about keys what if I can learn about
skulls what if I can learn these things
and transfer them to this game then I
think we would also see a big speed-up
in performance but I think those sorts
of learning algorithms can be very
tricky to make them general as opposed
to something that specifically for
Montezuma's Revenge the nice thing about
feudal Nets is that they don't just work
on montezuma's revenge they actually
work better and get faster learning
performance on most of the the Atari
games and on a whole set of memory games
and other tasks so and those results are
in the paper alright so the last in my
seven seconds that I have last left I
just want to talk quickly about one
other challenge and method and this is
continuous control so the idea here is
that you know we haven't talked about
how do you control an actual articulated
body how do you control muscles how do
you choose
how do you choose continuous values as
actions and this is something that's
quite hard and it's quite hard to
include into a learning algorithm one
thing one approach that I think is
really nice has come out fairly recently
from a group led by Nicholas at deep
mind and this is on transfer and reuse
of locomotor control and the basic idea
here is that we're going to separate the
types of observations that we make into
proprioceptive and XT receptive so
proprioceptive observations are
observations about things that are close
to oneself meaning in this case how do
the the joint angles of a body the
velocities and maybe the tactile
information that's coming in these are
all things that a baby learns
immediately or is born with some
knowledge of then there's extra
receptive observations so these would be
things that are far away from the body
that are about the goal that are about
what we are trying to do and about
long-range perceptions so vision but
also information about where is the goal
and what am I trying to do and this the
this the intuition behind this research
is that we want to separate the learning
of these two we want to learn how to
have how to interpret proprioceptive
information without having any goal then
we want to learn how to transfer that
it's how do I move my body and then how
do I move my body to satisfy some
external goal that's been given to me
and this is what the approach looks like
in a slightly odd form the idea is that
we're going to separate proprioceptive
and xt receptive different types of
observations and we're going to
separately learn a low-level controller
and a high-level controller that's going
to look at these different things
separately in the high-level controller
can both see the goal information like
where am I trying to get to or what am I
trying to do and can also see what the
low-level controller is doing local low
level controller just tries to move the
the body of the the robot or whatever in
a consist
way in a meaningful way and the way in
which it's trained is in three phases
first there's some pre training of the
low level controller think about this as
being again something that that a baby
might do before it is told to do
anything before it's trying to
accomplish any goal it's just trying to
figure out what happens when it moves
its arms around then we give it we use
IAD Gaussian noise to feed in and this
gives us exploration but simply using
noise and then finally we train the
high-level controller to actually solve
a specific task and the advantage of
this is that it really gives very very
fast learning again in the end it's all
about data efficiency in some ways but
this is amazing to see that if we just
try to train some of these networks from
scratch we get very flat or very poor
performance and the green curve at the
top is if we use this this process here
of first training the body then training
the task first the proprioceptive then
the Xterra separative and this is the
last video I have this shows on a few
different types of simulated critters
what what happens here so here the the
first the I'm behind the snake is now
being taught this is a six dimensional
snake and it's just trying to get to the
target and you saw that first sort of
move learned to move then it was given
this external goal information and it
could then learn very quickly
same thing here's a canyon traversal
task when it's just trying to get
through here alright so the second
critter is a quadruped ed we call this
the ant
although it does not have enough legs
and so again it's the same process with
the last part of it being seeking a new
task and we can learn multiple different
transfer tasks multiple different high
level tasks while keeping that same
efficient good low level controller so
this is an ant playing soccer well and
ant making goals and this learning is
impossible if you start from scratch but
if you start by learning the body and
then learning
then it suddenly becomes feasible and
that's what we that's what we need to be
doing there's one more one more example
that humanoid which i think is 23 dolf's
degrees of freedom so first it this
looks awful sort of we're torturing this
poor thing there is there's a rhyme and
a reason to it though we could make this
look a lot more realistic but really
it's not it's saying we know nothing
about this body we know nothing about it
but now we're going to manage to teach
it how to with a set of high level
controllers how to get how to run in a
consistent way and how to avoid these
big obstacles by sort of slaloming
around it and you know this is how we
sort of start out we flail around a lot
we do a lot of ridiculous things we fall
down in lots of ridiculous ways but then
we get to a point where we understand
how to use the different joints and
angles and muscles that we have so that
we can do any task that's set before us
and I think that in any way possible
this is the reinforcement learning the
end if it's something that's a can be a
little bit odd-looking but I think that
this that that all of these three
approaches are going to help us towards
taking deep RL and all of its potential
towards a well a lot of very interesting
real world problems real-world tasks so
I'd encourage you to check out some of
these different papers look at them in
more detail and see what comes from deep
mine next it's always fun anyway thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>