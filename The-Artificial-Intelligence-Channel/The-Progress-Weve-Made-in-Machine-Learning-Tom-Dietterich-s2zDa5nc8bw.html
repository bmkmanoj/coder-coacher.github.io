<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Progress We've Made in Machine Learning - Tom Dietterich | Coder Coacher - Coaching Coders</title><meta content="The Progress We've Made in Machine Learning - Tom Dietterich - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Progress We've Made in Machine Learning - Tom Dietterich</b></h2><h5 class="post__date">2017-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s2zDa5nc8bw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so it's a pleasure to have this
opportunity to try to review in 30
minutes the progress in machine learning
over the last 10 to 20 years so the the
plan here is I'm going to focus just on
the basics and begin with review of
those then discuss the the the big step
forward with deep learning but also pay
equal attention to the framework of
probabilistic modeling and then I'll try
to focus on the weaknesses of both of
these paradigms in terms of the research
challenges I did want to touch briefly
on anomaly detection at the end and then
then provide my my suggestions for the
capabilities matrix so of course the
basics of machine learning is the area
known as supervised learning and we
assume were given a set of training
examples which are typically pairs of
some sort of a description X I say a
feature vector e and d dimensional space
and a class label or it could be a real
valued response and the goal is to learn
a classifier or a function that maps
from the D dimensional feature space to
the class labels so the the you know so
paradigmatic example would be optical
character recognition or handwritten
digit recognition and so these are some
shots from the feminist character
recognition dataset so the input is a 32
by 32 bit image our pixel image and the
output is one of the none of the ten
digits so there are two fundamental
paradigms that have been explored in
machine learning the first is the
probabilistic modeling paradigm and then
the second i'm coining a term here but
the end-to-end function learning
paradigm so the probabilistic modeling
paradigm a very simple example this is
Fisher's linear discriminant so what we
do is we build ourselves a probabilistic
model of how the data is being generated
and in this particular case we can
describe it by this little graphical
diagram here
we imagine that whoever's generating the
data first chooses one of the digits
that they want to to write with their
handwriting and they do that from some
discrete distribution P of Y and then
they decide what what actual pixels to
produce and a very very naive model that
would be a multivariate Gaussian over
the over the pixels so there would be
some mean pixels for say the digit 3 and
then some covariance matrix over those
pixels and if we're going to assume that
all the covariance matrices are equal
then we get Fisher's linear discriminant
which is I don't know 1930 ish so the
learning process in this paradigm is
just fitting the model to the data which
in this case is extremely easy but
classification that is making
predictions then requires us to do some
probabilistic inference because we don't
observe Y first and then predict X we
observe Eckford X first and we have to
infer why of course fortunately there's
this thing called Bayes theorem that
lets us do that but more generally in
very complex models we may need to do
probabilistic inference both during the
model fitting process and during the the
prediction process the alternative
paradigm is the end-to-end function
learning paradigm and this is a picture
of one of the early deep neural networks
a convolutional neural network developed
at Bell Labs back in 1998 that is the
sort of granddaddy of all the things we
have today so the idea is that we define
a space of parametrized functions a
parameterised in this case by a vector
theta and a loss function which says how
bad is it to output y hat when the real
answer is y in this case we'd usually
use zero one loss so if you get it wrong
you have the loss of one if you get it
right you have the loss of zero and then
we need to solve an optimization problem
which is to find the setting of the
theta parameters that minimizes the sum
of the losses on our training data and
we usually need to include another term
which penalizes the complexity of the
parameters because we typically have a
highly over parametrized model class
with millions of parameters potentially
and we don't usually have that many
degrees of freedom in our data even with
big data then to make predictions is
very computationally very cheap because
we just have to evaluate this function
in the forward direction and nowadays we
do that on a GPU so it's easy to put
into a phone for example well there's
been a lot of progress in both of these
paradigms over the last say 15 to 20
years the first thing is in is in the
probabilistic modeling space there's
been the development of very rich
probabilistic models we don't have to
assume everything's gaseum anymore we
have for instance complex mixture models
so it could be a Gaussian mixture models
or hidden Markov models and and these
these this was promoted a lot by the
development of probabilistic graphical
models and perhaps you do pearls book is
the best-known source for that there's a
development also of nonparametric models
where the number of parameters can grow
or shrink depending on the amount of
data and I think a lot of people would
point to the development and the
statistics community of Jewish that
process mixture models and then in the
last five years there's been a real
flowering of something known as
probabilistic programming and DARPA has
just finished a program in that area and
two examples of that would be the the
language church which is basically a
scheme like language in which you can
describe a probability distribution by
writing a program so it's an executable
program but the executions are
stochastic and the distribution you're
describing is the distribution over all
possible executions of the program
that's only interesting because you can
then say I want a condition on the
output of the program tell me what the
inputs had to be in order to produce
those outputs so you you have to be able
to sort of run the program backwards
which is quite interesting and perhaps
those popular probabilistic programming
languages known as Stan and this was
developed at Columbia and Andrew Gelman
this group and and it's just had a huge
uptake in the statistics community of
course it's not enough to write
beautiful languages we have to actually
reason with them and so there's been
a similar big advances in probabilistic
inference methods starting with belief
propagation which was in Paul's original
book but then variational inference
techniques were imported from physics as
were Markov chain Monte Carlo techniques
and the idea of MCMC is you define a
Markov chain whose stationary
distribution if you run it long enough
is the desired probability distribution
you're trying to model and another
notable advance there has been
Hamiltonian Markov chain Monte Carlo
which is the inference engine underlying
stand and and that has led to really
create model processes that converge
very quickly to their to their
stationary distribution interestingly it
uses the derivative of say the
likelihood function and to do that it
uses automatic differentiation
techniques so you've written your
program it's automatically
differentiated to get the derivatives so
one of the strengths of the publicity of
modeling paradigm well one of the big
things I think is that it can represent
knowledge that we might have about the
problem so coming back to Brahmas
question about geometry or or physics we
can put that into the probabilistic
model we can also represent the reason
about latent variables which i think is
very important in intelligence analysis
and a models are interpretable if if
they're identifiable so you have
techniques for figuring out whether
those latent variables can be uniquely
reasoned about and then finally we get
calibrated assessments of our
uncertainty you know our conclusions
conditioned on assuming the model is
correct so just as an example of using
latent variables for say multi sensor
fusion we might have the true phenomenon
be some sequence of variables here might
be say the location of a person we're
trying to track or something like this
and then but we don't observe that
directly instead we observe some noisy
signals maybe one of these is from I
don't know GPS and then others from
computer vision or something
but to or financial transactions so the
the coding here is that the gray
variables are the ones we observe
and the black variables are the ones
that we just need to make inferences
about so now let's turn to the function
learning side sort of similar things
have happened there so there's been
development of very expressive function
classes starting with nearest neighbor
methods but then the development of
support vector machines decision tree
and decision tree ensembles the
multi-layer perceptron which we would
now think it was just a very shallow
neural network convolutional neural
networks long short term memory networks
residual networks and right now we're in
the midst of sort of giant hackathon in
the field as people are experimenting
with all kinds of different models
another thing we discovered along the
way is that ensembles are very powerful
and so we have various methods for
creating those so even with deep
networks typically you can get some
boost in performance by training
multiple networks and having them Bo's
which is sort of astonishing because the
number of parameters in those models is
is mind-blowing well of course again
it's not enough to come up with
beautiful languages for describing your
models you need to solve the
optimization problem of fitting them and
and there's been a lot of progress there
mostly by machine learning people making
very good friends with people in
operations research and optimization
theory so maybe the the unique
contribution of the machine learning
community was the development of
boosting or something known as
functional gradient descent but we've
imported convex optimization techniques
stochastic gradient descent has been
explored from many angles and there's a
an exciting development they're called
Nesterov acceleration that gets us to
very rapid convergence many techniques
underlying the recent progress in deep
neural networks that I won't go into you
and I and I just wanted to address this
question about confidence assessment so
one one interesting framework that's
come up recently there is something
known as conformal prediction and it
gives you point wise confidence
assessments of the predictions of the
model unfortunately it does just as
improbable istic modeling you have to
assume the models correct here we don't
have to make that assumption but we do
have to assume the data
exchangable or near iid which we know is
false both of those things are false of
course we know our miles wrong and we
know the data really is an iid so a
better the strengths of function
learning well with sufficient data it's
usually more accurate than the
probabilistic modeling framework so if
you look in some of you may know about
kaggle calm they run machine learning
competitions and those competitions are
virtually always won by some function
learning approach so and I think the
reason is that if you have enough data
to fit them the flexible function
classes that in deep nets or boosted
tree ensembles make many fewer modeling
assumptions they also require a lot less
model development effort and machine
learning people really don't like to
think about modeling so they would just
rather point a black box at the data and
have it run and then the very faster
runtime well the general methodology
right for building a machine learning
system involves these steps of data
acquisition and then feature engineering
to try to transform the data into the
form that will match the assumptions of
your model and then model design model
fitting a model evaluation well usually
about 90% of the effort goes into that
first two steps particularly the feature
engineering point step and so the big
advance of deep learning has been to
automate that feature engineering step
so if we look at computer vision before
say 2012 the whole field was using hands
designed or human design features so we
had these cool things called sift
descriptors and histograms of gradients
but after 2012 we switched to
convolutional neural networks and and
here for instance is the top 5 error
rate on an image net which is a
challenge problem with 1000 object
classes and we can see that with the
introduction of deep learning we went
from error rates in the in the 25
percent range down to error rates in the
7 percent range so it's been really a
dramatic improvement
so the key finding is deep Nets can
design better features than we can
design by hand
this is particularly valuable when we're
doing a signal dissemble type of
transformation right where we have
low-level signals and we need a high
level interpretation and there's a big
gap between them and and we don't know
how to engineer the futures along the
way but we can use this deep learning
technology to do that on the other hand
if you have data that is already more
feature eyes let's say let's say like
medical records or well that ignoring
text but like measurement physical
measurements that can be treated as
variables themselves then there's
probably less to gain by using the deep
learning technology and the technology
is not easy to apply so so often it's
it's best to avoid and in those kind of
settings
okay well similar advances have happened
in speech recognition right before 2012
the field was using probabilistic
graphical models so hidden Markov models
and Gaussian mixture models but after
2012 people were into deep recurrent
networks and and we've seen similar
advances similar reductions in the word
error rate for new reports that they had
a 23% word error rate for their
telephone speech recognition system and
now they reported this year they're down
to four point nine percent well it turns
out that there is a collection of
weaknesses shared by both of these
paradigms and I wanted review five of
them so the first one I want to think
about is the problem of the assumption
of stationarity so the whole idea of
learning from data is that we can
collect data that's going to be
representative of the queries that we're
going to have to answer and we have
lovely machine learning theory that
tells us if the test queries are drawn
from the same distribution as our
training data that is our training
distribution and our test distribution
are the same and if our learning
function or learn probabilistic model is
accurate on the training data then with
high probability it will also be
accurate on those test queries so this
is a beautiful thing but the problem is
it does make this assumption of
stationarity and we see violations of
this everywhere essentially right so one
that natural language people are very
from
with is that if you're trying to build a
parser you're probably going to be using
the Wall Street Journal corpus the penn
treebank well that's trained on the
corpus but maybe you want to use it on
social media and and so those are vastly
different in their structure another of
course is there there are trends in the
data so any kind of financial
transactions we might train on data from
the 1990s and then try to test on data
today where we know how cryptocurrencies
the world population has changed the
economy is functioning in different ways
and then it was mentioned before the
problem of adversarial learning that if
our adversaries are criminals for
example they're constantly changing
their tactics in response to our efforts
and enforcement and surveillance the
second weakness is the big data
requirement especially for deep learning
but but in general you know the more
complex the problem you want to solve
with machine learning the more data
you're going to need so in many
applications there is very little data
available in natural language processing
this problem of low resource languages
has been attacked a lot and born here
has been funding a quite a lot of
research in that area but but we could
see other situations novel weapon
systems or also just trying to make
inferences about a small subpopulation
within a much larger population where
although we might have millions of
people and and and billions of training
examples maybe for that sub population
of interest say young people are
vulnerable to to being radicalized we
don't have very much data in the
computer vision setting for object
recognition of their promising direction
is what's known as fine-tuning deep
networks so you can take a deep network
that was trained on say image net data
and then fine-tune the final layers
using the wealth of a small amount of
data so I have colleagues I do a lot of
work in ecology I have colleagues that
have been looking at recognizing insect
species okay from microscope images this
is very different kind of image
you would have an image net and yet
surprisingly you can get excellent
results like 90% plus correct
just by fine-tuning the image net
networks the third problem is biased
data collection and this is something
that the machine learning community has
only recently woken up to although I'm
sure every intelligence analyst could
have told them that this is a problem
the data are often collected through
some biased process and an example I'm
familiar with is we send bird watchers
out to try to get observations on bird
species some bird species are really
easy to see you know Cardinals are
bright red some songbirds just sing a
lot but others are dark brown and hide
all the time
and if you just use the number of
sightings from bird watchers you might
think those species are highly
endangered because nobody can seem to
find them but in fact there might not be
a problem another example that's been
allowed in the news lately is training
criminal justice classifiers say to
predict recidivism if we use the exist
the current prison population to do that
then we're replicating whatever biases
might exist in the current legal system
and another instance of this kind of
biased data came up in the Google photo
application so some of you may have seen
Google has a photo sorting app and it
mislabeled some black people as gorillas
because it has insufficient data a
training data for them now it'd be
interesting to look at what were the the
data sets that they used to train this
system but clearly there was not enough
data to correctly handle this sub
population within the larger a larger
training set there are the probabilistic
modeling framework there are good ways
of dealing with bias sampling so to take
another example firms from species
surveys there's something known as the
occupancy model that was developed out
here at Patuxent USGS lab and the idea
is just briefly that we're really
interested in whether species are
present so Z sub I is true
for particularly species is present at a
critical site and these are the features
of that site so we're very interested in
this manner which is the habitat model
for the species will send out observers
say capital T observers each one of them
not only has a certain probability D sub
I T of detecting the species and so we
get these very noisy observations y sub
I T and so it is possible to fit this
model to the data and figure out learn
the detection probability model and also
the species a habitat model
simultaneously so that's just one simple
example another example is there's been
a lot of work in crowdsourcing of course
we use Amazon Mechanical Turk and so on
to get people to to rate things or
debate movies so the data's hipping it
looks like a rain or by item matrix but
the items have unknown different amounts
of difficulty for the humans to rate
them say we're asked them to label the
objects and images and also the Raiders
have unknown reliability and some of
them might be actually just yessing
randomly or even behaving adversarially
so we can build a probabilistic model
that tries to model the the reliability
of each Raider and the difficulty of
each item and and it looks something
like this
an estimate both simultaneously the I
don't have lost track of my count first
fifth issue is the build list of
end-to-end training so it's been
mentioned this problem of adversary
examples how am i doing for time let's
see okay okay great so one of the early
papers showed that if you take a picture
of a panda which was classified by an
image net system with 57% confidence so
wasn't super confident about the Panda
but given that there are 999 other
categories that's still pretty high
confidence and then they add what
appears to be random noise but is
actually very carefully optimized
perturbation of the image and they can
convince the system that it's a Gibbon
with 99% confidence even though to our
eyes the image is unchanged and
similarly there's another group that was
a universal homing
that was able to use genetic algorithms
to come up with these crazy patterns
that also would be classified with high
reliability and there even people
selling t-shirts now that will make you
be recognized as a school bus and things
like this more as a joke of course but
but there was a stuff just in the news
this week about some guys I think in
Germany that had developed a set of
stickers you can put onto a stop sign
and then the the self-driving car system
would think it was a speed limit sign
instead so so this is this is a real
problem
another example brittleness is known as
the problem of open category recognition
so we train our system on a thousand
categories and the thing is that our
systems basically have the the mental
model of the world that there are only a
thousand kinds of things in the universe
right so if we show it some new object
that it's never seen before maybe an
iPhone or something it will it will fit
it into one of the thousand bins it
knows about it might be not confidence
but it also surprisingly might be quite
confident because the image might be
very far away from all that's so-called
decision boundaries so this is a plot
from some work by Shiro and colleagues
where they're comparing a measure of
system performance against what they
call openness which is basically the
amounts of test images that belong to
new categories and that we can see that
the sort of standard and and training
systems performance falls off quite
quickly as we introduce these they've
got some new algorithms something called
a viable SVM that's able to it doesn't
suffer quite as much from this but this
is an ongoing challenge for us the last
thing I want to mention is verification
validation and monitoring so the machine
learning and I and maybe it's just speak
for the machine learning community we've
been totally focused on how accurate can
we make our systems and we've really
ignored all the other ill --'tis that
engineers might be concerned with
testability diagnose ability reliability
maintainability and so on and so we
really need new methodologies for
establishing trust in these machine
learning systems or maybe I should say
more generally we
to somehow bring together the sort of
software engineering reliability
engineering communities with the machine
learning community because it's
happening in some companies and google
has published some very interesting
internal coursework and and guidelines
that they've developed but but in terms
of funding parties I think if there's a
way we can encourage those communities
to get together this would be very
important so there there are some
projects things happening now but but
things like verifying that our training
data really is representative of the
input domain what would be some
methodologies for that
verifying the absence of adversary
examples within a certain distance of
the data there have been some papers
using model checking so logical
inference to address that question and
there's a lot of existing work on how
can we detect that the input
distribution is changing and at least
raise the flag that our system is
probably not going to work well and you
can adapt to those changes although let
me take a slight detour into the
question of anomaly detection this is a
problem that has not been studied that
much in in the machine learning
community but it's it's are currently
becoming more prominent so let's assume
you have a collection of data that is a
mix of what we call nominal to avoid
confusing with the normal distribution
nominal data and anomalous data and
we'll assume these are our ID points and
our goal is to detect the anomalies so
over the last 10 years particularly in
the data mining community many good
algorithms have been developed for this
problem and one place they're being
applied is to actually screen for
adversarial and open category inputs say
to computer vision systems so like that
Weibull SVM actually uses a something
called a world-class support vector
machine as an anomaly detector in front
of the of the classifier so I was a
performer on DARPA program called atoms
for detecting insider threats and I have
to show off our results we had data from
from a corporation that had 5000
employees and
and then a red team had been hired to
insert onto some unsuspecting employees
additional desktop activity data that
where they were acting out some kind of
a an insider threat data exfiltration
threat or intellectual property theft or
something like this and the arrows are
for the Oregon State University
algorithms so in in G in the June 2013
challenge I'll show you the good case
right we did very well but the segments
were generally working well if you're
interested only in say the top 100 hits
if you're interested in finding 99% of
all the anomalies that's much harder but
if you are interested in the most
obvious ones we can do that and we have
ways of incorporating analyst feedback
to do that as well so I think that's
part of the key to making progress on
many of these things okay well let me
quickly do my capabilities discussion so
I think the current capabilities we have
we can apply deep learning to signals
type data if they're stationary and if
we have enough training data we can
rapidly adapt to new problems using fine
tuning and to some extent with anomaly
detection we can detect unusual
behaviors and potential threats in the
near term I think we're making good
progress on this open category of
classification problem we're making
progress on biased and untrusted sources
and so I hope that we could begin to
automatically detect or at least have
tools for auditing the bias in our
systems may be building Wyatt reusable
libraries of models of information
sources a big challenge is anomaly
detection on time-varying and network
data and and then there I think will
have some initial model methods for
validation and system monitoring and
then in the longer term really
successful defense against adversaries
examples is going to require a lot more
work and other huge challenges
integrating large knowledge bases such
as knowledge graphs with machine
learning we really don't have ways to do
that yet I think it'd also be very
interesting to capture the meta
reasoning to development test hypotheses
about data source reliability
so a good analyst can look at an outlier
and say that's strange I wonder if the
definition of this feature changed and
then they can do some they can compare
other things from that same information
source and make inferences about what's
been what's going on that there are
current systems cannot and so I think
another challenge is multi scale and one
shot at zero shot occurred computer
vision systems for example if we just
changed the dimensions of the image
slightly the performance degree is quite
shockingly and that's certainly true for
anything else since a reinforcement
learning okay if you have any questions
please push this button so it can beat a
cop how much is the learning limited by
the computational horsepower the
computational horsepower was really
critical to being able to process more
data so in some sense it has it is has
been limited by that and of course this
is why Nvidia stock is way up and and
also we see the Google developing
specialized things like tensor
processing units and other companies in
that same space so improving the the
hardware is there's a lot of heaven
there I think we can make a lot of
progress there and the better the
hardware will use it up with with data
so so you know the machine you guys are
like sponges for this yeah thanks Tom
really wonderful overview thank you
throughout that you know all of you or
your survey of the technology and
examples of use and even overcoming
challenges seem to be limited to
application of machine learning within a
particular modality of data and
considering the focus of this workshop I
wonder if that implicitly says something
about the readiness of machine learning
to handle multimodal data heterogeneous
data sources sort of on their own yeah I
would say from inside the machine
learning community you're exactly right
we are developed point solutions to a
single data source I'm hoping that maybe
some of the computer vision and natural
language folks can say a bit more about
fusing that with multiple sources but
you're absolutely right that I if you're
looking for expertise in that probably
the machine learning community is not
the place to look first any other
questions yes please the question was
could I say a bit more about multi scale
well let's let's think about something
like change point detection typically
our system was there would you would
have some sort of a pair of sliding
windows overtime of a fixed length and
you would compare the distributions in
those two windows but but that would
only detect changes that are happening
at a certain rate and you will really
need methods that would do that at many
different scales and then make the
appropriate compensations and the same
goes I think for object detection in
computer vision basically usually
sweeping some sort of a window across
the image and you're going to find
objects of a certain size and there
there has I mean I would say that that
the best work in multi scale is there
but the same in reinforcement learning
we usually assume there's a fixed time
clock ticking and we get to make one
action every so many seconds if you tell
us that that we're going to make it be
doing making actions over multiple
orders of magnitude the things just
don't work at all so</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>