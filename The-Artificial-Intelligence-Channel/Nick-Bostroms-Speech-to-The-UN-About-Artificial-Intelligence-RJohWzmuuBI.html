<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Nick Bostrom's Speech to The UN About Artificial Intelligence | Coder Coacher - Coaching Coders</title><meta content="Nick Bostrom's Speech to The UN About Artificial Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Nick Bostrom's Speech to The UN About Artificial Intelligence</b></h2><h5 class="post__date">2017-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RJohWzmuuBI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">machines get smarter and this is
something I'm going to address it to
Nick Bostrom and I will introduce him
he's a professor at the Faculty of
philosophy at the Oxford University he's
the founder of the found founding
director of the future of humanity
Institute the author of The New York
Times bestseller super intelligence the
book and he was named one of the
foreign-policy Magazine's top 100 global
thinkers Nick please tell us what
happens when machines get smart get
smarter thank you very much for the
invitation and everybody who is
contributing to making this meeting
happen so just while we're getting the
PowerPoint slide up III can say
something in general about so I want to
sort of expand on some of the things
that max were saying in his talk and and
this grandiose renamed a research center
that that around the future of humanity
Institute we see ourselves as in the
business of of trying to put a little
acceleration on into the wisdom side of
this race between wisdom and
technological capability so to start
with I want to introduce a concept that
that we find is useful for organizing
our thinking when you're really zooming
out and looking at the human condition
from a high altitude and look at the
really big picture this concept of a
backhaul and existential risk there's
never been an existential catastrophe in
all of human history and there will only
ever have been either 0 or 1 so an
existential risk is one that imperils
the survival of earth originating
intelligent life but I could permanently
destroy our future so all the things
that have gone wrong in human history
all the wars and earthquakes and plagues
from from this strange perspective or
sort of like me ripples on the great
pond of finality when you thought up the
total amount of suffering and happiness
at the end of time these might not
really register where as an existential
risk would be important in that context
so we define it as a risk that threatens
the premature extinction of earth
originating telenet life or the
permanent drastic destruction of its
potential for desirable future
development so this focus is our 10
we have this very wide mandate the
future of manatees today that could be
anything pretty much but when you put on
the lenses of focusing on existential
risk like almost all the concerns that
preoccupy the world's population fall
away because they just aren't possible
existential risks in there and a very
small number of concerns remain which we
can divide broadly into two categories
so risks arising from nature and risks
arising in some way from human activity
one early finding of this field of
existential risk studies is that all the
really big existential risks certainly
if we're talking about the timescale of
a hundred years or 200 years are in this
anthropogenic category you can see this
quite easily if you just reflect on the
fact that the human species has been
around for a long time we have survived
earthquakes and fire storms and plagues
and asteroid impacts for a hundred
thousand years so it's just not very
likely that any of those things will do
a scene within the next century
whereas we will in this century
introduce entirely new phenomena new
factors into the world so if there are
going to be existential risks in this
century they're most likely to come from
these new things that we will do and and
and most of the possible ones here have
to do with anticipated future
technologies and another way to look at
this is is to consider this metaphor of
a giant urn full of malls and and you
can sort of see human history as the
process of reaching into this urn and
extracting one ball after another these
Falls represent ideas technological
discoveries the products of human
creativity and throughout our tenure
here on this planet we have extracted a
great number of these balls and most of
them have been good some of them have
been mixed blessings none has been such
that it has spelled our disaster we
might wonder what would it be like if
there were one of these black balls in
the urn is there some possible discovery
some technology that could be invented
such that it invariably spells the doom
of the civilization that discovers it it
could run a kind of counterfactual
thought experiment and think back 100
years ago eight years ago before you
weapons have been invented and you can
ask yourself what would have happened if
it had turn out that instead of
requiring highly enriched uranium or
plutonium like a really difficult
processes to unleash the power of the
atom what if it had been some simple way
something like baking sand in your
microwave oven or something like that
all right so so now we know that you
can't have a nuclear weapon by breaking
sound in your microwave oven but before
we did the relevant physics how could we
have known how it would turn out like it
could have turn out like that and in
that scenario that might well have been
the end of human civilization at that
point because if anybody just by doing
some simple thing that they can do the
kitchen could wheel the destructive
power to kill millions then it might
just be impossible to have cities and
concentrated populations and so forth
but nuclear energy turned out not to be
a black ball but maybe maybe gray ball
instead so it looks like our strategy
currently is to continue to pull balls
out of this urn and just hope that there
isn't a black ball in there because if
there is we will eventually pull it out
and then that would be the end of it we
have a lot more ability to invent things
than to uninvent things so so this is a
general reason also for thinking that
the biggest existential risks over the
course of a century might be from
possible future discoveries that we
might make and I've put up a partial
list here of some of the perhaps more
likely candidates for areas where
existential risks might emerge there are
several things to notice about this
there's also all of these technologies
here have great potential for beneficial
uses which and paradoxically is one of
the factors that makes them go higher up
on this list because it increases the
likelihood that we will actually develop
them if there was some technology whose
only use was to cause the destruction of
humanity then maybe we would have a
greater likelihood of steering clear of
that but if it's something that has wide
beneficial impacts for health and
environment and economy chances are we
will eventually develop these another
thing to notice about this this is that
at the bottom there I've I've put in
some unknowns so if you think again back
a hundred years ago and
consider what the answer would have been
if it at that time would have asked what
are the biggest existential risks over
the next couple of centuries then none
of the ones that we might now be tempted
to put near the top of this list would
have been mentioned I mean certainly not
machine in heaven if they didn't have
computers synthetic biology wasn't the
concept nanotechnology was not a concept
that might have worked some about
totalitarian tendencies but for the most
part what now seems to be the biggest
risks are ones that have only in recent
decades popped up on the radar and there
might yet be others that we haven't yet
conceived of which is one reason why we
think there is potentially a high value
in in doing this kind of research just
in case we can find something else that
we might be able to do something about
so now let me transition to speak more
specifically about possible concerns
from the future of artificial
intelligence at the very most basic
level the the the point is this that
intelligence is an extremely powerful
thing it what makes the difference
between the human species and and our in
many respects very similar relatives the
great apes that that share most of our
ability and only in very recent
evolutionary time has departed somewhat
and and these small differences in our
brains have resulted in all these vast
differences in in our ability now to
shape the future of the planet Earth
it's our small increases in intelligence
that have enabled us to develop this
modern technology and so forth
and it therefore seems possible just
just even at first sight that if if
there ever were a time when machines
became as much cleverer than we are as
we are than other animals then that
those machines could be a very powerful
shaper of the future maybe they would be
able to shape the future according to
their preferences and then that this
therefore it seems to be a topic that is
worth transferring out of the domain of
Hollywood movies and science fiction and
kind of entertainment and into the arena
where academic researchers can begin to
think about it as a topic where the goal
is not to have fun and be entertaining
but where the goal is to develop like
increasingly accurate
leave and proposals so max was already
mentioning some of the advancements that
have been made some milestones that have
been crossed if we look under the hood
behind these applications then we see a
great number of developments in
algorithmic techniques that have
occurred and pretty much all of these
really only since you know in the living
memory of a lot of people alive today I
mean the computer is still quite young
and so if we think about how far we have
come in these past 70 years and it makes
one realize that within the lifetime of
us or our children that we might come
perhaps all the way in addition to these
advances in algorithmic design and
architecture there have always been
developments in in hardware and if you
look at particular domains such as
computing you find that roughly half of
the improvements in performance have
been due to computers getting faster and
half due to better algorithms and and
that as a rule of thumb seems to be true
across the board that both hardware and
software are contributing roughly
equally in recent years I think maybe
the reason two or three for years to
have been a new sense of excitement in
the world of artificial intelligence a
sense of having comment on stock that
the field was kind of stagnating a
little bit before but now particularly
with developments in its known as deep
learning and some other techniques there
is a sense of renewed progress a lot of
exciting frontiers to explore also
reflected in industry activity with some
some high-profile acquisitions and kind
of war for talent among some of the
large software companies of the world we
find artificial intelligence already in
wide application throughout the economy
I'm not going to read off the whole list
but a lot of the inventions that were
originated in artificial intelligence
research laboratories we no longer tend
to think of as artificial intelligence
wants to actually work that just becomes
software and this is sometimes
frustrates a guy researchers that they
don't kind of get credit for all the
things that that have been accomplished
but but but AI techniques are in white
to produce already and that that list
will continue to grow longer if we look
for example that
gay mayor as one particular area where
it's easy to compare human and machine
performance we find that machine new
talents already in in many games perform
as well as or better than human beings I
think that the next big game were where
computers will exceed us will probably
be the game goal which is kind of the
Asian equivalent to chess a big organ
great complexity some challenges that
that remain today is better methods for
transfer learning this is the kind of
technology we need to be able to use
insights that you learn from solving one
problem and then apply them in a very
different area and this is still
something of a challenge that AI
researchers working on concept learning
more flexible reasoning with learned
concepts as opposed to just sort of
symbolic tokens that don't mean anything
long-range hierarchical planning reading
and more complex system architectures
like tea you might get a slightly
different this depending on which AI
researcher you ask but but these are
certainly some of the major outstanding
challenges that stand between where we
are now and replicating the full
functionality of the human mind learning
ability and planning ability that makes
the human mind so powerful so reflecting
on these developments I think as Mark
said that it's very important to make a
clear and emphatic distinction between
the near-term and long-term
both of these contexts have serious
legitimate challenges and opportunities
to think about but they're quite
different so in the near term we have
issues such as autonomous weapons that
next mentioned we have of course non
autonomous applications of these you
could have in many situations perhaps
the human making the final decision by
pressing a button but with a lot of AI
assists image processing etc you have in
a very different direction
people are thinking about the impacts of
automation on the labor market and
whether
the problems with chronic unemployment
that one is beginning to see in some
countries have something to do with with
that or whether it in fact has to do
with completely other things like
offshoring of labor or the economic
cycle but as machines become more
capable this is likely to become a
bigger issue surveillance and data
mining of course cybersecurity
self-driving cars have issues for
regulators like exactly what will the
legal frameworks be for allowing these
on the road and and a bunch of other
things and these issues are quite
different from the issues that arise if
we ask the question what happens if AI
actually succeeds in its original
mission which has all along B not just
to create domain-specific applications
little tools here there but actually to
do all the things that the human mind
can do and that's obviously farther off
but also the implications are much more
profound so we did a survey of some of
the the world's leading experts a couple
of years ago on one of the questions we
asked was by what year do you think that
there is a 50% probability that human
level machine intelligence could be
achieved which we defined for the
purposes of this survey as the ability
to perform most jobs at least as well as
a normal adult so so real genuine human
level machine intolerance and as you can
see the median answer to that question
was 2040 or 2050 depending on precisely
which group of experts we asked and that
estimate should be taken with a large
amount of salt in that it's based purely
on the subjective impressions of people
expert in the field but there is really
no science that enables us to predict
with accuracy how long these kinds of
developments will take it could have
been much sooner or it could take a lot
longer so I think instead of a
particular year think of a probability
distribution so smeared out over a wide
range of possible arrival dates there is
a a different question also about timing
but which must be distinguished from the
first so far I asked about this kind of
first era there on the horizontal axis
time until take off like how
how long between now and human level
machine intelligence there is a second
question if we ever do reach that level
how long between that point and until we
have something that is radically super
intelligent and you might be quite
pessimistic or optimistic depending on
how you look at it but you might think
that it will take a long time before the
field of artificial intelligence will
actually reach human level maybe you
think that these opinions about the
practitioners are biased maybe they want
to believe that their field is really
important and it will succeed maybe you
think it will take a hundred years
rather than 50 years or more
you might nevertheless still think that
if we ever do reach that level that the
transition to super intelligence will
then happen quickly and in fact that is
my view that it would be harder to get
from here to human level than to get
from human level to 2 radical super
intelligence one way to think about it
is is this intuitive we will have this
notion of smart and dumb that maybe it
looks somewhat like this we think at 1
and we have like the village idiot
completely hopeless
bungles everything and at the other end
you have sort of your favorite
scientific guru what Einstein or Ed
Witten or something and these kind of
define the extremes of human cognitive
performance with regard to how difficult
it will be for artificial intelligence
to achieve a particular level of
performance however I think that the
picture will look more like this that we
start at the left of this diagram with
zero capability when we invented
computers let's say zero artificial
intelligence and then slowly over time
the AI train moves along this track and
after many many decades of really hard
work by a lot of researchers perhaps
eventually we reach mouse level
artificial intelligence something that
maybe can navigate a cluttered
environment about as well as a mouse can
and then after a lot more work we reach
chimp level and after a lot more work
beyond that which we don't really tell
yet level but I don't think that at that
point the train will slow down I think
in the soosh past human will station the
brain of the village idiot in the brain
of Albert Einstein are almost exactly
identical
same size same number of neurons more or
less the same biology
there's no particular reason to think
that it will be got harder to match one
than to match gathers so to wrap up so
what I have argued I recently wrote
about a book on this is that we then
will confront this control problem which
is the problem of assuming it could
solve the intelligence problem like how
can actually make machines intelligent
like how could it then ensure that these
very intelligent machines will be safe
and beneficial to humanity and I argue
that this raises unique challenges
technical technical challenges and
foundational challenges that there are
possible scenarios in which super
intelligent systems become very powerful
and for the reasons I alluded earlier
like intelligence is a general-purpose
thing if you have nothing telling us you
can invent all the other technologies
you don't already have and and also as I
described in the book there are these
superficially plausible ways of solving
the control problem ideas that
immediately spring to people's mind that
on closer examination turn out to fail
and so there is this open currently
unsolved problem of how to develop
better control mechanisms that is more
difficult because it will need to be
solved before we actually have these
fully intelligent systems by that time
we already have the solution so so I'm
very glad that people like Elon Musk are
stepping into the breach here where
there has been a complete funding vacuum
until recently and and that some
activity is beginning to happen and and
I recommend that that we sort of
accelerate this work of establishing a
field of inquiry to do foundational and
technical work on the control problem
and in recognize that as such as we
stinked legitimate academic endeavor
that some small number of the world's
best brains should be working on them
just as so many other things something
started by academics that we should try
to attract top mathematics and computer
science talent into this new field that
we should build strong research
collaborations between the AI safety
community and the AI development
community both in industry in academia
because ultimately the path to success
is that whatever ideas for safety are
developed also get implemented and both
of these need to learn from one another
rather than take up antagonized
positions not in long-range scenarios
and planning
we should consider a superintelligence
as a possibly important factor in
shaping humanity's long-term future this
does not commit one to thinking that
this is just around the corner that we
should hold our breath and be like super
excited about every single announcement
in the media but but if you're really
thinking long-term about humanity's
future decades out then I think this is
a legitimate thing to take into account
and finally that it is important to
integrate into this research community
and into society's thinking about the
long-term future of artificial
intelligence that this is a unique
technology that should be developed only
for for the common good of all of
humanity it's too big to just be thought
of as something that will raise the
profits of one firm a little bit or give
one country a slight edge this is really
a concern for all of us everybody in the
world if this is developed will share in
the risks whether they like it or not
and it also seems fair that everybody
should stand to gain if things go well
and then have a slice in the upside
thank you very much Nick thank you very
much for this absolutely great
presentation and and one of the
recommendations</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>