<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Joelle Pineau - The Basics of Reinforcement Learning | Coder Coacher - Coaching Coders</title><meta content="Prof. Joelle Pineau - The Basics of Reinforcement Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Joelle Pineau - The Basics of Reinforcement Learning</b></h2><h5 class="post__date">2017-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/313kbpBq8Sg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great thank you for joining us bright
and early on Monday morning what is for
some of us Canadians a national holiday
I hope I will make it worth your while
to have gotten up early quick thing to
kind of get us warmed up how many of you
were with the deep-learning summer
school last week ask them how many of
you went to the Jazz Festival over the
weekend on a cruise last night all right
who is joining us for the first time
this morning excellent you guys missed
out a little bit on last weekend that
weekend but look around there's tons of
people who were here last week they can
tell you probably where to eat what are
fun places to visit in Montreal you're
not necessarily going to need the
knowledge that was shared last week in
terms of technical knowledge we are
starting afresh this morning and so I am
starting with really the basics of
reinforcement learning but we have a
fantastic lineup of speakers coming in
the next three days for those of you who
are the deep learning summer school last
year we had one day of an RL over the
six or seven day program and because
their interest in RL has really been
growing this year we decide to extend
this to three days so I sent out
invitations to my favorite RL
researchers in the world and I'm so
happy to say that most of them accepted
the invitation are going to be here this
week a few of them had to say no we've
got them lined up for next year but we
have really a spectacular I think lineup
of speakers the schedule was down a
little bit last last night or this
morning and so I have it up here in case
you were looking for it I'm gonna give
you a first hour and a half session this
morning that will be really the basics
of reinforcement learning so if you're
one of these people who feels like
you've never learned anything yet about
reinforcement learning I am here for you
I've got your back covered you can ask
questions we're going to go at the pace
that you want and after that we're going
to be launching in some of the more
advanced material on Peter I bill rich
Sutton will be here later today and at
4:30 there's a part
sessions that is a little bit in
evolution but essentially a few of the
graduate student from the reasoning and
learning lab will come and show you some
of the tools environments that they've
been using in their research that they
find the most helpful this is really
geared towards people who haven't done
research in reinforcement learning and
are thinking like where should I start
right what's the equivalent of em nest
for reinforcement learning how should I
be training my first agent what does the
training loop looks like we don't
usually have a training set in a test
set in RL how do I think of training and
testing in these kinds of contexts so
that's really the purpose of the
practical session this afternoon
tomorrow morning we have two of the deep
mine researchers coming in telling us
about what they have been building doing
research on some of the tricks that went
into building the very famous alphago
system chiba-chiba serviceberry is one
of the best reinforcement learning
theorists so he'll be there in the
afternoon and then we have more special
topics on Wednesday all day and so I
encourage you to come all the way
through the week there is coffee breaks
and things like that I just cut it out
in terms of space so today I'm really
giving you the sense of what are the
basics for reinforcement learning this
summer school was I should say conceived
up and planned with my colleague join a
pre cup who's also a well known
researcher in reinforcement learning
unfortunately doina was called away this
week due to a family emergency so she's
not going to be around but much of the
work that went into the planning was
encounter collaboration with her so when
we talk about reinforcement learning
right let's start from sort of the
intuition of what we're talking about
the the idea is inspired by some of the
concepts that are coming from psychology
and the idea is really that you can
learn trainer system by allowing it to
perform different actions in its
environment and by observing the effects
of its action there's some feedback
system that sort of tells the agent when
it's performing good actions and bad
actions and so any of you who has a pet
may be
child maybe even a spouse it's quite
familiar with this kind of situation
right you try out different props
different actions see what kind of
response you get sometimes you give
positive reward and sometimes you give
negative rewards and you try to shape
the behavior of the other agent using
these kinds of guiding principles and so
when we do this in computer science and
with machine our insight is to try to
use these same kinds of notions to try
to train our agents and so the idea is
that through multiple actions the agent
can learn to shape its behavior based on
a reward mechanism probably for many
years the most famous reinforcement
learning system was a system called TD
gammon developed by Jarrett asaro in the
1990s and in this case the system had
been trained to play backgammon at a
world-champion level essentially through
self play so the system was not trained
by playing actual people it played
against itself through many millions of
games and over time learned to be to
human players and most recently we've
had some other spectacular successes of
reinforcement learning particularly the
alphago system where it was shown that
again an agent trained with
reinforcement learning can learn to play
the very challenging game of go and do
so at a level exceeding the best human
players there was some very nice results
in 2016 where it'd be one of the top
players but maybe not the world champion
then most recently in 2017 again in a
set of matches in China we saw the
system beats that really well
acknowledged some human world champion
and so hopefully later this week we'll
hear about some of the more advanced
reinforcement learning techniques that
went into the system for alphago a
couple weeks ago I was at the RL DM
conference R liam is reinforcement
learning and decision-making and I
quickly glanced through the proceedings
I only made it through about half of the
proceedings checking what are some of
the recent applications of reinforcement
learning it's not all about games and
this is a brief list
the applications like I could find by
just scanning titles and abstract and I
only got through half of the accepted
presentations and so you have
reinforcement applied of course to
robotics video games conversational
system right chat pad type of agents or
one other popular applications trying to
train medical decision systems but you
see some more interesting things like
improvisational theater showing up there
and so there's really a little bit of
something for everyone's taste if you
are interested in that one of the things
I would like you to get out of today's
session is some insight about when to
think about applying reinforcement
learning many of you from what I
understand don't necessarily come from a
reinforcement learning research lab and
you have a lot of expertise probably in
doing supervised learning maybe some
unsupervised learning so what I would
like to help you develop is some insight
for what problems should or must be cast
as reinforcement learning problems and
what kind of problems can we handle
using a supervised learning framework so
one of the in clues that you might be
reeling with a reinforcement learning
problem is that your data comes in the
form of trajectories so there's a
sequence of observation and there's a
dependency between these observations so
that usually means that you're very
precious IID assumption is not going to
hold you have to take into account the
fact that there's dependencies between
these observations that's not quite
enough it's a necessary condition but
it's not quite enough because we have
time series data for example if you're
observing a set of weather patterns or
stockmarket patterns in those case you
have data in the form of sequence but
it's not yet a reinforcement learning
problem so the other thing that you need
in a reinforcement learning problem is
the need to make some decisions or
interventions so there's a notion that
you're going to take actions that are
going to affect the sequence of
observations that you get so not only
does the data come in the trajectory but
there's a means to affect the course of
that trajectory through a set of
actions decisions interventions these
are usually the two indices the two
pieces of information you need the third
one which is also crucial is that you
need to have a way to observe feedback
about their choice of actions so there
has to be some mechanism through which
you can assess whether an action should
be positively rewarded or negatively
rewarded and typically that reward isn't
a binary signal it's not a zero one but
it's a real number that quantifies how
good or how bad was the action so it can
be any real number we usually assume
that it's a bounded signal and so as
long as you have that signal that gives
you information or the choice of action
then you have all of the pieces you need
for reinforcement learning and I would
add one of the things that reinforcement
learning does quite nicely is dealing
with sparse rewards so you don't need to
have direct feedback about each action
that is applied in the system if you
think of training an agent to play a
game like backgammon go chess you don't
need at every move to receive a reward
signal that tells you that was a good
move that was a bad move it's enough to
have reward signals at some point along
the trajectory and in the case of games
usually that reward comes at the very
end so you're either win the game or you
lose the game and that's the reward
signal that you get so sparse reward
along the trajectories are fine but
there has to be some reward to drive the
learning if there's no reward that is
observed at all it's very hard to learn
anything you're not going to be able to
shape the behavior of your system in the
last bit that I note and this one isn't
so if you meet the first three criteria
you're probably on on good grounds to
look at your problem as a reinforcement
learning problem the last one though
that is useful to think about is that
this is typically used for cases where
your problem requires both a notion of a
learning right you have some information
that you can use to assess the quality
of your decision but also that there's a
notion of planning and so you're
interested in making a sequence of
decision and maybe there's interaction
between the choices of decisions along
the way if all of the decisions can be
independently typically it's not
required to move to the reinforcement
learning framework so these are sort of
some of the things that you can look at
for those of you who are most familiar
with supervised learning I like to make
this parallel where in supervised
learning you have a notion of inputs and
outputs and in reinforcement learning
your inputs are typically called States
and your outputs what the agent chooses
are typically called actions in
supervised learning you do have
something akin to a reward function
right it's your target signal so it may
be the correct class right your
supervisor learner predicts an output
but then you get to look whether that
was the actual output that we wanted
that's your label in reinforcement
learning instead of a label we have a
reward so this is this number that says
how good or how bad was your choice of
action and because you're dealing with a
system over several instances there's a
feedback loop that says depending which
action I took imagine an agent that is
playing chess depending what action the
agent took there's going to be an effect
on the environment it's gonna change the
state of the environment that effect
will be fed back in the next state and
so this feedback loop this dependency
between the state at every time step is
one of the reasons why we can't make the
ID assumption and it's one of the
reasons why in some cases some of the
analysis that we have to conduct with
reinforcement learning gets a little bit
more complicated or requires a different
set of tools than what you would do for
supervised learning and so there's some
several practical technical challenges
that come along in particular with the
fact that we have this feedback loop one
of them is the fact that typically to
train a reinforcement learning agent you
can't just look at a static data set
it's not possible to have just a
training set because you need to have a
sense of how the actions are going to
affect the environment and that effect
can only be measured from an actual
environment there's still some
possibility to train directly from our
data set if you have data set that
essentially covered all the choices of
actions in all states in sufficient
quantity that you can estimate
stochastic effect and so that's a pretty
strong assumption right it's saying
assume that my agent has been in all
states and tried all actions infinitely
many time then I can do reinforcement
learning from a fixed data set in
reality that that's those conditions are
only going to be satisfied for very
small simple domains and so most of the
interesting domains that we're working
in including all the domains that I had
listed from this are ldm conference most
of them are cases where you can't learn
directly from a static data set you need
this kind of interactive system so it
means you need an environment to try out
your actions in some sense this first
point is one of the reasons why the rate
of progress in reinforcement learning
has been considerably slower than in
supervised learning in terms of sharing
knowledge between different research
groups it's very easy to send over a
data set nowadays you just put it on the
web people download it you're doing like
your data set if I'm doing reinforcement
learning research I have a dynamic
system maybe it's a robot maybe it's a
biological system maybe it's a set of
them for example a stock market type of
situation it's hard to encapsulate that
and send it over and so one of the
reasons most of the reinforcement
learning successes and research has been
done with games is because in the case
of games we can actually encapsulate the
dynamics of that environment write out
the rules of the domain in a way that is
concise and share that between research
groups and so that enables facilitates
progress so we need access to the
environment we need to plan and learn
simultaneously and finally the last
point is the fact that because we have
these dependencies right this feedback
loop the data distribution also changes
over time as a function of what actions
are taken so if you change your action
strategy you're going to see different
things right if you build a robot that
is always going right then the robot is
gonna see a certain part of the world
and now you suddenly make the robot
always go left it's gonna see another
part of the world so depending what is
your choice of action you see a
different distribution of the
and you need to take that into account
when you're doing learning so you can't
necessarily assume that your data
distribution is stationary as you would
for a standard supervised learning so to
formalize things a little bit more we
typically cast reinforcement learning
problems using Markov decision process
so Markov decision process are formally
specified by the following the set of
states so that it defines the space of
the problem these states can be discrete
they can be infinite they can be
continuous but you need a set of states
to be defined there's a set of actions
typically discrete but they can also be
continuous and we have a probabilistic
distribution that describes the dynamics
of the environment and so we make
specifically the assumption that the
probability of seeing a state is fully
defined conditioned on the previous
state and the action that was chosen
we'll talk a little bit more about that
assumption in a few seconds but that is
our model that defines how the agent
changes over time
there's the reward function that's a
real number defined for each state and
action pair and typically in some cases
we also assume an initial state
distribution so that can be the starting
state if you're playing a game right the
initial starting distribution is just
the initial state of the board
trajectories always start from that
state but for some other systems robotic
systems and others you can actually have
a distribution over a possible initial
state and so let's visit again the
assumption that we're making when we
specify our transition model right I
said that the probability of the next
state is fully defined by the previous
state in the previous action and this is
known in the literature as the Markov
assumption the Markov property in this
case more generally if we didn't want to
make such a restrictive assumption we
could assume that the probability of the
next state depends on the full history
all the state that the system has
visited before and all the state all the
actions that have been taken previously
and so this would be a system that has a
much different structure in terms of the
and probability and so some of the
reason we make this Markov assumption
one of them is really just to limit the
complexity of the model that we're
looking at we're looking at this dynamic
model and we want to learn this from
data if we start having a probabilistic
model that depends on full histories
then it requires a lot more information
to estimate accurately now I haven't
told you necessarily how you need to
define your state and so essentially you
have to think of the state as the
sufficient amount of information that
you need to predict the future that's
probably the best way to think of how to
define your state space so if we look at
simple systems that we've seen before
right all of you have seen traffic
lights before if you are faced with a
traffic light and the light is yellow
you don't need to think really long
about what was the previous states of
this traffic light to know what is
coming next right it's going to go to
the red light and so in that case the
color is sufficient to predict the next
color so we would say that this traffic
light meets the Markov assumption chess
is another one of these cases if the
board is in a particular configuration
and you take a particular action then
you know what's going to be the board
configuration one step down the line in
both of these cases these are
deterministic system right so if there's
a state and action you know what the
next state is in reinforcement learning
we don't necessarily assume that there's
a deterministic transition we can handle
stochastic transition where there's a
probability distribution but the point
here is that the information is fully
contained one classic game where often
the Markov assumption is not respected
as a game like poker right you may see
some information about the game there's
some cards on the table maybe some bets
have been placed and now you're trying
to determine what the next state based
on how you're going to play that can be
affected by a lot of prior information
and if you look at poker using a very
narrow notion of state then you're
probably not going to do very well and
so if you want to have a state
has sufficient information to predict
the future for a game like poker you
probably need to fold in other pieces of
observation from your previous play
history and call all of that the state
so the state can incorporate information
about what other players have done in
previous rounds of the game and so the
objective when you're dealing with a
reinforcement learning problem is to
look at some trajectories and over the
trajectory try to choose actions that
are going to yield good reward and so if
you choose several actions that I'll
yield good reward then you're probably
doing well and so there's two classes of
tasks and depending which class of tasks
we look at this notion of a return is
defined slightly differently so in the
first class we are dealing with episodic
tasks so these are tasks where we know
the task is going to end there's a
finite horizon things like games right
you're going to play and at some point
the game will end so we know there's an
end point in those cases the return of
the trajectory so a trajectory is the
whole sequence of action from the
initial state to when the game ends the
return of that trajectory is the sum of
the rewards over the trajectory in the
case of continuing tasks these are tasks
where we don't necessarily know whether
there's going to be an end point then we
look often at an infinite sum so we
assume that the task can go on forever
and we still want the return to be the
sum of rewards over that potentially
infinite sequence maybe you're building
a robot to juggle balls and you don't
want to say you know and after 10 rounds
of this juggle as long as you can and so
the danger in defining the return as
we've done for episodic tasks in the
case of a task that goes on forever is
you could essentially have an infinite
return right infinite return might sound
great if you're you know thinking about
your financial investments but in
practice for analysis of the ability of
these systems infinite returns tend to
be not so convenient and so in the case
where we're looking at continuing tasks
with potentially infinite length
horizon we introduced the notion of a
discount factors often it's always
denoted by gamma by convention and so
gamma is this discount factor that is
added in front of the reward as a
multiplicative factor in front of the
reward on each round and it's increasing
in the sense that gamma is typically
below one and so it reduces the
influence of the reward over time and so
this discount factor always bounded
below at zero about it above that one
typically we keep it close to one and
what happens if we look again at this
equation over here if my gamma is equal
to one I get back my return for the
episodic task if I set my gamma to zero
what happens is all of my future rewards
essentially don't matter and so I'm a
very myopic agent that is only picking
actions to maximize the immediate reward
and I don't care about long term
consequences and by balancing gamma
between zero and one I can essentially
trade off how much do I care about
immediate reward versus how much do I
care about long-term reward and so
there's some natural intuition about
this discount factor in some sense right
if I were to offer you a certain amount
of money today right I is meet you I
offer you a hundred dollars today and
then I ask you would you rather get have
a hundred dollars today or maybe 120
dollars tomorrow right some of you might
prefer the hundred dollars today because
you don't know if I'm gonna show up
tomorrow or not and some of you who have
a discount factor close to one and are
thinking well I'd rather wait till
tomorrow it was a pretty trustworthy
person she's gonna come she's invited
all these speakers let's just wait till
tomorrow and get the hundred and twenty
dollars and so the discount factor
allows you to mitigate essentially this
these different these different
considerations between short term and
long term according to a particular
criteria yes yeah
yeah so in in this case where there's
sort of a geometric decay that's a given
by gamma mm there is very convenient
mathematical reasons why to put it that
way and in particular we can show
convergence of that some with this
particular formulation of gamma
depending what decay rate you assume you
may not have convergence of that sum and
so it's convenient from that point of
view and so most of the reinforcement
learning theory has all been derived
with this but there's potentially other
formulations you could look at so one
more piece of notation that is going to
be very very useful is the notion of a
policy so the policies typically denoted
PI I'm going to stick to PI and
hopefully my fellow speakers throughout
the week are also going to use PI to
denote the policy this is how we define
the selection strategy so I've alluded
to the fact that you want an agent that
picks good actions and so we need to
define a way for a formulation for how
is that strategy going to be defined so
this is the policy we can have a
stochastic policy so that means that we
have a probability of picking different
actions at a given state PI SMA is my
stochastic strategy or we can have a
deterministic strategy in the second
case the deterministic strategy then we
have a mapping from state to action so
the policy is not part of the
environment the policy is the thing that
you're learning when you're learning an
RL agent right you're not given the
policy the policy is what you're trying
to get out of learning and planning and
so there's different cases where you may
want to have a stochastic policy or
deterministic policy and we're going to
review this as we get through some of
the more advanced concept and so more
specifically what I want in terms of
objective is really a way to pick the
policy of the space of all policies what
is the policy that maximizes my expected
total reward over the trajectory so I
have an expectation
over the policy so as I change my policy
I can expect to receive different
rewards and I have a maximization over
all the policies right for those of you
who might not have seen the Arg max
functions similar to the max but it
returns that argument which maximizes
the function so it says overall the
policy's return the policy that gives me
the best expected reward so just to make
this a little bit more concrete right
let's look at a familiar decision-making
problem for some of you I've cast it in
very simple terms right you have sort of
four different states you might be in
through the course of your career some
of you at some point
not very many given current conditions
are going to be unemployed some of you
are probably going to work in industry
some of your may be working or will work
in academia and some of you may be
currently in the future or have been in
the past attending grad school and so in
these cases there's different choices of
actions that you can take right you can
do nothing and doing nothing will
usually keep you in the same state that
you are including in academia except if
you're in industry if you do nothing
there may be some chance that you end up
somewhere else over here and so you can
apply to industry and most of the time
if you apply to industry from unemployed
or grad school right you might end up in
industry but presumably the
probabilities are not the same and any
of you are in academia and you apply for
industry then that's pretty
deterministic that you're gonna end up
over there these days and so we can add
numbers to this right we can add
probability distributions to each of
these actions and we can also add
rewards and I've posited some reward
function over there but the reward
function may be different for different
individuals and so now you can start
seeing that the choice of what action to
take really depends on that sequence of
things right if you're someone for whom
academia is highly rewarded then
probably you need to go through grad
school to make it to that
and so in this case I'm you need to take
into account both the transition
probability and the reward function to
figure out what is the best policy for
you and so in this case if I'm asking
what is the best policy
there's several policies that I can look
at right in each of my state I have four
choices of actions and I have four
states and so that means the space of
policies is essentially fortresses of
action for the first state times four
choices of action for the second state
times fortress so we have essentially 80
different actions ^ s my number of
states in terms of my space of policies
so that's a pretty large space of policy
and when I'm solving a reinforcement
learning problem essentially my
objective is to pick the best out of all
of these policies in this case I've
given you the transition probability and
I've given you the reward function so
from that point it's not so much a
learning problem it's more planning
problem right and so in the next half
hour so I'm going to review some
algorithms that are useful for planning
under the assumption that we know the
transition probability and we know the
rewards function that's not really
learning but it turns out to be really
useful to have this basis before we make
things harder and we start tackling the
case where we don't know the transition
probability we don't know the reward
function and we need to estimate those
from data but bear with me over the last
half hour or so while we talk about the
case where we know the transition of the
reward function so let's get back to our
notion of optimality in this case right
I mentioned that we want to look at the
expected return of a policy so I define
the expected return of the policy that
is defined at every state so depending
which state you're in you have a
different expected return over your
trajectory and so we defined a notion
called the value function the value
function by definition is this expected
future return and that's defined for
each of my state and so a very simple
strategy which I've alluded to for
finding the best policy would be the
following
step number one in numerate the space of
all policies right so I have four to the
four different policies I'm just gonna
list them all out in an attempt to
figure out what to do with my life then
estimate the expected return of each one
so I can look at the expectation for a
particular strategy what's my expected
return over all of this in this case I
would use a discount factor because
there's no notion of like a terminal
state right in the MDP that I've posited
over here there is no kind of you know
end state in retirement where you live
there forever and so in this case since
there is no termination point I would
use a discount factor so I estimate the
expected return of each of my different
policies and then I keep the policy that
has the maximum expected return right
and so this is a simple strategy it has
some limitations in particular because
the space of policies quickly gets very
very large as you have just a few more
states in action so it's not a practical
one but conceptually it's going to be
sort of based on that that we build some
more efficient algorithms so I want to
pause just for a second before we get
into these algorithms and make sure that
everyone's clear about these different
terms that sometimes get a little bit
thrown in together right I've talked
about reward right reward is the one
step feedback that you get so for a
particular action in a particular state
I get an atomic reward I've talked about
return return is the sum of rewards over
the trajectory I've talked about value
right the value is the expected return
over the trajectory so the return may be
one sample along a trajectory but value
is in expectation so if from a
particular state I take several of these
trajectories and I get a number that is
my expected return if you have a
deterministic system then the return and
the value are going to be the same but
if you have a stochastic system then one
trajectory will give you a sample of
your return but then when you run many
trajectories you'll be able to estimate
your expected value and finally we have
a notion of
Tilla D right I didn't talk about
utility very much in reinforcement
learning
we essentially assume that the utility
is the same as the return and so we
assume that we have a linear concept of
return so the return is the sum of the
rewards in many other cases of decision
theory there's a distinction between
value and utility and utilities can be
nonlinear utilities can have different
types of functions and so for
reinforcement learning we always make
this assumption that value function is
essentially utility is the same as value
we're not going to get into more complex
notion of utility for the purposes of
this there's some of the reinforcement
learning literature that does get into
that space for example methods that are
sensitive to risk or will have at the
end of the week notions of safe MDPs
in that case we look at slightly
different notions of value but for the
purposes of today probably the first
couple of days you can make the
assumption that utility is the same as
your value yes exactly
yeah and that's a very common way that
we've looked at now that I should say
that's coming in the literature in the
Latin you know everything I'm presenting
here is essentially known in the
literature for the last 40 years or so
yeah so let's look a little bit more
closely at the value of a policy in
particular line number one over here is
the definition I've given you so far
right the value of a particular policy
at a given state s is the expected value
over that policy for the sum of reward
given that you start in a particular
state s so I can rewrite this equation
by just splitting up the first term
compared to the other terms and now in
my first term all I want is to get the
expected value of my immediate reward
right the expected value of my immediate
reward depends on what's the probability
that I'm going to take a particular
action in that state and what's the
reward I get for taking that action and
so this PI s of a that's part of
my definition of the policy and over
here I've left all the expected sum of
rewards from the step T plus one and
they're on and so it's very common to
look at the value function and split it
up this way have the notion that you
have a first term as the immediate
reward and then you have another term
that's the future reward and so I've
bundled up all the future rewards
together and I partitioned out just my
first reward and when I do that I can
start looking at my expectation over
here and essentially I'm trying to get a
recursive formulation for the value
function to make it more concise because
right now my value function dip term is
depends on the full future and I'm
trying to get a recursive formulation
for that so if I look at this term over
here I have expectation with respect to
my policy PI of my future rewards
starting at t plus 1 but I'm still
conditioning on s T so if I couldn't pay
the term over here in the term over
there they're the same except in this
case I start in SP and I have our t in
this case I start in SP and I look at
our t plus 1 so I need to do a little
bit of work to express this expectation
condition on starting at t plus 1 and
the way to do that is simply take into
account what's the expectation of where
I'm going to be what's going to be my st
plus 1 and so to do that we just need to
fold in the transition probability I
need to I'm in a state s at time T I
want to know where am I going to be at
time T plus 1 well I need to factor in
my transition probabilities and the
probability of taking each of my
different actions so there's a
probability that I can take each of
these action that's defined by my policy
and given which action I took there's a
probability that it's going to take me
to a new state and so after I've folded
in this one step expectation the term
over here is in exactly the same form as
the term I had when I started with so I
can call this V PI of s prime whatever
state I'm going to be in and now I have
a recursive formulation where my value
function at this time step depends on my
immediate reward the transition
probability of being a new state and
the value at that state and so this is
essentially a form of a dynamic
programming algorithm where the value at
each of my state are the variables for
that dynamic program yes so in this case
The Horizon is just defined over the
lifetime of the system so it's the
expectation over that lifetime and so in
this case or it may be it may be a
deterministic right it may be set or may
be a random variable and you can redo
the same derivation with the discount
factor in there and then probably you'll
be bothered a little bit less yes it's
not the most complete answer typically
when you do it with finite horizon you
actually keep time indexed value
function and if you want to prove
convergence you're gonna need to take
into these account these time time index
value function but if you have the
discount factor than you don't yes a
policy and a trajectory good a policy is
like a policy is a strategy for acting
so the policy will be have to specify in
all states what is the action that I
should take or what is a distribution of
actions that I should take in contrast a
trajectory is one particular experience
through that space so if we go back to
my example about what occupation should
you follow write a policy maybe when
you're an unemployed go to grad school
when you're in grad school apply for a
job when you're in industry keep on
applying for other jobs right that would
be a policy now a trajectory might be I
was unemployed then I went to grad
school then I went back to being
unemployed then I went back to grad
school then I ended up in industry right
it's one sample path
and that path depends on your choice of
actions that's the part that's defined
by the policy but it also depends on the
stochastic effects and so that's the
expectation the the transition
probability so in this case I've just
rewritten over here the value function
we have for a fixed policy this doesn't
take into account searching over the
space of all policies it's just for a
fixed policy in this particular case we
sometimes have an alternate formulation
that's called the Q function and the Q
function in this case corresponds to the
value of a particular action and so
whereas in this case I'm folding in the
policy pi si in the Q function the
policy is used as an argument and we
define it for each possible action but
the first term is the same we have the
immediate reward for doing a particular
action in a particular state s we have
gamma our discount factor we have the
transition probability and then we have
the future provides the future value
function so both of these are instances
of a form we call the bellman equation a
bellman equation always has these kinds
of dynamic programming recurrence form
where you have your value function or
your q function on one side and the
value function of the q function of the
next state on the other side and it
comes in different forms and flavors but
it's always going to have these kinds of
structure with the immediate reward
pulled out the future returned appearing
here and the transition probability
appearing also over here and so if we
look at this equation over here and I've
gone back in this case to the
formulation with the discount factor
gamma if we assume that your set of
states is finite and we assume that our
set of actions is finite we can actually
see this as the just a set of linear
equations right we have s different
equations one for each of our States and
we have equations essentially with s
different
terms depending on the value of all the
future states I could end up in and so
seeing this as a system of linear
equation we can actually rewrite this in
matrix form if we want so I'm asked is
via PI is just a vector it's the value
for all of my state our PI is another
vector is the reward for each of my
states my transition matrix that's going
to be an S by s square matrix and I have
the value for all of my states also so I
can solve this as a system of linear
equation subject to some certain
conditions these conditions turned out
to be a little bit problematic in
particular this matrix over here is not
always well conditioned and it's not
always possible to do it but I so we
don't typically solve systems this way I
bring it to your attention just so that
you understand a little bit better than
nature of these dynamic programming
equations and also to insist on the fact
that this is for a particularly this
condition on a fixed policy we haven't
yet tackle the problem of looking at
finding good policies but if you're
going to go with my simple strategy of
let me enumerate all my possible policy
and now let me evaluate all of my
possible policies this is one method to
evaluate these possible policies because
solving this exactly
oh and I've lost the Mike okay so
because we may not want to solve things
exactly this matrix may be
ill-conditioned there's some very
convenient iterative methods to do the
same thing again drawn from dynamic
programming and because it's a dynamic
programming algorithm we can actually
start with an initial guess of the value
function the reassuring part is you can
the choice of that initial value
function doesn't matter at all in the
case where you have a discrete set of
states and actions and so you just have
some initial guess of what your value
function might be one popular guess is
to set it all to 0 and other popular
guess is to set it to the reward
function and now over several iterations
you're going to update the value for all
of my states and so in this case I've
indexed
by which round which iteration you're on
and so at round K plus 1 the value is
going to be the immediate reward plus
the future expected reward future
expected value based on your current
last round estimate of the value
function and all these other states and
over several rounds this is going to
converge and you can assess convergence
because the value functions are going to
stop changing or they're going to change
less than some threshold value that
you've put in so in this case it's
guaranteed to converge for simple
reasons in particular because you have
this discount factor over here and the
effect of this discount factor we
already alluded to is to sort of shrink
how much you care about the future and
so that means on every round of this
dynamic programming you can actually
show that the difference between the
value function from one round to the
next has to shrink so if you define the
value function at Round K plus-1
compared to the value function of your
estimate that you have previously then
you can actually separate these two
terms out of course the initial reward
terms cancel out and you're left with
just the difference between these values
at the next round and these two values
are multiplied by your discount factor
gamma and because this gamma is by
definition less than 1 or it's less than
1 so that we achieve contraction
property and you can actually show that
you're going to shrink your difference
between value functions on each round
and so over some time this will actually
go to 0 and so you have convergence now
this is for the case where you have
discrete states and actions in the case
where you have continuous state and
actions things get a little bit more
complicated but in the discrete cases
things are quite nice and in this is for
the disk the case with discount factor
if you have finite horizon episodic
tasks we can also show that we have
convergence but we don't have the
discount factor so in that case in
relation to my comment earlier the proof
of convergence goes through the fact
that you're keeping different value
function estimates for each of the
different time horizons so now let's
step back
a case where we assume we fix the policy
this was the case where I fixed my
policy and look at the case where we
actually are looking for a good policy
so here there's a notion of an optimal
value function I'm going to denote this
v-star this is the best value function
amongst the value of any of my policies
so in my simple approach I'm in
numerating all of my policy looking at
the value for each of them v-star is the
one that's the returns the best one over
all of my policies so any policy that
achieves this greatest value is called
an optimal policy and I'm going to use
PI star to denote an optimal policy and
so we have some results dating back to
the seminal work of bellman that says
that for each MVP there's actually a
unique optimal value function but at the
same time the optimal policy is not
necessarily unique and that's not very
hard to understand you can have two
policies right to choice of actions that
give you the same value so in that case
they both have the same value at the
optimal value restart but there are
different notions of policy we can have
multiple optimal policies one optimal
value function and there's a nice
correspondence between the optimal value
and the optimal policy and I've put in
the equations in really small font here
because I don't want you to look at them
but they're there in case you want to go
back to the slides and puzzle through
them later but in this case the
equivalents I want you to understand is
the fact that if we know the value
function V star and I haven't told you
yet an efficient way to get it I told
you a very naive way to get it but if we
have the optimal function V star and we
know the reward transition discount
factor then we can compute the optimal
policy very easily easily in terms of
like number of computing operations
similarly if we know the optimal policy
then we can recover the optimal value
function equally easily and so there's a
simple correspondence between these two
so the reason I emphasize this is I like
you to think of both V Star and PI stars
as solutions to the MVP but there's sort
of alternate solutions and we can talk
later about whether you know you're
better
to find V star and then calculate pi
star from that or find PI star and
calculate V star per month but both of
them are essentially solutions to the
problem question yes the policy is
defined assisant if you think of the
deterministic case then the policy is a
mapping from state to action so for each
state you need to define what's the
what's the action that's taken that's PI
star and that action is going to have a
value that's V star yes because the
policy is defined at each state right
the policy isn't like one action that
you use in our state the policy says for
each state here's the action you need to
take and so the policy is that full
mapping for all the states but if
possible that at a particular state
right there's two actions that give you
equally good reward so that's why we say
that the optimal policy is not
necessarily unique but then the optimal
value is
okay so let's finally get to the stage
where we're going to find a good policy
I'll present a slightly less naive
algorithm not a whole lot less and then
a slightly less naive one and then
colleagues will follow me throughout the
year that we can present some really
smart algorithms to do this so I'm
presenting the fundamentals here today
which are really used for small problems
as well as theoretical analysis so the
first algorithm is called policy
iteration we're not very imaginative
with the names in this field there's not
going to be a hundred and fifty flavors
of Gans there's going to be value
iteration and policy iteration so in the
case of policy iteration we assume that
we start with a random policy so let's
assume in each state Ridge is going to
randomly pick something to do so if I'm
unemployed I will randomly pick
something to do maybe I'll apply for a
faculty job and see
that goes and then you're going to
interleave two steps in iteration so
step number one is going to be to
compute the value of that policy so my
initial policy if I specify it it's
going to say for each of the states
here's an action to do now I'm going to
compute V PI and I can do that in closed
form using the linear system of
equations we rarely do that but you can
and you can do that with iterative
method using the iterative policy
evaluation method I just presented and
once you have V PI as the value of that
particular policy then I'm going to
compute a new policy PI prime that's
going to be greedy with respect to that
particular policy V PI and so in this
case I'm going to actually do this in
iterations until I'm done and my policy
doesn't change anymore and if I do
several rounds of that I can actually
show that this is going to converge for
the simple reason that I have a finite
number of policies so I can in the worst
case go back to my naive algorithm which
is to enumerate them all and I don't run
out we ran out of batteries on one thing
we don't want to run out of batteries on
two things in the same talk okay and so
in this case I'm going to stop in the
case where my policy doesn't change
anymore and so there's a few things in
terms of computational cost for this one
of them is you need to think about
what's my computational cost for
computing V PI one of them is what's my
cost for computing that new policy
that's better a policy is going to be
better it's gonna be an improvement in
some state and I need to think about how
many of these iterations do I need to do
before I'm going to terminate so in the
worst case write my number of repetition
around maybe the number of policies that
I have to consider so that's something
to worry about the alternate methods for
doing this is just
why am i bothering with computing the
policy on every round let's go back here
right I'm computing a value function
here now I'm extracting a policy I'm
computing a new value function so on
let's take inspiration from the
algorithm we have for policy evaluation
right the method with the iterative
method we have and instead of computing
that only for a fixed policy let me fold
in this improvement step within that and
so this is what value iteration
essentially does it's taking our bellman
equation and turning it into an
iterative improvement rule so we start
with some initial value b0 for all of
our states and then on each iteration
this looks a lot like the equation we
have for Policy evaluation the only
modification is that on each round
instead of updating my value based on
just the reward and the next value for a
particular fixed policy I'm going to
fold in my greedy improvement so my step
of maximizing over my actions right in
that update rule so for each of my
states I will look at for all possible
actions I can take for that action what
reward would I get where does it take me
to and what's the value of that new
state and this algorithm stops also when
the change of value between two
iterations is below some threshold and
the algorithm is going to converge in
the case where you have your gamma
discount factor that imposes contraction
on it so we have essentially three
related algorithms right the first one
policy evaluation fixes the policy and
only estimate the value algorithm number
two it finds the best policy at each
state and then mixes rounds of policy
evaluation which is algorithm one and
rounds of greedy improvement in value
iteration instead essentially folds in
the greedy improvement right into the
policy evaluation so does all of this in
a combined update rule to find the
optimal
value function and so we can look
briefly at the time complexity of each
of these steps right we can think about
how we're going to do policy evaluation
there's different ways to look at the
complexity of that but if we look at the
simple case which is for solving the
system of linear equations then that's
cubic number of operations in terms of
the number of states if we look at
policy iteration that has the policy
evaluation step in there so the S cubed
term is over here and then greedy
improvement essentially requires you to
estimate the bellman equation once for
every state if we look at value
iteration you're essentially finding
that greedy update rule without doing
full evaluation so you save yourself
some operations they have the per
iteration complexity of S squared times
a but that's kind of up sophisticated a
really important factor which is how
many iterations do you need to do that
right and so people often wonder am I
better off using value iteration am I
better off using policy iteration in
terms of per iteration cost value
iteration is definitely better than
policy iteration if you do it the naive
way what often happens in practice this
policy iteration requires many fewer
iterations than value iteration but we
don't have really good bounds on that in
terms of we have very loose bound on the
number of iterations but we don't have
very good results that show us for
specific problem which will be faster or
slower yes the value iteration yes
exactly so that was the point of my
slide with the tiny equation point which
is if you have the optimal value you can
extract the policy quite easily and it's
not very hard to see if I look over here
at this equation when things have
converged if I run this equation with an
Arg max operator instead of a max over
here so instead of finding which action
maximizes with the maximal value I ask
which action maximize this particular
equation so this equation here the
complexity of evaluating that is 8
s squared right for each state I need to
look at each possible action and then I
need to see where are all the next
States I might see and so that's the
complexity of just retrieving the last
policy but I do that just once at the
very end the two will converge to the
same optimal value function right but we
know there can be multiple pop temple
policy so they'll converge to the same
value function under some conditions but
those conditions are met for discrete
state action spaces for continuous state
action spaces
I can't run these kinds of bailment
iteration because I might never see the
same state again then we need to we need
to handle that case separately but for
discrete and often people will talk
about tabular reinforcement learning but
tabular is the case where you can
discretely illuminate the state and
actions and you can essentially you know
keep your values in a big table yes
there is an optimist palace policy that
is not stochastic yes so in the case of
tabular so discrete state action spaces
there is an optimal policy there may be
more than one optimal policy but there's
an optimal deterministic policy so we
talked about deterministic versus
stochastic and I've been mostly straying
in the space of deterministic policy
because for these particular cases
deterministic policies are sufficient
now there's reason you may want to look
at policy that are stochastic Peter
erbil will give you many good reasons
this afternoon why that's something that
in some cases very sensible to do
particular deals with the fact that when
you're dealing with larger state action
spaces it's not feasible to enumerate
all the actions it's not feasible to
look at all the policies and then having
notions of gradients how much propel is
the improvement can I get and using
gradients to do that it's a lot easier
to do with stochastic policies and
deterministic policies let's look at a
simple example just to get a taste for
how these things go this is a really
simple agent that moves in the different
cardinal directions this particular
agent is limited to a grid of size three
by four it usually starts somewhere up
here and if it ends up over here it gets
a +1 reward and it but falls in the dump
over there it gets a negative 10 reward
so you can imagine that it wants to get
here you can start in I can move in four
different cardinal directions up down
left or right those are its four actions
whether stochastic actions so there's
usually a 70% chance gonna end up where
it wanted and a 10% chance it's going to
randomly go in one of the other
directions accidentally and so in this
case we're using a pretty high discount
factor um close to one I should say and
so 0.99 so that means that there's a
little bit of discounting but not very
much so if we start value iteration at
round number one I've initialized my
values to be equal to the reward
function on round number one and I run
this for one round after one round of
iteration this is what the values will
look like right if I run my evaluation
algorithm essentially it's saying in
this state let's look at all the actions
I can take for each of these actions
let's see where I can end up and what's
the value of ending up in there so for
all the states over here they can end up
in neighboring states but all those
states had an initial value of zero and
so the value stays zero even after one
round of value iteration we see the
neighboring states which had neighbors
with rewarded for with value information
pick up some of that value information
in this case it seems like potentially
you know the best action to do might be
to avoid the negative ten right to move
left and in this case the best action
might be to move downwards to be able to
avoid these kinds of about places if you
run this for several rounds at some
point the value changes and converges to
some amount that is quite consistent
what I've plotted down here is what we
call the bellmen residual that's the
maximum difference in value over all of
the states and so in this case we see
that that's the number that's
essentially getting smaller and smaller
because of that discount factor that's
imposing the contraction and so after
some time we have a value function that
is plotted over here and now we ask how
do we extract the policy for this
particular value function and to do that
you need to apply your bellman value
function again but it's quite intuitive
to look at what the policy might be for
doing that I usually have like a plot of
the policy but I say I didn't put it in
today essentially most of these states
write the policy is to go right towards
the plus one in some cases though the
policy over here in particulars to go
left because if you go through the top
over here there's a pretty good chance
you're gonna end up accidentally falling
in the pit and so the policy over here
is to move to the left in this case you
can just look at it greedily by saying
for each state you know which
neighboring state has the higher value
and the policy is going to be imposed in
that direct
that's essentially was gonna come out of
veranda bellman equation for that and
you can do that for larger domains right
the four rooms domain and there's
another one that's commonly used when
you do your value function updates if
you're trying this on your own at some
point it's usually a good sign if your
value information is sort of radiating
away from the reward points when you
initialize your value function to be
equal to the rewards so there's a way to
sort of easily diagnose whether your
value iteration algorithm is doing what
it should should be sort of radiating
away from the points of reward over here
there's reward over here the reward gets
propagated away from the goal and if you
have a more complex value function with
different reward points you see these
same kinds of effects the reward
information propagating through I've
presented these notions of value
function value iteration and policy
duration in pretty strict terms it turns
out that you have quite a bit of
flexibility on some of the fronts in
terms of how you implement this so for
example for value iteration it's
possible to do what we call asynchronous
updates right maybe you already know
that some states are a lot more
important than other states so let's not
run these bellmen backups synchronously
through all of the states and do the
same number of passes through all of the
states there may be a way to establish a
priority order
maybe excuse me maybe you can generate
some trajectories through your MVP as
you generate the trajectories you'll get
some statistics about which states are
visited often you do more value backups
around those states if some states are
never visited it's probably not worth
wasting computation on them you can also
choose to update the states whenever
they appear on the trajectory and so on
and it's the same with generalized
policy iteration so if you want to do
policy iteration right there's two steps
in policy iteration one is when you do
the policy evaluation and one of them is
when you do the policy improvement you
don't need to systematically do
evaluation and improvement on all of the
states at every round you can sort of
mix improvements and evaluation steps in
a non-uniform way that represents some
other information you
have about the domain and so this gives
you some flexibility and they can be the
way to get drastic improvement in terms
of time complexity for applying this on
some larger domains and so let's step
out for a second about these very simple
cases that are the fundamentals of
reinforcement learning and talk a little
bit about what are some of the harder
challenges that we face right one of the
things that is quite difficult in many
cases and I'm not going to be able to
give you too many solutions on that is
how to design the problem domain state
action and cost reward one of them which
we'll start talking about in a few
minutes is this notion of incorporating
learning into this so far I've really
been in just planning and solving how do
we incorporate you know data and
learning and then we'll get to some of
the later topics down here but it is a
good time to take some questions yeah
when what sorry when facing our problems
how would you choose gamma yeah so for
many years I thought that gamma should
be part of the environment someone
should tell you what gamma is right I'm
not designing the environment I'm
solving the system so someone who's
specifying the environment should tell
me how to discount my future versus my
present and whenever you're in the case
of planning so the case I've described
so far where your transition
probabilities your rewards are specified
you should also expect someone it's a
problem designer in you know along with
the first bullet to specify your
discount factor when we're moving to the
case where we are dealing with learning
so assume that your transition
probability your reward function are
estimated from data samples then there's
recent results in the literature that
that are suggesting that you need to
think of gamma as a hyper parameter and
in particular if we look at our bellman
equation that's balancing immediate
versus future if my transition
probabilities are poorly estimated my
value function is poorly estimated maybe
I should use a more
sev gamma to reduce the variance that
comes in from poorly knowing the future
values and so using gamma as a more a
hyper parameter that I can tune my son
how complex do I want my plan to be
versus how much data do I have is one
thing that's really starting to get a
little bit more traction in the last few
years and there's a few interesting
papers last two to three years on this
yes in some sense yes if you buy
learning you mean like estimate right in
some cases in some models if you think
of in supervised learning as having
lambda your parameter that regular Rises
that that weighs how much regularization
you want right if you have a loss
function that has like a the supervised
criteria and then you have your model
complexity that's weighed by lambda
usually you'll use maybe
cross-validation to estimate lambda so
in that sense you can learn it you can
adapt it in a way and so I think we
don't have all the answers of how to
adapt yeah my yet in reinforcement
learning I think that intuition is
starting to permeate and there's gonna
be interesting results coming through
yes yeah so there it doesn't make a
difference it's just you need to be a
little bit careful handling how you take
the expectation right so in some most of
the literature right they define RSA in
some papers they define our s as prime
so it's a function of where you are what
you took and where you're going and
eventually you could define it as a
function of a s prime which is where
you're going all of these can be made
equivalent by just taking expectation
appropriately with respect your
transition probabilities in some cases
maybe just R of s and in that case you
assume that the reward just doesn't
depend on the action okay I will make
some grounds and we'll have time for
questions at the end so let's talk
specifically first
learning in particular how do we do
online reinforcement learning there are
many many cases where you don't have
your transition probability your reward
function in advance you need to acquire
this right you need to take some action
see how well you do get an information
and leverage that in some cases the TD
gaming system was based on that there
was an agent playing against itself
through many millions of games to be
able to evolve a good function and so in
that case you have what we call the
reinforcement learning loop and that's
probably one of the things you'll see on
a more practical hands-on session there
has to be a notion that you can take an
action observe the effect of that action
so that action causes a transition in
the system you get to observe what's the
new state you're in what reward did you
get for that action and then you adjust
something what you adjust changes
depending on different paradigms but you
can adjust your policy you can adjust
your value function you can adjust your
cue function you can adjust your model
of transitions and rewards but there's
some adjustment is there's some learning
that's gonna happen and then based on
that learning you're gonna be able to
take a new action so most human animal
agents operate in this kind of a loop
and so there's been quite a bit of work
in reinforcement learning that is on
what we call more the online learning
case and so there's a few different
classes of approach for online learning
one of them is based on very simple
Monte Carlo estimation principles which
is to say we are going to look at
samples of our empirical return so I go
back to this notion of you as my
empirical return and I'm going to look
at the difference between what's my
empirical return which I'm observing for
this trajectory I'm gonna pick my policy
run a trajectory right like play one
game of backgammon see how it goes
estimate my return you have s and now
compare how far this return that I've
observed on this trajectory differs from
my current estimate of the value
function for that state call that's my
error signal I'm gonna weigh that by a
learning rate you're gonna call this
alpha and we're going to update our
value function in
proportion to this area signal so this
is essentially just an error update rule
right here alpha can be thought of as
the learning parameter U is your actual
prediction your actual observed sample
and V is what your model predicted and
so you're going to do some correction
based on that so this is sort of a
gradient rule but just look like it
doesn't require you to know the reward
function it doesn't require you to know
the the transition probabilities but it
does require you to do typically is run
many many rounds of this because
typically this empirical return is over
the full trajectory so if you have a lot
of stochastic city in your system you're
going to need many trajectories to
properly estimate this particular return
so these methods and particularly when
the planning horizon are long when the
trajectories are along this method tends
to have quite a bit of variance and so
you need a lot of sample to estimate it
properly but we can show that it
actually gives you an unbiased estimate
of your value function V when you do
this
alternately there's a second class of
method called the temporal difference
learning methods and rich Sutton this
afternoon will tell you much more about
this particular class of learning the
idea of temporal difference learning is
to say instead of waiting till the end
of my trajectory to estimate my return
let's try to estimate that return based
on just based on essentially a bellman
equation so instead of estimating U at
the end of the trajectory I'm going to
replace this by the immediate reward so
I'm going to take the one step sampled
reward and then for the future reward
I'm not going to wait till the end of
the trajectory I'm just gonna plug in my
current estimate of the value function
of the next state and discount that so
the V over here is the corresponds to
the V over there right that's my current
prediction of what's the empirical
what's the expected return at state s T
and this part over here together that is
my empirical observation
of what it is now one thing to note is
that my function V shows up serve on the
left side of my error signal and it
shows up on the right side also right if
you do this for supervised learning it
looks more like this right maybe you're
trying to do regression with a gradient
descent approach over here this you
would be what did my regression what is
my actual output Y that I'm observing
and this V would be what did my model
predict the Y should be and I look at
the difference between those two so in a
supervised learning case the U is a
sample and V is predicted but one of
them is the ground truth right U is the
ground truth and most of the time we
assume that this is correct when we do
TD learning V over here is produced by
our model or current estimate of V and
now this part that should be the ground
truth well there's one little bit of
ground truth which is R and then there's
this piece V that's also our model and
so when our model is poorly estimated
when we haven't seen a lot of data this
estimate of the error tends to be very
noisy and so in many cases this TD
learning approach can lead to
instability not in the case where we
have a discrete set of states if we're
in the tabular case discrete discrete
actions things go quite well we can
actually show that we have convergence
and so on but when we start dealing with
very large continuous state space we'll
see that we have stability problems and
many of the things that have been I
would say put forth over the last two
years in terms of the deep Q Network and
so on really look at how to have better
stability on this kind of estimators
specifically for this reason that the V
function shows up on both sides and it
gives you for stability yes yeah it's
because at sto the RT plus 1 this is the
immediate reward I should probably use
our T over here but then it's the
immediate reward plus gamma times the
value and the next one goes to compose
together these two pieces together
correspond to UST right it's my current
estimate of what's the empirical return
so I'm going to replace just the
immediate reward by a sample observation
and the future I'm going to plug in my
current best estimator for the value
function so this TD method is
essentially what was used in TD gammon
with the addition that the value
function wasn't expressed in a tabular
case but the value function was actually
expressed with a neural network and
we'll get to that in a second but in
cases where you're number of states gets
too large and you can't list the value
function for all of them you can
actually replace that by a function
estimator so let's talk about this
question of function approximation in
particular function approximation is
used in all these cases where you have
too many states or you have continuous
states where you can't actually go
enlist the full set of states so in
robotics in most games this is crucial
to getting good results and so if we
look at how to do this essentially what
I'm positing is that my value function
or my cue function can be replaced by a
approximation and here I've put in sort
of the simplest form of approximation
that we consider which is a linear
approximation and so in this case I'm
going to assume I have a set of features
right these features may be features of
the board right where are my pieces on
the board how many of such-and-such
pieces do I have these features can be
designed manually for some game and now
instead of learning my function Q or V
directly I'm going to you learn these
linear coefficients such that when I
take the linear combination of my
features and my weight theta are my
weights that I'm estimating then I get a
value function so tabular case when we
can list the states we estimate V
directly cases where I have large state
space I assume that there's a functional
form to define my value function and I
estimate the parameters of that function
and so the linear function is the
simplest one that has been used for many
years
over here I'm picturing one of the
classic problem of reinforcement
learning they mounted car domain where
you have a little car that's trying to
get up a hill and it doesn't have quite
enough acceleration to get to the top so
you need to kind of go back and forth a
few times to build up momentum in that
case the state space is usually the
position of the vehicle along the curve
as well as its velocity so you have a
two dimensional state space it can be
discretized or continuous and often
we've used linear function approximation
to represent the function of different
actions it's usually cast in this case
not with a continuous acceleration in
terms of actions but with on/off
acceleration so you have binary action
choices in this case so you can have a
different linear function for each of
your actions as we get to larger and
larger domains and we'll see many of
them this week you need themes your
function approximation because you have
larger state spaces and more complex
representation are needed so a lot of
the work that has been published for
example by some of the deep mind
researchers and other people active in
deep reinforcement learning looks at
taking as your state representation some
really raw version of your observations
so in this case the case from the Atari
learning environment where the goal is
to learn to play several different Atari
games so in that case the input space
the state space is really in the space
of pixels and rather than look at linear
function approximation we use a
convolutional neural net to capture the
value function so the convolutional
neural net essentially evolves a
representation of the features from the
raw input space and the output of the
convolutional neural net is not a label
it's not like is this pac-man or not
pac-man it's really a value function so
it's the value for a particular state in
the system we can learn that and from
that value function extract an optimal
policy when we've done that and as deep
reinforcement learning has been
broadening to other games than Atari
learning environment for example there's
been some work on using reinforcement
learning in Minecraft you might get a
demo of that later this afternoon you
can incorporate many of the
technology from deep learning for
example notions of memory and context
and attention and so on can all be
incorporated into that representation of
the value function and what changes is
that the output of the value function is
no longer label but it's an estimate of
the value function for that particular
state and so as research progresses on
deep learning architectures and models
reinforcement learning leverages that in
most cases through the use of more
complex models for approximating the
value function let me close off with
just a few bits and pieces I call it the
RL lingo is these kinds of terminology
that comes up which when you've been in
the field for five or ten years you've
kind of developed the understanding of
all this but if you haven't been in the
field it's useful to get a sense of what
these terminology might be because it
will pop up in several of the talks
probably in the next few days I've
already talked about episodic versus
continuous tasks I've also already
talked about batch learning versus
online learning mostly I talked about
online learning and I didn't talk much
about batch learning you'll hear more
about it later on but let me go through
a few of these also quickly in the five
minutes we have left one of them is the
difference between an on policy
reinforcement learning system and an off
policy reinforcement learning system
that goes back to something I mentioned
early today which is that when you
change your policy in your RL agent when
you change how you choose your action
you're essentially inducing a new
distribution over the states right you
change how you play the game you're
going to see different states and so in
some cases you can actually show that
the data distribution changes every time
you change the policy and one of the
challenges with learning from a batch of
data is that this batch of data was
collected under a particular policy
right your agent had a particular
strategy choice of action for collecting
the data now if you try to do
reinforcement learning it's possible
that you're estimating a different
policy think of the case where you're
doing policy of
you Asian you're estimating the value of
a different policy but you're trying to
do that with data that is distributed
according to the data collection policy
right what we call the exploration
policy and if these two policies start
diverging too much then your ability to
properly evaluate the policy your
candidate policy is going to weaken as
your distributions become further and
further apart and so one of the
consequences of that is typically you
need very very large batches of data I
mentioned earlier that you need and
essentially to have tried every action
from every state that's not possible in
some cases right so if you think of a
system that is trained for example to
play a very large game think of the
system that was trained to play go it
wasn't possible to look at all the
different actions in all the different
states and furthermore right they were
stuck to a distribution of data that
they had observed from previous games
but that tells you nothing about what
might be the distribution of data if you
start playing in a very different way if
you have a simulator things are okay
because you can go and try out your know
your strategy and get new data every
time you consider a new policy you can
run that policy and get the data but in
cases where the data has been collected
a priori you're stuck with that you need
to think about how to correct for the
differences in distributions in the data
and so one of the ways that is used to
correct this is using an important
sampling measure so we look at an
importance factor between the behavior
policy that's usually the policy we use
to collect the data and PI our target
policy that's the policy we're wishing
to evaluate right now and we look at the
difference between those and weary way
our data according to this importance
factor for each state and action weary
way the sample according to this those
of you who've worked with important
sampling maybe in other cases know that
in many cases as your policy start
diverging your importance factors can
get really really small and so there's
two problems in one case if you've never
tried something under the behavior
policy then essentially right you can't
say much about it and if you have your
policy are really different then there's
not much that you can say about it so
you typically Phil need really a lot of
data to
important sampling correction there's
more sophisticated mechanism but that's
one thing to watch out for if you're
dealing with a batch of data and now
trying to estimate many different
policies the other thing to take into
consideration is related to this where's
your data coming from how did you
collect your data in some cases the
space of possible data and things that
you need to try to collect the data is
very very large as you're collecting
some data it's tempting to start acting
according to what you think might be the
better policy and to stop what we call
exploring right exploring is the
mechanism through which you randomize
your choice of action to gather more
diverse information exploitation is the
mechanism through which you start acting
according to what you've collected so
far and what seems like the best
strategy so far right all of you who are
coming to University of Montreal for the
first time last week in this week
probably found a path that somehow get
you to this building maybe you're using
the same path every day and you don't
dare explore and take other paths
because it's a big mountain
what's one mechanism that you sometimes
use to find another path you follow
someone else right one day you meet
someone at the bottom of the mountain
and they go a different way and then you
find another way
so one practical way in which people
have overcome this exploration problem
is through methods related to imitation
learning where you get an agent to
demonstrate a good behavior and now you
start saying this is at least some
example of a good trajectory you don't
need to use random trial and error and
explore all the paths around the
mountain finally in some cases I haven't
made a big distinction between these it
will come through several of the talks
this week by there's some attention
called to the fact that some
reinforcement learning methods are what
we call model based methods whereas
others are called model free methods
I'll just give you a brief definition of
it you can keep an eye out as you go
through some of the other talks in
model-based learning we typically assume
that we're going to get a lot of data
and use the day
to estimate our transition and our
reward model and then we will plug into
some of the policy and value iteration
methods that I presented earlier in
model free RL we typically assume that
we are only going to directly estimate
the value function we will never
carryover an estimate of the transition
or the reward function never an explicit
estimate of them we will directly
estimate the value function there's pros
and cons to both of them I would say as
kind of a one-liner of pros and cons to
both of them whenever you're tackling
large difficult problems it's useful to
be able to put in domain knowledge in
some cases it's easier to put the domain
knowledge on the model itself right you
can constrain the dynamics of your
system you can constrain the space of
your reward function if that's the case
maybe model-based is better in other
cases that's not feasible and you have
some notion of regularity of your
solution space in that case model-free
might be more appropriate essentially
where can you put in your domain
knowledge to constrain the solution
space efficiently is one way to think
about this the last piece of lingo I
will clarify is the separation between
what we call value function methods I've
only talked even though I talked about
policy iteration it's essentially all
value function methods methods that go
through estimation of a value function
on the other end there's all the methods
where you directly optimize the policy
and in that case this is gonna be the
material that's tackled by Peter Peter
abia later today and so stay tuned for
all of that just a quick summary I'll
take one more minute of your time
essentially I hope you feel like you're
a little bit more equipped to detect
reinforcement learning problems as you
go through the world and solve AI
problems broadly I would say one thing
to keep in mind is this last point over
here which is your intuition about
what's easy and what's hard that you've
possibly acquired through other cases of
machine learning supervised learning in
particular in some cases turns out to be
quite different in reinforcement
learning things that are easy and one
turned out to be
during the other so keep an eye out for
that as you stick through the other
talks I put this out this is probably
not for today but you will have it I
should say all the slides not just from
my talk but all of the speakers the
slides and the videos are going to be
available online in the case of my talk
if you're really in a hurry you can
probably look at those slides and videos
from last year it's not all that
different but the talks for the other
slides and videos are going to be up
slides very shortly within a day or two
typically videos probably within two
weeks or so and if that's not enough
there's a set of resources that you'll
be able to reach out to I will close it
at that for now I'll take maybe one
question and then let you have coffee</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>