<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Stuart Russell - Building Artificial Intelligence That is Provably Safe &amp; Beneficial | Coder Coacher - Coaching Coders</title><meta content="Prof. Stuart Russell - Building Artificial Intelligence That is Provably Safe &amp; Beneficial - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Stuart Russell - Building Artificial Intelligence That is Provably Safe &amp; Beneficial</b></h2><h5 class="post__date">2017-09-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RVFXjqStg6w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so let's get into it let's start
the scientific program of the conference
so it is my pleasure to introduce to
Russell known for his many many
contributions for the visual
intelligence who has not used his
textbook or night vision intelligence
around thousand three hundred
universities in 1600 in 16 countries
used his book store was born in
Portsmouth England received his Bachelor
of Arts degree in first class honors in
physics from Whatcom College in Oxford
in 1982 and his peers in computer
science from Stanford University in 1986
for research and inductive reasoning and
analogical reasoning after his PhD he
joined the faculty of the University of
California at Berkeley where since 1996
he is professor of computer science he
also holds an appointment as adjunct
professor of Neurological Surgery at the
University of California San Francisco
where he pursues research in
computational physiology and an
intensive care unit monitoring in 2016
he co-founded the center for human
compatible artificial intelligence at UC
Berkeley and is on the scientific
advisory board for the future of life
Institute and the Advisory Board of the
Center for the Study of existential risk
his work has been internationally
recognized with many awards and honors
among them the each guy computers and
thought award we just awarded a few
minutes back in 1995 and the ACM Calvi
Kallstrom outstanding educator award in
2005 he was appointed to the Blaise
Pascal a chair in Paris in 2012 and is
also a fellow of ACM and Tripoli
without further ado please join me
welcoming us to a Russell
okay thank you very much Carlos it's
wonderful to be here in Melbourne and
let me begin by talking a little bit
about some of the progress that's
happened in artificial intelligence I
can figure out how to get this lines to
move
destroy this one okay good all right so
this is Lisa doll one of the world's
greatest go players and he's having what
we sometimes call holy cow moment where
he's realizing that in fact a AI has
progressed a lot faster than we expected
it wasn't that long ago that the New
York Times predicted it will be a
hundred years before a computer beat the
world champion and it turned out to be
just a couple of years here's our robot
his name is Brett at Berkley and if I
could figure out how to get the video to
play we should see Brett doing the
laundry so this is quite an impressive
piece of work from my colleague Peter
erbil and just to show you the rate at
which AI is progressing
Brett is soon to be a museum piece so
the Victoria and Albert Museum in London
is going to have an exhibition next year
and Brett will be the centerpiece of
that expedition exhibition people
hopefully will be able to bring laundry
and Brett will fold it for them
here's a graph that many of you have
seen this is the rate of progress on the
image net competition and you can see
that with the advent of deep learning in
2012 the error rate started to plummet
to the point now where it's relatively
straightforward for an undergraduate to
beat the vision performance of a human
being on this task even a human being
who spent several weeks training on this
particular dataset and this particular
challenge so that's very impressive we
could look at an even more impressive
task actually writing captions not just
recognizing objects but writing captions
here is a group of people shopping at an
outdoor market there are many vegetables
at the fruits dance so a very impressive
task that you know ten years ago we
would have been utterly shocked
if that was possible now it doesn't
always work and I'd like to commend the
authors of this particular paper on
being very honest about the performance
limitations of their system
unlike many vision papers they actually
give examples of where it fails so
here's a here's a street sign which
apparently is a refrigerator filled with
lots of food and drinks now when you see
that you wonder okay perhaps the
performance on the previous example
wasn't quite as impressive that maybe
there was some fluke involved or so you
go back and look at that picture and the
group of people well there is no group
of people there shopping well nobody is
shopping actually in the picture and
there is no fruit stand so I think we
need as Ramone said we need to be a
little careful before we announce
success in the field of AI that from now
on it's just a matter of bigger data
sets and and more CPUs and and we'll be
reaching humans lovely I actually
believe we are still a long way from
that and I would like to say in
disagreement with Ramon that those who
for example like Nick Bostrom
who are predicting possibly negative
consequences from super intelligent AI
are actually more pessimistic about the
rate of progress they put the
achievement of human level III further
into the future than the average AI
researcher does so I don't think that
the the doomsayers as they are called
are saying that doom is right around the
corner but nonetheless saying that we
need to prepare for it eventually and
that's what some of my talk is about so
is is deep learning just going to grow
and grow and grow until it consumes the
entire field I don't think so we can ask
the deep learning people
Francois Sciales a very respected deep
learning researcher author of one of the
major python-based deep learning
packages and Anna a very impressive
textbook and he writes many more
applications are completely out of reach
for current deep learning techniques
even given vast amounts of human
annotated data and I think this is a
very important point
in the 1960s when we were able to solve
you know three or four step planning
problems we assumed that it was just a
matter of time and we we'd be able to
solve all planning problems and we
forgot about exponential computational
complexity and I think we're in danger
now making the same mistake with data we
are forgetting that limited techniques
knowledge free learning methods may for
some tasks require exponential amounts
of data that could neither possibly be
learned or in fact could not possibly
exist he goes on to say that the main
directions in which I see promised our
models closer to general-purpose
computer programs as opposed to
feed-forward circuits and I agree and in
fact there's already a field that does
exactly that called probabilistic
programming so para ballistic programs
are formal models in in a language that
has Turing equivalent expressive power
which means that any any language for
writing any probability model is
effectively equivalent to or subsumed by
one of these probabilistic programming
languages and they have the property
like Bayes Nets that every well-formed
power ballistic program denotes a unique
distribution over an arbitrarily complex
set of worlds with with possibly
different numbers of objects different
relations among objects different causal
structures different evolutionary
histories over time and so on so you can
represent extremely complex theories in
these languages and bring to bear on any
learning problem a great deal of prior
knowledge and they come along with
general-purpose inference algorithms so
that in principle subject to the usual
caveats about convergence and and
computational complexity because of
course all these things are really
undecidable
but the general-purpose arguments will
find the answer to any query with
respect to any model and any data set
and when I give talks about the power
ability programming research in my group
I often you know I'll show the power
ballistic program and I'll show you a
couple in a minute
and then I talk about how the algorithm
works and I and I give some results and
I think what I should do is pretend that
these are really deep learning systems
because then the audience would like
them much more and in fact in some sense
they are they they are automatically
dynamically generating deep generative
networks of unbounded width and depth
and variable structure during the
inference process and in fact the one
I'm going to show you for seismic
monitoring can generate between a
hundred thousand five hundred thousand
random variables in a fairly deeply
connected and very complex network so
let me talk about that seismic
monitoring application this is a small
nuclear test that was carried out in
Nevada was the aftermath of a small
nuclear test that was carried out in
Nevada and in the 50s and 60s in fact
this used to be a big tourist
destination so people would get on tour
buses and drive out and see nuclear
tests they would watch them with glasses
or they would go and look at the crater
afterwards it was at different time the
largest nuclear test was actually five
hundred times bigger than this one and
to put an end to testing in the hope
that we would put an end to the nuclear
arms race a comprehensive nuclear test
ban treaty was proposed originally in
1958 it's finally opened for signature
in 1996 and the the treaty comes with a
verification arm called the
international monitoring system which
consists of a very large number of
monitoring stations around the world
several hundred of these a large
satellite network in all it cost over a
billion dollars and there's a data
processing center in Vienna where all
this data is collected and processed in
real time
in order to detect seismic events and
classify ones which might potentially be
nuclear explosions so the evidence if we
formulate this as Bayesian inference the
evidence is the waveforms collected from
all these seismic stations which look
like you know as you know
seismic Wiggles and so there are
hundreds of these terabytes of data
every year the query is what happened so
what were all the seismic events that
took place in the last 24 hours where
did they take place how big were they
how deep were they and so here's a
little map of seismic events over a
three-month period and you can see that
most of the stuff happens in Southeast
Asia and then the model is what we know
about geophysics so we have an idea
about the geographical distribution of
natural seismic events we have knowledge
of the physics of signal propagation we
have information about the level of
seismic noise in the background of each
of the detectors and we know about how
fast signals travel through the earth
and the paths that they take so all of
that geophysics our knowledge including
the uncertainties involved in those
models are expressed in this program
which is written in a parable istic
programming language called blog and
this is the entire program it took about
half an hour to write once we understood
you know we went to some sessions in
Vienna where they explained to us the
nature of the problem and the underlying
physics and it was very straightforward
to write this it took a little longer to
get it to run fast enough to handle all
the data on a global scale in real time
but that's exactly what it does it runs
an MCMC algorithm in real time doing
Bayesian inference with respect to all
the seismic data collected from the
world and with respect to this
probabilistic model and to show you the
performance this is the original United
Nations automated system which is the
result of about a hundred years of
seismology research and as I said about
a billion dollars of investment and this
is the failure rate of the system so
this is the fraction of events that the
system failed to detect that then
subsequently the human experts have to
go back into the data and trawl through
it find all the unexplained blips and
then see if they can reconstruct the
events that produce those blips in the
data so it's between 30 and 50 percent
error rate and the net visa system which
is the model that I just showed you has
an error rate between 10 and 40
dissent so a very substantial reduction
in the error rate and as a result the
United Nations in 2014 announced that
net visa will become the official
monitoring system for the treaty and
here's a little example this is North
Korea this is a test that took place in
February 2013 in fact it took place the
morning that I was giving a talk in
Padua and so I actually had to update
the slides to add one to the count of
nuclear test that had taken place and
then my my former student NEMA Aurora
who actually did all the work sent me
this picture and so it shows the region
in North Korea where test was suspected
to take place and that the top left the
green triangle is the the location that
was the consensus location of the
combined geophysicists of the world
analyzing all the data and this is where
they announced the test to have taken
place this is the net visa location and
then the the black cross is subsequently
by satellite imaging
they found the entrance to the tunnel of
the testing facility and so it looks as
if net these are actually did a better
job of locating the event even though
the detectors are four or five thousand
kilometers away from the testing site we
got within less than the kilometer of
the actual site and subsequently we we
built a more sophisticated model which
includes a model of the the detailed
form of the seismic signals that are
produced by each event depending on the
geological location in which they occur
and I'm not going to go through the
details of this shot but just down here
we see that in the low magnitude range
which is the area where we are most
worried about the detection capabilities
of the seismic network
we're actually detecting 20 times as
many events as the combined
human-machine system that the united
nations is running so we really reduced
the detection threshold by by more than
a magnitude on the rig on the Richter
scale
here's another blog model
and this one is for natural language
understanding so it's a it's a
generative model of text but it's a
little different from a typical
generative model a typical generative
model basically says well word 12 is on
the page because words 11 10 9 8 and 7
were on the page which is not a very
satisfactory explanation you know if you
were a physicist and someone said well
that's why text occurs is because
previous texts occurred no text occurs
on the page because there's something
true in the world and someone is trying
to say it so let me see if we can
express that in blog so here we go I'll
write it in English first of all there
are some things in the world a lot of
them don't know how many there are some
relations in the world quite a few don't
know how many and then the relations are
expressed by some strings don't know
what they are and some of the objects
are related by some of relations don't
know which ones and typically not very
many and then people choose some facts
so some relation that holds among
objects they choose one of those things
to say and then they say it how do they
say it well they say it by making the
arguments of a relation into the the
subject and object of the sentence and
the string that describes a relation
into the verb e bit of the sentence so
that's the generative model and this is
completely unsupervised so this is the
this is the Bayesian model we provide
evidence in the form of sentences from
the New York Times there's a special
subset of named entity-relationship
descriptions that Andrew McCallum's
group extracted from the New York Times
and then the query is what's true in the
world ok so this system knows nothing
about the actual language that doesn't
know that it's English it knows nothing
about the world it does not know what
relations exist or what objects exist
and then we just give it text and say
what's true and so it runs in trance and
here's the result so here's an one of
the relations it discovers it doesn't
know what we call it but it calls it
relation number 46 and when you look at
these these power strings for the
dependency but
Queen the two arguments you see that
this relation is the relationship of
corporate subsidiary would and in the
New York Times it turns out there are
sixteen ways that you can describe one
thing being a subsidiary of another and
it identifies those sixteen and exactly
those sixteen and then you can ask it
well what facts are true for relation
forty six and you see a whole bunch of
facts like this most of them apparently
about advertising agencies because
they're all collected into conglomerates
and we had accuracy rates of over ninety
five percent for the relations that we
looked at so this just shows the power
of a little bit of prior knowledge and
Bayesian inference in a sufficiently
expressive language and what you can do
with these kinds of techniques so let me
pause then briefly and look at where we
are with artificial intelligence so and
ruing has a nice way of describing the
current state of the art it's not
perfectly accurate but as a rule of
thumb he says that if an ordinary person
can do something in one second then we
can probably make a machine learning
system do it if we collect enough data
and I think that's approximately right
we have fairly dexterous you know
folding towels requires a fair amount of
dexterity for example but not perfect
dexterity we have wonderfully agile
robots if you look at Boston Dynamics
big dog and Atlas robots for example
they're really incredibly impressive and
to a large extent we would say that leg
locomotion is pretty much a sole problem
when you look at the agility of
quadcopters for example from Vijay Kumar
scoop they're unbelievably scary and so
we're in a good position with regard to
the physical platforms that we have
although I think the the big open
problem is still the truly dexterous
hand and manipulations using those hands
and the perception is now to the point
where it's certainly getting across the
threshold of enabling self-driving cars
for example and that means that we will
start to see application
where robots are operating successfully
in unstructured environments in homes in
agriculture and so on the other areas
where we might see better performance as
a result of improved perception and also
improved modeling is the smart home
which has been you know just about to
happen for the last 50 years or so
literally I mean there were there were
smart homes being designed and built 50
years ago they just weren't very smart
the AI part was hopeless and so it was
an unbelievable nuisance to live in one
of these houses but that I think is
going to be soulfully soon we're not
going to have real natural language
understanding for a while but we'll have
natural language understanding that's
sufficient to build web scale question
answering systems and we're already
seeing for example Google is gradually
shifting from returning the results of
keyword indexing to actually returning
the answer to your question another
application of shallow natural undred
language understanding will be the
personal digital assistant a lot of
those are available right now but
they're done by Wizard of Oz technique
is actually a whole lot of humans on the
other end of that system and not AI but
it's very clear what's going to happen
in that sector and other amazing tools
will arise I think for for doing country
scale micro modeling of economic
processes and all kinds of medical and
scientific research so there's still a
lot missing and I think Ramone was
correct to say we we are not close to
having human-level AI we don't have real
understanding of language we don't
really understand how to integrate
learning with knowledge and one of my
favorite areas is to figure out how it
is that humans manage to make decisions
on scales so with respect to our
primitive actions our primitive steps
which are motor control actions coming
to which Chi is more than a billion
steps now alphago does successful
decision-making on the scale of 20 to 30
and that was considered an amazing
achievement right but there's nothing
that's going to
20 to 30 up to billions of steps now
clearly we don't do look ahead in detail
we don't plan out the billions of steps
that we're gonna take but we plan it out
at some multiple levels of rejection in
a very complicated interlocking
hierarchy of activities and this is
still something that that's in its
embassy in artificial intelligence
particularly the discovery of that
hierarchy how do you and we build it in
the first place and similar remarks
apply to our development of concepts and
theories so when you think about physics
how many layers of concepts are built up
before you get to the Higgs boson is
centuries of discovery and invention of
those concepts and we don't know how to
do any of that really in AI now it's not
the case that just adding more CPUs and
faster CPUs or quantum CPUs is going to
solve these problems and all that's
going to do is get the wrong answer more
quickly so it's going to require these
conceptual breakthroughs but it's hard
to predict exactly when those conceptual
breakthrough they're going to occur and
some of you may have seen this before
and some other talks but I want to
re-emphasize the unpredictability of
these kinds of conceptual breakthroughs
and the significance of the breakthrough
when it does happen so this is Ernest
Rutherford who was the equivalent of the
President of each guy if you like he was
the the distinguished senior figure of
nuclear physics in his time and he gave
a number of speeches this is just one of
them on September 11th in 1933 in
Leicester where he said anyone who looks
for a source of power and the
transformation of the atoms is talking
moonshine and this was reported all over
the world and he was absolutely adamant
that all this talk of extracting the
energy that they knew to be there in the
atoms they knew exactly how much energy
there was because a relativity theory
and he was adamant that we would never
get it out
this is Leo's illa that he read a
summary of this speech in The Times and
he went out for a walk and crossed as he
was crossing the road he'd figured out
the answer which is to have a neutron
induced nuclear chain reaction which
would create a new possibility of a
nuclear explosion and he also figured
out how to have a damped negative
feedback system which would enable you
to maintain it just at the subcritical
level where you could have nuclear power
and he patented both ideas fairly soon
after that so it went from never
completely impossible
to pretty much solved at least on the
conceptual level within 16 hours so I
would say do not bet against human
ingenuity and do not bet the future of
humanity on our inability to solve the
problem of AI so I'm gonna state this as
a premise and you could say I'm not
making this as a definite claim but at
least I would say I don't believe the
negation of this premise which is that
it's impossible that we will ever reach
AI human-level AI was super human AI and
I find it a bit distressing when I see
senior members of the AI community
stating in print that we will never
achieve human-level AI and having the
field has spent 60 years fending off the
critics who said that you know you guys
are too stupid you have no idea what
you're talking about
you're never going to have human-level
AI it's impossible and we've spent 60
years fighting off that criticism and
now with when it seems like it might be
possible we're saying oh no no no it's
never gonna happen so there's no
possible risk of that right that that
seems like a defensive reaction that's
not based in real thought so
if this is true if systems will be able
to use for example their ability to read
everything that the human race has ever
written they'll have a lot more
information than any human being does
we've already seen and i in alphago that
they're better at doing look ahead and
if we solve the hierarchical law
timescale issues then they will be able
to look further into the future with
more accuracy than we can and so they'll
be able to out decide us as humans so
this has some wonderful consequences I
don't want to say that better AI is
necessarily bad it's actually
extraordinarily good because everything
we have comes from our intelligence so
if we have access to more of it this
cannot be anything but a step change in
civilization in our capability to
improve the overall quality of human
life to the greatest extent that's ever
happened so it's a very amazing
opportunity now people talk about the
downsides and some of them are actually
quite immediate here's one of them this
notion of using artificial intelligence
to develop weapons that can choose to
kill humans
in fact there's there's two downsides in
this slide I'm only going to mention the
killer killer robot one people sorry
people talk a lot about the issue of
employment this is still very much
subject to debate I think gradually the
the economics community is coming around
to the idea that in fact this is real I
was at a meeting where there were
several Nobel prize-winning economists
and they all said that this is the
biggest problem facing the world economy
but this is what I want to talk about AI
possibly being the end of the human race
and what what could that mean right and
there is a lot of noise about this in
the press and and some of it may look as
if it's scare mongering but I think you
have to remember that what the press
does is take anything you say multiply
it by a thousand take out the bits you
actually said and then report the
results and so you have to you have to
filter what you read in the media to
understand probably you know to guess
what was actually said so if you
remember for example there was a report
a couple of years ago that was written
by several people in the community
called research agenda for robust and
beneficial AI which was the most boring
document that we could write it did not
have the word risk it did not have the
word extinction existential none of that
stuff it just said the only mention of
anything down any kind of downside was
that it was important to understand both
the benefits and the potential pitfalls
of progress in artificial intelligence
and still journalists published articles
with terminator robots all over them and
said that Elon Musk was predicting the
end of the world and so on and so forth
so and this is one of the reactions
among many from the AI community right
well the people who are talking about
this they don't know anything about AI
right they're not even real scientists
well some of them are real scientists
and here's one example if we could keep
the machines in a subservient position
for instance by turning off the power at
strategic moments we should as a species
feel greatly humbled so who said this
was it was it one of these media
pontificate errs know this was Alan
Turing 1951 and this is one of the
sources of unease that people might have
you make something that's more
intelligent than you are what do you
think is gonna happen so that unease we
could call it the gorilla problem
because the gorillas did it or at least
their ancestors did it a few million
years ago they produced the human
species which is a lot more intelligent
and the gorillas and they're having a
meeting here to discuss whether it was a
good idea and you can kind of tell from
their general sort of demeanor and body
language that they're really pretty
unhappy about about that decision it was
a terrible idea to produce the human
species because now they're in a lot of
trouble but if that's the only source of
unease there's really not much we can do
about it except stop right and then we
don't get any of the benefits so we have
to understand just as with other
powerful technologies like the nuclear
technology clearly
nuclear weapons were a potential
downside of nutri of nuclear technology
so the fact that uranium sufficiently
enriched and a large enough mass of
uranium explodes is a downside of
nuclear physics so how do you prevent
the explosion right reasonable question
but to do that you have to understand
why it explodes and then figure out how
to prevent it
so why is better AI a problem right what
is the nature of the process by which
something bad could happen and this is
another quote we'd better be quite sure
that the purpose put into the Machine is
the purpose which we really desire and
this is the core of the issue and this
was pointed out by Norbert Wiener in
1960 the founder of cybernetics
brilliant mathematician and he actually
wrote this having just seen Arthur
Samuels checkered playing program
learned to play checkers much better
than Arthur Samuel could play and he
sort of extrapolated from there and he
was very explicit that this is not an
immediate thing this is something on a
very large time scale but what we do now
is going to have consequences on that
very large time scale that makes it very
difficult to know what is the right
choice but because of the scale of the
consequences we have to think extremely
hard about it now you could also
attribute this quote to the King Midas
because he had the same problem
he put a purpose into the machine I want
everything I touched the turn to gold he
got exactly what he asked for and then
his food and drink and relatives turned
to gold need to hide in misery and
starvation and this is ubiquitous in
human mythology there's the genie who
asked the three wishes the third wish is
always please undo the first two wishes
because I made a mess of it right we are
terrible at specifying what it is we
really want and that's the nature of the
problem if we don't get it right the
first time and you put it into a super
intelligent machine then you going to
face
a competition between you and the
machine because now you don't have the
same objective you have different
objectives and so it's a non-cooperative
game now more recently Steve Omohundro
wrote a paper talking about what's
called instrumental goals and these are
the goals that follow as a consequence
of any original prime objective so for
example even if the objective is sample
something as simple as fetching the
coffee a sufficiently intelligent
machine realizes that if someone
switches it off it won't be able to
fetch the coffee and therefore because
it has the objective of fetching the
coffee it necessarily has the objective
of preserving its own existence so we
don't build in self preservation as a
muffs third law that the robots supposed
to preserve itself is completely
unnecessary because it's a logical
consequence of any other objective there
are very few things you can do better
when you're dead than when you're alive
except for putrifying so so this is the
takeaway message just to remember this
one thing you can't fetch the coffee if
you're dead okay now if you combine this
notion that a system is going to
preserve itself against any attempt to
interfere with its pursuit of the goal
it will also try to acquire
computational physical financial
resources to improve the probability of
success - to ward off any possibility of
failure and you combine that with
misalignment in the objectives then you
get not the Terminator robot which is
the sort of spontaneous malevolent
consciousness problem but you get the
2001 Space Odyssey you get hell because
how has that objective which is not
aligned with those of the two humans on
the spaceship and he ends up in conflict
with the humans on foot I guess
fortunately for the humans how is not
super intelligent eventually Dave does
outwit Hal and managed to turn him off
but had how really been super
intelligent that would not have happened
so I'm not sure what happened to this
slide anyway so it says reasons not to
pay attention and there are a lot of
them and I think the AI community is
coming up with one after another reason
to ignore this issue because it's a
difficult issue it makes you worried it
makes you feel under attack it makes you
feel like you might be a bad person if
you're contributing to the extinction of
the human race I think that's the wrong
response denial is not that it's not
going to solve the problem so one of
those responses as I mention is it will
never happen right now I just refer you
back to Rutherford and is and Zillah in
1933 there's a response well it might
eventually happen but it's too soon to
worry about it
well what's too soon right if if if I
said that you know a giant asteroid is
gonna hit the earth in 40 I guess that's
49 years time would you say okay well
that's too soon to worry about it just
you know come back in 2064 and we'll
have another conversation no no that's
not what would happen we would
immediately put the entire scientific
and engineering community of the planet
Earth into action to try to figure out
how to deflect will destroy this
asteroid before it hits the earth
because we don't know how long it's
going to take to solve the problem and
we don't know how long it's going to
take to solve the problem of controlling
intelligent systems that are more
capable than ourselves so there's a lot
more reasons which I won't go through
you can you can read them as they go by
I do want to point to one of these you
can't control research that's another
response well you know yes you're right
it is going to destroy the human race
but there's nothing we can do about it
so it's gonna happen because we're going
to continue doing research
well that isn't true we've controlled
the genetic engineering of humans for
more than 40 years and that might be
breaking down right now but clearly
there is evidence that when we put our
minds to it as a community we can decide
to do and what not to do I'm not
necessarily recommending that we stop
doing any research what I'm recommending
is that we work on solving the problem
I'm not going to talk about yeah we won
the Luddite of the Year award but that's
kind of odd because the Luddites include
Turing Minsky and all between err Bill
Gates Elon Musk Oh some of the major
technological figures of the 20th
century it's hard to describe these
people as Luddites but don't worry we
can just switch it off right this is
something that I've heard very senior
and successful intelligent brilliant AI
people saying as a reason not to even
think about this problem well of course
it's probably likely that a super
intelligent system would think of that
and another one yeah I don't mention the
risks it's bad for funding I'm not
kidding this is this is again in writing
from very senior figures in the
artificial intelligence field and I
would just point to the history of
nuclear power where the nuclear industry
did not mention the risks and anyone who
talked about the risk was was treated as
you know some pinko commie who needed to
be silenced and then Three Mile Island
happened and then Chernobyl happened and
a nuclear industry was destroyed by its
own insistence that there were no risks
so this is not a profitable strategy nor
is it an honorable one okay so let's
assume that I've convinced you now
you're asking I hope what do we do okay
what do we do so just all I can do is
tell you what I think I'm gonna try to
do we set up a center for human
compatible AI and the goal is to figure
out how do we make AI systems that are
provably beneficial right that that
cannot act in ways where we are opposed
talk unhappy with what they did or you
know or we might be dead in which case
we're very unhappy with what they did
and there are actually a lot of other
groups now working on this problem
there's the future of humanity Institute
there's the Caesar in Cambridge which
also has a new center called the Center
for the future of intelligence funded by
the levy whom trust there's me read
machine intelligence Research Institute
the future of life Institute open AI the
partnership on AI has among its goals to
solve these problems and funding
agencies and professional societies are
getting getting the idea and trying to
help solve this problem
so provably beneficial AI is it's almost
a an oxymoron because provably makes you
think about theorems beneficial makes
you think about touchy-feely wishy-washy
goodie woody kinds of things and it's
hard to put those two things together so
what I mean by that is is the following
right we're not going to be able to
prove beneficial you know an AGI that
lands from outer space right and a lot
of the literature in the AGI community I
think sort of views the AGI is to some
black box that landed from outer space
and is arbitrarily intelligent and has
arbitrary objectives well if that's the
problem we can't solve it we are toast
but that isn't the problem we have to
figure out how to design things to avoid
these problems in the first place so
we're going to have to have a formal
framework a formal definition of what is
the problem that we want to build us the
system to solve let's call that the F
framework and we're gonna build F
solvers and we have to operate under the
assumption that this F sole that can be
arbitrarily capable that the components
out of which its constructed can be
arbitrarily good at the job that those
components have in the architecture but
if we design the the training regimen ie
these sort of the the the objective
function for learning and the way the
subsystems are connected and so on if we
do it in the right way we want to be
able to prove that the human is going to
be better off with the system that we're
designing
so there are three simple ideas on how
to do this one is that the robots only
objective is to maximize the realization
of human values so this is its only
objective and I'll say what it in a
second what I mean by human values it's
probably not what you think the second
point is that the robot has to be
uncertain about what the values are this
is crucial
the robot should never believe that it
knows for sure what the objective is
unless it is in fact correct about what
the objective is and then the third
thing is the robot has to learn
something about the objective and it
does that by primarily by observing
human behavior that the human choice
behavior provides in some sense ground
truth and it's a complicated sense
ground truth about the underlying human
objectives so let me rephrase that
to get rid of this of this possibly
problematic notion of human values we'll
talk about puffles instead so puffles of
preferences over future lives so this is
what I really mean I don't mean that the
machine is trying to somehow develop by
observing humanity an ideal ethical
theory that's not what this is about
this is just about the ability for a
machine to predict what each person
would prefer their life to be like and
then to try and help them get that so
why is uncertainty in the objective
important it's important because it's a
form of humility and that humility is
what allows the machine to be controlled
by us humans and to be safe and you
might ask well okay we've had
uncertainty in AI since you know late
70s we've really focused centrally on
uncertainty on certain sensory
information on certain transition models
in Markov decision processes and so on
but we've always assumed that the
objective is known for sure perfectly
and why is that well if you actually
look at the definition of MVP if you add
uncertainty to the reward function it
doesn't matter because you get exactly
the same optimal policy whether you have
a distribution over objectives or you
replace that distribution by its expect
because everything is linear and it just
works out that way so in an MDP and upon
DP uncertainty in objectives has no
effect on the optimal policy unless the
environment can provide additional
information about the objective so the
irrelevance of uncertainty only holds in
environments that cannot provide
information about the objective but as I
just said anything that a human does in
the world provides information about
that that humans objectives and so the
standard formulation of MDPs and poly
piece is actually inadequate to handle
this issue so observe all human actions
are one source we could also just have
you know human and special kind of
action is to just give a reward signal
as happens in reinforcement learning so
the general theory of how a AI is going
to work has to include both the human
and the machine as agents so thinking of
the agent itself decision making
decisions with respect to its own
objectives is a special case that's
actually extremely restrictive and I
think that was probably a mistake that
this is the way we thought about AI for
quite a while and inevitably then the
human is going to have special status in
this theory and if you sort of look at
it of course it's going to have special
stages because we're not doing AI for
the benefit of bacteria right we're
doing AI for our benefit and so we're
bound to have just as in economics
there's a theory called principal agent
theory the principle is supposed to be
benefiting from the work of the agent
and that's what AI is supposed to be
about we're building these intelligent
systems to help us so here's a graphical
model way of thinking about it right
here's here's the classical idea right
that there's a human objective which is
observed right and it affects human
behavior but it's also given to the
Machine and defines the Machine behavior
and so that the fact that that variable
is observed then decouples the machine
behavior from the human behavior in fact
once the machine has the objective it
doesn't care what
human does writes completely irrelevant
but in fact it's this is not really the
case because really the human objective
is not directly observed we don't know
the full preference over future lives
that human beings have and so when the
human objective is not observed then the
human and machine behaviors remain
coupled this is a notional graphical
model I'm not trying to be too
mathematical here but this is an
inevitable consequence that this becomes
a multi-agent problem so let me give a
few simple examples one image
classification we're all aware of the
googles PR disaster when their image
classifier classified some people as
guerrillas well how did that happen I'm
betting I haven't actually you know
asked them what happened but I'm betting
that they were using a standard machine
learning algorithm that minimizes the
squared loss and that loss comes from
the loss matrix which tells you the cost
of misclassification and they didn't
even bother to write the loss matrix
right they just assumed it was uniform
and that was an incorrect description of
their true value function so how should
it have gone the machine learning system
should have not a loss matrix but a
distribution over loss matrices which is
a much more complicated high dimensional
thing and it should refuse to classify
somewhere the examples because it's not
sure that might be too risky until it
knows more about the details of loss
matrix that should say I don't know what
this is or I'm not going to say what it
is or what I think it is because it's
too risky and maybe ask the human to
help out and say you know is it is this
a bad idea to classify one of these as
potentially one of those and so you get
a much more complex and rich interaction
between a learning algorithm and and the
owner of the learning algorithm to make
sure that you don't have those kinds of
disasters another example is just going
back to this effect in the coffee so
when you fetch the coffee the
traditionally we view that as just a
goal for a planning system right and in
utility theoretic terms that would mean
that any state where you have the coffee
has value won in any state well you
don't have the coffee has value zero I'm
simplifying but we viewed it as a
specification for what successful
behavior means that's completely untrue
right if I said to the robot fetch the
coffee and I was in a really expensive
hotel in Paris and it came back you know
and I charged 31 euros to my credit
credit card to buy a cup of coffee I
would have said you idiot you know why
why did you pay 31 euros for a cup of
coffee well you know you could have gone
somewhere else or or just come back and
said it's too expensive you know you
certainly wouldn't want the the system
to you know kill all the other people in
Starbucks so that it could get the
coffee more quickly and so on so there's
a lot of stuff involved and you might
say well if the robot doesn't know all
these things how is it going to be any
use whatsoever until it's sort of learn
and do all the things you care about
this is gonna be paralyzed well actually
it's not quite true because as long as
it leaves the rest of those things if it
doesn't know whether they're you really
care about them or really don't care
about them if it doesn't know the value
of all the other attributes at the
environment as long as it doesn't mess
with them then the instruction fetch the
coffee says you know I would be better
off if I had coffee they're not all
other things being equal so leave all
the other things being equal and you can
still be useful to the human even though
you have the great degree of uncertainty
about their overall preference structure
so we briefly talked I think I'm gonna
run over a little bit let me briefly
talk about value alignment so how do we
get the the values actually in line how
do we learn more about values by
observing human actions so there's a
subfield of AI called inverse
reinforcement learning which is the
inverse of reinforcement learning
instead of learning behavior from
rewards we learn rewards from observing
behavior and this this seems to work
quite well because you know if you think
about the other way around when I want
to get behavior out of a machine the
reward function is usually the most
concise way to describe what I want so I
can get great go playing behavior out of
a machine
just by telling it what it means to win
lose at go that's the only thing that we
tell Alfa go along with the rules we
just tell it what it means to win and
lose and it figures out how to behave
from that information so it's a succinct
explanation for arbitrarily complex
behaviors now we actually want something
more sophisticated than that because as
I said the human has to be involved the
humans behavior is coupled in and we
really need a two-player game or in
general and you know it'll be in humans
and M robots but just think about one
human and one robot so the human acts
according to their own internal
preference structure and but may not be
able to explicate it exactly the robot
has to optimize with respect to that
preference structure but doesn't know
what it is so that's the game and when
you solve that game you find the Nash
equilibria have exactly the properties
that you hope that the robot will ask
questions before it does anything rash
the human will try to teach the robot by
demonstration what what it like more the
human does and doesn't like so let me
illustrate one very simple instantiation
that's called the off switch problem so
in the off switch problem the robot has
the objective like fetch the coffee and
azzam mohandro pointed out it has an
incentive immediately to then disable
its off switch so that nothing can
prevent it from getting the coffee and
we don't want that to happen right that
seems like an extremely unsafe
development if that's how machines are
going to behave so how do we stop it
right I mean it seems like it's just a
logical consequence of the objective
that we gave it but somehow we have to
stop it and it's no good saying well
just give it a rule saying it's not
allowed to disable your switch because
it has an incentive to disable your
switch so just as we find ways around
tax laws it will find ways around and
eat any attempt to to prevent it from
disabling the off switch so the answer
is once you have uncertainty about the
objective once the machine knows that it
doesn't know the full objective then it
has an incentive because the human will
only switch the robot off if the robots
doing something or is about to do
that human doesn't like the robot
doesn't know what that is but it knows
it doesn't want to do things the human
doesn't like that's that's rule number
one
and so it should actually be quite happy
to be switched off if it's uncertain
about the objective and you can prove
this mathematically that there's there's
a positive incentive to allow yourself
to be switched off and that incentive is
in some sense monotonically related to
the amount of uncertainty you have about
the objective and when you design the
robot that way you can show that it's
provably beneficial to the human
okay I may have to zoom through this a
bit
okay so if you want more details those
are the two papers that are in the
Proceedings on 1:00 this afternoon and
one on Thursday so please go my students
will be presenting those papers so
here's just a little preview of how it's
set up so it's an incredibly simple
model just to illustrate this concept so
the robot gets to go first in this game
and it can choose to either do some
action immediately so sort of bypassing
the off switch and that action has some
value you a which is unknown so there's
a distribution over the values because
it doesn't know how much the human will
like this action or the robot can just
switch itself off so it can just commit
suicide because it believes it's not
good enough to act right but a third
choice is to say okay let me give the
human an opportunity to switch me off so
we tell the human okay I'd like to do a
but I'm gonna wait and you can switch me
off okay then the human can choose to
switch the robot off in which case it
gets value zero or it can say well I'm
not gonna switch you off I'm gonna allow
you to do action hey so then the robot
will do action a and what does the robot
believe at that point about the value of
the action well it knows that it must be
bigger than zero otherwise the human
would have switched me off and so it has
a posterior distribution that looks like
that right then the negative part of
distribution has now been conditioned
out and you can show very simply that in
this case waiting and allowing the human
switch you often is always preferred to
either switching yourself off or
bypassing the off switch and just doing
the action and this is completely
straightforward theorem it's exactly
like a non-negative value of information
theorem in fact that's what it is
because the humans decision is providing
information to the robot about what the
utility function is so in the interest
of time I'm going to skip over how we
solve the wire heading problem but we
can talk about that afterwards and just
let me wrap up by saying I'm reasonably
optimistic that this research agenda is
going to be fruitful and we've already I
think made a little bit of progress
along the line towards probably
beneficial AI so one reason for optimism
is that sufficiently intelligent AI
systems particularly ones that can read
will have access to arbitrarily large
amounts of data about human behavior
because everything we've ever written
describes people doing things and then
other people being upset about it so
that's good right so we can we can go
look at everything the human race has
written and learn a lot from that
another reason for optimism is that
there's actually a very strong economic
incentive to solve this problem long
before we get to super intelligent AI so
let me just give you one example right
so here here's the Rope domestic robot
and domestic Roberts home and you're
late from work and the kids are hungry
and there's nothing in the fridge and
then the robots use the cat and
something happens the robot course
doesn't know the nutritional value of
the cat is not as high as the
sentimental value of the cat and so so
the cat gets cooked for dinner alright
so this one incident would be the end of
the domestic robot industry right
because it lacks it lacks a certain form
of common sense we thought up to now
have common sense knowledge as being
about you know what happens when you
spill the milk or you know putting
blocks on top of each other and things
like that but a lot of human common
sense is about what people want and
don't want and really we don't really
distinguish between these kinds of
knowledge and mathematical theories do
one is transition model and one is value
function but humans are expected to
learn the value function properly and if
they don't they're considered
psychopaths and you know no one wants to
have a psychopathic domestic robot so
that would be a strong incentive to get
it right there are some difficulties
right I'm not I'm not saying this is
straightforward I think it's you know
decades of research to really get it
right that's one reason to start work on
it now and and its really us
right because to understand what we want
even assuming that there is some way of
describing us as wanting something
having any kind of stable meaningful
preference over human lives so assuming
that we do we have to invert our
behavior to get at it and our behavior
is produced by our cognitive system
which is incredibly complicated
computationally limited and very very
inconsistent so we have to invert
through this whole complexity of human
psychology which of course seems to be a
difficult problem so there are some
drawbacks but let me just point out for
example computationally limited you know
when Lisa Dahl played there was probably
one move where he changed the game
theoretic value of his game from maybe
drawn a 1/2 loft so he made a losing
move now if the system observes that
losing move it shouldn't say oh Lisa
doll wanted to lose the game right he
was actually trying to win but his
computational limitations prevented him
from making winning lose so we have to
understand human computational
limitations or come you know as well as
computation limitations of machines in
order to understand human behavior we
have to understand how our behavior is
generated and it's generated actually
from an extremely complicated
multi-layered hierarchy of interlocking
subroutines of behavior you know I'm I'm
in the giving a talk at each guy's
subroutine and there are many things I
can't do in this subroutine I can't sort
of pick a baseball out of my pocket and
throw it at the audience I can't you
know by you know take out my phone and
start buying AT&amp;amp;T stock it's just not
part of this subroutine so we have to
understand the subroutine structure of
human behavior in order to understand
what someone is doing and a lot of us
are nasty now I want to point out just
because we nasty does not mean the robot
is going to act like us right this is
just not the case
we are nasty often because we are
selfish and we don't care that much
about the well-being of others and we
have our own objectives to pursue but
the robot is purely altruistic it
doesn't have any objectives except those
of the people it's trying to help and it
doesn't have to act in a nasty way in
order to do that so it's just learning
to predict what people
want and it is not learning to one
things there is one caveat to this that
if people actually have a negative sign
on the well-being of others we don't
want the robot to take that into account
in how it trades off the well-being of
of those two people right so I think we
have to filter out one aspect to
people's value functions which is the
pleasure they derive from the
unhappiness of other people and I don't
see a way around that if someone can
figure out a different solution I'd be
very happy
so just to summarize I think that if we
follow the path that we have been on
which is building intelligent systems
into which we put objectives then we do
face this risk from misalignment and
that's a serious problem but if we
change the way we think about AI from
this unary notion that doesn't tell the
system with with objectives that it
somehow has and is pursuing to a notion
where the only objectives reside in us
and the machine is simply there to try
to help us with those objectives that's
actually a better way of thinking about
what AI should be and it solves this
problem of misalignment and it seems to
give us some some degree of safety and
if you're worried about the old form of
AI don't worry it is just a special case
where the uncertainty is reduced to zero
and and the machine is absolutely
positive it knows what the objective is
but it seems to me that in doing this
work the actual nature of humans is
going to be inextricably intertwined
with with our field and that's a change
and it makes me nervous because actual
humans are extremely complicated but I
think we have to do this I think we have
to also not think about this as a
separate field of AI safety and draw the
analogy to civil engineering
a civil engineer says I build bridges he
doesn't need to add oh but I'm you know
I'm a I'm a safety bridge designer I
design bridges that don't fall down
right it's built into the meaning of the
word bridge that it's not supposed to
fall down and it should be built into
the meaning of the word AI that is
beneficial to us and we shouldn't we
shouldn't approach this as there's AI
researcher than that as the safety
people nagging and the ethics people
nagging the AI researchers it should be
us intrinsic to what we do as a field
and I think in the process of making our
preferences over future lives and our
value structures more explicit we may
actually become better ourselves thank
you and sorry for running
thank you very much for the sake of time
I think we will escape the session the
questions I mean you can meet the
steward during the week and talk to him
so now we are going to break for coffee
there will be coffee served outside but
also on the second level so some of you
can go to the second level for coffee
make sure you are at the session at
10:30 sharp okay bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>