<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AI Ethics Panel: Russell, Yudkowsky, Tegmark, Wallach, &amp; Petersen | Coder Coacher - Coaching Coders</title><meta content="AI Ethics Panel: Russell, Yudkowsky, Tegmark, Wallach, &amp; Petersen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AI Ethics Panel: Russell, Yudkowsky, Tegmark, Wallach, &amp; Petersen</b></h2><h5 class="post__date">2017-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LShKHZkc34M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we get one more chair for the for the
stage otherwise Steve is really gonna be
towering over the super intelligent
super ethical and super tall
I thought we start off by seeing if
anyone on the on the panel has questions
for still taller start off by seeing if
anybody on the and the group has
questions or replies can I just quickly
seize the mic to ask for a marker so I
can write intelligence or and you can
find the rest of my slides resources any
chairs so I think that David Humes
argument still goes through for
something like meta preference
frameworks and there's going to be more
than one of those that is reflectively
consistent in the sense that an agent
reflecting on its preference framework
will approve of that framework so indeed
what is probably a very large space of
reflective reflectively consistent meta
preference frameworks including
frameworks for solving the ontology
identification problem of figuring out
exactly what is a paper clip when your
goals have previously been specified
over a representation that does not
exactly match physics so like somewhere
in that space is probably a reflectively
consistent framework that would look
back in its history look at the humans
figure out what humans wanted and
implement what humans wanted in a way
that we would find intuitively
attractive and stop there and not go
back to have further to natural
selection and decide that inclusive
genetic fitness is everything but it's
going to be like a special point in the
space you'd have to reach into the space
with very precise
targeting and pick it out and then have
me yell at you about how there was no
way to test us without an actual super
intelligence and you should have tried
something much simpler instead for your
first day out your second day I can be
more complicated your first thing I
should but should not be that
complicated it's probably it's probably
a fascinating conversation about whether
it should be stretched into into how how
Eleazar use it but I think you must I
think just saying that values aren't
things what is you know is not what is
the things you can point to are not odds
so it's really it's really a question
about how we really look at what values
are now and if you notice and a lot of a
conversation here is this movement to to
reinterpret values through human
behavior and that human behavior is
telling us what the values are should be
that's a that's a complicated question
about whether that's effective or not
and that's I think that's all I was
trying to bring out in this so yeah so
actually like talk a little bit about
the things he was saying I mean I I
think actually there is some overlap
between what he's saying and what I was
saying but you know one way to think
about it is when we build AI we're
creating you know a particular piece of
software and given the rest of the
universe and that particular piece of
software some history is going to unfold
from that point forward and I think
Steve is saying that if this piece of
software satisfies certain internal
coherence constraints
I cannot design it such that the history
that unfolds is is really undesirable to
humans and I don't think that's true I
think I could pretty easily design it so
it satisfies these internal go here
constraints and and still produces some
pretty awful consequences so so I think
it's it's more than internal coherence
constraints I think we have to give
special stages to humans in in a way
that's more than just connecting it to a
tailored teleological thread that passes
back through humans I mean if you you
know if you go back to you know sort of
the teleological purpose of life itself
you know humans are pretty good at
getting rid of a lot of life and
reducing its diversity so so perhaps the
right decision would have been never to
have humans in the first place at all
but I don't think we want to go that way
so I but I I I sympathize with the idea
that you know going back to King Midas
right
what should the deity that granted King
Midas's wish have said if it was a
really intelligent deity it should have
said well you don't really mean that
everything is gonna turn to gold how
about we make a little procedure where
you point to something and you say the
magic word and then I'll make it turn to
gold right and I'll give you another
magic word to reverse that if you need
it right that would have been an
intelligent intelligent response right
but that's that sort of presumes that
someone designed that deity that AI
system where the understanding that that
humans are just providing faulty
information about what they really want
when when they specify tasks and goals
and so on and that's exactly what I'm
proposing we have to figure out how to
do
I'm no I mean I so yeah i i i hope
nothing I've said suggests that you guys
don't have to you know you can quit your
day jobs but I mean yeah I I guess I'm
just trying to provide some hope for the
possibility that like just being a
certain kind of coherent just value
learning is enough plus this plus the
blurred lines between agents but you
know I I had to as I say I was kind of
like just trying to respond to boss from
as best I could and the best I could was
maybe budge the needle a little of risk
assessment that you brought up in your
talk Steve is how even though
instrumental sub goals can sometimes be
very specific for the final goal is
often kind of way vague and squishy and
and as a physicist again I feel that
that can be that's something we really
need to think hard about when we start
talking about what our ultimate final
goal is gonna be in the long term
because it's it if we if we start
looking for the final goal that we want
our cosmos to evolve towards as far as
we can tell from what we've learned from
science so far you know we humans are
not the optimal solution to anything we
are a historical accident so any thing
we've so far been able to write down is
a final goal that's specific enough we
can do an equation for know if we
actually drive towards that goal the
final result is not gonna involve humans
right so I I'm not sitting here claiming
I have an answer to it but I feel that
the vagueness of what we often view as
final goals is disturbing and it's
something we all really need to think
hard about
I want to make a distinction between
what are these top-down approaches which
seem to have final goals and bottom-up
approaches which I tend to favor frankly
and so it's approach Falls very much
into that is that you are trying to
maximise but you don't necessarily have
a specified goal that you are focused on
so even in a genetic algorithm than
they're trying to maximize let's say
investments that has been used in a
financial thing without necessarily
having a procedure or a clear-cut idea
of how you do that if we humans are
trying to maximize something that may be
to understand what the hell our goal
should be in the context of procreating
and surviving and enjoying ourselves
along the way but we don't even know
what the goal of human life is beyond
that in the context of the finality you
presented it as an argument against the
thesis but you might also think that
well the orthogonality thesis is
actually very weak it says that it's
possible for intelligence and final
goals to vary independently nothing cut
inclined to find that plausible but
that's just to say ok somewhere in AI
space an intelligent space there are
some architectures in which these things
vary independently that's totally
consistent with a big vast regions of AI
space where these things don't vary
independently so one thing you could you
could see your project is doing is
trying to find out well here is an
architecture which involves a certain
kind of value learning for which means
end reasoning and final goals are not
going to value very independently but
which will turn out to be substantially
correlated as far as I can tell
at least the orthogonality thesis as
classically stated is very much
consistent with there being all kinds of
correlations one example we were talking
about yesterday take the ideal gas law
PV equals NRT
well pressure and temperature are highly
correlated they also obey the they also
obey the orthogonality thesis it's
possible to have arbitrary values of
pressure
arbitrary values of temperature
depending on how you set the volume
parameters the orthogonality thesis
alone doesn't seem to settle very much
and if it turns out there are
it's consistent with their being this
class of architectures for which the two
go together and that strikes me is at
least if we're looking to develop
beneficial AI focusing on that kind of
architecture there's maybe a place to
look yeah I have one thing that
impressed me about vos from yesterday is
he really seems to think in terms of
possibility spaces and probability
distributions over them I have a little
bit more trouble with that but I guess
yeah it's it sounds like there if I'm
using this terminology right there might
be sort of a tractor basins where you
just want to get in this region and then
we can slide downhill but and that would
be that would be a plus anyway right
yeah if you're doing anything
complicated make sure you test it first
on a superintelligence that is
completely harmless sort of like in this
way it sort of like both David and
Wendell simultaneously they're like the
the utility function way of looking at
it is on the one hand sort of like one
of those theoretical ideals that you
never get and on the other hand
extremely likely to be practically a
very good approximation because we have
all these wonderful things called
coherence theorems which say something
like if your uncertainty violates the
probability axioms or your
decision-making is not compatible with
having any coherent utility function
then the dead will erupt from their
graves the seas will turn to blood and
the skies will rain down dominated
algorithms and combinations of bets that
produce certain losses so there's a
sense in which anything that cannot be
seen as having a utility function by the
theorems must have been in some way
stepping on its own foot going in
circles like engaging in combinations of
Gamble's such that like there's some
other combination that strictly
dominates under like whatever its goals
could have been or conversely if there's
no way to view it as being incoherent in
the sense we can see it as potentially
having a utility function even if that's
not how it actually works underneath the
hood like doesn't need to actually have
like a little utility function section
of code in order for us to be able to
view it as having a utility function and
I suspect that similar
it like theorems like this lead one to
suspect that there isn't that a stronger
version of the orthogonality thesis is
going to be true which is something like
there's simple or obvious D couplings or
there is no coupling such that it is
hard to to build a non nice ai a point
which I strongly personally suspect
because I have a saying write down
anything formal you can do with say a
hyper computer so you don't have to
worry about a lot of the practical
issues but like write down anything
formal you can do and I will explain to
you how it destroys the world if it is
sufficiently formal I'm not sure the
issue is whether you have a utility
function or not the issue is whether
having a utility function is consistent
of the Meccan with the utility function
being highly emergent from a whole bunch
of basic mechanisms and is consistent
with the view where the basic mechanisms
that underlie say utility dynamics are
highly overlapping with the basic
mechanisms of until I say belief
dynamics or planning dynamics so just
say yeah you had a mechanism which did
both of these would like would final
goldwood utility and means in reasoning
be orthogonal
well maybe not because there were common
mechanisms and that's the you know the
class of the class of views that Steve
was talking about I think was can be
understood maybe as a kind of view on
which the mechanisms underlying value
learn you've got a lot to do with the
mechanisms underlying belief learning
and or not so orthogonal so the history
of the AI might be such that there was
sort of like a blurry confused phase of
its youth where the utility function
precipitated out from things that that
blurred all sorts of things that we can
view ourselves from this perspective in
a sense but the the coherence theorem
suggests to me that as it self modifies
self improves gets older things will
shake out and one of in any number of
possible ways but however they shake out
you'll be able to point to it and say
like when this thing makes observations
the thing that corresponds to like an
epistemic model the probability
distribution is updating and this thing
that we're going to call the utility
function is staying constant and if
that's not true like the skies are rate
raining
the strategies or something did anyone
else want to come in before we go to the
audience max so we've had a lot of
interesting discussion at a quite
abstract level here which is very useful
but I think it's good to be a little bit
concrete also when we think about how we
can create a good future and I think
it's striking how we spend much more
time making long lists of things that
could go wrong then we spend thinking
about things that we really want and it
reminds me a little bit of something
that happens a lot it just in my day job
at MIT if a student walks into my office
and that's for career advice I the first
thing I would always ask is okay where
do you want to be in 20 years right if
she says oh maybe I'll get the cancer
maybe I'll get run over by a bus you
know it's a terrible way to go about
this this I want to hear her tell me
about her visions this is my dream this
is where I want to be and then you can
talk about strategies for getting there
how to avoid the problems and so on and
may I likes to point out that
collectively humanity though we behave a
lot like this travesty of a student I
mentioned but if you look at our movies
they're almost all about this topic
futures where things go horribly wrong
in different ways and I would really
like to encourage all of you to really
ask yourselves and ask your friends at
parties and lunches and so on how would
you like our world to be in 20 years in
50 years because if we have no clue what
we want we're less likely to get it in
psychology everyone got really pissed
off about this right about field was
focusing on all the ways in which the
human mind can go wrong and that people
thought founded this whole separate
field called positive psychology but all
the ways I can go right maybe we need
positive AI actually so yeah one of the
things you're reminding me that
motivated me to to write this paper was
an interesting article by Dylan Matthews
and Vox about an effective altruism
contract conference where he was a
little bit concerned maybe rightly so
maybe wrongly so where a lot of
altruistic money is going now too
to AI instead of to curing cancer taking
care of starvation malaria and so on I I
mean that might be the right decision
but the point is it's it's worth doing
very careful risk assessment because if
we delay for too long people are dying
of cancer and starvation in the meantime
but of course it's again I'm not in any
way suggesting that the - at the end of
the table you should quit their day jobs
I mean I think it's it's obviously a
super important catastrophic risk that
we shouldn't play too fast and loose
with so ok I'm time for a couple of
questions from the audience
do you know ok so a lot of the talk has
been about formal problems in value
learning for AI and and there's been
some avoidance of social context so I
wanted to ask about that le as you said
at one point that it trusts the motives
and many of the people now working on AI
but I don't know if we know all the
people working on AI a lot of them are
probably in intelligence agencies and
military scattered around the world and
we don't know their identities or what
they're doing and they might have
broadly good intentions but I think if
you think about the history of
Technology think about nuclear
technology in the Second World War you
have advanced economies competing
against each other seeing each other's
existential threats racing to be the
first one to perfect this technology and
employ it because of Technology itself
is an existential threat with that in
the background imagine the next time
there's a large-scale war of that sort
you have all these militaries racing a
perfect AI because they each see it as a
potential existential threat and those
are circumstances where people are very
unlikely to be careful and to pursue all
of the techniques that we've been talked
about today to be assured that AI will
learn the right sorts of values so I
guess I'm asking is is the priority on
these formal methods or is it on
thinking about the social context
whereas most likely to be launched I
think there is a very real risk I mean
it the Deputy Secretary of Defense
recently said with regard to the u.s.
programs on building or colloquially
known as killer robots that he wants
Russia and China to be worried about
what we have hidden
behind the curtain which is pretty much
a challenge you know it's just like a
red rag to a bull right let's have an
arms race and so I I completely agree I
think there are irresponsible ways of
moving forward on this and I think this
is one of the questions that Nick
Bostrom raised about these competitions
it's one of the motivations behind open
AI to try to move the center of gravity
out of the private corporate labs where
things are kept secret
you could also ask well you know should
we be developing nuclear weapons and and
publishing all the the methods and and
so on as soon as they we figured them
out that's another another story but I
definitely agree that policy is going to
be important and I'm pleasantly
surprised to see that the US government
is is now taking these questions
seriously races of any kind are super
super bad like it's not about like not
just races between two militaries races
between two private companies races
between two universities safety is going
to take extra time and if you if you are
sort of racing along dude like oh no I
can't slow down or and like or you do
slow down and they like second fastest
project that didn't slow down it didn't
take any time for safety builds their AI
Doom happens first so like if I if I
could make any social change like the
absolute top priority would be something
like no none of this competitive mindset
at all between more than one major
project on the forefront of things there
should be like one project on the
forefront of things it frankly should
not be publishing all of its code
because then you don't stay like with
you don't have a lead time and should be
using that lead time to take a few extra
units of time for all that safety stuff
so as director of industry research lab
focused on AI I want to dispel a myth
here which is that there is not
going on in secret and whatever it goes
on in secret is way behind the stuff
that goes on in the open the very fact
that we practice open research at
Facebook Tsubaki our research is
precisely because it it's better quality
when you do open research also you can
attract the better researchers if you
perform it's time that they can they can
publish not only that you actually
required that they publish you get
better research out of it and the third
thing is that the field as a whole
progresses faster when ideas are
exchanged among the wider community so
if it's it's it's one of those kind of
weird situations when where there is a
mode where it's in everybody's interests
to publish all their their result we
even published most of our code in open
source so there's this this legend
somehow that there is you know super
secret nefarious research going on in
industry research lab it's just you know
completely wrong it just doesn't happen
there is there there is secret product
development going on in some companies
so there are companies like Apple and
Amazon you know Apple certainly Amazon a
little less that do you know secret R&amp;amp;D
on on AI you see it in their products
but it's it's behind it's not it's not
at the forefront so in private in secret
decades before public key encryption was
developed in public yeah so there are
certainly examples where what you're
saying isn't true
well this cryptography there is you know
certainly nuclear weapons in Second
World War but if you look at the the
state of science now in an in in you
know secret research labs government
research labs etc at least in the domain
of AI it's way behind optimistic that we
can create an awesome future with
technology as long as we win this race
between the growing power of the
technology and they're growing wisdom
with which you manage it and we just
heard very eloquently here from from
three people how sound difficult it can
be in any way slow down racer number one
and this thing the growth of the power
of the technology so I think that by far
the easiest way to improve the chances
of winning this race is instead focus on
the second
runner the wisdom of the race and really
double down our investments on and
growing that research all right because
so far that's been getting a tiny tiny
fraction of the funding it's been
getting better quite recently thanks to
the generosity of Elon Musk open
philanthropy object and and and many
others and what's so wonderful about
that which I think you Stewart and you
Eleazar knew Yan will all agree about is
wisdom research is something that
everybody is gonna be happy to share
because no private company for example
building self-driving cars wants an
other competitor to build unsafe cars
and run over that toddler right it's
just bad for everybody so doubling down
and investing much more in AI safety
research seems like like a really smart
thing to do
thanks very much this has been
fascinating I'd like to think a little
bit about the may be shortened middle
term
as was suggested by Wendell it seemed
that Stuart Russell and Eleazar outlined
somewhat different ideas about the way
that social interaction would affect the
trajectory of the development of
artificial intelligence perhaps into
what Eleazar might call the first
generation of AI which then might
condition the second so I'd like to hear
you discuss a little bit more maybe
between yourselves what you think the
relative role of social interactions is
going to be is the is is there some
reasonable hope here as I think Stuart
Russell was saying that this will be an
incentive for them to become good at
representing our value functions or is
this so to speak just a epicycle within
an underlying dynamic that's more
worrisome I think there are there are
clear economic incentives for building
systems that adapt to the values of
their user while simultaneously not
trashing the values of everybody else
and I actually do believe that the
personal digital assistant will be one
area where this kind of technology will
be deployed very soon because you know
an assistant like that it has to book
hotel rooms for you and it has to know
whether you're happy being booked into
the $20,000 or night Presidential Suite
just because the hotel is full or would
you like to be half a mile down the road
at the Hampton Inn and that's going to
depend on the person so when you sell
the PDA it will have just like the old
speech recognition systems used to have
to do you will have to adapt to the user
and their preferences it will probably
ask them questions you know how much do
you know do you like aisles or window
seats and so on so forth this is not
terribly complicated but it's an example
where you can quickly get into
complications for example when do you
just delete an email without showing it
to the user
when do you tell someone who wants an
appointment that sorry I'm you know I'm
completely booked up for the next 17
weeks you know those are things that
where the system gets it wrong you're
gonna be very very unhappy with it and
you you will you know take by someone
else's PDA that doesn't have those
problems i I think that sort of like
fundamental source of concern on the
later slides of my talk I probably
should have just started with the second
half of my talk at intelligence org
slash NYU intelligence org slash NYU -
talk all over case but the the problem
III think that the problems that should
I I think that the problems that should
concern us most are the ones that don't
materialize early on I I think that all
that all the people I know of trying to
construct AGI who have the slightest
real hope in it are sort of basically
well motivated they will test their
systems they will look for problems the
part that I'm worried about is the
things that go wrong when things are go
over like a absolute threshold of
intelligence of first being able to
appreciate the big picture that there
are these programmers out there in the
world and you can deceive them and stop
them from editing your utility function
when they go over the like sort of
relative threshold of being smarter than
us and certain in some domains or all of
them and being able to see options that
we didn't think of in advance so like I
think that robotic car solving trolley
problems have almost no connection to
the serious problems unfortunately and I
think that these sort of like modeling
the human at in the early stages like is
it like the sort of the problem of
conforming to human goals when you are
stupid and have programmers watching
over you and tweaking you is going to be
very discontinuous with the problem of
like not tiling the universe over with
slightly miss served versions of things
that could have once been called human
goals like when you are super
intelligent like like like the testing
and phase one is not going to
successfully verify behavior and phase
two unless optimistic than eliezer about
our ability to effectively test some of
the systems we're putting out there we
are really building complex adaptive
systems it's not only that they are
complex adaptive systems but they're in
complex adaptive socio technical
contexts and the simple fact about
complex systems is they have behaviors
that are unpredictable under stress they
often reorganize into totally new forms
and to exacerbate the matter if you have
learning systems it may even be
rewriting its algorithm as its learning
progresses so this is all the
complicated way of saying we are
building technologies that we cannot
effectively test we can sometimes go
through complex simulated models about
how they might perform but the
likelihood of having unanticipated
events actually goes up the more complex
the system is and it actually goes up
the more learning capabilities that
system intelligence once well I think
that that's the problem and we and and I
think Peters question was really about
this interim where we are now and I
think where we are now is we're being
hopelessly naive about the character of
some of the systems we are implementing
intending to rely upon we are actually
linking them closer and closer together
which exacerbates their complexity and
their unpredictability and I if we don't
also attend to that then it's not just
whether we may be able to
you know moral decision-making faculties
or whether we can trust the people who
divine devising the systems it's really
just where there were being naive about
the likelihood of what it called system
failures
what shells Perot called normal
accidents what
Nassim Taleb calls black swans and just
the failure to underestimate the
probability of low probability events
occur because they do occur okay thank
you very much David
gesture just a reminder Zucker is a
lunch I mean a gathering for people
interested in careers in AI safety that
I'll be meeting here at lunch common t
on and group will gather around there
otherwise plenty</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>