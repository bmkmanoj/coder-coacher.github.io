<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Interactions Between Humans and Artificial Intelligence - Manuela Veloso | Coder Coacher - Coaching Coders</title><meta content="Interactions Between Humans and Artificial Intelligence - Manuela Veloso - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Interactions Between Humans and Artificial Intelligence - Manuela Veloso</b></h2><h5 class="post__date">2017-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/P9sCjl7xAaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">she's setting up I'll mention that she
is the Herbert a Simon University
professor she's got a cool endowed title
for Simon one of the fathers of a very
cool and at Carnegie Mellon and she is
the head of the machine learning
department there and she'll be talking
to us about human AI interaction and she
looks like she's about ready
you know she's up okay thank you very
much so I will explain a little bit
about this concept of AI from a
perspective of what I call what we call
autonomy so I am going to focus on the
aspects that these AI machines are
supposed to be able to perceive the
world which is sensing reasoning about
objectives and eventually actually
moving so this is an example of a robot
Carnegie Mellon navigating in our
corridors this kawatte robot
autonomously which means that nobody's
remote controlling the robot and
basically the robot can decide which
forced to apply to its motors which
turns to take when to stop how to avoid
obstacles so it's completely able to
navigate so in these days in which
people are so and we are all talking
about autonomous cars you have to see
that these robot sins and sins are what
we call autonomous indoor robots so they
don't navigate in our streets but they
are capable of navigating in our indoor
environments so I'll spend a little bit
of time just explaining the from the
point of view of the what's in essence
here in terms of actually how do they do
this and just to try to explain a little
bit about is closing the loop between
perception cognition and action so
basically these robots and these AI
devices are equipped with sensors and
sensors are just devices in robots
cameras in in other
the elements is like text readers or OCR
character readers you have temperature
sensor
it doesn't matter something that takes
into account what's happening in the
world and transforms these into some
data that machines can process so there
is this concept of sensing our eyes our
sensors and somehow our we here's our
our ears our sensors so we are able of
perceiving the world so these robots in
particularly these covert robots is an
example since the world by having these
devices it's a laser which is a kind of
a laser branch scanner but let's see
given that the topic is a little bit
about the limitations
let's see these lasers basically do a
swipe of 2d swipe you can have more
sophisticated letters you pay much more
and they can have much bigger swipes of
the 3d space but basically it's kind of
a mechanism to build information on
where are the obstacles in this world so
they basically tell you at which
distance the obstacles are and the
connect the depth sensors also does
instead of a 2d swipe in our in kawatte
it gives you like a 3d view of where the
obstacles are and with these type of
information they can build inside of the
computer information that's kind of like
the distance to obstacles or a 3d view
of the world in terms of distance this
is what we call a point cloud so this is
what this is this cloud so I'm going to
just show you in terms of navigation
what happens for the 2ds robot so and
this is I just want to emphasize the
following so this is until the robot is
equipped with a map of the environment
which are the architectural four floor
plans of the building basically a PDF
file that the architects provide and
believe it or not most of our buildings
in the world have PDF files there are
the floor plans of the architectural
floor plans of the buildings and if not
the robot can also learn this but I will
not mention the beautiful thing about
these floor plans is that they are a
static
features of the environment no chairs no
tables no plants no anything except the
things there are what we call the
long-term features and the robot uses
this kind of long-term features to
devise its position now here comes an op
ik that it's important for us to
understand basically based on its motion
and in sensors robots have or these
these these devices have uncertainty
about their exact position orientation
which is captured here by showing these
multiple or in circles which means that
the robot somehow is a little pick of
distribution that it must be there it's
belief but it does not know exactly the
specific orientation and position in X Y
location in the building but the
beautiful thing is that as it moves so
this is actually the view of the depth
this is our filter point cloud let me
just explain these this robot is only
looking for walls and as such is
processing its images to find planar
surfaces that can be assigned as walls
and here is the floor and the different
colors show the distance but all the
floor is not used for localization
purposes but eventually these walls as
you'll see there is a certain point here
and I'll stop just for you to notice in
which here right there a little bit of
the wall was being seen and at that
distance it was mapped to in the map
that particular part of the wall so
there is this matching algorithm that
tries to match what it sees with what
the map says it should be so if I were
here this should be the distance to that
wall if I were here this should be the
distance and based what what do you
sense you revised your belief and
eventually you decrease the uncertainty
of where you are and so the robot when
it sees over there actually that corner
right now look at the corner appearing
from a sensing point of view that
detection of those two walls at the
particular distance identifies and
matches perfectly with that corner
and in the robot decreases its
uncertainty and knows much better where
it is and basically takes its turns and
does all its navigation based on this
combination and it's on motion with a
sensing so when you see the robot moving
around at CMU that's basically what it
does humans are perceived and all these
other objects that are not Walla
perceived but they are not used for
localization purposes but for obstacle
avoidance in that application it does
not run over the things that it finds
but it doesn't use them the dynamic
objects for any navigation that clear so
this is what a robot is and I'm sure if
you would see a robot moving around you
would wonder how do they move and now
this is it that's how basically one
approach to move is to actually sense
and map to some mapping to submit has
and tries to see where we where it is
and plans the route accordingly to its
destination so within this particular
kind of like navigation purpose and
there is something that became that I
want to emphasize that becomes crucial a
robot because it actually knows where it
is in the building it knows it's XY
location in some for plan can be used as
a mobile data collector this is side
effect of the fact that the robot
localizes well these robot at CMU
gathers data about for example Wi-Fi
data in every single point of the
building because it now can assign
whatever sensor it has on board to
capture these external vital of the
building temperature humidity noise
level Wi-Fi pollution you name it
because the side effect of knowing where
it is can assign the value of the
sensory data to exact locations in the
building this is something that I really
would like also to emphasize because we
tend to think about robots moving around
or cars moving around with performing
services for people moving around
carrying people and so forth that is a
side effect they actually also can
capture a lot of data so for example in
said of relying only on the data that
humans actually may be able to collect
we can use these robots in autonomous
robots to have them actively collect
data as needed because they are great
machines that actually in this
particular case of mobility know where
they are and therefore they can give us
data that maybe no one else can give us
so it's this is just a parenthesis but
this is something really interesting in
terms of like being able to use these
robots for this type of data collection
so these robots have navigated at CMU
just to finish these navigation parts
for more than a thousand kilometers
currently probably this was in 2014 a
thousand but maybe now 1300 kilometers
and they just navigate everywhere in
these buildings in particular also at
NYU I believe this is an NYU building in
which we were I was on sabbatical at NYU
in a summer and in the whole year and
they navigated there too so they brought
the robot so what I'm saying is that it
is like these concept of just having
them go by themselves nobody follows
these robots ok so this is the first
part but let's just let's just Bill have
a little bit long a little bit more on
this autonomy aspect the first aspect I
would like to ask valve odd is the
limitations that these creatures have so
for example they don't understand many
of the things they see maybe with more
deep learning don't understand more but
I'll talk about this later they might
not understand what people ask and they
definitely have actuation limitations
our particular cohort robot cannot go
upstairs there's a blanks and definitely
cannot open doors he doesn't even have
hands to open all the doors of the world
so in some sense there is this kind of
realization that these are young
machines we produce be them like the
actual robots or be them like a CD or be
them like a recommender or devices that
have limitations there are like
contributions that we are
but they have limitations so given these
a few years ago I was very upset because
our robots in some sense were not able
to navigate in the building how would
they go from floor to floor without
actually my students helping eventually
so we kind of invented this concept or
we we this concept that the robots
actually ask for help so these robots
moving the building by themselves we
don't follow these robots even the other
day I was I mean I constantly meet them
when they are going from place to place
and I nobody follows these robots but we
did need help they asked for help and it
became a feature the fact that when they
actually are not capable of doing
something they just stop and basically
ask any human for help so here is a
robot how they take the elevator at CMU
and they just go to the elevator hall
and basically they just shout up you
know can you please push the UP button
and hold the elevator door in this
manner it's not targeting it just a
second it's not targeting any specific
human and in fact it didn't even know if
it is human that around it just screams
you know it says it's actually speeds
than this rate I mean the synthesizer of
the boys tells with one town then we
actually the second round we say louder
and then louder and then poor guy stops
but I'll explain what happens in but
then there is always a generous human
that says yes I'll be able to help you
and eventually it does not so detect
whether it's the elevator on the right
or on the left us it's on the right or
on the left here is Stephanie Rosenthal
actually helping and the robot moves
into the elevator by itself so it
doesn't ask a human push me into the
elevator and then when it enters
elevator basically it says okay I need
to go to eight press that now every
button someone does that when it gets
away it says it's at the 8th and the
robot goes out so the interesting thing
about this is like this concept that in
it's all plan which involves
going through this building taking turns
at specific intersections and so forth
there is that thing which is the
elevator that it cannot do by itself so
basically it needs help for that much of
the whole plan and that open in some
sense these ability to really let them
go when be autonomous so these things
was not is not capable maybe you can
tell me like this oh man well why don't
you do a little arm that press the
elevator button or why don't we connect
with the electronics of the elevator I
could I can actually I have now a little
arm but no matter what there will be
some other limitation that it will have
so it's not about like assuming I cannot
do an arm or I cannot like connect with
the elevator it's more like the
realization that is AI systems basically
we need to develop them aware of their
own limitations now you can ask me what
if like nobody presses the elevator
button what if like screams and shouts
and nobody helps well guess what it
sends as email so it's very smart then
after some time if nobody helps if
nobody really is around or if there is
someone in front of it that doesn't let
go for a while it just says come and
rescue me so that's it it just doesn't
have any problem and we actually don't
need to interfere with its autonomy
because part of its autonomy is like
okay I'm stuck and then if by any chance
nobody doesn't helps his robot it
basically just sits there or if it's in
the floor where the lab is it goes back
to the lab I couldn't say the email to
the person it was going to make a task I
can't do it I was stuck there nobody
came and helped me or her that elevator
nobody pressed the elevator button
that's it and nothing wrong happens
because this robot just didn't deliver
the book on time or didn't escort the
visitor or into a task but the beautiful
concept it ate so the thing that
fascinates me is and they do you have to
understand is that it's autonomous it's
on its own it can fail it can ask for
help but somehow
it can do it by itself do you understand
so this is like the the thing about is
AI that I really work on it's like how
do you how do you kind of deploy these
things and let them be independently of
always eventually doing it by themselves
so these robots now from the actuation
point of view they cannot do anything in
terms of like becoming better at going
up the elevator because when a person
presses the elevator button the robot
doesn't learn anything I mean there is
no arm nothing so it's always it's an
it's always incapable of pressing
elevator but it's however in terms of
cognition if the robot actually does not
know something for example if someone
tells bring bring a coffee to the lab
and these robots these robot you have to
understand something cobalt does not
know how to scramble eggs it does not
know who won the last football game
knows nothing these robot basically is
very kind of narrow like we talked
before very and very well known knows
very well how to navigate but basically
it's always trying to that where should
I go to pick up the object why should I
go to drop it it's really a navigation
robot and basically that's what it's
looked for so if you tell it likes
anything you tell the robot tries to
match all the language we say to where
should I go which object should I pick
up where should I drop it so here it
says I don't know what coffee thing and
this object is not in my knowledge base
and now it instead of asking the human
where should I get coffee it actually
goes to the web and the web which is the
thesis of my student Mary Samadhi this
I'm going to search the web to see if
this coffee thing wears it and this
knowledge on the web says this this
coffee with high probabilities in a
kitchen or in an office and it has some
color which oh it's go to the kitchen
now it look how beautiful I know
knows everything I will collect the
object coffee and deliver it at the lab
I recognize lab la B from your speech it
had learned that le B was some kind of
room is that correct is that what you
mean yes okay I'm done and now I'm like
fine I'm on my own don't bother me I can
do it by myself
I I was fine until of that now you can I
can ask you so how is it going to get
coffee at the kitchen how does it do it
just stops there in the kitchen and just
says hello could you please put the
object coffee in my basket so there is
no limit there is nothing wrong with
just asking for help and tell me when
I'm done maybe somebody says I'm gonna
thank you okay and now I can do it by
myself this transportation so this and I
there is no sand it goes everywhere and
and does these four objects for people
and so forth but this second part of
this asking for help is this asking for
help when asking where coffees is now
fundamentally different from asking to
put an object in its basket or form
asking for taking the elevator because
actually learns it actually learns and I
actually want always to show this this
is the internals of the robot some kind
of simple object routes to coffee 7602
which is the number that the robot knows
about the location of the kitchen and
some kind of like how many times some
kind of weights to this knowledge so
what happens internally is that the
moment that actually knows that coffees
in this kitchen it saves that
information and currently the robot if
you tell it go to Manuel his office it
knows eight zero zero two or seven zero
zero two
it knows lab it knows kitchen but it was
not because people eventually entered
all this knowledge but because it
acquired it by interaction with humans
it can also acquire whole like plants
but I don't have time to explain we can
actually instruct the robot on what to
do but let me just skip this
mentioned this first part which is so
I'm exactly at 20 minutes so I'll spend
the last 10 minutes saying about another
aspect of this human AR interaction but
let's first like just summarize this
first part so in this kind of human
Ayana interaction so the robots plan
based on directions and others actions
they can actually do this they can
proactively request help and then learn
from these replies while the humans
directly remotely because of email or
through the web they can request tasks
they can execute actions for the robot
when needed and they provide it they
provide requested information so look at
my thesis in some sense or my point is
that what we have to get used to is this
collaboration between the human and the
AI and not just that the AI will do it
or just that the humans will do it and
then they AI does but the humans don't
are part of not these kind of cycle so
this is like the human AI interaction
now one of the last aspects of these
human interaction is the following so an
AI system and particularly robot does
something by itself and look our
classify the name adjust something by
itself so look at the look at your
experience or my experience when these
robot stops in front of your office so
if the robot stops in front of my office
which stops many times I mean I have my
I will report on the exact number but at
least it many hundreds of times you have
stopped in my office constantly stops in
my office bringing visitors and so forth
so this machine looks like it does it
it's like kind of a metal thing with a
camera on top and some beautiful
omnidirectional wheels built by Michael
etc and so you have this thing that you
got used to row to the robot you know
that there are others things but
questions arise you would like to be
able to actually ask what are you going
to do next what which path did you take
how did you come up here I did not
follow you you just arrived what
happened by the elevator
how long did it take to arrive here did
you successfully escort the visitor
later why are you late so
since it's now question all these
questions about his autonomy started to
arise and this is the biggest moment in
which you understand I don't want to
interact with NII system that's
completely opaque that's completely
opaque so that started a new a new kind
of like focus now that week they could
move they could localize they could
eventually learn from the web and so
forth
now another aspect of this AI system
became important how do we find out what
have they done while they were by
themselves while they were anonymous
whatever they did that I did not have
interference on so that became this goal
this goal is trying to explain what
they're doing and first we made some
effort on trying to explain through
lights and the robot now actually turns
left shows when 2/3 is going to turn
left and turning right and also another
very interesting thing is that when it
is quartz a visitor and it you follow
this robot the visitors have no clue if
my office is like 10 kilometres away or
just one meter away they start following
the robot completely like where is it
going and now the robot actually because
the robot knows the robot actually shows
in its kind of like column as it's
approaching this progress bar goes from
blue to green and push the people now
that they are approaching this thing and
the robot in some sense did not use to
reveal its state and now it reveals much
more of what it's doing for the humans
so this transparency and also this thing
is like trying to find out what happens
to the robot
the robots function in another world and
all the AI systems have weights
parameters they have distances they have
angles they have all sorts of
information that does not match exactly
answering the question what happens
while the robot is on its own and it's
it's a long experience they can go down
corridors they take elevators
rangamma round they do all this kind of
navigation and they see tons of things
as they go so we kind of like log all of
these and again I wanted to show you
this is what the internals of the rabbit
are this is like in some sense the
brains of the robots at some level of
abstraction but they record time like
timestamp
thirteen nine two two five zero seven
one seven eight what is that Tuesday at
10:00 a.m. or what is this August at
4:00 we don't understand one three nine
two five zero seven one seven eight but
that's what they know internally so now
it becomes a question of how do we go
about translating these two things that
humans understand like locations how do
we know this this is like an office so
we have to put semantics and now we are
working on this problem of trying to
have the robots generate explanations so
go from that internal kind of like
representation to say I went straight
for 15 meters and turn left then
straight for 51 meters and turn left
then straight for 10 meters to reach the
destination at least this is more than
giving us the XY coordinates in some
space that I don't understand and we in
this we introduced this verbalization
space that enables the robots to
generate explanations of different level
of detail it can say I started these or
can say I travel 26 meters so basically
you are you are you you have this
concept of having the robot explain it's
an experience so it becomes this more
transparent robot it cannot explain yet
why are you late
this is the thesis of my student
Vittorio Pereira that is trying to
understand these this concept of light
and what is this being late or rather
concepts of how many people did it was
the kitchen crowded Wow
I don't know two people is it crowded
and it depends on its experience all
sort of thing well did you wait a lot
for a long time for the elevator all
these types of questions that we take
for granted require the robot really to
analyze its experience and to try to
answer this in line
and so finally we also were able to
learn when humans actually say things
like tell me exactly how you got here
that's a point in a verbalization space
that the robot actually generates an
explanation for but then if the human
says okay now only tell me what happened
near the room seven zero zero four that
means that the explanation needed is of
a different nature more specific and the
robot generates another explanation and
eventually when you say can you only
give me a brief summary the robot
generates another summary we beep
learning we are able also to now
understand the scene so the robot is
capable of looking at the world and
trying to say I should be near the
kitchen because I can see a microwave
and a sink so it's eventually its
correlations between vision produce
additional abilities to explain things
and we are in the process of combining
these all with the deep learning sensing
to try to make more sense always with
the goal to explain to the humans what
happened while it was autonomously doing
something we also have been using a lot
of deep learning for simplification so
the robot knows in which store it is so
in conclusion I want us to understand or
at least this is the research that I
will that we are doing and that
eventually I will be happy to discuss
later which is this human AI interaction
in which is it's more about
understanding how these humans and these
AI systems are going to be interact in
terms of the humans and web and actual
mobile robots we have a future in which
potentially all these things will
coexist they will eventually be able to
coordinate and distribute tasks between
them and benefit between each other and
there is this concept of transparency
that is now a very great value so that
we do not in fact interact only black
boxes that just tell you how do these do
that and without being
at least question why am I supposed to
do this
why are you recommending to do that and
therefore that we are doing a lot of
research on this so just as a final
thing I would like just to mention that
at Carnegie Mellon in some sense one of
the things we realized because of these
AI is that it's a field of many
disciplines is an interdisciplinary
field you have to know tons of like
personal robotics and machine learning
iii-it's signal processing all sorts of
like human compute a SPECT so we
actually created this new new new entity
called the CMU AI which tries to bridge
all these different kind of how do you
say interests and areas towards
eventually solving the larger problem of
really having AI and intelligence as
part of our human societies thank you
very much
so questions for dr. Veloso looks like
somebody stepped up to the mic No
so let me start with the question and
you you you referenced I mean about the
first 30 of your talk this notion of
limitations so the robots have no arms
they have no fingers and so there are
limitations and the limitations you're
able to overcome because they're able to
talk to humans have humans help and so
forth would it be too far of a stretch
of the imagination to to basically
conclude that in order to achieve
autonomy there will always be a
limitation robots will always be limited
and so maybe you put in an arm maybe you
put in some fingers maybe you put in
hands and so forth but even if you did
that there will be another limitation is
is it a stretch to say that your belief
is that robots will always be limited I
guess in some sense I think that humbly
if I look at myself I have a lot of
limitations and I don't see that I don't
think that there are any humans that
don't have limitations themselves the
amount of knowledge that we have around
us it's going to eventually no no
machine is going to be able to
understand it all all the time given
like the complexity of our world so I do
believe that things will hide machines
robots in particular will always have
some level of limitation and my approach
is in fact to embrace the fact that they
have limitations and provide them to
with the ability to actually invoke give
me more data because I cannot classify
this thing as a slug I cannot mean
whatever you gave me this comes up at
zucchini but I mean I think it would be
more into that it would be interesting
to have the machines themselves invoke
for this help and that's what I've been
trying to do research on it is to have
them proactively ask for help as part of
their autonomy so they will be
autonomous but from time to time they
will let us know they can't do it the
same thing with asking like which papers
talk about something
you can say I mean I've looked at all
everything that's on the web and I don't
know how to do this unless you give me
more guidance unless you specify more so
in some sense I believe it's inevitable
that we will always be interacting with
machines that will have some level of
limitations so thank you for you know
discussing under especially the
transparency two related questions one
is that from the transparency aspect can
you would anything that was surprising
to you that you cannot diagnose and the
second is how can you tell that the data
that it gave you were not modified
change or how can you authenticate that
those are real data do you want to come
to CMU to do a PhD so these questions
are very very important but you know
actually let me address the second one
in particular so when the robot arrives
to my office and I say where you come
from it tells me route and how do I know
this is right or not so what happens now
is that in this my students is Vitoria
we are actually asking redundant
questions to see if they are coherent so
we can when they become cryptographers
and asking things to see whether they
are right but it's very interesting that
if I asked like this how long did it
take you to come through the to my
office and that thing says you know 90
seconds then I say how long did it take
you to go to the elevator and it says 45
and from the elevator to my office 45
then the 90 was right so we are trying
to eventually devise mechanism ourselves
to exactly what you were saying to
assess that what the robot is saying is
right by asking these multiple questions
and in some sense this transparency is
something that you can always one way or
the other the data is all recorded and
you can go through these data and
eventually do a Python program to check
yourself
but it's beautiful to be able to through
language to also validate that the
answer is correct and which we have done
yes very interesting talk and and very
fascinated by how you're having the
robot collect other data as it moves
through the building so for plants make
a lot of sense to humans but they may
not be the best navigation aid for a
robot yeah so at some point are you
going to say see if the robot can
navigate by Wi-Fi hotspot' sensitivity
or something like that something netting
I guess a mechanism that a human
wouldn't be able to sense or even
perceive but the robot can do very well
that's my first question very good
question so in fact the Cocotte robots
in the beginning we're not using floor
plans and they were using Wi-Fi
they still use Wi-Fi if they are in a
very large open space that they they
don't see walls to actually map they
have a signature of the map of the
building and Wi-Fi it's beautiful
because Wi-Fi is like GPS it's a global
Global localization sensor the only
problem is that while with the floor
plans we down land download them from
the web and it's a PDF file and we
transform into a vector map
representation automatically for the
Wi-Fi signature of a building the robot
needs to traverse the building acquiring
that Wi-Fi data as soon as it has the
Wi-Fi map of the building then it
navigates beautifully using it but the
question of the Wi-Fi map is more
problematic than using the floor plans
but it can use okay and they are we have
several papers on showing the two of the
two the two things when one is useful
more than the other very good question
yeah yeah and my next question is so far
it seems that you're you're interacting
with very cooperative humans when are
you going to introduce new Farias humans
that take it to different floors than
where i wanted to go so but are you i
tell you i mean they're cooperative but
both faculty and students and everyone
at CMU loves to say
that the robot that there are gate floor
when they are on the fourth floor and it
happens the only thing is that again
this is very robust to the problems with
a bad answers so the robot gets out of
the fourth floor and then it realizes
it's on the fourth floor just turns
around and says pressed elevate about
that I want to go to the eighth and
interestingly interestingly it there is
always some generous humans we did not
prove these but there exists the human
that presses the right button and if
that's not the case but it's true if
that's not the case after some time
again it has a threshold that if had
gone up and down may not making progress
towards the task it sends his email
again it says I've been trying
desperately to get to the eighth floor
I've got out here there and so forth
come and help me and if not that's it
but you have to you have to I just want
you to understand what I mean I really
think that people don't realize the
feeling of letting it go I don't care if
it stays forever in the elevator what I
want is that it is by itself and the
moment that we actually I was in my
office the first time in there and the
robot went all the way to the lab until
the end I have been doing a lot I've
done a lot of robots robots are curved
robot these robot Baxter's manipulators
that you see them always to get that
feeling that the robot disappears from
your sight and then not even you can
only hear it and it's gone I was on the
phone always didn't get there already is
it in the lab where is it where is it
let's go and check it was it almost had
to someone to retain me there from not
following this robot and that's really
what now for me became common but it was
very hard to just let that thing go by
itself so in some sense I don't care the
go is what matters yes
one fine question yes so I have a
question about I think what I would
think of is like another part of another
step of autonomy I seems like a lot of
the research they were doing with these
remotes is about them learning I'm
curious about what is about these robots
learning yes yes I'm curious if you have
plans to incorporate any sort of them
teaching so for example teaching each
other without just copying the data set
or teach or or being able to verbally
give instructions to someone who comes
into your office how do I get you know
how do I get to San Jose office very
good question I did not have time to
give my talk on learning by instruction
but actually Baxter I don't have here a
picture of back sir but we have robots
there only perform things because people
instructed them we have what we call
instruction graphs and we tell for
example this is an example we say pick
up the object and display I grab the
object is it visible drop whatever this
was actually instructed instructed and
the beautiful thing about instruction is
that you instruct but then you can also
correct so we are in that loop of
instructing and correcting and letting
the robot accumulate also a lots of
instruction one of my papers send me
email and I'll be happy to send you a
very technical answer to your question
in the sense that also they accumulate
all the all these instruction so next
time you might want to know do they know
how to get to the kitchen do they have
like how to grab these and you don't
even know anymore what you have
instructed so now it becomes not a
problem of just instructing but also
recalling that I actually know this and
the paper I'll send you is about this
auto-completion
you start entrusting in instructing
something and the robot magically gives
you like this do you want me to do tap
tap tap data all these things and the
person says oh yeah up to here that's
what I meant but then after these duties
it's like this Google auto-completion on
search you can actually autocomplete the
instruction and it's really beautiful
and it's a whole other area of
interacting with the Machine by this
instruction okay but please say NEMA
thank you very much thank you okay
please join me in thanking professor</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>