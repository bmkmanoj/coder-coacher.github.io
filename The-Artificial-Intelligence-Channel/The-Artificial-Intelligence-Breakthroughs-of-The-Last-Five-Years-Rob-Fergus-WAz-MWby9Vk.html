<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Artificial Intelligence Breakthroughs of The Last Five Years - Rob Fergus | Coder Coacher - Coaching Coders</title><meta content="The Artificial Intelligence Breakthroughs of The Last Five Years - Rob Fergus - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Artificial Intelligence Breakthroughs of The Last Five Years - Rob Fergus</b></h2><h5 class="post__date">2017-11-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WAz-MWby9Vk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">is doing some some double duty between
between Facebook and NYU and he'll it'll
start us off alright okay great well
thank you for the introduction there we
go okay alright so my talk it's gonna be
just a very general overview about some
of the great exciting progress that's
been made in the last five years or so
on a range of tasks in AI and as a
result you know this has been picked up
by the media I'm sure you've seen
various articles about it and you know
buried in all this excitement is the
fact that a lot of these advances have
really come from quite a narrow range of
techniques within machine learning in
particular these models called deep
neural Nets so that's what I'm talking
to focus on and I just also want to say
that you know while there has been a lot
of progress were still really a very
long way from solving the sort of
general problem of you know of true AI
and there are many challenging problems
that remain on that path and so I'll try
at the end to you know just touch on a
few of them okay so neural Nets actually
been around for a very long time so this
the current wave of excitement is really
the sort of third time that people have
got you know very excited by them so the
the idea originates back in 1940s and
the basically underlying principle that
underlies all neural nets is that as
something called connectionism and this
is this idea that if you take some very
simple computational units and you
connect them in a very large network you
can get you know complicated behaviors
emerging from them and so in this spirit
the researchers back then started
looking at artificial neurons which were
a very simplified version of the sort of
biological neurons and use them to sort
of build some very simple models called
perceptrons which are basically are so
single their neural net and they were
very much hamstrung by the kind of
hardware limitations of the day but they
were actually able to train these things
to recognize very simple patterns as you
can see on the figure this let us see on
this wall here and so these models
really I'm the the the little model I'm
showing on the left is really a single
neuron and all that happens there is a
simple series of additions and
multiplications and some kind of
non-linearity that's really all the sort
of ingredients you have in each
competition
unit but by putting them together you
can actually do something interesting
now basically people realize that with
these single ad models you couldn't
actually do an awful lot and there was a
sort of low end in progress and then in
the sub 1980s people figure out how to
train models with multiple layers using
a technique called back propagation and
that ignited a whole wave of new
applications and sort of performance
results any budget one particular
interesting model that in most of the
time was a type of neural net that was
designed essentially for images its
architecture was kind of adapted to the
you know the structures you find in
images being on a regular
two-dimensional lattice and so on and
it's some of this some of the sort of
components within that model were very
much inspired by biology by you know
plumpie by Coulomb diesel and so that
using these convolutional nets they able
to show very good results on sort of
handwritten digit recognition at the
time and did creates practicals of check
reading systems and things like that now
that was so that was a second era and
the currently Rory now is really I guess
started in about 2011 and what we've
seen is a whole range of performance
breakthroughs on a range of perceptual
tasks so that is a computer vision
speech understanding now called natural
language processing and things like that
and as far as there was sort of three
ingredients really to behind these
breakthroughs that are common to all
these different domains so first of
these big deep neural network models so
they're very much in fact the same
models for 1980s but just much bigger
than they were back then
I'm and that's reading permitted really
by the availability of these very fast
GPUs and also the third ingredient which
you need is lots of training data and so
I'll come to each of these in turn and I
should also say there has been a
exciting progress on some other AI tasks
which I'll talk about at the end as well
okay so just talk about the data sets
for the moment so one facet of these
models is that they do need to be
trained in a supervised fashion I'm
going to go into the details of that
just a moment but this was these are two
examples of sort of modern data sets
that were basically collected by using
sort of you know crowd-sourced
crowdsourcing on the internet through
Amazon Mechanical Turk the idea is
you're gonna have a whole bunch of
images and get humans to say what's in
the image okay so you can see these
things contain millions of examples
these are just orders a magnitude big
than the datasets that we know people
had back in the 80s where we had some
tens of thousands of examples and things
like that now the hardware it turns out
that these neural net models are
actually incredibly convenient to
implement on GPUs because they basically
consist of matrix multiplications and 2d
convolutions and things like that and so
and those are can be paralyzed very
easily and that's what GPUs are very
good at and so the latest generation of
these GPU cards from Nvidia deliver you
know huge performance in actually you
know a fairly small package so they you
know ten teraflops in from one GPU that
you see here and that just to put that
in context that's faster than the
fastest supercomputer in the world in
2000 and it's 10 million times faster
than the Sun workstations that the
researchers been using back the 1980s to
train those first comp nets the Kombi
from the old network models so it's
really been enormous speed ups since
then and so using these GPUs and with
the big data sets geoff hinton and his
group at university Anto ice screws have
ski and Ilya sutskever they in a seminal
paper in 2012 applied basically came up
with this big neural net architecture so
it has eight convolutional layers it has
you know 60 million parameters it was
trained on this there's very large image
net database from Stanford and it used a
GPU implementation and the whole thing
took you know a week to Train on a pair
of GPUs so it's you know the Boreas but
they got some very exciting results
which I'll go through in just a second
however I just want to explain how just
give you some insight into how these
models are trained so as I mentioned all
this these models are supervised what
that means is as follows so the idea is
let's just take the image classification
task so miss in this problem what you're
trying to do is you're given an input
image and you're going to try and
predict like just a label for that image
okay so the idea is that you are you
going to pass this this image through
your neural net which has some
parameters theta and you know you hope
that the out the output from the model
should be some label say it in this case
abacus so if it predicts that it's got
this example correct and you don't do
anything well then you present another
you know sorry they're true labels
abacus in the two agree now you put it
present another image during training
and you push that through and let's say
the
to predict predict zucchini okay well
actually the true labor was slug all
right so basically there's some kind of
error signal you can then take and you
can then push this back into the model
to update those parameters so the next
time you show an image of zucchini
sort of slug it doesn't say zucchini it
says the correct thing okay and so the
idea is you do this you know
millions of times you take show millions
of images like this which slowly
basically update the parameters of the
model and then what you do is you take
that the parameters the model and then
evaluate them using new examples that
it's not seen before and that's a
crucial point so it's not enough just to
memorize the training examples the
models must generalize to things that it
hasn't seen before okay so then
hopefully when you present these you
know held out examples it gives the
correct prediction the key point though
is that you for each of those training
examples you have to know what the true
answer was okay so that's something
which the moment is provided by humans
now the deep learning models I just want
to contrast them for a moment here with
traditional approaches from you know for
image classification and machine
learning so in the in the traditional
approaches what would happen is you
would have a bunch of hand design
features so these were things that would
be crafted by you know computer vision
researchers that would seemed like good
things to you say for example edges or
something like that now there wouldn't
be any explicit parameters in those
there would be updated during that
training phase all the what happen is
you would take those features you know
they were taking the image of extract
features and you have some simple
classifier on top of those features that
would be trained at your training time
okay now that contrasts with the deep
neural Nets so what's effectively having
there is that each learn the net is a
sort of little mini feature extractor
that extracts successively sort of more
abstract representations of the input
and goes all the way up all the way from
pixels up to the output label okay so
the idea of things than that thing is
not going to be trained and to end each
stage of this is going to be learnt so
essentially you're learning which
features to extract within the model and
this actually proves the piece of a
crucial thing it just turns out that
human intuition isn't enough to handle
good features both you know for in this
case for the image classification tasks
but as well see also for other domains
as well okay so just to show you now
some performance some empirical
performance this is there's a challenge
run by the sample folks
the imagenet challenge and they
basically asked these models to classify
images into one of a thousand different
categories and they measure the error on
on an evaluation set okay so what I'm
showing here is the winning performance
the error of the winning performance as
a for each year of the competition so in
2010-2011 you can see the performance
was around sort of you know twenty-six
twenty-five percent that kind of thing
and this was using the traditional
handcrafted features and then this that
paper from the Toronto Group in 2012
drop the performance down to sixteen
percent and people were very surprised
because previously these commercial Nets
hadn't really worked very well on these
complicated images and once perhaps even
more interesting is in the subsequent
years the communities managed to sort of
engineer the architectures of these
models to dramatically reduce the
performance that you know really
substantially not just a few percent you
can see now that's gone all the way down
to sort of you know I think the latest
results actually are better than three
below three percent error on this metric
and just to put this in context if you
take a human and get them to do this
task which admittedly is a rather
artificial task and you can see they're
actually doing you know a little bit
worse than the best models that we have
at the moment I just want to emphasize
of course this is a somewhat constrained
task it's much easier than just
recognizing completely general things
out there in the world now the question
is what happened to the architectures to
give this remarkable game over the last
three or four years well what if one
thing we can do is to look at the depth
of the the models so one thing that's
happened is that people have figured out
how to train much much deeper models
than before so eight layers previously
was considered very deep but now the
winning model you know countries have
you know hundreds of layers
representation within them and so it's
this is the single factor it's not that
they've increased the number of
parameters in the model particularly if
anything actually they've decreased
slightly it's really this the death of
the models being the sort of key factor
and just to sort of give some slight
insight into why the depth is important
the idea is as I mentioned very briefly
at the beginning each layer consists of
a simple nonlinear function and the idea
is that when you compose these nonlinear
layers when you compose these nonlinear
functions together they build very
complicated functions and so if you just
in this little toy example I'm showing
two classes both of some red and blue
dots
and we're trying to separate them with
some kind of decision surface which I'm
showing here in purple and if you just
have a single heir model that you only
have a straight line that can separate
the two classes which clearly is
inadequate with this sort of complicated
data but when you have in this case a
three layer model you can already see
that with three compounded sort of
nonlinearities you can construct this
quite you know complicated separation
between the classes and so on and so
this is obviously example but if you use
some sort of visualization techniques to
look at what happens in those these big
continent models you'll see they're able
to learn some very complicated in
variances and things like that so for
example you find features that
correspond to sakes of concentric
circles features that correspond to sort
of text on the side of objects and
things like that and it's just proven
very difficult to actually hand in no
one's ever been able to sort of hand
engineer features that were able to
capture these kinds of structures of
course once you have those kinds of
features it's fairly it's a much easier
to build the representations on top that
can then classified the class of object
so just to give you some sort of you
know qualitative feel for how well these
things work so this is just looking at a
recent you know one of the safety art
systems running on one of the evaluation
data sets so you can see this is just a
typical sort of indoor scene in this
case the model is outputting a bounding
box they can also actually I put a
little per pixel mask as well and
there's a little confidence attached to
the you know class prediction as well
for each of the boxes and you can see it
captures really a lot of stuff even
though even people who are you can see
this person here is quite heavily
occluded and still gets them just fine
does miss one or two objects as some
umbrellas here that is it's not captured
has another example you can see some
quite different lighting conditions
people in the shadows back here it can
find reliably different viewpoints of
the United say the motion the mopeds
it's doing pretty well on and so on and
then you can also get these models to
you know this is just showing some
latest results from a system from some
colleagues of Facebook so this is a
little video process for each frame
independently where they're tracking
human pose and stuff like that and you
can see that they're able to sort of
really do a good job of kind of you know
figuring out the configuration of the
person and this is a more challenging
example here some sort of breakdancer
doing some crazy things you can see it
does lose the tracking somewhat at some
points but it still gets hit most of the
time so anyway so these are it's got
these systems already gone from kind of
you know pretty hopeful
performance actually being you know
working really pretty well now the story
is actually very similar with speech
recognition so what's moral is it's very
much the same model something they used
and the story has been very much the
same that these traditional handcrafted
features have been replaced with these
deep neural Nets and so what you see
here is a little plot showing you here
this technology the previous technology
in blue this is an error rates of lower
is better and just showing a plot here
as a function amount of data they ingest
during training and then the same curve
for the deep neural Nets and what you
notice in fact you might think well you
know maybe these deep neural Nets just
you know just need tons and tons of data
to Train and it's true but they do
actually still even with you know small
and relatively small amounts of data
work better than the traditional methods
as well so just to give you so this is a
snapshot of a kind of state-of-the-art
speech recognition system from by do
this so this thing's a huge system here
this in this case you know 100 million
parameters 11 layers it's a slight
different architecture this one involves
a recurrent net rather than a
convolutional net and this is some
benchmark comparing here to human
performance and this is this is an error
rate so lower is better and you can see
this getting at least on these
benchmarks anyway sort of comparable
performance to humans so another domain
which i think is a little bit you know
there has been great progress but
there's still a long way to go is
natural language processing so this is
so one example this will be something
like machine translation translating
between different languages and so
recurrent neural network models here
have been proven very effective
the idea is you somehow basically have
recurrent neural net is a type of neural
net that can capture the temporal
dependencies within data so a sentence
of course is that you can think of as a
temporal sequence of tokens and this
these models can sort of ingest a
sentence and end up with a basically
some hidden state within the model
representing that entire sentence that's
the red blob and then that has passed to
a separate recurrent net which then you
know transcribes it out to some sort of
output representation you know type in a
new language in this case french and so
again you see also a plot here with the
y-axis here is some measure of
translation quality which is actually
quite difficult thing to measure and
and here we see the so performance of
different models in some sub
competitions and yeah this is a very
dramatic increase over time in this case
on so English to German but in practice
of course it you know we're seeing
similar gains on other sort of language
pairs as well another task in natural
language processing is language modeling
so there's lots of uses for this but one
easy to interpret one is just
synthesizing realistic text and so this
is a plot here just showing some sort of
measure of quality of the text and so
this is a just lower is better and this
again is just looking over time for 2013
- so last year and this is coming down
quite a lot and just to give you a sort
of feel for the quality of these of the
most recent models these are just sort
of random you know burbling z-- from the
model if the models just you know
free-form you know you know riffing is
it were just generating sentences and
you can see they do sort of a reasonably
coherent right I mean they they're sort
of not completely sensical but they
definitely you know have some you know
look vaguely like English to us okay so
of course all these breakthroughs have
unleashed a whole wave of applications
okay and so you know just taking the
translation one for example of course
you know probably most of us are
familiar with things like Google
Translate that's something which you
know is now able to translate you know
all kinds of you know crazy language
pairs out there in the world the quality
varies but it still works surprisingly
well in some cases and then you know
Facebook also it has you know does
translations too so they handle about
two billion translations a day in
between 40 different languages and then
it's all using deep nets to do these
things and then in general of course the
internet companies have you know
deployed these deep nets in a wide range
of settings sorry you know I happen to
know a bit about what Facebook does so
they have over a billion images a day
uploaded each of the one of those goes
to two of these deep nets these
conditional nets for recognizing images
one does face recognition to see if
there's any of your friends in the
picture and then there's also one that
looks for offensive content nudity or
violence and it also just generally sees
what objects are there in a picture you
know dogs and you know
you know things like that so other of
course applications that you see now
your smartphone uses a deep net to do
speech recognition and we see things
like the self-driving cars of course you
know there's many aspects of that
problem but of course if one primary
problem is perceiving the environment
around the vehicle and so that all the
latest you know systems are using
basically deep nets for that task I
should also say that you know there's
also been a lot of interesting inrease
in the last year or two on using some of
these models for scientific problems so
particle physicists now using them for
sort of you know looking through of
course all the data from these big
particle accelerators I think one area
where there's a lot of potential
progress is things like medicine lots of
in radiography type tasks we are looking
at scans or images and you want to try
and find some sort of anomaly like
melanoma or breast cancer and you also
see it in also other settings like in
the emergency room like looking for
sepsis which is something that you know
these algorithms can help with so just
to take a step back and look at talked
about some of the issues involved so one
thing that is very important with these
models is the architecture so previously
the humans of input went into designing
the feature representation and in deep
learning that pieces being replaced
basically by you know it's being is
learnt now okay but the human input
still is there in terms of deciding
which is the appropriate architecture
for the model and what you'll find is if
you just take a completely a network
with completely arbitrary structure they
don't work well so the the the
particular architecture involved in a
commercial net turns out to be very
important for images and indeed it's
been designed to exploit a lot of the
sort of structure the fact you have
local dependencies pixels lie on a 2d
grid or if you're doing with video it's
on a 3d grid and things like that and if
you don't sort of you know adapt the
architecture to the domain at hand
these models don't tend to work very
well so I think it's you know it's an
open question here how when you're
presented with a new domain happens that
automate this process of coming up with
a good architecture so the moment is
essentially it's done by sort of you
know trial and error essentially with
graduate students or researchers in
industrial labs and things so another is
thing to mention of course is that we
don't have a very good theory
understanding of these models okay all
but any good performance guarantees and
you can imagine you if you're trying to
design a self-driving car system you
might really want to have some sort of
cast-iron guarantee about you know it's
performance it's hard to give away these
kinds of models and but the part of the
reason is that you know these models are
very high-dimensional there's lots and
lots of parameters within them they're
also in Britain on convex and so that's
a big issue because all of the
mathematical tools we might use to
analyze them unfortunately don't apply
here so you know this is a big sort of
open problem I mean the good news is
there's a lot of very smart theoretical
people now thinking about these models
okay and trying to explain that's why
you know why they work and trying to
make predictions about what they might
do you know under certain settings it's
also worth saying that they're quite
difficult to inspect these models so you
know you shove in the pixels at the
beginning of the input and out pops um
you know categories category label for
the image and it's it's very unclear
what's really going on in the center of
these models and so you can't look at
it's very hard to look at an individual
you know unit in the model and say you
know this is recognizing a cat or a dog
I mean you can hand Waverley do it but
this is you know contrast to you know
other types of machine learning model
like say for example privacy graphical
models what you should know it has a
very definite meaning and instantiation
in the world and then the final point
which I discussed earlier is you do need
lots of label data and that's we're
always possible to obtain in many
domains and so this brings me on to
server I think one of the big unsolved
problems at the moment which is
unsupervised learning so this is the
idea that you should be able to learn
you know without hand annotations or
maybe from just a small handful and this
is really therefore about capturing the
sort of structure inherent in the data
and it's of great practical importance
because what you'll find is in the real
world there are lots and lots of
categories the only pair appear very
rarely okay so what I'm showing here is
a sort of log histogram of the frequency
of objects in say a vision bench data
set and also words in Wikipedia and what
you see in these things you're showing
you I said there are a few very very
common words or indeed object classes
but there's also a very long tail out
here there's lots of classes that you
see that really only occur like a ham
times an entire Wikipedia and so on so
it's this notion of trying to capture
millions of labeled examples for every
single class just isn't gonna be
possible and you need so we need some
way of sort of you know figuring how to
learn from a much smaller set of
examples and that's when it boils down
basically to be able to subdue this
unsupervised learning problem now just
one other sort of arguments which I'm
just going to so it's like detour is
that there's a sort of biological
argument that you know perhaps it seems
very unlikely that our brains are purely
using supervised learning there seems to
be some sort one supervised learning
going on although it's very very unclear
what it might be so this is an argument
from Geoff Hinton so he said his
argument is that well our brains have
about 10 to the 15 synaptic connections
within them and we only live for about
10 to the 9 seconds so somehow we need
to sort of get about 10 to 6 bits of
information per second to set these
connections and there's just not enough
information from sort of high-level
semantic labels I mean your mother
occasionally says you know this is a
spoon or something but she doesn't say
this you know every few seconds you know
continuously for your entire childhood
and even that wouldn't be enough to sort
of give you to set all those connections
ok so the only source of information
that sort of has enough information is
really the input itself and that's
really what supervised learning is
really about sort of essentially
modeling the input distribution and so
there's a lot of people working on this
it's very unclear sort of you know I
mean there are sort of solutions that
work ok but there hasn't been the same
sort of breakthrough that we've seen in
you know in in other settings so that's
one I think sort of you know big problem
that's remains may need to be solved
the other big thing of course is that
you know solving AI is not just about
perceiving the world right you have to
have to act within it and also a reason
about the world and also plan and so you
mention you've got some sort of you know
AI agent a robot or something and it
sort of you know uses a deep map has to
perceive the world but then a must
decide how it's going to act to achieve
some goal and part and the key to doing
this really is to be able to predict
well the outcome of your actions will be
so effectively that the agent needs to
have inside it a little model of the
world so it can sort of you know in its
mind try out a few possible sequences of
actions and figure out which one it
things will be likely to you know yield
some real good reward or something like
that and so this building good world
tomaters is actually another sort of big
area
if endeavour at the moment I just want
to sort of this is a good point to
discuss you know one example of you know
being able to sort of plan and reason in
the world is this is google's alphago
project so this was it trying to design
a computer to play the game of Go which
is much harder than chess so that they
essentially used combine deep learning
with some kind of classical research
techniques trained it with lots and lots
of data from expert play and also self
play where the machine will play against
itself run ran on you know huge numbers
of GPUs but they built some incredibly
incredible system that could actually
beat the world champion recently and
this was you know huge event so in Asia
where go is a you know very much a
widely played game now while this was a
very impressive feat it's a it's
important to realize that in go it's the
world is completely visible
indeterministic that is you know fine if
I place a new stone down in a new
position it's completely clear you know
if that's my action it's completely
clear what the new position of the board
will be there's no uncertainty there's
it's completely there's no sort of it's
completely visible as well so you'll see
all the consequences of your actions and
of course the real world's just not like
that
okay so the real world of course there
are many of you know you might take and
try to to go turn left but maybe your
car doesn't quite make it or maybe
there's some other car on the way and so
on and so forth so you know building
general predictive models of the world
is another big research you know
endeavor at the moment and so you know
people are trying to do this with no
natural video trying to predict
subsequent frames you know I was
involved with some small project just
trying to predict such trajectories of
wooden blocks falling and things like
that
and there's a lot of recent work you
know from you know labs at Berkeley you
know deepmind Facebook and so on on this
kind of problem once you have a good
forward model like that you can put it
into sort of planning algorithms and try
and get a I am to reason about things
okay so those that's another sort of big
area now I've actually gone a little bit
faster than time so I'm just gonna wrap
up but I'm very happy to talk more about
so other unsolved issues so I think
there's been a lot of there has been a
great progress on for these perception
tasks and the many at pratik
applications that just didn't work now
possible and then you used you know
millions of times a day by you know
companies around the world but there's a
lot of algorithms on
thing we still have sort of fundamental
scientific roadblocks I mean to overcome
to be able to tackle you know other
tasks in AI okay so you know in
supervised learning when I mentioned
creating models of the world has also
touched on other things like program
induction as well or you know some
aspects of reinforcement learning as
well I think it also very fundamental to
progress and so there's essentially the
Hydro message here is there's still many
unsolved problems and there's no true
timeline relief with solving for
obtaining true AI in the general sense
of it so I'll stop there and happy to
take questions okay so any questions for
Rob Fergus if you have any yeah go ahead
if you could step to the mic in the
center that'd be great thank you sure if
you could just expand a little bit on
when you said that in the deep learning
model right the features that kind of
intermediate layers are not predefined
they're not selected by humans what
actually is happening in those middle
layers you know when you think of when
you think of a simple data set where we
hand tag it and create the attributes
what does she have what's actually
happening in the middle right that's
strictly speaking it's hard to say
exactly I can give us our hand wavy
answer it depends very much on the
domain at hand so the idea is that you
would want to be so it's easy as perhaps
to look at think about this from upon
you with images which you know it's easy
to visualize things and so those these
little plots I was showing at the bottom
here these are just showing you little
image patches that that cause
intermediate I mean one particular
intermediate feature map in one of these
big deep nets to activate very strongly
okay so this is these are sort of
patches if you like that that particular
part of the model really likes okay so
in this case it's learning some sort of
you know mid-level feature I think is
that is the term that people would use
in QP division which is essentially sort
of text on the side of things and this
up this is another one a separate
feature map but this one it seems to
really respond strongly to kind of you
know concentric structures and you can
see there's a wide variety of kind of
backgrounds and things it's that sort of
concentric circle structure that it
seems to respond to so the answer is in
general it's hard to say you can try
these sort of tricks of kind of
you know take inputs and see which types
of input get a particular model to
respond strongly in general as you take
the DS deep nets the first layers will
be finding edges the second lettuce
finds of conjunctions of edges they're
sort of middle layers have these sort of
middle of a concepts of sort of you know
parts of object and things like that and
then the higher-level features tend to
be all more object specific so you'll
find in these models that we have high
level features which all you know
respond very strongly to say you know
Alsatian dogs or to you know plants or
something like that or flowers and and
so yes but that again it's very
dependent on the data set and also the
domain that you're in so what these
things were learn so that's one the
remarkable things you can back propagate
that feedback through a hundred layers
of representation and there's some very
interesting maps as to why you would
connect I mean the latest models have
been designed very carefully to ensure
that that signal can be pushed all the
way back other questions yeah
stepping to the mic oh yeah you
mentioned earlier the one of the issues
with AI currently is a lack of label
training data yeah would you mind just
discussing recent pushes toward
developing label data specifically with
effect to Black Swan events or events
where there is lack of evidence for
right so so actually that the Black Swan
event phenomena you're referring to is
just find my thing so this is actually
what I was trying to show in these plots
here so these are sort of very rare
events that only Coe occasionally but I
guess you know obviously the Black Swan
scenario they too have outsized impact
on the particular system that you're
concerned with so there's a lot of
efforts to label data in the world the
catch is that these models footwell
first thing to say is that they the big
models still need a ravenous amount of
data it's you know millions of examples
to be trained effectively and you
typically see the performance just
increases it's continuously increases
you collect more and more data you can
train these things also without from
noisy sources so if you take images say
for example on social media platforms
users often provide hashtags for images
so there's
automatically provided by the person
upload of the image you can train on
those and they will be not they won't be
as effective as having completely clean
labels that you can't you can train
effectively on them if you have these
very rare events well that's really bit
of a now that these very rare classes or
very rare situations for which you just
simply cannot get many labeled train
examples well that's actually one of the
Sorokin research problems is how you can
sort of how you can generalize you hope
would be that you use your plentiful
data for which you do have lots of label
examples to build a good representation
for that domain and then once you have
that high level representation then you
can afford to train a sort of very
lightweight detector for your Black Swan
event on top okay so the hope is that
you know that Black Swan event still
should be somehow represented
representative you know should be
somehow you know in the same domain as
the other data for which you have you
know perhaps thousands or millions of
labeled examples but it's definitely an
open research problem is to have to do
this effectively right this is you know
so-called one-shot learning or low shot
learning as it's called how you can
train from these just you know one or
two examples so other questions know
what I actually do have a question so me
Oh John yeah yeah I mean that's that's
yes that's a very interesting point I
mean there's a interesting dilemma would
you rather have a system that you know
on any objective test and give it
performs better but you can't explain
exactly how it works or do you have
something that works less well it is
explainable and so that is you know it's
not I'm not quite I'm in that I think
depends on you know the application at
hand I think it is a big open question
as to exactly when these models
fail and there's a little sort of small
area of research in constructing
pathological inputs for these models
that will cause them to fail in very
unexpected ways you know you can take an
image of a giraffe or something add some
very carefully designed noise and
suddenly I don't think it's affirming go
or something like that okay and so it's
not quite clear whether these things
that you know fully practical in the
real world I mean people have been
trying to push these so-called
adversarial examples in that fashion but
it's you know it's a bit of an arms race
in some sense because you know you can
try and you know design these networks
they won't be fooled by them that's one
particular aspect sort of you know
creating sort of no sensitivity to
pathological examples but in terms of
giving hard guarantees that you know
this network will fail you know one in
you know ten million times or something
like that which is what you might want
for self-driving car unfortunately it's
you know other than gathering you know a
data set of say you know trillion
examples running through and just
checking out raters you know less than
one in ten billion 10 billion there was
something you know it's it's there's no
theoretical way you can show that at the
moment I mean maybe you go figure these
things out I mean this is a lot of
implications for things like
self-driving cars right because you know
you have very few of training data of
like people driving into a ditch or
something but um you know you send me to
be able to recognize when that's
happening you know with a car and you
certainly would like some reassurance
that this thing is going to be able to
you know you know we can explore this
little bit more on the the panel
discussion but as she described these
models they largely come across as a
black box and maybe multiple black boxes
one black box leads to another black box
so so we really don't understand what's
going on so when when when it
misclassifies you know a slug versus a
zucchini it really doesn't know anything
about vegetables and about slugs or
baseball bats or you know whatever
what are those sort of the the the
implications for the possibilities of
misuse given the models really don't
understand anything right so you mean
exactly by misuse
I suppose is I mean I mean they they
don't have any common sense if that's
what you mean so that they are there
this is just the introduction they're
very narrow in the sense although all
they're doing is just learning how to
sort of give up a correct label for an
image right so so Sonny if if they're
deployed in settings outside of that
then yes I mean all bets are off exactly
as to what they're going to do I would
say so that mean that in general is a
problem that if the distribution of data
changes from when you train the model
then you're in then you're in trouble I
mean there are techniques that people
working on so because you look at domain
transfer and it's like this to sort of
adapt and make models more robust to
that kind of thing but yeah I mean it's
okay so what pursuing the panel
discussion had a endow some metres
public with some common sense okay so
please join me in thanking Rob
okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>