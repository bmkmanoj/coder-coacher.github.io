<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ethics of Artificial Intelligence - Yann LeCun, Nick Bostrom &amp; Virginia Dignum | Coder Coacher - Coaching Coders</title><meta content="Ethics of Artificial Intelligence - Yann LeCun, Nick Bostrom &amp; Virginia Dignum - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ethics of Artificial Intelligence - Yann LeCun, Nick Bostrom &amp; Virginia Dignum</b></h2><h5 class="post__date">2017-09-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IfSQWKvrwAQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's happened twice okay now we've just
got time for a brief panel discussion
let's see if any of the other speakers
have questions for each other of the
machines but typically ethical
principles are very abstract and
machines are trained for very practical
functional things and there is a whole
link between discrimination or privacy
sorry there is a lot of big distance
between the concept of privacy or the
concept of discrimination or the concept
of safety and the very practical
functions that the systems are built on
how we would propose to bridge that
distance yeah I mean certainly it's not
a solve engineering problem there's no
recipe of how you design those objective
functions and and presumably you know
you can imagine the the isomer of Three
Laws of Robotics could be kind of an
example of the objective functions but I
never like them because they're not
operational they're not something you
can sort of reduce to you know to an
objective that a machine can optimize so
you have to find substitutes of that
that achieve the same goal but at the
same time are easier to kind of
implement if you want now for the
question of which is more short-term
ethical question about AI a machine
learning which is the issue of bias
that's that's not a question necessarily
of designing the objective function or
the machine it's more of a question of
what's the appropriate data to train it
on so a learning machine will reflect
the biases that are present in the in
the training data that has been used to
train it and we've seen examples of you
know some small companies that are not
very experienced with the the you know
the process of doing this making very
stupid mistakes in training
systems with you know reflected biases
in their data and you know predicting
that you know crime recidivist some
systems like this as some cities of
about to predict the rate of recidivism
for for particular people and they're
biased against you know people with the
skin and so there are mistakes like this
that people shouldn't make so one of the
goals of the the partnership on AI that
it was was alluding to earlier some of
the other funny members are in the room
here Francesca for my privy am one of
the purpose of this this partnership for
yeah is to define perhaps guidelines and
you know ethical des guidelines of how
you build AI systems so that they're not
systematically biased they don't reflect
the the worst aspects of society a
hundred thousand years ago there was a
conference of Neanderthal like creatures
and it was called the ethics of modern
human intelligence and they were very
worried about being out competed by
humans and they raised all sorts of
concerns but now when we look back maybe
they really ought to have been out
competed and maybe it was a good thing
so when we worry about the future of
artificial intelligence what do you
think about the possibility or the the
depressing possibility that we ought to
you know start preparing ourselves for
dinner in the distant future because if
we're worried about multiple emulations
maybe they ought to have more votes than
us is that possible that that will
what's ethical with respect to
artificial intelligence when it's
sufficiently smart it's just a question
we ought to ask it
[Laughter]
well there's room between the claim that
we have reached the maximum of what is
valuable and the claim that whatever
happens is valuable or whatever is the
most intelligent is valuable it might be
that there's a huge space of possible
mines possible modes of being possible
types of lives and experiences and that
with our human wetware we can explore a
tiny corner of that vast space of
possibilities and in this much larger
space there might be some that are more
valuable and a lot that are less
valuable and what we then would have
reason to do is to try to find a
trajectory that eventually leads to one
of the more valuable regions it doesn't
mean that just because something is
smarter than us therefore it would be
good thing if it took over you could
have I think a super intelligence is to
take the classic example and in this
domain whose only goal is to make
paperclip so you have a paperclip
Maximizer and it's very very good at it
and it produces a world which is full of
paperclips and nothing else except
machinery to make more paper clips by
the metric of intelligence it would be
far superior to us it might have the
brain the size of the planet and yet in
terms of possible accounts for what has
value it would be inferior cause it
might not even have any internal mental
life so the question remains even if you
think that there could be ways of
existing that would be more valuable
than our present ones we still have to
put in work to ensure that we reach
those and I think to me it possible
account would be that the future is very
big that there's this enormous petri
dish out there that we could have space
both for fantastic continuations of the
7 billion people happen to live now and
also for all kinds of new exotic super
beings that it we should first make sure
we satisfy all legitimate easily
resource satiable preferences
if we can do that with a small fraction
of the total and then we can have a
discussion as to how we use the rest of
it what additional things could be built
so the the the interaction between two
intelligent entities with different
capabilities is something we're actually
quite familiar with already first of all
inside of our own brains right so we
have or let's say our reptilian brain
that basically you know has all those
kind of low-level drives that makes us
feed and etc and and then you know the
the cortex on top of it that essentially
is subservient to it even though it
builds on top of it is subservient to it
and so you can imagine that AI system
would be kind of a you know an EXO
context if you want that it's
subservient to our brains but extends
its intelligence the same way or cortex
extends the intelligence of a reptilian
brain so Gary Marcus is Gary here has
okay so you know it has this little you
know we disagree on many things
ya know but but because it's nice to
book a kludge where you know he
described this example of you know
you're you're if in front of a piece of
chocolate cake and your regular brain
says go for the calories and your you
know photo cortex which can do long-term
prediction says wait a minute this is
actually not good for you
in the long term so you shouldn't go for
it and sometimes your children brain
wins so I think you'll be kind of the
same thing in our relationship to AI
systems they they will probably do a
much better job at doing world world
prediction right now they're terrible
but eventually it might be good at it so
they might do a good job at predicting
what's gonna happen next and that will
allow us to make better predictions but
in the end hopefully we'll be in control
it's not very different from the way we
use computers today right we have large
supercomputers doing weather prediction
and they are used as a world model as a
world predictor as a world model from
which we can take make decisions so I
don't think you'll be qualitatively
different from the interaction we have
today also I don't know about you but as
a manager in a industry research lab I'm
very used to interacting with people who
are way smarter than me and I'm the boss
right so
thank you all this questions for Janvrin
it could be open to the other two
speakers I was just thinking of your
predictions and the face of uncertainty
and the kind of prediction ribbon in the
in the graph and then I was thinking
about the video and imagery predictors
that that you showed and I was just
wondering with regards to say evidence
in courts in the term in like the very
applied case of criminal justice at what
point would you be comfortable with that
being used as a application of evidence
in terms of predictions of facial models
when it comes to CCTV footage and etc
right different questions there so I
mean certainly there are pattern
recognition vision AI systems today they
can be used for you know forensic
purposes and how how they'll be accepted
as evidence in court I have no idea
another another lawyer and really no
specialist of those questions but you
know perhaps the same way DNA evidence
is which is kind of statistical in
nature is also is also accepted or
photographic evidence which is has
diminishing value now that we can kind
of tweak in terms of like the ability to
predict basically what what forensics is
about is retro addict you have you
observe a state of the world and you
have to deduce a sequence of actions
that that led to the state of the world
essentially right you go to a crime
scene I have to figure out what happened
so it's reproduction of prediction but
it's the same problem it's really very
much very similar kind of problem well
machine help us win this not anytime
soon
a question for young probably sorry
popular today so if the machines three
have an intelligent machine and it's
primarily is a reinforcement to specific
trainers when the easiest way to get
reinforcement to be to put a gun to the
head and force the positive signal yes
so you suggested a mechanism for control
for an intelligent machine is to assign
specific trainers that the machine will
crave positive feedback from that was in
one of your slides so my question is
wouldn't the easiest way to get the
positive feedback but to get them to
push the button would be to put a gun to
their head well I mean what a statement
I made is that I thought reinforcement
Oni was actually a relatively minor way
of training machines it's it's a
rotating manual way of training humans
as well so there's very little that we
learn as humans is conditioned by reward
and Punishment most of what we learn is
the vast majority of our knowledge is is
through this you know possibly it's a
hypothesis of course there's no proof of
this but it's for this prediction and
sort of modeling the world which is
tasks independent and then on top of
this you run a task perhaps using
reinforcement or supervised running and
that if you have the proper
representation of the world that has
been built through unsupervised learning
then learning those tasks is very quick
and requires just a few examples so this
is what is surprising today that's what
creates the the difference between the
amount of samples that are necessary to
train a supervised system today which is
thousands whereas you know you show
three pictures of an elephant to a child
and you try to figure out what an
elephant is so it's it's because there's
a vast amount of knowledge that the the
child can build on top of it already has
a good representation of the visual
world so yeah I'm not trying to answer
your question but I see reinforcement as
kind of playing a relatively minor role
I hate to return to this theme but I
continue to have the impression that the
role of consciousness is being
downplayed so for instance when you said
look should we be morally worried about
disconnecting them well we could build
them in such a way that they don't care
about being disconnected and then
somebody rightly pointed out that it
might be just part of the functioning to
have a certain kind of functional
instinct for self-preservation the
question is you know if it has the
functional instinct for
self-preservation you know Apple could
easily design my laptop so that whenever
I touch the off button it said ouch
don't turn me off and so forth and
exhibited all kinds of seemingly caring
about being turned off the real notion
of caring about being turned off is one
that appeals to the notion of
consciousness which has been left out of
the discussion so I think all of these
issues about whether we ought to have
complicated moral views about how to
treat systems turn on notions of
consciousness which have not yet been
figured out they certainly the
epistemology of saying whether a unit
especially one that functionally quite
complicated and intelligent in some
sense his conscious or not has not been
settled and I think before we do that we
can't really address these questions in
it in the correct way well so we have no
problem going through anesthesia which
is you know essentially
indistinguishable from death except it's
reversible and so you know somehow so
that's that's the first point the second
point is there's a lot of small animals
who clearly fear being turned off turned
off who I think most people would say
are not conscious or maybe they are it
depends on your definition consciousness
so third of all I don't know how to
define consciousness so that kind of
makes this this question very difficult
for me to answer but if I had to make a
guess as to what people think of as
consciousness it's perhaps the ability
to of this prediction system I was
telling you telling you you know I was
talking about to see yourself as part of
the world right so you kind of
observe yourself in your world simulator
the wall simulator in includes yourself
and and so you have this feeling of
observing yourself you know from from
the outside why is that qualitatively
different I mean why is there you know
some news or a moral question about this
particular effect in a internal machine
and so any bottom line is basically I
don't see the problem
I think consciousness probably as like
many other aspects that is gradations to
it there will be I agree that I don't
know how to define consciousness but
there will be very many different levels
of consciousness from it making us
believe that it's called conscious to
simulating consciousness to the full
consciousness like we attribute to each
other so somewhere there in between will
be the machine consciousness hi thanks
so much this is a follow-up on some
things that Professor Dignam was saying
but I think everyone could chime in on
it it's about accountability and
transparency so if a machine learning
algorithm has selected me as someone
that the police should stop we might
think that I have a right to ask why I
was selected but it also seems like the
more powerful and efficient the
algorithm is the more complicated it
will be and the harder it will be for us
mere humans to have really any grip on
why it's making any particular decision
based on ten thousand different
parameters so I was hoping you could
speak a little bit to the trade-off
between transparency and accountability
and how powerful and intelligent these
things are yes you're right you did
complex the algorithms are more
difficult it will be for it to be
transparent I have the making them more
complex and more efficient in the
operations they are trying for is maybe
not always the way to go we have to make
a trade-off between
optimal operational function and
transparent operation and I think that
that trade-off is where we have to to
look at in designing those algorithms
and now we are mostly designing
algorithms from the operational
efficiency and a lot of discussion of
artificial intelligence itself operating
as a moral judge as a moral system but
the more pressing question and the
immediate question is on what basis
should we decide what kinds of choices
and decisions to delegate to automated
systems and what safeguards or what
conditions on the operation of those
systems are required by moral standards
that we ourselves are going to use and I
mean I would just like to ask the panel
whether they think that that issue the
question that this question is
necessarily going to involve some
attribution or design
moral judgment in the systems themselves
I don't think that that's obvious it may
be that all of the moral work will have
to be done by us and just by way of
analogy there is a much bolder
technology of delegation of choices to a
quasi automated procedure which has been
extremely important and that is the
legal system and about the design of the
legal system the question these
questions are very important what should
be the moral standards that we use in
deciding to turn over to a set of
largely automatically applied rules
certain important decisions and that
kind of delegation has been a vital
aspect of human civilization now in the
legal system the parts of the machine
that carry it out are human beings and
sometimes at least in common law systems
they have to make but a great deal
some it is just carried out
automatically and I would like to know
why you think that I could I mean I
think part of that connects to an
earlier question and one distinction is
whether the domain of application is the
same or known to be very similar to the
domain of testing so in the limit in
case you have a set of points that you
might want judgments over and you can
randomly select some and see how the
algorithm works on those and then you
can statistically have guarantees that
with a certain likelihood it will
operate within the wider domain from
which you randomly sample and and there
you might just want accuracy from the
algorithm and you can you kind of feel
the strong guarantees that it will
behave as you intended but if you intend
to apply it outside the domain in which
you can test its performance then you
need to have some understanding of how
the system works in order to be able to
guess how it will extrapolate from the
cases it has seen and if you can't have
that kind of guarantee then you would
want to have the ability to revise it to
supervise it to go in and change its
decisions yeah this is a funny thing
which is that the in certain languages
we call the law the legal code right and
it's not by accident it's it's code in
the same way as a program is code that
we call a computer program code so it's
you know basically a set of rules that
implements the combination of two things
one is in the little
diagram that I showed where you have the
objective which you know encodes the the
moral value you give it a state of a
state of the world or a sequence of of
things of actions and you tells you
whether it's good or not and then there
is the agent that tells you produces
actions that are according to this
objective or not and legal code I think
is a combination of the two so it's a
it's it's a way to define an objective
function for for people in society to
make people behave certain ways so it
defines the objective function for
people in some ways that's why there are
incentives and and and Punishment
basically in so it's it's kind of a
substitute objective function that makes
it such that when people optimize the
subjective function they ultimately
maximize the common good if it's
designed properly of course it's very
difficult but you're right it's very
similar I mean I see it in very very
similar terms except legal code applies
to societies which are collections of
people whereas the objective I was
talking about applies to a single entity
or a single AI entity and we have kind
of a similar modules in our brain that
sort of is a moral fiber
he won't yeah this is a quick question
I was just wondering so instead of kind
of prescribing these more general
objective moral claims to a machine is
it possible that we could sort of use
this system of neural networks to upload
if you will or dump
I guess the history of philosophy or
moral theory more in general and have
the Machine conclude on its own what
might possibly be the best philosophical
conclusion or standard to then hold
other machines to if you will right
you'll find their biases people don't
agree as well between cultures are
between the different people on the the
morality of many actions and on the
concrete translation of Morocco
Sybil's into actions so I think if you
would take that that line of that
approach we would find the same kind of
bias in the decisions that the machines
take is what we find now in bias in
profiling and whatever it's interesting
approach to consider so I think there
are two parts to I mean whatever I can
see though one is the sort of model of
the world part which is you know if if
we behave in such a way here is what's
gonna happen in the long term like why
is it you know good to have I don't know
democracy or something like that or you
know sort of general principles that
organize behavior what would that lead
to in the long term so that's the issue
of predicting what a particular type of
behavior will produce will it be good in
a long term a very long term and a lot
of the issues political issues that
people disagree on is basically
differences in prediction of what's
going to happen in long term or
differences in waiting of short term
versus long term objectives like you
know time of change or things like that
and so I think the it's gonna be
machines that surpass us in general
cognitive abilities in all domains then
I think it casts a new light on
intellectual priorities so there is a
bunch of problems that we could delegate
defer to the more capable reasoner's
that we think might exist in the future
that would include a lot of moral
reasoning insofar as we think that there
are progress that can be made by
thinking about moral questions that
progress could be made better and faster
and easier we these machines today the
task then becomes to isolate a subset of
problems that we need to solve ourselves
first either because they are very
pressing or because we
read the answers to those questions in
order then to be able to create this
super intelligence in the right way
and all the other problems that we don't
need to have solutions to between now
and then I think have reduced urgency
and can maybe be delegated to the future
and so I argue for this principle of
delegation which is that when possible
with a lot of qualifications we could
sort of defer a lot of these detail
judgments to the the more efficient
Minds that will exist in the future and
and we shouldn't beg the questions we
shouldn't sort of lock in our own
possibly flawed guess is about factual
matters if we could instead leave that
open to be revised and analyzed more
efficiently by the better minds that we
might build oK we've gone over time so
we better end up there um we go to go
eat at one of the very many fine
restaurants nearby and be back here at
1:30 but let's think all of our speakers
for great discussion</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>