<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Nvidia - AI for Generating High-Resolution Images - Tero Karras | Coder Coacher - Coaching Coders</title><meta content="Nvidia - AI for Generating High-Resolution Images - Tero Karras - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Nvidia - AI for Generating High-Resolution Images - Tero Karras</b></h2><h5 class="post__date">2018-05-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zrVDfX9XFpE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so our next presentation is going to be
a great work by several people from
Nvidia so taro grass who will be giving
me the presentation Tim oh Allah Somali
Somali lane and Jacko Lehtonen on
progressive going of Gans for improved
quality stability and variation and
impressive images thanks for the
introduction Pascal so hi I'm Derek Aris
from any p.m. generative adversarial
networks are quite impressive they can
turn zebras into horses cats into dogs
dogs into smoke mushrooms and problems
into smiles however they are very
difficult to train and these techniques
are usually limited to fairly low
resolutions inspired by this we wanted
to see what it will take to scale up and
be able to generate photorealistic
images at mega pixel resolutions but
first we need to understand what is
going on during the training our
intuition is illustrated nicely by
looking at this map showing them Finnish
archipelago the islands represent
clusters of real images in the vast sea
of all possible images and we wish to
obtain a generator function that
produces something similar we train the
generator to shift the distribution of
this generated samples closer to the
real thing let's take a closer look at
one of these dots our view of the
situation looks like this the generated
sample is the surfer guy who is seeking
the shore everything is governed by the
loss functions in our case the improved
water Stein
most importantly gravity is pulling the
surfer down but then we have this
adversary who is raising waves wherever
at the surgeries however they cannot
raise waves on the shore and waters have
to be calm in between so what is
happening is the term surfer is sliding
down the way keeps following them and so
they'll be able to ride it to other
Shore what is the problem with hybrid
high resolutions then well if the
resolution is low than both the
generator and the discriminator are
rather simple functions and our picture
is accurate but if we increase the
resolution they become deep networks and
they represent complex functions with
shattered gradients so it's almost as if
we had thrown the surfer in the middle
of the ocean there's waves everywhere
and the shore is nowhere to be seen in
many ways we are now looking at the
situation from to close if we decrease
the resolution
it's like zooming out we get a much
clearer view of the big picture but we
can no longer discern the smaller
smallest waves motivated by this our
approach is to start the training from a
ridiculously low resolution in hopes
that we will start making progress
toward the right direction once we get
closer to our goal we'll then increase
the resolution by adding more layers and
keep going like that so that we'll be
able to model finer and finer details to
all the end so that's the basic idea
let's look at the implementation our
generator is made up of this replicated
blocks that consist of up sampling and
3x3 convolutions we can extract a low
resolution image out of the network at
any point by passing the activations
through this one by one convolution when
we want to increase the resolution we
cannot go and add the new layers just
like that because
are completely untrained and doing so
would cause a sudden shock so what we do
instead is extract two images at
different resolutions and do a linear
crossfade between them to gently
introduce these new layers once we're
done we'll scrub the lower solution
image and keep going
our discriminator is a perfect mirror
image of the generator and both networks
keep growing in synchrony using exactly
the same kind of linear crossfades
let's see how this looks like in
practice on the Left we have standard
fixed resolution training and on the
right we have progressive growing
with fixed resolution the convergence is
somewhat chaotic and I would claim that
it's not always clear whether we are
making progress toward better images but
with progressive growing even though the
resolution is low at the beginning the
images still stay pretty reasonable true
throughout the entire process and
there's a clear tendency towards better
and better results
besides quality we must also consider
variation what often happens is that all
of our our generated samples flock
toward the same nearest shore and we
have a mode collapse right there in
order to avoid that we need to have some
way to force the samples apart and keep
them apart throughout the training we
obviously cannot do that
if we're looking at just individual
pictures so the obvious way out is to
extend the discriminator to look at
multiple images at once this has been
proposed before and it's a good idea and
we built on top of top of it by
introducing this spatial layer to at the
end of the discriminator the input is
the set of activations from the previous
layer for every image in the mini batch
these activations are passed through as
is but in addition we also compute the
standard deviation for every activation
over the images we then average these
standard deviations over the teachers
and over pixels to build a single scale
of re value with then replicate that and
upend it as a new feature map the output
the surprising thing is that even though
we are looking at a seeing a single
statistic it seems to be enough in a
practice to enforce exactly in the right
amount of variation the benefit of our
technique compared to previous ones is
that the the number of our trainable
parameters
does not increase almost at all there's
one more obstacle to overcome though
what sometimes happens is that the
server stumbles a bit and accidentally
ends up on the wrong side of the wave
gravity still pulls them down and the
wave still keeps following that's not
something we want to happen we have
observed that one of the early symptoms
of this scenario is rapid increase in
activation magnitude and also that any
kind of normalization in either network
is very effective in preventing that so
it's no wonder that virtually all
previous techniques employ some kind of
normalization usually in both networks
in a particular they employed both
normalization which is problematic
because it requires huge mini batch
sizes and with high resolutions we
cannot afford those furthermore we seen
that small mini mini batches are
generally better in terms of convergence
so what kind of normalization should we
use then first of all it's enough if
just one of the networks is unwilling to
participate in this concept kind of arms
race so it's not we can safely leave out
any kind of normalization in the
discriminator then in that generator we
employ this new enormous Asian scheme
where we look at the activations of
reached convolutional layer and we treat
each pixel as a vector over the teacher
maps and process every pixel
independently what we do is simply
normalize these vectors and pass them on
to the next layer the surprise here is
that this kind of normalization does not
seem to harm
later in anyway but it's well enough to
prevent about scenario from happening
let's look at some results next these
are examples of images produced by our
generator after training it with a new
data set that we call select a HQ it's
based on the standard cell at a but we
used various tricks to improve the
overall image quality it consists of
30,000 images at 1k by 1k resolution and
is available in public we feel that most
of these images are almost
indistinguishable from the real ones
which is pretty nice
since the generator is a continuous
function it's interesting to look at
interpolations here we are borrowing the
input the generator
randomly using spline interpolation and
what we can see is that many of the
transitions look quite natural the hair
is growing and changing shape the
viewing direction and lighting
conditions are changing smoothly and the
facial expressions are changing quite
naturally I would say that the generator
has learned to model various
characteristics of the images and is now
mixing these characteristics in novel
ways some of the transitions are still
somewhat weird and look broken and I
think this is an interesting open
question about future research what
exactly is causing that we also tested
our techniques using the Elson data set
at 256 resolution the data set contains
30 categories and here I'm showing
examples from certain categories I
should emphasize that these days it is
extremely challenging because there's
immense amounts of variation in all of
the categories nevertheless our
technique
was able to finish training up or every
similar category without fail and many
of the images look quite reasonable it's
especially interesting to see some 3d
rotations and Zhu means in many of the
categories we are particularly proud
that our network has learned to generate
low cuts with some gibberish captions
even our paper covers are several other
contributions that I will skip here but
I should mention that we found it
extremely important to have some kind of
a reliable metric to judge the quality
of the results especially for hyper
parameter search we didn't have any good
ones when we started so we have to
invent our own but these days there's
for example the fresh a inception
distance which is pretty good our
implementation and networks and all
kinds of other material are available in
public and if you have enough GPU power
you'll be able to train these 1k by 1k
monster networks in two days if you're
interested come see our poster on
Wednesday thank you
a couple of questions
thank you that was very impressive this
this is still very image based oh yes
this is still very very focused on on
images and then in fact your whole
technique but is based around low and
high resolution images do you have any
way to generalize this to other other
fields other problems speech text or
anything like this we don't have any
results on those domains yet but we are
very interested in trying it out for
example with audio I think you could
quite easily and do the same kind of
progression just by resampling the audio
signal and I'm interested in finding out
whether it is actually more about the
resolution or the capacity and depth of
the networks it might be that even if we
don't produce the resolution of the
domain we might be able to benefit just
from the fact that we are adding more
depth in the networks as we go
any other question
in the last question before everybody
gets me Jasha this looks too good to be
true did other people reproduce your
work independently thank you well
reproducing it is quite easy just
download the code and run it I have seen
implementations by other people by
Google at least and something running on
my torch haven't tested them myself okay
thank you very much
so this closes our oral session the
break we're late for a break just in
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>