<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning &amp; Robotics - Prof. Pieter Abbeel | Coder Coacher - Coaching Coders</title><meta content="Deep Learning &amp; Robotics - Prof. Pieter Abbeel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning &amp; Robotics - Prof. Pieter Abbeel</b></h2><h5 class="post__date">2017-10-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SYqV543LWoY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone to the ex-coworker
young today we have one of our own as
our speaker
Peter Beale is of course a professor
here at UC Berkeley in ECS he's also at
embodied intelligence a new startup
company and he is one of the founders of
great scope which many of you have
probably come across either as students
or TAS in your courses here at Berkeley
Peter received his BS and MS from Kuo
even in Belgium and his PhD in computer
science at Stanford his researchers at
the intersection of machine learning
robotics and control now we overlap the
graduates to as graduate students at
Stanford I think it was a year or two
ahead of me and back then he was known
around campus as the guy who would fly
helicopters autonomously upside down and
then once I followed in his footsteps to
Berkeley his group developed a
reputation of making the pr2 robot from
Willow Garage to an entirely new feat
such as picking up and folding arbitrary
laundry items now underlying all these
amazing applications are deep in
pioneering techniques in reinforcement
learning imitation learning unsupervised
learning and meta learning his works
been featured in the popular press pick
any outlet New York Times wired MIT
Technology Review Peters there and he's
also won a long list of awards such as
the Sloan research fellowship the Air
Force Office of Scientific Research
Young Investigator award the MIT TR 35
the I Triple E robotics and automation
society Early Career Award and the dick
vaults best u.s. PhD thesis in robotics
and automation award please help me in
welcoming Peter Biel
all right my presentation here will be
with my research hat on but if you're
interested in putting more AI and
robotics your manufacturing logistics
operations come talk to me or if you
want your grading to happen
automatically also come talk to me a lot
of the work I'm doing here is motivated
by this video over here there's a video
from around 2008 and what we're watching
here is the pr1 robot developed by a few
PhD students at Stanford Inc in
Salisbury's lab and the PR one robot is
doing a lot of the chores that we wish
the robots would be doing for us but
there's a catch and the catch is that
this robot is actually being
teleoperated Eric burger one of the
students is sitting inside a harness and
puppeteering every single motion the
robot is making so actually it's more
time consuming to do things this way
than to do it yourself but what it means
for us computer scientists is that at
this point the problem is largely a
computer science problem that is an
artificial intelligence problem because
we have the hardware that in principle
can do the job well do we really have
the hardware Willow Garage released the
pr2
but it was $400,000 so maybe not
everybody would would buy one right away
then four years later rethink robotics
came out with Baxter for $30,000
unbounded robotics since transform into
fetch robotics came out with a you VR
one for $35,000 fetch is a little more
expensive than that one and so I've been
hopeful for a while now that in 2017
another four years later we'll do
another factor 12 of the price and well
we have a three months left it still has
to happen but I remain hopeful now
what's the problem setting that we're
trying to solve here it's not a standard
machine learning problem we build in the
eye for a Baltics it's actually a
learning problem with a feedback loop so
this is a robot this environment around
the robot then based on the current
state of the environment your AI agents
supposed to make a decision choose an
action given current state that actually
gets executed then robot an environment
change and this repeats over and over
and over so you're dealing with the
consequences of your own action in this
kind of setting
what are we optimizing for reward
so any problem in robotics that you
tackle this way you define a reward
French for example quality of the cooked
meal time to get to a destination may be
negative rewards for getting into an
accident things like that
and so your reward defines what you care
about what you and how you specify your
problem and then we try to find
hopefully automatically is a policy PI
theta that optimizes reward over time
this isn't just in robotics by the way
there's a lot of spaces promise places
where you can apply this
marketing/advertising when you
repeatedly interact with the same
customer dialog anything where there is
after action something that changed in
the environment that would have to deal
with the consequences of that change
this makes new challenges compared to
supervised learning which is the
standard pattern recognition kind of
learning we have an input corresponding
up and you try to find the pattern to an
input and output like an image
recognition or machine translation and
so forth what are the new challenges
stability is a challenge because this
feedback loop can destabilize if you
have a poor controller here credit
assignment because you're learning this
controller when something good happens
you need to somehow understand what it
is that you did in the past that made
this good thing happen if your robot
cooking for half an hour somebody at the
end of the half hour gives you a
five-star rating
what was that you did right in that half
hour to get that rating maybe something
she did wrong and so exploration if you
want to learn to do well find a good
policy you have to try things you
haven't tried before otherwise you're
not going to acquire a new skill and how
did that cleverly is a big challenge
despite all these challenges there's
actually been a lot of breakthroughs
over the past few years in solving this
problem what I mean with that is
reinforcement learning algorithms that
train this policy PI theta so that it
becomes a good policy through just trial
and trial and error in the environment
that's placed in for example you could
look at video games the challenge here
would be learn to play these games where
your neural net let's state takes in bra
pixel values is supposed to output the
action to take in the current situation
you need to do image processing control
decision-making all that needs to be
learned to somehow lead to good behavior
in the game quite a few results starting
with the 2013 deepmind results also
I'm out of Berkeley the TRPA results
some more deepmind results parents of
dqn showing that this is now possible
you can train from scratch a deep neural
net to play these games and what's
interesting here is that that deep
neural net can be the exact same
architecture and run the exact same
algorithm when you go to the next game
it'll just learn different parameters
that are appropriate for the new game so
a very general approach similar idea has
been applied to learn to play go
classical game that people for a long
time thought would remain out of reach
for computers for a much longer than it
did here's not a problem continuous
control so now the inputs here will be
joint angles joint velocities the output
torque at each of the motors reward
function is the further you go to the
right the better and the less impact
with the ground the better here we're
actually seeing the learning in action
initially it's not doing well because
it's initialized with random parameters
and random parameters don't lead to good
behavior pretty random mapping from
input to output but over time it makes
sense out of the better attempts the
worse attempts differentiates them from
that calculates updates to the weights
in the neural net finds a better neural
net that can control the robot better
and collect more reward the beauty here
is that the algorithm used underneath
frustration policy optimization can be
used without change to optimize the
neural net to learn to play Atari games
or to learn a neural net to learn to
control a different robot and so what's
really intriguing here is that while
classically if you were worried about
let's say locomotion you would study the
specifics of two legged locomotion and
when you switch to four-legged robots
you look at a whole new literature about
how you stabilize four-legged robots
none of that is happening here you don't
need that expertise you just need a
simulator and expertise in how these
learning algorithms work and it'll learn
to control the systems actually a little
caveat that you also find out when you
run these things is that if your
simulator has any bugs for example there
was no way that robot could run that
fast in reality then this our and we'll
find help find those bugs for you
because I'll do something that's
unrealistic you go fix your simulated
here the reward function is distance of
the head to standing head height the
closer to standing head height
the higher their award and so over time
it figures out a way to get up again
with zero prior knowledge about what
standing means it's just measuring
distance of the head to standing head
height it figures out the right thing to
do to stand up here's another one with
the real robot is the pr2 that Bjorn was
alluding to pr2 has been programmed to
fold laundry but here what it's doing is
learn to stack Lego blocks and what you
see here is something that takes about
15 minutes when run in real time learn
to stack the block on to that corner
spot starting with zero prior knowledge
about the environment or about its own
arm he's not a robot we looked at this
is in a collaboration with NASA for
their planetary exploration projects
this is the superball robot it's a robot
that's essentially cables and rods and
the beauty of this robot is that you can
compress it to be near planar makes it
easy to ship very small volume and then
you can expand but you can also
withstand impact on landing now the
question is there's no wheels here
there's no legs how do you make this
thing this thing move and go where you
want it to go turns out that you can
pull the cables to shorten edges on this
contraption if you shorten that just the
center of gravity will shift and it
might tumble and they might do this
again again and again and get this thing
to roll actually very hard to design a
controller for that it turns out with
reinforcement techniques it was possible
to design a controller that is have a
controller be learned that can reliably
roll the Super Bowl robot in fact the
real robot not just in simulation and
this was the first time this robot was
able to do continuous rolling as opposed
to the hard-coded controllers the best
people had gotten to was one tumble and
then a long relaxation phase and then
another tumble and repeat so what this
shows is that it is possible to have
reinforcement learning result in mastery
of a new skill and
actually masters it pretty well
essentially often up the human level
even beyond for example for the game of
pong a good human might score nine point
three in this game whereas dqn was able
to achieve eighteen point nine which is
higher but the experience needed to get
to eighteen 18.9 was a lot more 40 days
of experience to learn with DQM two
hours for the human and so one could
easily kind of conclude here and I would
say so that this is pretty slow learning
compared to this which is much faster
learning and what we really want is fast
learning not slow learning and so
everything I've shown you so far has
been successes of slow learning and it's
nice if you want to problem solve be
willing to wait for a long time it'll
solve it for you just sit there wait a
few days or 40 days you might get the
solution but often you want to solve
many problems you want to robotic and
solve new problems when it encounters a
new problems and so forth and you don't
want it to take another 40 days before
it does the next thing so how can we get
to much faster learning more like human
level learning well let's take a step
back for a moment at a high level what
have we seen in computer vision this is
a slide I borrowed from Andre karpati on
the vertical axis here I'm looking at
the types of data
so initially computer vision worked with
the data set that consists of one image
called the Lenni image and so 10 to the
0 was the data set size then Caltech 101
a little more images Pascal views even
more image nutiva more images and then
the Google Facebook size data sets are
even larger then a horizontal axis here
the approaches people followed initially
people hard-coded what to do with an
image and if you have only one image you
can actually succeed at that because
well you only need to turn that one
image into one result then we had more
images people started hard-coding image
features like edge detectors if a small
number of images you might be able to do
that now when the data has became larger
for example when we got to the image net
point it turns out these hard-coded
approaches saturated weren't able to get
to really good performance but training
deep neural nets was able to do better
and so we'll see happen here is as we go
to the right in terms of techniques
these are techniques that require more
and more compute also benefit
more and more from having more data and
can solve harder problems and so the
kind of projection that we're seeing
here is that I also get more computing
more data we'll need techniques that are
more and more out here to be successful
let's look at the analog for
reinforcement learning so used to be
that we hard code controllers let's say
you want some we're about to do
something armed to move somewhere you're
hard-coded controller for that it might
not just work then we develop things
like value iteration which work for
small dimensional state spaces then we
went to more complicated algorithms like
dqn and the recent policy grade methods
and then where I'm interested in is
where we're headed it is this rectangle
over here where we're going to hopefully
enable things that these current
techniques are not able to to do and the
reason we're going to be able to do them
in the future and nobody's done them in
the past is because they'll require an
amount of compute it's actually pretty
high because the computer is going to do
most of the lifting computer is going to
invent the algorithms for us essentially
rather than us inventing the algorithms
which of course is a bigger calculation
to do than just solving one problem but
it once the computer has invented the
algorithms then that algorithm has been
invented by the computer can solve many
many problems very quickly at least
that's that's the hope okay so why will
compute continue to increase because
well there's a lot of money made right
now with deep neural nets and people
realize that and people build more and
more specialized compute for neural nets
so NVIDIA has been doing that for a
while now but it's not hyper specialized
in neural nets googly specializing more
into a Nirvana or specialized in neural
nets graph core will be at the explosion
here in two weeks three breaths and so
forth so it's likely by specializing to
neural net compute that within the next
two years we have between ten and a
thousand times somewhere in that range
to compute that we have today in a
single note that of course will scale up
the cloud even further as is already
going on so the amount of compute that
we'll have available will be so much
more than we have today which will allow
us to work on the right side of that
graph and solve problems that are higher
up in terms of difficulty
okay so what's going to be underneath
this or what what do I think might be
underneath this think key will be
learning to learn rather than us
designing the learning items
our selves which have been doing for
years and years and we've only got them
so far let the computer do more of the
lifting for us let's see how this might
work in reinforcement learning
so again businesslike we looked at
before slow learning is dqn style
learning for any current deeper
algorithm not just DQ and all of them
are pretty slow and then humans are much
much faster
the reason this current state of your
rhythms are slow could be because they
are fully general algorithms that is
they can solve any mark of decision
process and Markov decision process or
MDP is kanaeva that's just the
mathematical definition of the
environment you're interacting with they
can solve any MVP no matter how it is
defined
they're always applicable but in the
real world what you encounter is only a
very small subsets of what you can
mathematically define and so it might be
that maybe the reason we can solve
problems more quickly is because very
attuned to the types of problems that
are happening in the real world not
attuned to all mathematically definable
problems which is a very wide set of
problems and so the question becomes can
we discover that is learn somehow fast
rll rhythms that take advantage of the
fact that you only need to understand a
tiny sliver of all possible NDP's namely
the ones that are realistic for our
world okay let's try to formalize this
this will come back a few times so let's
take our time to parse through this
here's how we're going to try to
discover a reinforcement learning
algorithm rather than designing it
ourselves so what we want
ultimately is a reinforcement learning
agent parameterize by theta what could
be theta
it could be parameterization of a bunch
of code parameterization but neural net
could be anything but somehow there is
something that is going to be our agent
what do we want the agent to do want to
be agent the agent to be such that if
dropped into a MVP M at random
presumably drawn from the realistic MVPs
then on expectation overall trajectories
that it will encounter it'll collect
higher reward and what's important here
is that when we look at these
trajectories K from 1 through capital K
there will be a first
trajectory that's your first attempt in
the new world during that attempt the
agent can change its policy this is not
a fixed policy we're trying to find this
is a generic agent think of this like a
human human gets dropped in an
environment gets an attempt see what
happens while they're acting in that
environment they can change what they do
they can change their strategy then the
episode is over second episode happens
in that environment they've probably
updated their strategy quite a lot
already are doing a lot better on the
second trial and can do this capital K
times and so we're hoping for is somehow
discovering this agent that went dropped
in a new environment in capital K
episodes in that new environment
collects a lot of reward even though
it's never seen that environment before
Victoria Lee let's say capital K was
equal to 2 you would be dropped in a
first environment as an agent you get an
episode a second episode you see how
much reward you got total that's this
summation over here if you've got a lot
then you did well if you didn't get a
lot that means you had a bad agent and
you need to improve your agent and then
you go again another MVP and this keeps
repeating so that's the problem we're
trying to solve to do this in a machine
learning framework we're going to have a
sample set a training set of MVPs and
we're going to look at instead of the
expectation over all pop ups we have a
set of training MVPs with some over
those and in those look at an
expectation how well does our agent do
all right now we need to decide what
this agent looks like we somehow need to
learn that agent learn what it is and
the hope of course is that even all the
training MVPs it does really well then
when dropped in a new environment is
going to do really well too for now
here's what we'll do we'll take for the
RL agent a recurrent neural net why a
recurrent neural neural net is a generic
computer architecture if you wish it
learned some computer program then in
principle if your RN n is big enough it
can learn that computer program for want
to learn something else let's say finite
state machine you can learn that too and
so forth so it's a generic computational
architecture but the beauty here is that
it's differentiable as you change the
parameters a little bit they'll change a
little bit the program that this agent
is running and so you can try to
end-to-end optimize the
program that's encoded inside this agent
what does that mean practically if you
change the weights in the recurrent
neural net you're effectively changing
the reinforcement learning algorithm
that the agent is running internally if
you're changing the activation of the
hidden units then then what that means
is that you change the current policy
the agent is executing so you can think
of it as still there being a
reinforcement learning algorithm and a
policy but it's all living together in
the recurrent neural net with the
algorithm encoded in the weights the
policy encoded in the activations and
this objective can be optimized with a
standard slow reinforcement learning
algorithm that will run ahead of time
and after you run that long enough
hopefully we find a good agent that's a
faster relation that we can use in the
future
okay so the result of this is now going
to be a policy the result of this is
going to be an agent an agent you can
drop in new environments and will adapt
quickly to whatever is needed in those
new environments this is in contrast to
standard reinforcement learning where if
we optimize something we end up with a
policy and that policy is tailored to
the specific environment you were
trained in very very different doesn't
generalize to new environments okay to
make is explicit in standard
reinforcement learning
you would have an environment a policy
that interacts with the environment by
taking actions getting observations and
their rewards that gets fed back to the
algorithm that we design and we hope we
design an algorithm that knows how to
update that policy such that this
becomes a good policy okay and the
result is that hopefully we have a
policy that mastered this one
environment in RL squared we have a
recurrent neural net which is our agent
make it explicit we want this to become
our fast RL agent takes actions gets
observations also gets the T rewards
which means it can do whatever
enforcement learning algorithm usually
does which is based on observation every
ward
change its policy and change how we
would act and repeat it also is faced
with many environments because she
wanted to learn to adapt to new
environments and that's not possibly for
only training one environment okay so
the result here is the mastery of a
learning process the result is an agent
that's good at learning when faced with
new situations
okay so we evaluated in a standard be
enforceable learning setting bandits in
bandits each bandit is an action you can
choose each bandit has its own
distribution over payouts an episode is
one pool so one choice I've abandoned
and then the goal is that you kind of
pull the different bandits and over time
you figure out which one has the most
payoff or you keep pulling that one
there are provably optimal algorithms
for this so let's see if we can train
our faster El agent to become as good as
a provably optimal algorithm for solving
this problem if we can do that that's a
good proof of concept that indeed we're
learning something worthwhile turns out
we can so give this in the CSR is from
asymptotically optimal highlighted on
the left are l-squared learners an agent
that achieves almost as much reward
sometimes more for the bigger problems a
little less bill than the gettest
indices so we're able to learn a faster
algorithm our algorithm that when faced
with a new bandit problem can solve it
as effectively as the Bayes optimal
approach here's what the learning curves
look like why they're still learning
curves these are learning curves at meta
training time what's meta training it's
where you're training that's fast RL
agent that faster our agent is faced
with a bunch of bandits gets to try and
then you'll update that recurrent neural
net that encodes your agent so they
become a better agent next time faced
with a noob set of bandits he can learn
more quickly
she actually learns quite pokey it
doesn't need a whole lot of data to
master something as good as kittens
indices for the bigger problems when
there is in this case 500 episodes and
then large number of bandits then it's a
little below gives us indices which are
up here but it's quite remarkable how
close it gets and this is compared with
what the best effort the result of the
best effort a lot of humans have put
into solving these kind of problems and
this is just relying on compute rather
than human ingenuity to discover this
algorithm we will see if algorithm other
small random MVPs where there are
optimal algorithms available again it's
quite close to the optimal algorithms in
doing well when dropped in a new
environment here are the learning curves
learns quite quickly when it's a small
environment
small number of episodes a little slower
when it's a long number of episodes then
here's another one
we'll look at half cheetah standard
locomotion environment in mojo Co what
is the set of tasks it's the reward can
change the cheetah is always the same
cheetah but a different environment
means that the target velocity of the
cheetah is different sometimes it's
forward sometimes backward
sometimes it's standing still and it
could be florid backward at different
speeds so as you act in that environment
you'll experience reward and hopefully
from that you understand how fast you
should be moving and immediately adapt
to that turns out it adapts pretty much
from the second or third time step to
that reward signal and immediately you
don't even see any exploratory behavior
here it starts and it right away
achieves maximum reward so it learns
something internal that can generate all
possible locomotion behaviors for this
cheetah and immediately zooms in on the
one that's desired based on the reward
it's getting right now do the same thing
for ant oh that didn't play running
forward-backward different speeds again
what we see here is effectively that
this agent has mastered something that
encodes a prior over all possible
locomotion behaviors and then when it's
dropped in a new situation where the
reward is associated with one specific
locomotion behavior it immediately adopt
adapts to the reward that's present
there and masters that task so this is
very fast Tyrell may be even faster than
humans at this point here is a very
challenging task so we're gonna look at
here is an agent in a maze and the agent
does not have access to the map that's
just for our purposes agent just seized
first-person view
monocular input current image raw pixels
output is two degrees to the left to the
reis to the right or straight and a
couple centimeters of forward progress
that's it
okay so very low level vision - very low
level action controls what'll happen is
agent gets drop in the maze what you
would hope for is when it gets dropped a
good agent would do a good is you would
understand to move along hallways check
where the target might be and then when
it sees a target run right to the target
collect a reward then when dropped in
the same maze
it should remember what that maze was
like and immediately take the shortest
path to the target if it does that that
would be a well-trained agent that's
what we'd hope for but keep in mind
we're not including any of that that's
just we're hoping would be the resulting
behavior so when it's still meta
learning so at the very beginning of
meta learning this is what happens it's
just bumping into the wall
it doesn't really understand anything
yet it's just bumping into the wall and
maybe randomly making some progress
every now and then this is one episode
of course that meant a training time
we'll have many many many of those
episodes to train that fast RL agent
once it's trained the fast RL agent we
deployed in the new maze on the right
keep in mind this faster L agent will
not have access to the map that's just
for us to understand what's happening
this is what the faster L agencies right
now when it starts and all it gets his
raw pixels streaming into it so she
acquired a behavior to quickly run
around the maze look around corners not
waste any energy going into dead ends
and go to the goal and then second time
dropped it goes right to the goal so
that's exactly what we hope for here's
some tablet results showcasing that on
the 2nd 3rd 4th 5th drop and average
it'll do better than on the first drop
busier is saying if you dropped in a
small maze 91.7 percent of the time on
the second drop in that maze you're
doing better than on the first drop
meaning you learn something from that
first job that you leverage in your
second drop to get to success more
quickly it doesn't always succeed it's
actually not an easy learning problem to
learn the fast RL agent if the faster L
agent learns well like the top curves
then you get the kind of behavior that I
showed you but about half the time
actually the meta learning process does
not get enough signal and does not learn
that faster l agent so there's still a
lot of research to be done to make sure
that the signal gets there especially if
we go to more difficult environments is
already pretty difficult but it's not as
hard as the real world not anywhere
close so still a lot of work to be done
to to get this to learn more effectively
so we've seen so far is that we've
formulated learning to learn as
follows we want to find an agent that
does well been dropped in some random
environment and gets to try a few times
in that environment we put in a generic
agent the recurrent neural net and just
found that that can encode something
that learns a good policy very quickly
can put more structure into this here is
one idea what if we do the following we
say the first two episodes we execute
the policy PI theta and then after that
we execute a updated policy PI theta
plus Delta Theta where delta theta is an
update we apply to the parameter vector
theta so this is a very classical
approach to solving this kind of problem
reinforcement learning would do this you
start with a policy then you update the
policy and you go again okay so and then
how you obtain the update may be a
policy gradient it received TRP OPP or
any of the standard D power logarithms
could provide an update for you and then
you hope that this thing does well well
for this to do well you need to start at
a really good theta and so the hope that
we have here in this approach is that
maybe there is some kind of
initialization we can work from from
where it's fast to adapt to new tasks ok
so there's actually we can look at this
in a much more general setting we'll
call this model agnostic meta learning
we assume here or we hope for a set of
pre train parameters theta such that
after an update let's say graded update
based on the training loss that we get a
new theta and that that new Tay that
actually does well on the task so we
don't care about how well the original
theta does we care about how after one
update the resulting theta how well that
one fares on the tasks that we're trying
to solve well I might be hope this might
work look for example at computer vision
people trained on image net and then
care about something else and all they
do is fine-tune on that new data set and
often that actually works what that
means is that some how about training on
image that you're able to discover a
neural net that is such that with a
small number of grained updates you will
do well in a new computer vision task
well maybe the same could be true for
reinforcement learning maybe you can
find a neural net that is such that just
with a small number of grained updates
Daniel net becomes good at a new
tasks okay so here's what it looks like
pictorially we would have many many
tasks in this case three on the slide
then each would have optimal policy
parameter settings and we'd hope that we
find some theta that lies between them
and such that a grand update would right
away take us to whichever one we're
faced with right now and solve the
problem
Russia tried us on the same kind of
environments that we looked at earlier
with our l squared here what you see is
you would see you would do a bunch of
rollouts
under the pi theta the initial pi theta
then do a gradient step and what you see
that after one graded step it actually
acquires a good policy if you just pre
trained it doesn't work too well but if
you do it is cleverly with Mull agnostic
meta learning you see that after one
grade in step it immediately requires a
good behavior turns out that one
gradient step is a little slower than
with our l squared we'll do our l
squared even updates within the episode
and actually after one time step already
doesn't update and acquires the behavior
so in this particular setting our l
squared tends to learn a little faster
but I think there's a lot of interesting
results to be had by exploiting this
structure and we'll see some a little
later where it's not obvious how you
would do our L square then this actually
will work very well here top is if you
do just random initialization and then
policy gradients from there bottom is if
you pre train to find as good theta
using mammal and then from there to
grading step so you adapt very quickly
to the new task okay so we've seen so
far as a setting of learning to
reinforcement learn where you get
dropped in an MVP in another MVP and so
forth I'm gonna get dropped in MVP is
supposed to do really well after a few
after a small amount of experience in
our MVP we've used the black box agent
in our l squared and we've used model
agnostic meta learning which is depicted
here we can actually think about the
same ideas in the context of imitation
learning it's another way to acquire
skills for a robot imitation learners
have many many successes but classical
successes tend to operate in this regime
you collect many demonstrations for your
tasks your mutation your room from that
it's a policy which can do something
well in that environment when you train
for assembling a chair you start from
scratch next time you train for
assembling table and so forth there's no
sharing you but is shared is the
algorithm for learning but then the data
is not shared in any way and you need to
learn from scratch for every new setting
we'd like to do instead is learn a one
shot imitator what is this this is a
network that is trained through meta
learning such that when it sees one
example of a task it understands what
the task isn't can already solve it much
like humans once you've seen somebody
demonstrate something once you
understand what they're trying to do why
because you've seen them do many things
before and so you have a lot of context
that context is where you get to see at
meta training time many demonstrations
of many tasks from that you extract the
essence of tasks inside this neural net
and then when you get one demonstration
of a new task you understand what the
essence is of that new task and are able
to do it so then the way you get a
policy is by combining this one-shot
imitator with a single demonstration and
together they'll make up the policy you
give a new demonstration together
there'll be a new policy how can you
train this here's one way to train this
you have a video of a demonstration you
have a current frame of another
demonstration for the same task so a
training time you need paired executions
in some sense like you need two
demonstrations of the same task so if
you wanted to do assembling a chair you
need twice the demonstration of
assembling a chair now just once so
similar chair here or stack some blocks
here stack some blocks here then what
you do here is this neural net takes in
this entire video processes it takes in
the current frame here does some more
processing and uploads it predicts what
motor torques would be applied over here
if you can do that then it means that in
the future from one demonstration it can
predict what to do in a new situation
that's related to that one demonstration
so we can train this end to end and this
what's sitting in the red box here is
our one shot imitator it's a lot of
detail that goes into the architecture
to make this work but is the high-level
picture we tested it for block stacking
a task instance here is a specific final
come
operation of the blocks maybe Dion to
see on to be onto a that would be one
task instance what you'll see on the
left here is a demonstration on the
right the policy that is executed by the
one shot imitator so you'll see that
it's achieving a certain block stacking
configuration here and from that one
demonstration this robot is achieving
the same block stacking configuration
it's doing this at the level of
controlling the motion of the gripper so
it's it's not just commanding the robot
oh now pick up block a and place it
there it's actually controlling at the
level of the gripper motions velocity
and angular rate of the gripper based on
what it's seeing on the left well you
see at the bottom here is which blocks
it's paying attention to currently I and
J and we see here is which time slices
it's paying attention to in the
demonstration to get demonstrations we
actually scripted demonstrations because
we need a lot of demonstrations to train
this one shot imitator and so what you
see here is our scripted demonstrator
and how well it does on training tasks
and on testosterone are not perfect now
let's look at how well our one-shot
imitator does almost as well as our
scripted demonstrator so it's learned
everything that that demonstrator is
able to get we have a few variants here
where you might look only at keyframes
or only look at the end frame and see if
that's more effective to learn from than
looking at the entire video turns out
it's more effective when you take in the
entire video it's a lot more data so you
might be worried about signal-to-noise
but somehow it gets the right thing out
of that and learns more here's not a way
to go about this quote but what I showed
you for now was the black box approach
very much like RL squared is just this
black box RNN that supposed to acquire a
RL agent on the inside what I showed you
here was a black box neural net becoming
a one-shot imitator now we'll look at
the mammal counterpart so the mammal
counterpart is where you say grande
descent is a great learning procedure
let's keep that intact and let it not
don't force it to relearn that part just
hard code the brain the cent part and
see what we can do from there
a typical way to do imitation learning
is to say well we see an action then our
newell man from the observations should
output something close to that action
and that's our loss predicting actions
as closely as possible over a wide range
of tasks and time slices into the tasks
meta learning loss would be something
like this then what we want this we want
it to be the case that we train a neural
net with parameter vectors theta such
that if we see one demonstration and
then we run behavioral cloning which is
we optimize against this loss the
standard imitation learning loss we do
one gradient step on that loss that will
give us an updated parameter vector
theta theta minus alpha grab grab theta
and so forth this updated parameter
vector is the one that we want to
perform well and we're going to measure
the loss of that updated parameter
vector and if that's good that means we
can learn from one gradient update from
one demonstration here's the neural net
architecture going from raw pixels all
the way to torques when I showed you
earlier with the block stacking we went
from state due to kinematic control here
we go all the way from pixels to robot
action so it's this includes all the
visual processing being learned may be
more possible here thanks to putting a
little more structure into what's going
underneath at training time meta
training time the robot I gets to play
with these objects gets to get
demonstrations on these objects and
trains this parameter vector theta that
should make it ready to imitate quickly
a testing time it'll see new objects so
it's supposed to be able to learn to
manipulate a new object it's never seen
before from one demonstration here is
the single demo it's a pushing task here
is two objects on the table they decided
to push the chess piece that was the
demo so we hope for is that if faced
with a situation with those two objects
on the table no matter where they are
the robot would understand that the
chess piece has to be pushed to the red
circle let's see if that happens
indeed just from that one demonstration
its acquired a policy that goes all the
way from raw pixels to motor commands to
succeed at this task and this is
actually what the robot sees so these
are the pixels the robot is processing
and just get that one demonstration to
learn this
here's another one here the demonstrator
pushes the big pink object and indeed
the learned policy does the same thing
here's another one demonstrator pushes
the big yellow cube and the learned
policy is again from just one update
have acquired the ability mostly to push
that one to the target here's something
we did with a real robot so here we have
again we're now going to place objects
onto targets on the left are the objects
seen at training time when the
initialization of the neural net
parameter vector theta is learned and
then at this time we'll show things with
the other objects so this is test time
one demo placing that Apple into the
circular bowl and then imitation and we
see the robots for you here's the robot
learned this from raw pixels from one
demonstration that this is the thing to
do here's another one it goes to the red
cube and it also learned to do that okay
so at this point we're done with kind of
the main concepts I wanted to get across
in this talk I want to give you some
hints out future directions related to
this teach you a few things that I think
I can teach pretty quickly there's some
good insights for RL and then point out
a few future directions inside you can
reformulate what we just saw to learn
from video only oh okay
so these will be just looked at mammal
permutation learning will optimize this
loss which is how good your parameter
vector theta is after one update nobody
says that the update you do on the
inside has to use the same loss as what
you use on the outside so we'll do is
wash you put a different loss on the
inside this will be instead of
predicting the action that was taken by
the demonstrator which might not be
available in practice if it's a human
doing it you're not going to know their
motor commands you just mailed to watch
them so instead in the inside here will
predict the next frame that we'll see
we'll do a video prediction effectively
we have a loss in what we'll see and
that loss is living inside a neural net
that is shared with the neural net that
will do control and somehow we're going
to initialize this parameter vector
theta through meta training that one
update on video prediction is enough to
result in a neural net that is good at
controlling this robot
since we're neural net with two heads
one related to predicting video the
other one related to the torque commands
that need to be output actually so these
results are in progress and very
promising but I don't have any any
videos to show just yet there's another
thing that we can vary here learning to
explore exploration is a big problem in
reinforcement learning how do you
explore people can with a lot of ideas
exploration bonuses pseudo counts maybe
something about variational information
maximization did you learn something new
about the dynamics of the environment
and so forth very complicated heuristics
well if we have more and more compute
why don't we let the computer figure out
how exploration should happen we can
look at the same setting as we looked at
before
but the basic idea now becomes we don't
care how much reward was collected in
the first episode only how much was
collected in the second episode that
means the first episode is free that
means the first episode should be
exploratory episode where this RL agent
has learned that in Episode one I do a
lot of exploration very smart so I learn
as much as possible about the
environment because then in Episode two
I can collect a lot of reward so just a
very small tweak to everything we've
already seen is just a tiny change in
the objective it's all it all it takes
and you can start learning to explore so
here's the standard objective and here's
they learn to explore objective which
only considers the reward from a certain
time onwards that doesn't mean the agent
doesn't have access to rewards
accumulated early on of course the agent
sees the rewards that are happening but
they don't contribute to the reward
that's being optimized for they're just
for learning about the environment
here's some example environments in
which we're testing this so we call it
crazy world in crazy world there's nine
tile types you could have gold in a tile
that's great that's what you want to
collect there could be ice which means
you slip over it could be death which is
where the episode ends it could be a
wall you just can't go there could be a
lock
which means you can go through but if
you have a key then you can so you can
go collect a key
there are teleporter squares there are
energy squares this here is how much
energy you got lift at the bottom you're
out of energy you're also in death mode
and then normal squares there are also
where nothing special happens we swap
the layouts we swap out color palettes
dynamics so moving up might not mean
moving up and so forth and those are
some sample instances and what you'd
hope for in this kind of environment is
that an agent would be dropped in it try
out what the meaning is of the different
squares maybe also understand that
usually there is a lot of normal squares
so whatever is the most off is probably
the normal squares and then understand
from what it's seen in the past how to
very quickly do the right thing in a new
situation by some quick exploration
first so here's an agent in action
exploring in this world this agent is
running around the red dot and it's
quickly trying out all the different
things that are available in this
environment and from just a few episodes
of exploration able to figure out how
this environment works and start
collecting a lot more reward in future
episodes here's a game we've been
testing this on there's work in progress
the way we test this here is we look at
different levels in the game so most
video games have multiple levels we
train on some levels from them it builds
up a prior of how the system works and
then we test it in a level it's never
played before and see how fast it can
acquire a good policy what you're
watching here is something where we're
watching one game in action in parallel
there's 31 games running and so when you
when you think it only had one
experience its have 32 but this is still
an extremely low amount of experience to
learn to play a new level in a game that
it's it's never seen this game level
before
all right it's not a thing to think
about when I talked about meta learning
here learning to learn have all always
been talking to context of action be it
reinforcement learning or imitation
learning it's not do some work on that
in the context of optimization just as
well including from tutoring Malik here
and at Berkeley some work on this in the
context of genera t'v models how to
learn to generate something similar to
what you recently saw and also in a
comics of classification once you have
classified a certain number of
categories if somebody presents you with
a new category how fast can you
understand that category have a common
example would be if you've never seen a
Segway before you see one of them now
you know what a Segway is you don't need
to see a hundred of them because you
already have a notion of what a category
is you can quickly Zone in on new
categories so what it would be is at
test time
you get to see five images with five
labels and then you get test images and
you're supposed to classify those you've
never seen any of those five categories
before you supposed to acquire those new
categories on the fly instantly that's
very similar to needing to learn to act
in a new environment instantly just
slightly simpler so training time you do
something similar you would face
yourself with what is a disjoint set of
classes and pretend going through this
exercise train end to end differentiate
through the whole process to optimize
for this and becoming good at
recognizing things you've never seen
before
turns out that the model agnostic meta
learning that I've presented to you
which we thought of in the context of
reinforcement learning outperforms the
existing meta learning methods for
classification even though it's unlike
the existing methods not specific to
classification at all it's a much more
general idea than what's being used in
any of these other methods that are much
more attuned to trying to solve
classification both on Omniglot and on
mini imagenet which are the standard
benchmarks it's not a thing we're
looking at learning to grade you might
wonder when are we gonna run in though I
get a new class and a new class again
new class again why don't we just train
ahead of time for all the classes we
care about well you can pose a new exam
new questions new student answers so we
can do the same thing there you can for
example categorize questions as simple
multiple-choice type questions meta
train on a lot of them from the past and
then quickly learn to grade from a few
examples how degree
new ones same for complex multiple
choice for fill-in-the-blank for more
complex fill-in-the-blank for diagrams
and for a short answer and so this is
all a work in progress that hopefully
will be released in the next couple of
months okay one thing you also might
want to consider is the architecture you
use underneath what I've described you
when I say a big RN n 4r l squared what
what should that structure be and
although RNs are the same and if I
decide not to use an Orion recently for
sequence to sequence
people have noticed that wavenet type
architectures dilated convolutions work
really really well so we can actually
swap in a wave net for the RN n and run
our l squared again the exact same way
just with a wave nip rather than RN n if
we do them this is what it would look
like we actually get better results on
Bandits it's a work in progress on some
other environments for image recognition
where should get better results than any
of the things I've shown in any of the
state of the art out there there's a new
state of the art doing wavenet like
processing on sequences of images and
labels to get best classification
performs on new images in new categories
that you hadn't seen at meta training
time state of your item Omniglot state
of the art on mini image net now she
quite a bit ahead of even just state of
our results from less than a year ago
okay here are two quick insights I want
to share with you often in reinforcement
learning there is not a lot of signal
what do you mean with signal if there's
zero reward you don't know what to do
you can't learn anything if you need to
pick up an object place it somewhere and
you only get reward when it's there then
it as long as you don't do the job
you're not getting reward here's an idea
to get around this it's called Hans had
experience replay it's probably easiest
to think about in the context of just
you must smite two let's say you wanted
to go eat pizza and you end up in the
ice cream place
oh yeah ate ice cream instead okay you
could say well zero reward this is
supposed to eat pizza and you didn't eat
pizza see reward you can learn nothing
or you could say if just I had wanted to
eat ice cream I would have gotten really
high reward so what what how can I use
the fact that if just I had one of the
ice cream I would have gotten high
reward and turn that into a learning
signal for learning anything else that
we might want to do in the
okay so for that we need universal cue
functions that take in state goal and
action not just state an action which is
the typical thing once use a universal
cue function your replay buffer which
traditionally consists of state action
reward and state will also include the
goal that you had standard experience
replay we'll look at this bellman
equation don't worry if you're not
familiar with this if you are familiar
with this this will be very clear you're
looking at the bellman error trying to
minimize it well with hindsight
experience replay you will swap in goals
that you actually experienced and
corresponding rewards such that you get
nonzero rewards signal and can learn a
lot more quickly very simple thing to do
very simple modification to standard eqm
but gives you a lot more scenic and
learn things a lot more quickly here's
an example of a robot learning some
skills so we'll see here on the left is
standard VT PG which uses cue lighting
underneath and then sorry on the left is
our approach and on the right is the
approach without the hindsight
experience replay and so we'll see is
that on the left it effectively learns
in a very small number of attempts
whereas on the right it doesn't learn
anything because it never gets a good
amount of signal to learn from one of
the right only learns when it achieves
something otherwise it gets zero signal
so it needs to effectively solve the
task before we can learn about solving
the task whereas the one on the Left can
do anything and learn from that and then
generalize to new goals this works for a
wide range of tasks here's not a quick
insight standard Atari setting you'd
output you have an output in your neural
net for each possible actions each
partial action if your actions are
continuous you can't do that
so you want to take it as an input but
then how do you generate actions max
overall actions is not clear you can
actually set up an imprint network pi5
that samples from the exponentiate at Q
values if you do that you can actually
do Q learning on continuous control
problems in very effective ways there's
the Baxter the soya robot over in sitar
Jedi one of circle Evans robots learning
in about two hours to learn to stack a
block with model free reinforcement
learning
okay so where is this headed I think one
big thing is that we'll since we want to
rely on more compute we'll need to rely
on simulation I'm gonna go fast here the
key idea I want to get across here is
that it turns out that your simulator
does not need to be that realistic to be
helpful it's enough to have a lot of
randomization in your simulator
if it randomizes enough it'll have the
right signal for you you can learn
things in fact you can learn from these
images none of those look realistic but
what you learn from these images is good
enough when faced with a real image to
still do the right thing and pick out
the object that you want to pick out
between a neural net base and only
simulated images deployed on the real
robot who's that sees real images and
actually knows to find the block and go
grab it and so here is enough
randomization it's enough to make the
succeed you don't need realism in fact
you don't even need pre-training so you
might think pre-training imagenet and
then do this the red curve is pre turned
on image it it helps you in the
beginning but ultimately it doesn't help
you so you can ask you this cleanly
without any pre training the two core
ingredients of AI are planning and
learning if you take let's say one idiot
here two core components of the class
are planning and learning last 10 years
all we've seen is like major booms in
learning and not that much attention to
planning it's likely in the near future
we'll see a lot of combinations of
planning and learning do more
interesting things that you can do with
just learning we worked in valuation
networks here working on something very
related that enables vision based
navigation of a car this is a car
navigating in quarry hole based on raw
pixels executing motor commands and this
is using a combination of planning and
learning to be able to do this here's
another example so Kabam a standard game
you're supposed to push blocks while
little circles to their target locations
we need to be careful about your
strategy because you could lock in one
of these purple dots somewhere and you
can could not get behind them anymore
and be stuck and it turns out by
combining classical a-star search
planning with learning a heuristic for
this it's possible to solve problems
that classical approaches cannot solve
and just learning cannot solve either
here's not a way do you combine planning
and learning it turns out that it's
possible this is quite surprising to me
to fake in a video see the first frame
of another situation that's similar and
fully predict what the video will look
like if you were to watch from here once
you can predict that video you can then
try to control your robot to make that
video happen so your plan by imagining
the future visually seeing the future
and then you try to make it happen what
you see here on the left is the video
the robotic gets to see the middle is a
still frame is the initial frame and
then the right column every time is what
the robot imagine robots imagination is
of what should happen to achieve what
was happening on the left you can see
the thing on the right is quite a
different situation from one on the left
it's related but different it actually
extrapolate from the first frame forward
and understand what is supposed to
happen the last thing I think will be
very important in your future is
continual learning what do I mean with
that and what is it why do we need it
the current machine learning paradigm
tends to be step one learn step two
deploy
why is machine learning useful in this
paradigm because it's too difficult to
hand-feed a solution to the problem so
you want to learn the solution and then
deploy but all the learning happens
ahead of time it's learning then deploy
strict separation but for real
deployments the world is going to be
continually changing and your system
needs to adapt to those changes at all
times and so what we need is continual
learning rather than one time learning
and deployment now she phrased it in a
very similar way to what we were looking
at with mammal what we're doing now is
we're going to try to find an agent
parameterize by theta mammal as well as
RL squared agent parameterize by theta
that is good when faced with a sequence
of environment so the age in that
training town will be continually faced
with changing environments and the hope
is that what this agent would do what
internalizes it sees the current
environment it sees the next environment
and it can now anticipate what the next
next
we'll be like and be better at the next
next environment because it understands
how the world evolves over time and so
that would be better than an agent that
always reacts after the fact oh the
world changed I need to change my
behavior with this kind of training you
hope you can anticipate future changes
we looked at these in the context of
robot sumo so we put two robots on a mat
and they have to push the other one off
and you push the other one off you win
the game or you flip the other one over
you win the game - here is a ant which
was trained with meta learning which
means it has learned to anticipate
changes in the world what are the
changes in the world here is that both
of these agents are learning and so
because they're both learning if you can
see how the other one is learning and
then anticipate what their behavior will
be in the future you can beat them in
the future and that's exactly what's
happening here the red one is using that
strategy initially it's weaker it starts
out weaker but thanks to its
anticipatory capabilities of how the
other one is changing it's able to be
the other one is using regular
reinforcement learning whereas the red
one is using meta learning actually you
can take this to the extreme if you put
these creatures in a world where some of
them are meta learners some our regular
reinforcement learners somewhere and
somewhere bugs somewhere spiders and you
let them play each other let's say for a
hundred episodes you see who won the
most where they're both constantly
evolving and so the one that evolves
better will win more towards the end
then after the hundred episodes you
retain the one that won and you repeat
you get a population you get a
publishing dynamics game and here's how
that plays out so what we hope for
if meta learning is helpful that meta
learning would result in more stable
creatures that can do well in these
one-on-one fights and will survive and
be in the next generation what you see
here is Dom initially it's uniform
distribution but then over time red
starts to dominate red is effectively
our L squared which we saw early on
trained in the context of where the
environment is evolving over time
and so our l-squared training that
context outperforms regular learners or
stationary policies who don't know how
to adapt to the world and so this I mean
the great extent is inspired by if we
think about human intelligence maybe the
reason we have to be so smart is not so
much because the world is so complex but
because the people around us are so
smart
we need to essentially always be better
than the person other person around us
understand are they gonna take our food
from us are they gonna be our friends
not our friends and so a lot of the
complication of what we learn and why
we're smart is not because of just
finding food but because the population
that we live in and we're constantly
competing and that may be by sending up
these competitive environments we can
actually in a very simple way create
some very strong intelligences that are
very difficult to create otherwise
because otherwise it would be no driver
to create intelligence but due to
competition will always be a driver to
become smarter because wherever smarter
will do better and said that way you
drive up the intelligence okay these are
the current and future directions here
are the topics that covered thank you
very much
I also like to open it up for questions
so please raise your hand if you have a
question and I will run the mic to you
while the shuffling happens let me start
out with a question so you mentioned in
the big role that simulation is playing
here what are what are the domains
you've shown us some domains where
simulation is extremely successful what
are the domains where simulation is
really hard or not applicable that you
think are the toughest nuts to crack so
where simulation worked really really
well was perception actually so the
example I show was a little fast but was
pure perception of his doom no the the
robot grabbing the blocks so there there
was training in unrealistic renderings
and was good enough to learn a vision
system that understands the essence that
is the geometry it can abstract away
lighting conditions occluders color
texture and just focus on geometry where
stimulation has for now still more
trouble is modeling detailed physical
dynamics and maybe there will be some
progress in near future how to randomize
that but it's a little less obvious how
to randomize in a way that captures what
we need to capture when let's say in
simulation you work with a rope how do
you make sure we work with a rope in the
real world somehow what you learn in
simulation carries over and some of
these dynamical systems are seemingly
quite complex to learn something in
simulation and actually have it work in
the real world I think where this
simulator eventually gets most
interesting is this thing I showed at
the end which is the notion where
actually you can build a very simple
simulator you just need to limit the
resources that you make available such
that there is a natural competition
between the agents and so what that
means is that you don't need to design a
complex environment let's say you want
to build intelligence by let's say
building a beautiful self-driving car
simulator as realistic as the real world
that's an enormous amount of work
and that's one way to get to
self-driving cars maybe but another way
to potentially get there and maybe not
the fastest way because it would be
slightly roundabout way but you would
get a lot more if you set up
environments where agents have to
compete because then they have to
continuously outsmart the other and you
constantly get signal because you
usually they're at the same level so
they kind of win half half if one of
them does something a little better
they'll get signal to become smarter and
this might be able to repeat it requires
a lot of compute admittedly so well
probably need more compute than we have
today but maybe not more than we have a
year from now other questions for meta
learning how can we tell apart actual
learning to learn to do the task versus
just learning to do the task it seems
like you need to a notion of like task
distance or generalization to tell these
apart yeah so the question here is in
some sense how well would a zero shot
generalization perform where it can you
can you do a new task from the zero that
attempt
in which case is regular generalization
versus you need one example and acquire
a lot of information that one example to
then do the task we did I didn't have
the results here for the mazes we did an
explicit test for that by looking at
small mazes only and see if it can learn
to solve bigger mazes so could have
never acquired the notion of bigger maze
in the small mazes but I think there's a
lot more to be thought through there in
how to and I think that's a big actually
one of the big things that's that's
challenging in in meta learning is how
to define a wide range of tasks and so
sometimes it's it's funny quizzes but
let's say you look at computer vision
right how do you define a wide range of
tasks turns out like there was image net
but then there's still in that many
categories it's still limited what you'd
have and so turns out that grading might
be a good example because there's so
many categories but in general it might
be that the only way to get a lot of
tasks is actually by having opponents
and trying to exploit the dynamics of
opponents introducing new tasks in your
world and so you've faced with a new
opponent adapting to a new opponent
might require learning something new
that you didn't learn before
hi I wanted to ask about your quickly
teachable insights research I know that
you have worked on and other
reinforcement learning researchers have
worked on multi-part training systems
where either you have multiple agents or
you have multiple tasks have you
extended these research to more
complicated training regimes where you
have you know auxilary tasks or you have
multiple agents having to be trained to
work together sure yeah so a few few
directions there in terms of auxilary
tasks there's some work I should add
Evan did for a while here front on
introducing orderly losses related to
dynamics prediction and thatwe recover a
neural net that's at the same time next
state or next frame predictor as well as
a action predictor and by doing so you
might learn a representation more
quickly deepmind's done some work there
unreal it's a paper that comes to mind
where they would predict where pixels
would be at the next time or if it would
achieve a certain pixel configuration
and that would help learn the thing it
actually cares about more quickly the
comics that you see here one way it's
happening is that as these in the last
part if the ends spiders and bugs play
against each other in the robot sumo
game they actually start from the same
neural net or many of them start from
the same neural net they play each other
and by playing each other they actually
are at this they're playing an opponent
that's at the same level as themselves
and because you're playing an opponent
that's roughly at the same level as
yourself you but win about how the time
lose about half the time and you
actually get a lot of signal so you get
an automatic curriculum where you
automatically face harder and harder and
harder tasks as you go along so also
some work we've done where we've
explicitly designed curriculums where
you would change the task itself and say
this task is easier this is harder
harder harder and that way is another
way to get signal into the whole process
rotate is still an open problem to do
that fully automatically definitely
humans are pretty good at designing
curriculums designing simple problems in
a medium medium problems hard problems
and then have the robot first Traynham
easy then medium then hard and learn
things gradually rather than right away
going into the hard problems overall I
would say exhilarate losses haven't
helped as much in RL as you might hope
for and I mean maybe it's not too
surprising because same in computer
vision straight up training on imaging
that works quite well and doing
unsupervised learning doesn't help a
whole lot in those context eaters so
might just be that we don't have the
right machinery to really give it a
major boost through unsupervised
learning
one more question it seems to me that
most of the all of the meta learning
tasks so especially the bandits all of
those sub 11 new pas are very
structurally similar and so basically
what the agent is sort of learning to do
is interpolate between the different
environments it can find itself in it's
been here before it's been here you put
it here it's gonna say oh I'm in between
those two period previous experiences I
know what to do mm-hmm people can do
something that's more like extrapolation
where you saw the simple problem and
then you can actually add a new
dimension or really like make it
structurally much more complex but it
has some core similarity and they still
are able to generalize in that case
mm-hmm
do you think do you have any evidence
that this kind of algorithm can work and
if not what would it need to be able to
do that kind of extrapolation so the
last example the continual learning
tries to go in that direction and the
way it tries to go there is by the agent
through a lifetime is faced with an
easier problem at first then harder
harder harder and it's supposed to learn
to anticipate how things get more
complicated and by anticipating how
things get more complicated it can do
better as shown from the population
dynamics time agent that don't
anticipate so it's not exactly what
you're referring to I think what you're
referring to is a really good problem to
try to tackle but it's going a little
bit in that direction and the way was
going there is by setting up a loss
function at training time that
specifically
tries to do well as you go through a
progression of difficulties difficulties
here defined by the other agent also
becoming better and so you can imagine
doing something similar where you have a
progression of difficulties of tasks and
as you train it always has to go through
a progression and if you can define a
wide-enough family of progressions that
see tasks become more difficult than
when faced with yet another task then
maybe just faced with the first version
of that task it can quickly also acquire
the ability to solve the more
complicated versions remains to be seen
though and maybe it needs a completely
different approach I think it's a good
problem to think about at this point I
think we should conclude the public you
a maybe some of you will have individual
questions let's thank Peter again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>