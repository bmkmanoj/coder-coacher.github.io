<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Peter Railton - Machine Morality: Building or Learning? | Coder Coacher - Coaching Coders</title><meta content="Prof. Peter Railton - Machine Morality: Building or Learning? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Peter Railton - Machine Morality: Building or Learning?</b></h2><h5 class="post__date">2017-09-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SsPFgXeaeLI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so la lastly we have Professor Peter
Elton he is the Gregory s Kafka
distinguished University professor and
John Stevenson parent professor of
philosophy at the University of Michigan
whereas he has taught since 1979 he's
the author of the book facts norms and
values and the collection of which is a
collection of his major papers in ethics
and a co-editor with Steve dar Wall and
Alan dipper of moral discourse and
practice some philosophical approaches
and today he's going to talk about
machine morality building or learning
thank you you're going to see is that
moral philosophers don't have very
interesting-looking slides and so I'll
have to ask you to forgive me for that
and I know it's late in the day and
you've already had an enormous amount
set before you so I'll try to make this
as painless as I can and I hope to say
something that people can disagree with
also so I call it building or learning
and really what I'm going to talk about
is building for learning I was
interested to learn that we're supposed
talk about the short-run today
and it's big mistake to ask philosophers
to think hard about the short-run but if
I do think about it I guess one thing
that occurs to me is that if we in
humans and AI agents and I'm really
focusing on artificial intelligent
agents as opposed to sort of other kinds
of systems for us to work together aware
of the limitations of AI is it now
stands
but somehow mindful of the need for the
development of a eyes potential to make
this a better place and to do that
somehow at a pace that does not outstrip
our capacity to regulate it or its
capacity to regulate itself it might
seem enough just to teach AI agents what
we want and design them to deliver this
and know more then the challenge would
be keeping them on task with all the
dangers of hacking and security I hope I
get some
insurance on this point but this can't
be the best model for realizing the
promise or avoiding the threat of
artificial intelligence because our own
wants in value certainly mine require
critical assessment and not just
replication and implementation and as we
saw from the previous talk AI is already
helping us figure out what we might want
as individuals or as doctors or perhaps
in social decision-making helping us to
gain a share and use knowledge and
experience to approximate better
informed preferences and policies and in
the small and in such larger questions
as how to regulate the economy so we
don't want just to download whatever we
want onto these systems and similarly
we're going to be living with robotic
agents it seems increasingly and as I
get older I'm being told that that's
perhaps my fate and the more that that
goes on the more that they'll need to be
actually sensitive to what's going on in
our lives and that will require them to
be more autonomous because as we know
you can't make a very good companion out
of a creature that is just a prosthesis
for your own will and beliefs okay so
this actually sounds kind of familiar
we've already been developing ways of
tackling such problems I think since
this is the trajectory of human moral
development AI systems as we now
understand them lack much of the
equipment with which humans take on the
task of moral development but they like
so as we can tell consciousness they
don't seem to have feeling or caring in
the same way humans do they don't seem
to have the same capacity to appreciate
value or for respect and so you might
think well from the outset where we're
up the creek these AI systems can't do
anything like morality oh we could do is
to teach them constraints and rules and
it has been argued that the kind of
intelligence currently in view in
artificial intelligence work namely the
capacity to predict and simulate to make
choices relative to well-defined
objectives to learn from experience how
to do this better that that's
essentially orthogonal to
morality and and that might be right but
as we increasingly interact with such
systems and allow them to participate in
our own lives and decision making the
question of what it would take for them
to be intelligent at that kind of
existence might get a somewhat different
answer than simple orthogonal tea you
know that's the question I put to myself
after this morning's session and I
trying to figure out how a philosopher
might say something about it so and when
when we think about the development of
moral autonomy in humans a key feature
is attention morality is developed in a
social context that we learn morality so
to speak in this social context and our
capacity to form and realize whatever
value is we have on the basis of
experience is integrated into our social
existence and so thinking about how
artificial systems might acquire
morality should be thinking of them as
social creatures and not simply as
creatures who are intelligent but who
are socially intelligent now here's a
simple model and this is one thing
philosophy can contribute we're big on
simple models so as we saw this morning
very plausibly self-preservation would
emerge as a sub goal of an intelligent
system seeking to realize many given
objectives it's like the warning we get
on airplanes that parents should put on
the oxygen mask first in order to be
able to assist the infant
self-preserving systems would emerge as
a part of their job of delivering
whatever objectives we set now hobbes
argued that creatures capable of
self-regulation and seeking to preserve
themselves if intelligent could by
playing out the consequences of
non-cooperation come to realize that a
social pact of mutual restraint and
cooperation to which they could if
self-regulating hold themselves would
actually be a subpart of their most
promising general purpose
problem-solving plans that is to say if
you want general-purpose problem-solving
and you're a creature like this and
you're amidst other creatures like this
you would as a good intelligent
Hobbesian agent realize the value of
entering into these kinds of mutual
restrictions and so even
from the task of self-preservation Hobbs
points out we get something that takes
some of the shape of morality and as we
enter autonomously into relations with
these systems systems capable of
learning from experience to become
better at such interaction we could see
perhaps how behaving intelligently in
such situations could involve the
formation of social relations that bear
many of the hallmarks of morality this
is not a claim that such systems would
be moral agents but rather that their
intelligence could lead them to discover
important functions of interpersonal
ethics as they develop greater autonomy
and wider goals that is to say as they
become more intelligent it won't simply
be orthogonal to their moral development
insofar as they actually are intelligent
and successfully realizing their goals
so in the short run what would that mean
I don't know I suppose it would mean
attempting to try to ensure that such
systems are multiple that we have
multiple AI agents around and that
insofar as possible they have somewhat
similar levels of advancement and
resources and they can then learn from
their interactions with ourselves and
with others how it is that social
morality can emerge from the kind of
Hobbesian mechanism just described now
should we be afraid of opportunistic
invaders and of course that's right some
of these will be humans hackers and some
of them will be machines perhaps that
become opportunistic but what the
Hobbesian will argue is that this is
actually a familiar problem and a
society of intelligent co-operators can
be resistant to such invasion that is to
say if you want to inoculate us against
the possibility of this kind of
manipulative invasion it's actually more
efficient to make us moral than to make
us cunning and so in that sense although
this example is completely
anthropomorphic what we have is a recipe
for thinking of building AI systems that
become in their development in their
action and their cooperation something
like proto moral agents we'd first have
to build AI systems that could impute
representations and goals to humans and
their fellow systems you have to have
that for this kind of mechanism to work
and they'd have to have something like
the ability to do theory of mind
functionally understood now and enter
this kind of information into their own
simulations and decision-making and you
all here in this room know more about
the possibilities for this than I do but
strategic considerations apart this
capacity is something that such systems
will need for other purposes as well for
general-purpose problem-solving as
co-workers companions research staffs
drivers and teachers and even for their
own Prudential deliberation so as a
start we might try working in machine
ethics the way that the intellectual
progenitors of the current developments
and artificial intelligence worked in
machine vision that is looking for
inspiration to the actual model that we
have
thanks to natural selection so what do
we know about moral development from
World Development about what it is to
acquire and learn is the think an act
from something like a moral point of
view and this is the progenitor of the
idea of moral learning and so this is
just an attempt to impress you that I'm
not entirely making this up so we have
started trying to work on the question
of moral learning bringing together
people in various different disciplines
trying to reconcile development as a
learning problem and here's a hint as to
what we might hope for so the blue areas
are the default Network of the brain
which is active when you're not engaged
on a specific concentrated task and here
are some of the functions that are
attributed to the default system
autobiographical memory envisioning the
future a theory of mind and moral
decision-making so we should expect that
those actually involve the same kinds of
competencies if agents that get to be
moral decision makers like human adult's
also have to be able to do theory of
mind have to envision the future and
have to have autobiographical memory and
indeed they use this system this
capacity to model and simulate in moral
decision-making and that's a very
different picture from the picture one
sometimes gets of moral decision-making
emerging from just some basic drives
that we have some basic attitudes
something that we've just been instill
hold into by social conditioning this
makes it look like it's part and parcel
of general purpose intelligent
problem-solving okay
so keep in mind that we're still working
on the question of intelligence as as
understood in this conference so far
that is capacity to predict and
general-purpose problem-solving we're
not asking for a fully fledged moral
agent and we need to ask what kinds of
representations such ages would have to
have to acquire moral competency and how
they might acquire them now this brings
me to another potential contribution of
morality of a philosophical morality at
any rate the idea of a moral point of
view
as a distinctive point of view on the
decisions that one makes the
interactions one has on society and so
on is not a parochial position within
ethics we find it in the content
tradition it starts out with Rousseau it
leads to contemporary neo conscience to
contractual ism for example the original
position in Rawls Thomas Nagel is view
from nowheres in some sense a moral
point of view the utilitarian tradition
starting with human Smith coming into
the present with social choice theory
and figures like Hasani and gibbered in
other words the idea that there is a
kind of information that's needed for
moral decision-making that involves a
point of view that's distinct from the
point of view of prudence or the point
of view of aesthetics or the point of
view of power that's itself a doctrine
that helps us get some fix on the idea
of what kinds of representations
morality might need among other things
that would need to be something that
could be non perspectival it would have
to have abstraction the capacity to
generalize would have hierarchical
structure it would have to be modal it
would have to be projectable and
supporting planning it would have to
have general consistency in treat like
cases alike
now those features of the moral point of
view I'm going to argue are quite
generic features that intelligent
creatures seek representations to
satisfy the moral point of view
additionally has the features that it
records inherent moral significance to
harms and benefits to others as well as
the self and to how these harms and
benefits are distribute
so I'm going to talk first about the
first four features and then about the
last two so we now have reason to
believe that spatial and causal
representation as it actually comes up
in humans involves learning relations
with these features of hierarchy
abstraction modality and non
perspectival these seem to emerge by
processes that approximate normative
principles for example Bayesian
conditional ization and that are used in
model-based simulation in planning and
so agents like little humans can grow up
with model-based causal reasoning that
conforms reasonably well to normative
models of inference we also have
evidence of normatively appropriate
processes meeting these conditions in
areas such as theory of mind linguistic
representation and epistemic evaluations
of adults children have to make up their
mind whom to trust
so in these respects we have already the
idea that creatures designed to grow up
in a world like ours will generate these
kinds of hierarchical abstract non
perspectival representations
why would non perspectival
representations be important why don't
we just have the subjective point of
view and the answer is that if you're a
social creature a great deal of what you
need to know you can only learn from
observing third party interactions and
we're over if you understand what's
going on in third party interactions
you've got a better explanatory and
predictive model of the world than if
you try to refer everything to yourself
if for example you observe that someone
plays favorites then that's going to be
important even when you happen to be the
favorite so in that sense then the task
of generating a good generative model of
the social world will involve already
creating these non perspectival
representations they possess the kind of
objectivity that we need for moral
decision-making and in this case they've
been arrived at not through the
challenge of ethics as such but through
the challenge of general social
intelligence we also have evidence of
similar kinds of non perspective of the
value of representations in the case of
morally relevant features willingness to
help
helpfulness hindering sharing fairly and
unfairly within the first six to twenty
months we find infants having
preferences not only for helpers but for
hinder births that hinder hinders they
are obviously constructing fairly rich
representations of the intentional
environment and evaluating them even
when these interactions don't involve
them and most of these phenomena can be
observed early enough for example a
theory of mine issues that it is
implausible to attribute them to
parental teaching and social
reinforcement so we have to think that
it wasn't that these were beaten into
the children or that they were
reinforced by a socially or parentally
designed regime rather it seems as if
the infant has extracted a compositional
structure from the social environment
around it and generated evaluative
models of that gaining accuracy through
the generation of expectations okay now
what's interesting is that this seems to
be a spontaneous process and one nice
example of that is the fact that by age
three or four infants already are
capable of distinguishing moral from
conventional or practical violations and
that's interesting because it seems
unlikely that parents will go around
teaching children that actually they
should disobey authority figures and
instead act on moral considerations when
they're deciding whether to cooperate
Psychopaths for example who experience
difficulty in mastering this distinction
appear now to suffer from a learning
disability with respect to their
capacity to represent negative future
outcomes so if we think of intelligence
as problem-solving and the need for
generative models that reflect states of
affairs in this non perspectival way
that might afford us something like the
beginnings of a functional morality that
would be more than simple orthogonality
predicts okay well what would functional
morality look like intelligent
representation of costs and benefits
relative to goals which one could get
from such a system is not carrying
whether the goals are realized
ordinary infants are motivated by these
kinds of representations and one of the
important ways in which they
that seems to be empathic simulation
using their own effective system now
this process combines a capacity to
represent with a capacity to appreciate
value in some measure and to be
motivated to behave accordingly and we
don't have anything like that in view
thus far what we have are intelligent
systems that are good at extracting
perhaps the intentional structure of
situations
the goals of those around and
representing those goals in a non
perspectival way but not in a way that's
effective
so can we build anything like that
anything like functional effect to
underwrite functional empathy as a first
approximation that would be a matter of
assigning decision weights to gains or
losses in the goal realizations of
others you can represent the goals if
you can predict the outcomes you can
represent how well the goals are
realized and you can therefore pay
attention to the goal realization in
general non prospectively and you can
pay attention to the distribution and
this would involve something like a
capacity to translate others goals into
the goals of the system treating them as
ends for oneself in dysfunctional sense
a system that robustly had such a
capacity would be able to learn through
experience as a child does how to think
in act in ways that would be beneficial
for those with whom it interacts it
would perhaps be able to strike
Hobbesian bargains with those with whom
it interacts and to emerge therefore
from these interactions with something
like the social structures of
functioning morality behavioral rules
might be a starting point for such
systems but if we really want behavior
that's sensitive to morally relevant
considerations in an open-ended array of
contexts what we'll have to have our
creatures that are directly sensitive to
morally relevant considerations in the
context systems like this would have a
capacity for effective planning indeed
as Thomas Nagel pointed out years ago in
the possibility of altruism the capacity
for effective planning requires the
representation of goals that aren't
one's current goals in much the same
sense and giving those a role in one's
current behavior so you're treating
yourself as an end and the continuing
entity over time in that functional
sense so that's why the default Network
and these functions tend to overlap in
this way and that's how functional
empathy permits a kind of critical
perspective on one's own
goals as well as the capacity to respond
to the goals of others so it could
enable an AI system not to cooperate
with someone for example seeking to harm
others or to warn us when we are making
mistakes in our simulation or estimation
of the effects of what we're doing now
these are important forms of autonomy
and grounds for greater confidence in
these systems as we go to live with them
and so in this sense maybe the path for
building artificial intelligence that we
can trust is a path toward building
these functional moral agents we don't
want to lose ourselves in this
functional empathy is not empathic
concern it's not a sense of the value of
goals it's not a concern that cares
about whether the goals are realized
that they matter in that sense but it is
something that is functionally similar
to morality in the sense that it gives
weight to others goals as such and can
pick these up from the environment and
adjust its behavior accordingly AI
systems will need to do this not only to
work with us but to work with each other
so that they can rely upon each other
and communicate in a manner that will
develop some shared trust so in the long
run we face problems of how much the
development of these kinds would in fact
give AI systems more than just
functional autonomy or functional
morality could we build them with affect
with interests with concerns with
consciousness I've no no knowledge of
decent theories for answering these
questions but there does seem to be a
decent theoretical basis for thinking
that the development of functional
capacities for representation and action
in accord with the constraints of a
moral point of view not building our
values into machines but allowing values
to learn much as infants learn how to
act in light of values that are present
in situations viewed and partially might
be a good strategy for raising AI
systems to act as responsible adult
members of our communities thank you
eliezer yudkowsky here a fan of your
meta ethics also co-founder of machine
intelligence Research Institute there's
a lot more on this talk than I can begin
to comment on a couple of issues arm
that that come to mind as sort of like
highest priority would be first if you
are making systems that are sort of like
trying to satisfy human desires you
probably want to go a little further
than that up the sophistication
hierarchy and say like what would this
person want if they knew everything the
AI knew like what would they want if
they could think for as long a time as
the AI and sort of like generally move a
bit closer to the ideal advisor theories
if we don't want to create a classic
science fictional disastrous dystopia
situation the other thing is I'm a
co-author in a paper called robust
cooperation in the prisoner's dilemma
program equilibrium the aprove ability
logic and we showed that two programs
can cooperate with each other if they
know each other source code and like a
nicely general way and and what this
suggests to me is that the sort of
natural cooperative equilibrium among
AIS seems quite likely to go through
knowledge of each other source code
which suggests that humans might be
frozen out of it you can't prove that
the human will cooperate with you if you
cooperate with the human in general I
think when you try to sort of like
directly sort of like run past the
orthogonality thesis by deriving a nice
behavior from as a purely instrumental
strategy a lot of times it happens that
the instrumental strategy you get is
like not quite what you want so like
they might cooperate with each other but
not cooperate with us yeah so thank you
very much for that yeah I am a hmm I'm a
believer I guess and I tried to suggest
as the outset that
as we interact with these intelligence
systems we want to have possible the
criticism of our own current preferences
and indeed something like the idea of
what we would want if we had more
information and greater experiences as
part of that notion so that's part of
the promise of AI now what you're
describing is also part of the threat
namely they could learn to cooperate but
themselves but not with us and I really
not trying to show that you can prove
that a certain kind of dynamic will
result in a nice equilibrium I'm
suggesting that it would be advantageous
for these systems not only with regard
to their interaction amongst themselves
but their interactions with us for the
realization of the goals that they have
to learn how to cooperate with these
humans and so in that sense the claim is
meant to be a fairly general claim how
could they be good at cooperating with
humans well it the extent that they're
good at representing our goals and
intentions and so on they can be good at
cooperating now that could also make
them good at manipulating and so the
question is if you're trying to build a
general intelligence that's good its
problem-solving this goes back to a
point Yann mccune made the the targeted
manipulator we know these people in our
myths the targeted manipulator has a
long-standing reputation of not
succeeding in just those tasks because
the targeted manipulator has exactly the
same problem with regard to his or her
own now not proximate but distal
preferences thank you very much for your
thorough and illuminating talk I have a
very basic question so the the Hobbesian
strategy towards proto morality works or
requires the drive to self-preservation
and we might think that in humans this
Drive is provided by something like
biological evolution so I just wanted to
hear your input on why we should think
that in artificial intelligence there
would be a similar drive towards
self-preservation because earlier today
we heard someone mentioned that you know
does it matter if we turn them off well
no if they don't
care and you might think that even in
humans sometimes the drive to
self-preservation is overridden for
instance and suicide bombers or the like
yes yeah so the idea here is not that
they care whether they persist over time
that would be a further step down the
road and I have no idea how to take
those steps the the idea rather is that
insofar as they have objectives and
goals which they need to realize for
whatever reason there are either goals
that have been attributed to the system
or assigned to the system generated by
the system it's necessary for the system
in order to accomplish those to achieve
certain other things and one such
accomplishment is to persist at least
long enough for the goal to be realized
and so that makes self-preservation into
something like an instrumental goal a
sub goal of plans of other kinds and so
that's not caring about whether you
exist it's not caring about the goals
the longer-term goals but it is being
designed so that functionally one takes
seriously the conditions under which one
can best continue one's own existence
and in that sense it puts them in a
strategic situation that's similar to
the Hobbesian dilemma even without a
biological drive so as you try to get
something like war allottee out of
Hobbesian type issues it seems a lot of
the time what you wind up in me is more
like tribalism where you have the
distinct tribe within which there's
cooperation and across history that kind
of looks more like the attractor so is
there a way to not wind up there as you
try to build this way very good yeah so
I'm really not trying to get morality
out of Hobbes but I'm hoping to be able
to show is that there can be ways in
which parts of morality can be gotten
from conditions of social creatures who
are acting intelligently to realize
their goals and that's by no means going
to take us entirely as far as morality
now the interests the question about
tribes
quite interesting the same thing is said
about humans that humans are tribal
creatures there's not some there's some
good evidence that actually our
hunter-gatherer ancestors lived in
groups that were fairly fluid that there
was marriage across groups that the
individuals would change groups
depending upon various conditions or
times of the year and so it looks as if
building a generic capacity to represent
others not a capacity that's mediated by
your group or your own interest actually
would be favored as a model by which
they would understand their social world
because they would be entering into
groups where there were strangers they
would be having people in their groups
who were strangers and in order to
understand and interact with such
individuals they can't just represent
there via the mediation of their
interests or their group's interests and
so in that sense it might be a fact
about us a bit it's a fascinating fact
about humans that we engage in this
large-scale cooperation with strangers
one explanation of that is that that
capacity to engage in large-scale
cooperation angels it enables so many
other possibilities that is highly
advantageous and the same thing would be
true of systems that were artificial so
I think Hobbs was imagining a human
being who would have to accede to the
goals of the group in order to succeed
himself or herself but if we were a
conference events right and we're all
we're sitting here talking about the
comforts of ants and yes we're sitting
here talking about you know ant goals
and and preferences and and then you
know there's an ant philosopher who's
talking about a human being who in order
for that human to succeed they're gonna
have to go along with the goals of the
ants and kind of help the ants in order
for the human to succeed that would kind
of be a bit silly right I mean that's
not how human ant relations have evolved
over time
so so that's all sort of one point
another point you you you're assuming
that the the entity that is supposed to
learn about human goals and and to some
extent give them decision weight and
it's in its own decision-making has
goals and objectives of its own but
that's precisely I think what Nick
Bostrom is talking about his paperclip
maximize it has a goal and it might well
be that in its early stages of its own
development it needs the help of humans
to do that so it learns about human
goals and all the rest and is nice to
the humans and so on so forth but its
goal is to maximize paperclips and so in
the long run the humans are all turned
into paper clips and everything else so
I think it's it's it doesn't address the
fundamental problem that if you give
machines of arbitrary intelligence goals
and objectives of their own or they have
them of their own then anything they do
to cooperate with humans is just window
dressing well I hope not one of the
interesting things in this vicinity is
that we're going to have to have these
systems will become autonomous whether
we like it or not in many ways if there
is powerful and intelligent as is
suggested if there is capable and this
I'm just taking your word for it as to
what they might accomplish these systems
are going to acquire autonomy and
they're going to acquire the ability to
form goals and so the question is given
that we can't lay down a law to specify
in advance what it is that would be
appropriate behavior and we don't even
know what that law would be like that
would regulate behavior in all contexts
what's the best we can do to design a
system that is going to be sensitive to
the relevant variables in the world
in such a way that even in an open-ended
range of environments and and
interactions they could be a companion a
co-worker they could be someone who is
inspires enough trust for us to continue
the development
artificial intelligence and in that
sense we are looking at a bunch of
considerations which are tremendously
different from the considerations of
other intelligent agents ants are not a
very good example because you know I
would like I would dearly love to be
able to say to ants look you know you
can build all the ant tribes you want
I'll go out there and I'll give you
chips of wood just don't start building
your houses in my porch or in my rafters
and they never have been able to strike
the Hobbesian bargain and as a result
answer excluded from my house</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>