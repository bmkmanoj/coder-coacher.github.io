<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stephen Wolfram - How to Tell Artificial Intelligences What to Do | Coder Coacher - Coaching Coders</title><meta content="Stephen Wolfram - How to Tell Artificial Intelligences What to Do - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stephen Wolfram - How to Tell Artificial Intelligences What to Do</b></h2><h5 class="post__date">2017-09-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LriBLE6ts6c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so tomorrow we're gonna be in a
different room at cantor there should be
a map somewhere where you'll be able to
find the location of that building okay
tomorrow we're going to be in a
different room at Cantor and there
should be a map somewhere in your
package that tells you where it is and
at 12 o'clock a bunch of AI safety
speakers are they want to host a
networking lunch and to talk about
careers in AI safety so this is this
will be an opportunity for attendees if
you want to meet some of these leading
researchers to sort of meet them and
talk to them so if you are interested in
do that doing that it's it's gonna be at
around 12 o'clock just hang around the
auditorium in the Kenter the the room
will be in and they'll come and find you
or I think yon here is look for him and
so that's tomorrow okay so right now now
we're in this session on building
morality into machines the first speaker
we have is Stephen Wolfram he's one of
my personal heroes because he's the
creator of Mathematica it's one of those
really groundbreaking software back in
1988 he's also the founder and CEO of
Wolfram research Wolfram Alpha and the
Wolfram language and he's the author of
a new kind of science an elementary
introduction to the Wolfram language and
ideas make idea makers and I think he's
also the youngest to have won the
MacArthur Fellowship if I got that
correctly and today he's going to talk
about how to tell a is what to do and
what to tell them please
okay well thanks for inviting me here
today it's it's actually quite funny to
be here because my mother was a
philosophy professor in Oxford way back
when and when I was a kid I always used
to say the one thing I would never do
was be a philosopher or talk about
philosophy so here I am but before I
really get into talking about AI I
thought I should say a little bit about
my kind of personal worldview I mean I
basically spent my life sort of
alternating between doing basic science
and building technology and I've been
interested in AI for about as long as I
can remember as a kid though I started
out doing physics and cosmology and
things like that that got me into
building technology to automate stuff
like math that worked out so well that I
started thinking about okay like how
would one really commute compute
everything about everything and that was
in about 1980 and at first I thought I'd
sort of had to build something like a
brain to be able to do that and I
started studying neural nets and so on
but I didn't didn't get very far
meanwhile I kind of got interested in an
even bigger problem in science sort of
how to make the most general possible
theories of anything and the dominant
idea for about three hundred years had
been to use math and equations and
things to make theories in science but I
sort of wanted to go beyond that and the
big thing I realized was that the way to
do that was to think about programs and
the kind of sort of whole computational
universe of possible programs okay so
it's sort of a a computational universe
of an array of very simple programs
these are cellular automata and these
things led to sort of my personal kind
of Galileo like moment which is you know
just point the computational telescope
at these very simplest possible programs
and then turns out you see some amazing
things like for example my favorite is
this one called rule 30 that just seems
to it it's based on that very simple
rule down at the bottom there
and it just keeps on going and just
seems to keep on producing complexity
forever from essentially nothing so
after I'd seen this I kind of realized
that it's actually something this
phenomenon of sort of being able to
produce lots of complexity from almost
nothing it's something that kind of
happens all over the computational
universe and for that matter all over
nature and the natural world I think
it's really ultimately the secret that
lets nature make all the complicated
stuff that we end up seeing in the world
but something else too it's also sort of
a window into what raw unfettered
computation is like well at least
traditionally when we do engineering
we're always sort of building things
that are simple enough that we can
foresee what they'll do but if we just
sort of go out into the computational
universe things can be much wilder I
mean my company happens to have done a
lot of mining out there in the
computational universe sort of finding
programs that are useful for different
purposes like for example this rule 30
is really useful for making randomness
and modern machine learning in some
sense can be thought of as sort of part
way from traditional engineering to this
kind of free-range mining of what's out
there in the computational universe okay
so but what can one say in general about
the computational universe well one
thing one could say is all these
programs out there can be thought of as
doing computations and well years ago I
came up with this thing that I call the
principle of computational equivalence
that says that if the behavior of
something isn't obviously simple it
typically corresponds to a computation
that's maximally sophisticated so there
are lots of predictions and implications
of this principle like that for example
Universal computation should be
ubiquitous out in the computational
universe and as should undecidability
and it also implies a phenomenon that I
call computational irreducibility so if
you look at a sort of typical
complicated case of this is a very
simple program this is what it does can
you predict what it's going to do you
know are these streamers going to go on
forever is it eventually going to die
out what's going to happen well this is
an example of a phenomenon that is
probably computation
reducible which means you you can't
figure out what its going to do without
effectively tracing each step and going
through sort of the same computational
effort to work out what's going to
happen as the system goes through itself
to work out what it's going to do so
it's completely a deterministic system
but to us it's got what seems a bit like
freewill because we can never know what
it's going to do so here's another thing
we think about in this kind of context
about what intelligence is well are sort
of big unifying principle this principle
of computational equivalence says that
everything from sort of a tiny program
to our brains should be computationally
equivalent which means that there can't
be there sort of no bright line between
intelligence and mere computation I mean
when we say something like the weather
has a mind of its own that has some
truth to it it's it's doing computations
that are just sophistic are just as
sophisticated as the computations that
are going on in our brains well to us
though it's pretty alien computation
because it's not connected what the
weather is doing isn't connected to our
it's form of computation or intelligence
isn't connected to our human goals and
experiences it's just a sort of raw
computation that happens to be going on
so the question then is so how do we how
do we tame computation we have all this
all this stuff that can happen out there
in the computational universe how do we
mold it to our goals well the first step
is to describe our goals and actually
for the past 30 years or so what I've
basically been doing is trying to create
a way to do that
I've been building a language that's
this now called Wolfram language that
has its home page that allows us to kind
of express what we want to do it's a
computer language it's not really like
other any other computer language
because it's in a sense much crazier
instead of sort of just telling a
computer what to do in its terms the
language tries to build in as much
knowledge as possible about computation
about the world right into the language
so that essentially we humans can
describe in our terms what we want to do
and then it's up to the language to get
those things done as as automatically as
possible
well that basic idea is worked really
well and in the form of Mathematica it's
been used to make endless inventions and
discoveries and things over the years
it's also what's inside Wolfram Alpha
where the idea is to take pure natural
language questions and understand them
and use the kind of curated knowledge
and algorithms that our civilization has
has produced to be able to compute
answers to these questions and yes
that's a sort of very classic AIH thing
and in practice it's of course computed
answers to billions and billions of
questions from from humans for example
in Siri and other places so I actually
had an interesting experience recently I
figuring out how to use what we've built
in our Wolfram language to teach
computational thinking to human kids and
I was writing a book about this and I
was writing exercises for the book and
the beginning was really easy to write
these exercises the basic form of an
exercise was make a program to do x-man
was pretty easy to say what X was in
kind of English but later on it was much
more frustrating because it was like I
know exactly what I'm trying to get
people to do I can express it very
easily in the Wolfram language but it's
really hard to express what I want done
in English and I realized gosh this is
why I just spent 30 years developing a
language to express things well so
English has about twenty five thousand
common words that the Wolfram language
has about five thousand kind of
carefully designed built-in constructs
including all the latest machine
learning stuff and so on together with
together with millions of things based
on sort of curated data and the idea is
that once one can the sort of concept
the goal is once one can think about
something in the world computationally
it should be as easy as possible to
express that thing in the Wolfram
language cool thing is it seems to
really work I mean humans including kids
can write and read the language and so
become computers it's kind of a
high-level bridge between human thinking
and its whole sort of cultural context
and compute
okay so what about AI well technology is
sort of always been about finding things
that exist in the world and then kind of
taming them to automate achieving some
particular human goals and in AI the
things we're taming are things that sort
of exist in this computational universe
of possible programs well there's a lot
of kind of raw computation kind of
seething around in that in that
computational universe of possibilities
just as there's a lot of complicated
computation that we don't necessarily
understand that's going on in nature but
what we're interested in is computation
that somehow relates to our human goals
okay so we want to talk about ethics so
maybe we the way to think about that is
we want to constrain this computation
this this AI process to only do things
that we let's say consider ethical but
somehow we have to find a way to
describe what we mean by that well in
the human world one way we have to do
that at least partially is with laws so
one question is how do we connect human
laws to computations we we call them
legal codes sometimes but today laws and
contracts and things like that are
basically written mostly in natural
language I mean there have been simple
sort of computable contracts in areas
like financial derivatives for a long
time and now one talks about smart
contracts in the context of
cryptocurrencies and things like that
but what about sort of the vast mass of
law that exists out there well I mean
like nets for example who who died 300
years ago next month actually was always
talking about making kind of a universal
language - as we would say now express
all of law in a kind of computable way
well he was a few centuries too early
but I think now we're actually finally
in a position to really do this and I
just actually posted a blog a couple of
days ago about this but I'll try this
this is a long long blog but I'll try to
summarize some of the things that I
talked about that um so I mean with what
we've already done with the wolfman
language we we've been able to we
managed to express a lot of kinds of
things in the world like the kinds of
things that people you know ask
Ceri about or something like that and I
think we're now within sight of what
live Nets wanted which is to have a
general sort of symbolic discourse
language that represents everything
that's involved in human affairs so I I
see the basic issue of building this as
a language design problem I mean yes we
can we can look at natural language to
get clues but ultimately we have to
build sort of our own symbolic language
- if we want to be able to create sort
of this computable system it's actually
basically the same kind of thing that
I've done for ages and ages in in
designing wolfing language I mean take
even a a word like + well in the Wolfram
language there's a function called + but
it doesn't necessarily mean the same
thing as the word it's a very specific
version of meaning that has to do with
adding things mathematically and so on
well as we design a symbolic discourse
language it's going to be the same kind
of thing I mean we can take you know the
word eat in English for example can mean
lots of things but we need a concept
that will probably refer to as eat
that's a very specific version that we
can then compute with so let's say we've
got a contract that's written in natural
language well one way to get a sort of
symbolic version of it is to use some
kind of natural language understanding
system just like we do for all those
wolf now for inputs and sometimes we
have to ask humans about ambiguities you
know which Springfield did you mean and
so on well another thing we might do to
get from our sort of original
representation to the symbolic
representation we might have a picture
and we might use machine learning to to
generate a description of that picture
in this in the form of this precise
symbolic language but ultimately the
best way to actually know what you're
trying to write is to write in this
symbolic language in the first place and
I actually I'm kind of guessing that the
the lawyers of the future will spend a
lot of their time writing in some kind
of symbolic discourse language mechanism
well okay so once you have a contract in
sort of symbolic form you can start to
compute about it you can automatically
see if it's satisfied in a particular
set of situations you can simulate
different outcomes you can automatically
aggregate together
bundles of huge numbers of these
contracts and and figure out
statistically what's going to happen and
build all kinds of funky financial
instruments out of them and so on well
ultimately though a contract has to get
input from the real world some of that
input will be born digital like data
about accessing a computer system or
transferring Bitcoin or something often
though that input will have to come from
sensors and measurements and so on from
the real world and it will take
basically machine learning kinds of
methods to turn those those those
measurements into something symbolic
we'll have to that that's sort of where
those judgments come in well if we can
sort of express laws in in computable
form maybe we can start sort of telling
a is how we want them to act um it might
be better if we could sort of boil
everything down to you know ask the mobs
laws of robotics or you know the idea of
utilitarianism or some such other other
thing but I don't think anything like
that is going to work I think what we're
ultimately trying to do is is to sort of
find perfect constraints on computation
but computation is it's something that's
in a sense infinitely wild so let me let
me give you a example of what I mean by
this I mean the issue already shows up
in for example girdle's theorem like
let's say we're trying to look at
integers and we're trying to set up
axioms that constrain us to be just
talking about the everyday integers that
we thought we were talking about well
one of the things that girdles theorem
shows is that there is no finite set of
axioms that you can construct that
constrain things so that you guarantee
to be just talking about ordinary
integers there'll always be some wild
form of thing that isn't what we Dourdan
really think of an inter as an integer
that is still consistent with those
constraints those axioms that you've set
up well actually this phenomenon of
computational irreducibility that I
mentioned implies a much more general
version of this basically given any set
of laws or constraints they'll always be
in a sense unintended consequences this
isn't it's not particularly surprising
if one looks at kind of evolution of
human law that something like this will
be true
but the point is that they're sort of
theoretically no way around it it's a
it's a feature of sort of just what
happens in the computational universe
it's it's ubiquitous in the
computational universe now I think it's
pretty clear that you know AI is going
to get more and more important in the
world and sort of eventually going to
control much of the infrastructure of
human affairs kind of a bit like
governments do now and like with
governments perhaps the thing we'd like
to be able to do is to sort of create an
AI Constitution that defines what the AI
is should be doing so a question is what
should this Constitution be like well
it's got to be based on a model of the
world and inevitably a sort of imperfect
model and then it's got to say sort of
what to do in lots of different
circumstances and ultimately what it's
what it's got to do is provide a way of
constraining the computations that
happen to be the ones that align with
our goals but okay so what should those
goals be well I don't think there's any
right answer to that in fact one can
sort of enumerate goals just like one
can enumerate kind of programs in the
computational universe and there's no
abstract way to choose between you know
this goal is better than that goal and
so on but for us there's a way to choose
because we have a particular biology and
we have a particular history of our
culture and our civilization and so on
it's taken sort of a lot of irreducible
computation to get here but now that
that that that now that we're just at
this particular point in the
computational universe that that
corresponds to the goals that we have so
you know human goals have clearly
evolved a lot through the course of
history and I suspect they're about to
evolve a lot more I think it's sort of
pretty inevitable that our sort of
consciousness will increasingly merge
with technology and eventually you know
the the certain scenario is that you
know our whole civilization will end up
as something like a box of a trillion
kind of uploaded human souls but then
the big question is okay well what will
these uploaded human souls choose to do
well the bad thing is probably we don't
even have a language yet to describe the
answer to that I mean if we if we look
back even to like notes this time we can
see all sorts of modern concepts that
hadn't formed yet and when we look
inside for example a modern machine
learning
system it's some or for that matter
theorem proving system it's very
humbling to sort of see how many
concepts those systems have effectively
created that we haven't yet absorbed in
our culture and for which we don't have
any words or other ways to describe them
so maybe looked at from our current
point of view it'll seem like those kind
of disembodied virtual souls are just
sort of playing video games for the rest
of eternity and you know at first maybe
they'll operate in sort of a simulation
of our actual universe then maybe
they'll do something that I've enjoyed
doing which is to go out and start
exploring the computational universe of
all possible universes but at some level
all they'll be doing is computation and
the principle of computational
equivalents says it's computation that's
fundamentally equivalent to all other
computation it's kind of a bit of a
letdown I mean you know the the proud
future of this whole civilization that
we we built ends up sort of being just
computationally equivalent to you know
plain physics or little rule 30 or
something like that of course in a sense
that type of conclusion is just an
extension of kind of the long story of
how science keeps on showing us that we
are somehow not fundamentally special we
can't turn we can't sort of and and and
that and since we can't look for a sort
of ultimate meaning in where we've
reached in through the processes of
history that we've gone through we can't
define a kind of ultimate purpose or for
that matter an ultimate ethics and in a
sense we sort of have to embrace the
details of our our existence in our our
history so I think my my conclusion is
that there won't be sort of a simple
principle that encapsulates what we want
in our kind of AI Constitution there'll
be lots of details that reflect the
details of our existence in our history
I mean the first step is just to sort of
understand how to represent these things
which is what I think we can now start
to look at doing with something like a
symbolic discourse language and yes
conveniently I happen to have just spent
30 years building the framework to
create such a thing and I'm keen to
understand how we can we can actually
use that to
start creating things like an AI
Constitution well I thought I I better
stop talking about philosophy here
usually I spend my time building
technology but I'll stop here and I hope
there are questions and comments and
things Thanks
I don't peter voss Peter Vasya I'd be
curious to know what your best guess is
as to when we'll reach human level
intelligence and AI so
when we'll reach human level
intelligence you know I've been working
on AI kinds of things for probably 35
years and people keep on saying whenever
when the system is able to do X then
we'll be really impressed and you know
I've done quite a few of those X's
through technology that I've built and
it's kind of disappointing because
people aren't that impressed but you
know I think the the main point is that
the disembodied intelligence is
something that we reached long ago
disembodied intelligence is just
computation the issue is is it
intelligence that we recognize as being
human-like intelligence and that's
really a question of exactly how we kind
of how we imprint kind of human-like
things on this computation that we in a
sense already have so I think it's a
it's a it's it's really it's very much
in the eye of the beholder and I think
their questions about how human-like the
experiences and an operations of the
thing have to be in order that we
recognize it as human intelligence
there's there's really a very close
analogy between extraterrestrial
intelligence and artificial intelligence
you know we see some it's like okay how
should the Stars be arranged so that we
know that something intelligent arranged
those stars well there are many ways
stars would arrange themselves with
gravity or whatever else there may be
some incredibly intelligent arrangement
of stars out there but it doesn't relate
to our human notions of intelligence and
so we don't we don't recognize it as
such I think it's the same type of thing
with where the I so I think it's a I
think it's a deeply slippery su hi my
name is Chris Harbor I'm an alumnus of
Columbia University the question is
about the Wolfram language so putting on
the asking the language designer
implementer a part of you
what were the difficult sorry this
question is about the symbolic discourse
language the new language you're
creating
based on your experience over the last
thirty three thirty years building the
Wolfram language
what are the difficulties whether
implementation or theoretical scaling
difficulties partial extra AST
interpreter etc that you foresee will be
problematic in the development of the
symbolic discourse language and finally
this is an open-ended question so that's
right as you will yeah so so I mean the
thing that's language design is really
hard I mean I've worked on a bunch of
different things that people think of
hard like physics and math and other
such things language design is really
hard compared to those things because
it's really it pushes you right into
having to really understand things with
maximum possible clarity I mean what
you're trying to do when you design a
language you're trying to design a
framework for people to think in I
suppose a little bit like philosophy has
tried to do but in as a language
designer you don't get to just write
essays about these things you actually
have to implement stuff and then you
have to stick with what you implemented
the so it's really you know drilling
down I view the role of a language
designer more or less as this when
you're thinking about computation you're
imagining all the possible kinds of
computations people might want to do and
then you're working a bit like a natural
scientist you're kind of drilling down
underneath what you see people want to
do to find what primitives exist that
you can then build up from to let people
conveniently create what they want to do
well I think the symbolic discourse
language it's the same kind of thing we
have to look at sort of everything
that's out there in sort of the world of
human affairs and so on and we have to
say what are the fundamental concepts
how should we best slice things up and
you know people like in the 1600s it was
quite a popular thing to do to invent
philosophical languages there's a chap
called John Wilkins who had a fine one
of the better developed philosophical
languages which was an attempt to kind
of do this the sort of thing of slicing
up concepts in the world and some in
some way well the hard thing I think is
is is actually doing that and it's
something where in a sense to do it you
have to take a flying leap and just
start doing it because there isn't
really it's not like there's a there's a
big framework that already exists it's
just you've got to start deciding how
it's going to work as as people have
done you know when it's roche making a
piece thesaurus or something like this
but that i think that's the main the
main difficulty is it's just that you
have to
stand things clearly and be able to
drill down there hi um over here I'm
just curious do you think we live in a
deterministic universe and tangentially
what are your thoughts on this question
of whether our present reality is a
simulation or not so I've worked quite a
bit on fundamental physics I kind of the
tantalizing thing is when you see these
simple programs like the cellar automata
that I showed and you see how tiny their
rules are and you see how complicated
what they do is you start thinking well
gosh maybe our whole universe is like
that maybe there's really some simple
rule underneath a whole universe if we
think about you know our universe being
determined by a program we might you
know the program might be the size of an
operating system you know millions of
lines of code seems implausible more
likely it's something short that's a
weird claim in itself because it's kind
of very anti Copernican you know
Copernicus kind of a sort of taught us
that we shouldn't think of ourselves as
special in some way so why should we get
the universe that's this special one
that has a simple program in terms of
kind of thinking about simulation you
know is our universe as it is now
existing a simulation I think that
argument is a model okay because the way
I see it you know this question okay you
asked about determinism my when you say
something is non-deterministic what
you're saying is I don't know everything
about how the system works there's
something outside the system that is
randomly kicking the system there's a
part that I know that's deterministic
and then there's the non deterministic
part that's kicking it that's kind of a
wimp out when you're trying to make a
theory in science and so you know the
question is can you know the first cut
is let's just say maybe our universe is
deterministic maybe there's just a rule
and an initial condition maybe
everything that happens the question you
ask the answer I'm giving is all
ultimately determined by by those things
I think that's a good first hypothesis I
don't know any way to prove that isn't
true so that's a hypothesis that I'm
working on in terms of if you I think it
is quite plausible that there is just a
simple deterministic rule that
determines what how the universe works
it's kind of like the digits of pi
there's a simple deterministic rule that
determines them and then you generate
all those digits and it's like you
generate our whole universe
and here's everything that happens just
like we see all the digits of pi now you
say well what would it mean if there's a
simulation going on there you know
underneath there's a down at the Planck
scale there's this whole civilization
that's arranging things very carefully
to make our world be the way it is but
actually they're stuck in the same
deterministic system - so the question
of whether there is something down at
the Planck scale that's a whole
civilization comes exactly back to the
previous question that was asked about
intelligence actually it's a question of
how do we tell you know it's like we're
searching for extraterrestrial
intelligence it's not extraterrestrial
it's everywhere inside our universe but
we're searching down there and we're
saying is it intelligence is it a
civilization or is it just physics so to
speak hi my name is Jim I didn't major
in philosophy either but since we're on
that kind of spirit I have a
philosophical question as well so it's
in the same similar spirit as the
previous question so if you think the
universe is a computational in some
nature how do you what's your best
hypothesis on how consciousness arises
out of this fundamentally computational
universe and on that topic I guess
whether you think Free Will exists in
the whole related philosophical
conundrum whether whether we really
exist you're asked you know whether free
will exist and darkness arises from
complication so I mean the first
statement about free will is that I'm
really pretty I'm really pretty sure
that the way we you know free will and
determinism you know work together is
through this idea of computational
irreducibility that even though there is
determinism you can't know what's going
to happen any any sort of faster more
easily than the system itself so the
system itself is it's like deciding what
to do and we're trying to say no no no
you know what you're going to do you
don't have free will I know what you're
going to do actually you know you can't
tell what the system is going to do and
except by just following through exactly
what it does and you can you can start
tracing through all the sort of
implications of free will about
responsibilities and so on is this
system responsible for its
actions well actually it is it's more
responsible for its actions than if you
say there's a system which is being
kicked by the outside world in certain
ways that's being sort of the
environment is forcing it to do this or
that now you asked about consciousness
and so on I you know I I hate this term
because I think you know if you look at
the sort of stack of different things
that start from life go to intelligence
then you get to consciousness and so on
every one of them has the following
feature that you know let's take life as
an example okay so how do we define life
you know what is alive what is not alive
on earth it's pretty easy there's shared
history to everything everything that's
alive has RNA in it that's a very
specific thing it's probably not the
general abstract notion of life that
life has to have RNA in it but we don't
really have a good abstract definition
of life you know people have tried
different definitions they never really
work and the reason is I think because
ultimately the only thing is that
there's sort of sophisticated
computation going on but there's nothing
that defines life except by this sort of
historical definition of saying the same
thing about intelligence and I think
it's even worse with consciousness I'm
really surprised that you think that
freewill has something to do with
unpredictability because I think most
philosophers wouldn't say that that they
wouldn't deny first of all that
something or someone was acting freely
simply because it was predictable that
they would do something because let's
say a completely rational person you can
predict what they'll do but you wouldn't
necessarily from that on that basis deny
freewill but the other the major point I
was concerned with is when you talked
about any given set of constraints those
rules will always lead to unintended
consequences sometimes philosophers
distinguish between a rule and a
principle so that it's something like
the difference between the letter and
the spirit of the law so you try to set
up a set of rules okay and you find
yourself when you deal with certain
cases okay in the law that the rule
leads you to peculiar consequences all
right there's something wrong all right
you haven't captured what you had hoped
to capture but you understand that
because you understand a principle that
lies behind the rule and I'm wondering
if there's anything like that that is
computable for AI namely that they see
the peculiarities the
will generate but they understand the
principle behind it I mean a simple
example is you know HLA Hart talked
about rules that said cars should not be
right driven in the park and then
somebody has a toy car well there's
obviously the intent of the law is by is
not captured by the rule and we
understand that and is there something
comparable okay interesting question so
so I mean I think well first of all
about freewill
I'm not a professional philosopher I
decided not to do that when I was five
years old or something so I can't I
can't debate all there all the details
of how philosophers talk about freewill
although there's there's a lot more that
I could say than the few sentences that
I just said and in terms of of sort of
the intent the principle as opposed to
the letter of what's going on in a sense
what I'm what one does when one sets
things up computationally when one talks
about symbolic systems and so on is once
trying to grind out all of those sort of
humanistic types of things and sort of
desiccate everything to the point where
it is just a bunch of symbols and where
there is something definite that can be
said now if you ask the question is
there can there be more general
principles in other words you've got
some particular set of rules let's say
is there a more general principle at
work let me give you an analogy okay in
mathematics so in mathematics there is
in a sense there's a we can prove that
there's sort of an infinite frontier to
mathematics you will you will prove
certain theorems but there'll always be
more theorems to prove there'll always
be more in a sense principles to
discover you'll never reach the edge of
the principles that there are to
discover in fact it's sort of a
consequence of this computational
irreducibility idea that there will
always be that there'll always be more
to discover so give you an example let's
say we're we're looking in physics at
the universe what can we build in the
universe can we build a spaceship with
warp drive something like this it we may
be able to just straight answer that
question but there's no guarantee it's
kind of an undecidability type thing
there's no guarantee that given the
building blocks that we have in the
universe there may be no sort of finite
way to decide whether we can build a
spaceship that will support warp drive
and it's the same type of thing I think
with this this question about I mean
you're asking about you know can there
be a deeper principle can one discover a
deeper principle I think a consequence
of this principle of computation
the principle of computational
equivalence and computational
irreducibility I think a consequence is
there will always be deeper principles
come on get to the point where where one
there will never be an ultimate
principle or or a lowest principle but
there will always be more that can be
discovered
like in mathematics if what you won't
reached the edge where you just say now
we understand everything we've got the
complete set of rules where we've got
everything nailed down there'll always
be more that can be discovered it's an
interesting question and there's
probably probably more that I'm that I'm
not oh just one more comment about this
okay the the the the thing that happens
for example when you look at the output
from an automated theorem proving system
you can establish that something is true
but what you see is a bunch of little
sort of atoms of fact there it is not
something that humans can relate to
because there are no sort of waypoints
that are you know Smiths theorem Joneses
theorem that everybody knows what they
are they're no kind of they're no kind
of things concepts that have emerged
that we understand and I think when you
talk about principles I think in part
what you're maybe talking about is these
kinds of things that have emerged as
culturally known things to us that and I
think one of the things that happens in
this sort of computational world is that
you get all these little bits of
computation and the question of whether
we can understand parts of that in some
in some way where we can talk about them
is a different and sort of more it's
it's more a question about human history
than it is about the raw computation
okay
I know I miss some people but there'll
be a panel at the end so you can ask you
a question then next we have professor
Fang Jessica Rossi she teaches at
university of padova and is a computer
scientist she's the president of the
international joint conference on
artificial intelligence and is associate
editor in chief</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>