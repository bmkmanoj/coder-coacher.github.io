<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Artificial Intelligence Debate - Yann LeCun vs. Gary Marcus - Does AI Need More Innate Machinery? | Coder Coacher - Coaching Coders</title><meta content="Artificial Intelligence Debate - Yann LeCun vs. Gary Marcus - Does AI Need More Innate Machinery? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Artificial Intelligence Debate - Yann LeCun vs. Gary Marcus - Does AI Need More Innate Machinery?</b></h2><h5 class="post__date">2017-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aCCotxqxFsk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone welcome to the big debate
I'm David Chavez a co-director of the
Center for mind brain and consciousness
along with my colleague Ned block the
Center for mind brain and consciousness
is a interdisciplinary Center here at
NYU devoted to exploring foundational
issues in the mind brain sciences and we
organize a lot of activities among which
are a bunch of public activities a
series of organizing a series of annual
conferences every year I think a bunch
of you were at the conference last year
on the ethics of artificial intelligence
coming up in November this year November
17th and 18th we have a conference on
animal consciousness with a bunch of
really interesting people involved
you're all encouraged to come along to
that one but we also organized a series
of debates one every semester on
foundational issues debates we've had
recently include plans over mirror
neurons unconscious perception moral
psychology art in neuroscience in a
concepts cognition perception many many
others and the thought is to bring a
foundational controversial issue into
sharp relief by having proponents of two
different sides on that issue present
their point of view so the debate today
I think will be our first one devoted
specifically to issues in artificial
intelligence which is of course one of
the main brain sciences broadly
construed artificial Minds or minds to
but if they the debate here really in
some ways grows out of and parallels a
very familiar debate in psychology and
the sciences of the human mind the
debate between nativists and empiricists
about the human mind broadly speaking a
nature nurture debate over what is
more important learning from the
environment or a certain kind of innate
endowment of innate mechanisms built in
at Birth though this is a debate that
goes back for centuries and indeed
millennia in the history of philosophy
for example you know Plato was a great
nativist to held that we had all kinds
of knowledge built-in to us at birth
there are many empiricists in the
history of philosophy to John Locke and
David Hume the British empiricists who
tell that basically you start from a
blank slate tabula rasa and go on to
acquire to learn empirical associations
from the environment from which all of
our knowledge and all of our cognition
develops and this debate which started
from philosophy turned into a debate in
psychology for many years where there
empiricists such as the behaviorists the
association of more recently some
connectionists and some beige Ian's put
all the emphasis put on a whole lot of
the emphasis onto learning from the
environment at the same time there
nativist Satyam was famously probably
Chomsky but then more recently people
like a voter and Pinker and indeed many
people would then develop developmental
psychology where nativist approaches
stressing the need for innate
domain-specific mechanisms have been
extremely populist so what started this
debate in philosophy turns into a debate
in psychology now what we're we're
seeing is this is to some extent turning
into a debate in artificial intelligence
as well so the current debate is about
AI systems and it's very much triggered
by looking at the recent prominence of
machine learning in AI and in particular
the explosion of work in deep learning
where the emphasis is very much on
starting from relatively simple systems
and learning from the environment
and the question is really how far can
learning mechanisms in particular but
not only deep learning get you yeah
proponents of deep learning of which
Jung Lacan is one very prominent
proponent say that deep learning can get
you very very far and if you want to get
further the best place to put the focus
is on more and better learning
mechanisms critics of deep learning of
which Gary Marcus is a prominent Lance a
deep learning will get you so far and
maybe even some good way but there going
to be some very serious principled
limitations on where it will get you and
to get beyond there we need to really
put some serious focus on the right kind
of innate machinery and on innate
mechanisms as in the debate over
psychology you know in the end everyone
concedes there's some very significant
role for innate mechanisms and some very
significant role for learning and who
gets to win the debate often depends on
who more successfully defines the terms
you know and sufficiently puts the
burden onto the others so what degree of
an innate nurse and what degree of deep
learning of learning mechanisms actually
counts in framing this debate we went
back and forth for weeks on exactly what
the correct topic should be for the
debate since everyone thinks is gonna be
some innate mashite or everyone thinks
deep learning will will do a lot
everyone thinks it won't do everything
in the end we said does a I need more
innate machinery which of course raises
the question more than what more than it
has now
probably both gary will say most
definitely yes a whole lot more than it
has now
probably even yarn will say we can we
can use some more innate machinery in
the end what we thought was that really
what the issue might come down to is
given the limitations on current AI
systems where should the focus be what
do we need most to move forward towards
more advanced AI systems towards human
level artificial
intelligence and both sides will agree
yes better learning mechanisms are
important yes more innate mechanisms are
important but which one of those things
do we need more which is the most
important in moving forward yarn is
going to say better deep learning
mechanisms that's the most that's the
most important thing Gary's going to say
we need more innate machinery so really
the topic should be does a a a I need
more innate machinery but does a I need
more innate machinery more than it needs
better deep learning mechanism but that
was really way too way too long for a
for a poster so we're going with the
shorter version instead but we really
have the perfect debaters for this for
this topic
yeah and Lacombe is is one of the
pioneers of deep learning done important
work on neural networks since the the
1980s we here at NYU for many years as
professor of computer science founder of
the Center for data science is now also
director of AI research as Facebook and
been a very very prominent advocate and
developer of deep learning systems Gary
Marcus has for many years been a critic
or at least a very a constructive critic
of certain kinds of neural network
approaches to to cognition his first
book the algebraic mind was arguing that
at the very least a neural network based
approaches needed to be combined with
certain kinds of simple processing
mechanisms in order to be maximally
productive has written a lot on from the
perspective of psychology and
linguistics on the need for innate
symbolic structure in cognition members
recent years been a prominent voice
criticizing or at least you know
pointing to a need for going beyond
parent deep learning approaches if we
really want to make progress deep
progress in artificial intelligence and
Gary's professor of psychology and
linguistics here at NYU also started a
machine learning company geometric
intelligence which not too long ago he
sold to to Oberer and it was very
briefly heading out
ái at uber before seeing the better of
his ways and returning back to join us
at NYU for which we're very pleased this
is incidentally our first all NYU debate
it turns out it's then we look for the
best people worldwide to take on the
sides of this debate but the search just
ended up in our backyard who better than
Gary Market and yanma come to to debate
this topic so the structure of this will
be Gary will go first because he's
arguing for the affirmative
hey I needs more innate machinery and
will speak for 25 minutes
young will respond for yond will speak
for 25 minutes then we'll have 8 minutes
each
in response and follow-up to each other
then we'll bring everybody up on to
stage a little bit more interaction and
we'll take it to the audience for what
I'm sure will be a lively discussion but
for our first speaker it's a great
pleasure to welcome Gary Marcus
thank you very thank you very much can
you all hear me is this good mic level
okay so uh David stole my my title slide
so I can repurpose it now I'm Gary
Marcus um so I'm gonna start actually
with a quote from the head of AI
essentially at Microsoft which captured
something I've been arguing for a number
of years pretty nicely he says computers
today can perform can perform specific
tasks very well but when it comes to
general tasks AI cannot compete with the
human child and the question is why not
I want to before going too much further
start with a news flash which is Yan and
I don't disagree about that much of
course I'm sure we will find some things
to disagree about today but on the
weekend I made a list of seven things
that I thought we agreed about and some
of my friends said do you really know
that Yan agrees with you on these things
and I thought by now young and I've had
a lot of discussions and I had a pretty
good theory of his mind so I emailed
them and he wrote back half an hour
later or something like that and said
not only do I agree with these seven
points but they've actually been the
essence of what I've been talking about
for the last three years so we agree
that AI is still in its infancy we agree
that machine learning is fundamentally
necessary for reaching strong AI we
agree that deep learning is a powerful
technique for machine learning we agree
that deep learning is not by itself
enough for cognition we agree that
something called model free
reinforcement learning which is popular
deep mind is not the answer either we
would also agree that AI needs better
internal forward models and since I
don't have enough time to describe
everything I'll leave it to you on to
teach you what that is and I think we
fundamentally agree that common-sense
reasoning remains fundamentally unsolved
and that's one of the most important
questions facing AI today what I'm going
to suggest today is the human grade
intelligence is too complex to be
manually hardwired I think probably
everybody here will agree with that but
I'll also argue that current machine
learning systems come too close to being
blank slates and so it very much is an
echo of the nativism empiricism debate
I'm going to suggest that AI largely
tends towards the empiricist side of the
spectrum and that's the problem and
hasn't served AI particularly wealth and
as a result what people have come up
with our kind of sterile machines that
are tied brittlely to
enormous datasets much faster than
children might need and that they've
little ability to transfer their
knowledge to new problems I'll give you
an illustration that later so you might
say that they're strong on narrow AI the
techniques that we have right now but
they're non-starters on strong AI
there's been very little progress on
general-purpose AI where you could just
sort of point the box at whatever
problem you have without having humans
massage the data and so forth and just
expect a reasonable answer from the
Machine yan i think is going to take the
empiricist view we'll see whether we
actually disagree but at least in the
media he takes the empiricist view so in
this technology review report he's
attributed the idea that you can just
have AI that learns just by observing
the world and that very much echoes what
john locke's it I think Jonathan Locke
is the kind of paradigmatic empiricists
he said all ideas come from sensation or
reflection oh to skip a little bit but
he says basically of a blank slate and
where does everything come from in one
word experience I think yan has sort of
modern tools I put in old versions but
has modern tools for doing this so we're
John Locke just had an abstract idea
that maybe you could learn everything
from experience yan has built tools that
actually tried it can test that
conjecture and in many cases get pretty
far I'll argue but not far enough but
they certainly make a lot of progress
the tradition that I'm in I would
attribute to Plato first of all his
famous slave dialogue - Khan - as I
would psychologize him argued in the
critique of pure reason that we had to
have an innate notion of time and space
that's not his words I read the
translation and I'm told even German
scholars read the translation because
it's so hard to parse but that is how I
take conte of course Noam Chomsky is
famous for arguing there's a language
acquisition device and I actually come a
little bit closer to lists belfies view
where she talks about a core of innate
primitives that allow us to know about
things like sets and objects and places
and so forth I've been arguing in
different form for for nativist views so
in the book the algebraic mind I focused
on representations and primitives you
can think of the brain is something like
a microprocessor and I said these are
the things that that microprocessor must
have I'll come back to this a little bit
later in the book the birth of the mind
I talked about nativism from a
biological perspective and basically
asked is nay
is incompatible with biology where where
might you get your innate structure from
the key claims that I will make today is
first of all I would concede I don't
think any reasonable person wouldn't
that individual human beings learn a lot
in their lifetimes
I don't think its nature versus nurture
I think its nature and nurture but I
think that the nature side has been
gotten in short shrift in AI but that
learning is only possible because our
ancestors have evolved machinery for
representing things like space-time and
enduring objects and my prediction and
it's only a prediction I won't be able
to prove it is that AI will work much
better when we figure out how to
incorporate similar information into AI
it's logically possible of course that
could be lots of innate structure in
humans but we wouldn't need it for AI
and I'm sure what discussion of that
today so neither of us is gonna be able
to prove our position today we simply
don't know we don't I haven't done the
right empirical studies of AI with and
without innate machinery to really be
able to answer this question but in a
way that's partly my point it's an
empirical question and we haven't been
gathering the right data we haven't been
looking at am AI models with nativist
structure to compare them with ones that
lack native a structure and I'm gonna
try to suggest this good reason to take
a more nativist position more seriously
so first question is could the gap
between machines and toddlers stemming
part from a lack of innate machinery
well there's certainly lots of reasons
to believe that there's innate machinery
in human beings so anybody who has read
Steven Pinker's book and if you haven't
read it you should read it the language
instinct which should be familiar with
some of these arguments I'm not going to
go through them in detail but I think
there are a lot of them ranging from
poverty the stimulus arguments which
Pinker of course got from Noam Chomsky
saying you know there's limited amount
of data and yet you have an infinite
language to arguments about the relative
speed at which toddlers learn language
compared to learning lots of other
things so calculus is much harder than
language why is that there's a
robustness to input regimes so some kids
learn in upper middle class environments
where there's a lot of correction maybe
of the kids or at least some and focus
on what the kids say and there are other
kids learning environments where parents
think talking to small kids is like
talking to a plant why would you bother
and still kids in those environments man
managed to learn from incidental
language quite well there's also
dissociative um other cognitive
functions there's a long literature on
native as I'm not going to go through
all of this psychological literature
today
but I'll just tell you it's ongoing so
there was a study that just came out and
in Current Biology which showed that not
even newborns but kids in the womb could
tell the difference between a face and
an inverted face so it was stimuli like
in the top right the little dots shown
to the kids through through the womb
they get a little blurry that's what the
next picture is supposed to show you and
it turns out kids like to look towards
the one on the top and they turn away
from the one at the bottom so there's a
long set of studies like that people
never like to believe in nativism about
people so I like to show them animals
these are baby ibex is climbing down a
mountain they don't get to do million
trial learning right if they make a
mistake it's a problem they might come
close but they don't actually okay deep
learning typically presumes that most
connectivity is determined by experience
we might have a chance to drill down on
the details of this quote but I think to
a first approximation it's right in the
initial wiring of deep networks is
largely random I know largely is not
fully correct I'm happy to talk about it
later but the first approximation and
that's true but if you look at the brain
it's just not that way so we've known
since the time of Broadman that there
are different brain areas they do
different things there's a lot of
differentiation I like this quote from
David Hubel whose idea of hierarchical
feature detectors underlies some of
yawns greatest work and Huebel is no fan
of this idea that you start with an
initially random Network and back to a
first approximation the structure of the
brain doesn't require even internally
generated experience which was something
popular a couple years ago you can
actually cut off synaptic transmission
using genetic tools this was an article
in science in 1999 and there are pairs
of pictures and you look at a mouse that
has this knockout that knocks out
synaptic transmission and you compare it
to one that is a normal Mouse and you
can't tell the difference until they're
born
turns out synaptic transmission is very
important once you're born but you get a
rough you die right if you don't have it
but you get a good rough draft even
without anything that plausibly could
convey prenatal experience etc and
that's because most of the genome is
there in part we can talk about the
theology of it later but has evolved
such that its shape
the brain so ninety nine and a half
percent of your genes are expressed in
your brain at some point and this is
data from the Allen Brain Institute
that'll be published in the next month
or put online in the next month and most
of those genes are expressed during the
process of development a significant
number of them are expressed in specific
ways they're there to make the brain
have a particular structure not just be
a kind of piece of spam at the beginning
and there are a lot of mechanisms for
this like high precision axon guidance
cell migration all the kinds of stuff
incidentally that I talked about in the
book the birth of the mind a large array
of evolved mechanisms have in fact
yielded greater precision in human brain
development so I'm not going to go
through all of these but there's a great
review by Gershwin and Rakesh a couple
years ago talking about all the specific
mechanisms like gene duplication that
have given us more precision for wiring
up our brains in advance of experience
than other creatures so the view of
innate inist from biology that I put
forward in the birth of the mind and
which I still believe to be correct a
dozen years later is that pre wiring
first of all doesn't preclude rewiring
people talked about nature versus
nurture that's a crock nature's would
allow nurture to happen at all and the
fact that your pre wiring doesn't mean
you can't have a lot of plasticity to
rewire things there's plenty of room in
biology both for pre wiring and and for
all kinds of learning but there's also
ample evidence that biological creatures
start with strong starting points which
is a polite way of saying innate
knowledge even before learning begins
there's ample machinery in the jeanna
for genome for wiring up the initial
circuitry evolution has shaped that
machinery and Locke was just wrong we're
not blank slates now of course the next
question is whether that matters for AI
I would say that we don't know because
the field hardly ever even looks so when
we had this back and forth discussion
what are we going to debate about
Chalmers sends out the text he says when
I do a google search on nativism and i
get a did you mean message like it's
that rarely even discussed that you
couldn't find a few basic web hits there
are a few in the 90s um my point is that
in the space of design choices for how
you could build a neural network or
machine learning system attack system
innate machinery usually gets ignored so
you have quotes like this I totally
agree with you on the first part this
was an interview belief fear of human
children are very quick at learning
human dialogue and learning common-sense
about the world we think there's
something we haven't
yes we agree on what the problem is and
then Yan points to where he thinks the
solution is which is some sort of
learning paradigm that we haven't
figured out well logically that's
certainly possible and I bet we have a
lot of things to figure out about
learning paradigms but also logically
speaking there are three different
possibilities for reaching next level AI
maybe we need better learning algorithms
which mine you know they're themselves
innate we can talk about that later but
we might also need machinery for
constraining what those algorithms are
and of course I think the answer is
likely to be both that we're gonna have
to have better learning algorithms and
we're gonna have to have more innate
constraints if neural nets have taught
us anything it's that pure empiricism
has its limit so here's illustration
from a talk that was presented at NYU a
couple weeks ago by deep mind this is
the company that is solved or the
subsidiary of alphabet that has solved
go right they know how to play go but
when they try to learn language by
having a little character wander a
simple world and have labeled objects
they do terribly so it's a kind of
slightly complex neural network but not
anything too far from the standard and
then you look at the data and it learns
things there's 59 words it learns and it
takes it to million trials to learn 59
words anybody who knows the
developmental literature would be really
worried about that if you had a child
that took two million trials you would
have spent a lot of time in the speech
pathologist office and you'd be very
very upset kids can learn things in in a
single trial sometimes and then you go
over over to things like negations stay
away from the fridge and it's even worse
it's so bad that in the archive paper
that they describe this they didn't
include the negation but the guy showed
it and I tried to quickly take the shots
on my camera so look what I said in in
New Yorker when I first wrote about deep
learning as such when the term first
became popular is keep learning is a
better ladder but a better ladder
doesn't necessarily get you to the moon
I would stand by that the amazing thing
is that some other people have started
to agree with me this is Geoff Hinton
who is kind of often seen as the father
of deep learning although Yann invented
convolutional networks which are an
extremely important addition so I mean
one of the co-founders with Yann yoshua
bengio and so forth and he's saying
maybe backpropagation isn't the right
thing maybe we need to start over you
read this in Axios I think maybe I got
cut off a couple weeks ago meanwhile in
a constraint
often play an unacknowledged yet vital
role in getting neural networks to
actually work so here's the nature paper
about alphago and you look at the top
figure and you say oh it's a neural
network they solve go with the neural
network isn't that amazing and then you
read interviews with demos hospice and
that's the impression you get because he
says the most significant aspect of all
of this is that alphago isn't just an
expert system built with handcrafted
rules instead it uses general machine
learning techniques to win it go it
sounds like you know another victory for
empiricism for deep learning and so
forth but if you look carefully they
have another thing in there which is
called Monte Carlo tree search a lot of
the subtext of the debate we're having
here is about symbol manipulation versus
neural networks Monte Carlo tree search
is a technique that has come directly
from simple manipulation since you have
an explicit representation of abstract
trees which is one of the things that I
argued for in the algebraic mind and
everybody said was ridiculous but now
when when it's down in the trenches
people use it it's actually even worse
than that though buried in the tables is
extended data table to the input
features for the neural network so
rather than learning from pixels which
is what deep mines famous Atari game
system does they actually learned not
only from pixels but from features like
the number of liberties after a move and
my favorite is sensibleness whether a
move is legal and does not fill its own
eyes so there's a little innate feature
detector that says that's too dumb you
would never ever bother to do that so
there's an anus but it's not principled
innate that's what I think we need to do
is to look at innate and it's in a
principled way in the context of machine
learning which I don't think anybody has
done a whole lot of though I will give
you an example in a moment so what
should we be looking for I love this
quote from Liz felt he was in cognition
in 1994 a special issue this volume 50
of cognition and she said if children
are endowed innately with abilities to
perceive objects person sets and places
then they may use their perceptual
experience to learn about the properties
and behaviors of such entities and then
she makes what some people call a learn
ability argument which basically says I
don't know how you would get here if you
didn't have that stuff in the first
place I think she's pointing to exactly
the right sorts of things that we should
be building in innately capacity to
perceive object person sets in places
I've made a number of proposals like
this and this is sort of a synthesis of
her proposal in mind a couple of
but a lot of money algebraic mine so
representations of object structured
algebraic representations operations
over variables a type token distinction
a capacity to represent sets locations
paths trajectories obstacles and during
individuals way of representing the
affordance of objects spatial temporal
continuity causality translational
invariance capacity for cost-benefit
analysis you can think of this as my
rough open opening bit how much more do
we need and what do we mean by more what
we mean more than we have now most
models don't have much of this except
one that I'll talk about later
most of them ignore it and I think they
get into trouble as a consequence of
course there could be much more innate
miss than that I mean you know I'm only
in the middle of the spectrum tanker
wrote a book called the language
instinct that I recommended a few
minutes ago
it ends with a proposal for 15 adapted
evolved modules that do things like
intuitive mechanics intuitive biology
habitat selection etc etc what we really
need I think in fact are systematic
thoughtful analyses of what happens when
we embed differing amounts of innate
machinery into machine learning so
happens that somebody actually did this
once I there's not a lot of papers in
the literature but here's a great one
called
generalization and network design
strategies it looks at five different
models they differ in their amount of
nativism so the top one is the simplest
kind of neural network you can imagine
it doesn't even have a hidden layer so
it doesn't have any nonlinearities and
by the end the author has invented this
brilliant thing called convolution that
completely changed the world it builds
in the notion of translation invariants
essentially wiring it into a neural
networks we can recognize an object in
different places without having to
experience in all of them of course when
the conclusion was as expected
generalization performance goes up as
you have fewer free parameters to deal
with importantly as the amount of built
in knowledge goes up the author of that
is sitting over there and will join us
in a moment so to wrap up so far and if
you have a little bit more to say we
don't know precisely what is innate in
people even if people are chock-full of
innate stuff it's possible we wouldn't
need an eighth stuff for machines but
sheer bottom-up statistics hasn't gotten
us very far on an important set of
problems and these are the ones I
pointed out in that New Yorker article I
mentioned earlier five years ago
language reasoning planning common sense
even after 60 years of neural network
research even after we have vastly
better computation much more memory much
better data and I think the field is
invested disproportionately in models
the lack principle innate machinery nine
plus percent of papers at nips for
example which is probably the leading AI
conference at the moment don't have to
say anything about Anita so maybe it's
time we given a TAS a chance I want to
say a little bit about unsupervised
learning because I'm sure there's gonna
be a focus for y on unsupervised
learning can mean different things so
one is like I show you a bunch of
animals and you know just wanted at a
time not so neatly classified as this
and you use a technique like clustering
to try to figure out which ones go
together that's not probably the sense
of unsupervised learning that Yan will
talk about today but he's very
interested in unsupervised learning
systems as a way to get to common sense
what he has in mind is basically you see
videos you see a bunch of frames of a
video and you try to predict the next
frames of the videos you don't have a
teacher there kind of telling you about
the world you just observe the world
watch it go forward and try to get
better and better at prediction I'm not
sure you can do that arrow Simoncelli
and I were talking the other day he gave
a great quote which is you can't model
the probability distribution function
for the whole world because the world is
too complicated so maybe odds approach
can work but I think there's reason to
be skeptical in terms of how complex the
problem is itself there are related
system neither you nor I like very much
but it show a problem that I think might
arise so this is deep Minds a3c
algorithm which is like the Atari game
system thing but faster and my company
geometric intelligence trained it on a
kind of slalom race for a race card it
learned from pixels as as they famously
do and then we played a fiendish trick
which was we moved around the obstacles
and it crashed into the obstacles
repeatedly what is the lesson from that
well the lesson is if you learn things
from pixels you tend to learn things at
the wrong level of abstraction so what
you want to learn is like going in
between the posts but the particular
system that I'm critiquing we did not
mind you yawn system doesn't really have
an abstract representation of a post or
a flying or anything like that when I
talk about unsupervised learning which I
do think is a good thing but in a
different way I think about my daughter
Chloe she's sitting here at a Whole
Foods in Vancouver over the summer and
she notices this is a reenactment the
second time she did this I didn't have
the presence of mind to catch her the
first time but she reenacted it for me
so she looks at the
chair and she says I mean I'm gonna you
know put words in her mouth that she
naturally say she looks at the chair and
she says I wonder if I could climb
through that now this is not 10 million
trials she never saw the Dukes of
Hazzard so it's not copying a television
program and she tries to do it so she
has AI hypothesis about what she wants
to do she's thought about the apertures
of the chair her own body she actually
gets stuck and then she does problem
solving how can I get my arm through and
eventually she makes it so unsupervised
learning as currently understood in
machine learning
I don't think does anything like what my
three-year-old was doing here it's not
about forming goals or plans
it's not about determining affordances
it's not about problem solving
I'm not against unsupervised learning I
suspect we'll talk about it a bunch but
my bet is that it will only succeed if
it's performed using a system that has a
richer set of primitives and
representations than just pixels and
that's where the innate structure comes
in so last thing I'll say the last
little section and I'll finish on time I
guess is there's actually a ton of an
anus in every neural networking so the
real issue is not even how much but
whether we have the right kind of an
agent so I made a list of some of the
things that are neat to every neural
network model so every neural network
model has an architecture how many
layers with types of layers I'm the
units in each layer how do those units
interconnect they're a bunch of
representational choices what should the
inputs units stand for in the 90s people
played around with almost everything but
pixels because they didn't have enough
computational power to do it with pixels
nowadays they can use pixels on the
input but it is a free parameter and we
saw it in the go paper we saw input
units for things like the number of
liberties a stone has on a go board
which are not pixels network dynamics
what is the activity function of unit so
unit tabulates a bunch of data and then
it makes a decision do I have enough to
fire well there's free parameters there
there's free parameters about how you
should change the unit's over time
depending on whether they make errors
what algorithms should do the training
there's also a free parameter it's
almost like innate to the teacher so all
these models have a teacher that provide
input to the models and people play
games with that so the the deep mind
paper with language a whole section of
the guys talk was about what he called
curriculum design or something like
curriculum design which was basically
like we have to teach it on the simple
problems before the hard problems or it
doesn't work well I didn't have to do
that with my three-year-old I just
talked to her and she learned the
language same with my corner
the real question is what kind of a
maintenance do we want so if you haven't
seen this xkcd cartoon I well I guess
you get to see it now it's making fun of
the kind of state of the art of the
machine learning and I don't think it's
entirely unfair it says one guy says the
other this is your machine learning
system the other guy says yeah you pour
the data into this big pile of linear
algebra and then you collect answers on
the other side yeah the guy says well
what if the answers are wrong and the
guy says well you just stir the pile
until they start looking right this is a
lot of what happens in machine learning
and it's a kind of claim about nativism
it's a claim that you need to write
piles of linear algebra in order to
predict the particular piles of data
that you've got
here's another theory is they we want
the kinds of things that kids have which
are representations and primitives that
are built for comprehending the behavior
of objects and eight entities the
physics of the world so you should be
able to look at that and even if you
haven't seen videos of twisty slides as
we call them in my house with that exact
shape you should be able to predict
which what's gonna happen next
and there you go thank you very much
and now young laocoon great thanks very
much Gary now please welcome down the
cone all right three it's very nice to
speak after Gary I get the advantage of
taking a few minutes to reflect about
what he talks about really thankful that
you use a lot of my slides and quotes
all right so that they'll say I need
more innate machinery the answer
probably is yes
but the answer is probably also not
nearly as much as what Gary thinks so I
have a confession to make which I made
to some of you a few months ago when
David organized the meeting on the
ethics of AI which is that I got
interested in machine learning through
philosophy I was a young student in
engineering and I stumbled on this book
the debate between Chomsky and Piaget on
language learning and creation and trans
be brought their teams of you know
people to argue for the other side and
there was you know a lot of arguments
for you know machinery for language and
of course Kesha is an advocate of
structure in learning and stages of
running etc but in that in among his
team was a gentleman by the name of
Sigma peppered with computer scientist
mathematician at MIT who had worked on
the perceptron which was sort of the
original model from the 50s that was one
of the first model that was capable of
running and I'd learn about the
existence of the possibility of machines
learning at the time and that's what got
me into the field so I got into machine
learning through photography but some of
you I guess that's interesting I you
know but it has a special place in my in
my mind so I think there's a number of
questions we can ask ourselves and
there's at least a few things that the
list of seven things that Gary mentioned
there is one point that you kind of said
implicitly which is all of these AI
systems that we see none of them is real
AI in the sense that and this is a quote
that I I stole from Josh Tenenbaum that
he said at a conference on competition
or neuroscience just a few weeks ago in
at Columbia and this is true so current
AI in the form of deep learning or
various other implementations of AI is
not true AI in the sense that it's
nowhere near the capabilities that we
observe in animals and humans and so
none of the techniques that we have can
build representations of the world
either through structure or through
learning that are you know any match for
what we observed in animals and humans
so I would agree with with Gary in the
sense that we do need for AI we need
systems that have representations of the
world that have the potential
possibility of simple manipulation that
have abstract representations of various
constraints in the world the question is
all those learned and how much prior
information are required for running
them and I think where we differ perhaps
is how much we need to build in for
those structures to emerge so all of my
career at least in when it comes to
machine learning and AI has been trying
to find the minimum amount of structure
to allow a machine to actually learn
things and a reasonable amount of level
of performance and I'm a scientist as
well as an engineer and the AI I think
one way of validating whether such
concepts are are good is to actually
build machines that work based on those
principles and then sort of use that as
evidence for the necessity or non
necessity of structure so the history of
AI over the last what his certain parts
of AI over the last 30 or 40 years in
areas like speech recognition like image
recognition and like natural language
understanding has been to go from more
structure to less structure there was a
famous quote from Fred jellineck who is
a pioneer in speech recognition at IBM
and we eventually went to Johns Hopkins
and what he said was you know we had
linguists in our in a good to do speech
recognition to sort of you know have
some knowledge basic knowledge about
about language and every linguist I fire
my error rate goes down by by ten
percent now
that said I apologize to linguists in
the room it's not my quote and he
actually claims he never said that he
said actually I'm actually too nice to
fire anybody but but this works are
reported as kind of a half-choked well
what did we what it reflects what it's
the the indication of this is the fact
that the less structure you put in the
system and the more you rely on learning
and data the better it works and that
that has been certainly my experience I
have any experience and you can just
observe the evolution of of AI over the
last few decades and you will see that
trend in very obvious ways so what a I
really means today where the reason why
we hear about AI
I mean AI is a field that has you know
history of values fall starts and
various contributions that after a few
years work we're called differently than
AI so for example you know the algorithm
that you are GPS uses to plan a path
between two to series that's in every a
textbook we don't consider this anymore
it's just you know a star algorithm or
whatever to do path planning tree
exploration to play chess that's also in
every a text book but you know it's just
tree exploration now we kind of know how
to do this in recent years AI has become
synonymous with learning and it's not
used to be this way there is a whole
area of AI that has nothing to do with
learning doesn't mean it's not
interesting but the reason why we hear
about AI in the last five years so much
is because of learning and particularly
deep running now what does that mean it
means supervised learning so super
version consists in training a machine
in a similar way that we would train
that we would show a picture book to a
child so except is considerably less
efficient so you have a machine which is
basically a parametrized function that
has knobs on the only thing you can
adjust that you know basically determine
the input output function of the machine
and you want to train the machine to
distinguish car some airplanes usually
dozens of examples of cars
thousand examples of airplanes and every
time you show a car and the machine
doesn't say car you adjust the knobs so
that next time around you saw the same
picture the iput is closer to when you
want let supervised running you
basically tell a machine the answer you
want and it wants to associate the
to the output so this very kind of
input/output there's no sort of internal
state to the system so it's okay for
perception but probably not for anything
else there's no reasoning or anything
like that and it's supervised in a sense
that you need to have data that has been
annotated by by humans and the problem
with this is that it limits the type of
information that the machine can learn
about the world it's basically things
that humans have have labeled so that's
a very strong limitation but it used to
be what-what-what-what the appearance
although the preponderance the reason
recent preponderance of machine learning
has caused in the field of computer
vision speech recognition and other
field of this type is is the ability to
run tasks end to end with very little
sort of hardwired machinery in the
system there is structure but is very
little sort of hardwired machinery unit
so it used to be that to build a pattern
recognition system using supervised
running you will have two kind of
hardwired a lot of structure in the
what's called feature extractor that
takes the raw signal and turns it into
an appropriate representation that then
it's simple enough for simple learning
algorithm to figure out and over the
last several decades the the deep
learning model which consists in
basically stacking modules which are all
trainable allowed us to train those
machines end-to-end basically without
having too much hard wiring so the
that's an indication of the direction
the history of you know removing hard
wiring as much as we can removing innate
structure as much as we can over history
and every time we've done this which was
enabled either by new algorithms larger
data sets or faster computers the
accuracy of pattern we can choose says
pattern recognition system that's gonna
that's gonna be that said as Gary
pointed out there is a lot of structure
in those in those systems they are not
randomly connected or fully connected
neural nets there is a lot of structure
and in fact I cannot build my career
around the idea of building structure
into neural nets something called
congressional nets and I don't know how
you managed to dig up to the first paper
on this but that's pretty much the first
paper on this yeah it's on my website
but you know it's it's far down the list
so nobody can
looks at this and commercial nets are
actually inspired by biology so weekly
inspired by the architecture of the
visual cortex so there is structure
individual cortex in the way so this is
Nemo classic work in neuroscience Nobel
prize-winning work as a matter of fact
back and with all about the architecture
of your cortex that you know identify
simple subtle complex cells and and sort
of a sort of a hierarchy of layers and
people have had the idea of implementing
computer models that of that that sort
of were capable of doing very simple
pattern recognition tasks one in
particular is Fukushima is new incognito
on which is gonna represent it here on
the left so this idea of hierarchy of
local connections shifting variants as
very was was mentioning is that the
basis to Congress on that and that's
what makes it work and a lot of the
applications of AI that you see today so
things like you know you talk to your
phone and it recognizes your voice it's
a commercial net the if you go on
Facebook and the post is being
translated from one language to another
one
chances are it's accomplished on that we
don't do it for all kinds of languages
but for common culture languages
actually covers on that your image
images are being recognized as
congressional net the car your car is
you know drives itself on the highway
it's a convolutional net doing the
vision so this has this is used a lot
and it's just a little bit of structure
it's not a huge amount of innate
machinery and that's enough so my quest
has been to minimize the amount of in
machinery that will enable the machine
to learn with whatever data we have
available and you can recognize
characters multiple characters in the
last few years people have figured out
how to implement this on GPUs and run
this on object recognition and those
things can essentially recognize you
know obscure breeds of dogs and you know
species of plants and and of birds and
things like this two degree of accuracy
that is higher than most people in this
room unless you're a birdwatcher so
those things are incredibly accurate on
kind of standard data sets but there are
subject to failures and some of which
are not so great this
sort of a so I'm gonna kind of
contradict something that Gary said it
said you look at the newspapers and
nobody has any principled idea that you
know what kind of machinery and
machinery it should be there it's it's
all about you know learning methods et
cetera it's actually exactly the
opposite
almost every paper is about new
architectures new ways of connecting
neurons and modules with with each other
in such a way as to solve a new problem
almost all of the papers are that this
so here's an example there is the
evolution of the architectures that are
used particularly shown that type
architectures that are used for object
recognition over the last several years
they all have a name Alex Knight bgg
Google net or Google a net resonate
dense net I mean there is a menagerie of
all those things and they there are all
special architectures so this works very
well for for vision it works very well
for speech recognition works very well
for certain types of language
understanding and we might ask why and
the this idea of having layer is layers
is structure the idea of having local
connections in structure we might ask
why does this work and it's probably
because the world is compositional so
the perceptual world is compositional
the world itself is compositional
objects are made of parts and parts of
sub parts of parts of motifs motifs of
particular arrangements of edges and
things like this I mean a lot of work on
this individual psychology as well as
neuroscience and various other fields
and so to some extent the structure is a
reflection of the structure of the world
but you know it's inspired originally by
a very weakly by by by neuroscience so
here is so far we can push this model of
supervised running we can push it quite
far we can do things like label every
pixel in the image with the category of
the object it belongs to which means
that we can identify a piece of a road
is traversable or not which means you
can build several cars and that has
societal consequences it works it
requires a lot of engineering but it you
know it works and we'll have this in
every car within 10 years 20 years
something like that
conceptually very simple neural net
architecture with some structure but not
a huge amount of structure just you know
a lot of training can do things like
basically recognize that we are
objecting in an image outline them and
you know figure out exactly what they
are and there you know again there is
structure in those networks but a very
very small amount of structure when you
think about it in terms of I don't know
the equivalent amount of information
that would be required in a a kind of
virtual genome of those machines it's
actually relatively small the the length
of the program that specifies the
architecture of those systems is very
short it's one page or something like
this you can do its are pretty amazing
things of you know you can push the
system quite quite fat quite far in
terms of performance you can train them
to do language translation it wasn't
mentioning before so there's number of
questions that pop up there is this
sufficient for building truly
intelligent machines and as I said
before I don't believe that's sufficient
and I'll point pinpoint a few problems
that I've identified our own along this
dimension so they're questions now we
can do supervised perception as I showed
what about things like reasoning or you
know episodic memory right we read a
text and we can sort of answer questions
about this text or do we remember how do
we kind of you know represent the story
that we just read you know head allows
us to answer questions about it or to
summarize it or or to kind of imagine a
follow you know a follow up to this to
the story and that you know ultimately
asks you know do we need to build
special structure for all of those
things into the machine or well the
Machine run this spontaneously so let's
see all nature nurture debate of course
which is where we here it's a couple
other questions so this is another thing
that Larry talks about in some of these
books there are some of us in the field
of machine learning deep running in
particular or neural nets and and people
in neuroscience also who work under the
hypothesis true or false that there is
some underlying principle in the way the
brain
although the the brain sort of organized
itself through perception and action and
one question for example is that is
there a single learning algorithm we
can't really call it an algorithm that
sort of learning principle or method or
or or procedure in the cortex or are
there more like 50 or is the question
nonsensical in the sense that it's just
a big collection of cruge and hacks and
there is no way there is we will find
any underlying principle so I'm you know
whether it is the hypothesis that there
is a single or a few learning algorithms
that in in the cortex is true or false I
work under the assumption that it's true
and the idea the methodology being that
we try to build machines that have a
relative you know a minimal number of
underlying principles and moving parts
that exhibit behaviors that we expect
from animals and machines so we don't
really know the answer to this but it
would be nice if there was an underlying
principle so perhaps an analogy to this
is flight
there is another lying principle to
flight whether this applies to airplanes
or birds or helicopters
it's the same principle seminar in
principle right the Bernoulli principle
for generating lift by pushing something
for the air and the details might be
extremely different but there is an
underlying principle so one question I
ask ourself is is there an underlying
principle behind animal and human
learning or learning in general or AI
you know intelligence in general whether
it's artificial and natural and I think
it's a perfectly good working hypothesis
to assume that there is the other
questions all right
but as Gary mentioned and there is at
least what you experts here who has
thought about the question of you know
getting machines to acquire common
science any David here is you know it's
thinking about this for 30 years
something like that more perhaps as you
know none of the method methods that
we've developed the learning methods
will develop whether it's supervised
or reinforcement learning so far are
powerful enough to allow machines to
develop enough knowledge about the world
to kind of have common sense so when you
talk to your you know personal assistant
you know Siri or at Excel Google now
whatever it is it has a very very
shallow understanding of what what you
tell it and in fact most of those things
are completely scripted by humans there
is very little learning going on there
those systems have been built by hand
and engineered not trained or there's
very very little training this training
in the speech recognition part but no
training in the kind of reasoning and
question answering part essentially and
if you deviate from the kind of
scenarios that they've been built for
they can answer the either don't
understand the question or they tell you
a joke or you know so we we haven't
solved this question common sense so
first question in my ask is what is
common sense and we may you know it's
probably the very different opinions
about what common sense is in this room
but one one way that I think about it is
common sense is the ability to fill in
the blanks so we have a partial view of
the world through our perception and
because of our knowledge of how the
world works we can complete the
information that's missing so for
example when we observe a particular
situation we can probably predict to
some extent what's going to happen next
and that's because we have predictive
models of the world that allows us to
make those predictions so we've learned
by observation and also by interacting
with the world so predicting the future
is a particular special case of filling
in the blanks right now you're you're
seeing the you're seeing my face you
don't see the back of my head but you
can you know even if you haven't seen it
before you have a pretty good idea what
it looks like because you kind of have
an intuitive model of what human human
heads are supposed to look like you know
we all have a blind spot you know in a
retina and we're not conscious with it
because our brain cannon fills in the
missing information there so so
predicting any part of information from
any other part of it is or the ability
to do this
at a kind of high level is I think what
kind of domain would I say instance or
manifestation of common sense and and we
see this in in children right so
children learn very basic facts about
the world extremely quickly so it thinks
like you know object permanence kids
learn this you know at the age of two
months roughly the fact that an object
is still there even if it's hidden
you know Picabo is so funny because you
know you hide and you're not there
anymore right and you know we're not the
only ones to have some sense of some
level of common sense Sirius is along
with on here you know they don't have
any language they're not even social
animals they leave each other eyes and
this guy is doing a magic trick on him
and the honky Tonk is running on the
floor laughing right because the object
in the car disappeared and we didn't
notice so obviously he has some all of
the world that was broken here that made
it really funny and you know
psychologists so this is a couple of
slides that I borrowed from a manual
depuis who is going to neuroscientist in
Paris where he kind of tries to figure
out at what stage babies learn basic
concepts intuitive physics for example
for the world and you measure this by
looking at babies and and and how
surprised they are by something so you
know before the age of eight months if
you push a little car basically you make
it look like it floats in the air babies
like say sure that's the way the world
works no problem after eight months they
look like the little girl here at the
bottom left they they're like this and
they say what's going on they don't say
it but you know you think it and so we
made this chart of you know what stage
do we learn all those concepts and you
know the question is whether those are
pre-existing concepts that just happen
to kind of during development appear at
a particular stage or whether they are
actually learned from from data if you
won't from scratch from interaction with
the world or from observation at that
point and so hitchhiking happens within
minutes
and you know we actually have
computational models that basically can
try to track faces within minutes of
training or real-time training so this
could be learned extremely quickly in
humans you know the identification of
inanimate objects versus animate objects
you know biological motion of the
permanence stability of support gravity
so we learnt gravity about around eight
months the fact that objects are not
supported will fall and you know there's
kind of a similar chart for sort of more
linguistic and social concepts so you
know babies and certainly animals learn
this by just observing the world by
playing you know playing in it
interacting it with it but but human
babies are kind of helpless in the first
few months they don't interact the world
with the world very much most of what
they learn about the world is by
observation very little action actually
and how do we do this and this is
probably what allows us to learn for
example that the world is
three-dimensional right so here is an
example of how we can learn the notion
of depth using predictive learning so by
training ourselves to predict what the
world is going to look like when
something happens so if I move my head
20 centimeters to the left the world
changes according to sir parallax right
objects are near can I move more than
objects that are far and the notional
object can emerge from this notion of
depth emerges from this so if I if I
want to train a machine or myself to
predict what the world is going to look
like when I move my head 20 centimeters
to the left inevitably whatever entity I
train is going to have to represent the
notion of death because that's the best
way to explain how how the world changes
okay so that's an argument for the idea
that's this sort of you know way of
training training yourself by predicting
the future are pretty key missing
information that you eventually observe
is is how we learn so much about how the
world works
in fact that's an argument that Geoff
Hinton has made many times that we we
need if we need to if we want our brain
so brains are extremely powerful you
know statistical engines if you want or
learning machines with many many
parameters and
we want to have enough data to constrain
or brain to learn anything
all right needs to predict everything
from everything else it needs to have
this ability of credit in the future
from the past or train itself to predict
the future from the past predicts you
know the hidden part of the world from
from the ones that we see and eventually
kind of look around to get what the
correct answer is you know things of
that type so in the in the context of
machine learning there is sort of three
categories three paradigms main
paradigms of learning we first met
running supervised running which I
mentioned and unsupervised learning or
predictive learning within the way I
described it and reinforcement running I
I sort of used this analogy that
reinforcement running is like the the
cherry on our cake if intelligence is a
cake we france when tony is the cherry
on the cake because the amount of
information you ask the machine to
predict is basically just a singer a
single scalar value of how well is doing
not more and so it's getting very
detailed feedback from the world which
means that as Gary correctly mentioned
if we use pure reinforcement running to
train ourselves to drive we would
probably have to crash in a tree about
50,000 times before we learned that it's
the bad idea and that of course doesn't
work and this is the state of
reinforcement running today it only
works for games doesn't work in the real
world and the reason is because it
requires so many trials that you have to
run the real world or the world that
you're interacting with faster than real
time otherwise and you have to do things
that are not dangerous that won't kill
you so we cannot train start driving
cars with reinforcement running today
because of that problem now why do we as
humans why don't we have to crash into a
tree 50,000 times before we realize that
idea because we have good predictive
model of the world we know what's gonna
happen you know if we you know
deviate from a proper trajectory and and
and hit a tree we had this ability to
predict in fact I would argue
intelligence the essence of intelligence
is the ability to project in fact this
is something that one of the founders of
reinforcement learning said a long time
ago which Saturn in 1991 proposed an
architecture where basically a machine
would learn
a model of the world first and in use
this model of the world to plan I'm
gonna skip ahead a little bit and can I
go to the I mean I've kind of given the
punchline this is famous poem right the
revolution will not be televised it will
not be supervised that's for sure and
and so what's missing today I think is
not more structure although people do
work on in its structure prior prior
machinery yeah in it machinery for
machine learning systems in fact some of
those actually wanted to show quickly
there is a lot of work on particular
architectures on their own that today
that are designed for handling language
so language seems to be you know it's
kind of a segue sequential signal with
structure in it and it seems that you
need you know particular type of
structure to handle language although
you know for translation we use
commercial nets which are very similar
to what we use for images so it's not
like we need to put in there anything
that's really special about language but
there is an infrastructure but my main
argument is that what's missing is not
so much the structure although our
meeting structure what's missing is
principle that will allow our machine to
learn how the world works by observation
and by interaction with the world
learning predictive world model is what
we're missing today
and in my opinion is the biggest
obstacle to significant progress in the
eye and structure will come thank you
thank you and by the way to stay a
comment on the topic of predictive
processing that's actually the topic of
our next debate in the in the spring
it'll be debate on the role of
predictive coding and predictive
processing in theories of perception
involving David Hager Lucia Maloney Andy
Clark and Michael briscola dates soon to
be announced so look forward to that one
okay now we're gonna have Gary who has
eight minutes respond so as I predicted
we agree about a great deal I think we
agree about the importance of having
better predictive models although I
didn't stress that but I certainly think
that it's true we both agree that our L
reinforcement learning as practice now
has no chance in doing that that it
takes many many to many trials can't do
it in the real world we actually
disagree on the cognitive development
literature so I actually think that the
evidence for object permanence being an
age is pretty strong in any rate I think
it's very clearly present by four months
there's lots of experiments by little
Kieren a buyer Jean can win and so forth
that I think are very much consistent
with object permanence being there at
four months and it might actually be
present even earlier with the right
methodology so my own most famous
contribution to the developmental
literature was a experiment that showed
that babies could learn rules at eight
months I was published in science in
1999 and what happens in that field is
people come along and say well I can
find kids who can do it even younger son
an ass so there's now a paper showing
that the rule learning stuff that I
described can there's missing control
but it seems to show that newborns can
do it at the same time I want to add by
the way that just because something
isn't there at Birth doesn't mean it's
not innate so facial hair in in human
adult males appears you know relatively
late in life there's a lot of maturation
I believe in in the brain that means
some things happen especially in humans
because of the birth canal relative to
the head size
it just gets baked after you're outside
the womb so if you do see something
eight months it's not a knockdown
argument that it's actually learned as
opposed to developing so stereopsis is
interesting in this regard like it it
happens faster in monkeys than humans
but I don't think it's because the
monkeys are quicker studies I think it's
because there's a different
developmental clock the most
Fortin place where we actually I think
disagree is on the significance of
history so and in two ways so you're
making an inductive argument you're
saying that because history shows that
sort of less and less allows us to do
more and more which I believe is true in
the case that you described that we
shouldn't add more and more so it's true
in the case that you describe so for
perception and pattern recognition maybe
not for understanding a scene but
identifying an object I think it's fine
to have a deep learning system acquire
the features you get into problems in
complex scenes so I didn't show my
favorite scene of a captioning system
miss identifying a parking sign with
stickers as a refrigerator filled with
food and drinks so there are still some
bugs in the system and I think you
technology smiling so it's not perfect
but it's pretty good it's a pretty good
architecture for that and I would take
your point there but I don't think even
there that it's fully correct that less
is more so it's true there's a lot of
learning but I think historically the
most important thing and getting object
recognition to work is that paper by you
that I put up on the screen right if you
take away convolution then you're back
to multi-layer perceptrons and they
don't really work right I mean we know
they don't really work you showed it
nicely and there's a million study since
then they'd have shown that in one way
or another so what I was mostly arguing
about in the algebraic mine was
multi-layer perceptrons that didn't have
convolution there's one thing I would
change whether two things I think one of
the two things I would change about that
book if I wrote a revised edition there
would have paid more attention to
convolution which I knew about dimly but
didn't at that time appreciate the
significance of but that's a major piece
of an eighth structure that has really
changed the field I you're against made
here were invented and is with the
students the LST M which is a way of
adding memory to a particular kind of
neural network and that's used
everywhere I mean his technique in your
two techniques are the backbone of most
of the field but they're new pieces of
innate structure Schmidty humors I still
don't really understand I don't exactly
understand what in Ellis camp is doing I
know roughly why it works is in terms of
it it allows you to have memory and of
course the memory networks or another
way to do at a different scale
convolution is just transparently a
representation of translation and
variants translated into a neural
network it's beautiful all I want is
more of that stuff and I think it's
actually worked and the trend actually
goes the other way so that's the first
half of that argument the second half
the argument is what works for
perception doesn't work for cognition as
far as I can tell so you can fake it you
can confuse yourself and I think this is
the entire literature is neural network
cognitive science literature is based on
a confusion here so if you have a
limited data domain you can emulate
abstraction by memorizing all the cases
or a bunch of key cases and generalizing
within a space of training examples that
you've seen before but if you move out
of that space of training examples
things fall apart and so when people
build language models using neural
networks they do really well on some
sentences and then they get these
ridiculous errors like the refrigerator
with food and drinks this is always my
view I can't prove it to you but it is
always a matter of generalizing beyond
the training space so I did this demo I
think you know about it in my 2004 book
algebraic mind
sorry 2001 lookout to pick mine in which
there was a identity function so we
trained in their own network to
represent identity on a subset of even
numbers and if you test it on another
set of even numbers not in the training
set it appears as if it is learn the
function of identity but then you go to
odd numbers which have a bit that the
network hasn't seen before and people
say that's not fair the network but a
person can do this that's what my 99
science paper with the babies was all
about the babies can do this kind of
thing babies can generalize outside the
space of examples but the network can
you give it to even numbers and it can't
go to odd numbers you might actually get
that particular problem with convolution
incidentally but in general there's a
set of problems where what we know is
something deeply abstract and you can
fake it for a while by having a lot of
examples but you move outside that space
of examples and it doesn't work anymore
these problems are fundamentally
different the point of the algebraic
mind is these problems I took that quote
took a quote that says thought is like a
count of algebra which I think William
James attributed to Barkley there's an
algebra to how we think about for
example language so we know things for
noun phrases and verb phrases and we can
substitute things in arbitrarily common
sense we know that any kind of container
if I put a spider in and I seal it and
there's no hole I can shake the
container up Thank You Ernie Davis for
the example Ernie and I have
collaborated some some work on common
sense and I'm stealing for example you
shake you shake the thing it could it
could be you know circle-shaped it could
be elephant shape it doesn't matter I
don't need to do forward simulation like
Josh 10
would do where I need to know where
every particle of the vases I know what
a bottle is and I can make inferences
over this broad class so far anyway the
kinds of architectures would have worked
great for perception just have not been
able to get the job done I think you
kind of agree with this for reasoning
for language for planning and so forth
there's a weakness there I think it's a
principled weakness and I think the
inductive argument even if I agree that
it was run properly wouldn't really
apply here it's a different set of
problems which is why
60 years no progress 60 years no
progress in common sense no progress to
my mind in open-ended conversational
interface ten thousand or ten million
times more computation than we have
before and still no progress there it's
cuz it's a principally different
problems the inductive argument does not
apply and probably I'm out of my eight
minutes and so I'll just leave it thank
you very much
so I'm not going I'm not going to
disagree on the on the facts but sort of
more on the on the the the course that
we should we should employ in to proceed
so it is clear so we agree on the fact
that we don't have the underlying
paradigms that would allow us to get
machines to run common sense that was
kind of the main theme of my talk we
certainly agree on this but we we do
have a new paradigm today that only
popped up in the last few years or
became practically the last few years
around deep learning that at least gives
us a chance to have a shot at it and
people really haven't tried very hard
yet this is a lot of things that my
personal research is on for the little
time I have had to actually do research
myself or with my students at NYU is
about basically you know unsupervised
learning on supervised models of the
world by observation with a hope that
eventually when we figure out how to do
this properly we'll we'll have machines
that have at least some notion of into
18 physics if not common sense the one
thing that you said during your talk
which I agree to which was a quote from
a woman Shelley who is somewhere here
here is that the the problem with
learning intuitive physics and
predictive models of the world is that
the world is intrinsically unpredictable
you know either because it's actually
stochastic because the economy can be so
just because it's not fully observable
and and for that reason are predictable
so if I put this pen since you append by
the way thanks right thanks for leading
it here because you know I'm going to
use it as a prop so if I put this pen on
the table I use this example all the
time and I tell you I'm gonna let go of
my finger you can tell the pen is gonna
fall you probably can't tell in which
direction it's going to fall and every
time I do the experiments going to fall
in a different direction so if I if I
use a traditional supervised learning
technique to train a system where the
input to the system is here is the
position of the pen
and the output that the system should
predict is what is going to be the
position of the pen after I leave my
finger
there's no way the system can make an
accurate prediction it cannot it cannot
work if it has to make a single
prediction of where the pen is going to
fall because it's essentially
unpredictable but it can predict that
the pen is going to fall so if you are
mathematically inclined you say well
what you want to what you want a machine
to model is not just a single prediction
but a distribution the probability
distribution of a possible outcomes and
as arrow said it's very difficult to
model probability distributions of the
real world because it's too complicated
and so what we've been trying to do is
figure out alternative ways of
representing multiple outcomes for for
for a predictive system and still be
able to try to predict the future to
some extent or fill in the blanks in the
presence of a certainty so that's where
the technical questions are and where
Gary perhaps sees a failure of the last
60 years I see an opportunity that only
appeared in the last few years of
actually being able to attack this
problem first of all being able to
formulate it and second to actually
being able to attack it so yes there's
been very very little progress in the
acquisition of common sense by a machine
over the last 50 years but there is a
new set of techniques that I think you
know are giving us a chance now back to
the original topic of the discussion I
think again in my talk I said that we do
need more structure
we certainly need structure for we seem
to have as humans the ability to kind of
reconfigure or or prediction engine if
you want to any new situation that we
encounter so we don't have a little
predictor for every situation we've ever
faced we have we seem to have some sort
of reconfigured reconfigurable
prediction engine that we seem to fit to
the situation at hand and things become
it's when we do things very often that
they become subconscious I mean maybe
some specialized hardware is is devoted
to it always trying to to do the task
but at least when we do a task for the
first time we kind of vary
consciously do this task well we don't
know how to do this but it's probably a
lot of structure we had to build for
this to be possible and I say again
almost all the papers you see at lips or
you know any machine on in conference or
revision conference or neural or natural
language understanding conference is
about architecture and innate machinery
or prior knowledge if you want all
structure that's that's what a lot of
people work on so nobody in the business
is going to deny this this is really
what's going on but in addition to this
I think we're missing the principles for
can you hear me a couple people are
leaving we'll wait it will say wait a
second for them to leave once those
people are out of the room I'm going to
concede a point sure you want to go
right so the point I'm going to concede
T on is yon is optimistic about doing
unsupervised learning he's made the
point that he can only really do it now
there was no computational
infrastructure essentially to do
unsupervised learning from video before
I think that that's right and I think
that gives some reason to pursue that
research program I still wouldn't do it
the way that Yann does it but I think
that that's a fair point we're much
better positioned to explore it now on
the point of architecture at nips it's
sort of right and it goes back to the
last slide that I have the architecture
at nips yes 90 percent of the papers
have fiddling around with architecture I
should say it in a nicer way I'll come
back to that later but it's mostly kind
of using the same Lego blocks over and
over again I mean there's certainly some
innovations there but it's mostly kind
of in the same space of Lego blocks and
it's a particular vocabulary
it's a vocabulary I noticed when you had
your questions for AI before they're all
like about gradients and stuff like that
so like there's a big question in the
field of how you don't have an exploding
grant gradients or a vanishing gradient
we talk later about what that is but so
there's a vocabulary like that
and there's another vocabulary like
translational invariance the cognitive
scientists think in and at the very
least I would think it's worth looking
at whether there's anything there so
what you did in this great paper that I
keep saluting is you took translational
they're invariants I don't know the
history about why you did it but you
took something that is certainly
recognizable in the vocabulary of
cognitive science instantly recognizable
to any vision scientist and translated
that into the language of what you do
when you have gradients that's what I
would like to see and don't see naps
well neither history of this is you know
Google and Wiesel 1962 and Fukushima
1982 so that's very clear they were
certainly thinking about translation
today they certainly I mean they did you
know the idea that you have neurons that
look at local receptive fields and are
basically you have a version of that
neuron everywhere in the visual field
that's the idea of translation
invariance and collisions but people in
signal processing also came up with the
idea convolutions in apparently
of biology just because you know if you
do any kind of local processing and you
do the same the statistics of images is
is translation invariant so if if there
is an advantage in detecting a
particular feature and a particular
location in the image it's probably a
good idea to detect it in another place
in the image and you can make this
argument and there are sort of more
mathematical arguments behind it as well
that you know I always been making and
you know stupid matter and various other
people like that so you know can I
interrupt just once all this Balki is
saying is to do the same thing for
object persistence over time to say that
we have conservation of mass or
something like that and then you know if
an object is over here it's probably
going to be in another place nearby over
time it's qualitatively the same it's
more complicated to implement but why
aren't we trying to do things like build
that in as a prior right so let me tell
you a story about this the the first
time I started playing the first few
times I started playing with those with
those networks and I started publishing
about them the first question I had from
people who had some experience in image
processing in computer vision was why
don't you why do you learn the low-level
features in your own networks because we
know what they what they have to be you
know if if we look at biology its
oriented edge detectors that we see in
the primary primary visual cortex area
you could build that in you don't need
to learn it and my response was I don't
need to build it in I can run it and
then it's the same it's exactly the same
algorithm that is used for learning
every layer so I don't need to do
anything special for those things to
emerge and they emerge naturally
whatever the test today with modern
commercial Nets regardless of the test
that you train those commercial Nets to
do if you train them with natural images
you will see oriented edge detectors
appear because it's really intrinsic to
the data it's really more reflection of
the nature of images than the task
you're trying to solve so I was gonna
say and that goes to aesthetics the one
slide that I most regret not having had
I had many slides obviously and I had to
kill many the one that I most regret
killing was was one of Maxwell's
equations and the joke I was going to
make is that physicists want to have all
the physics reduced to four equations on
a t-shirt and I have this feeling that
many people in machine learning are
questing for something else the title of
the slide was physics Envy so I
understand
and the aesthetic which says wouldn't it
be cool if we could get everything with
these four principles but at the same
time I see this huge failure and you you
have a nice asterisk on it but it is
huge failure over 60 years to make
progress on a different set of problems
and so like I think reasonable
interpretation of the literature would
be these techniques are the bomb for
doing perception so like they're not so
good for the other stuff so maybe we
need another equation or maybe maybe
maybe that's where you would actually
say we'd need another other unsupervised
learning algorithm and I'm saying let's
borrow a little bit more generously from
cognitive science right so I don't I
don't think it's fair to say there has
been you know continuous efforts by many
people over 60 years to kind of to do
this the ability to train system end to
end you know you have to kind of walk
before you can run and so just the
ability to be able to recognize images
through a machine learning at the
reasonable performance only popped up in
the last four years and so there was no
point in trying to kind of attack this
problem before because you know who
would have believed any effort towards
that direction nobody was actually
working on it and so it's only in the
last four years that the question of
going beyond supervised running and
perception has popped up it's very very
recent and it's only in the last two
years that people in because it you know
people define is even more recent that
people have thought about unsupervised
running at something that's really
important for you know interactive
running of robots and things of that
type so it's very recent it's not 60
years we're talking two years maximum
and it's on the learning and supervised
running for learning for models for
example for the purpose of running tasks
you know robot learning and things like
this there is a conference called the
conference on robot learning the this
conference has not taken place yet the
first instance of this conference is
going to take place in November that
tell you something right tells you that
the idea of using learning for robots is
very new nobody is really seriously
thought about this for a while I think
now it now is maybe a good time to open
it up to the audience so why don't we
start with Ned here this is a question
for you for en so you're against one
build structure and for another kind of
inbuilt structure the kind you're
against is the one you evoked with the
remark about we do better when we fire
more linguists the kind you're for came
out in the fact that if you stick a unit
in for low-level features one middle
level features high level features so
more sort of separation of modules it's
good convolution is good so what I'm not
getting is what's the difference between
the kind of inbuilt structure you're for
and the kind of inbuilt structure you're
against can you clarify that okay I
don't have any a priori for the kind
that's good and the kind that bad but my
attitude to this is Occam's razor so I'd
like to find the minimum amount of
structure that will still get the
machine to do what I want and the reason
I want the reason I came up with this
sort of heuristic strategy is because
the more structure you put in the more
chances of that stroke you have the
destructor would be wrong that you have
an underlying assumption about
destruction that it will turn out to be
wrong so for example the way a for
example the handwriting recognition
system used to work before we use neural
nets was by essentially analyzing the
shape using a you know algorithm said
that that could be you know designed by
hand measuring things like you know how
many pixels are black proportionally to
the you know within a bounding box and
and measuring the ratio of the perimeter
to the area and then you know competing
for coefficients of the image and you
know all kinds of things like this that
we thought were relevant and every time
you you you compute one of those things
you make some assumption about about
about the nature of the character so a
lot of those features get destroyed when
you add a little noise or when you have
characters that touch and you can
separate them and so you want to
minimize the amount of prior knowledge
you put in because this prior knowledge
limits the maximum performance for this
system
so you want to apply against it so
cancer is
have the minimal amount of moving parts
in your model so that it can be
maximally flexible how comes Razer
doesn't help you have to there has to be
some principle difference between the
kind of structure you're against in
built structure against in the Condor
for just because you're for some and
against others that can't just be a
principle of minimizing and built
structure no it is I mean I wish I had a
magic evaluation form so they told me
which kind of structure is good which
can is bad because then I could you know
write all the 90% papers it's that
actually but but you know nobody has
that we hopefully for the purposes of
this debate to get clear between you on
you know what kinds of inbuilt structure
would count as vindicating one view
rather than the other view for example
my intuition is if the only kind
relevant kinds of inbuilt structure or
analogous to convolutional structure but
you know cool geometric patterns of
connectivity which we use as the basis
for our deep learning then young wins
and Gary loses or what Gary requires is
something that's really different from
that kind of thing in kind it was
symbolic structure that would be if that
would be ideal maybe something weaker
than that would also count as you
winning but can you have to kill eight
the standard for you winning the debate
I mean I don't know if I can fully do
that but I want to at least object to
one piece of what you said so um I think
the convolution is vindication for
nativism and I think that the critical
question is actually mapping so if you
could map all of the things that Liz
belfie and I you know our separate
papers have advocated and how and Leslie
and so forth on causality have advocated
and show that they map onto the gadgets
that you needed to add to the neural
networks to make them work I would take
that as an indication if it turned out
that you couldn't find any mapping
between the stuff that we talked about
and what was there I wouldn't so if you
look empirically at the array of gadgets
that people have thrown some of them
have nothing to do with the kind of
stuff that lives and I have advocated so
there is a really important recent
discovery recent like four years would
ever call the rel you rectified linear
unit which is a new activity function or
translating what happens in a neuron
that
there's nothing to do with anything that
I would have predicted it's cool it
makes things computationally faster on
GPUs but that really properly belongs in
the toolkit of of messing around with
the linear algebra in an
implementational context but if somebody
had said in advance of discussion and in
fact even diesel said and probably some
other people Germans in the late 1800s
it said that you need innately to have
translational invariance and then Yann
walks along and finds a really elegant
way of putting in a neural network that
is not a victory for less nativism if
you have to build it into the model and
it Maps perfectly on to the things the
nativists are saying so I think mapping
is really the key criteria and then you
could look at the list and say do we
need half of them or quarter than or
whatever how many of those things map
onto what we actually need so on your
long list of things that you you might
hypothesize are necessary as per our
structure I would agree maybe on two of
them out of the list of how many twelve
let me put it back up or you mean which
two we can deduce the rest
well maybe translation you variant even
that one you you build not actually you
can't watch that one back now come on
can actually I can so in a sense that
there is no such thing in the the human
visual system right it's not like we
have the equivalent of wedge sharing in
the well this interesting question about
the fovea for example yeah no I mean the
the the translation invariance trick in
commercial Nets it's a way of reducing
the amount of training samples I need to
do in supervised running if we were to
use on supervised learning we wouldn't
need to actually hardwired translation
invariance we would still have our local
connections but not not translation
invariance and in fact the visuals the
visual system is not translation
invariance because we don't have a
constant density of of so it I think it
emerges naturally because images are you
know naturally have translation
invariant statistics and if we used
unsupervised running we didn't need to
hardwire wage hearings that would
disappear okay great so back to David's
question if it turns out that to make
your unsupervised learning algorithm
work you actually have to have something
that maps on to representing objects
sets
laces spatial temporal continuity etc
then I win if it turns out that your
math doesn't have anything to do with
those things and it's just stuff like
rallies to make making it all go then
I'm wrong absolutely I totally agree
with this so Wow happy to get that on
recording so the no I mean absolutely in
fact there is no way I really do have a
debate here if there is no way to do
unsupervised on without putting all
those things we talked about I wouldn't
be the one to to find it because I don't
want to build those things in just so it
turns out you have to build in some
spatial properties like say
convolutional structure possibly if you
have to then you think Gary wins no yeah
there's a way these debates get kind of
boring pretty quickly which is everyone
accepts you need some minutes friends
here I don't think it's boring but let
me ask you on one question then we can
go back to the audience let's move now
to Barbara you began with babies and
presumed certain things about babies and
what they come into the world with why
not begin even earlier because evolution
is some sort of selection and can we
learn lessons from that which is Rihanna
for me or we just both of you you both
begin with babies
I mean evolution is running it's just on
a different time scale and well sure I
mean so so the question is you know for
for the purpose of survival individual
species
I think evolution probably arrived at
different trade-offs of machinery versus
you know flexibility afforded by
learning and so you know humans seems to
be kind of on the on the far scale in
the animal kingdom of adaptability and
that what allows us to you know aa
species to survive in all kinds of
environments are extremely diverse much
more diverse that most animal species
that's adaptability so I think there is
an evolutionary advantage to having a
very powerful learning engine if you
want and the ability to build predictive
models of the world you know to plan
complex sequences of actions to predict
the outcome of a constant sequence of
you know complicated actions that's what
that's what really that's why I said you
know prediction is the essence of
intelligence but but animals have this
to some extent and to a large extent as
a matter of fact and I would be happy if
you know at the end of my career we had
a machine that was as smart as a cat or
even a rat in that respect had enough
you know as much common sense as a cat I
have two things and answered you in
answer to something answered so first of
all there is a sense of learning that is
so neutered of all of its power that it
encompasses evolution I'm not happy with
that sense of learning so I would you
know you well you just said it but I
mean so you said evolution is is just
learning at a different time scale
there's a certain sense in which that's
true but it's a certain sense in which
it just undermines the whole debate and
there's nothing to talk about if you can
compost both you you really want to know
in the case of biology what what evolved
in it and then what do you learn and it
could be different in different species
some things might be an eighth for some
species to learn for some species if you
just have this kind of X let's broad and
encompassing that you can't have the
conversation anymore so that's the first
thing I'd say the second thing is there
is a field of evolutionary genetic
Norrell networks it hasn't succeeded so
far I think in either of our views
there's been some interesting papers
there was one from open AI earlier this
year it hasn't gotten that far but I
think it's deeply interesting I think
the reason it hasn't gotten far is
people are trying to do too much in the
course of like one dissertation so you
have a graduate student who tries in
four years you historically with much
less power than we have now to get
something to evolve something pretty
complicated and evolution is a slow
process it took you know a billion years
to get here and you know you could
evolve the stuff that a worm does and
think that you're on the wrong path
because it's not very impressive and
people haven't pursued the path far
enough what is true of a human is that
in something like a hundred thousand
years we evolved language right it means
a relatively quick transformation from a
previous ancestor but that ancestor had
a whole lot of evolution that already
happened right primate brains are really
sophisticated you can think of their
genomes as this big library of
subroutines and there was some you know
jiggling of the box and recombination of
those
teens and representational formats and
whatever to build something new and
exciting but there was a lot of stuff
already there and so if you your
primitives in doing the evolution is
like I can sort of move this one
connection wait in that one it's
probably not capturing the hierarchy of
genetic processes where you can have a
gene like pack six that controls
thousands of other genes and Cascades we
don't have that yet in the evolutionary
stuff it'll happen eventually but but
there is there is quite a bit of work
now I think the limit of the limitations
of the the power of this idea of using
evolutionary algorithms to evolve the
structure of network is just a
limitation in compute power so you know
companies like like Facebook and Google
have infrastructures with you know many
thousands of GPUs which are the cars
that we use to do the computation and
even that is not enough to kind of
simulate evolution that kind of the
neural net level so we need graduate
students for this so you know the
learning algorithm that's used to train
neural Nets it's called SGD stochastic
gradient descent
there is another algorithm called GSD
which means graduate student descent and
that's deep and that you know the
consistent sort of you know asking
whether student to come up with sort of
new structures for neural nets okay a
next question from problem high so just
trying to keep getting clear on the
disagreement Gary can you say more about
what a mapping is and maybe get happing
yeah maybe give an example meaning in a
sense I guess originally comes from math
that you can understand process a in
terms of process being you can
completely relate the things in a to the
ones and be so translational invariance
can be mapped on to convolution
convolution is just a realization
so translation is an abstract algorithm
there are a number of ways to do it and
I think any reasonable person would say
this is a form of translation of Barings
we could have a sidebar about whether
humans are actually translation averring
or whether they approximate it but you
just look at the algorithm and the
components like you could talk about
translation invariance is this thing and
this thing are identical well here's the
mechanism within convolution to make
sure that they're treated the same way
by having weights that implement that so
it's it's basically saying there's like
a parallel
I can understand this in those terms so
I can map this stuff that I know about
chemistry on to this stuff that I know
about physics I certainly didn't invent
the notion okay question over there
there's two behind you hi um so I know
the example but you both have been
referencing is language with dialogue
systems and with self-driving cars and
also you've also been talking about
babies and like facial recognition face
tracking these are all very social kinds
of problems right so so I mean right now
I mean multi-agent RL is something that
we has been working on we've been
working on that's been slowly growing
same the same thing with people
interested in safety with the kind of
thinking about like inverse RL and these
kinds of motivations so I'm kind of
trying to understand how much do you
think the the social aspects of being
able to work with with people it's going
to be a strong part in this component of
growing or do we need or are we able to
just be relatively agnostic about our
understanding of our even our own cells
so I mean certainly something called
inverse Terrell which women inverse
reinforcement running or which is a
particular way of doing what's called
imitation learning so training a
intelligent agent by imitation with a
teacher essentially so it's not
supervised running in a sense that the
teacher doesn't give the agent the exact
answer but it's I know you know the
answer very but but it's learning by
demonstration so you observe someone
doing something and you infer what
underlying objective function the the
the teacher uses to do the task so this
is obviously a very powerful way of
training a machine or this would be a
powerful weight training machine if we
knew how to do it properly
and there's quite quite a bit of work on
this in the context of robotics on on
the mutation learning but
we can't claim you know what I can
supervise money we can't claim that we
have principles that really kind of work
to the extent that we can observe this
in animals and humans you know with
things like mirror neurons for example
spontaneously appear or do we have to
hardwire it that's a good question for
you know whether innate machinery is
required there so I think there's
probably that's probably one domain what
I would concede there's probably a need
for some sort of you know machinery to
sort of enable a mutation running I
think Gary said what works for
perception won't work for cognition
what's a perception a really special
entity that's an easy target and what
what do you mean by cognition I mean I
was a little bit sloppy about that what
I think really is the case is there set
of problems you can do there are pattern
recognition problems we're fundamentally
what you need to do is classify things
and these algorithms are very good at
that there are lots of other things you
need to do in cognition the line between
perception cognition is actually gray so
I would say that when I interpret is
seen as a whole I'm putting some
cognition and maybe things that I know
about people and rooms and and physics
and all these kinds of stuff at some
point you go from I'm just recognizing
entities that are sort of like stored
examples I've seen before - I'm
reasoning about them roughly speaking
when I'm talking about cognition its
reasoning its language and so forth and
these are the domains where I think
we've made less progress as a field
so my question is I guess one source of
innate information is like the sensory
modalities we have in other ways that we
can interact and sense the world do you
think it's plausible to either you think
it's plausible that we just are not like
we thought it's a hardware problem to a
certain extent but we're not collecting
the right data with the right types of
hardware and that once we have say like
robots that have very exquisite senses
of touch and much better motor control
and so forth
if some of this difficulty will go away
so I don't think so I mean certainly
hardware should make you know well my
progress robotic hardware sensors will
make progress but sensors are already
very good and we have sensors that we
can put on robots and cars that are far
superior to the basic sensors that we
have you know eyes for example the you
know Google's autonomous cars use lidar
and this is an active sensing system
that gives you a depth map of it's sort
of a distance for every pixel in an
image you can do this with we can do
this with our eyes with television but
it only works up to about 10 years you
know beyond that we can't use stereo to
really estimate a distance so I mean
certainly we can use things like if you
have a car that has a camera but also
has a lidar then you can use the lidar
to get you know reliable debts estimate
and then use that to teach the system
the vision system from the camera to
estimate that without need for the lidar
and there's you know a lot of
experiments this in fact I worked on the
project like this about 10 years ago
where we used kind of a hardwired in a
machinery for stereo vision to estimate
depth and then we use this to Train
accomplish on that to estimate that from
a single image and we could use this to
drive a robot in you know off-road in
nature so yeah this kind of stuff you
know touch and you know all kinds of
other sensory modality you know increase
the different views that you have of the
world and sort of allow you to make
better build better models of the world
there's one sentence on it I think
faster hardware is good for the
scientists to evaluate models faster but
I don't think that my brain is so much
better than a whole in terms of sheer
computational power than a whole huge
cluster of Facebook GPUs and computers
and so forth a I think whatever is the
software in my brain or firmware or
whatever could probably be run on
today's hardware maybe you'd be a little
slower or something like that I don't
think it's a principal problem it's that
we don't know what what to get the
hardware to do but still it's good to
have it faster so we can test things so
I completely disagree with this okay I
think the the raw compute power of the
human brain is actually gigantic so I
have more compute power than Facebook
that's great oh you do you definitely do
even you sorry you were asking for it
and what about you yarn if part of
Facebook therefore here is I'm gonna let
you skip back only thing I can tell you
is that I'm the dumbest guy okay
research I only hire people who are
smarter than me so you know we have 10
to the 14 synapses roughly they can't
change state about a thousand times a
second
maybe you one person or brain is active
any one time me roughly you can say the
brain does you know if you kind of don't
care about a factor of 100 or so you
know roughly 10 to the 18 to 10 to the
20 operations per second we don't have
GPUs they can go that far
the fastest GPU cards that we have can
do 10 to the 13 operations per second
we're off by a factor of at least
100,000 and we are off by another factor
of 100 in terms of power consumption so
the brain consumes but how do I have if
they agree yeah and you know GPU card is
250 what's a single a single GPU card 10
to the 13 operations per second look
there's two things one is we have no
idea what the right measurement is for
first room and two is already even if
you sort of stipulate to the facts as
young just suggested them it's still you
can see there's a software problem in as
much as like whatever that measly amount
of computational power is that Facebook
has for some things it's way better than
people like playing go right
so it really does depend on what
algorithm you're trying to perform the
image recognition system that Facebook
runs it actually runs for the image
recognition system on every image that
is being uploaded on Facebook this is
around 1.5 billion times per day great
and person couldn't do that right of
course I really want to ask you on the
last question we're running out of time
yeah let's take one or two more animal s
over there on the other okay so my
question would be to yawn so you know
back to the argument about how adding
more structure to language has been
failing and you know you made the
example about how someone asks you at
some point why not bacon like some
feature you know detection and you're
asked for vision and your answer was
because we can do it so initial language
understanding is specifically throughout
the past you know few years nothing as
revolutionary as object detection you
know success has happened and it you
know you're doing much much better for
like extra many more you know going back
to what Gary was saying pattern
recognition kind of tasks so we can do
better QA as long as the answer is
explicitly and say given you know piece
of text but like for going beyond what
we can see and like overfitting to the
very particular test set that we are
creating we don't see much happening and
so my question to you is what do you
think we can do to go beyond that if not
build on up you know build on top of the
kind of knowledge we already have about
languages structure about like maybe
common sense knowledge so how far do you
think we can go by just you know
inventing new architectures and doing
end-to-end tasks if we want to do actual
national language understanding as
opposed to you know building models
they're just overfitting - intricacies
of the data sets right so there is
actually you know a significant portion
of the worked on it that physical care
research is on language understanding
and dialogue systems and translation
things of this type I would I don't
entirely agree with your premise that
there is you know although there's been
a revolution in image recognition and
speech recognition due to deep learning
that there has not been a similar
revolution in natural language
standing there has what is your example
so an example example is translation so
until about two years ago or yes I think
that's come you know kind of
controversial right I mean machine
translation is not necessarily English
understanding but it's a mapping right
you go from left to right there's a
direct mapping so you don't necessarily
reason beyond what's explicit so I count
that as more in the you know domain oh
yeah we have to kind of be quantitative
about it so you know the same way speech
recognition error rates have gone down
by you know a factor of two within a
year or two when people started using
deep running and then has gone down by
another factor of two or three after
that for image recognition is you know
there is kind of a step function way you
know went down by a factor of two around
2013 and then it sort of kept coming
down so you went you know in 2012 it was
or 2011 the error rate on image net was
26% and and you know as soon as people
started using commerce on that it was
about 15% I know it's less than 4% human
performance is about five so you know
there's huge progress due to the fact
that those things work we have better
hardware to run it better software
better and more people working on it and
so more exploration of the parameter
space and more ideas about architectures
in fact now the same thing is happening
for neural for natural language
understanding and it's been happening
for the last few years where you know
about to about three years ago the first
experiments I showed that for things
like question-answering
translation and various other tasks or
end to end tasks in natural language
understanding like intent classification
for example things would work a lot
better you know by using deep neural
nets for translation it was really
really a big you know big progress it
took about three years or about two
years for people to sort of turn this
into the operational translation system
that actually works at scale and can be
used every day but now all the
translation system you see by by Google
Microsoft and Facebook used neural nets
and so that's a big progress they work a
lot better than the previous systems
that were also a machine learning based
but more kind of superficial in terms of
their understanding of the text very
very very little hardwired
linguistic language about this in those
systems all right now you can you know
are the response to this or you can ask
your question okay if I have to choose
then I'll ask my question which is this
why do you think that unsupervised or
low back of a sense we both agree that
reinforcement learning from pixels is
not getting us anywhere
it's in my view it's not abstract enough
you might have your own take on it but
we both think that it's vulnerable to a
lot of problems why do you think that
unsupervised learning is going to evade
those problems to me they seem like at
some levels trying to do the same thing
which is trying to abstract too much of
a very complicated world without enough
initial structure so why what's the
difference between the two in your view
so I refer to the the cake analogy that
I had in one of the slides by the way
it's become a bit of a meme in the
machine learning literature now
community the the fact that the bulk of
what we learn is is models of the world
that are not particularly linked to a
set of tasks so ultimately the the
system that enemies AI systems were
going to build are going to use
reinforcement learning but it's going to
be model-based reinforcement learning
and so well what's happened in the
history of machine learning is that
there were people working on model-based
reinforcement running in the 80s and 90s
and got some really good success
charities over for example had a
reinforcement learning based system that
trained a neural net to play backgammon
and he beat the world champion with it
and then the same phenomenon that
occurred with neural nets for perception
occurred in reinforcement learning where
in the mid-90s people you know had some
theoretical results that showed that
marbles we prospered running had
problems with convergence and it started
only focusing on much more much more
simple models or reinforcement running
which he can analyze theoretically and
sort of abandon the ambition of using
reinforcement learning to Bellary
telogen machines each mistake which was
a mistake so same mistake occurred with
neural nets where people abandon the
idea of using neural nets and can revert
it to using much simpler learning
algorithms in the mid-90s it's only in
the last five years or so that you know
the neural nets have come back to the
fore and model-based reinforcement
learning is only in the last year or so
people really
working on so people as you said I
define a very enamored with mumphrey RL
and they are starting to get interested
in model-based RL as well as we are at
Facebook and various other other places
at Berkeley there's very good group at
Berkeley working on all these RL
University of Washington as well it's
not very well understood theoretically
there's a lot of work to do it doesn't
really work yet and this is why we don't
have you know cat robots that are as
agile as a cat this is why we don't have
robots they can kind of grab you know
many polite objects the the way animals
and humans can do it do you know we
don't have this ability to with machines
to build models of the world that's
what's been seeing that's in my in my
view that's what's missing a little
inane structure might help you get a
long way towards that a minimal amount
of it yes thank you very much okay we
got some more time Chris all right
thanks there's a question for yan you
said that common sense is the ability to
fill in the blanks another way and
looking some of the cases you discussed
is there's a general ability to employ
inference to the best explanation and
that might actually have the status of a
fundamental in a principle here if that
is the bet if that is the correct
account of the examples then it would
have consequences elsewhere I mean if
you could just say something more
generally about how you regard inference
to the best explanation in relation to
your general program so inference and
and producing expeditions inference to
the best explanation
conduction abduction yeah yeah so I
think inference is sort of a inference
or reasoning I guess would be sort of an
orthogonal direction with unsupervised
learning so I think there are several
types of reasoning right there is a type
of reasoning you do when you face faced
with sort of a situation in the world
and you are kind of trying to figure out
what to do to reach to make the world
reach a particular end state and there
is certainly reasoning there that
involves not simple manipulation but you
know intuitive physics simulation or
some kind and sometimes that involves
kanna discrete events and and perhaps
that we interpret as symbols because we
like to use language to express them but
no one who talks doesn't do wrong we
tongs don't do this right they don't
have language there
actually not even social animals so and
you can do all that stuff where they can
make knots they can make no they can use
tools and the shrines for your question
well it seems to be more general
phenomenon so when you see the half face
and you conclude there's occlusion and
the best explanation that's the real
face because there's not many half faces
around and elsewhere so that's that's a
much more general explanation of some of
these phenomena so that's a possible
candidate for something that someone
might put forward is something innate
that's used throughout all of these
cases well I mean the the the problem is
how do you discover how do you get to me
you know how do you get the machine or a
brain to learn the structure of the
world in such a way that it will produce
those most likely explanations and
that's that's what I'm after my take on
this is that inference the best
explanation is super-important totally
under study though there is certainly
work on it nai and you can't do it
unless you have a symbol manipulating
apparatus that was at some level the
point of my book the algebraic mind I
don't think everything that looks I
don't think everything that looks like
inference to the best explanation
actually is so I think that some of the
things that Yann does could be conceived
of as inference to the best explanation
and aren't really there they're just
three cases in cases but there are some
cases like that that I think certainly
like scientists do for example and we
don't have AI tools that can do it I
mean we reduce very often we reduce this
kind of inference to simple mathematical
principles like energy minimization
right so we have some model that tells
us whether an interpretation of a
solution is consistent by basically
measuring you know computing some sort
of score of whether you know is the left
side of my face compatible with the
right side of my face things like that
and inference is basically finding the
configuration of least least energy and
that can sort of encompass a lot of what
we think of as as reasoning and finding
of explanation let's see if we can fit
one or two just quickly No all right
though um hi yes this question is for
both Jana and Gary so yeah and you gave
a definition of intelligence that was
about prediction and I think that works
well on the perception
things I actually favor a different
definition intelligence which is novel
problem solving solving problems that by
definition we don't know how to solve
and I think that's a little bit more on
the cognition side and requires
reasoning and I guess my question is you
see deep learning or neural nets ever
being useful in reasoning tasks I see I
mean young will have his own answer I
see them as being useful but not being
sufficient so I think that there's all
kinds of pattern recognition people do
in problem-solving and deep learning is
a good tool for a pattern recognition
then there's abstraction and that
doesn't seem like it's a good tool for
that so I think it's actually a good
tool for abstraction too in fact I think
it's the essence of the success the fact
that we have this kind of layered
architecture that allowed system to
produce you know abstract representation
of the world although they're relatively
simple and in terms of reasoning I think
the idea of using the the the main issue
with sort of classical ways of building
systems that are capable of reasoning is
that they are based on symbols and logic
and the problem is that that's
incompatible with learning because
learning likes continuous and
differentiable stuff and so if we want
to implement
reasoning in intelligent systems I can
learn then we will have to replace
symbols by vectors and we will have to
replace logic by algebra those who
learned logic in graduate school will
realize that that claim was false hi
I'm wondering just through all of this I
mean the obvious difference between
artificial intelligence and native
intelligence seems to be instinct you
know primal instincts emotion I'm
wondering how much has earned even any
integration of hormonal pathways or stud
like more studies about like the
structure of the amygdala involved in
artificial intelligence because I feel
like so much of reasoning is so grounded
by our emotions I mean we're all here
because we love this area of science
that I feel like that's a huge part of
why of our you know everyday lives I
guess how does that translate into
machines yeah so if you if you look at
the syllabus
or the components that a autonomous
intelligent system will have to have it
has to have a perhaps a model of the
world if you wanted to act intelligently
so that it can serve reason before
acting it needs to have a way to
generate a sequence of action and it
needs to have an objective function it
wants to optimize so and to be able to
take a sequence of action or predict the
outcome of a sequence of action it also
has to have what's called a critic in
the context of reinforcement learning
which is a module that predicts the
expected long-term value of the
objective so the objective measures if
the mission is happy or unhappy in some
abstract way right it seems like
something's missing from within right
and and the critic predicts ahead of
time if a particular action for example
is gonna bring a good actor a common art
and you can think of this as a primitive
form of emotion in the sense that you
know if the critic says you know
something really bad is gonna happen or
is very likely that something bad is
gonna happen it's kind of like here
right avoiding this course of action is
a bit like behaves a bit like fear now
the the questions like no what about
hormonal circuits in the brain why do we
have this you know do we need this in
robots and things like this actually I
know it's not entirely clear to me that
it's required but one thing that I get
asked all the time in you know when I
talk to the media but but AI is that you
know our robots gonna take over and kill
us all you know is there gonna be a you
know a terminator type scenario and and
this is a complete projection of human
character traits onto robots and there's
absolutely no reason why robots will
want to dominate because they are
intelligent it's not because an entity
is intelligent that it wants to dominate
others or take control and you know we
have perfect examples of people who are
in positions of power we're not
particularly intelligent I often use the
same example do you have the same
president I do I know I think the desire
to have power is more correlated with
the toaster on that with intelligence
really Gary last word
I don't think I have one we good okay so
thank you very much we will now adjourn
to a reception over there over there
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>