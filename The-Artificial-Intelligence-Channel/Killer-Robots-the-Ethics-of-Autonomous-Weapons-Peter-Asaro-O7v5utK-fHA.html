<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Killer Robots &amp; the Ethics of Autonomous Weapons - Peter Asaro | Coder Coacher - Coaching Coders</title><meta content="Killer Robots &amp; the Ethics of Autonomous Weapons - Peter Asaro - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Killer Robots &amp; the Ethics of Autonomous Weapons - Peter Asaro</b></h2><h5 class="post__date">2017-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O7v5utK-fHA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so this session at the beginning of
Dave Chalmers made a distinction between
the short term issues and the long term
issues and that's distinguished from the
very long term issues but the long term
and the very long term have to do with
AI systems as moral agents the short
term is something quite different
they're not moral agents they're systems
that hopefully can help us and that's
what we're gonna be talking about in
this session we're gonna be talking
about autonomous weapons autonomous cars
autonomous financial decision-makers
autonomous sex robots and our first
speaker is Peter Asaro he's a
philosopher of science technology and
media and a professor at the new school
his work focuses on the social cultural
political and ethical dimensions of
military robotics he's writing a book
about social and ethical issues raised
by robotics and his title is killer
robots and the ethics of autonomous
weapons and we're gonna have a strict 20
minutes and talk and 10-minute
discussion policy because otherwise he
won't be finished by the time for the
next thing thank you
so today we're going to talk a little
bit about autonomous weapon systems or
killer robots as they're known in the
media and other places and how that kind
of relates to AI epics and some of the
questions you've been talking about
already today and talked about tomorrow
I want to define what we mean by
autonomous weapon systems in a more
general way talk about the ethical
fundamental ethical questions that are
raised really by these systems and the
kind of response that I think we can
come to and it's really kind of driven
by some philosophical reflection on what
the real problems are both practically
and ethically and meaningful human
control how this concept has emerged as
a sort of solution to these problems and
then kind of come back to talk a little
bit about how software and the
delegation of certain kinds of moral and
legal authority and responsibilities
more generally in AI ethics can learn
something from what we've done here with
autonomous weapons so what do we mean by
autonomous weapons you're probably
already familiar with drones and drone
strikes and targeted killing these are
remote operated systems so they have
certain autonomous functionalities but
the crucial functionality of choosing
what is a target and when to fire at
that target is still under human control
despite that these technologies are
disruptive of international legal
frameworks and we have here a graph of
drone strikes in Pakistan over the last
decade or so and in general though these
are not considered autonomous weapon
systems because they are still under
human control but there's a great deal
of development going on within the
military and different forms of
automation and robotics being applied to
weapons systems and weapons platforms we
have guided missile systems that are
becoming increasingly sophisticated that
are capable of communicating with one
another in swarms of missiles choosing
amongst themselves what to target and or
where to target a particular target
there's also development of the next
generation of drones which you kind of
see up here at the top which are going
to be far more
they're gonna be jet-powered drones
they're gonna carry far more armaments
than the current predator and Reaper
drones they'll travel much faster much
greater ranges and they're capable of
doing much more sophisticated forms of
autonomous potentially targeting and
this is where it's a concern a lot of
this work is classified so we don't
really know exactly how they function or
what they're doing and when our campaign
to stop killer robots started a lot of
the websites that were advertising
sophisticated capabilities of these
systems were taken down or the language
was changed to make them seem not so
autonomous but it's not clear what's
actually in development or not but the
US has an x-47b program that's looking
at fully autonomous takeoff and landing
from aircraft carriers the British
Tyrannis system which is depicted here
is very similar they all look very
similar because they're also applying
stealth technologies Europe is
developing its own system the EU but
there's a whole range of robotic systems
being developed by the military for
surface space naval warfare coast guard
patrolling things like that automated
Russian these are Russian tanks that are
remote operated at this point but could
be given artificial intelligence could
be given autonomous targeting
capabilities driving capabilities of
various sorts so the platforms are
coming online there's also of course you
know bomb disposal robots which aren't
weapons and are used to disarm weapons
and those are sort of not problematic so
it becomes a question where do you draw
the line we don't just want to ban
everything there's certainly good uses
but what is an autonomous weapon some
current systems that actually use
autonomy in these critical functions of
targeting and engaging with weapons
systems include some anti-missile
defense systems
there's the Patriot missile system and
the Phalanx Gatling gun system which
essentially use radars to detect
incoming missiles and then fire in this
case a missile or a machine gun to
intercept an incoming missile for a
number of years the u.s. deployed and
over the horizon
cruise missile which is depicted here
they've since been decommissioned the
idea there was the ship would not have
radar contact with an enemy ship because
of the curvature of the earth so there's
a big mountain of water in between you
and the enemy but you would fire this
missile over the horizon and it would
use its own radar system to identify a
target and attack it interestingly it
was never used and it was never used
because no commander ever found
themselves in a situation where they
knew there was an enemy that they
couldn't see and we're willing to fire a
weapon at it without really knowing what
that weapon was going to target and I
think that's a lesson for autonomous
systems more generally sAMSUNG TECHWIN
has deployed for a number of years now
along the Demilitarized Zone an
automated century gun which is a machine
gun that uses cameras infrared and
visual spectrum to identify human
targets right now it's set in a human
supervisory mode but it is equipped with
machine guns and it has an automatic
capability and so it could fire on
targets automatically if it was put in
that mode and here's the Taranis system
I mentioned before that's being
developed by BAE
that originally was claimed to have a
lot of autonomy and targeting and now
it's not so clear so we have things like
cruise missiles already but we give them
a GPS coordinate and humans determine
that target and that GPS coordinate and
determine that that's a valid legal
target under international law and then
they send the missile to it and the
missile may actually make decisions on
its own of the route that it takes in
order to avoid obstacles and radars
things like that but the humans are
still designating the targets for these
systems these are more fully autonomous
systems the anti-missile systems but
there are actually only fully autonomous
for a matter of seconds where there's an
incoming barrage of missiles the systems
activated
the missiles are moving very fast so
humans aren't capable of targeting them
themselves
but there is a sort of human supervision
over it and these systems are targeting
projectiles missiles weapons these are
not vehicles they're not humans they're
not human occupied vehicles or buildings
and so the ethical questions I think are
significantly different in those cases
but within the the international
discussions so far at the United Nations
there's been a focus on laws or lethal
autonomous weapon systems which is an
unfortunate acronym because we're
engaged in making laws about laws I'm
much more partial to AWS or automated or
autonomous weapon systems some people
call them fully autonomous weapons which
implies there's semi autonomous weapons
which then gets into questions about how
do you define fully versus semi and of
course what is autonomy the campaign's
stuck with the tried and true killer
robots and I think that's also a good
moniker within the parlance of the
military there's been a long-standing
discussion about humans in the loop and
this is humans that are in the loop that
control the firing and targeting of
weapons there's been movement towards a
notion of humans on the loop which would
be humans in a much more supervisory
mode where systems are automatically
doing things but the humans watching
what it's doing and can intervene at
different times and of course humans out
of the loop means the system is fully
automated it's going to do what it's
going to do and humans aren't really
involved in the process within the sort
of decision-making process of what is a
target at least within say the US Air
Force deciding what is a valid bombing
target you have these sort of six steps
find something fix its position track it
if it's moving and then you target and
engage it it's important to realize that
targeting is an act in a moral sense I
believe pointing a weapon at somebody is
an act that's very different from just
having a weapon and then choosing to
pull the trigger or fire or engage that
weapon is another act and I think these
are the two crucial acts that we should
not have become fully autonomous these
need to be under meaningful human
control and I'll get to what I mean
exactly by or generally about meaningful
human control exact definitions are
still hard to come by and international
relations
in terms of morality
we don't necessarily share moral
perspectives moral theories and moral
approaches but there is I think a good
broad sense of ground rules in the
international community that emerge from
notions like human rights and the UN
declaration on human rights that sets
some parameters for what we consider
moral and immoral we also have the legal
frameworks of the Geneva Conventions and
others that establish laws of warfare
human rights law and so forth and so we
can drive principles from those but we
can also apply some pretty standard
philosophical theory and we can look at
the consequences are these gonna have
good outcomes or bad outcomes in a
utilitarian framework we can look at
rights and duties and what sort of
threats are posed to human dignity and
human rights by these systems we could
also look at virtue ethics which is
actually turns out to be very important
in military ethics the the virtues of
commanders and soldiers is highly
indoctrinated and these systems actually
threaten those virtues and then there is
sentiment
cinnamons moral sentiment which I think
there's a more nebulous concept but is
valid that we feel moral repulsion that
we feel that this is just wrong or
balance' but I kinda want to go through
these in a little more detail since we
have a philosophical audience to some
extent in terms of consequentialist
approaches and this is typically the
argument that's given in favor of
autonomous weapons systems as well
they're gonna be really great they're
going to reduce civilian casualties
they'll make less mistakes than humans
won't that be good and that's a very
narrow construal I believe of the
consequences that's not an estimate of
the full consequence what it will mean
for states to adopt these weapons
systems and that for that to become a
standard warfare practice for these
things to become proliferating all over
the world and coming to non-state actors
and so forth so I think we also have to
look at the wide consequences and it can
be very dangerous to look at a simple
problem like an arrow consequentialist
argument which says well in this given
situation wouldn't you rather protect
your soldiers and eliminate an enemy
with this system sure that sounds good
but what principles are being violated
and I think we can make a comparison to
slavery or torture where you can easily
make consequentialist arguments in favor
of slavery it's really good for the
people who aren't enslaved to enslave a
bunch of other people and have them you
know produce stuff for you or similarly
this sort of ticking time-bomb questions
around torture that if there's one
person who knows where the bomb is
hidden and torturing them is going to
reveal the information isn't it
worthwhile to torture them and get the
information and defuse the bomb but I
think in general we have principles
against torture and against slavery
because on the whole it's better for
human rights and human dignity that all
of our rights are respected and the
violation of any one individual's rights
in terms of torture or slavery is a
violation of all of humanity in a more
fundamental way and so this is where you
get into questions that aren't just
consequentialist in terms of narrow
consequentialism there's serious
questions about whether these systems
will actually be able to do better than
humans in any meaningful sense in terms
of the legal sense they have to be able
to distinguish civilians from combatants
because it's only lawful to attack
combatants that armed conflict you know
computer vision has gotten pretty good
lately a lot of advances are made and
certain forms of visual distinction are
actually probably going to be better by
AI systems than by humans other notions
about behavioral patterns whether
somebody intends to to harm you or do
harm to somebody else requires a lot
more psychological theory of mind about
human agents what they're doing why
they're doing things in many cases even
at warfare intent of an enemy is a
crucial factor in determining whether
somebody is a combatant or not
especially an irregular warfare where
you have gorillas or insurgents who are
not wearing uniforms for instance you
also have questions of proportionality
and within just war Theory the
proportionality question is whether the
harm that you're going to do
and potentially harms to civilians is is
warranted by the military necessity or
the threat that's posed to you so if
somebody hits you on the arm it's not
valid to shoot them with a gun that
would be disproportionate or if you have
a military ammunition depot next to a
school or a hospital is it valid to
destroy it knowing under the doctrine a
double effect that you're going to also
destroy the school and perhaps kill many
children or destroy a hospital is that
value worth while and those are value
judgments that are very difficult for
humans to make but commanders make these
decisions learn to make these decisions
under dire circumstances how we exactly
translate that to an AI system is not
straightforward at all there's also a
number of situations in international
law such as the ability of combatants to
surrender or the fact that a wounded
combatant is no longer a valid target
and so they cannot be killed and
shipwrecked sailors things like that and
this is called or to combat which means
they are out of combat so they're no
longer a valid target then you have a
whole lot of negative outcomes that are
likely with these systems make mistakes
fratricide mistakes harms the civilians
unintended initiation and escalation of
conflicts these systems can be hacked
they can be spoofed you can do
behavioral hacking where you coax the
system into attacking another enemy
system normal accidents these systems
will malfunction they'll be very
complicated common mode failures these
systems will be in warfare so they're
going to be battle damaged it's not
exactly clear what that could do to
different kinds of systems and the
environment is broad and unpredictable
in a number of ways so in total there's
a lot of actually narrow consequences
that are bad in the wider sense of
consequences as I said unintended
escalation initiation of conflict means
humans will tend to lose control over
conflicts arms races that could lead to
regional and global instabilities as
well as the huge waste of resources and
threats to international law itself the
norms and standards that we rely
to hold people accountable for targeting
decisions to hold States accountable for
military interventions and so forth as
they're able to blame systems for
malfunctioning instead of taking
responsibility for decisions that were
actually made by humans and then you get
the argument that these systems are
going to be great we just should ask
well what's driving that innovation what
are the mechanisms that are gonna
increase accuracy and precision to
reduce civilian casualties and are those
operative for whom what states has not
all states are those operative is that
going to carry when we have
proliferation warfare is very different
than cars so we have class action
liability we can sue car companies that
improve safety over time these sorts of
lawsuits aren't permissible
international law victims cannot sue the
manufacturers of arms straightforwardly
so they don't have that kind of
accountability and responsibility so I
don't know that we should expect the
technology to be driven in that
direction but then we get to the
fundamental questions I think of human
rights and dignity and whether it's
morally justifiable to allow these
systems to kill and I think the answer
is no and I think it's because it
matters how one is killed we retain
dignity we retain human rights even when
we're liable to be killed in
international conflict that cannot be
arbitrary summary or extrajudicial there
has to be a human being who's a moral
and legal agent who makes the
determination that that is a justified
killing and if you don't have that then
it's an arbitrary killing and so in this
sense for the stupid AI systems out
there that are not moral agents that are
not legal agents all of the killing that
they will do will be arbitrary and as
the Special Rapporteur for arbitrary
summary and extrajudicial execution said
it's mechanical slaughter so this is
improper because they're not legal and
moral agents it also undermines
responsibility in a more fundamental way
because as individuals or commanders use
these systems they don't know exactly
what they're going to do they become
more and more unpredictable as they
become more sophisticated and so
psychologically they feel less
responsible for what those systems are
going to do so they don't internalize
responsibility in the
Samwise psychologically and from the
legal perspective they aren't as easy to
hold accountable for war crimes you have
to prove intent if you don't really know
what a system is going to do there's no
specific intention it's an almost
impossible to prove in that intent so
meaningful human control I believe is
the way to go as far as trying to
regulate these systems and think about
how we control them which is a complex
term that involves several parts part of
that is positive control you actually
have to have control over the system
system cannot be out of control but the
system has to allow a human to actually
exercise their moral agency and that
means being able to call off an attack
when they see that it's necessary it
means actually warfare is an act of
creating meaning an automated systems
will just perform algorithmically
they're not going to necessarily
generate any meaning humans are
generating meaning and so they need
information in order to make these
judgments and that needs to be accurate
and clear but if you are just you know
every time a target is selected the
light goes on you press the button you
have no idea what's going on that's a
sort of meaningless form of control and
it's just nominal human control and then
you need this internalization
psychologically of the responsibility
that operators feel responsible for what
they're doing and that that's sufficient
to hold them accountable under legal
systems so what's to be done so we've
been working at the United Nations now
for a number of years 2009 I found out
the International Committee for role at
arms control in 2013 we founded the
campaign to stop killer robots we've had
three years of meetings at the
convention on certain conventional
weapons in Geneva where admits of this
discussion about meaningful human
control has come out of last summer
there was a letter from scientists
sponsored by the future of life
Institute where thousands of AI and
robotics researchers joined in support
of banning fully autonomous weapon
systems and hopefully this coming spring
we're going to see the meetings of the
UN elevate to a new level called a group
of governmental experts which would have
a mandate hopefully towards moving to
treaty discussions
we believe new law is actually necessary
in this case there's nothing that
explicitly prohibits autonomous weapon
systems in current law however I think
it's implicitly the case certain weapons
reviews might make that pasta Cerie but
current weapons reviews are quite
insufficient for these the complexity of
systems what would it mean to test a
system to verify its legal compliance
and what we really ultimately need is a
norm that would be established and that
norm would also apply I think more
generally to automated systems that have
control over people's lives and I think
that's where it comes back to some of
the other questions around AI and ethics
and I've sadly run out of time but thank
you for this thing
so you say that because autonomous
weapon systems are morally wrong they
should not be developed by nations and
so you support this campaign for
stopping killer robots my question is um
what if in the future it became
necessary for a country to develop a WSS
in order to conduct warfare like it was
not feasible for a country to win
without these systems was your opinion
on them change well I think there you
would have a situation where states have
already developed them we're hopeful
that we can get a pre-emptive ban so we
can create a situation where states are
not racing against each other to develop
the technology or feel that their
security is jeopardized because other
states have developed this technology so
that's part of the notion of pre-emptive
arms control and in multilateral arms
control more generally where it's in the
collective interest of all the states to
prohibit these weapons systems and I
think we're still in that situation if
we fail to get a treaty and it moves
forward then that sort of reasoning
becomes you know more justifiable I
still think it's fundamentally wrong and
that we should still try to rollback
that technology and when we have had
bans on weapons after they've been
deployed like chemical weapons for
instance oh hi Steven Levy at
backchannel I'm kind of wondering what
Stanley Kubrick or Joseph Heller would
make of this discussion here you know
and I think that's one benefit of having
a conference like this is we can look at
what humans do in light of what
artificial intelligence might do there
so you know if you're to me if you're
killed by a drone it probably doesn't
matter to you whether it's a general
taking responsibility for it or whether
you know at some point on the line the
drone took responsibility there so I'm
actually wondering it's a serious
question whether considering putting
weapons like this into effect might make
us reconsider the way we do our own
actions
yeah I think that's a valid point and I
think in terms of how the discussion is
evolved at the United Nations previously
manatorian
law disarmament campaigns have focused
on specifically the effects of the
weapons systems on civilians and this
this is campaigns a little bit different
because we're really focused on the
decision processes and the
justifications for using weapons or
force at all and I think thinking
through that and making those norms
stronger and clearer is good for
international law so I want to make one
point so you actually showed some BAE
Systems weapons that araÃ±as comes from
BAE Systems so they're chairmen back in
January declared that his company would
not make fully autonomous weapons that
they were fundamentally wrong and he
would never allow his company to build
such weapons I think that's an
interesting data point in this debate
you mentioned on one of your slides
weapons of mass destruction but could
you expand a little bit on what you
meant by that and why that's relevant to
the debate yeah thank you I do think it
is interesting that the bae systems and
other roboticists and other robotics
companies have come forward clear paths
robotics in canada and i think in terms
of the the weapons of mass destruction
the critical thing to think about in
what it constitutes a weapon of mass
destruction is a small action by a small
number of individuals resulting in
massive devastation so whether that's
chemical biological nuclear and I
believe robotic at some form that you
would be able to in the way that a
single person can release a virus on the
internet that can infect thousands of
computers millions of computers and have
global effects as you have robotic
systems networked into these kinds of
global networks a single individual will
be able to launch massive attacks that
result in massive destruction and that's
it's a weapon of mass destruction I
think that's incredibly destabilizing
globally whether
it's you know States or rogue states
that are getting these or whether it's
you know cults and nongovernmental
organizations who are getting this and
non-state actors and terrorists and so
forth I my name is Jane como my question
actually builds on your previous comment
on rogue states or individuals so could
you make the argument that developing
these kind of weapons would be sort of
defensible in as a defense mechanism so
let's say an individual I recognize it's
hard now for one individual to build a
thing like this but maybe 10 years from
now it will be relatively easy what's
your view on sort of the defensive
stands against something like that well
I think the moral question turns on
whether the system is ever authorized to
target humans so I think I think there's
ways to permit missile defense systems
for instance that are targeting missiles
or non-human you know material then it
becomes a question of risks to humans
and minimizing that risk that you
accidentally target an aircraft or a
vehicle or a building of some kind or an
it or a person but as long as it's a
human then the moral question is there
and I think whether it's offensive or
defensive this distinction is very
difficult to make when you look at a
weapons system outside of a missile
defense system so I don't I don't think
that's necessarily the best way to frame
it
hi my name is Bruce Costa with granite
Forrest Stanga and I'm wondering if you
could go down a little further down this
road that this gentleman started you on
so it's not the drone
it's the automation of the drone right
so how have you thought about how you
would construct a law that would enable
me to know that another country has
utilized automation and how would how
could you keep that genie in the bottle
what teeth are on the dog yeah so this
is a question and arms control it's
called verification how do you know the
other signatories to the treaty are
meeting their obligations under the
treaty and I think on the one hand if
you see a vast fleet of like robotic
aircraft or robotic soldiers and they
aren't training very many people to
operate them that's a pretty good
indication that there's a lot of
automation going on in that and that
they're probably acting in certain
autonomous fashion forensically if you
capture a system you can take it apart
try to figure out how it works there's
ways to try to avoid making it look like
a fully autonomous system or an
autonomous system there's also the
argument that it you know a system that
looks like it's not capable of
autonomous targeting could have a
software update once the war starts that
gives it that kind of automated
targeting and you know not all treaties
require verification and we can look at
things like the chemical weapons treaty
where you know if somebody acts against
it and uses chemical weapons there's a
huge sanction internationally for that
kind of action and so if you can find
out after the fact there's also of
course a lot of trouble with attribute
ability which we see in cyber warfare
that could also extend into these
systems so it could be very easy to
obscure who's actually launched the
attack by not having a certain
designations on the robotic system or
actually mimicking another nation
system so there's a lot of risks around
that but if you have a norm in place
then you can act based on a case-by-case
basis as those situations arise the
really dangerous situation is whether
you know they would have a mass army
that would actually lead to some global
instability situation but I think we
would pick up you know that kind of
level of production fairly easily that
they're amassing resources and not
training operators
yeah I think that's it's a yeah so for
the live stream the question is how do
you know that the people making the
decisions actually have the moral
justification for firing on a target and
in many cases they don't right they're
just given an order to fire on a target
they may not actually have access to
that information but the person giving
the order should and I think if we think
about designing systems for meaningful
human control and actually making it a
positive obligation and the treaty
rather than saying don't build an
autonomous system you say every system
you build you have to demonstrate the
meaningful human control of that system
right it has to be apparent in the
operation of the system so you wind up
then encouraging the the design of
interfaces that provide that information
because it becomes tangible that you can
prove that it's a meaningful human
controlled system</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>