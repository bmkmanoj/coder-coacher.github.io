<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Quantum Machine Learning - Prof. Lilienfeld | Coder Coacher - Coaching Coders</title><meta content="Quantum Machine Learning - Prof. Lilienfeld - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Quantum Machine Learning - Prof. Lilienfeld</b></h2><h5 class="post__date">2018-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Z34AQnOSeU0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah okay sorry yeah yeah that works
yeah great so I we use classic machine
learning models to to model quantum
properties of matter and particular
chemistry an interesting field because
it's dictated by the physics of the
valence electrons which are quantum
objects and so the equations you need to
solve our quantum mechanical equations
and we try to use machine learning to do
that now before I get to that I should
acknowledge the people doing the work so
these are recent pictures from my group
we be a part of the University of posits
the oldest university in Switzerland
fourteen hundred something or so it was
founded and and that's the Faculty of
science here we we received a lot of
funding for partially for that work and
so we are really thrilled to see this
interest including communities which are
foreign to chemistry and so I feel a
little bit like a faun I hear and I I
think what I try to do is to portray the
kind of problems we have in chemistry
and to maybe hopefully convince you that
this might be of interest also to to
look if some of your expertise could
help with the problems we have and and
the problems are quite substantial
now why generally why would you even
bother about chemistry I picked two
examples one is more philosophical this
is Frederick Villa and 1910 that Center
1900 century chemists from Germany he
demonstrated that the atoms in an
inorganic solid like this ammonia
cyanide aka the mother-in-law's the salt
for the mother-in-law that these atoms
can be transformed into urea an organic
compound which you would find an
organism so so there's this transition
that that atoms
I don't know from Cleopatra also
happened to be in your body or from some
some inorganic matter there's a
fundamental sort of magical or
alchemical twist and and that's
certainly fascinating now more from a
utilitarian point of view you could
argue that chemistry is just important
for human mankind and and at the end of
last century nature published his
millennium essay which started out by
asking what's the most important
invention of the 20th century and it was
narrow plains nuclear energy or quantum
mechanics
it was the synthesis of ammonia which
which the author picked and the reason
is if you look at the production of
ammonia shown here in green as a
function of time and this was crucial
for to be produced at the industrial
scale using the harbour Bosch I could
give it shock talk
all right I'm not advancing fast enough
so and also at the same time so
ammonia's is necessary for producing
fertilizers and this allowed to avoid
starvation for for millions of people
and so the world population grew a lot
and if you like humans that's a good
thing the the I think it's interesting
the word was you don't even see them on
on this growth right so so chemistry is
more important than politics
you could argue so how do we do on
chemistry in in terms of chemistry how
is our understanding depends a little
bit how you define understanding but if
if you just look at how well do we do in
predicting chemical behavior this is the
phase diagram of of water so it shows
you as a function of temperature and
pressure in which aggregation state the
metal curse and and these lines show you
the the transitions the phase transition
lines and you see in in blue this is
experiment and in black a state of the
art theory so that state of the art and
at best you are something of you off by
10 10 degrees so if you have 10 degrees
too much in your body you're dead right
so and and this is what is it's you it's
a molecule with just three atoms it
shouldn't be complicated and it's not an
infrequent molecule and it's not an
unimportant molecule so and also if you
could find some catalysts I showed the
the ammonia example that was a catalyst
if you found a catalyst that splits
water efficiently using sunlight for
instance and cheaply meaning being made
of abundant materials you would have
solved the energy problem fusion so
Justin and last year a paper was
published proposing this catalyst with
ayran which is very cheap in some simple
organically
and it's unfortunately it's not very
efficient but it illustrates that we
there could be fantastic catalysts out
there that they just do what we need
we just don't know them right because we
are too ignorant now why why do I
present the tsagaan on a projection why
don't we have screens all around printed
on the walls using organic materials
that emit light why don't we write these
are chemical questions now the problem
is we suck it at controlling this right
the way we we do chemistry right now is
trial and error you're walking around of
course every now and then even a a blind
Creole will find a not right so then you
find penicillin and you are happy until
all bacteria have become resistant so
what do you do right now why is it so
bad I think there are multiple reasons
for this one of them is is is this one
that well so chemical space is is really
gigantic this aspirin it just has 13
atoms it's really crucial if I mean in
my life I don't know about your life but
so I just said this martini lunch so
it's it's quite important anyhow so if
you look at the number of possible
molecules you could make as as a
function of size and some aspirin if you
don't count hydrogen's it has 13 atoms
and and you see these are organic
molecules and this is a very
conservative lower bound of what's
possible and aspirin is really small
molecules and yet it's it's one out of a
billion so it's it's a huge space and
why is this so large that it's a
convolution of two two components one is
the stoichiometry if you look at the
number of elementary particles that make
up chemistry this is protons or
electrons if you look at the
possible geometries you could make these
are integer petitions you see that this
number increases exponentially and so
you very rapidly have very many possible
stoichiometry now and this is protons
any single atom the second iam atom has
40 protons it's roughly half halfway in
your in your periodic table right you
have roughly 100 different elements so
and you you should go up to 100 if if
you wanted to cover the entire periodic
table and so just 10 protons give you
the stoichiometry for instance of all
all these common small molecules and now
that that doesn't in code structure yet
it's just composition right so you add
structure on that and and you all know
that just take carbon you can make
diamonds graphite or charcoal on
nanomaterials that's that's the only
difference is structure there's no
compositional degree of freedom it's all
carbon so you combine the two and and
that's sort of this situation and and so
there's there's an estimate 2004 there
was a special issue in nature on
chemical space the estimate was more
than 10 to the 60 possible molecules
which which you could make and these are
just organic molecules so a very small
fraction of the pay or dick table right
and that's that's a huge number so this
is on the order of atoms in the universe
so so people say you you could not we
don't have enough atoms in the universe
to make the hard drive to store the list
of all the possible molecules not
speaking of synthesizing or
characterizing them now in terms of our
understanding of chemistry things like
this that these as a function of
accuracy in terms of predicting energies
which which dictate the behavior of
atoms you can only consider
a mound of atoms a certain scale in
terms of number of atoms or sampling
them for a certain length of time and so
these are the different scales you have
to consider as you go up to something
that that would come close to
macroscopic experience so what what you
would perceive as a chemical as a human
and now we are working our methods live
on on so given a certain budget CPU
budget you can travel up and down on
this diagonal and so with with quantum
machine learning we would like to to go
to very large systems at very high
accuracy to move on on this horizontal a
different plot that illustrates our
landscape we are navigating on a daily
basis and negotiates costs computational
cost versus accuracy of your prediction
so these are typical acronyms of of
standard methods like force fields semi
empirical quantum chemistry methods mean
field theories hartree-fock and DFT this
perturbation Theory here of quantum
Monte Carlo a couple cluster methods
they all approach the exact solution to
Schrodinger's equation for any given
chemical and so typically with a
constant CPU budget you would move on a
diagonal which goes like this right or
thought orthogonal to that one and now
just to give you an idea in terms of
action numbers these are ballpark
numbers and you would spend CPU years
for something like aspirin to get to 1
kcal per mole accuracy which is
typically in thermo chemistry this is
the experimental uncertainty and
spectroscopy you can reach this sort of
accuracy so so then you're talking
decades
now we are trying to get here with
machine learning and we have reasons to
believe that this is possible but we all
weekend show is really just the
beginning wave I'm not saying we did
this actually for some cases we have
strong indications and no miracle
evidence that it's possible and for that
we use the data database we regenerated
couple years back we calculated 434
thousand molecules and DFT numbers so
from here
so by the way if your mean field theory
DFT is also mean field theory if it's
above this average right it's a sweet
spot it gives you more than you're
supposed to get in terms of accuracy
then you get a Nobel Prize so we have
this database and 134 thousand molecules
is it sounds like a lot and it's it's it
goes up to nine atoms not counting
hydrogen's and the most frequent
stoichiometry is these so these sort of
molecules see seven carbon atoms two
oxygens and ten hydrogen's
now if you arrange them in ascending
order of the energy looks like this and
you see here in this zoom in we have one
kcal per mole a curious a 1k cap of all
windows so this is the experimental the
chemical accuracy and within this window
you see over 100 molecules and this is
zoomin on on seven of them and you see
the structural diversity of these
molecules right so and this pushes us to
- I mean this is statement chemists made
that enumeration surpasses the human
imagination and hope that this
illustrates that and there's also the
same set of molecules need 6,000
molecules the color corresponds to the
energy and these are moments of inertia
axis here so all the 3d
molecules on this corner the linear ones
here and and the planar ones here so you
see this is how they're they are
distributed now there's an interesting
thing you see here it's much more sparse
and there's no reason in terms of energy
for this region to be sparse actually
energetically you should expect much
more molecules here but we do have an
inherent human biases so how we perceive
chemistry typically we we draw these two
deep figures and so when we generate
with human-made rules when we generate
molecules they often inherently bias so
that's how this came about now before I
show you results here I'd like to point
out an interesting thing that ordinarily
when and and physical chemistry we talk
about fitting this question comes up how
do you avoid the the overfitting regime
and how do you make sure that you are in
this optimum of test error versus
training error and in fact what what we
really want is something slightly
different we don't actually need this
kind of read regression here rather we
actually want to go through every data
point because every data point is a
solution to Schrodinger's equation which
we obtained with an American noise many
orders of magnitude smaller than the
variance of the property we are
calculating so for all intents and
purposes our noise is negligible and so
we have the problem rather of
interpolation than then real regression
so this has a severe effect on how this
learning curve curves look like our
training errors are typically closed
very close to zero we actually use this
to detect bugs and the code is a typical
tests you do and then your your test
error must come down
systematically this was shown by vapnik
already in the 90s that this has to be
the case and so if your test error
doesn't come down I would argue you're
not doing machine learning because you
stopped to learn
right and then it's something else you
just have a model and so it was shown by
vapnik and others and and Mullen in the
90 said your error decays inversely with
training set size the prediction error
and what we do in particular is to use
kernel rich regression methods so we
estimate a new material new materials
out-of-sample property we estimate as a
linear combination in in your training
where you have these kind of functions
and the regression coefficients you
obtain the solution to the least square
the regularized least square problem but
our noise is is practically zero so i io
mitad the the noise term here so we just
invert this training kernel matrix and
if you plot then the the prediction
error as a function of training set size
and it should obey this inverse
relationship and so on a log-log plot
this is a linear one where the the slope
B is the exponent here and so this is
something you should really find for
your model if unless this there's some
problem and this is a typical problem
that can occur that your machine
learning model actually gives you a
better error for a small data set but
then it levels off and you you don't
have learning anymore and and that's
something as quantum chemist we we don't
like that so we want something that that
continues to converge down to arbitrary
accuracy and so this can happen if for
instance your representation is not
unique I'm showing you two examples
there's a planar geometry for instance
of ammonia imagine the nitrogen in the
center and then you have these inter
atomic distances SNL and if you
construct a pyramid where s and L where
s represents the the edges in the plane
and
the edges of the the nitrogen atom being
above will be behind the plane and you
you can arrange for geometries where l
and s in these two cases are exactly the
same and if then you have a two-body
representation which only accounts for
four distances you couldn't distinguish
these two geometries so they really only
differ due to many body effects and you
see there's a these free curves they
illustrate this if you only have two
body you get for both geometries the
dashed if you add three body terms this
spurious degeneracy gets lifted and you
get the dashed
solid line here and the solid line and
and then you can distinguish them so if
you use the only two body terms in your
kernel retrogression your learning curve
looks like this
another thing you you'd like to have is
of course a very smaller offset so this
is interesting to us and we investigated
what effects the offset and here's an
example of ethanol and the question how
you represent it can really affect how
quickly your learning curves change and
there's a paper we recently published
with collaborators from Google where we
demonstrated that for these organic
molecules you can reach chemical
accuracy and the the best model here is
the kernel rich regression models model
other colors and symbols correspond to
neural networks and other
representations we did this for many
quantum properties that's just the
energy and and the question then is of
course what what determines the offset
of these various learning curves so so
we did a little exercise where we asked
okay what kind of functions could we use
as a representation in our kernel and we
arrived at this a
this sort of experiment may be said okay
suppose you have a representation where
you encounter for all inter atomic
distances with this Coulomb term so it's
nuclear charges of atom I and J divided
by the distance but now we we have this
exponent N and in the Coulomb's law N
equals 1 if you make N equals minus 1 it
means that this term grows with the
distance of the atoms right so that will
be unphysical right so if this
interaction grows with a distance and so
if you have one your learning curve is
here if you become unphysical you are
here if your interaction grows
quadratically it's even worse and your
learning curve goes up even more so so
in other words the the more physical
your representation that the lower your
learning curves the more resembles the
energy of your target and so we then
decided okay let's let's put a force
field as a representation and we can
have false fields that account for atoms
bond angles and torsional degrees of
freedom so these are two body free body
for body effects in in your system if
you provide them in in the
representation your learning curves come
down from black to red and blue these
two are just from the literature and and
this works for all sorts of properties
here we have other electronic structure
properties which which on the data you
can also see for the outliers that as
you go from the dressed atom to the
bonding terms to the anger to the
torsion the air on the outlier
systematically decreases so on the left
hand you have 9 properties for these
isomers dc-7
or two molecules and here on the right
hand side you now have learning curves
for the 134,000 molecules and we always
observe the same trend that s you
increase and
the degree to which your representation
is realistic your learning curve comes
down systematically now these are the
latest results and you see that for for
something like in between a thousand and
ten thousand molecules so you reach
chemical accuracy for this large qm9
data set so another question is if we
can affect also the the slope of these
learning curves the slope has to do with
the dimensionality and and here's a
spoiler that we succeeded in getting to
to do this and the way we did this was
by asking this question if if we can
look at each atom in a query molecule
separately right and we we say okay
let's let's look at this atom let's just
account for its environment this atom in
this molecule and we can increase this
environment systematically by including
in the in the pay auric table we extend
the pay or dictated by the environment
so we add neighbors two neighbors three
neighbors and so on and so forth and and
so we can have an extended pay Yodok
table of chemistry's which accounts for
four atoms in the molecule and if you
then look at add the kernel rich
regression model it sums over your
training instances these are your
regression weights and then you have a
double summation over atoms and
molecules for J being in your query and
I being in your in your training
instance now you can pull the j
summation in front of this and then you
have these terms for each atom j in your
query so you have a sum over atomic
contributions in your query molecule
furthermore the the number of atoms j in
your query can be much larger than the
number
Adams I in your training molecule so you
can you can have very small molecules to
which you compare this atom in its
environment independent of all the rest
and you do that for every atom in your
query separately right and so so you can
ask this question okay suppose this atom
here is similar to water then I can grow
this and then it's methanol and you see
I can add more and more neighbors
there's ethanol then I have propanol
cetera et cetera and once I'm done with
its oxygen I moved to the next atom in
the query molecule and I repeat and so
so then I I learned the local the local
regions independently and and I trained
on on this training set I trained this
before I'm doing my query prediction so
it's like training on the fly in
comparison to a DFT calculation which
which takes easily one CPU hour for the
entire molecule training this on the fly
is negligible so how do we know these
fragments here and it's a simple
procedure we have here it's a flow chart
we you go through every atom and you do
a sub graph matching procedure where you
identify fragments and for this query
molecule these are the fragments you
would find as you go to larger and
larger fragments and up to seven you
have something like forty different
fragments and if you remember before I
mentioned thousand to ten thousand
training instances now we you have forty
right so this is the number of fragments
here this is the error on the energy you
make for predicting this molecule and
this is the size of the fragments you
you need so once you go up to seven
you're here or forty molecules and in
black you see the error on the error
comes from above it crosses zero it
becomes negative and then it comes up
again as your fragments grow
you approach the the energy of your
molecule it becomes more similar so so
you come down you gain by the
delocalization of the electrons but as
your fragments become even larger your
the query molecule actually contains
some strain it it's not as relaxed as
small fragments so the fragments will
actually be low on energy then your
query and only once the fragments become
large enough again to include the strain
you have for instance this five membered
ring here you come up again from below
so we can understand this kind of error
curves in red you see a different
property than the energy namely the
polarizability this doesn't only work
for the energy also other properties can
can be modeled this way now this was
just one query molecule we can do this
for our data set so these are average
prediction errors made for these for
11,000 molecules out of the hundred
thirty-four thousand and this is how
that error comes down in it 10 to the 0
you have chemical accuracy and this is
from from our paper with the Google
collaboration this is chemical accuracy
in these units and electron volt I'm
sorry this kcal per mole is roughly a
factor of 20 in between these two and
you see you hit chemical accuracy at
30,000 right and here you had something
like what is this 50 so we we can change
the slope by encoding for a different
dimensionality in that and and so we
compared this also to taking random
fragments if you take the random
fragments rather than selecting them by
this by the graph the sub graph matching
you get this kind of learning curve
which has this this sort of slope you
can since these fragments are they
repeat throughout the molecule but also
for different query molecules it's like
a dictionary of small fragments of
chemistry's
like the DNA of chemistry right so it's
a very finite set of chemistry's you
actually have to account for and if
you're compared to the total size of
chemical space which is 10 to the 6 or
so and that the number of possible
fragments that actually occur is quite
small and so we looked at the frequency
distribution of of these fragments and
that's shown here in black so the
smallest fragments are obviously the
most frequent ones and as you have
larger and larger fragments the less
frequent the iron in your average query
molecule and if you go up to 7 these are
the least frequent examples with 7 atoms
and you see they are highly specific for
certain query molecules for certain
chemistry's so and this is very similar
to the frequency for instance of words
and in in sentences right so what we
have is something like the analog to to
a dictionary so it's a it's a dictionary
of chemistry if you wish and then your
sentences are the query molecules so
since since these fragments are being
shared among different query molecules
we can look at overlap so these are
three different query molecules and they
share here in the overlapping region
they share fragments and the the extent
of this overlap also tells you how
similar these two molecules are and it
also gives you a measure in terms of
energy because these energy
contributions are always the same no
matter what's your query compound and
now they let me let me correct that they
and the the reference values for these
overlapping molecules are the same but
they enter in the kernel regression
different training sets right so it's a
subset that that's identical but because
the the overall set is not the same
they're regression coefficients will
actually do
now because it's an atomic summation we
have we can print out atomic energy
contributions and you see those here and
numbers next to the atoms and what you
see in brackets these are numbers that
you would obtain from a physics base
force field estimates and so these the
the numbers in brackets are numbers
which were derived from the physical
arguments from humans and you see
ballpark wise it's very similar on the
scale so your hydrogen's are typically
around 50 or 60 or so your carbons are
150 160 the oxygen is 90 to 100 the the
nitrogen is 125 or so so ballpark wise
the machine discovered that these are
energies which are physically meaningful
now because this is fragment by base its
gates so so we can make predictions for
very large molecules with what we think
is chemical accuracy and if you remember
the the the plot where showed accuracy
versus size there's a relationship in
the tradition and the deductive models
we have but because we have something
scalable we don't suffer from that so we
can predict very large more molecules
such as LSD here or cholesterol or
viagra or taxall which is an anti-cancer
drug these are very complex molecules
and what you see here is many different
machine learning predictions versus
reference validation numbers and so
these all these predictions were trained
on these small fragments right and we're
making predictions for large molecules
so chemically this is extrapolation if
you wish but from the machine learning
point of view these chemical chemicals
are redundant so so machine learning
wise it's an interpolation and what you
see in brackets here is the average
error made over all these molecules so
if you just have
one atom in your fragment you basically
don't have bonds so so you're really off
now if you if you allow for bonds your
arrow is this is the kind of error you
would make if you were just counting the
bonds so no only two body interactions
if you allow for free body you get this
down and this is a typical force field
sort of accuracy but DFT you you would
get this kind of error and this is the
experimental uncertainty so this
chemical accuracy reach on average the
fragments up to seven atoms now this
also works for polymers here you see
very large polymers and and here you see
the number of monomers we tested and you
see systematic improvement the
prediction error or decays
systematically the system size at a
hundred fragments or so you typically
reach chemical accuracy so this this
sort of problem and we can definitely
solve these are water classes this is
what you have to get right for instance
to get the phase diagram for water right
also here you see systematic improvement
down to chemical accuracy for predicting
these water clusters this also works in
periodic boundary conditions these are
2d sheets of materials doped with carbon
or or gold and also here you see
systematic improvement so reducing
prediction arrows with training set size
and here is a bulk example this is
silicon you see the error on predicting
the energy of silicon decaying and the
fragments we use here these silanes
which are saturated with hydrogen's it's
sort of the DNA of chemistry so of
course we wanted to look if we can use
that to make predictions on DNA and
that's what you see here the
watson-crick base pair of guanine and
cytosine this is its energy and these
are the fragments you need for guanine
these are the fragments you need
cytosine and these fragments the two
share and these down here are all the
fragments you need to cover the hydrogen
bonding in this DNA base pair so if this
I I come to my concluding remarks
there's one analogy due to quantum
mechanics if if you solved by
variational principle or with quantum
Monte Carlo you solved and the shooting
equation you get the expectation value
say of your energy and you have the the
wave function and then you can use that
wave function to calculate expectation
values for any other operator to get any
other observable right so the wave
function doesn't really depend on your
property now there's some analogy to to
the kernel Ridge regression because here
when you train your kernel the kernel
inversion doesn't depend on your
property you can multiply in any
reference vector to get the
corresponding regression coefficient so
in some sense the regression
coefficients correspond to your property
the Kerner to your wave function we
showed this for molecules these are very
different properties of many molecules
and this really works and when
traditionally we we map composition and
coordinates to energy with by solving
the wave function problem as described
by my previous speaker the machine
learning really and allows us to to
infer of that solution given sufficient
training set sizes so in some sense the
traditional relationship between
experiment and quantum mechanics
informing the human is amended by a
third component here where we also use
machine learning to accelerate the our
understanding of chemical problems this
is an Outlook and we used our model to
predict forces now and these are forces
on a molecule which was not part of
training again using this fragment
approach and before you see the
deviation in geometry and you can relax
the geometry and you you obtain very
good agreement this also works for for
non covalent bonding finally I leave
this in case you're looking for a PhD in
chemistry or a postdoc position don't
hesitate to contact me thank you for
your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>