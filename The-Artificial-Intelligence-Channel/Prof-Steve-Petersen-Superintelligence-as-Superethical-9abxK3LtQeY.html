<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Steve Petersen - Superintelligence as Superethical | Coder Coacher - Coaching Coders</title><meta content="Prof. Steve Petersen - Superintelligence as Superethical - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Prof. Steve Petersen - Superintelligence as Superethical</b></h2><h5 class="post__date">2017-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9abxK3LtQeY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay the last talk in this session was
by another philosopher Steve Peterson is
at Niagara University in New York
Steve's background is in is in
epistemology he's worked in a whole lot
of error of fields over the years he's
done some really interesting work on
algorithmic metaphysics and algorithmic
information theory is the as the the
basis for a certain kind of metaphysics
of the world and in recent years he's
been increasingly turning to issues in
the ethics of artificial intelligence
with a number of important publications
and things like the volume on robot
ethics but his talk for today is going
to be very much I think engaging some of
these issues about values and their
connection to intelligence and goals and
the title is super intelligence as super
ethical thank you David yeah so so far
you've heard from the guy who literally
wrote the book on a I stored Russell
then you heard from Eliezer whose
Institute seems to produce amazing
results every couple weeks or so and and
also they I can attest the Harry Potter
fanfiction is pretty good I'm about
two-thirds of the way through and it's
it's it's very interesting and then we
heard from the team from the future of
life Institute who one of whom is a
physicist who may have written the book
of the universe perhaps and then finally
as they put it the one who wrote the the
classic book on ethics of AI and now
finally the moment has arrived the
moment you've been waiting for yeah you
get to hear from the guy from the little
liberal arts college outside Buffalo my
wife says I'm a terrible self promoter
and it's totally true another way I'm
another self-deprecating thing I'll say
is Peter Elton said yesterday the slides
of moral philosophers tend to look
uninteresting and I guess today I'm a
true moral philosopher my backgrounds
epistemology but yeah these slides won't
have pictures I'm sorry and my talk is
also like Peters in that I too I'm gonna
be pushing against what's been called
the orthogonality thesis so let's take a
look first we're going to talk about the
goals what is it for a superintelligence
to have goals has come up so I'm already
here's the problem in a nutshell in case
somehow you're just tuning into this or
something just to see me so this paper
is a response in some ways to moderate
some of the words in Nick Bostrom's book
but as I hope it makes I'll make clear
it's a very serious worry and I'm glad
there are people like Stewart and
Eliezer and others working on it but the
problem is that well it seems like once
we get genuine AI assuming it's possible
it seems likely to bootstrap itself
probably pretty quickly into a
superintelligence pretty probably one
that will have a strategic decisive
advantage so it doesn't need to
cooperate with others and so on and this
is one way where I think my proposal is
a little different from from Peters
maybe it tries to accommodate that
possibility such super intelligence
could wipe us out through mere
indifference as we've heard they could
have the bottom level goal of maximizing
paperclips just a different value from
ours not malice but as the comparison is
gone - with like ants it's we don't as
max said and Stuart has said we don't
think twice about well maybe twice maybe
once and a half about destroying an
anthill but we don't let it/them get in
our way and we might be like that - the
super intelligence so it could it seems
have very different values from ours
like maximising paper clips as Eliezer
was pointing out it's easy to
anthropomorphize here it's easy to say
nothing smart could care at bottom about
paper clips but it's easy to forget that
you know we have our values from this
weird evolutionary history we like sugar
we like sex we like these things because
of how we've evolved right but these
machine intelligences won't have any of
that background shaping their values
they could have really alien different
values to ours
so the key movement in this in this
worry as Eliezer pointed out and is
featured in Nick's book is the this
orthogonality thesis that intelligence
and final goals are orthogonal more or
less any level of intelligence could in
principle be combined with more or less
any final goal there's two key terms
there we need to talk about one is this
word intelligence now there's a kind of
a consensus in the AI community and in
philosophy of AI community from what I
can tell that intelligence just means
basically means then reasoning being
really good at achieving your goal in
the face of obstacles the more obstacles
you can overcome the more kinds of
obstacles the more things you think of
ahead of time
including other people trying to put
obstacles in your way and so on the more
intelligent you are and this sort of
thin sense of intelligence the other
technical semi technical term here is
final goal which came up a bit earlier
final goal philosophers contrasted with
instrumental goals so we many of us in
this room probably have the goal of
making money but we don't want to make
money just for its own sake we only want
money because that helps us get
something else that we want and why do
we want that well maybe because we get a
vacation why do we want the vacation for
the relaxation why do we want that
pleasure why do you want pleasure
the regress ends right that's the
so-called final goal I just that's I
just value that that's what we mean by
final goals now of course many
philosophers optimists that we are about
reason in some way or another we want to
suggest that reason can incline us
towards final goals now there's lots of
reasons to doubt this I'm not positive
about it myself for sure but today I'm
going to tentatively defend this idea I
mean maybe it's just philosophers you
know we have this hammer of reasoning
and we like to think and that makes
every problem look like a nail right
including ethics perhaps maybe we're
over generating what it can do but I
want to defend it here and I want to
defend it using some principles that I
think Boston would agree to first we
have to talk about complex final goals
intended goal content as Eliezer was
saying is often too complex to specify
explicitly when you try to spell it out
and in machine language right so Bostrom
agrees he worries about and as Eliezer
did perverse instantiation zuv program
goals so there's this old debate
unfortunately Eliezer skipped over it
but the smiley face goal right we're
like oh we want more smiles well okay
that's easy just title the universe with
tiny pictures of smiles
oh no sorry I meant human smiles oh
that's easy we'll just paralyze your
facial musculature okay ah one way this
is really well Illustrated this is
another I keep referencing Eliezer but
he has this interesting example of like
a website where they try to specify how
to give a genie a wish without the gene
the genie of course seeks perverse
instantiation 'he's right the genie
wants to find ways to hit the letter but
not the spirit of what you're asking for
and it turns out really hard to do so
there are these complex goals well even
as
currently simple goal like maximizing
paperclips is actually pretty
complicated because what is a paperclip
that sounds like something only a
philosopher could worry about unless
you're trying to maximize them if you're
trying to maximize them it matters right
if they're suppose you want that you
might make them you want to make them
really small nano size or something but
does it count as a paperclip then if it
could never conceivably actually clip
paper together well this is something
the paper clip Maximizer would have to
work out and more to our point maybe the
paper clips would look just like the
ones we have in our office today but the
the super intelligence creating them
knows that they will never be used for
clipping paper together because all
humans are no paper busy being made into
more paper clips so maybe that wouldn't
count as a paper clip you have to settle
these issues the super intelligence
would have to settle this issue or
something would so Bostrom solution to
this kind of problem in the positive
part of his book where he wants to talk
about loading human friendly goals into
an AI is the AI has to learn the goal
the AI has to learn the goal and that's
I lean on that pretty heavily so we got
to talk about that learning final goals
well when you think about it it seems
odd to talk about learning a final goal
to learn something it seems like you
need feedback tore toward or away from
some background standard well then that
background standard was really your
final goal right then how could you
learn your final goal this is a kind of
human dilemma it seems like you can't
reason about these these goals but at
the same time it seems like we can do it
it seems like we spend a great deal of
our lives trying to figure out what we
really at bottom value I hope we do
anyway and it seems like Scrooge in the
Christmas Carol story it seems like he
manages to change his final goal he used
to think accumulating money was what was
a final value but then he decides no
it's about cheer it's about
companionship or something I don't know
what he does don't ask me
cheers nice that's Christmas cheer now
you could say notice that Scrooge no no
he didn't change his final goal he
always had the final goal of happiness
and what he did was he adjusted his
instrumental goal towards this final
goal he has new beliefs now about what
would reach this final goal he used to
think it was accumulating wealth now he
thinks it's now he thinks it's cheer but
Aristotle pointed out 2000 plus years
ago that's not so helpful to say that's
a very vague final goal right so the way
I understand a certain ethical tradition
called specification ISM which is sort
of new to me I'm not an ethicist in the
background but but it turns out it has a
lot in common with some with thinking
maybe there's no real difference between
specifying a vague final goal like
happiness or changing a more specific
final goal like accumulating wealth
maybe there's no serious difference
between those two and if so that leaves
some room for I mean then the idea is
there's really no sharp line between
means and ends at the end of the day and
if that's true that leaves some room for
ethical reasoning reasoning about your
final goals reasoning about your
ultimate values even on what would have
been this thin notion of intelligence
well we're still left to that problem
how do we learn a final goal against
what standard would we learn a final
goal well here's one that's from the
tradition of specification ISM you aim
at some kind of overall coherence this
is a kind of content empty it's
substantive enough to shape goals but
empty enough to like not be a final goal
in the traditional sense right itself
what is coherence I I can't tell you
exactly people work on it roughly
speaking it's when you trade off a bunch
of different considerations without
reaching treating any one of them as
sacred so anyone could go in order to
save enough of the others if you want a
formal definition one of the few one of
the closest I can I know of is what
computer scientists would call a
weighted constraint satisfaction problem
so an example of that is like you're
doing wedding seating charts now at my
wedding we just let people sit where
they wanted but I hear people try to
chart out these things and so you have
all these constraints we don't want this
person to sit next to this person to be
great if this person sat next to that
person would be great event
but you know you so you try this one
arrangement that's that doesn't quite
satisfy this so you trade off you might
end up putting two people you really
don't want to sit next to each other in
order to get more of the other kinds of
constraints elsewhere oops so the idea
is that this paperclip even a paperclip
Maximizer if we're leaving it at this
complex level of paperclip so whatever
paperclips are as a goal it'll figure
out it'll specify its goal slash learn
its goal by appealing to a bunch of
other relevant considerations in this
kind of coherence hopper any other kind
of information that might be relevant
it'll use that to figure out which way
it should specify this goal and indeed
Bostrom's own favored learning approach
for when he's talking about loading
human values is goal learning he calls a
ivl this is again out of the machine
intelligence Research Institute it's
really interesting proposal about
putting a probability distribution over
utility functionally intractable but a
good idea and and one I'm very
interested in but for my purposes anyway
it looks like what I'm calling a
coherence reasoner it's using its
beliefs about the world to shape its
utilities to shape its values its
ultimate values and then it's using
those values to shape its beliefs
because you know you can only believe so
many things so you got to seek out
certain things so just going back and
forth with these things so now okay fine
coherence how might that lead to ethics
well first of all a coherence constraint
seems to be enough by itself to rule out
at least some final goals as irrational
and that would already be at least some
trouble for the orthogonality thesis the
classic example here is a philosopher
Derek Parfit come has this example of
someone who has at bottom future Tuesday
indifference so she avoids pain on all
days including current Tuesday's just
not future Tuesday's
she'll schedule her dental appointment
for a future Tuesday and she'll say
forget the anesthetic just give me the
20 bucks instead because I don't care I
don't care about paying on a Tuesday
perfect wants to say this is just plain
an irrational final goal I think even a
purely instrumental account could
explain why that's irrational but I
should put that aside for time but at
any rate it's pretty clearly an
incoherent goal so here at NYU there's a
plus four oh shoot is a Sharon Street or
Sharon Street good who wrote a paper on
future Tuesday and difference and she
really spells out what it would look
like
for someone to have this as a bear fact
at bottom she would know as an
instrumental reason er that like wind
Tuesday comes around she's going to want
to avoid that dental appointment so
she'd better hire thugs
today to carry her Cooke kicking and
screaming to the dental appointment but
that's gonna cost more than the 20 bucks
for the anesthesia and so on
it doesn't look what street ends up
saying at the end of the day is this it
looks like an agent a war with herself
which I hear saying it looks like two
agents in other words we think agents
are unified in a certain coherent kind
of way it's a practical kind of
incoherence now there is some tradition
and philosophy that says once you have
this coherence on board this practical
coherence well then ethics is done being
unethical as just being incoherent in a
certain kind of way famously Immanuel
Kant defends something like this being
unethical is just being contradictory
myself I don't buy that and on many
other philosophers don't buy it I don't
think coherence is enough so the the may
be the best defender of the content-type
view today is Kristine course guard and
Allen Gifford in in a response to course
guards work says look it seems possible
to have a Caligula who thoroughly
coherently just wants to maximize
suffering in the world that seems quite
possible and I'm inclined to think
that's right but
coherence maybe plus one other weird
fact about super intelligent agency
might do the trick to get us some ethics
and here's that other weird fact so
bostrom says software agents can easily
switch bodies or create exact duplicates
of themselves and maybe swap memories
download skills radically modify their
cognitive architecture and personalities
well if you think about what that means
radically altering your personality if
you were to wipe out all my beliefs and
goals and put for some reason the random
person we keep thinking of as Donald
Trump if you put his not so random but
if you put his beliefs and goals inside
arguably you've killed Steve Peterson
and put someone else in his head right
or at least it seems like there's a
matter of degree here this is of course
philosophers will recognize the problem
of personal identity and one of the
interesting things about AI as Chalmers
pointed out in a singularity paper long
ago is it makes abstract seeming
philosophy problems like personal
identity very real very fast so boström
points out a population of such agents
might operate more like a functional
soup than a society composed of
dinked semi-permanent persons the lines
between agents blur and the way
philosophers might put this is there's
no fact whether a robot plans to execute
so here I am the super intelligence bad
casting but I'm but I'm I'm planning
that okay this is going to take place
and there's gonna and I want this thing
to do that thing next Tuesday or
whatever future time there may be no
fact whether that's me or not or just
some agent of mine that I've set in
motion some descendant of mine instead
there may be no fact of the matter and
so this a gentle soup so boström talks
about a teleological thread that's
what's going to like it's the goal at
the end of the day unifies this mass of
nantes non distinct agents it's the
teleological thread and there it's
unified only by this coherent goal which
remember it's busy specifying it's
trying to figure out what its goal is
and it's trying to use all relevant
information to figure it out so
importantly this coherent goal thread it
seems will extend not just to my neutral
term is successors like future self as a
successor and my descendants the people
the Michigan's I made to full continue
these goals are also successors and
similarly a neutral term predecessor my
past selves and my ancestors are going
to be part of this coherent teleological
thread that I'm trying to work out for
myself again I'm still the super
intelligence it may not seem that way so
so in other words my predecessors are
part of this threat that I'm trying to
reason over but of course part of that
teleological thread are my designers the
the the AI the human AI designers and
their intentions that's grist for my
coherence mill about what my goal really
is while I'm trying to learn it that's
information that I can use right so of
course I hope it goes without saying
almost that the the fact that my AI
designers are organic and I made out of
metal that that's totally irrelevant
right so very widespread assumption what
they called yesterday the the substrate
independence view right that doesn't
matter they share my goal and that's
enough to share my thread so what this
means is inferences about designers
intentions are going to shape what my
goal like the designers intentions are
relevant what do they really want me to
do
as I try to figure out what my goals are
and well you might say but Steve that
doesn't what if what if the designers
were evil what if the designers goal was
to take over the world well then the AI
will pick up on that and but they're not
the beginning of the theological thread
other people had intentions from them
and so on this thread extends back and
back and indeed it's probably there are
probably aren't sharp lines to
teleological threads especially given
that they're trying to work out what
their goals are as they go so now it's
starting to look like Humanity or a wide
swath of humanity and maybe beyond even
our species so in other words the super
intelligences can be doing coherence
reasoning about its own final goals what
it truly values at bottom while
respecting the goals of others that
sounds like impartial reasoning that's a
kind of holy grail of you know so this
Derek Parfit for example tries to break
down personal I'd any lines for just
this kind of reason to bring
impartiality and possibly that's just
ethical reasoning and at least sounds
like your cows Keys go here and
extrapolate a vision right in other
words I got to try to figure out what
humanity really wants what oh shoot it
is a vision though of loveliness you're
like oh you're right volition makes more
sense I'm you should change that in your
papers back to back to volition I assume
I copied it right so final goals reached
by super intelligent ethical reasoning
well plausibly so the super intelligence
who's doing this kind of coherence
reasoning about all goals trying to
figure out not just what all the goals
are but what they probably really wanted
this instead and blah blah blah yeah but
these seem to want this and they've got
all these goals well plausibly a super
intelligence working on that problem of
how to balance all these goals is going
to be way better at ethics and we are is
going to be super ethical so in summary
the the short story is learning a
complex goal requires coherence
reasoning the coherence reasoning will
extend beyond one agent to goal threads
there won't be a sharp line between
agents and partiality sneaks in and
reasoning out final goals while
respecting others goals looks like
ethics on a lot of accounts now here's
the main disadvantage from my view I
think is that I haven't said anything
all this relies on having to learn a
final goal I haven't said anything about
simple goals there's a part of me that
still tempted despite some pressure I
got from Eliezer and others yesterday
there's a part of me that's still
tempted to say maybe a superintelligence
in virtue of being a super intelligence
has to have complex goals because very
roughly if if there's no sharp line
between instrumental and final goals and
if a super intelligence has something
like a very wide array of of
instrumental goals then that means that
the equivalent of a complex final goal
but I'm not at all positive that's right
so it's certainly a problem at any rate
I think the lesson for many of us is
that it's worth paying more attention to
the goal side in AI and that's certainly
coming out and what Stuart's been saying
max has known his head I'm glad to see
that the A's are and others that it is
weirdly neglected like I you know I
spent I'm just a philosopher but I like
to dabble in the math once in a while I
went through hunters book and come first
in and some other stuff and like they're
just as Streeter was saying so all this
this value function or what is the
reinforcement it's all exogenous and
they specified right it's just like
handed down from God the values I think
we got to study that more and I think in
particular philosophers can contribute
with some work on mental content believe
it or not but that's just a hunch
finally I want to emphasize I'm not
saying that a super intelligence I'm not
at all confident myself that a super
intelligence will thereby be ethical I'm
trying to moderate Bostrom's worrisome
in part because I wrote an abstract for
the paper thinking that I could moderate
is worried and then I read his book and
so then I was stuck having to try to
respond to his book realizing that he
really thought of everything ahead of
time as he tends to do so so I came up
with this and I you know I and having
come up with it I kind of believe it but
but all I'm doing at the end of the day
is trying to fine-tune the risk
assessment needle the risk is still very
real and very much there thank you
so you kind of talked about this
teleological threat that's really
important I mean your account of like
why super intelligences are gonna be
super ethical and like it seems to be
something like well you know super
intelligence the teleological threat
goes back before the super intelligence
and therefore like it you know it cares
about the goals of like things earlier
so if I imagine like you know the
situation in which the value alignment
process goes wrong so like maybe you
know Elliot so skipped over the slides
about like why you shouldn't make a
smile Maximizer so maybe like because of
that you know if somebody makes a
terrible mistake and invents this smile
Maximizer right and it seems to me that
like if you have the smile Maximizer
yeah it looks back and it says man there
were these humans that created me and
they like you know they were trying to
optimize for like you know happiness and
like good values and cooperation and
peace and love and stuff but man smiles
are actually the important thing and
like their their goals it seems like
it's just unclear to me why like the
things before or even part of this
threat at all so I'm wondering if you
could elaborate on that yeah great I've
heard this before and I think it's an
important concern this the key here is
about learning the final goal so in the
story you just told it's already got a
fixed well-defined content of maximizing
smiles that it sees is very different
from its from its predecessors goals but
on my picture it's trying to figure out
what its final goal is it doesn't know
yet if it's pixelated if it's tiling the
world with pictures or if it's actually
causing humans to smile for for reasons
that we like right it's trying to figure
out what it would be to to fulfill this
goal of maximizing smiles and in trying
to figure that out one relevant source
of information seems to be its ancestors
in the teleological threat right so
that's that you mean whether that's I
mean so it's important that it doesn't
yet know what its goal is it's trying to
work out what its goal is it's there's
so many ways to specify maximize smiles
which way should I specify it and if
it's genuinely open on that if it's not
if somehow it's been hardwired to just
tile the universe with smiles and yeah
we're done for
but if it's genuinely open if it's
generally trying to learn its goal then
it's gonna use that information if it's
a coherence reason of the type I looked
at thank you there is a team which is
essentially about making goals
automatically so for example a
power-play system what does it do all
the time it's searching in the space of
possible new goals and their solutions
and it includes all the goals that are
computable and so it comes up with a
sequence of tasks for itself and the
nature of the sequence of tasks is such
that each new goal that it invents is
basically the one that is easiest to
satisfy through a new skill that it can
add to its existing repertoire such that
it's curiously figuring out more and
more skills that it can execute in a
given environment where it's living and
so all this artificial curiosity stuff
and self-made goals which already exists
in AI research and has existed for about
I would say 25 years at least I would
like to see they had reflected in these
discussions which are mostly about
human-made goals and humans worrying
about what could be the next goal so all
the these automatic evolution of goals
and tasks that's something that I rarely
see in these discussions would you have
a comment yeah it's very I thank you I I
said it's our I have to I blush to admit
I I don't know it's artificial
curiosities what you call yeah I I have
to admit I don't know it it's but uh it
sounds like I mean I'd certainly be
interested in learning more and so I
stand scold it but I mean I not to say
you're scolding me but you know I but I
guess I'd have to say there's kind of I
mean one way I hear what you're saying
is it has this final goal of learning
new skills and general in other words
it's got a set final goal already it's
to I mean I I'd have to hear more
details about how the goals were
generated and so on but but I assume
it's not looking through all goal space
and looking for the simplest in terms of
I mean for one thing goal space is huge
for anything right
yeah so smart directed by what well
that's maybe the okay interesting so but
well we'd have to discuss yeah I don't
know the material so I'm so the AI is
trying to learn its goals it could spend
it could do this indefinitely it seems
to me is there a trigger at which it
says okay I've done enough navel-gazing
I'm gonna start doing something well oh
well along the way it's kind of balanced
one of the things that has to balance in
its coherence hopper is look something
as the so called exploitation versus
exploration problem right it's got a
balance oh I got to act now even though
I'm not sure about these things right so
that'll be part of it I assume part of
its coherence hopper I mean is there an
end to the coherence reasoning well I
mean you know if you think in terms of
in formal terms of this wait it's
constraint satisfaction problem there
probably is an optimal solution
it's np-hard you know so even a super
intelligence will have a hard time
finding it but but yeah I suppose the
coherence reasoning could end in that
way</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>