<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Max Tegmark's Speech to The UN About Artificial Intelligence | Coder Coacher - Coaching Coders</title><meta content="Prof. Max Tegmark's Speech to The UN About Artificial Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Max Tegmark's Speech to The UN About Artificial Intelligence</b></h2><h5 class="post__date">2017-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C2-nmWBACjI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">happening now what is going to happen in
the future and what are all the
implications first I'm introducing and
we are absolutely it's our pleasure and
we are honored to have you max here max
tegmark who is a who's a physics
professor at Massachusetts Institute of
Technology and the scientific director
of fundamental questions Institute he's
also a president of the future of life
Institute and which recently launched a
7 million research program for keeping
artificial intelligence beneficial now
with more than 200 technical papers mr.
tegmark has featured in dozens of
science science documentaries his work
with the sdss collaboration on galaxy
clustering shared the first prize in
science magazines breakthrough of the
year 2003 max please tell us how current
technological developments are alter the
balance of society security and beyond
thank you I'm so honored to be invited
to this and I'm delighted that you're
organizing this I also want to thank
uniquely European Union and everybody
else who's made this possible we just
heard about what our organization the
future of life Institute is perhaps best
known for so far this seven million
dollar research program that we've just
launched but before delving into details
about that let me take a step back and
say a few words about technology in
general our organization consists of a
lot of thinkers who love technology but
who as Cindy Smith very very eloquently
put it here earlier feel that technology
is something that both can empower and
do fantastic good in the world and at
the same time gives us new power to
screw up an even grander ways than
before so we feel we want to do
everything we can now to make sure the
technology gets used for good if we look
at not very powerful technological
inventions
fire for instance we used a strategy of
learning from mistakes we screwed up a
bunch of times and then we vented the
fire extinguisher but with more powerful
technologies nuclear weapons and telic
biology artificial intelligence etcetera
we don't want to learn from mistakes we
want to get things right the first time
because that might be the only time that
we have right and the way I think about
this is trade a great future for
Humanity we want to win this race this
race between the growing power of
technology and the growing wisdom with
which we manage the technology by
investing more in this wisdom we're
gonna hear more in this session from
Pierre and Daniel and Nick about nuclear
and bio and an AI but let me start by
talking just a little bit about nuclear
weapons because even though I want to
end up with talking about AI I feel that
while we celebrate our successes here in
the National Action Plans and CBRN it's
very important at the same time
highlight our failure so far to learn
from them when we take on new more
powerful technologies so we don't repeat
past mistakes and I think that nuclear
weapons is a great case study of inner
inadequate risk management why do I say
inadequate since we still haven't had a
global nuclear war well let me just ask
you this question which one of these two
people is more famous and let me ask you
a follow-up question which one of these
two people should we thank for us all
being alive here today because he
single-handedly stopped the Soviet
nuclear attack during the Cuban Missile
Crisis I'll give you just one hint
he wasn't Canadian so that only says
something about how little attention we
as a species pay sometimes really
important issues and moreover I would
say the lesson that we should draw from
this is that that you know relying on
luck there's a really poor long-term
strategy the issue with let's see the
Arctic cod was just one out of a
hare-raising the long string of near
misses with global thermonuclear war and
although we've
mostly focus so far about nuclear
threats from terrorism and crime we must
remember that there had been also a lot
of close calls or we almost had an
all-out nuclear war between superpowers
and even if the chance is as low as two
percent per year that that happens by
mistake you know the probability that
we're gonna screw up and within
centuries is virtually a hundred percent
so you would love you need to do better
than just hope for luck in the long term
if you play Russian roulette long enough
we all know how it ends the second
lesson I think we can learn from from
the nuclear case study here is that it's
really important to understand risks in
advance before you fully build out the
technology and I feel that we epically
failed with nuclear weapons here I feel
personally guilty about this because I'm
a physics professor I feel this was our
fault partly as physicists so let's look
at the tricky of the fact when nuclear
weapons were first built the the
decision-makers in scientists generally
thought that the worst the main risk was
that you would literally get blown up by
it and people had these risk assessments
that if things went really really bad
maybe we would kill 300 million people
or something like that now we know that
that's hopelessly naive and that this is
not even the getting blown up by it
isn't even the number one largest risk
to worry about
for example oops this is a photo from
downtown Las Vegas in the 60s you see
the mushroom shaped cloud in the
background that's how close it was to
downtown because people have totally
underestimated the dangers of
radioactive fallout and acknowledging
that by the US government has paid out
more than 2 billion dollars in damages
to settle these down winter cases and
there have been more people who were
killed by fallout from these peacetime
nuclear tests than who died in Hiroshima
Nagasaki combined but that's also not
the number one risk even though always a
big oopsie in the 60s realized that if
you set up one single hydrogen bomb 400
kilometers up above the Earth's surface
you can create an electromagnetic pulse
your pens of thousands of volts across
pretty much the whole continent
potentially this permanently disabling
Tronics cars cellphones the power grid
which can lead not only to catastrophic
infrastructure meltdown but also if you
have a long power failure together with
all these thousands of nuclear
devastated cities then there are
additional oopsies people haven't
thought about for example if you if you
actually have a long-lasting power
failure in a nuclear power plant you
know what happened in Fukushima well if
you don't keep the pumps on that
circulate the cool the water the covers
these spent fuel rods and pools like
this one it boils off within a matter of
weeks
then the Chaconne ium cladding on the
fuel rods catches fire and then you get
the super chernobyl and you you could
get that basically all of these fuel
pools there are 300 of them here's I
only draw draw drew little wind plumes
around f5 of them but you can imagine if
you do that around all of them it's just
further adding to the misery I'm
highlighting this at the metal level
just because these are things that
people hadn't thought about for decades
and decades while the technology while
we build tens of thousands of these
weapons and yes and we still haven't
talked about even the worst risk that's
been discovered so far right now if if
we were to actually use a large fraction
of the 16,000 nuclear weapons currently
exist many of which are on hair-trigger
alert so we could have if you think of
the largest couple of thousand cities on
earth you could have them all destroyed
within an hour right now if we were to
do that then a nice-looking planet here
would before too long look maybe
potentially like this as at the suit
from the firestorms Rose high up in the
atmosphere and a shot out at earth and
and this was not realized how serious
this would be until the 80s about four
four decades after Hiroshima Nagasaki
and although this had a very powerful
influence this research and push to help
persuade Gorbachev and Reagan to
negotiate the largest nuclear cuts ever
done it turned out that unfortunately
these calculations were rather
inaccurate they were made on a
supercomputer which
less powerful than this phone and it
turned out that they were too optimistic
but more modern calculations done by
some of the world's leading climate
modelers on real today's supercomputers
show that this might last not two years
but more like 10 years and for the
following summer you can see here the
temperature drops
this makes global climate change seem
like peanuts in comparison you see in
the narrative breadbasket Ohio for
example the temperatures are dropping by
20 degrees or so that's Celsius it's a
40 Fahrenheit my American friends and if
you look in and saw in Russia China you
get drops I'm like 30 35 Celsius what
does that mean in plain English well we
don't have to be agriculture experts to
realize that if this turns into this
when you're gonna harvest it's not so
awesome for food supply and what doesn't
have to make fancy calculations to
realize that rather than maybe having a
few hundred million people killed and as
in some of the worst-case scenario is
that people had in the 60s it's very
plausible that the vast majority of all
people on earth would starve to death
and then succumb to pandemics and other
things that would've followed no I'm not
great and the thing to take away from
all of this I think is simply that this
is an example of where we built the
technology first and realize the bunch
what the main risks were way way later
and as we get more and more powerful
pack we want to learn from this mistake
and really understand the threats first
so that we can avoid them in the first
place so in that optimistic spirit let's
take a closer look here at the
artificial intelligence this is a
technology which has wonderful potential
of course to do great things and we've
also seen how it's been making a lot of
progress
maybe earlier the early progress in AI
impended be involved like when Garry
Kasparov lost that IBM is the blue for
example good old-fashioned AI were some
human programmers taught the machine to
do something that it could then do way
faster than Kasparov and beat him
similar sort of old-fashioned approaches
that are very successful now our
self-driving cars and to some extent
went when the Jeopardy beat blue
this place show is won by IBM's deep
blue however most of the most recent
breakthroughs have happened and there's
been a real real see amazing series of
breakthroughs just in the last five
years where things that people thought
would take decades to accomplish have
not happened all of a sudden most of
that stuff has involved a completely
different approach where the Machine
actually learns like a child it's it can
take vast amounts of data and using
technique using deep learning another
techniques that Nick Bostrom will tell
you about can actually learn to do also
the things that the programmer has no
idea even how it did it just like your
children learn to speak your language
and you don't even know exactly how they
did it so look at this picture for
example this is something that was
science fiction five years ago it was
done last year at Google you just send
in the pixels of this image and the
computer says that's a group of young
people playing a game of Frisbee you
send in this picture and the computer
says oh that's the herd of elephants
walking it across the dry grass field
and we don't really know exactly how the
computer did it because it just learned
you know from massive amounts of data
we'll hear more again from Nick about
under the hood of what's involved in
this stuff but I just want to talk about
quickly two issues that this raises so
first of all there is the there's a so
there are two completely separate issues
we should not complete there are
near-term issues with technology that
almost exists right now and then there
are longer term things about if machines
get smaller than us one day what might
happen then Nick will tell you plenty
about the latter but in a very near term
let's talk about RTI weapons a little
bit so our organization recently
launched an an open letter on autonomous
weapons where which was signed by over
20,000 people and and about 3,000 of the
world's leading robotics and AI
researchers and this open letter it was
very much inspired by the Chemical
Weapons Convention that we heard about
from the Dead Sea guy Nicola and the
biological weapons convention that we
heard about from David
why did these people these researchers
sign this well people who go into
biology generally want to make the world
better they don't go into it because I
want to make bio weapons people who go
into chemistry they want to make the
world better not to create chem weapons
it's the same of course with these AI
researchers they want to use AI the cure
diseases to help alleviate poverty and
do great things not to figure out new
ways of mass murdering people or
destabilizing the world and they feel
concerned that their their technology
that they're building is being
bastardized for really destabilizing
uses what are some of these things that
these people worry about well we for
example today when when drones are used
to kill people it's always a human who
makes a decision and is removed
controlling the drone from somewhere
right
but within years we will have the
technology that we can completely
eliminate the human from this just have
the drone fly around for a few hours
find somebody use its own a nice offer
just like that elephant recognizing
things saying oh this is the person who
looks like it's our enemy and then have
it have it killed with no
human-in-the-loop a big risk with these
things is that if once any superpower
goes ahead and mass produces this thing
of course all other superpowers are
gonna want to do so too and we'll have
it arms race on our hands but this arms
race these researchers feel will be very
very different from the nuclear arms
race because whereas it's very expensive
to build nuclear weapons and very hard
to get hold of the materials these
weapons will be incredibly cheap you
don't need any hard to obtain materials
a quadcopter costs few hundred bucks on
amazon.com today the software cost
nothing once it's developed and you can
have the potential that someone with an
ax to grind for you know under a
thousand dollars you know me back up if
super powers build this if you get the
arms race going before long North Korea
is gonna decide the bill lid and and so
on and so forth and before long some
country in the need of cash is gonna
sell this on the black market and then
and then all sorts of non-governmental
organizations with an extra I will have
them it and these are perfect weapons
for for example assassination you can
program in the fifth
your nemesis looks like have a thing fly
for two hours identify the person kill
him and then self-destruct no one knows
who did it
straight for ethnic cleansing you can
program these things to look for a
certain ethnic group only and kill them
that they're very very cheap you can
imagine what swarms of little bumblebee
sized things with which just the
recognizable face will find the eyeball
which is a source so off this part of
the skull fires our little bullet there
which is very cheap and you don't need a
lot of power kills people if you have
thousands of those you know it it would
completely transform warfare in a way
that's very hard for foreign nations to
defend against other than by creating a
police state and for this reason there's
a very quick broad consensus among the
researchers in this field that this is
an arms race you just shouldn't start
that's the best way to stop it and I
want to just conclude by pointing
forward a little bit towards Oh Nick
Bostrom who's gonna follow me here
looking at super intelligence sometime
in the future maybe in 40 years maybe in
hundreds of years maybe never we'll see
there's certainly the possibility that
we might make machines the King be where
everything that we humans can do and
then what well we our organization
organized the first ever conference of
AI researchers to talk not about how to
make things smarter but to talk about
this issue in particular how we can win
this race and have wisdom she paced with
the technology it was in Puerto Rico in
January of this year and it was actually
really productive there was a very
strong consensus that emerged that this
is something we need to think about the
goal of artificial intelligence should
be redefined from having the goal of
just creating pure undirected
intelligence towards creating beneficial
intelligence and there was a very look
we brainstormed up a very detailed
action plan a list of research projects
that should be done that would tackle
embarrassing unanswered questions that
we need to answer and we need to it
might take decades to answer them so we
should start researching now you know
not the night before a bunch of guys on
red bow you know switch or on their
thing and what was very exciting about
this was that Elon Musk
was present at the conference and he
said look I hear you guys you want to do
this research well let me give you ten
million reasons to do it
and with his donation we were able to
launch a worldwide competition for
research ideas we were overwhelmed by
getting three hundred teams from around
the world putting in wonderful proposals
and it's very painful for the experts
who have to review this but to pick out
winners but 37 teams have now been
selected and I've started to work on
this and it is I think you'll be very
very exciting to keep following how this
develops we view this as just a little
bit of seed funding for the wisdom and I
would encourage all of you with
resources of governments and big
organizations to remember that if we
want to win the race between the power
of technology and the wisdom with which
you manage it we have the mindful of the
fact that almost all the investments
right now just go into making the
technology more powerful there's almost
no investment on the wisdom side so if
you're involved with any organization
and get help a little bit ramp up this
sort of research you would do humanity a
wonderful service
Thank You max thank you very much for
this absolutely inspiring in wonderful
presentation we are now at uniquely
trying to invest in the wisdom side
certainly and want to have you join us
in this endeavor as well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>