<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IBM's Dr. Dharmendra Modha - Advances Towards Building an Artificial Brain | Coder Coacher - Coaching Coders</title><meta content="IBM's Dr. Dharmendra Modha - Advances Towards Building an Artificial Brain - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IBM's Dr. Dharmendra Modha - Advances Towards Building an Artificial Brain</b></h2><h5 class="post__date">2017-09-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SQ9l7vmSROU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'll tell you about the progress with
TrueNorth chip and the software
ecosystem which started from the DARPA
synapse project and where we are today
so we started by understanding
neuroscience and what you see here is
some of the reason the work that we did
very early part of the project in terms
of understanding the neural anatomical
structure it's right here very safe
I see great thank you this is
co-director of the human brain project
is going to join me in the true North
project now I just charged my computer
so the karl-heinz is a good friend so we
started in selling the neuroanatomical
structure of the brain especially the
large scale connectivity which is guided
how we have designed the communication
infrastructure underlying our chips and
then having understood the available
data on the micro and macro cortical
connectivity we created some of the
largest simulations we created
simulations at the scale of hundred
trillion synapses using 96 racks of Blue
Gene cue Sequoia 1.5 million processors
1.5 petabytes of main memory six point
three million threads and yet the
simulation ran 1,500 times slower than
real time meaning that a hypothetical
computer to simulate hundred trillion
synapses in the human brain would
consume about 12 gigawatts of power
that's the entire power generation
capacity of the small island nation of
Singapore just for one simple human
scale simulation so clearly current
computing isn't suited for energy
efficiency and throughput of brain
inspired computing and why is that this
curve provides the answer
on the x-axis log scale is the clock
frequency on the right on the y-axis log
scale is the power density and starting
from the very first microprocessor over
three decades we have increased both the
clock frequency and power density but
the brain sits alone in a totally
different part of this curve orders and
orders of magnitude away in terms of
power density and clock speed so this is
the mystery that we trying to unravel
and deliver in terms of a computational
substrate so what's the idea the idea is
you heard about neocortex today morning
in the ability that it sort of looks
more or less translation invariant
through the surface of the cortex and so
this is an idealization of the can in a
hypothesized canonical coracle micro
circuits that tile the surface of the
cortex each of this essentially consists
of a small module of neurons let's say
roughly about 250 hundred or so there
are tiled through the fabric of the
brain so we took this as an inspiration
and imagine a very small neural network
essentially a bipartite graph with 256
neurons 256 exons and 256 square
synapses and that's what we called a
neural synaptic core a neurosynaptic
core fundamentally brings together
computation neurons memory synapses and
communication and therefore already
begins to bypass the limitations of the
warren neumann architecture and then
what we did is we went back and we
looked at the white matter the long
distance macro connectivity in the
primate brain and by marrying the
neurosynaptic core concept with the
macro connectivity we imagined
architecture which is essentially a
network of neurosynaptic cores
each core is a small neural network a
bipartite graph
they interconnected on a chip by a
network on chip and the chips themselves
can be interconnected in an endless sea
of neurosynaptic course either in two
dimensions three dimensions in a wafer
scale or individual chips same idea
fundamentally natively homogeneously
scalable on one end and extremely
powerful and high speed on the other
hand a paradox the one cannot achieve
with one I'm an architecture and to
demonstrate the concept in 2011 we
demonstrate the first neurosynaptic core
the core could already do pretty nice
tasks for example it could play pong and
Todd Hilton who was our DARPA synapse
program manager it played with him at
the end of her face - he lost we got our
phase 3 funding Thank You Todd the chip
in of course was heavily featured but
then going forward we took at you a core
with 256 neurons we scaled it in power
by two orders of magnitude 100x we
scaled it in area by or magnitude 15 X
and we tiled not just one two four eight
but 4096 of them into the TrueNorth chip
the chip has 4096 cores in a 64 by 64
array with a network on chip 1 million
neurons 256 million synapses 5.4 billion
transistors so it was at that time the
largest IBM chip and one of the largest
chips made for square centimeters area
was made in samsung 28 nanometer
low-power process in spite of its
computational capability the real secret
is in this chart which shows the thermal
consumption the power density of the
chip versus an FPGA on the same board
which is just passing spikes to the chip
the chip consumes just seventy milli
watts
the power density is 20 milli watts per
square centimeter the power density of a
high-end server is over 100 watts per
square centimeter so we are talking
about 5,000 X reduction in power you can
tile this in 3d you can do wafer scale
you can actually just or you can make it
ultra small that's it the idea and going
forward what we have been doing so this
is to catch up and I'll tell you the new
information is we created a tiny little
single chip board size of my palm size
of an index card with 1 r2 North synapse
chip and now we have been giving this
out to universities and also we have
been shipping many of the boards and
systems to government customers as well
as commercials in once we build a single
chip board there was a desire on part of
Air Force Research Lab to build a scale
out supercomputer so here's a design of
a computer with 16 single chip boards
into a chassis 16 million neurons 4
billion synapses so since we build one
now we have order to build two more and
we'll be building one for army and we
are planning to build one for Navy as
well and so that's about a single chip
board and a scale-out supercomputer but
the chip has incredible capability in
terms of tiling this is the to not chip
you can just tile them seamlessly and
have them connect and behave like the
complex is a larger chip so there are
ports eight ports on each side which
allow the spikes to go between chips as
if they're transparent without need for
any additional communication circuitry
and so as a result we are building our
scale-up supercomputer with 16 chips 16
million neurons 4 billion synapses that
actually will be shipped to Lawrence
Livermore National Lab Department of
Energy god-willing March 31st of this
year so as always slip up right what's a
week or so in research so now this is
about the hardware
programming it so currently the dominant
paradigm in programming and showing
applications is deep computing deep
convolution neural networks now what is
the difference between these neural free
computing and deep learning well deep
learning is about capability it's about
accuracy in terms of application
performance it's used to program in an
offline fashion neuromorphic computing
and neuromorphic computing is about
unprecedented energy efficiency volume
efficiency speed and scalability and he
used to deliver
so think about offline learning versus
online delivery inference recognition so
the idea is wouldn't it be nice if one
could leverage deep learning to actually
program TrueNorth now this is a
difficult problem why is it a difficult
problem because to attain the
unprecedented energy efficiency and the
speed we had to look at spiking neurons
nobody knows how to do learning with
spiking neurons and back propagation we
had to use limited precision synapses
widely held belief that you need high
precision and we used canonical core
wise connectivity where a lot of the
networks have full connectivity or very
large fan-out
so the key insight that we will publish
shortly is the it is possible even with
these three constraints which seem
negative to map both back propagation
and collision neural networks to true
north and achieve state-of-the-art
accuracy energy efficiency and
unprecedented throughput and not only
did we do this we actually built an end
to an workflow a workflow that now can
be taught to a programmer in about a
half a day this is a far cry when we
first started programming TrueNorth
about two and a half years ago it used
to require six months of residential
training at Almaden came down to three
months last August we
held a three-week bootcamp which are 30
universities and government agencies
attended and now we are down to a matter
of days remarkable progress in
ease-of-use and now let me show you some
of the demos couple of the demos I'll
show you were before the deep learning
work so here this is actually a video
from DARPA neovision heli datasets a
helicopter flying over Los Angeles
Municipal Airport in a multiscale
Pyramid of edges being extracted with
milli watts of power at 30 frames per
second another demonstration same sort
of a dataset being able to extract
luminance color contrast again 30 frames
per second real-time and now taking all
these features and now here's where we
use deep learning and apply them to two
datasets
one is neovision stanford dataset
stationary camera one is moving camera
moving targets again 30 frames per
second state-of-the-art accuracy much
lower power than any of the existing
platforms going forward
Street View house number data set again
state-of-the-art accuracy thousand
frames per second and you can see this
live today with my distinguished
colleague dr. Palmer Ola you know
classic data set that's often used as c4
1010 class thumbnail images again beep
can you loosen Network achieving state
of the art accuracy here we are showing
of single chip version thousand frames
per second so frames per second for what
is out of the pit out of the ballpark
here now what is why is this working to
give you an insight here's a
visualization so this is C far 10 at
layer 0 the data that comes in you're
seeing a very high dimensional space
being visualized and you can see all the
data is bunched up layer 13 you can see
that data begins to disperse itself in
space
I wonder in the videos for long we're
getting out of time here and here's
layer 25 and you can begin to see
tremendous discrimination so neural
networks are fundamentally rich
structures that in spite of the surgery
that we had to do for the energy
efficiency are able to deliver the
fundamental accuracy and discrimination
moving forward you know this is another
data set Flickr logo 32 it's 32 brand
images collected by European graduate
students mostly beer
karl-heinz what do you do out there so
another one a german traffic sign thirty
forty three traffic signs again state of
the art accuracy one more data set see
four hundred again this is on a single
chip now give you another modality voice
so this is going to be a voiced activity
direction wage negotiations the industry
bargains as a unit with a single union
very simple voice background clean
he bought those rows the boats come in
stock
moving forward so now one more example
they'll be actually going to train the
network on the phonemes in wage
negotiations industry bargains as moving
the cycle Union leave on those ropes the
boats come unstuck the bungalow was
pleasantly situated near the shore okay
so what is the surprise here in my mind
this is an amazing result for totally
different reason the surprise is that
specification for True North was frozen
in 2011 one year before the current deep
convolution Network revolution so it
really speaks to the versatility the
universality and the power of the
neuromorphic substrate and not only are
we going to be able to do this but we'll
be able to extend it into many
applications because TrueNorth supports
feed-forward feedback lateral recurrent
connectivity as well as many kinds of
neuron modes and to teach that you know
we have actually been running a program
for the synapse University now we have
100 developers at 30 plus diverse
universities and government agencies now
in spite of my best efforts one
University and I'm going to single this
university out is not signing agreement
with us so but let that not be an excuse
for you so if you are in university and
if you'd like access to TrueNorth board
an ecosystem the please send me an email
and we'll do our best to make this
available to you and to make it easy for
you to learn we have a boot camp reunion
coming up in May and it will be happy to
invite you to it now again neuromorphic
computing is not meant to replace 1mm
computing it's supposed to be a
counterpart so if you think of one
norman computing as a left brain
symbolic structured if-then-else binary
sort of our computing then synapse or
neuromorphic computing brain inspired
computing is about pattern admission
slow synthetic parallel on one hand your
flops floating-point operations per
second on our side we have stopped
synaptic operations per second to give
you a perspective of you know since the
project started where we are project
Chile started in 2004 with a single
person you know leading from then on to
an Almaden Institute on cognitive
computing which actually just off your
present dad and so it's really great to
have you and then from then on we worked
on the largest blue gene/l
the largest blue gene P and largest blue
gene queue to carry out increasingly
larger simulations this is of course the
Henry Markham's favorite one in and then
we won the DARPA synapse project grant
and worked very closely with Todd Hilton
Gill Pratt and Dan hammer storm we
mapped out the wiring diagram by
collecting the largest data set we
created the warm scale chips with 256
neurons
I mentioned the simulation of the scale
of human brain with 100 trillion
synapses to development of the TrueNorth
ecosystem so not only do we have a chip
but we have an end to end programming
paradigm from user interface deep
learning down to a programming language
form where as well as the whole user
flow and debugging tools right in on top
of that we have created various boards
and we are making plans by December of
next year to create a 128 chip system
that should happen so we are now just at
the beginning of the next generation in
the next generation we already can
imagine new chips new software ecosystem
new training new systems and even new
sensors and new algorithms and
applications which is fundamentally
allow us to create completely new neural
networks and we have a roadmap for
building systems by using example of a
scalability of TrueNorth in the whole
idea is a sort of a spiral we went from
architecture to simulation to chips to
applications and now that we have the
benefit of insights not just from our
work from virtual colleagues as well the
we can actually again do another turn on
the spiral and the ultimate vision which
I believe will be possible before 2020
ends is that we will be able to produce
a brain in a box which was the original
vision of synapse project 10 billion
neurons in 2 liter 1 kilowatt this is no
longer science fiction it is happening
so the project you know has received a
lot of adulation
I just want to flash it in front of you
real quick but this is very important
last fall we had a chance to present the
tiled board to President Obama and we
received a real signed letter
congratulating the team which is become
a prized possession for our team and it
was presented to the House panel on
science as well as Senate panel on
science
and that I'd like to thank on the DARPA
program managers for the vision their
partnership I like to thank you know our
current customers Air Force Research Lab
DARPA Lawrence Livermore National Lab
Army and hopefully Navy coming soon
these are for the collaborative work I
saw professor Phillip Wong you know
who's been part of the project since
very early on Department of Energy eight
IBM labs and fabbs and just the synapse
project itself at six universities but
of course now we expanded I would like
to thank Livermore and then si for all
the supercomputers that they gave us and
all my wonderful colleagues it's a
really a joint work and some of them at
Alma than today with that thank you very
much for your attention and thank you
for inviting me Murad and please please
remember to see my colleague dr. Palmer
Allah today evening for the demos John
so is the question we agreed upon right
pardon the question we agreed upon right
no no so some of us do do back
propagation and spiking neural networks
and and some other things but the the
question that I have is right now your
training is done offline yes and I think
it would be fantastic to implement all
that on a 28 board chip do you have any
plans to put that learning algorithm
into the chip and accelerate that
portion of it we certainly do and it's
again you know it is a accelerating
learning has to be done with far more
clear than accelerating inferences you
can understand because you don't want to
tie yourself down so that you cannot
explore the full space of learning
algorithms I'm certainly happy to talk
to you about this so now for your for
your brain in the box one liter you were
definitely you to later sorry you would
guys this is not fair you would I give
you ten liters okay okay but you would
still need two different solid-state
circuit technology so what what will be
your choice seven nanometers that would
be sufficient to put a brain in a box
will be
Computers I see but you know just just
to be very fair iBM has a huge
investment in advanced process
technology right and we're continuing to
do that and as such technology becomes
available that can be integrated with
the existing process we stand ready to
take advantage of it so I know I am I'm
an engineer I'm not a scientist and I
want to make things work so I'll accept
anything that works thanks guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>