<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Stuart Russell - The History &amp; Future of Artificial Intelligence | Coder Coacher - Coaching Coders</title><meta content="Prof. Stuart Russell - The History &amp; Future of Artificial Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Stuart Russell - The History &amp; Future of Artificial Intelligence</b></h2><h5 class="post__date">2017-09-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QPxYd6rw9YI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my field of work is artificial
intelligence and since I started I'd
been asking myself how how can we create
truly intelligent systems so part of my
brain is always thinking about the next
the next roadblock that we're gonna run
into you know why what we understand how
to do so far is going to break when we
put it in the real world you know what's
the nature of the breakage what could we
do to avoid that you know how can we
then create the next generation of
systems something that will do better
but more recently I've also been asking
myself a question that I had in the
first edition of my book which is what
if we succeed so that was back in 94 and
in 94 I was you know it was a fairly
balanced picture that you know the
benefits for mankind could be enormous
but there were potential drawbacks you
know even then it was obvious that
success an AI might mean massive
unemployment and so on and then the
question of control if you if you build
systems that are smarter than you it's
obvious that there's an issue of control
right you only have to imagine that
you're a gorilla and then ask you know
should my ancestors have produced humans
from the gorillas point of view that
probably wasn't such a great idea so but
I would say in 94 I didn't have a good
understanding of why exactly we would
fail to achieve control over AI systems
I mean people draw analogies for example
to gorillas and humans or to humans and
superior alien civilization but those
analogies are not exact because the
gorillas didn't really consciously
humans and the we wouldn't be
consciously designing the alien
civilization so I ask myself what is the
nature of the problem and can we solve
it because I would like to be able to
solve it
the alternative to solving the control
problem is to in some way either put the
brakes on AI or prevent the development
of certain types of systems altogether
if we don't know how to control them
that that I think would be extremely
difficult because there's this huge
pressure right we all want more
intelligent systems they they have huge
economic value you know I think Bill
Gates said you know if you if you can
just solve the machine learning problem
that's worth 10 Microsoft's which at
that time would would be then that would
come to about four trillion dollars so
for you know four trillion dollars is a
decent incentive people to move
technology forward
so those are you know that those two
questions how can we make AI more
capable and you know what if we do and
what can we do to make sure that the
outcome is is beneficial those are the
questions that I ask myself and another
question I asked is why duh why do my
colleagues not awesome cells this
question is it is it just sort of
inertia that that a typical engineer or
computer scientist is is in a rut or on
the rail of moving technology forward
and they don't think about where the
railway is heading or whether they
should turn off or slow down or am i
just wrong I you know have I is there
some mistake in my thinking which has
led me to the conclusion that the
control problem uses serious and
difficult
but I you know so I'm always asking
myself you know am I am I making a
mistake and I I go through the arguments
that that people make for not paying any
attention to this issue and to me they
none of the arguments haul water and
they they fail in such straightforward
ways that it seems to me that the
arguments are coming from a defensive
reaction not from taking the question
seriously and thinking hard about it but
not wanting to consider it at all
because obviously it's a it's a threat
you know if you're and I think we can
look back at history of nuclear physics
we're you know very famous nuclear
physicists was simply in denial about
the possibility that nuclear physics
could lead to nuclear weapons I mean
it's the idea of a nuclear weapon was
around since at least 1912 I think was
when HG Wells wrote the shape of things
to come which included what he called
atomic bombs he didn't quite get the
physics right he imagined bombs that
would explode for weeks on end so they
would liberate an enormous amount of
energy but not once over a long period
so they would lay waste gradually to a
whole city but the principle was there
and and there were famous physicists
like Saudi who who understood the risk
and agitated to to think about it ahead
of time but then there were other
physicists like Rutherford who simply
denied that it was possible that this
could ever happen and he denied it was
possible up until the night before the
law invented the nuclear chain reaction
so you know the the official
establishment physics position was it
could never happen you know and it went
from never to 16 hours and I I don't
think the same thing could happen with
AI because I think we need more than one
breakthrough I mean you know even
arguably zealots break through this
figuring out that you could make a chain
reaction with neutrons which did not get
repelled from the nucleus in the same
way the protons do that was the key
breakthrough but it still took a few
more years but not very many five to six
years before a chain reaction was
actually demonstrated you know a five to
six years is an incredibly short time
and if we had five or six years to the
point where though there were super
intelligent AI systems out there we
wouldn't have a solution for the control
problem and and I think we might see
negative consequences if we well if we
were lucky they would be contained and
that would be an object lesson in why
not to do it sort of like Chernobyl was
you know a medium-sized object lesson in
why it's important to think about
containment of nuclear reactions I can't
claim to have thought too much about
containment and control early on I I've
been doing AI my first AI project was a
chess program in 1975 in high school and
at that time I mean I read a lot of
science fiction growing up so you know I
I and I'd seen 2001 so on so the the
idea of machine intelligence getting out
of control
you know there are Star Trek episodes
and what kind of things so the idea has
been around for donkey's years in
popular culture so I knew about all that
but I think I was
you know I pretty straightforward
technical techno optimist in my youth
and no to me the the challenge of
creating intelligence was just
fascinating and irresistible the history
was I so I studied computer science in
high school wrote a chess program was
very interested in machine learning read
some AI books didn't think that AI was
at that time a serious academic
discipline you know I wanted to be a
physicist so I studied physics is an
undergrad and man I learned that in fact
though there was a possibility that you
could do a computer science PhD and
study artificial intelligence and so I
applied to PhD programs in computer
science in the US but I also applied to
two physics PhD programs in the UK
Oxford in Cambridge and for various
reasons I just decided to take a break
from physics science spoken to physics
graduate students and postdocs and
professors and didn't get a very
optimistic picture of what it was like
to do particle theory that you you know
you would spend a decade creeping up an
author list of 290 people and if you
were lucky after umpteen years of being
a postdoc you might get a faculty
position but you might end up being a
taxi driver instead so this was so I
graduated in 82 and there wasn't that
much going on you know it was just
before string theory became popular
people were looking for grand unified
theories of physics not finding anything
very promising or even testable and I I
remember very clearly conversation I had
with Chris Llewellyn Smith who was on
the faculty that was shortly before he
went to be director of CERN and
of the people that I had met and taking
classes from rocks that he was the
brightest very engaging very intelligent
man and I had a meeting with him and I
asked him what was he working on and he
said he was working on taking all the
grand unified theories then in existence
of which there were 81 he was converting
them all into mathematical logic which
are you about having studied a little
bit of AI and then in the in in
mathematical logic it would be possible
to to directly compare two theories to
tell if they were actually equivalent to
each other or different and whether they
had testable consequences and I think
that was a relatively new idea for
physics to actually do that not by just
arguing but actually mathematical proof
and I think he got through 64 of the 81
theories and it turned out that there
were only 3 distinct theories so all
these people were producing theories not
even realizing they were the same theory
as everybody else's and two of the few
theories were in principle untestable
meaning they had no observable
consequences on the universe at all and
the third one could be tested but it
would take ten to the 31 years to see
any observable consequence of the theory
so that was a pretty depressing
conversation for me and I think that
probably tipped the balance
along with sort of the mood of the grad
students and postdocs to towards going
into computer science and going to
California so after I arrived at
Stanford's I I would say I hardly met
any computer scientist I had worked for
IBM for a year between high school and
college and they had some very good
computer scientists where I worked and I
did some interesting things there so I
think that gave me more of a sense of
what computer science was like as a as
an intellect intellectual discipline the
kinds of problems people worked on
I met Alan Bundy at Edinboro because I
also was admitted to the ph.d program I
didn't Edinburgh which was the best AI
program in the UK and but by and large
people advised me that if I if I got
into Stanford or MIT I should go to
Stanford or MIT and I got into Stanford
despite applying sixweeksoff to the
deadline and they were very kind to
consider my application anyway so when I
got there my first advisor was Doug
Leonard so he you know so several
faculty had given their spiel at
beginning of semester saying here's what
I work on I'd love to have PhD students
join the group and and Doug was just
incredibly upbeat and optimistic and you
know really working on cool problems he
described a little bit his eurisko
system which was a sort of multi-layered
machine learning system which was
intended to sort of be able to grow into
an arbitrarily intelligent system so
Doug was very ambitious and I like that
so I work with Doug for a while
unfortunately Doug didn't get tenure his
ideas were maybe a little too ambitious
for most of the academic community and
so some people did not see enough rigor
or clarity and his papers and
experiments and so he didn't get tenure
I then I then work with Mike Jeunesse
Roth who was a much more mathematically
rigorous he really liked every paper
should have theorems and
we wanted to build a very solid set of
capabilities and concepts but he still
had you know he still had the grand
ambition of creating truly intelligent
systems but you know also he was
interested in creating useful
technologies for machine machine
diagnosis does automated design things
like that well I mean I also I mean I
interacted somewheres l homme Anna who
was it was more of a computational
logician interested in verification
synthesis using formal logic so I
absorbed a lot of interesting ideas from
him although he wasn't particularly
interested in AI so it didn't didn't
quite match there was a time when I was
trying to have Doug Leonard and Zohar
Menna as my two thesis advisors and they
just didn't see eye to eye at all so I
didn't really work out so yeah 82 was
when I went to Stanford
so Feigenbaum was there and nels nelsen
was at s RI down the road and you know
Minsky I think had somewhat dropped out
of sight he wasn't publishing active
leads and lost he had done the frames
paper in 76 I think which had influence
but I think you know Stanford as as many
universities do they they had their
brand of AI and they didn't really go to
great pains to introduce students to
everyone else's brand of AI so with at
Stanford there was a heuristic
programming project which ed flagon of
Aharon which was very much about expert
systems Mike Nesmith was was part of
that but had a more logic based approach
her ability was viewed as not
particularly relevant there were
arguments as to why you couldn't use our
ability to to build expert systems
probability
but yeah so but it started to creep in
through largely I think through Eric
Horvitz and David Heckman who were
graduate students in the medical a I
program we Ted short leash and they had
read about new T'Pol's work on bayesian
networks or belief Nets as they were
called in and and I that I think was you
know I I began to understand at that
time how important that work was and
then when Perl this book came out in 88
I was pretty convinced that what I had
been told about probability was wrong
that it was entirely feasible to use par
ability and in fact it worked a lot
better than the sort of rule the rule
based approach to uncertainty which the
Stanford group had been pushing so I
think so my my thesis research was in
the area of machine learning but
applying the tools of logic to
understand what was going on in a
learning system and in particular how a
learning system could use what had
already new to learn better from new
experience and it seems to me that that
that problem is it was crucial and it is
crucial today because when humans learn
they bring to bear everything they know
already to help them understand the new
information and you know so humans learn
very quickly from often one or two
examples of some phenomenon or a new
type of object or experience that they
might have and current learning systems
might need tens of thousands or millions
of examples and their their design
current learning systems are designed to
learn
with no prior knowledge or very little
prior knowledge which is which is good
if you know nothing but you know that
only explains possibly the first five
minutes of a human's life you know after
that he would already know something and
he's already using what they know to
learn the next thing so tabula rasa
learning he's isn't it you know it's a
good thing to study but you can't be a
good explanation for intelligence you
know unless you can show that you just
start with this blank slate and and it
you know you keep feeding experience and
it becomes super intelligent and we're
not anywhere close to that right now
and if you think about what's going on
with current learning systems I know
this is a digression all right think
about what's going on with current
learning systems we're teaching them to
learn to recognize you know a sheep or
an Oldsmobile right these are discrete
logical categories and there we do it
we're doing that because it's useful for
us to have sheep recognizes our Altima
bill recognizes or whatever it might be
but if that's going to be part of a
larger scale intelligence system you
know what and you you know and if you're
a deep learning disciple you don't
believe that deep learning networks use
discrete logical categories or just or
definite knowledge that sheep have four
legs or any of those things well why you
know why do you think that training a
sheep recognizer he is a step towards
general purpose intelligence
unless general purpose intelligence
really does operate with discrete
logical categories which at least
introspectively we seem to so the first
thing I did that if you like was
considered to be a big deal outside of
you know one branch of the machine
learning community was the work on
bounded rationality so-so intelligence
is in my view the ability to act
successfully you know and the ability to
think correctly or learn quickly and so
on these are all you know they they have
a purpose which is to enable you to act
successfully to to choose actions that
are likely to achieve your objectives
and so that their definition of
intelligence has been around in the form
of what economists would call
rationality what control theorists would
call optimal control people in
operations research would call optimal
policies for decision problems and it's
it's clear that in some sense that's the
the right definition for what we want
intelligence to be in AI the the
definition of intelligence if there is
one had been a restricted form of that
which is that you have a logical goal
you know I want to get to this place or
I want to construct this building or
whatever it might be that's the
definition of success and an intelligent
agent is one that generates a plan which
is guaranteed to achieve that goal and
of course in the real world there are no
guarantees and there are trade-offs that
you don't I mean you don't want to get
to a place if it means dying along the
way for example and so uncertainty and
trade-offs these these are encompassed
in the economic definition of
rationality is a maximization of
expected utility but it seems to me that
couldn't be the basis for AI because
it's not computationally feasible so if
we set up AI as a where the field that
builds utility maximizing agents then
we're never going to get anywhere
because it's not feasible
we can't even you maximize utility on a
chessboard
chessboard is you know tiny discrete
simple well-known fully observable slice
of the real world you know we actually
have to operate in the real world which
is vastly bigger we don't know what the
rules are we don't get to see all of the
world at once there are gue zillions of
other players and so the world is so
much more complicated and in starting
off with perfect rationality as your
objective is just a non-starter so so I
worked on coming up with a method of
defining intelligence that would
necessarily have a solution as opposed
to being necessarily unsolvable and that
was this idea of bounded optimality
which roughly speaking says that okay
you have a machine the machine is finite
it has finite speed it has finite memory
that means that there is in fact a only
a finite set of programs that can run on
that machine and out of that finite set
one or some small equivalence class of
programs does better than all the others
and that's the program that we should
aim for that's what we call the bounded
optimal program for that machine and you
know and also for some class of
environments that you're intending to
work in and there we can actually make
progress because we can you know we can
start with very restricted types of
machines and restricted kinds of
environments and solve the problem that
decision actually show here is for that
machine and this environment here is the
best possible program that you know that
takes into account the fact that the
machine doesn't run infinitely fast they
can only do a certain amount of
computation before the world changes and
you know I I I think that basically
reading a lot of philosophy when I was
younger was very helpful in coming up
with these ideas because you know what
one of the things that philosophers do
is
look they look for places where you're
confused or where there's some apparent
paradox and they say okay there how do
we resolve this we we step back and we
say okay we're confused or we have a
paradox because we have bought into a
bunch of assumptions about what problem
was supposed to be solving and we
actually not you know we can't be doing
the right thing if we run into these
conceptual roadblocks so how do you step
back and change the definition of the
problem so what we had been doing was
trying to define what is a rational
action and then ie the utility
maximizing action and then saying okay
that's the objective for a I build
systems that always choose the rational
action and in fact there is with a
bounded system there is no notion of
rational action that makes sense because
you're trying to ask the question okay
what am I supposed to do if it's
impossible for me to calculate what I'm
supposed to do right well that question
doesn't have an answer right and it
doesn't have an answer because the
notion of rational action does not make
sense for a bounded systems you can only
talk about you know what is the what's
the configuration that I should have you
know so that on average all do best when
I'm faced with decision problems in the
real world no it was aware of Kahneman
Tversky oh yeah I was in fact aware of
it even as a grad student I mean their
critique of rationality is an empirical
one claiming that look here all these
experiments showing that humans aren't
rational in the classical sense but that
doesn't solve the problem from the point
of view of AI what should a I be aiming
for right we're not aiming to copy
humans and a lot of what humans do is
just you know consequence of
evolutionary accident there's no
assumption that humans are the pinnacle
you know that there is no better way to
configure a human brain than the way
nature has done it
but nowadays I think actually a lot of
people are reinterpreting evidence of
human irrationality is actually evidence
of human bounded optimality that least
if you make enough assumptions you can
show that the the so-called mistakes
that humans make are the consequence of
having a very program if you like that's
very well designed given the limitations
of human hardware and you expect program
to make that type of mistake and it does
seem that humans are you know so another
another part of my work at that time was
okay so if you are if bound it if you if
you can't do an infinite amount of
computation then what computation should
you do and so that led to what we call
rational meta reasoning which is roughly
speaking that
you do the computations that you expect
to improve the quality of your ultimate
decision as quickly as possible so you
can apply this to a chess program and
use it to control the search that the
chess program does it's looking ahead in
the game and you can look ahead along
billions of different lines in the game
but a human from what we can tell is
only looking ahead along a few dozen
lines if that how does the human choose
what is it worth thinking about right
that's the question what what is it
worth thinking about and obviously it's
not worth thinking about well if I make
this really good move and my opponent
does this really stupid response well
why why think about that he's not likely
to make that stupid response and so it's
not worth my time to think about how I
would win the game if he did that really
stupid response so so humans naturally
do this when you learn to play chess you
don't learn okay well here is here is
the alpha-beta tree search algorithm and
in order to play chess you need to learn
this algorithm for how to allocate your
thinking time to various branches of the
tree it's just natural
that our brains know or learn very
quickly how to allocate thought to
different possibilities so that we very
quickly reach good decisions and so I
figured out how to to do that and showed
that you could apply this technique of
rational meta reasoning to control
things like games research and get very
good results without even designing an
algorithm right and so Mike there was
sort of a background picture which is I
didn't I don't I still don't think that
we should think of AI as a collection of
algorithms right an algorithm is a
highly engineered artifact for a
specific problem we have very highly
engineered algorithms but for two-player
games right when you go to a three
player game you need a completely new
algorithm the two player out game
algorithms doesn't work right when you
go to a game like that gammon where it's
a two-player game with chance right with
the dice rolls again you need a
completely new algorithm because the
two-player oven doesn't work but humans
don't operate that way right you know
you learn to play chess you learn to
play backgammon you don't need some
Engineer to come along and giving you a
new algorithm so it must all flow from
some more general process of controlling
your deliberations in your computation
to get good decisions quickly and so
that if you work maybe you just want say
it's sort of one algorithm but it's an
algorithm that figures out what is the
value of the possible computations I
could do and then does the most valuable
one and that's the algorithm that's it
and the same algorithm operates across
all these different kinds of games you
know single agent search problems to get
two player to play of a chance
multiplayer you know planning problems
the same principle applies and so I
think so those those were the two things
you know understanding this notion of
bounded optimality as a as a formal
definition of intelligence that you can
really work on and then
this technique irrational meta-reasoning
so those were the two things when I
worked on the late 80s and early 90s
that really I think are you know the
contributions from that period that I'm
most proud of and then you know having
thought a lot about rationality and
intelligence I decided I had to write a
textbook because I wasn't seeing these
notions clearly laid out in the existing
AI textbooks which tended to be okay
well there's this field called natural
language processing so I'll tell you all
about that there's a field called search
so I'll tell you about that as a field
called game playing I'll tell you about
that and they there was no unifying
thread there was no overarching
integrating framework so I I wrote the
textbook to say it's all rational agents
or bounded rational agents and the
particular methods that are developed in
search or game playing or planning our
responses to particular additional
assumptions that you make about the
environment and how rational
decision-making occurs under those
assumptions right so in such problems we
assume that the world is fully
observable that it's deterministic
there's only one agent and so on so
forth and under all those assumptions
then search algorithms make sense but
they're all just a special case of
rational decision-making I mean I had
written some some notes for my
undergraduate class I think they were up
to about 200 pages you know they want
they want particularly intended to
become a book it was just that I found
what I was lecturing to be departing
more and more from what the existing
textbooks said and I was actually quite
worried that you know this was also a
period so the AI winter began around 81
I was at exchanging email with the
the Aspen Institute about this just
yesterday because they had used the word
I wanted to refer to the one that
happened in the late 60s early 70s in so
the AI winter the phrase came I think
with Hector Levesque in 86 wrote a
little paper saying the AI winter is
coming and the a winter came from the
phrase nuclear winter which as far as I
can tell was coined in 83 because the
National Research Council did a big
study on potential effects of a major
nuclear war on the climate so that was
where the nuclear winter phrase games so
I think AI winter per se with that name
the first one was the late 18th so that
was after the collapse of the expert
system industry and you know so after
that you know funding kind of dried up
students kind of dried up and I was
really quite worried that the field was
gonna fail and part of it was that we
was we were still using textbooks that
had been written in the 70s or early 80s
and were not really capturing what we by
that time so pearls book came out in 88
alright so by that time we actually had
Bayesian networks you know which solved
a lot of the problems that the expert
system that caused the expert system
industry to fail I think there were a
number of reasons why the expert system
industry failed so what they were doing
was the following they said okay there's
lots and lots of knowledge work in our
economy it's expensive
experts are hard to come by they retire
they disappear you know so there's a
huge economic niche in building
knowledge based expert systems and the
way you build a knowledge-based expert
system is you interview the expert you
say you essentially asked him to
describe his reasoning steps and you
write them down as
and then you build a rule-based expert
system which sort of mimics the experts
reasoning steps and unfortunately I
don't think it really works that way so
everyone appreciated that in many of
these problems not all but many of these
problems there is uncertainty medical
diagnosis is the canonical example and
everyone thought of medical diagnosis
partly because it's the way it's taught
in medical school as if you have these
symptoms then you have this condition or
this you know and if you have this
condition and this other thing then you
will then you'll go on to develop this
other condition and so they said so the
reasoning process was assumed to be from
symptoms to conclusions to diagnosis and
so they wrote rules in that direction
and of course from any given set of
symptoms you can't conclude definitively
that a person has a particular disease
like Alzheimer's for examples so there
has to be uncertainty involved there has
to be some method of combining evidence
to strengthen conclusions
you know disconfirming evidence and so
on so forth and so they essentially had
to make up a kind of calculus for
handling all this uncertainty which
which was not power ability theory
because paramilitary doesn't admit of
rule based reasoning steps I in fact you
there it's one of the main things Paul
did in his book was to explain why
chaining of rules cannot capture what
power ability theory says you should do
with evidence
and so what tends to happen with those
systems is that you know with with a
small number of rules you can tweak the
weights on all the rules so that on your
the set of cases you want to be able to
handle it behaves correctly but as you
get to a larger range of cases and more
rules and deeper levels of chaining you
get problems of over counting around the
counting of evidence you get problems
where you end up concluding with much
higher certainty than you really want
that such and such is true because the
rules essentially operate a kind of
pumping cycle where they the game or
more certainty because their own you
know cycles in the the reasoning process
and and so I think what happened in
practice is that companies found it
increasingly difficult as they built
larger expert systems they were more
more difficult to get right and to
maintain there were other reasons as
well like you know you had to buy a
symbolic slist machine to run these
packages you couldn't integrate it with
your other data processing hardware and
software yet hire special AI programmers
who knew how to program in Lisp
this kind of thing so there were many
reasons but I think the main one was the
technology was flawed technically
another interesting issue is you know is
human knowledge really in the form that
people thought it was which was these
these rules that you chained forward
from the evidence with you know adding
uncertainty as you went along and it
turns out I think to be quite difficult
to interview people and get those rules
out whereas if you ask the opposite
question not if you see symptoms a B and
C you know what disease do you conclude
with what certainty if instead you ask
if a person has this disease what it's
so what symptoms do you expect to see
right that's in the causal direction and
this is how an expert actually
understands health and disease right
they think about well this
this microorganism lodges in your gut
and causes this to happen this to happen
and and that's why we see you know
bleeding through the eyeballs or
whatever it might be and so when Horvitz
and Hackerman were interviewing experts
they found that they could extract these
kinds of causal conditional
probabilities in the direction from
disease to symptoms very very quickly
that it was very natural for the expert
to estimate these and and also though
those probabilities turn out to be very
robust so if you think about it this way
right if if a person has meningitis then
there's a causal process that leads them
to have certain symptoms and that causal
process is independent of who else has
meningitis right it's independent of you
know the size of the population of
patients etc but if you look at the
other way around right so meningitis
gives you a stiff neck right well if
someone has a stiff neck what's the
probability that they have meningitis
that depends I mean is there a
meningitis epidemic going on right why
is this patient in my office in the
first place right were they feeling is
really poorly or they'd have a stiff
neck because they got into a car crash
the probabilities that you can assess in
this in the causal direction turn out to
be much more robust they are valid in a
much wider range of circumstances then
the power abilities in the diagnostic
direction because you know though
whether that probability is valid or
whether whether you have meningitis
given that you have a stiff neck is
highly dependent on other circumstances
outside of the individual person and so
so all of those issues I think conspired
to make the expert system industry fail
and it failed pretty quickly so what
happens in these things is that
technology comes out and it's happening
now with deep learning right everyone
says if I don't get hold of this
technology and build up you know
a group within my company that knows how
to do it and knows how to use it then
I'm going to be left behind so they
start investing in the technology
without any evidence that it actually
works to solve their problem just as I
sort of on the assumption that if they
don't they'll be left behind that it's
that's there's a potential gain here we
can't afford to to lose it and so
they're all sitting there it may be
after six months or a year you know
they're still waiting for any return on
their investment whatsoever and then
they start to hear stories that oh this
other company you know they tried six
times and if just fails dry didn't work
for that problem doesn't work for this
doesn't look for that you know so then
they start to lose faith very quickly so
all those companies that haven't yet had
a success where the technology can
switch overnight from thinking it's
essential to write to maintain our
competitive edge - we better get out of
this otherwise we're going to look
foolish and so that's sort of what
happened with the expert system
technology in the late 80s which is a
shame because by then right we already
had the technological solutions that
would have alleviated a lot of those
difficulties yeah it was tough I mean I
remember going to a dinner in 93 with a
bunch of Wall Street people and you know
I was sort of the open person now I was
explaining they worked on artificial
intelligence them and it was like I was
working on cold fusion they William
like didn't I failed right it's just
like it doesn't exist anymore right as
if somehow AI and rule-based expert
systems in the commercial marketplace
are the same thing and so in in the mind
of Wall Street the investor community it
doesn't exist right you know their view
was forget it we're of course the field
academic field was still pushing ahead
as I mentioned I will I wrote a textbook
that came out in 94 late 94 that tried
to incorporate as much as we knew about
how to use so the right
agent framework and then a lot of
probability decision theory this is the
with Norfolk so so Peter Norvig is my
co-author and so to come back to the
story about how how he got involved so I
had some course notes and Peter was back
in Berkeley from time to time I think he
was working at Sun on you've been at USC
he had come back to Berkeley he then
going on to Sun on in Boston Aero
research lab there but we had discussed
when he was at Berkeley with Robert
walenski who had been Peters advisor
that we might try to write a Berkeley AI
book and Robert Wolinsky had his view on
AI he was a student of Roger shank and
after a while although I really enjoyed
Roberts company he's unfortunately he
passed on a few years ago we just
couldn't CAI I too high on the the
content I mean we just thought about the
field in such a fundamentally different
way that there was no way to get that to
working he had a strong personality it
just was a little that was too much
conflict where as Peter has a very is an
incredibly easy going person and I think
that's one of the reasons why he's so
successful he he does not try to project
his ego or his ability at all and he's a
completely reasonable you could you can
have a discussion with him and he
doesn't feel threatened he doesn't try
to threaten you and it's very so it's
very productive he's also a really good
writer and a great programmer so we
spend a lot of time writing code
together to go with a book so that was
another important thing we go with a
book was to try to build a suite of code
that was as fully integrated as possible
to
reflect the the principle of the book
that rational agent framework was was
General III think we succeeded to some
extent I'm he's not perfect but there
was a step forward so I think the book
helped to bring you know what pearl had
done for example and some other other
ideas reinforcement learning also came
out in the late 80s but was not widely
known or taught so the whole notion of a
Markov decision process was was unknown
to most AI researchers so we you know we
tried to build these bridges showing
that a I was really continuous with
statistics with operations research you
know which studies Markov decision
processes you know so decision making
under uncertainty is what they do you
know an economics studying utility
theory how do you construct these these
functions that describe value so we
tried to bring all of that in and create
these connections to other fields and I
think that was really helpful for the
field I think it helped a field to grow
up a bit to realize that you know
there's more to doing research than
reading one paper in last year's
proceedings and then doing some
variation on that that there's actually
a huge literature's in all these other
fields that are relevant to some of the
problems we care about so that was you
know that was 94 and the book was
unreasonably successful so it's an
interesting story I mean the the the
issue of risk from from intelligent
systems and goes back to the very you
know the prehistory of AI the word robot
came from a play
a Czech play in 1920 and in that play
you know machine servants rise up and
take over the world so there has never
been a time when existential risk hasn't
been a thread in the field and Turing
talked about it with I don't know
whether you could say resignation or
just blase just says yeah well you know
when when all of AI stuff starts to work
well we'll be lucky if they keep us as
pets and the intelligence explosion idea
which came you know that that phrase
came from IJ good in paper he wrote in
65 pointing out in particular that
sufficiently intelligent systems could
then do their own AI research and
hardware design and produce their next
generation very quickly in that process
would would then accelerate and the
human race would be left far behind and
so and you know people look at that and
they say yeah that sounds right and then
they just go back to work as if the
actual semantic content was irrelevant
you know Minsky pointed out that if you
ask a sufficiently intelligent machine
to calculate as many digits of pi as
possible which sounds very innocuous
then in order to do that it will take
over the entire planet or even the
reachable physical universe and convert
it into the machine for calculating more
digits of pi and this was the point so
no but we know wrote a paper 1960 so he
had seen Arthur Samuels checker playing
program which learned to play checkers
by itself by playing itself and became
more competent to checkers then then
samuel was so that was the very early
proof in you know 5758 that the one of
the objections to AI which is the
machine can never do any more than we
to do that that was just completely
misconceived that machines can surpass
their programmers if they if they're
able to learn and so we know I think was
at a stage in his life where he was
thinking a lot about the impact of
technology on humanity and whether this
in a lot this was going to be a
successful long term future if we
followed along the current path and he
said you know and use the Sorcerer's
Apprentice as an example if you put a
purpose into a machine you'd better be
absolutely sure that the purpose is the
one that you really desire and he says
you know this is this is going to be a
major problem for I think he was calling
it automation at that time but we would
call it intelligent systems or AI and we
don't know how to solve it and but you
know it's an incredibly difficult thing
to even imagine how things are gonna
play out over a long future but if we
don't get it right now we may have a
long future that's not good so we just
have to try our best to figure it out so
I find that paper quite inspiring I
would say generally speaking the
companies that doing AI prefer not to
talk about risk because they don't want
their brand you know associating with
terminator robots which is usually how
the media portrays risk it's a picture
of a terminator and that's of course
completely misleading right the media
and Hollywood the risk is always well
somehow machines become spontaneously
conscious and as soon as they do that
they're spontaneously evil and they hate
us and then all hell breaks loose and
that's not the risk the risk is just
machines that are extremely competent
that a given objectives where the
solution to those objectives turns out
to be something that
not happened with you know which is the
King Midas story or the Sorcerer's
Apprentice story all over again and
that's exactly what we know was warning
against you know so in in Britain in
particular there's this notion of the
Health and Safety Officer who was
appointed by the Council and son of a
busybody and goes into everyone's office
and says all you need to have you know
you need to have those windows locked or
you need to have wider doorways or
whatever it is but you just imagine the
health and safety often of going back to
a million years BC who these poor people
have just invented fire so they can keep
warm and and stop eating raw food and
health and safety officers I can't have
any of that you know it's not safe
you'll catch your hair on fire you know
your cause global warming you know we're
gonna put a stop to this right now and
it's so a million years ago would be yup
too early to be trying to put
constraints on technology but with
respect to global warming I would say a
hundred years ago would have been the
right time or one hundred twenty years
ago right so we we had just developed
internal combustion engine and
electricity generation and distribution
and we could at that time before we
became completely tied in to fossil
fuels
have put a lot of energy and effort into
also developing wind power and solar
power knowing that we could not rely on
fossil fuels because of the consequences
and we knew like aurelius other
scientists had shown that this would be
the consequence of burning all those
fossil fuels
you know Alexander Graham Bell wrote
papers about it but they were ignored
there was no vote there was no you know
I think governments tend to get captured
by corporate lobbies and not so much
scientists so you might say the
scientists invented the internal
combustion engine but they also invented
the wood you know
discovered the possibility of global
warming and warned about it but yeah
society tends to take the goodies but
not listen to the downside well the
people I think you know when when
genetic engineering got going in the 70s
most people I mean they didn't know what
that even meant people didn't even know
what DNA was I think had it been fully
explained they probably would have gone
along with what the scientist decided
which is that number one we need to
enforce fairly stringent safety
constraints on these kinds of
experiments so we don't accidentally
produce disease organisms that infect
people and number two we're not going to
allow experiments that modify human
genome and that's what they did and I
think they were pretty praiseworthy I
think what they did was really
impressive given that for a long time
you know one of the main purposes of
genetics was precisely the improvement
of the human stock as they used to call
it I mean good old-fashioned eugenics
which which was mostly born in
California I think and then exported to
Germany in the 30s were you know that
was one of the main purposes for doing
all most research so for them to say
okay now we could actually do it but
we're not going to because the social
consequences are undesirable I thought
that was a pretty brave it would have
been interesting to have a public a real
public debate I believe they did not
allow journalists at that meeting the
Asilomar workshop that they helped Dave
the new meeting which was precipitated
by by the new capabilities of crispo was
you know came to the same conclusion but
it's more leaky situation now right
there are too many there are too many
science
there are countries where there's much
less of a moral concern about modifying
humanity but the so I think it's always
very difficult for a democracy to decide
on what the right regulations are for a
complicated technological issues you
know how should we regulate nuclear
power how should we regulate medicines
you know and often the regulation
follows some catastrophe and can be
poorly designed because it's you know
it's in the middle of outrage in fear
and so on so so I would much rather that
when we think about AI that we think
ahead as far as we can and realize that
the you know the the right thing to do
is not to try to hide the risks you know
and I see for example the AI 100 report
which just came out a couple of weeks
ago which you know so Eric Horvitz set
up this AI 100 100 year study on AI at
Stanford and they're supposed to produce
a report every few years and they you
know so they assemble a panel of
distinguished scientists and they
produces a report which you know it's
intended to be a prediction about what
kinds of impacts AI will have by 2030 on
a typical you know North American person
living in a city you know what sorts of
technologies will be available for
impact and but they talk about risks and
basically they they deny that achieving
human-level AI is even possible which
tonie's seems utterly bizarre that if
that's the official position of the AI
community then I think they should all
just resign because the report says that
there might be risks but we shouldn't
talk about them because if we talk about
them
that might prevent people from doing
research on risks in order to prevent
them which is just doesn't make sense</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>