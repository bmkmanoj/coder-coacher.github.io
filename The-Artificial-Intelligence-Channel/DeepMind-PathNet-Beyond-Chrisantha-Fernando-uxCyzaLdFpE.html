<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind - PathNet &amp; Beyond - Chrisantha Fernando | Coder Coacher - Coaching Coders</title><meta content="DeepMind - PathNet &amp; Beyond - Chrisantha Fernando - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind - PathNet &amp; Beyond - Chrisantha Fernando</b></h2><h5 class="post__date">2018-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uxCyzaLdFpE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">unlesss next speaker is Chris Santa
Fernando he's currently a deep mind he's
famous for his work on path net and
other works on using your neuro
evolution for to optimize neural
networks he was trained as a doctor
medical doctor as at Oxford but then he
did PhD in computer science the origin
of life and now he's gonna talk to us
about path net so when I first came to
deepmind I was very interested in
combining evolution and learning but for
the first year I ended up just doing
evolution because nobody believed in it
so what I managed to do with Dillon was
basically to evolve lots of fairly small
things evolution at the time was and
it's very good for evolving small
controllers of things so his some
simulated robots doing a locomotion with
evolved continuous time recurrent neural
networks something we had used a lot in
Sussex mother we saw this cheetah in
Petri Bill's talk so this is a very
small neural network for ten neurons
that can produce that behavior and we
use methods developed by Jeff Clinton
who's probably here to do diversity
maintenance so this is using map elite
so to evolve a whole bunch of different
behaviors categorized by the absolute
amount of force used and the average
height of the torso during running so
that's map elites and we evolved small
neural networks with structured with
visual filters which actually just two
logistic units to do very good driving
on talk so you can do talks basically
with two neurons
and evolutions breaker for that sort of
thing moving around in a
three-dimensional world you can reuse
cpp ends to encode receptive fields with
small numbers of parameters and a
handful of neurons so that was all fine
but when we get to harder problems such
as Atari games and first-person games
then really you need to have many more
parameters and so for example a 3c has
around 700,000 parameters and recently
open a I has done a wonderful paper
where these evolution strategies to also
optimize those kinds of network so there
are ways in which it can be done and
there are other ways of compressing the
number of parameters so hyper Nets is a
nice one which extends on the DPP n
paper that we did which extends on Ken's
paper we a small network encodes the
weights of a large network so this is
work by David Hart and others so you put
in the coordinates of the two neurons in
the big network into the small network
and the small network outputs the weight
between those two neurons so it produces
an interesting structures but what I'm
going to talk about today is another way
to combine evolution and gradient
descent and this develops on an idea
that she's like Mary and I talked about
in Grenada in I think another nips
conference and exactly what it was the
idea that evolution actually some kind
of natural selection some evolutionary
algorithm actually takes place in the
brain in the brain so path mint is one
implementation of this so currently we
train our own
currently we train our own separate
neural networks each person trains their
own neural networks but wouldn't it be
nice if we could all trade the same
neural network
so this is sort of one step to the idea
that that such a thing might be possible
so we've taken the a2c architecture
so it has three convolutional layers and
a linear layer and two readouts one for
the representing the policy and one for
the value function and here we've got
two sets of those readouts so for the
source game that is the first game we're
gonna play in the second game call the
target game so I'm interested in a
transfer learning setup so I want to
learn the second game faster than I
could from scratch and faster than I
could if I just fine-tune the the same
Network on the first game and then
retrain that same network on the second
game just straight away so these are two
important controls for transfer learning
you want to beat these two controls the
independent learning control of the fine
tuning control so what we do is to break
up the each of those layers into little
modules and we actually in a lot of the
experiments made those modules a bit
different so some of the modules had
skipped we'll just skip connections some
of the modules were residuals some of
the Millennials with billionaires of
different sizes and so you choose your
set of modules and then we're going to
the genotype specifies a subset of those
modules in each layer so here we choose
four out of ten modules in each layer
the red squares there are the modules
that are chosen in each layer and we're
only going to train those that subset of
modules so we do the forward pass we we
train we do the forward pass through
these four modules and then we combine
them with the reduce Sun we just add
them up and then we pass that there's
activations to the the next four modules
that are chosen in the second layer and
so forth until the end and at the end we
always train the same readout
on the source game and this readout on
the target game so that's the forward
pass and then we do the backwards pass
we only update we only apply the
gradient updates we only change the
weights of this subset of modules that
are chosen so how do we choose this how
do we choose these this subset of
modules what we do is we have a
population of pathways through the
network that are initially randomly
generated and we play a set of episodes
of the game with one pathway and we
evaluate the fitness of that pathway
which is the the performance of that
that sort of parameters on that game so
these pathways are just views onto the
same set of parameters there's just one
set of parameters and a pathway is just
a view onto those and each worker so we
have distributed set up where it's
basically a three C so we have like 64
workers each playing their own copy of
the game and updating this one shared
set of parameters but each worker has
its own genotype which tells it which
subset of the path the the big network
is going to use to control its agent so
just a recap we initialize a random
population and then we do so once a
worker has finished say playing a
hundred games it looks for another
random worker and compares that workers
performance to its own and then if it's
better it copies the pathway from that
other worker into itself with four
mutation so here's a video of that
happening and what we find is the
pathways rapidly converge into one
pathway and then we fix the weights of
that pathway we fix the weights of that
pathway and then we reinitialize another
population of pathways and the workers
then play that second game so you can
still back
propagate through all this fixed weights
that we learnt on the first game but we
don't train those paths as any of those
weights anymore
so we find that for example if we
started as the source game is River aid
that blew the path net shows greater
better performance on the range of games
that we tried found both the fine-tuning
and the independent controls so here's a
video of the transfer task so the the
pathways rapidly commercial to one path
and then we flip to the next game and
very rapidly the pathways converge to
with reuse of these red modules so for
an order here's another run convergence
random initialization of new pathways
when I say so all the parameters are
just it's just one shared set of
parameters pathways are just four years
so here's a sort of summary of the the
positive and negative transfers we see
green is good blue is bad
yep
okay shall I just carry on without
slides the results are really good the
most of the matrix here is green so
there's positive transfer well while
he's doing that I'll just carry on so we
did this also in supervised learning so
we lived a task called binary M NIST
where you show one binary classification
so one versus two for example and then
you train it on eight versus several and
there's on average a positive transfer
in that case compared to the defined
unit controls video do you want to see
that so you can train it you can train
many things at the same time as well so
I've described sequential tasks but you
can also train two tasks together hence
the motivation of many people training
the same network so to conclude path net
is a good first step I think in one way
of reusing combinatorially different
neural modules and using evolution to
decide which parts of a large neural
network to Train effectively it does a
kind of architecture search with sort of
Lamarckian inheritance of these weights
because it's just the same set of
weights being trained but the evolution
is deciding to sort of chop and change
those modules an obvious extension is to
make the choice of paths conditional on
the image but that has lots of special
challenges and so I can't talk about to
have that at the moment yeah thank you
very much
yep hi very so you saw these pathways
quickly converge to one set of pathways
that are being used that's something
that you often see in evolutionary
algorithms and I wondered did you
consider using an explicit form of
diversity maintenance so we have
actually tried the to diversity costs
that Google brain used in their registry
large networks paper so we've tried that
one of the issues is that actually for
reinforcement learning we want rapid
convergence into one pathway so that we
could because the measure is that we're
trying to optimize is the total area and
the learning curve and comparing that
with fine-tuning prom with fine-tuning
is often very good control so if I use
diversity for reinforcement learning
like that it's not really working as
well and that's using subsets of the
network it could still work if they're
not yes yes thank you
I have a really naive question this is
the first time in my life attending an
evolutionary algorithm session but every
once in a while and Carlson's had a
couple of papers in the 90s that evolved
controllers and those controllers looked
very similar to the controllers that you
showed in the beginning of of your talk
so for for an ignoramus like me can you
summarize what changed in the last 20 20
years so I think nothing so basically I
think the methods that have been used a
very similar I think I think there have
been advances in for example natural
evolution strategies CMAs this is an
information geometry way of looking at
this stuff that's pretty good I think as
far as genetic algorithms are concerned
I think we still are only scratching the
surface of I could ask you the same
question about back propagation
I mean what's changed in the last 20
years well our result local colleges the
backprop results look qualitatively
different so right yeah yes the networks
that are being optimized now are
qualitatively on on a very different
level you're on 20 years ago so I think
what's changed is basically computer
power and so the the open AI result I
think demonstrates that various tricks
such as using virtual batch
normalization early stopping surrogate
models these definitely can be used to
scale things up and I think we still
don't know properly how to evolve neural
networks so there are probably little
tricks that we're still to discover but
yeah I think that the benefits are in
mixing evolutionary algorithms with
gradient descent yep
okay so I suppose Ken is saying that one
of the advances also betterment ways of
doing diversity maintenance so that map
elites example all those results and
just from one run but no I think Karl
Sims was pretty good I think so too did
evolutionary computing here you had to
split the network into different modules
sort of in discrete discrete way right
do you think would be possible to
specify sort of continuous mapping
captives of this modules and perform a
continuous assignment and then do
inference of you're done yeah so I've
set myself the specific limitation of
having to use the sparse Network so I
want this to generalize to networks that
are very very large so I want to limit
myself to using hard gating but that
makes the job harder
right so you we could still do this do
that
as for example using I don't know
attention mechanisms over model
parameters in in different ways I wonder
whether there are parallels but if you
use attention then you have to back
propagate through the whole thing anyway
to decide what to attend to well that's
right and then we have convex relaxation
so for example discretize in purpose and
so on anyway I suppose I was I didn't
want to I wanted to limit myself to a
situation where I didn't that propagates
through the whole thing but maybe I
didn't understand what you say
and you could explain</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>