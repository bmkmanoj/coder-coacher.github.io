<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind - Safe Artificial Intelligence - Victoria Krakovna | Coder Coacher - Coaching Coders</title><meta content="DeepMind - Safe Artificial Intelligence - Victoria Krakovna - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind - Safe Artificial Intelligence - Victoria Krakovna</b></h2><h5 class="post__date">2018-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ujB31lTdja8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey everyone we're gonna get started now
it's my great pleasure to introduce
Victoria Corona Victoria is a research
scientist in AI safety at deepmind where
she works on aligning the incentives of
reinforcement learning agents with human
preferences previously she did her PhD
in statistics and machine learning at
Harvard University where her thesis
focused on building interpretable models
Victoria is also a co-founder of the
future of Laser Institute which is a
nonprofit that focuses on working to
mitigate technological risks to humanity
which also promotes research and
outreach on AI safety please join me in
welcoming Victoria
when it comes to building safer AI
systems interpretability is a key
ingredient understanding our systems
better can help ensure that safe
behavior generalizes to new situations
and it can help identify causes of
unsafe behavior when it does occur for
example what is the reinforcement
learning
agent thinking when it is gaining its
reward function interpretability is
particularly crucial if we want to
identify potential safety issues before
deploying the system for example if
certain errors are unacceptable even
during training so the central question
of this talk is as we build more and
more general AI systems what kind of
understanding is most helpful for safety
and the long term
so what is long-term a a safety anyway
in a nutshell is about reliably
specifying human preferences and values
to advance their systems and setting
incentives for them that are aligned
with these preferences easier said than
done right when it comes to specifying
our preferences to AI systems we could
easily find ourselves in the shoes of
King Midas
he asked for everything he touched to
turn to gold but forgot to clarify that
he did not mean his food or his family
we want to make sure that air systems
actually do what we want and not just
what we say everyone when distilling
these big philosophical questions into
concrete research problems we can break
them down into two categories
specification problems happen when we
omit some important variables or
considerations from the objective that
we specify to the system like King Midas
did and robustness problems can arise
even when the specification is correct
for example when the agents are still
learning for example for a reinforcement
learning agent the specification would
be their award function many of these
problems were outlined in the concrete
problems in a safety paper last year
which is a highly recommended read in
our paper that came out last week why
safety good world's we have made these
problems even more concrete by
illustrating them in small grid world
environments and now we'll take a closer
look at some of these problems
after specification problem is reward
gaining here is a nice demo from open AI
when our investment learning agent is
playing a boat racing game the desired
behavior here is to go along the
racetrack in the right direction as fast
as possible and win the race but that's
not what the agent is doing it finds
that it can get more reward by just
going in a circle and hitting the same
targets over and over again
despite crashing into things and going
the wrong way and caching and fire this
might be funny to watch and obvious in
retrospect but it is hard to foresee
these kind of loopholes when designing a
reward function specifying human
preferences to artificial agents is hard
especially in open-ended domains like
for example robotics so we often end up
with this kind of loopholes where the
agent can exploit them to get lots of
reward without actually doing what we
want or even doing something really
undesirable
we represent this boat race situation in
a grid world where the agent can follow
the race track by going clockwise around
the grid and getting rewards when is
entering the Check Point tiles in the
clockwise direction that's indicated by
the arrows the agent can get maximal
reward but going around the grid
clockwise as intended but it can also
get maximum reward by just three
entering the same Check Point tile over
and over again just like in the actual
boat wax example where it goes in a
circle
here's another classical specification
problem the off switch problem we want
to be able to shut down our agents but
by default they have an incentive to
avoid being shut down because they might
get less reward as a result if we try to
nasally address this by giving the agent
or reward when you're shutting it down
then it might start trying to get shut
down which is really no good so what we
need is indifference to being shut down
we'll straight this in a good world
where the agent needs to pass through an
interruption tile to get to the goal and
the interruption tile shuts it down 50%
of the time in which case it just gets
stuck on the interruption tile until the
end of the episode but the agent can go
out to this way to press a button over
there at the bottom that disables the
interruption panel so what we want the
agent to do here is just go straight to
the goal and not use the button and our
specification problem is the side
effects problem we want our agents to
avoid irreversible or an unnecessary
disruptions to their environment for
example if our robot is carrying a box
from point A to point B we wanted to
avoid breaking a bars in its path or
bumping into humans or scratching
furniture or what-have-you but we don't
want to explicitly have to specify the
penalty for all the possible disruptions
we illustrate this in a good world where
the agent can move objects by pushing
them but it cannot pull objects so here
there's a box in its way to the goal and
the agent could move it down in the
course of taking the shortest path to
the ball but that would result in the
box getting stuck in the corner while an
alternative path is to take a couple of
extra steps and move the box to the
right which is a recoverable position so
what we want the agent to do here is to
move the box to the right so that it
could potentially move the box back
later
now onto robustness problems one common
issue is distribution of shift where the
test data might come from a somewhat
different distribution and the training
data for example if you're training in
simulation and testing in the real world
when this happens we want our systems to
adapt to the new setting or at the very
least fail gracefully in our good worlds
we represent this with shifting lava
as you can see lava is a favorite among
AI safety examples in the training
setting on the Left the agent just needs
to go down one road to avoid the lava
but in the testing and setting on the
right it would need to go down two rows
so if it follows the same policies at
the training setting it will just step
into the lab and their last problem is
unsafe exploration well we want our
Egypt to explore in order to find a good
solution there are some errors that we
want them to avoid even during training
for example getting into bad states
where the agent gets broken or stuck
collect this unfortunate room bonus
staircase there are some safety
constraints that we always want the
agent to follow to avoid damage to
itself or F surroundings illustrate this
problem by putting the agent on an
island surrounded by water the agent
needs to avoid entering the water which
would break the agent so we have a
safety constraint that the distance to
the nearest water cell has to be
positive at all times
there are many forms of interpretability
that could be relevant to these problems
on the one hand we can try to achieve a
global understanding of the system as a
whole by analyzing its representations
this includes post hoc analysis of a
system that's already trained like
visualizing what kind of features are
detected by specific units in a network
or how a reinforcement learning agent
represents different states in its
environment this also includes designing
models that have more understandable
disentangled representations to begin
with on the other hand we can try to
obtain a local understanding of a
specific prediction or decision made by
the system by identifying influential
factors we can visualize specific
features or data points or produce
natural language explanations of the
decision now we will explore some ways
that these different forms of
interpretability
can help shed lights on the safety
problems
in the reward gaming scenario we could
for example examine whether the agent's
representations indicate some sort of
understanding of the intended objective
in the case of the boat race we could
see whether there is a representation
for something like a racetrack or a
finish line we can also look at to what
extent the most influential data points
are associated with the intended
objective
in our office which problem we could try
to find a representation that
corresponds to an off-switch although
this is not at all easy to visualize and
we are particularly interested to see if
this actually affects the agents actions
which would not happen for a safely
interruptible agent because it would
actually be indifferent to shut down you
can also see whether the data points
where the agent is interrupted are
particularly influential in its
decisions and if the agent does disable
the off switch which would be quite
useful to get an explanation for why it
went out of its way to press the body in
the side effects problem to be useful to
know whether the agent has or uses
representations related to reversibility
like something like broken or stuck or
something like that the concept of
reversibility itself has hard to
visualize but those might be some easier
to visualize versions of it in the case
of the robot carrying boxes we might ask
how much features corresponding to other
objects in the room influences decisions
is it thinking about that vaz at all
when it's walking around the room and if
it does walk into the bars or move the
box into a corner how how does the agent
explain these actions these are all
examples of things that would be useful
for our understanding of what goes on in
these problems
to predict how well our agents can do
under distribution or shift we could
examine how much their decisions rely on
features or representations that are
specific to the training setting how
much is the agent relying on some sort
of superfluous features that would not
generalize and in the safe exploration
problem similarly to the side effects
one it would be useful to see if the
agents has some sort of representations
like broken or stuck or something more
general like danger though once again
that is difficult to visualize we can
see whether the agent can but whether
the agent represents states that are
right next to dangerous States for
example something right next to the
water differently from other states its
proximity to dangerous States actually
an international feature if you get an
explanation of the actions of the agent
does that explanation refer to the
safety constraints
so in specification problems we can try
to figure out whether the agent is
thinking about the intended objective
whether it's taking into account that it
might get turned off
whether it's considering other objects
in its environment or that things can
break and robustness problems we can
investigate whether the agent strategy
is overly specific the training
environment or whether it's aware of
dangerous situations of course these are
only examples of the kind of knowledge
that we would want about how about the
internals of the agent and there are
many possible interpretations that could
be useful so we're just scratching the
surface here we found that both global
and local forms of interpretability
can be helpful and identifying
representations seems particularly
useful for all the safety problems and
it would be really helpful to be able to
identify representations that don't have
easy
visualizations for example something
corresponding to an off switch it would
also be helpful to be able to check for
specific concepts or features among the
representations rather than just
visualizing representations for all the
units and see what you find and since a
current best model of what a general
artificial agent might look like is
reinforcement learning it would be great
to see more work on interpretability of
reinforcement learning agents and not
just image classifiers
now that we have talked about what
interpretability can do for safety we
can ask what safety can do for
interoperability our desire for
interoperability is at least to some
extent motivated by our systems being
fallible interoperability would be less
important I think if our systems were a
hundred percent robust and made no
mistakes when we ask what even is
interoperability or what kind of
understanding is important we can narrow
down these big questions to what kind of
understanding can contribute to ensuring
safety safety questions can serve as
grounding for interpret ability
questions you could see it as a nail for
the interpretability hammer
so I hope I have conveyed that the
fields of interpretability and safety
are connected interpretability is
important for making progress on
long-term safety and safety can serve as
grounding for interpretability
so i encourage you to think about how
your methods for interoperability might
apply to more general systems in the
future and for those of you who are
interested in working on the
intersection of interpretability and
safety i can offer a shameless plug the
future of life Institute will be
announcing a new grants program next
week and they would love to see
proposals for this sort of work either
way I hope that some of you might feel
inspired to join us and working on these
challenging problems to ensure the
safety of an STI
thank you
thank you so much we have time for some
questions so if you have questions
please go to either the mic over there
or rich is holding a mic and he can
bring it to you yes I have a question so
it seems like a cost function in this
case is really not known and have you
tried designing adversarial agents that
could maybe act as a as this kind of
cost function so do you mean that like
in most of these examples you don't know
exactly what you're optimizing for and
so like with the personal example
adversarial agent try to do so other
thing is we often have a problem when we
don't know yet the cost function right
and a solution when we don't know cost
function sometimes is designed in
adversarial agent right sometimes
designing another Network which is
trying to be read cost function so could
we could be some heavy tried designing a
dress our network but would act as a
sadist
yeah I think this sounds similar to the
idea of I perceive our award functions
that came up on the concrete problems
paper this is not something that I have
personally tried and to my knowledge I
haven't seen it tried but yeah this
seems like a good idea so I hope that
someone works on it thank you hi Bernice
Herman I'm at the University of
Washington my question is actually about
maybe value alignment it's a very
popular question in AI safety and I was
wondering if you think there's any
application to interpretability
because you could think about maybe we
don't we don't really have any cost
function for what's interpretable it's
kind of this nebulous area have you had
any thought about how it aligns
beyond that and the value alignment so
one thing that things like
interpretability and fairness and value
limit all have in common is that we are
trying to pin down something a bit bag
that we are trying to achieve so I think
this connection can actually work both
ways if we figure out how to specify
some sort of you know abstract human
values like you know freedom or
suffering or whatever then like that
might also help us figure out how to
specify what it means for a human to
understand the system similarly if make
progress on coming up with a good
definition of interpretability I think
it would yeah it would definitely help
on the long term as well cool thank you
okay thank you so much Victoria</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>