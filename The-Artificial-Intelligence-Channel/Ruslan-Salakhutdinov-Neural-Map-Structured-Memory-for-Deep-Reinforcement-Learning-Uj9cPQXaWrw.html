<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ruslan Salakhutdinov - Neural Map: Structured Memory for Deep Reinforcement Learning | Coder Coacher - Coaching Coders</title><meta content="Ruslan Salakhutdinov - Neural Map: Structured Memory for Deep Reinforcement Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ruslan Salakhutdinov - Neural Map: Structured Memory for Deep Reinforcement Learning</b></h2><h5 class="post__date">2018-02-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Uj9cPQXaWrw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's a great pleasure to introduce
Roslyn Roslyn I'm selected enough who is
a professor at Carnegie Mellon
University and right now one of the
leading figures in deep learning he's a
product of the deep learning of Toronto
University of Toronto which has produced
some amazing figures and where Rosslyn
was a professor there and then went on
to do his postdoc but we met at MIT and
we're and now he's a professor at CMU
and a Sloan fellow and Microsoft fellow
there so really looking forward to his
talk thank you
let's see if I'm connecting and I can
get this am i connecting like a button
then you switch okay so let's see if
that works okay great well thank you
very much for for inviting me to give it
talks I'd like to talk today about
neural map which is a structured memory
for deep reinforcement learning and sort
of go over the areas of memory so I'll
give you a little bit of a background
and then a show specific model which we
view is a very simple model but has a
few interesting properties and towards
the end of the talk I'll show you some
of examples of using memory in languages
particularly cooperating prior knowledge
with with memory so as you know that
we've seen a lot of successes in
supervised learning right in fact most
of the deep learning algorithms are you
know successes are coming from basically
mapping inputs to the outputs and if we
look at the supervised learning
traditional supervised learning the
environments is typically static and
typically the outputs I seem to be
independent of one another right so
labeling one image doesn't really impact
the other one in when we look at the
reinforcement learning environments the
environments are typically dynamic they
change over time actions can infect the
environments with arbitrary time lags
right so for example if you do in action
today it can really impact your if you
take a particular action today you know
it can really impact your future reward
labels can be expected it can be very
expect expensive or very difficult to
get so think
you know trying to get the right
actuations of a swimming octopus right
how do you get labels for that that's
very challenging and reinforcement
learning essentially allows us to work
in that environment that's in itself
enforcement learning is kind of a very
old field so instead of a label an agent
is given it's provided with a reward
signal so high reward basically means
good behavior and if you think about
reinforcement learning reinforcement
learning essentially produces policies
right you can think of the poetess is
mapping from observations to actions and
with the goal of maximizing some
long-term reward now what happens in the
space of deep reinforcement learning
sort of one one one slide here is that
we're essentially using deep neural
networks to parameterize the policy
right so our input could be a high
dimensional input and then we're using
deep neural nets and the output of a
deep neural network is in action and
weight that parameters to maximize
overall rewards using Q learning act the
critic there's been a lot of work that
I'm listing here that sort of looks at
all of these different algorithms
evolution strategies which is you know
akin to doing find a differences method
but let me show you one particular
example of a reinforcement learning
agent learning to play a 3d game and
this is without memory in this is work
of chaplet and lamb play a year ago so
what you're seeing here is the input to
the environment is just a 2d image right
which is a frame and the output is the
action move left move right go forward
and the rewards that you're getting here
is just collecting objects and what you
can see is that you're not will be
telling the agent how to navigate in the
environment you're just telling me or
the agent collect as many objects as
possible and so through the process of
learning basically have a couple of
hours of training across multiple GPUs
the the agent basically learns to move
around in this 3d environment which is
you know pretty pretty impressive it's
it's a simple environment as you can see
but it has no problem
you know navigating in
in that environment you can train with
random textures this is an example of
just having a very diverse set of
environments and what's interesting is
that once you do that if you actually
show the agent a new environment a new
map or you can think of it as
environment that the model has never
seen before at the training time then it
has no problems navigating in this new
inviting in an unknown map right and so
the textures are somewhat different this
the the the structure of the map is
different and the model has no problems
moving around so that's that's
interesting but the problem with a lot
of these techniques so far is that these
models are reactive right given an input
you take an action given another input
you take another action the question is
can we learn agents with a more advanced
external memory right something that so
that we can remember what happened in
the past so that we can act optimally in
the future and then it's been wonderful
work coming out of Alex graves and his
colleagues at deep mind on neural Turing
machines and differentiable differential
neural computers it's actually is
designed they are designing these these
external memory modules one of the big
challenges is it's very difficult to
learn these systems particular is very
difficult to learn memory systems
especially using reinforcement learning
objectives right so that's that's a big
challenge
now why memory is challenging suppose I
give you a very simple example suppose
agent starts the particular location in
the map and then you give an agent an
indicator somewhere near the initial
state and then the color of the
indicator the state of the indicator
determines what is the correct goal or
where the agent should go right so in
this case if the indicator is green you
have to find the red target if the
indicator is blue you have to find the
scienter target right if you look at
this particular formula I mean it's it's
very simple for us to understand what
the problem is it's extremely difficult
for machines to figure out
to solve this problem right obviously if
I tell you a priori if and if this do
that or if you see something else you do
that then the problem is easy the real
question is at the start in these
examples there is no a priori knowledge
about the color of the indicator right
and the time that can take between when
you see the indicator and and finding
the right target could be you know
arbitrarily long right so standard
models like a lot of people are looking
at recurrent networks long time short
term memory less TM models and stuff
among so that's what they genuinely fail
in these tasks all right
so in order to solve this starts the
following must hold you have to write
the color to the memory at the start of
the maze you can never overwrite the
memory over the next two time steps as
you exploring the environment and you
have to find the right target right so
how can we do that one solution that
we've seen so far is basically writing
everything into memory right and
particularly space of language
understanding there is been a lot of
work on memory networks and you think
about memory networks is just basically
storing the representations of what you
see of the last M frames and at every
single time step you perform something
is called a read operation which is
essentially doing an attention over what
you've stored and you write your latest
percept into the memory so it is
basically storing what you see now
that's easy to learn because you're
basically trying to store as much as
possible right one option would be to
store the whole history and then try to
retrieve the relevant pieces but it can
be inefficient right because you need to
store a lot of information and some of
that information might not be very
useful now in fact you're gonna be
storing a lot of useless information
with redundant information and obviously
there is a time in space requirements
increase with how much you willing to
store so can we do something else let me
show you one very simple idea so here
what we are gonna do is we're gonna
create a location-aware memory you can
think of it as a memory with
very specific inductive bias we're going
to structure the memory into a grid of K
dimensional cells to think of K
dimensional cells as representing
features of what we seen and W by W
representing the map that we exploring
and for every XY position we're going to
be writing to X prime Y prime position
inside of our our memories obviously
there has to be mapping so I'm making a
couple of assumptions here one
assumption is making here is that I know
my position in the environment which in
many cases could be unreasonable
assumption but I'm gonna show you how we
can get around that so for now let's
assume that we know where we are in the
environment and so if we're in this
environment what we are observing we
storing it in the memory if we're the
different location and so depending of
where we are we are storing at that
particular location so effectively the
map acts as as a map that basically
agent fills out as it explores the
environment and we have spots writing
operations right kids we only need to
write at one particular location and it
allows us for easy credit assignment
problem all the time and I'm gonna show
you some examples of that so the overall
architecture looks like this you given
some state you create some feature you
write it into the memory you take in
action give in action the environment
changes you read from the memory the
information necessary information you
update your memory and then you take
another action and so forth right so you
can think of the memory is this this
very structured two-dimensional
three-dimensional block that you update
at every single time step let me quickly
show you the underlying architecture
that that we've we've implement there is
obviously a whole bunch of different
things you can do but this is what we've
done there are two operations to read
operations there is something called
global summarization and then in this
context based retrieval there is a
sparse write operation we're writing
only the specific agent location and
then we're using both of these vectors
both of these representations to compute
the policy to compute what the next
action should be okay
so the global read operation you can
think of it as looking at the summary or
far of what's happening in our memories
so this is just a convolutional neural
network that essentially just produces a
vector that provides a global in summary
of what we've seen so far the next
operation is a context read operation
which is summarized by set of equations
but I'm gonna explain what they are
using attention this is what we call a
very standard attention mechanism so let
me give you an intuition of what it does
imagine that I'm looking at a very
simple 2 by D memory this is illustrated
here and I have some embedding of the
state and my global summary now what I'm
gonna do is I'm gonna take a dot product
between the query vector which is my
state and every memory cell to produce a
similarity score alpha now I can take
these alphas I pass them through softmax
and normalize so these alphas are
normalized they sum up to 1 and then I
take a dot product with the overall
memory I sum over all positions to get a
context with vector C sub T the
intuition behind me is operation is
basically you're trying to find a vector
which receive a vector C sub T from the
memory that's closest to your query
vector and that's essentially what the
attention operation does it's you know
very standard read operation couple of
caveats why am i doing all of these
things the reason why I'm doing all of
these things is that the entire
operations here are differentiable which
basically means that I can back
propagate through these operations now
there are some advantages and
disadvantages the advantages is that you
can back prop the disadvantages is that
you have to look at every single
location in your memory which could be
computational intensive so there's a lot
of work people are now trying to figure
out this something is called hard
attention which is you're only
retrieving you know a few locations or
there is hierarchical softmax type of
representations so there's some work of
people trying to basically speed it up
but that's that's one caveat in terms of
the writing operation we basically
create a k-dimensional deck
what we're seeing in the environment and
basically update the neural map using
the context what what you using our
observation right so given a particular
representation then we just update our
new or map we're also looking at gating
which allows us to not to update the
context of the map this is a standard
again practice to basically say well
depending on the environment and your
hidden state you might not update and
depends on on on the gating mechanism
and then we basically take the context
we basically take the route vectors and
the right lectors and use those features
to compute a policy right so that's that
sort of and again if I look at the
sequence of all of these operations this
entire sequence and differentiable so
we're basically going to be learning
what to write how to read and all the
parameters of the model let me show you
a couple of examples so here's an
example in two dimensional environment
and this is what the input state looks
like right so the input state is
partially observable that's the only
thing that I'm seeing and we also tested
you know the results a robust with a
small noise in the X Y position but
here's what the model does let me just
explain what what the model does so here
you know you are testing the system on a
completely random maze so this is
something that the mall has never seen
at the training time the indicator that
you're seeing here the pink indicator
basically means that the agent should go
to the green block that was the task and
this is what the agent does it moves
around in this environment
it actually stumbles across the wrong
target it retrieves from the memory the
information that it has backtracks and
through the sequence again of expiration
eventually finds the right target right
so this is an example of you know where
you a model that was able to store the
state of the indicator to learn to store
the state of the indicator and then
figure out that these are sort of rules
in the environment that you have to
follow again a priori we're not telling
the system that there is something is
called indicator so the
system through the learning decides what
it needs to store and how it needs to
read so that it doesn't go to the wrong
target
this is a setting in the 3d environment
this is again just an example of you
know you you start with a green
indicator so you have to find the green
target so it moves around the red target
does it know that if you hits the red
target you get negative reward it moves
around that explores the space up until
it finds the correct target and just
basically goes to the correct target so
another interesting environment that
they that we tested the southeast called
minnow Tommy's so this is an example
where I started a particular target here
I explore the space up until I find a
red target once I find the red target I
want to be able to return back to my
original to the green target as fast as
possible right so again this is tested
in a completely random environment so
the agent kind of moves around it's it's
exploring the space this is a random
maze was never trained on this maze so
and obviously you can see it's a little
bit of a challenging environment cuz the
walls look you know very similar up
until it eventually stumbles across the
red target once it finds the red target
it reaches the red target its goal now
is to reach the green target as fast as
possible but so it kind of has to figure
out how to move around in order to reach
back the original target so eventually
it goes and and and gets there right so
this is sort of another example where
the memory is is is important because
again you have to remember something
about the environment so you can come
back to to the original state now one of
the problems as I mentioned before with
neural map is that it requires mapping
right it requires us to know what the
position where we are so we need to have
already solve organization
right so one way around it is to get a
map which is egocentric so then the
agent always writes to the center of the
map and when the agent moves the entire
map basically moves in the opposite
amount right so this is something that
we've tried as well it actually works
much better than the original model if
we just look at from the egocentric
point of view and obviously we can try
to solve localization problem as well
and there's been some work that Devendra
was talking about looking at active
neuron localization that you can combine
both of these approaches some of the
results it's the numbers themselves are
not important what's interesting what
I'd like to point out is that you know
even in smaller mazes when we go to 8x8
mazes you can see that it's become still
very challenging tasks you know you know
being being able to solve both of these
tasks is still challenging right
especially if you're going to completely
random environments or random maps that
you've never seen before in a Minotaur
task when you have to you know find the
shortest path from your target back to
your original sort of where you've
started you can see that the numbers are
pretty low we're hitting like you know
forty percent accuracy so so again it
highlights that there's still a lot of
room for for improvement now in the last
five minutes what I'd like to do is I'd
like to show you that memory the notion
of memory is not just seen in the
context of reinforcement learning but
also in the context of language
understanding so we've been looking at
the settings where if I give you some
textual representation is there any way
you can combine some of your prior
knowledge about no dependency parsers
entity relations word relations and
combine both of these representations to
get a much better representation of text
traditional systems today are using
recurrent neural networks and they're
very much data-driven right without
actually being incorporating prior
knowledge let me show you an example of
why this is useful imagine I give you
the sequence of sentences Mary got the
football she went to the kitchen she
left the bowl there is coming from
so let's go baby baby I data set I can
build an RNN but then I can also use Co
references or I can also use synonyms
right and so that information now is
very valuable for us to create a much
more advanced systems where we can use
the states of what we've seen before as
memory to update the states as we go
through recurrent you know networks so
there is a special I don't have time to
go through the the mechanics of it but
there is a special ways again augmenting
that representation with the hidden
states of recurrent neural networks and
and incorporate that let me show you an
example of where it can help us imagine
that I give you a question and the
question is how many objects is Sandra
carrying and on the left side is
basically seeing a whole bunch of
sentences Sandra went to the whole way
Sandra grabbed the Apple there Daniel
move to the kitchen Sandra got the milk
there and after every single sentence
you're using the model to predict how
many objects is saundra carrying and you
can see that in traditional models in
fact here we've used something is called
gated attention reader which is the
state of the art model for question
answering for reading comprehension
tasks the model is basically saying one
all the time
and the reason why is because if I ask
you the question how many objects
somebody is carrying if I always say one
I'm gonna you know get like 99% accuracy
because most of the time people are
carrying one object and then basically
model figures it out and it always says
one no matter what the what what the
question is and here's the system that
actually incorporates the memory as it
goes through the sequence of sentences
so it starts basically with none and
then sound to get the Apple there it
understands that Sandra is now carrying
one object then there is a little bit of
confusion when Daniel moved to the
kitchen and then saundra you know I get
the milk there then you know that it's
two so again these kinds of systems are
helping us quite substantially on on a
number of data sets to to really improve
the the questionnaire same task
ultimately you know can we build agents
intelligent agent
that can half the external memory
because I think it's really really
important to have that mechanism in
place and it's not something that you're
just storing in the weights of a neural
network right traditionally if you look
at convolutional networks or recurrent
networks all information is stored in
the weights but there's when you have an
external memory you're actually storing
information you're learning what to
store and you're learning how to read as
well as reasoning and communications I
just wanted to highlight one particular
example of learning to execute
instructions so here's an example of a
model where we're basically saying go to
the short pillar and the agent is trying
to figure out what does it mean you get
the reward if you hit the right target
and get negative reward if you don't hit
the right target and this is you know
the some extent it's a little babies
step towards language grounding
obviously this is a very constrained
environment where we don't have a lot of
instructions but at the same time the
model eventually is learning what does
it mean to go to the tall red object
right what does it mean to go to the
smallest blue object without just
basically going through the
reinforcement learning and eventually
after lots and lots of training it's
learning you know things like what is
meaning at all what's blue and obviously
we're testing in the settings where on
the combination of words that don't
really we haven't seen at the training
time so these are really well defined
test protocols and the underlying model
if you think about the model is that you
know there is a pathway which takes the
language there is a pathway that takes
image there is something that's called
gated multimodal fusion which is you
know a special way of combining both of
these representations and then you
compute the policy what is your next
action you need to take right misses
work that was done by chaplet advantage
app load and I also want to highlight
there was another paper also coming out
of deep mind at about the same time it's
also trying to address a similar problem
understanding language grounding I think
it's really really exciting and very
important work so on that note just a
little bit of discussion what we try to
do is is we try to
extend this work into multi-agent
domains right can we actually get
multiple agents communicating through
shared memory as we explore the
environment and and you know can we
write to the same memory so that both
multiple agents can can decide to act
optimally can we train an agent that can
learn how to simultaneously localize and
map its environment and and again you've
seen a little bit of that in the
previous talk by Devendra on active
neural localization but we also try to
figure out how can we go into active
slam problem how do we solve
localization and mapping in an active
way right how do we take the right
action so that we can quickly localize
and map the environment and it solves
the problem of eating an Oracle to
supply X Y position and also we're also
looking how can we structure in your own
maps in the multi scale hierarchy
because right now it's fairly expensive
when we do the read operation we have to
touch every single location in the map
but perhaps there are other ways you can
do it by looking at the multi scale a
hurricane multi scale representation so
on that note thank you very much and I
guess if there is time for questions I
don't know if there is time but thank
you
oh yeah aha
hi great talk um quick question does the
memory support like hierarchical objects
let's say Sandra has a basket of apples
or a basket of toy cars and each car has
four wheels so how many wheels is she
carrying so can you repeat the question
what do you mean like a car has four
wheels and so basically originally the
query was how many objects does she have
what if the objects are kind of nested
and then it's a object that contains
other objects which contains maybe some
more so in the case of question
answering or in general that that
becomes fairly difficult I agree with
you that can you build hierarchical
structures yes absolutely
and then in the case of question
answering if you look at language one of
the biggest problem right now is is what
I've shown you that the you know the
sentences they're very simple they're
almost like templates if you go to this
space where you're actually dealing with
real text actual data then the question
is can you actually reason about objects
can you reason about what agents are
doing in the environment and that gets
to the next level and that's very
difficult to do at this stage I think
that there's been some work particularly
combining deep learning and and question
answering of reading comprehension that
goes beyond just simple pattern
recognition and it's trying to using
attention mechanisms as well as
incorporating some prior knowledge as I
mentioned looking at core references so
that you can work relations so that you
can do a little bit of reasoning to be
able to answer these these types of
questions but we still you know very far
from dealing with these kinds of
environments in
in in an open domain question answering
open domain question asking environments
thank you oh yes sorry if you have a
visualization of the two-dimensional
neural map of the trajectory of the
attention yeah so actually there is a
poster here and there is a visualization
so I should have put it in the slide so
what happens with the 2d visualization
what happens in the environment way the
agent goes to the wrong target if you
actually visualize what the attention
does the attention very precisely look
you know attempts to the indicator
because that's the only thing that
matters so it learns to attend to the
right part in the environment and it
learns to store the particular indicator
even though a priori we don't tell the
system that there is something's going
to get it so the way you can think about
the model learning is that you throw the
model in the random environment the
agent moves around stumbles across red
target gets positive reward now that's a
reinforced you say well maybe right I
guess I should go to the red targets
next time you hit the red target you get
negative reward right and so through the
process of learning you essentially
discovering these almost like causal
mechanisms but there is something an
environment that causes you to you get
positive reward or get negative your
work the the challenge is that if the
time between you see the time that you
see the indicator and you find the right
target could be very large and this is a
very well-known problem as a long term
dependency task and so let's look at
these large-scale environments it's you
know traditional STM's are intense
it's just they just don't work there's
no way for them to remember that far in
the past but we do see that the model
learns the right representation of the
memory and it does attend to the right
parts when it's make a decision of which
which target to go to or like to
backtrack don't keep the target actually
take an action and move back and explore
it's not somewhere else thanks so much
Oh
I know if we have
hello in a demo there are mazes as an
environment where your is going but have
you tried to extend the boundaries of
means it's not just the texture change
or light but actually a forest for
instance because eventually it's going
to the left them to the right going up
and down but the indicators are probably
slightly different it's the frequency of
the trees or heels and I don't know
valleys stuff like that so I know maybe
it's done
it just wasn't present thing and then do
you mean like looking at more realistic
environments is your question is that
try to work no not quite very different
environment but still have the
environment that has a notion of a maze
forest very very dense forest yeah so we
basically yes we've tried we right now
trying to look at Unreal Engine
environments and trying to navigate and
something you know a little bit more
photorealistic but to be honest we
struggling with getting the right
environment is there aren't a lot of
open source environments that we can
work with particular like do environment
there's one environment where it's very
easy to operate on because it's the API
was written there we're looking at
Unreal Engine but it's pretty
challenging because it fails it doesn't
so there's there's right now we're just
missing having a really good basically
good simulation environments obviously
there are a few ones that are coming up
so we're exploring those but yeah the
reason is a lot of people are struggling
with the same thing so you have a weak
spatial memory right so what if there is
a latent structure exists yeah yeah yeah
so there is there obviously like they're
obviously hierarchical extensions that
we're also thinking about and can you
learn that hierarchical representation
so that you can just yeah yeah but but
we haven't I don't think at this point
we have any sort of results that can
that I can show
thanks for us for a great talk maybe I
could just ask one last question which
is do you think there's something
inherently effective about a
2-dimensional memory structure even in
environments that don't have
2-dimensional spatial structure
themselves yeah usually so you're
thinking like that's a good question I
don't know I mean obviously like if you
if you have a drawn that flies around
maybe you want to have a 3-dimensional
sort of representation but ultimately
what you go to the continuous some kind
of a continuous representation of your
memory I don't know right now the reason
why we've chosen a discrete structure is
because the writing operation is very
precise we don't need to do distributed
writing what we've seen in neural Turing
machines and that helps us a lot right
by putting this inductive bias we can
actually start solving these and even
right now solving these tasks on these
simplistic environments is still
extremely hard it takes a very long time
to learn these associations in in the 3d
environment so it's not it's not easy so
I think that it's it's a good question
but ultimately I think that the notion
of writing is if you know you X Y
position or X Y Z position and you know
your pose you know if you could solve a
pose estimation problem then you know
maybe perhaps where to write but reading
becomes expensive because you know if
you won't have a differentiable
operation so we are trying to also think
about other discrete attention
mechanisms so that we don't have to you
know do the reading operation just
touching the entire memory but but we
haven't been successful so far because
as soon as you have discrete choices to
make that adds another complexity right
through some reinforce and then it
becomes even more difficult to chain so
we haven't been successful there yet
thanks the great talk thank you
mr. last question thank you for a great
talk
I am I was I was thinking about my bit
moosers work on neuroscience with the
place cells and such yeah and I was
wondering whether you are inspired by
this work and also if orientation is a
part of this your neural map yes that's
that's a good question yes we are we are
inspired by the clay cells obviously we
haven't really done a very strong
connection to that work I mean right now
we're just thinking about we just really
think about how do we set up the problem
so that we can at least try to solve it
in these very simplistic tasks which
again is very difficult for us to to
solve yeah I think I think that there is
needs to be a little bit more work with
you know connections to neuroscience
because I know it's it's a the other
thing is is we also trying to basically
figure out can we take these kinds of
algorithms and try to solve slam or
active slam how do you take actions so
that you can localize and map the
environment as quickly as possible
yeah so these are these are things that
we're looking we sort of we're looking
at right but again it's so one of again
in another area that we try to look is
how can we design optimization
algorithms so that we can learn in these
environments faster one of the
challenges right now is we're running
these environments across multiple GPUs
with training them for you know many
many hours before we can sort of get
some to some reasonable level of
performance and as you've seen from the
results we're not hitting hundred
percent accuracy we sort of hitting
forty percent which basically means that
we're not solving these tasks as well as
we could could be solving them so there
is something
you know difficult we're doing better
than traditional models like our hands
and OS DMS and just storing things the
past frames what you've seen that that
doesn't work well either thank you sure
perhaps perhaps orientation could be a
key so the relationship to the
orientation or the orientation so yes
the orientation is actually very
important for us right now for this work
we kind of ignored it which is really
bad because if I'm looking here versus
looking here I'm in the same location
but these two things are very different
but we are now looking again at active
neural localization where we're actually
looking at orientation there is a
follow-up work that will probably come
up very soon which is going to be
looking at this estimating continuous
orientation so continuous position and
continuous orientation so that comes to
more of a pose estimation a global pulse
estimation which is very challenging
problem what doing from it in Unreal
Engine or looking at more realistic
environments so thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>