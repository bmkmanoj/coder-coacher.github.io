<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Stuart Russell - Provably Beneficial Artificial Intelligence | Coder Coacher - Coaching Coders</title><meta content="Prof. Stuart Russell - Provably Beneficial Artificial Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Stuart Russell - Provably Beneficial Artificial Intelligence</b></h2><h5 class="post__date">2017-08-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zGaA2kif9vg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you hear welcome to ethics of AI day to
hope you all enjoyed it
yesterday today we are gradually
transitioning from the shorter term to
longer term issues in thinking about the
ethics of AI l although I think as
you'll see some of the issues that come
up today are still relevant at both
levels this session this morning is a
mega session artificial intelligence and
human values especially thinking about
the way to which certain kinds of values
and goals may want to affect the way
that we design AI in order to have
outcomes that meet with human values
this has been a major research program
among both philosophers and AI
researchers thinking about AI I mean the
key idea is that as AI becomes
increasingly powerful than its impact on
the world is going to be to some very
significant part determined by what its
goals are and in order to make sure that
that those goal that is that impact is
one that aligns with our human values
maybe we ought to think about building
in something like those values into an
AI system now this is a very rich
subject it raises all kinds of technical
issues on the the AI research side it
raises all kinds of formal issues it
also raises some very rich philosophical
issues for you know does this does this
whole approach embody certain
substantive philosophical assumptions
about the way in which for example value
interacts with intelligence and
rationality so to this end we've got
this this session with five talks this
morning approaching that issue from a
number of different directions the first
two speakers are going to come
broadly speaking from the from the area
of AI research the third talk is going
to bring in
physics and psychology perspective or
thinking about the issue of value and
the fourth and the fifth stalks are I
guess you could see is offering a kind
of philosophical response and analysis
to some of these issues so we'll have
five 20 minute talks each followed by a
ten minute Q&amp;amp;A maybe we'll take a very
brief moment to stretch our legs after
the third talk and in the thought is at
the end we may have hopefully we'll have
time for a substantial panel discussion
among the group of the issues but for
our for the first talk it's a pleasure
to have Stuart Russell here Stuart of
course is a major figure in artificial
intelligence that all kinds of important
work over the years and he's the
co-author of what's been the canonical
textbook in the field for some years now
he's also been very much ahead of the
curve and thinking about AI impact and
the various ethical and social issues
that arise he's now setting up a major
Center at Berkeley for thinking about
precisely these issues News played a
very big role I think in drawing the
attention of AI researchers to these
issues and his talk today is on provably
beneficial may i so please welcome to a
Russell
thank you very much David so I'll just
get right into it
I think we'll take this as a given and I
don't want to argue about this although
one can argue with it that we will have
AI systems that in the same sense that
alphago makes better decisions that we
do on the go board that AI systems will
make better decisions than we do on the
world board and this is a good thing
because everything that we have that we
that we hold valuable as part of our
civilization is the result of our
intelligence and so if we have access to
significantly larger source of
intelligence if we use it wisely then it
cannot help but be a step change in
civilization or if some people have
called it the biggest event in the
history of the human race and then the
downside
I don't want to be a lobbyist or
anything but there's there's a few
downsides killer robots being one of
them the end of employment being another
and the end of the world being the third
in some order so okay why so I'm going
to talk about this third one why why are
people worried about this well if you
just think about it right we're gonna
create things that are much smarter than
we are right and that just gives you a
sense of unease Turing expressed that
unease even before the field was named
artificial intelligence he said that you
know in the in the best case they'll
keep us as pets
so this is not a new concern and you
know if you ask these guerrillas you
know here they are having a meeting to
discuss it you know what were your
ancestors wise to create this this human
race these more intelligent beings and
us I think by now they'd say no it
wasn't it wasn't a good idea okay so why
specifically I mean we're not creating a
new species and these are these are
things that we design so perhaps it's a
different situation it's also perhaps
different from superior alien
civilization landing on earth so those
analogies are not exactly persuasive so
what's really wrong so even though we
will design these systems the problem is
that they could be extremely good at
achieving anything and in particular
things that we don't want and there is
no real discipline that exists right now
of figuring out what we want and making
sure that that's what the machines are
actually achieving so all the fields
that deal with rational decision making
assume that the objective is exogenous
that someone else is going to come along
and plug it in and then the machinery
whether it's the AI system or the you
know econometric optimization or
whatever it might be the machinery is
going to figure out how to achieve that
objective and that
why's that utility function and this
point was made by Norbert Wiener in a
very nice paper from 1960 and I
recommend reading that paper if you have
a chance so he says if we use to achieve
our focus is a mechanical agency with
whose operation we cannot interfere
effectively we better be quite sure that
the purpose put into the machine is the
purpose which we really desire and that
sort of says it right there and you
could say that this basic idea goes back
a lot further so King Midas put the
purpose into the machine so to speak
when he asked that everything turned
into gold that he touches and then it
was too late he realized that his food
and his drink and his relatives were
turning to gold and he died of misery
and starvation and there was this
problem of a superior agency carrying
out the wishes that you state recently
Steve Omohundro well recently 15 years
or so ago Steve Omohundro pointed out
that there's even a there's even a worse
problem or something that compounds that
problem that no matter what the
objective you put in it's very hard for
a machine to achieve that objective if
it's dead and therefore the machine will
attempt to preserve its own existence
even if you don't put that in as an
objective and this has been discussed
extensively yesterday as well and it
will also improve its chances of success
by acquiring as much resources as it can
and so if you have those tendencies
combined with an objective that it's not
quite the one that you want then you
were setting up a kind of a chess match
or a go match between the Machine and
the human race and that doesn't
necessarily go too well and so this is
the basis of the concern or one of the
basis for the concern there are others
such as misuse which are not going to
address today but that's also a serious
concern so there's a number of arguments
people have put forward why we shouldn't
pay attention in fact there's so many
that I can't go through even half of the
and I could just for those of you who
have doubts that this is even worth
studying please come up with a more
serious argument than the ones that
people have come up with so far so I'll
just give you a few examples so you
often see people within the AI research
community having said all along that you
know we're going to achieve super
intelligent AI as soon as you point out
that might be a bad idea
oh it's never gonna actually happen
right whatever we're not really gonna
produce super intelligent AI and I just
like to tell a little story about this
gentleman who's Lord Rutherford who was
a most famous nuclear physicist of his
time and on September 11th 1933 he
repeated something that he had been
saying in many venues in many ways that
anyone who looks for a source of power
and the transformation of the atoms is
talking moonshine right and his position
was that we would never be able to
figure out how to extract the energy
that they knew was there in atoms
because they had he knew about mass
defect and equals MC squared
but even Einstein was convinced that it
was basically impossible to get that
energy out and then on this is Leo
Szilard by the way on September 12 1933
he figured out he figured out how to do
it so you have to be a little bit
careful when you say that something is
never going to happen so you know some
people say well it'll happen it's so far
off that we don't need to worry about it
and you could say the same thing about
you know the catastrophic effects of
global warming you know they're sort of
off towards the end of the century so
let's not worry about it let's just keep
on doing what we're doing and you know
but if you if you for example detected a
large asteroid that was going to crash
into the earth in 50 years time you in
throwout so far off we don't need to
worry about it right you'd say well it
might take 50 years to figure out how to
divert it or destroy it or whatever so
so let's start thinking about it now I
and just to add to that this is not an
accidental event this is something that
we we the world are driving towards
right we are pushing we are using tens
of thousands of highly trained science
billions of dollars to move in this
direction so it seems worthwhile to
think about what happens when you get
there okay so I'm going to skip over a
lot of the rest of these arguments right
so why don't you know one of my
favorites it's well known AI people say
oh you know there's really nothing to
worry about we can just switch it off
right as if a super intelligent machine
couldn't think of that one so so these
these arguments a symptomatic of a
denial syndrome that is worrying
actually because these are smart people
and they're producing arguments that
don't they don't hold water for very
long at all and probably one of the most
insidious ones actually which in in the
most recent report from the AI 100 study
at Stanford so this is a large body of
distinguished AI researchers saying well
you know there might be risks but don't
mention them because if you mention them
that might damage research on risk right
which is just completely bizarre and it
hasn't been a good strategy in the past
it didn't it didn't do the nuclear
industry much good to pretend that
meltdowns couldn't happen and a nuclear
waste did not need to be disposed of and
so on so forth and and essentially the
lack of honesty and and which cause I
think a lack of of internal research and
development and an attention paid to
risk led to the effective demise of the
entire nuclear industry so there are no
good examples of industries that succeed
in the long run by pretending that
something that Israel isn't real so as
David mentioned we just started a new
center called somewhat rudely perhaps
the center for human compatible AI as if
the rest of the field isn't a human
compatible but it's the title serves who
just to remind us that the whole point
of AI is not to create intelligence for
its own sake although that's a really
cool thing to work on but actually to
benefit the human race and we ought to
make sure that happens and to do that we
have this goal of
building AI systems that are provably
beneficial and this is in some sense a
deliberate oxymoron because beneficial
is a very touchy-feely vague term and
provably is sort of the opposite so how
do you put these things together so I
have three there's three principles that
have been helpful so far in our research
so the first one I think we can all
agree with is that the robots objective
is to maximize values for humans and in
particular it has no objectives of its
own it has no self-preservation it pays
no attention to its own you know even
its own monetary value because it's only
the human that cares about the monetary
value of the robot and so you really
care just about the the maximization of
human values the second principle is
that the robot doesn't know what they
are this turns out to be very important
and then the third principle is that
there is information about what those
values are in the behavior of humans in
the what economy is called the reveal
preferences of human beings so this term
value alignment is used in in the
general AI safety area it means how do
we get the objectives of the robot lined
up in the way that I just described with
those of human beings and so one
technique that was actually invented for
entirely other purposes actually to try
to understand the motor behavior of
cockroaches
it's called inverse reinforcement
learning and it's the opposite of
reinforcement learning so reinforcement
learning you are given a reward signal
and then you try to adapt your own
behavior to to get more of that reward
to optimize your long-term sum of
rewards an inverse reinforcement
learning is gather way around you're
observing a behavior and you're trying
to figure out what is the reward
function that this behavior is
optimizing so if the machine is
observing the human and the choices that
the human makes that allows the machine
to gradually understand the value system
that the humans behavior is driven by
and it turns out just in the same way
that
when I want to define a task for a
machine then defining the reward
function is a very succinct way of
defining what the task is as opposed to
for example describing all the action
sequences that the machine should take
under all circumstances which is usually
not a very concise way of describing it
in the other direction the as an
explanation of the behavior the reward
function can be a very concise
explanation and hence very predictive
and so this can be a good way of
learning of the robot learning how to
behave well by first of all learning
what the reward function should be and
this technique has been used who to
program helicopters to do aerobatics and
and various other kinds of tasks it's
it's being considered for use in
self-driving cars so that the car drives
in a style that that you are comfortable
with and so on so we actually need a
slightly different version of this so an
inverse reinforcement learning usually
the robot is observing and learning this
objective then adopts that objective as
its own we don't quite want that because
if I'm drinking coffee I don't want the
robot to to learn that it should drink
coffee that's not quite what we want we
want the robot to learn to make coffee
for me rather than want the coffee so we
have a slightly more complicated version
called cooperative inverse reinforcement
learning which is the in the simple form
a two-player game so there's a human and
a robot and so the human knows in a
sense the value function in that the
human can act according to it but may
not be able to explicate it in a form
that could be directly programmed into
the robot and we can allow for the human
to be not perfectly rational like they
could make mistakes and so on and then
the robot doesn't know what this value
function is but it's job is to optimize
it and when you look at the solutions to
this two-player game you find that they
exhibit the properties that you would
hope that the robot now has an incentive
to ask questions of the human and to
explore but do things cautiously because
it doesn't want to do things that the
human would be unhappy with and the
human has an incentive
to teach the robot and these fall out
directly as solutions of this two-player
game we're not programming in teaching
as a behavior it just is an automatic
consequence of the definition of the
problem in fact the human does not
display the same optimal behavior that
they would if the robot wasn't there so
in fact that means that the inverse
reinforcement learning formulation where
the robot is assumed to you be observing
optimal behavior by a human is actually
an extreme special case we're sort of
looking through a two-way mirror and the
human doesn't even know they're being
observed but in general the human will
not behave optimally and will for
example demonstrate if it's a let's say
a surgeon right well not just so off the
patient but will demonstrate carefully
the steps of sewing up and maybe even
give a commentary and so on so so then
the things you want to have happen will
happen in this in this scenario so let
me just give one simple illustration in
the context of what's called the off
switch problem so as Steve Omohundro
pointed out if you have an objective and
the robot understands that it can't
achieve that objective if it's dead so
the the catch phrase of this talk is you
can't fetch the coffee if you're dead so
in that circumstance the robot would
seem to want to disable its own off
switch because if there's a possibility
that it might get switched off and
therefore fail to get the coffee it
seems like it's rational for the robot
to disable the off switch so that that
can't happen and we don't like this idea
right this is this is precisely what you
know the beginning of the the robot
apocalypse and how can we avoid that so
the answer is don't give the robot an
objective really don't give the robot a
precise objective the robot should be
uncertain about what the true human
objective is it might know that all
things being equal getting coffee is
better than not getting coffee but the
all things being equal covers a lot of
other attributes like whether or not
it's okay to assassinate other people
who are in line for coffee in case in
case they run out of coffee before you
get to the front of the line
is it okay to disable your switch and so
on and the obviously there can be
uncertainty about the objective as a
whole and the nice thing about that
uncertainty is that the robot now
reasons to itself okay well I'm trying
to make the human happy I'm not quite
sure how to do that and my actions might
be such that the human could could
understand that I'm going about this the
wrong way in which case the human will
switch me off but when you have
uncertainty about the objective that's a
good thing because the human is going to
switch you off if we--if switching off
the robot is a better choice for the
human and since the robot cares about
making a better choice for the human
then the robot in that situation would
be happy to be switched off it won't
switch itself off because it doesn't
believe that what it's doing is
necessarily bad for the human but it
knows that the human knows better and
will switch it off if that provides an
advantage to the human so you can prove
that under fairly general circumstances
if you set things up this way it's in
the robots interest to be
switched off thank you okay so this is
right this is a this I think is a fairly
general point I mean the the theorems
and and scenarios we defined so far are
quite you know simple specialized
pedagogical examples to illustrate the
basic point but I think uncertainty and
objectives is actually a much more
important and general issue that we've
largely ignored I mean in in AI we spent
the first 30 years of the field saying
there is no uncertainty period right
that will you know we have symbolic
logical definitions we have definite
transition models we have perfectly
observable environments like chess and
and then sometime in the early 80s but
okay there is uncertainty
you know our sensors are not perfect and
we don't exactly know what's going to
happen so there was a whole sort of
revolution in AI but even throughout
that revolution the idea that the
objective itself was uncertain was
completely ignored it's just not even
thought about and which is bizarre
really and there is actually a technical
reason why you might have ignored it but
I don't think this was the reason why is
that it was ignored which is that in a
standard decision problem like a Markov
decision process uncertainty in the
objective is actually completely
irrelevant because you can just
integrate over it and you behave exactly
like the agent that has the definite
objective which is the expectation of
your uncertain objective so what's the
what so what's the difference okay the
mean more than one minute sorry so
what's the difference the difference is
that in reality the environment can
provide more information about the
objective if it doesn't then it really
doesn't matter the uncertainty can be
integrated out but if it does then it
really does matter and you get very
different behaviors than you can get
from any agent that has a definite
objective and so observable human
actions
a source of information about what the
what the true objective is you might
also say well what about reward signals
what about the standard reinforcement
learning setting where our award signal
is provided and that could provide more
information about the objective well I
actually think that this this is an
incorrect mathematical framework I have
come to this conclusion reluctantly
because I think reinforcement learning
is great but I don't think the reward
signal is a reward I think the reward
signal should be viewed as information
about what the true reward is okay and
if you think about it right if your
objective is to maximize human values
then your your reward is in some senses
in heaven right that the that the human
is happy with what's going on and the
human can't give you a reward right all
it can do is tell you that it's the
human that the human is possibly happy
with what's going on and if you
formulate it this way if the reward
signal is just information and not a
natural reward then the wire heading
problem which is the problem that a
reward seeking agent will take over the
mechanism that provides rewards and then
feed itself as much reward as it can
possibly generate that problem goes away
because if you take over the human who
is providing the reward signal and
forced them to give you rewards you're
not actually gaining any information
whatsoever about what the true human
reward function is and so in fact it's
Dilla tereus to your objective which is
to make the human actually happy right
rather than make the human supply you
with reward signals and so this this
perspective I think maybe resolve some
of the concerns that people have had
about wire heading and and those
negative consequences of reinforcement
learning setups so what we're trying to
get then is a theorem like this right
that as long as the human can be viewed
as even slightly better than random in
choosing their actions to to optimize
their own objectives then a robot
observing that human will be a net
benefit
compared to not existing and we can show
this theorem in these simple settings
and we hope to be able to generalize
this to more more interesting things so
very quickly if we take this seriously
right we get outside of toy worlds and
we say all right let's think about 20 or
30 years where we really do want to
figure out the human value function so
that by the time we have powerful AI
systems they can actually understand
what we want and do things right without
everything having to be spelled out in
enormous detail there's a massive amount
of evidence about human behavior pretty
much everything we've ever written is
about people doing things and other
people being upset about it and so all
of this provides provides evidence for
what our value function is so that's a
good thing another good thing is that we
will have to solve this problem not in
the long term but actually in the short
term if you have a personal digital
assistant which is now a very rapidly
growing sector of the industry you don't
want your assistant and let's say Donald
Trump's assistant to have the same
objectives right that wouldn't work very
well at all and so your assistant has
actually got to learn your preferences
very quickly
and book you into the right kind of
hotels and put you on the right flights
and refuse emails from the right people
and so on so forth so we have to solve
this problem pretty quickly there's a
very strong economic incentive which I
think explains in part the emergence of
this partnership for AI so just to give
you another example right if if the
robot is in the house and has to feed
the kids because you were late home from
work and there's nothing in the fridge
and the robot sees the cat then you
could have you could have a problem
right and that only has to happen once
for the for the domestic robot industry
to be you know wiped out for a decade so
there's this really strong incentive to
get this problem right to understand the
nutritional value versus sentimental
value trade-off unfortunately right
here's the big
I'm sure all of you social scientists
know this already that that people are
much more complicated than you know just
straightforward uniform utility
optimizers there are a lot of nasty
people we have very strong constraints
on our computational architecture and
capabilities and we very enormous Lee or
possibly enormous Lee in our various
preferences it's not even clear that
even if we even if we allow for the
computational limitations of humans that
if we knew what those were when it still
be the case that we could model a human
as having a value system but optimizing
it in this very limited and inaccurate
way it's not even clear that that's true
but that's the working hypothesis for
the time being and the other question oh
I got these so the other so the other
question is and you know how do we not
learn from Hitler's behavior and this is
a complicated thing right
obviously if everyone behaved like
Hitler then all those Hitler's would be
very unhappy right because none of them
would have world domination and this
this would be a very unsuccessful group
of humans so there's a sort of lack of
self consistency in certain types of
value functions that prefer domination
of other people and are not happy when
other people are happy and so on so
forth so I don't want to be in the
business of dictating what the world
value function should be but we would
like some way of doing better than just
taking a sort of population average okay
so there's a lot of questions and David
will probably say why don't we have the
audience ask those questions so fine you
all just give you some prompts so so
this visit this is a big change I think
in the way AI thinks about its goal
which has been let's build intelligence
and then we'll plug in objectives and
hope for the best
instead let's build systems that are
provably beneficial for us not not for
ants or aliens but for us there's
interesting questions about where the
human values come from in the first
place I used to think that it was sort
of you know all sort of
from the basic biological drives but
actually I think it's much more cultural
and passed on by sort of built-in
inverse reinforcing inverse
reinforcement learning process that that
we come with and we adopt the values
that we use to explain the behavior of
others around us
yeah that's on so how do you keep the
robots from manipulating the human
values that would be a much better way
of making sure the robot knows what
those values are
to make them be a certain way also you
could produce values that are easier to
satisfy yes I mean that's that's true we
could the robot could do brain surgery
and change the values but in the and
model we have actually the the human
values are assumed to be fixed so that
wouldn't actually work there I agree I
agree with that as a general point but
we yeah one one step at a time
so you say that um these a eyes would
observe humans in order to determine
what the value function is but you
answer your question about how we
prevent the off switch problem by having
the a eyes assumed that my question is
how will a eyes account for human
mistakes while also maintaining that a
human turning it off is the best it
isn't the best interest for human values
so the robot could assume that the human
is making a mistake when it tries to
turn off the robot yeah so the model we
the model that's in the paper allows for
the human to to behave sub-optimally
right and there's there's a standard
bolts called the Boltzmann model where
the human picks in action but with
certain probability pick something
suboptimal on the probability of picking
that drops off exponentially with how
bad it is and you know so in that model
you can show that the the larger the
irrationality of the human the the more
of a buffer you need in the form of
uncertainty about what the what the
utility function is and if but if your
buffer is large enough then there's
still a positive incentive to allow
yourself to be switched off if the human
is deliberately anti rational right so
the bond on average the human works
against their own best interest then
that really is a problem right because
then you it doesn't make any sense to
assume that the human is going to switch
you off in order to benefit the human
because in fact this human will switch
you off in order to hurt themselves and
what do you do with a human who wants
who act sort of not once to vit the who
acts to hurt themselves even though they
don't want to
Stuart we desperately need your
assistance down here Eleazar in the
meantime I'll give a leisurely
introduction for</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>