<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sam Harris on Artificial Intelligence &amp; Existential Risks (Sept 12, 2017) | Coder Coacher - Coaching Coders</title><meta content="Sam Harris on Artificial Intelligence &amp; Existential Risks (Sept 12, 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sam Harris on Artificial Intelligence &amp; Existential Risks (Sept 12, 2017)</b></h2><h5 class="post__date">2017-09-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Anv0dPe8x04" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to just read a couple quotes to
you from end of faith our technical
advances in the art of war finally
rendered our religious differences and
hence our religious beliefs antithetical
to our survival were fast approaching a
time when the manufacture of weapons of
mass destruction will be a trivial
undertaking while it and these are from
three different three different quotes
well it's never been difficult to meet
your maker in 50 years it will simply be
too easy to drag everyone else along to
meet him with you
so we have this force multiplying spread
of ideas this proliferation of lone wolf
attacks we know what weaponry does what
weapons were you thinking about when you
wrote that when you said in 50 years it
will be simply too easy to drag everyone
else oh were you thinking of bio weapons
geodetic biology nuclear is harder to do
yeah although it's not that hard
actually I mean that maybe it was hard
to invent the technology whether the
Manhattan Project was hard it's not hard
to render much of Los Angeles
uninhabitable for 10,000 years it's far
less hard even so it was invented but
still you need the resources of a
nation-state to create the weapon right
well you actually don't I mean you could
actually if you're willing to die you
can be the weapon and what you need is
the enriched uranium or the plutonium
but you could literally you wouldn't get
the full yield you you would want if you
want to kill the maximum number of
people but you could take two like like
you know 50 pound plates of enriched
uranium and just put one on the floor
and slam the other one on top of it and
it would go critical you would not get a
hydrogen bomb experience yes but you
would get and you would be just but kind
of like the ultimate dirty bomb
experience right so you could you could
actually be the bomb but a much more
reasonable thing to do if you're in this
business is to just do something that's
analogous to the bomb design of
Hiroshima Nagasaki where you have a gun
style apparatus where you're shooting
one piece of enriched uranium or
plutonium into the other writing and
just slant essentially slamming them
together harder than you could
physically and again that the yield
there is not it's not as
complete as you know a nation-state
would produce but still you could get a
multi kiloton yield and they're the
technical issue is just getting the
getting new stuff you know she does
exist yeah and so yes you do not need
the the tools of a nation's day you just
need a few engineers and machinists you
know it's powered I believe simply by
ordinary explosives to get the things
slamming together and I mean there are a
bunch of scenarios that have been
described everyone's horror online where
you can do this in a shipping container
and you truck it into the DC and it
could be activated with a cell phone and
William Perry has a terrifying bit of
animation that he put online that just
shows you how simple and and how totally
destabilizing would be to our society to
do this so just a mad imagine you you
build a simple device which is just
again just like Hiroshima you know in a
15 kilotons explosion if you put that
you know right next to the Capitol
building right you just just like now
you have a continuity of government
problem you know who did you kill you
killed all the senators and congressmen
and and the president in courts and yeah
imagine doing it in one American city
right and then announcing whether this
is true or not who knows but then
announcing you have similar bombs placed
in ten other American cities which will
not identify it yeah and yet will do
them you'll you'll do you know one a
week until your demands are met right
how do we how do we begin to respond to
that right this is an act of terrorism
obviously orders of magnitude beyond
September 11th which ushered in a decade
of just arrangement you know and cost
trillions of dollars I in the aftermath
you know at least two wars and you know
financial crises and so imagine just
imagine this happening in one city this
is within the technical capacity of a
group like Isis or al-qaeda if you don't
ages you just need to get the fuel and
we have almost no way to prevent it I
mean we don't we're not screening things
that are poor
so assiduously as to no this couldn't
possibly get in do you worry about bio
weapons as well yeah you just have to
imagine weaponize in something akin to
the Spanish flu which killed something
like 50 million people in in 1918 yeah
this the sky is the limit there you
could get something that is as easily
transmissible and is even more deadly
when you're talking about a bioweapon
the worst possible case is something
that is easily transmissible and it
doesn't make you floridly ill for long
enough logging to Asia yeah yeah do as
much damage as you possibly can you
sneeze a lot yeah and lots of great yeah
lots of P for a good long time before
you die yeah and then those people are
sneezing on grapes and people and then
nobody knows there's an outbreak until
there's a million infectees or something
like that yeah
and something like Ebola doesn't have
going for it you know as bad as it is as
horrible as it is one of the reasons why
it's not scarier is it is very quickly
obvious how sick people are if you're
talking about airborne transmission of
something that has a very high mortality
and a long incubation period yeah
weaponize that that's that is a
civilization canceling event if we don't
don't have our factory now George Church
may be the only person who can do it
but in 25 years with biology following
with sometimes called the Carlson curve
which is even steeper than the Moore's
Law curve
who knows when 10 people than 100 than a
thousand people so I'd like to close on
something that I wrestle with a lot you
gave a great TED talk on the risk of
super a I I won't make you replay it
here because people can access it I'll
just pull two quotes from it to just set
the context you describe the the
scenario of a super AI having better
things to do with our planet and art
perhaps our atoms then let us continue
to have them as being terrifying and
likely to occur and also saying it's
very difficult to see how they won't
destroy us and I don't think that those
are shrill or irrational statements
personally I also don't think it's real
or irrational to think that what George
church alone can do today
we'll be the province of many millions
of lab techs probably in our lifetime
yes and with those two forces out there
I don't know what scares me more and I
think about proliferating democratizing
existentially destructive technology
just about the only thing I can think of
that might protect us against such a
thing would be an incredibly benign
super AI Europe that has functional
omniscience because of its its ubiquity
in the networks and it has functional
omnipotence because of its mastery of
who knows nanotechnology or something
else but boy we're both scared about is
super AI it's almost like super AI can't
live with them can't live without them
you know how do we navigate those twin
perils and do we need to perhaps embrace
a super AI as a protective mechanism for
democratized super destructive power
yeah well I do think it really isn't a
choice I think we will develop the most
intelligent machines we can build unless
something terrible happens to prevent us
doing it the only reason why we wouldn't
build the civilization is it's thrown
violently backwards yeah somebody said
you know George church loses his mind or
one of his techs does and we have some
pathogen that renders us incapable of
keeping our progress going on the
technology front and you just have to
imagine how bad that would have to be in
order to actually stop the march of
frogs yes yeah you know we would have to
have a world where no one understood how
to build a computer again and no one
ever understood how to build a computer
again going forward so canticle for
Leibowitz type of your activeness yeah
so if it's not that bad we will keep
making progress yeah and you don't need
Moore's Law you just need some
incremental progress the content of time
yeah yeah at some raise yeah
and at some point we will find ourselves
in the presence of machines that are
smarter than we are because I don't
think there's anything magical about the
wetware we have in our heads as far as
information processing so the moment you
admit that this can be that what we call
a mind can be
implemented on another platform and
there's every reason to admit that
scientifically now and I leave questions
of consciousness aside I don't know that
consciousness comes along for the ride
necessarily if you get intelligent
machines and and ironically the most
horrible vision is one of building super
intelligent unconscious machines because
in the presence of consciousness at
least you could argue well if they wipe
us out well at the very least we will
have built something more important than
we are we'll have built gods we will
built minds that can take more pleasure
in the beauty of the universe than we
can who knows how good the universe
could be inhabited in their hands right
now but if the lights aren't on if we've
built just mere mechanism that is
incredibly powerful they can be goal
directed but for whom there is nothing
that is like to be directed toward those
goals
well that really strikes me as the worst
case scenario because then the lights go
out if we go out so so it sounds like
you believe that the super AI is
inevitable unless something the other
really terrible happens yes so our best
shot of surviving is to do all we can to
make sure the super AI that one day
inevitably arises is benign yeah is
aligned with our interest intelligence
is is the best thing we have releases
it's our most valuable resource right so
it is either the source of or the
safeguard for everything we care about
right and there's overwhelming economic
incentives yes you thousand immediately
rich spenceley
smart people intensely well capitalized
companies to go screaming down that path
yeah so all the incentives are aligned
to get into the end zone as quickly as
possible and that is not the alignment
we need to get into the end zone as
safely as possible and it will always be
easier to build the recklessly unsafe
version then it will be to take the
further step of figuring out how to make
this thing so yeah so and that's what
worries me but but I think it it is
inevitable in some form and I'm not
making predictions that that we're gonna
have this in ten years or 20 years but I
just think at some point
and again and and and the human-level
bid is a bit of a mirage because i think
the moment we have something human level
it is superhuman yeah it's not blows
past that yeah yeah that's a mirage yeah
and people are imagining somehow that
that's a stopping point it will barely
get there and then we'll stay there for
a long time it could only be the case if
we are ourselves at the absolute summit
of cognition which just defies common
sense only but we just know that's not
true stake you know the calculator and
your phone I mean that's not human level
that's that is omniscient with respect
to arithmetic yeah you know and you know
just having that the totality of human
knowledge instantaneously accessible
through the internet and if we hook
these things to the internet it has a
memory that is superhuman and a an
ability to integrate data the superhuman
so the moment all of these piecemeal
cognitive skills cohere in a system that
is also able to parse natural language
perfectly ya know that you can talk to
it and it understands it does what you
want
if give all of all of the answers to the
questions are no longer like series
answer is where they contain you know
howlers you know every third trial but
they're the most perceptive best
informed most articulate answers you're
getting from any mind you ever interact
with right once those gains are made
they won't be unmade it's like chess
it's like once computers were better at
chess than people yeah and now we're in
this this sort of no-man's land which
again which I think will be fairly brief
where the software yeah the combination
of a person and a computer is now the
best system but at a certain point and
I'm amazed that anyone doubts this but
at a certain point I think it'd be the
case that adding the ape to the equation
just adds noise to the equation and and
you know the computers will be will be
better than cyborgs and once they are
there's no going back from that point it
may not be everything it may there may
be things we neglect to build into our
AIS that turn out to be important for
you know human common sense or I mean
that this is this is the scary things we
don't know what is required to fully
align
an intelligent system with our well
being you know and and so we could
neglect to put something like our common
sense because we would don't perfectly
understand it into these systems and
then you can get errors that are deeply
counterintuitive inordinate our innocent
alle gets to you know Nick Bostrom's
cartoon thought experiment of the
paperclip Maximizer I'm like well who
would build such a machine what we
wouldn't but we could build a machine
that in the service of some goal that
was is obviously a good one could form
some instrumental goal that we would
never think an intelligence system could
form and that we would never think to
explicitly prevent yeah and yet this
thing is totally infected everything
good equilibrium where it says more
paper clips good yeah I'm gonna do that
for a while yeah and soon the universe
is paper clips well Sam you have been
extravagantly generous with your time I
appreciate it
not at all it's a pleasure</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>