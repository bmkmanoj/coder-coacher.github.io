<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Max Tegmark - Artificial Intelligence - The Future of Life 3.0 | Coder Coacher - Coaching Coders</title><meta content="Max Tegmark - Artificial Intelligence - The Future of Life 3.0 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Max Tegmark - Artificial Intelligence - The Future of Life 3.0</b></h2><h5 class="post__date">2017-11-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HOdIDFNfAwM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">please join me in giving a big Town Hall
welcome to max tegmark watch this is
such a pleasure it's a real honor and
pleasure to be back here in Seattle
again and I can think of no more fitting
place to talk about questions to do with
the long-term future of humanity and
what we really want it to mean to be
human and what sort of future we want to
build then in in a church 20 seconds and
counting
t-minus 15 seconds guidance is internal
twelve eleven ten nine ignition sequence
start six five four three two one zero
so can someone remind me where this
rocket went to indeed this mission was
not only successful but also inspiring
because it showed us that when we humans
manage technology wisely we can
accomplish amazing things that our
ancestors could only dream of
so in this spirit I want to devote the
rest of this talk here to another
journey powered by something more
powerful than rocket engines where the
passengers are not merely three
astronauts but all of humanity so let's
talk about our collective journey into
the future with artificial intelligence
my friend and colleague young Colin
likes to emphasize that just as with
rocketry it's not an
to just make our technology powerful we
also need to figure out how to control
it
and where we want to go with it and
those are really key themes for tonight
13.8 billion years ago our universe was
very boring and since then it's
transformed not only from hot to cold
but also from boring to increasingly
complex and interesting and I've laid
even coming alive having small parts of
it becomes self-aware and marveling or
at its beauty and pondering what they
want for the future and in this cosmic
perspective I think it's important to
reflect on the fact that still today on
this scale life is an almost
imperceptibly small perturbation on
things on an otherwise dead and lifeless
universe but that might change if we
think about the Pacific Northwest it's
been completely transformed by life
right and if we continue our progress
and figuring out through science hard
cosmos works and we use the science to
make technology in particular artificial
intelligence as they argue my book
there's no reason why life couldn't
become a really driving force spreading
throughout much of our cosmos in the
future flourishing not just on earth but
countless other places for billions of
years so there's there's a lot at stake
here there is an enormous upside if we
get things right with technology now the
first life that arrived here on earth I
was actually really dumb I call it life
1.0 couldn't even learn anything in this
lifetime bacteria older software is
hard-coded in this DNA given to it by
evolution in contrast life 2.0 which is
what I call us we can learn and if you
look at me through the eyes of
of a computer geek then learning means
I'm I'm uploading new software it gives
me new skills if I decide I want to
learn Spanish I can study Spanish and
then suddenly I have all these abilities
this offer that I didn't used to have
and it's this ability for us to learn to
design our own software which has
enabled cultural evolution and which has
enabled us humans to become the dominant
form on this planet some think we might
be heading in the direction of Life 3.0
where we can design not just their
software but also our hardware we're
certainly not there but maybe we're 2.1
right now because we can get artificial
kneecaps cochlear implants make a few
other tweaks to our hardware but if you
were robots who are as intelligent as
you are now there will be no limits
whatsoever now you could upgrade your
hardware as well and life would have
them broken fully free from its
evolutionary shackles and have become
the master of its own destiny
now with this rocket metaphor we talked
about the power and then controlling it
and figuring out where you want to go
with it so let's start by talking just a
little bit about the power of artificial
intelligence which has grown
spectacularly in recent years I'm
defining intelligence in the book as
simply the ability to accomplish complex
goals and the reason that you have such
a broad definition is because I really
don't like this carbon chauvinism
attitude where we say if things can't be
intelligent unless they're made of cells
or meat or carbon atoms or whatever so
in this broader definition and it will
include both biological forms of
intelligence and also non-biological
forms which we call artificial
intelligence much of the recent progress
in AI has been not in old stuff like
pocket calculators where they never
learn and just keep performing the same
computation every time you press the
square root button or what not but
rather machine learning sis
which can can learn and improve what
they do through data from the
environment and the subset of that
that's growing the fastest has been deep
learning a kind of machine learning
using neural inspired architectures
neural networks so if you look at for
example AI breakthroughs from two
decades ago like when Gary Kasparov of a
chess champion got his posterior kicked
by IBM's deep blue the intelligence in
this machine was not learned it was
programmed in by humans who know news a
lot about chess and the reason it could
beat Kasparov was simply because it
could think faster and remember more in
contrast most of the recent stuff and it
impresses us is learned so for example
Google will develop a neural network
where you just train it on massive
amounts of data and it didn't really
know and then you can take just pictures
represented by numbers that tell you
what color the different pixels are and
spit out the caption saying this is a
group of young people playing a game of
Frisbee if she's dead feed at this image
it says that this is a herd of elephants
walking across a grass field and what's
so interesting is nobody had programmed
in any information into this neural
network about what an elephant is or a
frisbee or field or that there even is
such a thing is a three-dimensional
space or a world with objects in it it
just learned to perform this particular
task and even more striking example I
think is this one so can someone remind
me what game this is yeah so this is you
can date you can figure out when I was a
kid from the fact that this was modern
and cool back then so what's really
interesting about this is this is deep
neural network with so-called
reinforcement learning which is
gradually learning to play this game it
has no idea what a paddle is or what a
ball is what a brick is or anything it
just tries random things mentha
maximizes score and by now even though
it's tucked in the beginning you can see
now it catches the ball every time just
better than I can do on this game and
most amazingly of all the people who did
this at google deepmind they actually
had not wasted as much time
I had on this game as a kid and didn't
know that there's this really cool trick
where if you keep featured eventually
drill a hole on the side look you could
just rake and the points like this and
after the a I had to figure this out
look how it just ruthlessly does this
over and over and over again so you
might say well this is a very simple
little game world through dimensional
and dramatically less complex than the
real world but if you're a robot then
you can think of life itself is a kind
of game so you might ask what happens if
you use this sort of technology just to
see how well a machine can gather
performing or an AI certain tasks this
is what exactly what google deepmind did
this last summer to see if the heat if
mushy if simulated robots could learn to
walk and this is what happened
and what's interesting here is that the
no information was put into this about
walking or videos of walking you
anything say I system was just rewarded
whenever had managed us by sending
random commands to bend the joints move
forward eventually it came up with
this really begs the question
what are the how far can can a I go how
much damage can artificial intelligence
actually end up learning other things
that we humans do
I like to think about this in terms of
the following image inspired by my Hans
Moravec and I've drawn it here so that
the height represents how difficult each
task is in a landscape of tasks and the
sea level represents how good machines
are doing these different tasks so as
you can see some particular tasks like
playing chess or doing arithmetic have
long since been submerged by machines
that are better than us at it now
whereas others have not but we have kind
of global warming going on here in this
intellectual landscape where the sea
level keeps rising right so what's gonna
happen eventually
some people think and some serious AI
researchers think that for some reason
the progress in AI will eventually stop
and some parts will never be submerged
that there will be some things that we
do the machines can never do but there
are a lot of others in fact most AI
researchers in recent polls who think
that the sea level will never stop and
that eventually machines will learn to
do everything that we can
the median guess for when this might
happen and some recent polls have been
maybe decades from now so in other words
if you ask people who work on AI when AI
will surpass human abilities on all
tasks some people are up here in the
techno skeptic camp and think either
never or too far into the future for us
to need to talk about it now which means
there's no point in worrying about how
we should steer the development of AI
because it doesn't matter yet most
people are sort of down here but some of
those researchers are also thank you is
there's no point in talking about
steering either because they're
convinced that things are gonna end up
awesome
I call them digital utopians or because
they're convinced that things are gonna
suck so it doesn't matter what we do but
actually most AI researchers are fall in
this camp here in the middle where they
think what's gonna happen really depends
a lot on what we do now and how we how
we plan and how we steer the whole
development which is this is the place
where it's max let me motivating to try
to do things about this
we launched a survey of this my my
colleagues on the site age of AI org
well you can go and say what you think
and I'd last weekend I had just analyzed
the first fourteen thousand eight
hundred eighty six responses and it's
actually fascinating to see that the
general public it makes exactly the same
kind of guesses as the AI experts
some people think not never gonna happen
some people think it's definitely gonna
be awesome some people think it's
definitely gonna suck but most people
are here thinking yeah it's pretty
plausible it'll happen maybe in decades
or so and could be good or bad so then
that makes it very interesting to ask
what useful things can we do now this
year that's in a good direction which is
the perfect segue into this question of
steering our technology rolling it so to
help with this together with some
co-conspirators here we found that the
future of life Institute and I'm very
fortunate the two of my co-conspirators
are actually here so Tucker Davey and
may Nikita tegmark you wanted to stand
up for a moment to people can see there
they are
Tucker is actually in addition to be an
awesome colleague he is shown the great
wisdom recently of moving to Seattle
yay and you can see we have the the even
the word steer here right in our mission
statement the goal is we want the future
life to exist and be as awesome as
possible and we're interested in how can
we how can we steer our technology so
that this happens I'm optimistic that we
can create a very inspiring future with
technology including AI as long as we
win this race between the growing power
of the technology and the growing wisdom
with which we manage our technology but
that's an important if when we invented
fire for example we screwed up a bunch
of times and then we invented the fire
extinguisher with cars we screwed up a
bunch of times and then we invented the
seatbelt the airbag and the traffic
lights and so on and for all less
powerful technologies this strategy of
learning from mistakes is how we and
making sure wisdom stay ahead okay
but when technology gets more and more
powerful which of course it is time
progresses at some point we reach a
stage a threshold beyond which we don't
want to learn from mistakes anymore I
would argue there with nuclear weapons
and certainly with with super
intelligence learning from mistakes is a
really bad idea
you don't want to be like oopsie we
started World War three but let's see
what we can learn from this much better
to think through the risks plan ahead
and avoid them and get things right the
first time because it might be the only
time we'll have some people tell me max
no no no don't talk about risks with
technology because that's just Luddite
scaremongering
I don't think it's Luddite
scaremongering I think it's just safety
engineering when NASA thought through
very carefully all the things that could
have gone wrong with the Apollo 11
mission and believe me there are a lot
of things that can go wrong when you put
three people on top of a 100 meter tall
rocket full of highly flammable or if
you write and send them into space that
I wasn't Luddite scaremongering
that was exactly the safety engineering
that resulted in the missions success
and that's exactly what I think we need
to do with powerful technologies like AI
I think that we have not we it's pretty
clear though that we haven't really
absorbed this message yet that our tech
is getting so powerful that we have to
switch from learning for mistakes to
safety engineering nuclear weapons
illustrate that so yesterday for example
was that was that a great famous
anniversary of something that you know
of well it came pretty close to being
the anniversary perhaps of World War
three the 34th anniversary to be precise
because this guy Stanislav Petrov was in
charge of a Soviet early warning system
which showed that the US had just
launched five-minute man nuclear
missiles at them and fortunately for us
here tonight and in Seattle he decided
just based on gut instinct
that there was something fishy about
this and didn't escalate this and the
Soviets didn't launch their entire
nuclear arsenal asset at that time but I
don't think we want to keep relying on
this kind of luck in the long term so so
what can we do to win this wisdom race
and stay ahead of the game we've put a
lot of energy with the future life
Institute into trying to engage AI
researchers in this discussion we
organized the conference in Puerto Rico
2015 and then the beginning of this year
we organized the conference in Asilomar
California where we were very honored to
have 150 people including many of the
leading AI researchers both CEOs of
companies doing AI and many of the
leading academics and also a lot of
other great thinkers and key output from
this process was the 23 Asilomar AI
principles which are intended as helpful
guys the wisdom that we wanted to
develop here so I wanted to spend a
little time unpacking some of the core
messages from this so first of all the
whole goal of AI research people felt
should be redefined from just building
powerful technology because it's cool to
building beneficial intelligence so that
it's actually the steering is sort of
part of the the project I mean you live
here in Seattle where once upon a time
that the coma Narrows Bridge fell down
right if you're a civil engineer
building bridges that don't fall down is
just part of your job you don't first
study civil engineering and then you
take a special ethics class to build
like ethical bridges it's just part of
part of the very goal of civil
engineering to build things that work
and similarly people here in a DI
community feel that making AI systems
actually do good things should just be
part of the very goal itself now then it
says research funding here so there are
a lot of tough technical challenges
tough questions we need to answer for
example raise your hand if your computer
has ever crashed
I had a very sobering conversation right
before this year with with a cyber
security expert who told me all sorts of
horrifying things about them Equifax
breach and other things and and clearly
you know before we entrust computers
ever more with taking with operating our
infrastructure and weapon systems and so
on and we really need to figure out how
to transform today's buggy and hackable
computers into robust and I systems that
we can really trust right
for example if things can get hacked and
all the wonderful technology we build
with them can't be just be turned
against us so that's those are examples
of hard questions and safety research
and I'll talk about additional questions
later and there is I'm very happy that
the AI community is really joining this
whole discussion very vigorously and
engaged with it and there was so many
researchers out there who want to do
this kind of research but they have
right now have almost no funding for it
so it's very important to governments
around the world view funding AI
research is a safety research is just an
integral part of computer science
funding just like you would never fund
nuclear reactor research without funding
nuclear actors safety research a second
principle I want to highlight here is
number 15 but says that the economic
prosperity created by AI should be
shared broadly to benefit all of
humanity what's that all about well of
course technology has greatly grown the
economic pie in America over the decades
but as you're also well aware that
growth hasn't been spread evenly in fact
if you look at the bottom 90 percent of
income earners in the US they also
started getting wealthier and wealthier
until about when I was born so and after
that their income really hasn't
particularly grown at all maybe it was
my fault but a lot of economists think
this is very linked to automation
replacing higher pay paying jobs
by lower paying jobs and some people say
well don't worry about this there's
gonna be a big change soon when also
some new jobs that don't exist yet are
gonna try to change this and these
people are gonna get much better off
again but actually if you look at the
data it really doesn't support that
conclusion at all because you could have
made the same argument 200 years ago
when all and lots of farm jobs were
being automated by tractors and combines
and so on you could have argued that
those jobs will be replaced by new jobs
that didn't exist in 1917 but did that
actually happen well let's look these
are the most common jobs in the u.s.
today the one sorted by size in a giant
pie chart here and you should start
reading down from the top managers
drivers retail salespeople cashiers
cooks waiters etc it's only when you get
the twenty first place that they get to
which kind of job that didn't exist in
1917 so and that job which is software
developers employs only this tiny sliver
of all people so what actually happened
it was actually quite different what
actually happened was that people who
were automated out of farming and
earlier on automated and industrial out
of their jobs by the Industrial
Revolution they switched from jobs where
they were earning a living using their
muscles to jobs where they by educating
himself more could instead earn a living
using their brain okay and those were
largely jobs that already had existed
before and
in this case it turned out to be
generally a good thing because those
brain jobs paid more than the muscle
jobs so everybody so people got better
off this time is different because right
now what we're seeing is a lot of
middle-class jobs but people use their
brains or being automated away by AI
suite and these people then are being
switched into other jobs where they
typically pay less if you look at not
the bottom 90% but the bottom 30% of
income earners in America they're
actually the inflation adjusted incomes
have actually gone down a lot in recent
decades which is which is created a
really growing anger and bitterness
which has given us Donald Trump which
has given us brexit and giving us and
given us an increasingly polarized
society so what the AI community is
saying with a very United voice here is
that you know if we cannot figure out a
way to take all this fantastic growth
and wealth that's being printed can be
produced by technology by AI new
services and new product and figure out
a way to share it well enough that
everybody gets better off and shame on
us then I want to switch to just mention
briefly principle 18 here saying that an
arms race and lethal autonomous weapons
should be avoided what are they what
what's that all about this is not about
drones which are remote-controlled
weapons where a human is still they are
making the decision who to kill
instead of totally filament honest
weapons are completely automatic AI
systems where the where the machine
itself makes the decision about who to
kill and then go ahead and Minton and
kills that person with no human in the
loop whatsoever okay and this is not
something which is some sort of far-out
science fiction like thing or something
that requires superhuman intelligence
this is really technology that we almost
have now and the question is do we want
an arms race with these kind of
anonymous murderer machines get get so
cheap
everybody willing max to grind can buy
them and the reason for it significant
concern is that although most of the AAA
that's been invented so far has it been
invented in the civilian sector in
universities and by private company then
and there's tons of money invested in
civilian companies now the military here
and elsewhere is starting to invest more
and we're getting budgets that look like
this so that the loud sucking noise we
hear that places like MIT and UW here
where people try to recruit the bright
new AI graduates will come increasingly
from the military and the AI community
does not want AI to become primarily a
new way of harming people but rather
finding new ways of helping people any
any science can be used mainly to harm
people or to help people and there's
reason for hope here because biologists
for example they really really advocated
very successfully as a community that
biology should be used to help people
and manage to actually push for there
being a ban on biological weapons as a
result of which today if you ask people
what they associate with biology they're
much more likely to say new cures than
biological weapons right similarly if
you ask people what they mainly
associate chemistry with today they're
much more likely to say new materials
than bio weapons and even though there
has been a lot of cheating on those
treaties on bio weapons and chemical
weapons they have been incredibly
successful in stigmatizing these weapons
most people feel that these are just
disgusting weapons and the stigma is so
strong that even people who have cheated
for example like Assad in Syria actually
gave up the chemical weapons because
they were so stigmatized in order to not
get invaded and the AI community is very
much hoping that it's gonna be the same
with with with AI weapons and that yeah
it can go down in history as basically
new mainly new ways of helping people
this is not just some sort of long-term
pie in the sky stuff again I want to
emphasize there isn't there are meat
there's the meeting at the United
Nations in November and there's two
months to discuss whether to start
negotiating international treaty try to
limit leaflet hummus weapons finally
turning to more long-term issues here
you'll see first of all that the talks
here about existential risks super
intelligence the recursive
self-improvement and so on which are
words that if you said them
a few years ago a lot of people would
just totally dismiss you as being a
Luddite who had no clue what you were
talking about and obviously didn't know
anything about AI and yet these
principles are signed here by over a
thousand AI researchers including demis
hassabis the founder and CEO of google
deepmind arguably the leading AI company
in the world on laocoön from facebook
Jeff Dean from Google brain and really a
who's who of leaders in AI so what's
happened how have these concerns become
more mainstream what's the logic that's
led people to sort of take this
seriously let's unpack this a little bit
it's a super intelligence first of all
refers to intelligence that's not just a
little bit smarter than us but actually
way better than us it's everything why
would we anyone in the right mind think
that something like that might happen
even if you even if you assume that
maybe we will get machines they can do
everything we can in a few decades
why would all of a sudden you get go
from there
to machines that are like way way better
than us wouldn't that take you know tens
of thousands of years or whatnot well
actually maybe not there's this very
simple argument which is summarized by
just this one paragraph from Irving Jay
good two years before I was born
which basically points out that if you
can make a machine that can do
everything that humans can just as well
then that includes making AI systems
because that's something we can do so
then instead of high
40,000 people at Microsoft or at Google
in order to do this why not just use 40
million pieces of software for example
to do this stuff much faster so this
would mean that instead of instead of
being developed on through Yuma typical
human are in these cycles of like years
for each big improvement whatever the
and the time scale would now be set
instead by how fast machines can do the
research right which can be scaled up
dramatically and it could go much much
faster in fact in my book and the
opening you'll see an example of how
this might happen very fast and what I
could lead to and then of course this
improved machines could in turn develop
even helped be used to develop even
better machines and better machines and
you could end up with a situation which
has been called an intelligence
explosion where before long machines
will be just dramatically beyond us in
capability these people who scientists
are not saying that this is going to
happen they're just saying this is this
is possibility we should take seriously
and neat the plan for accordingly you
would think about really think hard
about whether we want or not and if we
want that then I think through the
pitfalls
it also says existential risk here which
refers to risks for example that could
just drive humanity extinct altogether
why would you take that seriously I've
rolled my eyes and feel so terrible if
I'm forced to watch The Terminator
because it's just so absolutely silly
and unrealistic these sort of these sort
of horror flicks and and why so why
would anyone in their right mind think
that it could be a threat to have highly
intelligent machines first of all it's
important to remember that intelligence
can give power what we humans are much
more how much more power on this planet
and Tigers not secure as we have bigger
muscles and sharper claws but because
we're smarter all right so if we create
machines that are much smarter than us
there is the possibility that for that
other humans could use
to take power over us and there's also
possibility that those machines
themselves might take power over us but
why would they part why would they have
anything against us especially wouldn't
they be just be grateful that we built
them couldn't he just pull unplug them
or something
well terminator and movies like that
make us worry about altogether the wrong
thing they worry but they make us worry
about somehow machines being evil and
deliberately having the goal just harm
us but actually it doesn't sound silly
the mission that we should worry about
machines having goals of harming us or
having that machines might have typical
silly alpha male traits like hogging
resources and defending themselves if we
are the ones who program the goals into
them you know why should we worry about
that there is actually a very simple
reason for why you might worry about
that and I'm gonna illustrate that by
just this very silly little computer
game I made up for the purpose of this
where you have so just imagine that
you're this very friendly little blue
robot okay but very smart and your only
goal is to save as many sheep as you can
from the big bad wolf you don't care
about dying or any or getting rich you
just love these cute little sheep bees
and want to help them okay but you're
smart so you think a little bit and then
you realize that if you run into the
bomb here and get destroyed you're not
gonna save any sheep at all
so you develop a self-preservation
instinct right you can immediately see
that this argument is very general if
you have a future very intelligent robot
then you tell it to go down to the
supermarket and buy you some good food
and fixed up a nice Italian dinner for
you and then on the way back the robot
encounter is a mugger they're trying to
destroy it the robot is gonna realize
that even if it's so it's not afraid of
dying if it gets destroyed you're not
gonna get your Italian dinner and that's
its only goal it wants to cook you that
dinner right so it's gonna defend itself
or try to find a way of avoiding getting
destroyed it just follows from
the goal you gave it the sub goal of
self-preservation
what about acquiring resources
well this robot would also have an
incentive to understand its it's world
better how it works and if it discovers
that this potion here can make it run
twice as fast then it's gonna want to
acquire that resource because then it
has time to save more of these cute
little sheep before the wolf eats them
and if it discovers that there's a gun
there it's gonna have an incentive to
acquire that resource so it can shoot
the wolf and save all the sheep not
because it wants resources per se but
because it really wants to save sheep
and having those resources help if you
have a very advanced computer in the
future that you programmed to really
help Seattle as much as possible and
that's the only goal you've given it it
would also have an incentive to quire
more resources or compute better and
help Seattle better and what if it
decides to take those resources from
from the coma or something right so then
you might have inadvertently given that
a sub goal that you didn't want and
therein lies the rub that it really it's
really crucial that we align if we
create machines that are very smart we
make sure that their goals are aligned
with ours it doesn't have to be a bad
thing to be in the presence of other
intelligent beings that are smarter than
us in fact we've already been there
all of us have been in the presence of
beings that were smarter than us when we
were about this big
they were our parents right and the
reason that was okay was because their
goals were aligned with our goals
so in summary almost whatever it were
all Tibet goal you give to a very very
intelligent entity if it's open-ended
the goal it will tend to lead to these
sub goals like you don't get better
Hardware better software quiet resources
and so on and that means we have to be a
little bit careful so in other words
when I said earlier that people really
want to support more anti safety
research it doesn't only include
short-term challenges like making
machines not buggy and not hackable but
also questions about how you can
make machines for example understand our
goals learn our goals and and retain our
goals and I want to show you just a very
brief video about super intelligence
that elaborates on these challenges a
little bit more will artificial
intelligence ever replace humans is a
hotly debated question these days
some people claim computers will
eventually gain super intelligence to be
able to outperform humans on any task
and destroy humanity other people say
don't worry AI will just be another tool
we can use in control like our current
computers so we've got physicists and AI
researcher max tegmark back again to
share with us the collective takeaways
from the recent Asilomar conference on
the future of AI that he helped organize
and he's going to help separate AI myths
from AI facts hello
first off max machines including
computers have long been better than us
that many tasks like arithmetic or
weaving but those are often repetitive
and mechanical operations so why
shouldn't I believe that there are some
things that are simply impossible for
machines to do as well as people saying
making minutephysics videos or consoling
a friend
well we've traditionally thought of
intelligence is something mysterious
that can only exist in biological
organisms especially humans but from the
perspective of modern physical science
intelligence is simply a particular kind
of information processing and reacting
performed by a particular arrangements
of elementary particles moving around
and there's no law of physics that says
it's impossible to do that kind of
information processing better than
humans already do it's not a stretch to
say that earthworms process information
better than rocks and humans better than
earthworms and in many areas machines
are already better than humans this
suggests that we've likely only seen the
tip of the intelligence iceberg and they
were on track to unlock the full
intelligence that's latent in nature and
use it to help humanity flourish or
flounder so how do we keep ourselves on
the right side of the flourish or
flounder balance what if anything should
we really be concerned about with super
intelligent AI here is what has many top
AI researchers concerned not machines or
computers turning evil but something
more subtle super intelligence that
simply doesn't share our goals if
heat-seeking missile is homing in on you
you probably wouldn't think no need to
worry it's not evil it's just following
its programming know what matters to you
is what the heat-seeking missile does
and how well it does it not what it's
feeling or whether it has
at all the real worry isn't malevolence
but competence super intelligence AI is
by definition very good at attaining its
goals so the most important thing for us
to do is to ensure that its goals are
aligned with ours as an analogy humans
are more intelligent and competent than
ants and if we want to build a
hydroelectric dam where there happens to
be an anthill there may be no
malevolence involved but well too bad
for the ants cats and dogs on the other
hand have done a great job of aligning
their goals with the goals of humans I
mean even though I'm a physicist I can't
help think kittens are the cutest
particle Arrangements in our universe if
we build super intelligence we'd be
better off in the position of cats and
dogs than ants we're better yet we'll
figure out how to ensure that AI adopts
our goals rather than the other way
around and when exactly is super
intelligence going to arrive when do we
need to start panicking
first of all Henry super intelligence
doesn't have to be something negative in
fact if we get it right hey I might
become the best thing ever to happen to
humanity everything I love about
civilization is the product of
intelligence so if AI amplifies our
collective intelligence enough to solve
days and tomorrow's greatest problems
humanity might flourish like never
before
second most AI researchers think super
intelligence is at least decades away
but the research needed to ensure that
it remains beneficial to humanity rather
than harmful might also take decades so
we need to start right away for example
we'll need to figure out how to ensure
machines learn the collective goals of
humanity adopt these goals for
themselves and retain the goals as they
get ever smarter and what about when our
goals disagree should we vote on what
the machines goal should be should we do
whatever the president wants whatever
the creator of a super intelligence
wants let the AI decide in a very real
way the question of how to live with
super intelligence is a question of what
sort of future we want to create for
Humanity which obviously shouldn't just
be left to AI researchers as caring and
and socially skilled as we are to
summarize things we should absolutely do
I feel to win this wisdom race trying to
positive future with with AI is not only
ensure that the wealth created by it
really makes everybody better off and
doesn't that the technology doesn't get
perverted into mainly new ways of
killing each other in a pointless race
to the bottom arms race and invested in
AI safety research of all sorts but we
should also think really hard about what
sort of future we want how do we
actually want to steer this rocket where
do we want it to take us so let me say a
few more words about this in the book
you'll find that I talked quite a lot
about goals and possible futures but I
don't say what you should want because
that's something there only you can
figure out and I would really encourage
you all to ask these questions over
there over beers or afternoon after
dinner parties with your friends what
sort of future would you actually be
excited about with or without technology
and to satisfy our curiosity those of us
at the future life in stood we launched
a little survey which you can take
yourself although the age of AI org and
looking at the first fourteen thousand
something responses I was quite
interested to find first of all that
most of the responders here actually
they want super intelligence although
many aren't sure and I'm pretty sure
they don't want it in terms of who
should be controlled in control of
society once there is super intelligence
a lot of people said humans even more
said humans and machines together but I
was also amused to see that there was a
block here people said nah only machines
don't trust us humans when when we asked
what if they want the machines to be
conscious or not then some people said
yeah sure so that they too can enjoy
having positive experiences and and so
on but there was also a big chunk of
people who said no now we prefer having
our helper robots and so on just be
unconscious zombies so we don't have to
feel guilty about how we treat them in
terms
what sort of a future so what sort of
goals a high-tech future should strive
for a lot of people felt and this is a
very appropriate question to talk about
in a church that one should try to
maximize positive experiences or
minimize suffering see the questions are
all written in the broad way so they
take it in principle apply also to the
conscious machines some people felt that
we should just let future intelligent
beings do absolutely whatever we want
they wanted even if it was something
that we realized was this pointlessly
banal like turn the whole universe and
the paper clips but but most people
actually felt no that since we are
actually the ones who are creating this
technology if we if we choose to do so
we should have a vote also as to what
happens in the future the one question
that there was a broadest agreement on
all was this one where almost nobody
felt that we should deliberately
confined life forever to only exists
here on this planet and that we should
is what we should allow for the
possibility that life could spread the
more of this of our cosmos as well in
the fifth chapter of the book I explore
a wide variety of society of societies
that one might create or might have will
with or without super intelligence and
here you can see there is total
disagreement about what people prefer
and very very curious again what what
you actually would prefer for our future
so to summarize as I said in the
beginning our universe itself has only
just barely woken up a little bit it
seems like only a very small part of it
is alive and conscious so far yet I
think this is something very beautiful
and worth celebrating but that there is
life at all I think galaxies are very
beautiful but why are they beautiful
it's because we look at them with our
scops we subjectively consciously
experienced them that's why they're
beautiful so I feel that it's not us
it's not our universe giving meaning to
us it's us giving meaning to our
universe and if we should screw up with
our technology and go extinct and
there's no one else to round with
telescopes then these galaxies won't be
beautiful anymore in our whole universe
just going back to being is the
pointless waste of space which I really
feel would be an opportunity lost and I
feel actually feel we have a moral
obligation to not squander this
incredible gift that we have and if we
do this right instead and are really
good stewards of our technology and not
only develop it and make it powerful but
also put in the care and thought and
hard work to make sure that it's
beneficial there are just incredible
opportunities for life to flourish for
billions of years not just on earth but
throughout much of our cosmos thank you
but thank you so much so I want to
remind folks to come on down line up
along the wall here if you want to ask a
question we have about fifteen or twenty
minutes for questions I guess I will
take curators prerogative and kick it
off while people are coming in lining up
so one thing that really struck me was
that beautiful map of sort of human
knowledge that had the different peaks
that the ocean of AI was rising towards
and I noticed that one of the peaks was
science like the highest peak of the
mountain with science but that struck
maybe wrong cuz a lot of research and
science peculiar things like physics is
very quantitative and I could imagine
like an MIT professor of cosmology maybe
sooner than I might imagine an AI poet
or novelist and I'm wondering what your
opinion is about the application sort of
near and medium-term applications in
like research science research
mathematics things like that and whether
you think you'll be able to understand
the papers that the super-intelligent a
professor AI publishes great questions
so first of all I absolutely did not
mean to imply that it's harder to do
science than to do art it might be that
this peak is much higher I just wanted
Ella straight that these are things that
machines are nowhere near being able to
do well now in terms of helping us do
science I think I feel very optimistic
that AI can help quite a lot even before
it can write papers that we don't
understand I mean think about even
science today or if you look at all the
latest breakthroughs and anything from
medicine to cosmology how how many of
those would just not have been possible
without today's computers ability is the
per that late process massive amounts of
data and so on you know even even things
like the work that you kindly mentioned
in the introduction where we had worked
on making the biggest three-dimensional
map of our universe to date and use that
to try to figure out the age of our
cosmos and stuff about its origin you
know we had pictures there of hundreds
of millions of galaxies suppose we had
given them didn't have a computer or
just give them to a grad student and say
okay I'll look at all of these and
analyze them and let me know when you're
done but I would
if they took vitamins and you know could
live for billions of years would have
changed a lot by then already so if even
today these computers has enabled
science to get so much farther I think
there's an incredible potential even a
very near-term where they are helping
helping health science forward so I see
a long line here so let me just remind
you all and try to keep your questions
brief and to make sure that they
actually are questions okay try to keep
it brief so you talked about this
wonderful boom which has been
represented by deep learning and the
power to build machines that learn and
through these certain deep neural
architectures and so that is really
wonderful we can see what it's created
but a lot of these you know even the
most top-of-the-line Google models they
work because if they have hundreds of
computers that can train for weeks and
if you play with these models on your
own they're impressive but not that
impressive so do you think that to make
this next leap towards these you know
the global warming and the rising waters
on your diagram can it be accomplished
by just throwing more computing power at
the models we have or do we need another
sort of breakthrough I think we need
more breakthroughs actually it's a very
astute question first of all when you
train a deep neural network to get very
good at telling cats and dogs apart
that's that you need to give it lots of
data right if you take a human child has
never seen a cat you only have to show
them one and they can already recognize
other cats so we have additional ways to
do anything there are interesting
attempts to try to figure out what that
is and and improve machine learning with
it but I think more fundamentally today
is deep learning software that
recognizes images doesn't actually
understand much with them about them at
all it doesn't understand what a catch
by mentally is whereas you all have a
world model in your head where cats fit
in as part of it and I think a key
breakthrough that we're still waiting
for is to really unify the recent
breakthrough is in deep learning with
the more traditional logic based
just a eyewear you have a model of the
world and you can reason about it the
parts of our brain that we can emulate
best with with AI right now our sort of
low-level input like the visual assist
input the visual system and the final to
the output but the stuff in the middle
you know where we have high-level
discussions and reason about things
that's where we're very much talking at
so I think I think we need we certainly
need breakthroughs like this unifying
all the different things and but and
then a second thing I think is worth
mentioning also about what you brought
up about deep learning not only doesn't
it seem like this alone is gonna get us
all the way to flood all this but it's
also something which has a lot of
challenges in terms of AI safety because
if you train a neural network that can
do all sorts of cool stuff and you have
no idea how it works it can be very
frustrating you know suppose I get
sentenced to 10 years in jail by a robo
judge in the future that's a and I ask
why and it says I was trained by 14
gigabytes of data and this is my
decision and it is final it wouldn't
feel so great right and people have
already found that you can resort called
adversarial training fool vision systems
into thinking that a school bus is a
camel or the fool vision system for a
car into thinking that pedestrians are
just not there and before we put AI in
charge of ever more of our
infrastructure and so on we have to make
sure that we understand it well enough
that that these sort of things can't
happen so this is I think a key text
challenge my group at MIT is actually
working very hard on what I like to call
intelligible intelligence well you make
things that aren't just capable but well
you can actually understand what they're
doing only then do I feel we'll be able
to start getting some trust in them
thank you so is there a I that is
helping individuals becoming their best
selves as defined by individuals
streamlined survival health and enough
wealth and so on and then develop
thriving personal growth
very very interesting question
ultimately technology is a tool right
which tends to be a double-edged sword
sometimes people ask me are you for AI
or against AI and I usually reply it by
asking them if they're for fire or
against fire you can and of course you
guys are probably for using fire to keep
your homes warm in the winter and
against using it for arson and with the
tools that AI is given giving us today
you all have an individual choice to how
you want to use it if you want to use it
as in your question to fulfill your
dreams become the person you want to be
or whether you just want to let this
technology own you you know and become
the person who is unable to have a
conversation for five minutes without
interrupting it by checking your phone
right so I encourage you all to think
about this
what sort of person relationship do you
want to have with technology and then
trying to make it so hi thanks for
coming out tonight
I've been dying to ask you a question
about our mathematical universe so I
hope you don't mind it's a slightly off
topic maybe actually what we should do
since there's a long line is if you come
up to me afterwards at the book signing
I love to talk with you about physics
and cosmology okay okay great I'll be
here I know where to find me
I hope I have a shorter question my name
is Doug I'm the head coach of the
Seattle canoe and kayak club I'm also
the annoying guy who yelled go eh I go
when I was inspired by the obvious
learning of the machine under its own
agency I had that same reaction
yesterday when I was teaching some young
people how to race Olympic canoes and
kayaks but what I'm wondering is when
you are out lecturing and working with
groups of people and individuals do we
really understand the difference between
machine learning on its own and
programmed computers or do we really get
it when we when we see that example of
the videogame while you're saying here's
200 lessons here's 400 lessons here's
600 lessons
and the Machine got it on its own it
wasn't programmed to do that do we
understand what we're getting into
what would you say no yeah I think so
I'm glad you you you you really
responded to seen that video just drive
home the fact that this is real you know
machines can can learn and that's
precisely the reason why it's not at all
science fiction to imagine that machines
can get way way better than us people
can also get better than their teachers
right our children often get much better
at odd things then we are why is it
because they ultimately start learning
for themselves right and the this poses
additional challenges to create a good
futures I mentioned earlier right it's
this technical challenge it's okay
instead that we really need to make sure
if we have really smart machines to
align their goals with ours how do we
accomplish that well first we need to
make them understand our goals and then
adopt um and then retain them as they
get smaller and if you tell your future
self driving car to take you to SeaTac
Airport as fast as possible and you get
there covered in vomit and chased by
helicopters new you like no no that's
not what I asked for and it that
response that's exactly what you asked
for right then you have appreciated her
hardest to get a machine first of all to
really understand your goals right and
raise your hand if you have children so
do you know how big the difference is
between having your children understand
what you want and actually adopting
those goals and doing what you want
right that can be very challenging with
the machines also we have with children
fortunately they they smarten up rather
slowly when they're six months old
you can't explain your high level goals
because it's not smart enough to get it
yet but they spend a lot of time in this
age where they're roughly as smart as
you are
and also I still might apply above those
so you can hope to instill your goals in
them but if a machine smart ins up much
much faster it might blow through that
magic window where it'll adopt you we
can persuade that the doctor goes
too fast that's a challenge and third
how do you make sure machines retain
their goals suppose we succeed in in
creating friendly AI that has is really
sick feeling has this goal of helping
humanity flourish but and then it gets
much smarter after that you know my kids
were really really excited about playing
with Legos when they were little now
that they're teenagers you know not so
much and we want to make sure that these
machines that don't eventually get as
bored with this childhood goal of being
nice to humanity as my kids are you know
with Legos so there are a lot of
challenges we have there and when I talk
about it I safe to research it
absolutely involves also solving these
problems that we they might take decades
solve so we should start the research
now not the night before someone
switches on a super intelligence
questions about that survey response
that you recorded about whether people
wanted their AIS and robots to be
conscious or to be zombies so something
I haven't been able to sort out as you
know once we get to the point where a
eyes are good enough to pass the Turing
test with flying colors how will we know
whether they're conscious or whether
they're zombies oh that's a wonderful
wonderful question the whole eighth
chapter of my book is actually about
consciousness which tells you how
fascinated I am about this question so
let me tell you first of all that a lot
of very well-known scientists and
thinkers think you all talk of
consciousness is just unscientific BS
Daniel Dennett for example it says
consciousness is illusion if you go look
up the Macmillan dictionary of
psychology here's what it says nothing
worth reading has ever has been written
on it right but I respectfully disagree
I feel that the intelligence is not
logically the same thing as
consciousness what which which I simply
define as a subjective experience right
so we are taking in information from our
senses and then
and somehow and then sending out
commands through our actuators or
muscles right and David Chalmers the
famous philosopher likes to call all of
these the easy problems there my mind
really problems of intelligence and you
can convince yourself that you've solved
them if you can build machines they can
do all those things too right whereas if
you're driving a car you're experiencing
colors and sounds and vibrations and
emotions does a self-driving car
experience anything does it feel like
anything to be a self-driving car that's
what he calls the hard problem and I
feel this is not BS I feel this is a
real scientific question that we that we
should ask him and just in case but
there probably are a number of you in
the audience who sort of defaulted to
figuring this is just pseudo scientific
BS let me try to persuade you that this
is actually a legit science question
that we don't have the answer to and we
should be humble about that look at this
I'm showing you here color of wavelength
four and a fifth I'm showing you light
the wavelength 450 nanometers and 650
nanometers and that's the question that
we can't answer with science today is
why do you experience it like this not
like this
why do you experience it like this and
not like this
well I claim this there is no answer
this says this is not simply answered by
any known any physics that we understand
well today and if you look a little bit
more in detail at neuroscience it just
sharpens the mystery it doesn't dissolve
it we know that when you see light of
450 nanometer wavelength it tends to
activate one of the three kinds of cone
receptors in the back of our eyeball
here in the retina where's the longer
wavelength light tends to activate and
are there one of those three which in
turn activates a bunch of neurons
especially in the visual cortex of your
brain right and we also know that the
experience you have subjectively of
colors actually doesn't even require
light the experience is created back
here we know that because you can
experience colors when you're dreaming
when there is absolutely no
and nothing going on in your retina
right so I think personally the
consciousness is the way information
feels when being processed in certain
complex ways and I think we scientists
should be honest about the fact that we
still really don't know what those
complex ways are maybe one day we'll
discover some principles or some
equations that the information
processing needs this obey to be
conscious and if we can do that that
I'll be wonderful
first of all emergency room doctors
would love to have a consciousness
scanner where they can figure out
whether this unresponsive patient who
just came in is actually conscious with
locked-in syndrome or whether there's
someone home there and and and and then
we would love to be able to apply if we
have a theory that can predict what's
conscious and what not figure out
whether I'm which machines are conscious
and which aren't and a lot of people
think oh no no that's impossible to test
these kind of things scientifically I
claim it's not I go into this and great
length in the book but in short you know
we I can put you in our magneto
encephalography scanner at MIT right now
and with six hundred and three hundred
and six superconducting magnet OMERS
measure the magnetic field around your
head and predict with maybe eighty
percent accuracy which of 50 different
things you're thinking about so you can
start to read out information in your
own brain into a computer if you have a
theory that predicts which kind of
information processing is conscious and
which isn't
you could program that into a computer
and it should and while you sit there
and you look the computer tells you on
right now you're thinking about a red
apple now yeah I am actually
they repast that test and then maybe it
predicts it right now I see information
and it sees information about your heart
rate in your brain and it predicts
you're aware of that and you're like
nope I was not conscious of that that
was unconscious information II Theory
ruled out goes in the garbage bin of
history that meant there was a
scientific theory right so I'm actually
hopeful that we can do science with this
with this kind of experiments and if we
can never find
that tells you what kind of information
processing is conscious it's really
really important if if you upload
yourself into a robot into some sort of
more long-lasting body in the future
that looks exactly like you and talks
like you and then you feel I don't mind
now my biological body dying because I'm
like living on wouldn't it be a bummer
if it turned out that that thing was
just a zombie it didn't have any
experience at all
then you would have just subjectively
died right and the biggest bummer of all
would be if if in the future life
spreads throughout the cosmos and does
all these wonderful things and we feel
like as a civilization we're proud
parents of these future life-forms doing
all these great things what if it turns
out the zombies not conscious and the
whole thing is just a play four empty
benches and our universities became a
giant waste of space that would be a big
bummer so I think this is a fascinating
question I think it's a scientific
question and I think we should be we
shouldn't be we should try this a crack
all right and like one main goal and
then a bunch of others came from it or a
sort of ordered list of goals but when I
think of human goals they seem to be
multiple and competing often and I was
wondering if you think that there's
really a sort of you know this hierarchy
for machines to behave ethically or if
there's the oftentimes with humans the
competition between their goals is kind
of what makes them ethical yeah that's a
really great question the whole seventh
chapter and the book is all about goals
because I'm fascinated by it by your
question and I talked a lot there about
the origin of human goals of course
originally very simple life the only
goal that the DNA had was for it
reproduce and then the genes to
Darwinian evolution gave us a brain
helped them reproduce better and so on
and it and it turned out there was this
very inconvenient every time you have to
decide whether to do this or that if you
have to like logically think back if I
eat this Apple instead of this peanut
here which is gonna give me more
offspring you know way too complicated
so it gave us this here is the rules of
thumb instead like if if you feel hunger
then eat if you feel thirst then drink
and and so on and that worked pretty
well but what that also means is that we
don't actually we can't be humans can't
actually be described by any simple
mathematical goal function just like
that you're saying here which means when
you said when we when we say things like
make sure the AI has our goals it's a
tough challenge to even make full sense
of what exactly you mean by that we also
know how hard it is ourselves that we
can style sometimes if you look at our
friends come ask us for advice about
life sometimes the goals they say they
have aren't really born out exactly in
the choices they make right so a lot of
fascinating work to be done on goals
alright but those of you who didn't get
to ask now feel free to come chat with
me afterwards at the book signing table
hi how would you communicate
goals to machine if they can't
understand language or words or the
meaning behind any words really good
questions so first of all I think in the
more distant future we will want really
if we really want smart machines to
understand our goals we will want them
to be able to talk to us in our language
so we can communicate with them easily I
think I think even in the shorter term
if you have a if a retired person has a
helper robot at home you know we can't
expect them to become masters of cara's
Python neural network programming or
whatever right it's with it's actually
this is a very interesting research
field called inverse reinforcement
learning where the idea is machine tries
from just looking at our behavior
to infer where we really care about
right like if someone is holding their
baby and they need to drop their coffee
cup and the Machine notices that they
let the coffee coffee they only have
didn't have enough hands to catch both
that let the coffee cup fall not the
baby that there's a lesson there right
and we humans when we're babies we
actually learn a lot more about our
parents values and goals from what they
do than what they say so that there's an
interesting science there but finally I
just want to end on an optimistic note
here that even long before we can crack
these problems of what they even mean by
human goals and how machines learn very
sophisticated goals I don't think we
should let perfect be the enemy of the
good because there are a lot of goals
but pretty much all humans agree on that
are actually very interests the program
into machines which we haven't done yet
like most industrial accidents are
caused by machines industrial robots not
even understanding the most rudimentary
human values like the human is not an
auto part and should not be smushed into
the sockets or something right and
Boeing here does not under any
circumstances want their airplanes to be
flown into fixed objects right yet
that's exactly what people were able to
do on September 11 and much more
recently and he has Lubitz depressed
German pilot of the Germanwings airline
was able to fly his passenger jet with
hundreds of people into the Alps and how
did he do it he asked the computer to do
it it's like okay he just told the
autopilot to go down to an altitude of
100 meters even though the computer had
within it the whole map of the Alps GP
and it had GPS
everything so so these sort of
kindergarten ethics that there's
universally near universal agreement on
we should absolutely start programming
the entire machines already whenever we
can so the machine start to at least
understand these basic goals that will
already save lives and and be a good
stepping stone towards the more
challenging go alignment questions that
lie beyond so thank you so much again
for all the wonderful question
thank you so much max tegmark live 3.0
is available in the lobby right there
from the wonderful ADA's technical books</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>