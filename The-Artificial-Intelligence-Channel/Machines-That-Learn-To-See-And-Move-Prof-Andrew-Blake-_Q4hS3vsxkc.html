<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machines That Learn To See And Move - Prof. Andrew Blake | Coder Coacher - Coaching Coders</title><meta content="Machines That Learn To See And Move - Prof. Andrew Blake - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machines That Learn To See And Move - Prof. Andrew Blake</b></h2><h5 class="post__date">2017-10-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_Q4hS3vsxkc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">madam chair deputy vice-chancellor
ladies and gentlemen it gives me great
pleasure to welcome you to the first IMI
public lecture of this academic year and
in particular to thank the Bath digital
Festival for helping us promote the
events and to link the cities and the
universities activities in provoking
discussion entertainment and education
on all things digital I'm going to start
with a couple of housekeeping
announcements as with other public
lectures we've had this later is being
recorded if anybody objects it's
possible to sit towards the back and the
sides of the lecture stairs that way you
will not be visible to the camera we'll
have time for a few questions at the end
of the presentation and there will be a
roving microphone please wait until it
reaches you so that your question can be
recorded as well as the answer thank you
the Q&amp;amp;A will also obviously be recorded
could you please take a moment to check
the settings on your mobile phone and
there are emergency exits located at the
back of the auditorium and at the front
here with the sense
Andrew Blake is currently research
director at the alan turing institute
based at the british library next to
some Pancras station in central London
before joining the alan turing institute
in 2015 Andrew was the laboratory
director of Microsoft's research lab in
Cambridge where he worked since joining
them in 1999 to establish their computer
vision group his undergraduate and
graduate studies were in Cambridge and
Edinburgh and moved between mathematics
electrical engineering and the venn
called departments of machine
intelligence and perception at Edinburgh
he subsequently left Edinburgh for
Oxford where he spent around 12 years so
overall he has personal experience of
working in three of the five university
partners who established the alan turing
institute i'm sure that experience has
come in useful
andrew was elected a fellow of both the
royal academy of engineering and of the
royal society he's received numerous
medals and prizes together with honorary
degrees from edinburgh and sheffield in
2011 together with other colleagues at
microsoft research
he received the McRoberts award from the
royal academy of engineering for their
machine learning contributions to
microsoft's kinect device for the Xbox
console and I should point out
immediately that other motion
controllers are also available Andrew
we're delighted that you've been able to
spend today with us in bath we've shown
you a little of our research capability
across those various areas I mentioned
and for your lecture this evening which
has the title machines that learn to see
and move thank you very much
Thank You Jonathan I have had indeed had
a great afternoon in the university and
seen some fascinating stuff racing cars
and other more serious kinds of cars and
motion capture motion capture studio
some wonderful stuff going on here so
yes I want to talk about machines that
learn to see I've been talking for a
while about machines to learn to see but
I'm a recent thing for me is making them
move as well and I want to talk a little
bit about about that the learning is
important it's turned out to be the case
that that's the way to make artificial
intelligence systems work when the
artificial intelligence began not
everybody believed that and so tempting
to think that you could program
artificial intelligence systems but I
think now we know that that is just too
hard
I suppose one thing to celebrate is the
machine the machine vision systems do
actually work so this has been a big
change in the time that I've been
studying vision and that's partly why I
went to Microsoft because what we were
doing during my PhD and afterwards was
definitely in the laboratory but then a
kind of magical moment came when the
ideas were good enough from the
computing was powerful enough that it
seemed that vision far from being
something you could only do in the
laboratory in a kind of speculative way
was something that you could install in
a machine and have it work in real time
and the in the mid-90s when this began
to happen was a very exciting time when
machines with cameras in them actually
began to react to their environments and
that was it's hard to overstate just how
exciting it was to have machines that
were reactive so now we are very used to
a lot of the things that machine vision
can do at the top left even from the
early 2000s our cameras started
understanding about faces and would help
you frame up a shot and the technology
transitioned into cameras very quickly
in the top in the in the middle 3d
cameras that the Kinect that Jonathan
was referring to also now a kind of
commodity that you can have with your
your gaming system or in the studio for
animation top right in medical imaging
turned out to be has turned out to be a
big application area for machine vision
what you see there is the images
captured of a particular patient on two
subsequent occasions one occasion on the
left both occasions together on the
right trouble is when you go back the
second time you've moved you're not
going to sit in exactly the same place
in any way your insides are quite
pliable and won't lie down in exactly
the same position as they were the first
time you went to the hospital and so
there's some non-trivial vision to do to
kind of align the two views in a way
that you can compare before and after
which is what the medics often want to
do bottom-left something which you
probably wouldn't think of as being
machine vision at all but actually this
is panorama stitching and Matthew Brown
who used to be on the faculty here was
one of the big innovators in this area
getting machine vision I don't if you
remember all the cameras that did
panorama stitching you have to kind of
traverse the scene very carefully in a
kind of orderly fashion now it doesn't
matter you can just take a bunch of
pictures and the the vision system will
work out the jigsaw and piece the piece
the the parts back together and give you
a an image and lastly automatic systems
this is some Mercedes advertising
literature that will assist your car and
we will come back to that a little bit
later well when people started thinking
about how you might division back in 66
MIT had a student project the sole
vision they could have done a lot of
other areas of AI and they thought that
perhaps it might take the whole of July
in 1966 for some students to solve the
problem of machine vision at least the
the simpler parts of it this is an
extract from an MIT a memo it's on the
web now so they can't take it away but
there were some harder aspects that they
thought might well spill into August
you know the objects with writing on and
more awkward shapes and so on and turns
out that you know 50 odd years later we
they're working pretty hard on this
although now a lot has been done why is
vision quite so hard well think about a
computer program that might want to work
out what your hand is doing something
I've worked on a lot what you want is
something like what you see on the right
the outline of the hand and surely the
computer is smart enough to work out
where the hand is it's not absolutely
obvious to us where the silhouette of
the hand is but if you do an honest job
of signal processing what you get is the
output on the left this is not sort of
being doing things deliberately badly
this is actually trying quite hard to
point up what are the high contrast
parts of the scene and that on the left
is what the camera and the computer
together honestly see and when you look
at it you cannot really object I mean
the stuff that's been picked up is all
stuff that's there there's the wristband
on the watch there's clutter on the
table there are shadows on the hand
objectively those things are all there
it's just that our amazing attentional
mechanisms in this fantastic
supercomputer that we carry around in
our crania effortlessly eliminates the
distractions and repairs any
inadequacies so what you see on the left
is just how ambiguous the scene is there
are lots of elements that the computer
might have to consider as candidates for
being the outline of the of the hand
even though you and I know what the
answer really is and this is rather
characteristic of vision and indeed
natural signals that that the perception
our perceptual brains can process here's
a picture from kind of 101 psychology
textbook some of you will have seen it
before and others that others of you
won't some of you will immediately see
what it is and others will be wondering
why is there a lot of scrambled egg on
the screen so I mean now don't say what
it is yet because that will cure the
others so how many people can see what
it is so I wouldn't embarrass the ones
who can't by saying you can't see
because that's the complement of that
set but they're there in the audience
now for some of you who are suffering
because you can't see what it is I could
say dog
and that might now is there anybody who
can now see what it is who couldn't see
it before it's not too embarrassing to
admit to that yes the wonder of those
Dalmatian maybe it could be a Dalmatian
dog does that help anybody else has not
got there yet and well we could it could
help even more and let's see there's the
nose of the Dalmatian dog is about there
and there's the ear and there's the
front leg as the back leg and has that
helps anybody else any other perceivers
now then there's another one that I've
got here which you probably won't have
seen know it's some of you will will be
suffering again not able to see what
this is scrambled egg on a blue
background but some of you probably can
see so we could allow the ones who can
see to say what it is what is it it's a
face of Jesus yes so the evangelist
evangelistic okay so now that probably
helped to a bunch of others of you to
see it that couldn't see it before
anybody been helped now because being
told what it is yes quite a few
and he's still suffering that they can't
see it it's annoying isn't it it's very
well I can give you one more cue which
is sometimes if you make the picture a
bit smaller somehow it's a bit easier to
see so here's the same picture of it's
smaller
did that help anybody that didn't have
it before yes there's some believers no
well anyway there's the eye if there's
another eye this is sort of beard round
here
this is there might be one or two of you
still can't do there's a lot of
ambiguity in this scene isn't there and
yet somehow our brains can make sense of
the ambiguity and piece together a whole
percent and what about this video what's
happening in the video can anybody see
hasn't moved yet from moving a minute
does it look like anything person what a
date was the person doing all the time
let's let's see what they're up to
is taking a call what's the phone down
listen to his music it's not a very good
video is it it's rather rather blurry
you might be wondering why I didn't
bring a better one well I've got good
news I did bring the better one that one
was deliberately messed up by blurring
so let's there's the better one let's
see what he's up to he's having a chat
on his shoe and cans the keyboard I
think is real but the mouse is a bit
suspect actually the computer doesn't
look too good either have you noticed
birds have been but what's the point the
point is that and the brain seems to be
a kind of hypothesis processing machine
if I give you very impoverished
information like in the first video your
brain is kind of working overtime so
sort of work out what could be going on
there you come up with a hypotheses
which are pretty reasonable given you
know your background knowledge and so on
but then when higher quality information
comes in your brain is willing to revise
those hypotheses and you know admit that
it's wrong and come up with a more
refined explanation of the see so there
seems to be some interesting clues going
on here about how vision systems need to
work seems like they need to be able to
deal with ambiguity and so that rather
points to the use of probabilities for
processing and so we should expect
machines that somehow our processing
probabilities and indeed Richard Gregory
who is a psychology professor at the
University not far from here but does
one speak at the Bristol University in
hushed ends or is it all very loving and
friendly bit of rivalry yes I can well
he was at that University and so he said
that he was very keen on this idea that
what the brain is doing is generating
hypotheses and almost like having a
scientist in your head
dreaming up hypotheses and testing those
hypotheses against the evidence that's
out in the world and he was very fond of
illusions if any of you are bold enough
to make the pilgrimage to Bristol you
may may see this famous cafe wall and
you know the one I mean at the bottom of
some Michaels Hill in Bristol where it's
actually honest-to-goodness made of
square tiles but depending how far you
are from the screen you may not be
seeing them assume they made the wedge
shape to you but they really aren't and
so there's some some illusion going on
here and here's another one that Richard
Gregory was very fond of the rotating
face and you know you and I know with
the kind of cognitive bits of our brains
that the sort of inside of that mask
must be concave but it's impossible to
tell your brain that it prefers to see
it as convex and going round the wrong
way which is pretty crazy and extreme
hypotheses but just to emphasize this
idea of the brain as a powerful
hypothesis processing machine so we have
some clues as to how vision surely is
going to have to work by computing with
probabilities to deal with the ambiguity
and also as I said at the beginning not
by programming instructions honestly
people tried hard at that you'd think
you could think of a suitable
programming language to describe objects
and then you know a cup you know surely
some kind of a C ad language ought to do
the job you know all those kinds of
things you can say them in those things
in a C ad language you ought to be able
to say that and then get a program that
will will recognize the cup when it sees
it but that really doesn't seem to work
and it's not for one to try so a much
better approach seems to be more like
what happens to little humans and little
animals to expose them to a lot of
examples of things and right because the
program's must be involved but the
program's need to have a lot of
flexibility in them so that they can
adapt to particular object so
programming with probabilities and by
learning but this comes in two different
style
and this is something which is
absolutely takes us to the kind of
forefront of what is happening in
artificial intelligence at the moment so
the two styles are empirical detectors
and generative models so the empirical
detector you some of you of course know
a lot about this and some of you may not
be so familiar with it assemblies of
neural networks we all come to this in a
moment but the main thing is that these
neural networks are black boxes that
receive inputs do some funny things in
multiple stages inside and produce an
interpretation at the at the end and
look inside the black box what I want to
say at this point is not very
informative you can sort of put your
oscilloscope probe at a halfway stage in
the black box and you'll have a hard job
understanding what a particular
component at that halfway stage is do
probably rather like inserting electrode
into the brain and certainly like
putting a probe on a individual
transistor somewhere in the middle of
your computer and these empirical
detectors are typically trained from
large data sets exploiting the ability
of neural networks to learn patterns
from from large data sets and of course
it's only the the fact that now we have
the Internet has brought us such large
data set so that with interpretations
attached labels as machine learning
people call them that's made it possible
to do the learning but the alternative
is something called the generative model
which has also been both of these styles
have been popular through the kind of
60-year history of artificial
intelligence and I've tended to kind of
alternate with one another at the moment
it seems to be the era of the empirical
detector and the deep neural network but
at other in other eras and what I'm
wanting to say in this lecture is that
we haven't seen the last of them is the
generative model where the data is
explained in terms of a hypothesis in
more of a kind of step by step we're
almost like a model in physics and if
this is not if you're not getting a very
clear
mentor image from what I'm saying the
moment we all have examples that
hopefully will make this clearer but the
point about generative models is that
putting your probe into the middle of
the analysis process is much more
meaningful than it is for black boxes so
that's sort of in some ways a plus
because for many reasons it's good to be
able to understand what the intermediate
stages of your analytical process are so
first of all that we'll talk a little
bit about the first of those paradigms
black box detection and then come back
to generative modeling so now I don't
know some people in this audience do
maths and others do other things I've
got them a little bit of maths in the in
the slide deck but if you don't do maths
that's the story but for those who do
the black box detector is taking in an
image let's call it I and has to infer a
state let's call it X so the image is
simply a pile of pixels with their
colors maybe a million pixels or ten
million pixels with their colors the
state X could be something about the
identity of the thing that's in the in
the image is it a person is it a dog is
it a tree and maybe something about
where it is in the image so that's all
gets piled into X but because we've said
we want to be able to deal with
ambiguity then rather than having a kind
of deterministic machine that takes in
an eye and spits out an X what would be
more useful would be to have a
probability distribution P of X given AI
that tells you all about the the likely
X is given a particular eye that has
been observed so the blackbox detector
and deep neural networks in particular
which is the kind of latest
instantiation of black smoke detectors
learn this conditional probability
machine px given AI given a lot of
examples of the pattern that is to be
learned so an example of that would be
the technology that went into those
cameras that we talked about before and
so this is a paper from the turn of the
millennium
from Mitsubishi research where they
first surprised people by showing that
using a slightly older black box
learning technology with a lot of
examples of faces and examples of non
faces actually the non faces especially
chosen to be kind of near miss non faces
if you looked quickly you might think it
was a face but when you look harder you
think I know that's sort of a bit of a
gargoyle or a bit of thumb you know
stonework or a cloud that invokes a face
the near misses are particularly useful
and by comparing them the hits and the
near misses and picking up some features
which are really patterns that may occur
at various points in those in those
examples one can build detectors that
are very efficient so here you see the
result of that third detector learnt
from thousands of faces and thousands of
non faces that is now very reliable at
detecting the faces and then that
technology very rapidly found its way
into the cameras amazingly rapid
transfer of technology very rarely
happens quite as fast as this especially
given that the computers inside your
cameras are very weak feeble little
things that are nothing like the power
of watts in your mobile phone and yet
the algorithm was so good that the
inventors were able to put it in cameras
within a couple of years well a more
modern version of the blackbox paradigm
the so-called image net challenge which
is people compete right now to solve
this challenge where you have a million
images and the there are a thousand each
of a thousand categories and that
wordnet thing there tells you what the
categories are and so the job of the
neural network is to recognize the
category and what was the neuron that
will it look like well here's a sort of
depiction of it from the first paper
that was published only 2012 it's not so
long ago that was able to make a dent in
the performance of the image net
challenge so what do you have as I said
earlier is that like a a sequence of
stages so are the
left-hand-side the images coming in all
of the pixels pouring into this kind of
plate here and through many many
connections they pass through secret
subsequent stages through this cascade
and the stages are a mixture of an
alternating of linear and nonlinear
stages and the linear stages are adding
things together the nonlinear stages are
doing something more like making
decisions yes or no and then at the far
right hand side what pops out is a
thousand wires a box with a thousand
wires sticking out of it and the signal
that lights up coming out of that box is
the identified scene so when this
technology was introduced it immediately
began to make an impact so what you see
here is the couple of years before the
technology came in when people were
using what was the what were the best
methods of it at the time more hand
constructed solutions to try and solve
this challenge you see that the error
rates were up near 30 percent and you
know between 2010 2011 with some extreme
engineering the error rates was pulled
down by a couple of percent but you know
not a big impact
then the neural network solution of the
deep neural network came in and
immediately you see an improvement even
with that first paper in 2012 that got
people excited now everybody was
competing on this challenge using deep
neural networks because they realized
this was the way of the future and you
see by 2014 that the error rate was down
almost by a factor of three human
performance however is a little better
than that on this particular dataset not
so much better but a little better I
mean it's quite a hard puzzle and you
know what I'm going to do next
the next two years actually the humans
have been beaten on this challenge so
it's a bit of an exaggeration to say
machines now see better than humans so
that's not quite true but on this
particular test which is quite a hard
test the machines are doing better by 30
of deep neural networks and learning on
a large scale what
point we might think about you know what
is vision for it's not just for sitting
there appreciating the world it's to do
stuff and Daniel Wolpert who likes to
show this slide would say that actually
vision is for one thing and that is for
moving the reason that we have vision is
so that we can move around the world if
we didn't move we wouldn't need it and
for him the proof is the humble sea
squirt which has a bit of a brain not a
lot but it has a bit it moves around the
world and which is under the ocean and
its sole purpose in in life up to a
certain point is to find a good
attachment spot on a rock and that
attachment spot is going to be its home
for the rest of its life so it finds the
attachment decides that it likes it
locks in there and then guess what it
eats its own brain because it's not
going to need it again it's something
it's done all the moving that it needs
to do so now consumes its own brain and
down your wall it likes to say that this
is a little bit like what happens when
professors get tenure in the university
I'm very interested in movement too
because as well as being part of the
Alan Turing Institute recently I've
joined a start-up 5ai
the bottom right there which is join
some of the other outfits around the
country which are working on autonomous
vehicles so this is really why I've got
so interested in not just machines that
see but machines that see and move I'm
very interested in what it's going to
take to do that and one of the things
we're going to have to do is recognize
pedestrians and well you know there are
various levels of which you might do
this so here is putting a bounding box
around a pedestrian which is useful and
you can do that very well with one of
these black box detectors but if you
want to know a little bit more about the
pedestrian like what is doing with its
limbs and maybe which way it's looking
that's kind of important if you're
driving a car then we might have to work
a little bit harder
so at this point I want to think about a
slightly different kind of model the
generative model we thought about black
box detectors the problem machine that
gives you probability of the state given
the the image input and now let's think
a bit about generative models slow
generative models strangely go backwards
so you start with the hypothesis and
then you ask the question if that
hypothesis were true let's say if there
was a person in the middle of the scene
what would you see so the mother the
generative model is a problematic
conditional probability distribution
because we're still wanting to deal with
ambiguity for what might the image look
like given a particular hypothetical
object that is in that image and then
having got that somehow where haven't
really said hair yet having got that
generative model this likelihood
function here then one can use Bayes
rule to get back to the thing that the
black box gave us which is the reverse
conditional probability of what the
interpretation might be given the image
and then finally given that machine P of
X given R you could estimate various
things about the state for example you
might like to know what is the most
likely state given all given all of the
possible interpretations X which one is
the most likely and so that's what that
last thing is showing and in a very
special case I'm not so special quite a
commonly occurring the special case
where one thinks of the image as being
generated as some function of the state
plus added noise then the the
interpretation X the most likely
interpretation is got by effectively by
simulation so this f of X you can think
of as being a simulation of the image
what it ought to look like if that
hypothesis were true and then this
expression here is the difference
between the image you actually saw and
the simulation so now what
your estimation machine will try to do
is to minimize that difference in order
to make the most plausible
interpretation so let me be a little bit
more concrete about this supposing we
wanted to build as we did in Microsoft
software which would tell you which is
the object in a scene and which is the
background now this is kind of
interesting you might think why didn't
we make a blackbox detector to do this
but one of the reasons we can't easily
make a blackbox detector for this task
of putting out the larvae from the scene
is this so little data you don't have a
whole Internet full of images you just
got that image so then from nothing else
so you've got to be incredibly efficient
about learning the the foreground from
the sea so to do this it turns out that
a blackbox detector thing isn't going to
work but the generative model might so
let me tell you now what the generative
model might look like so supposing
supposing we're in the foreground then
the generative model is a kind of color
palette of the typical colors that you
might see in the foreground but whereas
if you're in the background you have a
different color palette and each of
these color palettes are represented
probabilistically as the the range of
likely colors given foreground or given
the background here's a picture of those
models and what you see here is all the
pixels in the image projected into RGB
color space you can see it's all rather
mixed together because that llama was a
little bit camouflaged in its background
they were all kind of cocky colors with
both the llama and match up each shoe in
the background and now if we explicitly
try to separate those foreground and
background colors you see what they do
separate a bit but it's some they're
still very intertwined and so the
challenge for us in Microsoft was to
build software that would separate those
things and we we did that and I think I
can show it to you that's
this is um okay you might think this is
some experimental version of office but
no I'm happy to tell you this is just
office you can tell it's the real thing
because it's complaining I don't have a
license and okay my mouse is frozen up
for a moment which is annoying but you
can see it's how'd it go at doing this
no I can't be my mouse pointer the
software is incredibly clever but the
mouse is dumb as anything okay it's
moving again good and well okay that's
not too bad I could say well no idea I
did want the tail so I'll go and get the
tail with the by painting I have painted
in the whole title I just did a kind of
bra rough brushstroke and then the the
inference system that's doing all of
that Bayes rule stuff now actually that
bit shouldn't be there sighs good piece
of a fish that's got stuck to themselves
also I cross that one out and now say
okay that's quite good like that and now
I can move that around and the text
you've got this in your copy of office I
I strongly recommend you play with it
when you get back so that is using
generative modeling and that of course
is just on a static image some
researchers at Oxford a little while
after this used the same kind of
modeling of the the the color palette to
give them the foreground the cutter Pat
have given the background that those
pallets by the way have to be learned on
the fly nobody tells the system what
this panel saw it has to kind of work
that out for themself and they use the
same thing to make a very agile help
movement tracker this is the same thing
working in video you see she's really
throwing herself about of it and
actually what's happening really is that
she's staying still in the room is
twirling about so as you see on the left
he's pretty impressive stuff so these
models can be very powerful and
have this advantage that they didn't
need big data to learn them which is
really a very important point so now you
might be thinking well why don't we just
do generative modeling but when it came
to doing the machine learning for the
Kinect camera we did start with
generative models in fact the engineers
already had a rather good generative
model they built and so we we tested it
out and what do you think happened so
here's a trap in front of the 3d camera
are the different colors are the
different depths the camera is picking
up and the test chap is rushing is
moving around and you see the skeleton
there I got tangled for a moment but
most of the time it's pretty good a
little bit more movement now they're
jumping up and down no good and fell off
and oh dear doesn't seem to be
recovering and doesn't like it's going
to recover so that's not so good they
there's something good happening there
this analysis by synthesis fitting the
generative model but it's not good
enough and so you know that's all very
well in the laboratory or even on the
podium at a conference you can do a demo
and the inventor is there driving the
demo and those all the floors in the in
the system but it's not to sell this to
some gamer and they're going to use it
where any supervision in their living
rooms still got to work for example for
this system to get it started
you'd have to come into the living room
like this and stand in a very particular
spot to start the thing that's not going
to fly as a commercial product and then
you know you'd have to be quite careful
not to throw yourself about too much
well the whole point of the export is to
throw yourself about too much so that
wouldn't work either so they cut a long
story short we ended up bolting a
blackbox detector which at that point
was this based on decision forests
because deep neural networks haven't
quite been invented or at least
popularized and we actually were the
first I think to do the trick which has
now become very popular with deep neural
networks of using the GPU to do the
learning and we did that very early on
and until we did that we couldn't
actually do it because the learning
cycle took a week and the engineers over
in Microsoft head office wouldn't wait a
week they'd like to build their software
everyday so we had to make a system
where you
learn within a day and so this is what
we got we learnt this from a million
examples of different people at
different sizes and shapes doing
different things different
configurations and most of those images
were simulators so again we were rather
early in the game they're learning from
simulations and particularly satisfying
that when we started doing it all the
machine learning people told us it
wouldn't work they say no you try to
simulate perceptual reality you won't
get close enough and it won't work and
anyway we didn't have any other way of
doing it so we swallowed hard and tried
it and it did work for interesting
reasons which I could go into if I had
time nowadays my see if I can make this
one go this is what they're doing now
are the same kinds of generative models
and you see how much detail they've got
into this the hand is even harder to do
than the body because there are so many
ways of obscuring parts of the hand with
the fingers wrapping around the back and
the precision that you need and the
agility is all rather extreme and these
kinds of methods I wouldn't be at all
surprised if they're being used in the
hololens but i couldn't possibly comment
when the hololens as you may know is a
mixed reality device where you
manipulate things in the world with your
own hands but the things are not real
the things are artificial for example
you can walk up to a wall and the wall
has turned itself into a display screen
but you can use your real finger to
write on the virtual screen and to do
all this kind of thing the perception
systems have to be very accurate well I
just want to end what time should I stop
Jonathan
okay I just want to end by saying a
little bit about why I think in the
future what's going to happen is you
know we're in the era of blackbox
detection we've had five years of deep
neural networks incredible advances have
come as a result of that not just in
vision but also the reason we have such
good speech on our camera on our phones
now speech interpretation is because a
deep neural networks but I think in the
next five years we're going to see
generative models come back rather
strongly and there are a number of
reasons for that and I just want to say
a little bit about that to end with so
one problem is that there's only so much
data in the world and in some problem
for some problems there's lots of label
data out there for example images of
your holiday on the internet the
internet tells you often worthy if the
images are so there's a lot of label
data but other things for example
learning about you as an individual
course you know Amazon and Netflix they
do try to learn about your tastes as an
individual I don't think they're very
good is it I don't know am i weird is it
because I like such weird mixtures of
movies I don't know but certainly
there's a premium on being able to learn
about individuals as fast as possible
for example to learn about how you like
to travel to work you know by by car by
car when it's raining and when it's
sunny what routes you like to go you
really like your phone and the systems
behind it to pick that up very quickly
and not to have to to execute it up
hundreds of times before it gets the
idea so simulating data as a way of
generating more of this precious data is
going to be important data fusion very
important so in this business of
autonomous cars one of the striking
things about using machine learning to
run autonomous cars is that typical
machine learning programs are right
maybe 99 times out of a hundred actually
99 times out 100 is very good for most
machine learning programs 95 times out
of 100 is a bit more realistic but your
car if you're driving in it has got to
be right a lot more than 99 times out of
not just to 9-0 low reliability probably
more like seven nines so you know cars
typically have a major accident about
every million miles there's something
more like seven nights how on earth are
we going to use two nines reliable
machine learning to get seven nines
reliable cars and to be confident that
we've done that then there's the thing
about making detailed interpretations I
already mentioned this with the that the
people on the road and being able to
understand what they're doing and lastly
there's the idea of online simulation
that is the way that that things are
computed in your head that way you're
able to reconstruct the scene so I just
want to show you one or two of these
things to end with so the first thing I
mentioned was the ability to simulate
data if you're training an aircraft
pilot you could train them on real
flights but real flights are pretty
boring actually far more productive to
put them in a simulator where they come
out after a couple of hours dripping
with sweat because every damn thing that
could go wrong on the simulator on the
fly it did and I think we're going to
need to do the same kind of thing with
autonomous vehicles and indeed Google
are doing this so here you see anomalies
of various kinds the road is wet the
road is snowy the right lines are
obscured these are all the difficult
cases we might have to wait several
winters now with global warming to get
any snow on the road but with simulation
we can do it to order and other things
can go wrong what's gonna happen here
something bad I feel it in my bones yes
the cycle is coming down the wrong side
of the road and you know that may be a
relatively rare event but we want to be
very good at handling it and so we can
make it happen more often in a
simulation and the right lines again
obscured ok another bad thing box you
know you can wait a long time for that
to happen in real life but on the
simulation we can we can do that to
order and my second reason
was for data fusion and I have an
interesting experience here with
Mercedes I went on their test track back
in 2005 the Myra test track neared
Nuneaton
and they had a Mercedes car at the time
instrumented with computer vision trying
to detect pedestrians and let's see how
it does
there we go you see the the car ground
to a halt and the dummy was saved
actually when I was there they did it
with real live research assistants and
the research assistants life was saved
for the 10th time that day but then we
went off round Nuneaton with the same
car but now with the the connection to
the brakes switched off and attached
instead to a buzzer so you can see when
the car would have break we drove ran
Nuneaton or went pretty well but about
every 10 me 2 minutes the buzzer went
off because the car has seen a ghost it
saw a lamppost and thought maybe that
was a human or a shadow on the road well
that was a human doing pretty well by
machine learning standards that was
pretty good it's quite hard for machine
learning to do better than that and so
how are you going to do any better you
clearly can't sell a car that does that
and yet now they do sell a car with
automatic braking in it where the car
will brake that come to a stop much
quicker than the driver could and the
way they did it was to use radar as well
as vision they've done as much as they
could with vision on its own at that
time but radar makes completely
different kinds of mistakes than vision
so this is what people mean by data
fusion you put the two sources of
information together that are somewhat
independent and by combining those
sources of information you can do better
than ever
well perhaps just one last idea well
there's the I must show you this video
there's the idea of being able to read
what people on the road are going to do
I mentioned that earlier you've got a
pedestrian in front of them you
you want some detail for example if the
pedestrian looks at you you might behave
very differently than if the pedestrian
is gaily looking into space and stepping
out off the kerb and so being able to
read people's actions is rather
important and here is a fascinating kind
of envisionment which I got at the
Turing from the chief scientific advisor
for the Department of Transport DFT and
well what he said about it was that he
was very careful not to let any of his
ministers see this video
then if you're excited about so here we
are the I believe generative models are
going to come back we've had fantastic
value out of black box detectors the
deep neural networks but for all of
these reasons I believe we're going to
see a lot more activity from generative
models over the over the next few years
and so the cycle of research continues
we've here are laid out on this timeline
as some of the different technologies
that we've seen over the last few
decades and there's a kind of in
alternation going on between neural
network type architectures and these
generative models and I believe the
generative models will come back and
particularly to help us with this
business of being able to learn from
much more from data which has a very
high value and which is therefore scarce
and you know if you think about how
children learn about objects imagenet
the programs that did image net and
learnt about the identity of different
visual scenes needed a thousand examples
of each scene tell them about something
new an image net asks for a thousand
more examples before it gets the idea
but if you have a little two-year-old
who knows what the car is and it's
seeing its first truck it doesn't need a
thousand trucks to learn what a truck is
probably a couple again D is going to be
enough and at the moment we don't have
those tools in artificial intelligence
to learn that quickly and that
efficiently from very high-value data
and I believe that's where we're going
to see some of the most exciting
advances in the near future thanks very
much
Andrew thank you very much indeed we've
got time for a few questions if you'd
like to pose a question please put your
hand up and the microphone will come to
you so that we can record the question
thank you very much Andrea I was
interested in your discussion in data
fusion and I wondered if the synthesis
of those two was more logical and
logical or or whether was some
combination of the two in there well
because it would have to be a
probabilistic version of logic and I'd
say it's the probability probability
version of logical and because you are
able to if the combination from two
different perceptions the things are
relatively easy and you know statistics
knows what to do if the two observations
are genuinely independent but I think
one of the challenges that we all have
is that very Island and the radar and
cameras are kind of plausibly
independent that sounds like moderately
convincing that they would make very
different kinds of mistakes and
uncorrelated mistakes but what about if
you have video you know if you've got
live live cameras and you got several
shots of one pedestrian you really would
like to be able to do a kind of
synthesis of the of the information for
those successive frames and yet taking
the frame at time 31 seconds and
treating as if it was independent of the
frame at time 30 seconds doesn't seem
reasonable
it's neither completely dependent nor
completely independent and so doing the
fusion under those circumstances is I
think rather challenging
next time I'm just be curious about the
generative model oh yeah I mean
shouldn't the Machine get smarter by
dependent on how much wrong things it
gets so does it get smarter by basically
seeing this information and saying this
is wrong and then it could teach itself
rather than you feeding it much more
information like thousands of data sets
right well I mean of course having
having correct examples and wrong
examples is very important and you
should learn from the both and if you
have a thousand images each of a
thousand scenes you've got a thousand
correct images of you know what is a
fire hydrant let's say but you've got
nine hundred and ninety nine thousand
wrong images in that data set because
that's all the other things so you have
got a plentiful supply of wrong images
and indeed you know learning whether
it's generative or black box games a lot
from that you have a question down at
the frontier thank you we are being told
that it's not gonna be long and maybe we
already do have some models of
driverless cars going around and
obviously your work seems to be very
vitally important in that so how near
are we because I'm quite scared after
watching what you've shown us well I'm
confident that the Department for
Transport nightmare which is an
unofficial nightmare
no no that's kind of a joke but there
are different levels of competence of
driverless cars and these have even been
sort of formally codified so you have
levels one up to five where one is the
more familiar territory of cruise
control
and you know you can have a cruise
control for steering and if you have
steering and acceleration together they
call that level two so still you know
reasonably familiar level three is where
you have a car which is kind of capable
of driving by itself without you doing
anything but it really wants you to pay
attention you're not allowed to look
away you can't sort of just turn around
and read a book so that's level three
Google have said that they're a bit
dubious about level three because they
think people will lose concentration and
then actually level three is more
trouble than it's worth well we'll see
how that pans out level four is where
you have complete autonomy yuka the
driver doesn't have to drive you can
read a book play chess with your fellow
passengers but only in certain areas so
it might be on the motorway yes
absolutely soft walls but on the
motorway the motorway is a well
engineered environment where things are
more predictable there aren't
pedestrians the white lines are kept in
good shape there are kind of you know
cats eyes in profusion and so on it's
it's a structured environment so making
the autonomous driving work there is a
good bet and I think we all have that
you know in the next three four years
that kind of thing but level five is the
top of there at the range completely
autonomous driving where you didn't even
need to learn to drive you didn't pass
the driving test you have no idea how to
drive in fact your seat doesn't even
face forwards it's you know maybe your
level five car is a kind of oblong thing
with a kind of card table in the middle
and you will just sit around the car
table you know that's the long-term
vision now I don't think anybody knows
how long it will be before we get to
that because that really is very
demanding all the levels of safety that
will require so I think what we all have
is level four with the regions of
competence getting bigger and bigger and
the kind of confidence of the the
industry and of the traveling public get
it building up until we get to some
point which I feel it won't be less than
ten years away but it could be more
before we're all say oh yes you don't
need to learn to drive anymore
your president is fascinating again
obviously you're focused on the vision
but when we're looking at the system of
systems the convergence of the data
provides a real opportunity if you're to
bring in machine to machine conversation
get rid of all of the drivers is that
not allow you to get to level five much
quicker if we could get rid of all the
other drivers so it's not a very general
societal challenge it would and you're
quite right if we could have a world
completely like the nightmare video made
of autonomous vehicles then of course
they could talk to one another they
would follow known protocols they
wouldn't do unpredictable things we have
such a good model for them but we're not
going to have that we're going to have
to live in a world of mixed monomers and
human driven cars for a long time and so
we're going to have to solve this harder
problem of working out when somebody who
may be nudging it out of a parking place
is going to launch out into the traffic
stream or you know Americans have the
nightmare scenario of the four-way stop
everybody arrived in the forest stop and
everyone is waiting for everybody else
how did you break the deadlock and you
know typically what happens with humans
is that one will sort of nudge you out
there and the other sort of reads that
as a gesture if you like and says okay
well let that one go so we're going to
have to do all of that stuff of reading
the road and reading the environment so
there's no question yeah I suppose my
question is you've converged radar and
visual cameras what would your next
convergence area be in order to run
across further what more sensors can we
of course the slide are is the thing
that the car industry is playing with
now actually there's a sort of world
shortage of Lyde ours because everybody
wants them to play with them you know
they're not manufactured on a large
scale yet and I mean it's not quite
clear how that will work out so some
some manufacturers like Tesla are taking
the bet that lidar won't be needed so
they say that what's on a Tesla that you
buy now is all that you will need for
autonomous driving and although the
software is did
the hardware they say is all there and
so that's a bunch of a dozen cameras and
some sonars and inertial navigation unit
you know I am use and of course sat-nav
and all of that instrumentation but not
lidar others are taking the bet that
you'll need lidar at the moment is very
expensive because it rotates but
solid-state lidar is coming
another problem is will the lidar
interfere with one another when you've
got there all spraying infrared light
out into the world so is one color going
to confuse another by spraying the wrong
kind of light at the wrong time maybe
there's a solution to that with
multiplexing or something of that kind
so a little bit open I think what what
more sensitive Isis we're gonna have
thanks a lot for a really interesting
talk my question is on kind of the worry
around automation and particularly you
know with self-driving cars and
what-have-you and I can kind of see that
as you said like in the next 10 years
they could go something that might take
job you know potentially take a good
proportion of jobs yes but I rather
think that we might hear a wall after
that I don't sort of see the next like
thing in the pipeline the next big
advance but I'm probably just not smart
enough to think of it so I wanted to ask
you you know what do you think would be
like the next big sort of breakthrough
application for computer vision all
these kinds of things after the
self-driving vehicles well because
there's a lot of other things you could
do with computer vision I don't know
whether the other things will have like
for example you could make automated
security systems people have been
thinking about that for a long time I
don't think I won't have as big a
economic in fact impact as the
autonomous cars
I think AI will have big impacts in
other areas so you know you already see
it's not just it's not just manual labor
that's being replaced by robots but also
we thought of as highly skilled labor
for example paralegals
you know a lot of paralegal work now is
done by essentially they've got a Google
search like technology
with some more modern developments like
topic modeling that will go off into the
literature for you I mean drug discovery
also being affected in this way so some
really quite you know things that we
think offers being quite expert being
being done by AI so I think yes the the
nature of work is going to change a lot
I'm not yeah I am optimistic about it
because I think you know in the past
when new inventions come along
we just we change our school we do less
of that and we do more of something else
which is valuable and worth doing so I
think there'll be a lot of that one has
to worry a little bit about the
transience so you know the Industrial
Revolution generated transients and so
you know that transition period where
some people are in really are losing
their jobs and other people are training
for a new job so of course you know
there can be some real disruption doing
that and I think we'll have to think
very hard about that I'm going to draw
the discussion to a close there we've
had an excellent range of questions
thank you very much just to point out
that the bath digital festival continues
to the rest of this week our next high
public lecture is on Friday the 10th of
November Sean coming from the Department
for Health in the university is going to
talk about some of his work on growth
maturation with connections to elite
sport and the use of statistics to
improve coaching and how we understand
growth and maturity for teenagers in
particular so that's another
applications oriented to talk with a lot
of mathematics under the hood as andrew
has hinted at here today as well and I
just like to finish by thanking Andrew
very much for a great summary of the
narrative and the philosophy and the
future direction for artificial
intelligence</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>