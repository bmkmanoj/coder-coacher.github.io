<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ruslan Salakhutdinov: Deep learning - Changing the Playing Field of Artificial Intelligence | Coder Coacher - Coaching Coders</title><meta content="Ruslan Salakhutdinov: Deep learning - Changing the Playing Field of Artificial Intelligence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ruslan Salakhutdinov: Deep learning - Changing the Playing Field of Artificial Intelligence</b></h2><h5 class="post__date">2017-09-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2iMemrQm-io" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for joining us this evening
for a very special Global Leadership
event and it's great to see a lot of
familiar faces in the audience and some
new ones as well the Mars Global
Leadership series brings together
thought leaders and business experts as
well as celebrated entrepreneurs to
discuss relevant and up-and-coming
topics around global trends and
innovation the new economy and
technology my name is Sonia strimmin and
I work with the ventures group at Mars
our main focus is to support
entrepreneurs across three prominent
sectors in information and
communications technology health and
clean tech we help entrepreneurs access
five primary resources that they need to
help build their growth businesses
access to capital talent customers media
and a community of peers and experts as
part of this community we are fortunate
to collaborate with exceptional partners
like the Canadian Institute for Advanced
Research who have joined us today to
bring this event in the special talk on
the future of deep learning and
artificial intelligence I would like to
welcome Pakistan ergo senior VP of
research at CFR to tell us more about
the Institute and introduce our speaker
this evening thank you
Thank You Sonia and I am Pakistan irva
the I have the pleasure of being the
senior vice president at the Institute
so on behalf of CFR and Mars welcome to
this event the you're going to find out
all about the ins and outs deep learning
from one of the people who actually
developed the entire sort of field which
is what has now become the the speaker
russell ik Tinian is actually one of
CFR's researchers the to give you a
sense of where that places him the
Canadian Institute for Advanced Research
or CSR the other not-for-profit the NGO
that supports
the series of what we call research
programs that are actually focused on
answering questions that are important
the complex questions and the importance
is defined by their importance both to
Canada and to the world there are often
deep questions the questions that
typical research agencies or
organizations don't have the capacity to
tackle the because often they require
bringing together groups of people the
not just who are just outstanding in
their own the right of researchers but
also come from multiple disciplines they
come from multiple the backgrounds and
from around the world the and finally
you they have to bring them together for
a period of time up to a decade the work
that Russ is going to be talking about
actually the result of really a decade
long the effort on the part of CFR the
supporting a program called neural
computation and adaptive perception
basically a question was the how can we
understand the perception the way that
we actually perceive things
in a way similar to how our brain works
the so you can think this is just all
about computational vision the or
something like that I in fact geoff
hinton who was the director of the
program how does someone more far out
idea of what in fact the real solution
they could be to that problem how we
really understand perception and how our
brain actually is able to create it the
be from the sensory inputs as it were
actually receives these so the program
that russ has been a member of the
started the about 11 years ago Jeff
tension with this director for the last
10 he's now headed off to be scientist
at Google which has sort of took he
taken embraced the concept of deep
learning as has Facebook which has
captured the
one of the other members senior members
of the program young lacunae NYU the
that gives you a sense of the impact of
what they deep learning itself
they as at least a if not a complete the
revolution in the field of the
computational neuroscience the wanted to
think of it that way at least at the
mini tsunami and we'll see over the next
few years of to whether in fact it's
going to become the the real
breakthrough that we have actually been
working on they are looking for so I'd
like to give a few words about Russ the
speaker tonight the Russ as part of the
endcap program was in fact a graduate
student at the University of Toronto
working under Geoff Hinton after he
completed his doctorate he went to MIT
Mia for two years and then very quickly
was scooped back to Canada
the to be a faculty member here at the
University of Toronto the his either now
a professor of computer science and
statistics the at U of T and is been a
member of the CFR Institute appointed as
a fellow besides being a member of CFR
he's also been distinguished by a host
of other awards he's received a Sloan
the fellowship the Microsoft research
faculty fellowship a Google faculty
research award and a government of
Ontario early research award all in a
very short period of time so we're
looking we should expect great things
the firm breath his recent work have
actually involved the capture the
involved teaching computers the how to
generate image capture automatically the
learn video representations detect
objects and classify video all the sorts
of things you want to do with images
that the that currently we've just have
a hard time doing so please join me in
welcoming Russ
Thanks thanks for introduction can
everybody hear me good sometimes my
voice you know sometimes I speak softer
so if I speak softer please raise your
hand if you don't if you don't hear me
thanks for a kind introduction yes I
think that's c4
you know the exciting thing about CF is
that you know sefa brings together a lot
of us a lot of researchers working in
these areas so a lot of support was
coming from that group and and thanks
thanks for doing that in fact the deep
learning was was developed the regions
of deep learning were developed back in
Toronto and I really believe that the
sort of the founding father of this
whole community was Geoff Hinton my form
advice and I was lucky enough to work
with him when I was a graduate student
so let me let me tell you a little bit
about that technology what is that
technology I'm gonna try to give you a
little bit of intuition of what is being
happening in the field and try to also
give you a little bit of mathematical
background of what is happening in that
field okay let me start by saying that a
lot of what we try to do is we're trying
to find structuring data right that's
the goal of machine learning so we've
seen a massive increase in the amount of
computational power as well as the
amount of data that we have available to
us and we've seen that in the last
decade you look at the space of images
or if you look at the space of text and
language or speech right this is these
are datasets that we can just get off
the web if you look at the space of
product recommendation systems right
companies like Amazon and eBay or social
network data Facebook Twitter if we look
at the scientific data gene expression
data climate change data geological data
you know that's that data is is huge and
one of the things that I would argue is
that a lot of what we're seeing today is
something that I would call mostly
unlabeled data so the basically means
that I can go on the web I can download
a bunch of images but it's very hard for
me to get labels for those new images
right it's very hard for me to get
humans to annotate every single image of
what's going on in the images so maybe
you know tell me what you see in those
images so ideally with
we would like to develop models and we'd
like to develop models models that can
discover structure in these images we're
structuring the data and how can we do
it in unsupervised or semi-supervised
way right the other goal of deploying is
to try to develop models that we can
imply in a lot of different application
domains and I'll show you some examples
some examples of that and the answer to
sort of doing these things one of the
answers I believe is deploying their
multiple are the answers obvious if you
talk to other members of machine
learning community that other approaches
of doing it but I'm gonna try to
convince you that deep learning is is
the most prominent way of doing it
at least as of now impact of deep
learning so there's been a enormous
success right and that success being in
industry if you look at speech
recognition systems how many of you are
using android for doing speech
recognition you know if you say okay
google do find me nearest restaurant and
such that system was built basically the
origins of that system were developed in
toronto it was developed by geoff hinton
and a few of his students right and
that's actually sitting inside android
right now if you look at the computer
vision or if you look at recommender
systems eBay and Netflix they're using
these kinds of techniques language
understanding is another sub area that's
coming in and I'm gonna show you some
examples of that and also remarkably in
the last couple of years these kinds of
techniques have been used in drug
discovery or medical image analysis
right so if you look at companies like
Merck and Novartis these guys are also
trying to use that technology within
their own companies deploying
interactions what I would like to
mention is that we've been at the
University of Toronto we've been
developing the system where you can test
that technology so if you go to deep
learning dot CS the toronto dot edu you
can upload the image and then you can
classify can test these algorithms you
can see how well they're working and I
should emphasize the code and everything
is online anybody can use it so if
you're interested in trying out that
software you can go you can download the
code and you can play with it
here's one example this is something
that we've been developing in the last
year trying to understand the images
this is an image of Antonia Tarawa so
Antonio Tarawa is a professor at MIT and
that's an image of him that I took on my
cellphone and then we can ask the
systems to say well what is what is the
system see in those images and this is
what the system comes up with right it
says strangers co-workers conventioneers
attendants patrons now what if I want to
build a system that describes can I
actually build an artificial
intelligence system that tells me what's
going on in that image it's a very
challenging problem what people have
been doing so far is you take an image
you find a similar image in your
database and then you copy the sentence
or descriptions from Nats image that
that the similar one that you find in a
database right
and this is what the system does so the
nearest neighbor sentence is basically
saying people taking pictures of a crazy
person right so he'll like that that's
good but now if you actually try to
build a system that you know tries to
describe what's going on images this is
what the model does and this is what the
current systems can do you know by
looking at that image a system can
actually generate or produce a sentence
this is a new sentence a group of people
in a crowded area or a group of people
standing around and talking obviously
you can see in the last sentence a group
of people that are in the outside there
is a little bit of syntactic mistake
that the model is making so we're not
perfect yet at the same time we are
close to developing systems where you
know you just take a picture and the
system automatically captions it for you
right in natural language in human
language so that kind of technology is
being developed by Microsoft and Google
and such and this is you know why is
this useful obviously this is useful for
a lot of different things this is just
one example where it's can be useful
give it an image you wanna tag what's in
the image and you can obviously see it's
for searching for related images or
organizing collections on the web and
such or queering you know given a word
can you find me images that consistent
with that word obviously you can see
that for you know companies like Google
and Microsoft and
those things would be would be useful
but I can imagine that you know this is
one particular application domain you
can apply these methods and a lot of
different other domains
here's another success story speech
recognition this is a chart that I
borrowed from my friends at Microsoft
Research and this is what was happening
in speech recognition community right
since the nineties there's been a steady
you know increase in the performance or
decrease in the error and in the last
ten years basically nothing was
happening no speech recognition sort of
was working and I sort of wasn't working
wasn't good enough and since the
introduction of deep learning back in
2009 2010 there's been a huge drop in
the performance of these systems right
and now we're sort of getting close to
the setting where you know I expect a
couple of years two or three years five
years from now we basically gonna be
speech recognition is gonna be solved
that's gonna be very important and there
are a few challenges now but there has
been a huge gap you know these systems
are to the extent that you can actually
use them right before you couldn't use
them now you know they can actually use
them here's another interesting
application this is a molecular activity
recognition challenge so Merck it's a
big company pharmaceutical company they
set up one of these competitions where
they want to if they want to identify
molecules that are highly active towards
their intended targets so this is the
case where if you want to develop a new
drug or a new medicine typically you
have to solve the problem by you
developing a new drug and you're trying
to say well how much it's going to be
binding towards a particular molecule
right and the way the systems are done
is that you design a drug you test it in
the wet lab right and and the idea here
is that these companies are saying well
can we actually predict whether a
particular you know compounds going to
bind to a particular molecule and if we
can see in terms of prediction that yes
we can do it and we're going to go and
test it in the wet lab right and so in
this case in this case in fact a team
here in University of Toronto developed
one of these techniques and we won one
of the
competitions and as far as I know Iago
is actually implementing these systems
within their own company right same
thing happens with Novartis
here's another success story this is
coming from the Netflix right pretty
much a similar technique very much in
the same venue but this is a class of
restricted boss machines and I'll
mention is as well as models known as
probablistic matrix factorization
you know these systems for many of you
for many of the companies recommendation
engines are very important right you
have users you have movies you're trying
to recommend new movies to users and it
particularly becomes a very interesting
problem when you have cold-start users
somebody enters the system maybe enters
a few ratings and you want to be able to
predict what kind of movies a user might
like right obviously in the context of
Netflix in the context of Amazon it's an
even bigger issue you have millions of
users you have millions of different
items you know who's buying what and you
can try to recommend relevant items to
to users I'll mention a few words about
these kinds of polls but what Netflix
did we developed one of these we
actually developed two of these models
here in Toronto and what Netflix done is
they've taken these models and scale
them right this is something that the
companies can do so they scale them to
you know to having a 5 billion ratings
so they're using millions users and I
think in their case that we're looking
at hundred thousand different movies one
of the interesting things is that you
know deep learning was identified as one
of the 10th and breakthroughs in 2013
right and there is a reason for that
these models work surprisingly well I
think that none of us back in you know
2008 2009 none of us actually expected
that these models would work that well
to be honest right when we're looking at
these models and you know there's a lot
of news coverage of that you probably
heard some of some of that this was
a presentation by Rika sheet who was a
former head of Microsoft research
they're using deploying for speech
recognition you know Facebook and Google
just Google actually recently bought a
big startup company deep mind some of
you might have heard it's a london-based
startup a deep learning startup company
so they acquired it for five hundred
million pounds so that's a fair amount
of investment coming from from Google
into that area so one of the one of the
challenges what's happening in the
community now is how do you scale these
algorithms right so what you want to do
is you want to learn from billions of
data points right we're no longer in the
scale of dealing with thousands or
hundreds of thousands or millions we
want to be able to build models that
learn from billions of examples there's
a lot of work happening and how do we
develop parallel algorithms so that we
can process that much that amount of
data efficiently
and also how can we scale up
computations using clusters of GPUs or
FPGAs and such and what's remarkable
about these models is that the
performance of these models keeps
improving if you feed it more data so
these algorithms are very much data
hungry right which is good news for big
companies and small companies as well
and ultimately what we want to do right
and this is going to be the test for the
deep learning community is we want to be
able to develop algorithms that can see
and recognize objects around us that can
perceive human speech that can
understand natural language that can
navigate around autonomously that can
display human-like intelligence right so
that's that's the goal for us it's an
ambitious goal I don't think we're gonna
be there in the next year or two years
but that we're definitely gonna be
making a lot of progress in a lot of
these these directions you know things
like personalized assistance and
self-driving cars it's something yet you
will see more and more okay so my talk
is gonna be structured in three steps
I'm gonna give you a little bit of
introduction behind these models what
they are I'm gonna show you some of the
key deep learning models
some of the basic building blocks and
I'm gonna show you some of the
applications in particular applications
of dealing with multimodal data or
language modeling so that you'll get a
little bit of a feel of what these
models are by the way if you have any
questions feel free to ask me during
during the talk and that should be that
should be okay so let me give you just
an intuition of what these models are
let's say I give you this particular
image image of a Segway and I want to
classify this inch very simple task
I want to classify the image to say is
it a sigma a non seed way okay if I just
take a couple of pixels right then you
know pixel one versus pixel to the input
space is a mess you can't you can't
classify right on the other hand if
somebody would tell me actually what you
need to do is you need to figure out
there is a handle there is a will and
that comes down to feature
representation how do you construct
relevant pieces from your data and then
you apply your favorite learning
algorithm then you can classify right
somebody tells me that you know there's
a handle there is a will and such that I
can do that so what was happening in
with the traditional approach is how
things were done before the way the
things were done before is that you have
data you do some pre-processing of the
data some feature extraction from the
data right and many of you in start-up
world you know you get some data you do
some engineering or features what is it
the relevant thing and then you apply
your favorite learning algorithms like
logistic regression or support vector
machines your favorite machine learning
algorithm right so in the case of object
detection you take an image you get some
vision features so in this case these
are so-called hawk features very popular
features menu detect is there a car
there or not or if you have audio
classification you know you get an audio
signal you construct something that's
called ml/cc features and any good you
do speaker identification right trying
to identify who that person is if you
look at the computer vision features in
the last 20 years people have trying to
figure out what are the right features
to use if I want to recognize what we
see in images right so there are things
like safes and hogg and tax tones and
spin images and rift and so forth right
what are people trying to figure out
what are the right representations to
use because using pixels is actually
kind of hard
right and it turns out that you don't
need any of that stuff if you're
actually using these models you can
figure out what are the right features
to use right and that's been happening
that's basically flipping the computer
vision community completely upside-down
right because it basically is telling
that you don't longer need hand
engineered features you can actually
learn what the right representations are
based on looking at a lot of lot of data
same thing was happening in audio if you
look at the audio you know spectrograms
M FCC's there's something that's called
flux VCR roll often for a lot of
different things a lot of people in the
speech community we're trying to figure
out what are the right representations
to use so that we can do speech
recognition turns out you don't need any
of that stuff if you just do deep
learning you can figure out what the
right features are and that's actually
remarkable because you know I've heard
from one of the Google employees what he
was giving a talk at one of the
conferences is that you know one of the
students of Geoff Hinton went to do an
internship at Google and basically
proposed to do this thing you know they
had the whole pipeline he said scrapped
the whole pipeline we're gonna go from
the beginning to the end you know and
and she said well I initially thought
that that student was a little bit crazy
but then in the end by the end of the
internship he improved the Google system
by substantial margin right that that's
that doesn't happen very often here's
one example maybe I can just give you a
little bit of intuition what these
things are let's say you're trying to
model some data so you have some input
data like pixel intensity of the image a
word from web pages so speech signal
whatever that thing is and you have some
target variables let's say this would be
response variables my class label
categories for names if you want to do
recognition so one thing that a lot of
these models have right they have they
will have model parameters some of them
all promise that you're gonna be fitting
to the data and then they're gonna have
some latent variables these are called
hidden variables something that you're
trying to infer from the data and these
latent variables will help you model the
structure that you see in the data and
I'm gonna show you some examples of that
here's here's one example you take a
webpage you're representing a web page
in terms of the word count doctor so you
take a look at the web page and you say
how many times the word Obama occurs on
webpage how many times the word hockey
occurs in that webpage and such very
very simple representation and if you
take 800 new stories from the web this
is from the Reuters data set right and
you try to extract some structure from
from from those webpages this is how the
structure looks like right so this is
you can think of this as a form of
nonlinear extraction of off-off
structure so you can think of these
things as topics this is basically
two-dimensional representation of each
document and it's useful these kinds of
things are useful to visualize how your
data looks like but it's also useful for
retrieval given a document can you find
me similar documents and in this case
this was I think this data was back in
2000 and collected back in 2009 so you
can see things like you know energy
markets gets here you have banking
markets here you have European Economic
Community monetary policies here that
sort of next to disasters and accidents
right so you know that basically means
that there was a lot of correlation that
that you've observed in in that data so
I always make fun of fun of that figure
I think today you know those two things
are gonna be more aligned but this is
this is one example so let me let me
talk a little bit about some of the key
key models just to give you a little bit
of intuition one of the very basic
models is something that's called
restricted Boltzmann machine the
definition of the probabilistic
definition is not very important but
what I want to emphasize that these
models do define proper probability
distributions over input so what does
that mean
that basically means that we can
characterize uncertainty right we can
deal with uncertainty whenever I make a
prediction it's not only I'm gonna be
making prediction but I'm gonna tell you
how confident I am in that prediction
and that's very important right you can
also deal with missing or noisy data
that's important what's a sum of your
data is missing or some entries in your
tables are missing what do you do right
and you can also simulate from the model
so in these kinds of cases we can
actually say well can you actually
simulate me from the model so I can see
what the
is actually thinking about I'll show you
some examples of that so for example if
we take this model and we feed it to
four million images this is the kind of
structure that the model is discovering
right so a lot of these things look like
little edges and the way to think about
this is that if I show you a new image I
can basically write it as a linear
combination of subsets of these edges
right so it sort of like finds basis you
can think of it as finding basis in the
data and this is useful because it turns
out that the visual area v1 of our the
v1 is you know the processing that's
happening in the brain you sort of
people sort of discovering that this is
exactly what you see in v1 of the human
brain so that's that's interesting
if we take other kinds of data so this
is handwritten characters these
handwritten characters coming from I
think 50 different alphabets around the
world same algorithm you find strokes
right and we think that strokes is the
right representation for characters so
this is folks that the algorithm is
discovering if you take let's say words
from the webpages right and you apply
exact same algorithm you basically
discovering so-called topics right so
you can sort of think that you know
there's a topic about Russia there is a
topic about us this is talking about
computer software and so forth it's
talking about stocks right so this is
naturally comes out from these models
and the way to think about these models
is that every time I see document this
document is made up by some combination
of these topics that's effectively what
the model is doing so it finds these
atomic pieces whether would you work
with images or whether you work with
text which is which is interesting exact
same model can be applied in
recommendation engine right so in this
case for the Netflix data set you have
about half a million users 80,000 movies
and the idea here is that these
variables you can think of them as
modeling ratings or rating patterns of
users right and these latent variables
they're capturing certain patterns
certain structure in the data in
particular if you look at these latent
variables they discover Chandra
right by default you know we know we're
not telling the system that this is a
movie horror movie or this is an action
movie just by looking at the patterns
that the users provide us with we find
that the genre is the most informative
structure that can be extracted from
these rating patterns right so it finds
things like scary movie fact the
interesting thing it sort of finds
Michael Moore latent variable right so
it's just naturally comes out that it
turns out that either people like his
movie or they hate his movies so that
naturally just comes out from just the
rating pattern one of the other things
that's been happening in our community
is that well we sort of if we look at
these models so far they can model low
level structure right so for example in
images you model these little edges but
if we actually construct multi-layer
representation on multiple levels of
representation the hope is that we can
describe a higher level representations
right and that's the idea of using this
compositional learning you first learn
low level representations then compose
them into more complex representations
right and that's the basic idea of deep
learning models you have these multiple
levels representations at the low level
you're discovering you know some simple
concepts like edges or little
correlations between the words at the
high level you start discovering more
abstract representations right and the
idea here is that it's it's it's this
kind of representation this multi-level
representation is exactly what we see in
a human brain
it reads that's that's the connection
you know the brain has hierarchical
structure that's important cortex
appears to have a generic learning
algorithm we don't know yet whether it's
true but it seems to be that that's the
case the humans can learn a lot of
different from a lot of different
domains right and humans actually tend
to learn simple representation and
compose them into the more complex ones
so that's sort of a justification for
building these models at least the
inspiration that we draw by you know
talking to our neuroscience friends and
this is what's actually happening so
this is the work that was done by an
doing at Stanford and his group where
they've shown that if you train these
kinds of models on images of faces
what happens here is that at the low
level you're sort of capturing these
little edges and then these they just
get composed into parts so you see
little parts emerging and that's
emerging as you go up in the hierarchy
and at the high level you start seeing
you know faces right so you can
recognize different faces the same thing
happens in the speech recognition you
know at the low levels you're capturing
sort of low level representation at the
high level you start capturing high
level representation and so forth and
then you start capturing high level
linguistic representations and that's
the whole idea the hierarchy of these
models is what made it possible to you
know for these models to be so
successful so one question that we you
know we've been building these kinds of
models and one question I want to ask
you is that how good that a so what I'm
gonna do is I'm gonna do a little bit of
tests I'm gonna show you two panels on
one panel I'm gonna show you real data
right so training examples on another
panel I'm gonna show you simulated data
so you can think of it as a fake data
that the model simulates and you have to
tell me which one is which okay so one
panel shows sort of real data
you know another panel shows you
simulated eight how many of you thought
this was simulated and this was real
okay what about the other way around
perfect so I get fifty fifty that's
always good so if you actually look at
the samples you will recognize that this
is real and this is simulated okay so
this is simulated and this is sorry this
is simulated and this is real so it
captures a lot of structure of how
handwritten characters should look like
right but it means the some of the key
pieces the other thing that's happening
here is that the real data is much more
diverse compared to simulated data but
you see a lot of diversity in the real
data and that comes from the problem of
trying to build these models models that
can actually generate characters from 50
different alphabets around the world and
there's 1500 different characters this
really tough problem to generate
coherent characters but at the same time
is these models are remarkable in terms
of recognize you know doing character
recognition optical character
recognition here's another example of so
called
M news data set
these are just handwritten characters
and it turns out that you know this is
real and this is simulated or this is
simulated and this is real right but you
can basically know you know it's very
hard to see the difference if you only
start scaring a few things you'll see
the difference but these models have
been very good in terms of recognizing
handwritten characters right and in fact
some of these models are now basically
sitting reading check numbers right so
if you sort of like deposit the check
and it reads your mouths you know these
flavor of these models these deep
convolutional models are basically
recognizing what you've written on your
cheques
it's basically at this stage these
models have better than human
performance in terms of recognizing
characters and recognizing sort of what
you're writing so let me show you some
of the applications of these kinds of
models and one of the things that we've
been trying to look at is trying to look
whether these models can deal with multi
modality right when I give you data in
multiple modes so if you look at the
mountain in the continent where it's
it's not just a single modality its
images stacks audio on a product
recommendation system or in robotics
applications right if you want to design
a little robot you have to understand
what's going on with the vision audio
motor control touch sensors and such and
what we were thinking about right and is
the whole research happens in that area
is that can we take both of those
representations and get some high level
concepts by combining these different
modalities together right and obviously
you know an application such as medical
imaging we're talking to a lot of people
in the medical domain right and one
thing that you know in that domain
people would like to do is that you get
some scan an image and you want to write
a report automatically about the scan
but wouldn't that be nice so here it's
sort of the same the same concept given
an image can we actually describe what's
going on in the image right and
obviously this kind of multimodal input
would be very useful to improve
classification if you want to classify
if I give you that image and I give you
corresponding tags it's it's easy to
classify maybe you can feel in the
middle
teachers complete the text given the
image or given the text can you retrieve
relevant image right there's a lot of
things that you can do and this is just
one example where you're dealing with
images and text you can obviously deal
with other kinds of modalities here and
one of the challenges and a lot of a lot
of different domains one of the biggest
challenges that we've seen is that
sometimes different modalities can be
very different right images are dense
you have lots of pixels lots of colors a
text is very sparse you know I tell you
a sentence it's a sunshine I give you
like four words right so obviously
that's a very sparse representation so
it's very difficult to learn these cross
model features from low-level
representations the second challenge is
missing and noisy data right so here's
an example of you know you shown an
image like this one and somebody tells
me what kind of camera they've used to
take this image like to some extent you
know actually when we started looking at
this data we initially thought well
that's not going to be helpful for us if
we want to classify what's going on in
each house this helpful but it turns out
that it actually becomes helpful because
when people tell me what kind of camera
they used to take the image the picture
of the image it usually picture of some
scene right so you can classify you know
it gives you some guides that it's
actually scenic picture it's not gonna
be a picture of my laptop for example
right because people don't don't don't
do that so the data can tell you a lot
sometimes we basically see no text
sometimes it's just completely arbitrary
things right and this is what they
generated model is so this is the real
data this is what we see in Flickr data
set Yahoo has released and this is the
completions that the model yes right so
in terms of tagging what what it sees in
the images and it's pretty interesting I
mean these a lot of a lot of these cases
we can actually do really well in terms
of tagging taking these images and one
of the things that we can do here is we
can build this hierarchical model as I
mentioned before you know we start with
a low-level model and we basically get
high level representation from images we
get high level representation from text
and then we combine both of these
representations at the higher level
right so we're trying to find
dependencies between these two different
modalities by extracting high-level
representation first and this is what
these things do so here's an image of a
cat and sort of like says cat bad dog
kitten puppy and such I particularly
like this thing here I don't know why I
decided to say Canada nature sunrise
Ontario but at least it's consistent in
terms of what is describing you know
portrait kid butterflies so it's sort of
does pretty well but it's even more fun
when you research it's even more fun to
look at the cases when it fails right so
here the cases weight fails so if you
look at the top one it basically says
the portrait army sold you completely
missed the whole idea I particularly
like this one here did you think it's
Barack Obama lectures politics president
right so we tried to look us to why it's
making these mistakes and we looked at
the data set this was Flickr data set
and it turns out that you know these
biases in data set there aren't any
images of lions on in that data set very
few images of animals and none you know
very few images of birds basically but
lots of Obama signs right so now the
models basically saying that you know
this particular image just doesn't make
any sense I haven't seen ever seen
anything like that but I've seen lots of
Obama signs like blue and white and and
so it sort of makes a mistake there so
obviously there is a room for
improvement in those cases but you know
it's fun it's fun to look at these
examples you can go the other way around
you know if you give these words these
are the images that the model retrieves
right so it sort of does in a lot of
cases it does quite well here's an
interesting the last piece is a mistake
right even though if you look at these
two bugs they kind of look like
chocolate cakes right so you have an
excuse for for when it's making mistakes
and these were lost keep improving right
so these are this model that's about a
year old now and you know with the
technology and with the advance of the
data these models are much better
behaving right now and this is the kind
of data set that we're looking at right
so sometimes it's very informative
what's happening in the image an awesome
shot is a very common tag that people
who
on on the on the flicker and you know
sometimes it's use sometimes there is a
lot of noise sometimes it's completely
missing so we have to deal with a lot of
these things and in terms of results you
know the numbers themselves don't really
matter what I but what what I do want to
point out is that there are 25,000
labeled examples so somebody went and
labeled what we see in those images do
we see a sculpture in the image do we
see building an image a shape an image
and whatnot versus cases where we have a
million unlabeled images so these images
just on the web without anybody tagging
what these images are right and we can
see a huge boost in performance if we
actually make use of unlabeled data
right so if we make use of more data
that's available to us the performance
of these models actually goes up so
that's to us that was the exciting piece
when we looked at this model and these
are state of the art performance on
these models today they work fairly well
and then you know we started looking at
more interesting problems where we're
trying to basically say can we actually
design a system and I've shown you a
piece of that system where I show you an
image like this and what you want to do
is you want to generate the sentence
that basically says a man skiing down
the snow-covered mountain with a dry
with a dark sky in the background
right obviously you know as I mentioned
before this is just one particular
application but you can do the same
thing for videos I show your video clip
then you're describing what's going on
in the video you know I talked to some
some of my friends in the sports domain
you have a sports video can you actually
describe what's going on automatically
given that clip in sports video same
thing happens in the medical domain
given an image of some scan can you
actually describe what's going on in
that image right so a lot of different
applications are possible particularly
the domain of doing dealing with video
and here's what the key idea is behind
these models are what you'd like to do
is for each word what you'd like to do
you'd like to represent it as a
dimensional vector in some semantic
space so you're constructing this
semantic space let's say it's a 300
dimensional space in such a way that
dolphin and whales are close to each
other
tables and chairs are close to each
other and you know November is over here
it's far away from well and chair right
you can sort of find that semantic space
and imagine it's 300 dimensional space
it's pretty big space you can arrange
these things and semantically know a
semantically meaningful way and you can
do a lot of things for example what you
can't do is you can embed this image in
this space and you can embed a sample in
the castle reflecting the water in the
same space right and you can say I want
these two things to be close to each
other because they mean the same thing
right so I can embed sentences images
phrases I can embed sounds I can embed
videos a lot of different things you can
just build this whole big shared space
the question is how can we do it and
such mean that's a little bit more
technical but but we can't do that and
this is exactly what happens with these
cases if I show you this particular
image I embed it into the semantic space
you know look for the words in that
space the closest words then this is
what the model generates right so it
becomes very efficient way of tagging
images same thing happens if I given a
word and I embedded in a semantic space
I can find images that are most
consistent with you know with that
particular word we can also do fun
things and this is where the fun things
can come in I don't know when it could
be useful but it's just fun to look at
so if I show you like an image of a blue
car right I look at its semantic
representation I subtract blue I add
yellow and I get yellow cars isn't that
nice
well I add red I get red cars right
here's my favorite thing I take kittens
i said- ball plus box I get kittens in
the box all right or if I take this
image and I say - box plus boy I get
kittens in the bowl right so this sort
of like tells me that there is some
semantic notion of the algorithm
understanding you know there's a notion
of box and a cat so if I do this the
simple mathematical manipulations the
semantic space I can actually do quite
well right yep this question
see intuition behind when you so the
earlier slide we show those errors when
you get those errors other than just
giving it more training data to identify
yet yes so so one of the things that so
the images of with like Obama signs for
example right what happens there is that
I've shown you these examples but these
examples have a lot of uncertainty when
you look at the prediction so you
predict a word but you can also predict
the uncertainty the variance and in
those cases variance is pretty high so
effectively the model says I think it's
a bomber but I actually have no idea
right
I think the steps to improve these
systems would be to just collect more
data right it's one of those things that
if I train on a lot of data but I never
see images of birds never in my life
I've seen Birds somebody shows me an
image of a bird right the system the
correct behavior for the system would be
to say maybe it's one of the things that
I've seen before but there is a lot of
uncertainty maybe it's Obama sign but
there is a lot of uncertainty that's
effectively what happens with these
models but if you want to improve these
models then obviously getting more data
is important right that's the main
that's right it's a main way to improve
the model yes the the best way to
improve the model is to get more data
right right it's a good question so the
question is you know if is there a sort
of theoretical foundation where you can
say if I get em more images I can reduce
my error by a certain amount that's a
good question I don't think we have a
good theory at this point we can make
very sound probabilistic statement by
saying with very high probability my
error is gonna be below that you know
particular threshold we don't have that
yet so to this extent there's a lot of
empirical evidence that this is
happening but we still lacking good
theory behind it in terms of making
prediction and making very precise
estimates of the uncertainty yes it's
still it's still in the young age these
models so so we need more theory yep
there's a question there
well trading off recall against
precision it's one of those things that
you know the question is if your models
making mistakes on certain classes and
certain objects and such you want to
have statements of the form that if you
get n more data points for that
particular class you want to say precise
statements by saying yes my air is gonna
be below a certain number right so yes
we don't we don't have those statements
yet
here's another so here's what happens
with the caption generation right so
this is the case where I show you an
image and you know this image of a car
and it says a car is parked in the
middle of nowhere it's pretty good
description right or a little boy with a
bunch of friends on the street here's
there's a cat sitting on a shelf right
but here's the mistake here the model is
breaking like if you look at this image
it says the two birds are trying to be
seen in the water right and this is
basically telling you that the system is
not just copying these captions it's
actually trying on you know tell you
what it sees in those images and it
makes a mistake you know but sorry I
would argue that you know this is the
mistake that a kid would make for
example
so obviously there is still a lot of
room for improvement so these kinds of
systems you know they come a little bit
closer to artificial intelligence
instead of just giving an image you tell
me what tags you see you're actually
trying to build a system that describes
what you're seeing in the image and
there are two things happening right you
have to understand what's going on in
the images but then you also have to you
know do natural language processing you
have to be able to generate sentences
that are semantically and syntactically
correct and that's a very hard task to
do here's another example this is
actually two CFR members at one of the
c-4 meetings this is your wise a
professor at Hebrew University and David
filleted the professor at University of
Toronto and notice that what the model
does it basically gets colleague
entrepreneurs that's good but then for
some reason also says waiters and
Busboys so you know the two things are
close to each other I guess for that
particular image but if you look at the
model samples right I mean it's not
particularly very descriptive but at the
same time it captures a little bit two
men in the room talking on the table
two men are sitting next to each other
two men are having conversation at the
table two men sitting at a desk next to
each other so this sort of like you know
just accent is a boring descriptions but
at the same time they are somewhat
reasonable descriptions of what what is
going on in that in that image and as I
mentioned before it's always it's it's
even it's even funny to look at failure
examples so this is an example where the
system fails on the image part right so
if you look at this image it basically
says spiders spiders
yes creepy spook is is probably
descriptive and then obviously once it
fails on the image part you can see that
you know it basically says a giant
spider just talks about spiders right
there was a black spider web I like the
spider giant spy I found in the
Netherlands I'm not sure why decides
they found in the Netherlands but it's
one of the descriptions that the model
is providing right so and again these
systems right now it's in the infancy
yeah question
now these are these are descriptions
coming from the same model you can think
of this model as being a stochastic
model right stochastic model what that
means is that it will generate multiple
sentences right depending on random
numbers so it can have multiple
descriptions and that's certainly the
case because given an image there could
be a thousand different descriptions
right so these are just a sample from
the model but one of the things about
these kinds of models is that again we
try to build models that can actually
tell you know the user what what you see
in those images yes I know these are
just samples yes these are just samples
that's a good point I mean what we what
we're doing now is we know you can
basically display the most probable
sentence and then the next Provost and
and so forth
yes yes yes good good question I think
that what's been happening in the
community now and there's a lot of
groups working in this area right
Microsoft Google are working on in in
the air as well as Stanford and Berkeley
teams there because it's sort of like I
feel like it's the next frontier doing
but one of the problems that we're
facing is that how do you evaluate these
models right who's to say that this is a
good description or not and there aren't
any good metrics existing metrics to
evaluate these things so what people are
doing right now is actually you know
we're setting these things as things on
Mechanical Turk right and the Turkish
tell us oh that's actually pretty good
description now that's not a good
description and then potentially you can
learn off that as well well one of the
things about these models is that there
is a metric that we're optimizing right
it's called the likelihood probability
of observing the sentence given the
image there's always some metric that
we're trying to optimize right we have
to tell the model that you know the
model has some parameters there's some
latent variables and if we specify the
probability distribution we optimize the
probability distribution if there's some
other metric we optimize for that other
metric it's very hard to optimize for
the human metric because you know humans
might have different views and what it
means and such so the way that we're
doing it right now is we optimizing
something that's called blues quartz
which is used in machine translation and
then we try to correlate it with humans
and say how good these things are as
compared to humans but that's you know
that's a challenge for us going forward
for sure
good good good question and ultimately
you know I think that in the future if
we want to be able to build intelligence
we have to be able to deal with multiple
modes we have to be able to deal with
sound speech recognition video analysis
dealing with images text if you know if
you have later sensors or whatever
modality comes in you have to be able to
build a system that takes all of these
different modalities and makes use of
that and obviously the hope is that
we're going to be able to develop
learning systems that you know display
human-like human-like intelligence yeah
there's a question so the question is I
guess the question is you know when you
have all these different modalities how
do you account for these different
modalities may be just a single somatic
space is not the right thing to do
sure you know that's it's an open
research question I think that what was
happening up to now is that you have
vision communities focusing on video or
images you have natural language
community focusing on analyzing language
speech recognition community focusing on
speech recognition right but I think
what will be happening in the future in
the field of machine learning and deep
learning is that all these different
modalities will have to come together
right because in this caption generation
thing we have to solve images but we
also have to solve for natural language
processing right and that becomes very
challenging task when you're trying to
bring the two things two things together
but it's very important if you know I'm
sure that in many of your data that
you're looking into it's not just a
single thing it's maybe multiple data
sources that come into play and then you
try to basically figure out how do you
integrate these multiple sources into
the same into coherent learning
framework question so good question so
the question is like when you go from
static images to videos do you just
treat them as individual frames right
now people do treat them as individual
frames but we you know in more recent
work we've looked at the problems where
we actually taking time into account
right because if you have a frame at the
next frame things move but it's not like
it's a completely independent frame
right it's not like objects randomly
appear disappear these frames are very
highly correlated so if you want to take
time into account you actually you know
you can build much better models if you
take time into account so for things
like action recognition I'll show you a
video and you tell me is the person
sitting down standing up right or if I
show you a short clip
your new activity recognition by saying
is the person falling down or not right
you can imagine these kinds of things
like for smart homes for example could
be potentially useful we have sensors
and you detect somebody falling down for
example an elderly person falling down
they're taking time into account and not
treating each frame individual but
actually modeling the whole sequence
jointly typically gives you much better
results yes so there are a lot of
different tasks like summarization is
one of the tasks for shoots a very
challenging task the other task is we're
trying to get semantic meaning from
sentences right so for example if I go
to Google and I type in dog chasing a
cat or kept chasing a dog right these
are two semantically very different
sentences and I want to be able to get
images on I want again be able to get
answers based on that so and we're not
there yet so if you you know if you ask
a human you know show me images of black
cat that's sitting inside the box that's
very easy for a human but if you type in
in the google query or being you will
basically get some caps on inside the
box sometimes it's outside the box
sometimes the box sometimes there's a
cat right
so you can obviously see that you know
these systems are not semantically it's
very hard to get the semantics and
ultimately we're trying to build systems
that do question answering systems right
you know think about SATs you have a
paragraph you have a question you know
an 11th the 10th school grader I can
answer those questions the question is
can you build a machine that can answer
those questions right and these are not
simple questions these you have to
reason about paragraph what's happening
to do that so that's that's very
challenging to get the semantics out of
the sentence this is very challenging so
algorithm development is definitely one
thing that a lot of us are working on
just designing new algorithmic designing
better algorithms there is definitely
big progress happening on the
computational side
so you know the the development of GPUs
is one of the biggest advancement in
this in this technology so just to give
you an example you know looking at the
images there is a canonical data set
called image net that has 1.2 million
images and has thousand classes and try
to recognize all kinds of things like
different kinds of mushrooms different
kinds of dog breeds right whatever these
things are is just in the academic data
set it used to take us you know on a
modern computer on the CPU it would take
you two months to train the model if
you're using GPUs it would take you you
know a week now if you're using the
latest GPUs coming from Nvidia and you
do a little bit model of model
parallelization or data paralyzation you
know it takes you five hours right so
this scale basically the computation
becomes very very important so there is
a lot of developments on the hardware
side and obviously if you want to build
the real-time systems right where you
can track things or analyze the video in
real time you know 20 frames per second
40 frames per second you definitely need
efficient hardware so there's a lot of
progress happening on the hardware side
that's that's making you know that
technology to some extent you know you
be real-time and you train these models
you feed more data in terms of
scalability so the question is you know
what about the implementation details of
these models right how how they what's
the implementations there's a lot of
different packages that people have been
developing in the last couple of years
just to give you the beauty of these
models is that you know training can
take up some time so for example at
Toronto we have 10 GPU servers each
server has 4 GPUs right so that's the
kind of resources that we operating with
we can train these models on millions of
images you know within a matter of three
or four days but the beauty of these
models is that at the test time right at
the test time if I show you an image you
know one of these model takes about I
think 35 milliseconds to give you the
answer so you blink your eyes and it
gives you the answer right roughly
speaking so so these models
so you know with the advance of
Technology on GPUs they can be really
fast at the test time and that's very
important in terms of implementation
again as I mentioned before and deep
learning dot C as the Toronto dot edu
we've released the code for training
these models a lot of these models you
can basically download the code train
them it's open to everybody there's no
license or anything like that you can
just take it and use it yeah that's a
very good question is this sort of also
a philosophical question of what we try
to do and I know you know if you've
looked at the news they've lately been
sort of a little bit of scare like you
know people saying somehow we're gonna
build sort of evil robots that will take
over I don't think that's gonna happen
in the near future or in the distant
future I don't think these systems are
intelligent that they can basically you
know I think that you know these systems
ultimately I believe that they will help
our lives
to the extent that they will make our
lives a little bit easier right so so
for example to give you one example in
speech recognition a couple of years
from now if you no longer gonna be
typing your email on your phone right
and that's you know right now the system
is not working that well especially if
you in the bus that is noise around
people are talking and you speak to your
phones just mess so that problem is
gonna be solved I think that it's you
know my personal belief is that I want
to be able to build AI right I want to
be able to build intelligent machines
because right now machines are pretty
dumb to be honest you know there is no
there's no high-level intelligence in
the machines right now right you know we
can basically process lots of data we
can tag things we can classify things we
can you know write boring descriptions
and such but we're nowhere near of
building the tree I I think that the
world is gonna be transforming I mean
going forward for sure you're gonna have
personal assistants you're gonna have
robots that going to be replacing basic
routine tasks
for humans you're gonna have self
self-driving cars these things will
happen for sure yeah so so typically you
know you can think about what these
models are doing right is I should
mention that one of these model has 50
million parameters right so when you see
a lot of data the model actually
recognizes it but some extent has an
understanding or stores a lot of
information about those images and
understands how to process and parse
these images as we go through the models
and building more and more complex
models I think these models will just
grow in size they will become bigger and
bigger so you're gonna have billions of
parameters right I mean when I talk to
my Google friends you know and I say
this my model has 10 million parameters
they say well that's nothing we have 10
billion parameters in our models right
because they have so much data that they
can build models that can handle a lot
more data oh yes oh yes it's just a
matter of a scale yeah question yeah so
I think that I mean that's that's a good
question so you have a model that
basically you know adapts the one
environment you have another model
that's to another environment well you
have some kind of communication between
the two models for sure that will happen
you know it sort of happens with the
caption generation model because we
actually have a model that processes the
image part get some representation from
images and you have a language part you
know that borrows some of the techniques
from the natural language community
model and then you jointly train the
model and you sort of communicating
information from one model to another
model because if one model generates the
sentence says yeah I'm not sure what I
should be generating sort of relies on
the image model right so obviously these
things these things will happen for sure
oh yes yes absolutely I think that the
notion of distributed computing is is
there I mean in fact most of these
models the way you're training these
models right now is you have multiple
machines each machine sees a subset of
the data fits the model to the subset of
the data and then communicates that
information to the master machine the
master machine aggregates the
information from all the models updates
the master model and sense
the master model to the local machines
and say no that's your current model and
and these models so you have this a
synchronous training sort of algorithm
where you can have in fact you know
Google they've released the paper a
couple of years a year ago couple of
years ago where precisely that's exactly
what they doing right you have a hundred
thousand machines each one looks at the
piece of the data and there is a
communication happening between the
master and the children that's right
that's right parallelization becomes to
some extent the world with hierarchical
because you have to process so much data
you can't do it on a single machine yeah
right right it's a very good question is
sort of like how these machines that
we're developing right now writing
mostly finding patterns in the data when
you start thinking about you know the
cognitive bias to it sort of what you
know what the human learners do at this
stage it's very hard to say we think
that we are finding some cognitive
biases but I can't say that you know
right now you know the way that these
machines are working right now is is
mostly as a pattern recognition right
you're recognizing certain patterns so
that you can solve a task I can build
your system that can differentiate
between ten kinds of breeds of dogs much
better than a human can by basically
collecting lots of different data about
different kinds of breeds of dogs right
in that problem I can solve that problem
but it turns out if you start thinking
about semantics like humans do like if
you if I look at the whole scene I
understand I have some notion that you
know the people in the room I have a 3d
view of the chair I know what's behind
the chair right so if I don't see for
example somebody's legs I know that the
legs are there so all of these things
that humans can do right now machines
are not good at so we still you know far
away from being able to and a lot of
people are working in that in that
research direction to sort of like
trying to understand better how humans
do it versus how bussines do it and and
that's I think that's gonna be happening
in the next you know ten years or so
yeah uh yeah that's a controversial
topic what what we happen to go quantum
computing
I don't know so as far as I know what's
happening with the quantum computing you
know companies like D way for example is
that yes they can perform certain
computations but they restricted to
certain architectures so there has to be
the architecture has to be designed
specifically for these things too so we
can run and run a quantum computer at
this stage I don't know I just don't
know what what will happen in that in
that area yeah there was a question
there so I guess the question was about
personal assistants and sort of how it
relates to robotics I think that I you
know I don't work in the area of
personal robotics but a lot of my
friends do and I think that we'll see a
lot of advancements in personal
assistants and I think the role the deep
learning will play is going to be pretty
substantial right so personal assistants
the first of all you need to do speech
recognition right you need to recognize
what the person is telling you one thing
you need to be able to sort of have some
processing of that speech understand
what the question is for example right
get semantic understanding of that and
then maybe be able to say back it's kind
of like generating a description or
generating the answer back to the human
I think that you know that's a very
challenging area to be in but I think
that you know in the near future we'll
see a lot of advancements there right
just like you know being able to ask the
question so a lot of these things are
also based in question answering systems
I asked the question you have to give me
the answer right and there's a lot of
research happening in that area right so
for example one of the things that we
try to do with unfortunate we don't have
data sets is building a question
answering system based on images you
know I shown you in the cases where you
give me the image and I give you a
description but I think a more
challenging problem would be you give me
the image and you ask a question about
the image and I give you the answer you
know suppose I show you an image and I
ask you what's the color of the car
that's parked behind the bus
right now that's a very challenging
question to ask a machine right based on
the image because the machine you know
the algorithm the machine has to
recognize there is a car there is a bus
what does it mean to be parked behind
the bus and then be able to generate the
ant's in natural language that will
basically say well the color of the car
is blue humans can do these things very
easily kids can do these things very
easily machines cannot at this stage so
I think that question answering systems
will be gaining a lot of momentum going
forward in our community yes so so
there's been a lot of work on working so
called relational data so maybe I can
just just just put it in a slightly
different context this has been a lot of
work and trying to work relational data
or sort of building a knowledge base so
for example things like true concepts
like a car has a trunk a car has a wheel
so that would be like a has relationship
right or you know part of is another
relationship or being on top is another
relationship or being taller or being
younger is another relationship right
like somebody's younger than somebody
else so somebody stole than somebody
else right so these things there's a
whole community working on in that area
and I think that you know these machine
you know these algorithms can actually
do quite well you know sort of learn the
basic relationship between objects you
know it's it's it's a tough area of
research but they've been some advances
in the last few years in area right if
you look if you look for relational
relational modeling relational data
using deep learning there's been a few
papers on that on that front right so
it's a good question the question is
basically saying you know if you deal
with with medical images some of them
are very large some of them are
three-dimensional can you actually use
these models my answer to that would be
yes I think we are there we don't need
Google clusters to do that for sure
because I know that there's been a few
companies who've been focusing on own
medical dealing with medical data
there is been some work coming for
example in university of new mexico
there is a group there that I
collaborate with that looked at fMRI
data and also looking at the voxel data
which is three-dimensional and using
some of these models in that space so
there is definitely you know in
computationally you know if you have
couple of GPU servers each one cost
$8,000 or so so you know I have two
three or four of them that's enough I
think to deal with these kinds of to do
with kinds of data you had it's a good
question I think that I mean you write a
lot of these things data is the key for
sure you know I don't think that you
know it's a hard question right I mean
it's it's not just the data it's also
the resources right Google is sucking
and Microsoft and Facebook all of these
companies they're you know they're
getting the talent you know people who
are developing these systems and I see
them being the frontrunners in that
space I don't think that they're gonna
solve it just being in Google or
Microsoft just because in academia you
know in academic world there are so many
profs so many groups working different
angles so I think that the birth of AI
and you know these algorithms are
essentially going to be developed in
academia I think right one of the things
about the companies is that you know
here we can do crazy things right we can
do crazy things like capturing
generations and such which are not which
are completely useless to industry I
think right why do I need a boring
description of two men sitting at the
desk talking to each other you know it's
it's beautiful from AI perspective that
is probably not very good from the
product perspective and I think that you
know companies they're a little bit more
motivated towards specific products
right having said that I think with the
startup communities what I'm seeing in a
start-up communities is that is very
hard to compete with these big companies
but at the same time if you pick a
particular area right and you you just
focus on that in one area and this is
what I see is happening with a lot of
startups right a startup that comes in
and says I'm gonna be working with CT
scans and I'm gonna solve one particular
problem using these models and they can
do a lot of progress and you know
you know there's a company I talk to you
that does recognizing food or calories
like you take a picture of your food and
tells us 160 calories in it right and
obviously these guys are just solving
that particular problem whether it's AI
or not I know it's debatable is probably
not AI but at the same time you know so
there's a lot of opportunities for
startups I think that space is actually
very sparsely populated right now big
companies Google and Facebook and
Microsoft just because they have
resources and they have people they sort
of monopolizing this space but I think
there's also a lot of room for startups
to to use that technology to solve
specific problems yes yes neural nets
are behind a lot of these algorithms
that's right Boltzmann machines and
neural nets so these these algorithms I
mean a lot of them based on neural
networks recurrent neural networks when
we talk about language model there is
something that's called probabilistic
neural language models there's a lot of
different flavors of these algorithms
but the basic backbone of these
algorithms on your own that's yes so
maybe I can just summarize them just to
say that you know these are efficient
learning algorithms when you're learning
these hierarchical models right and if
you look at the space of a whole
different domain speech recognition
building some kind of hierarchical
representation dealing with multimodal
they're dealing with object detection
the beauty of these models is that the
I'm proving upon state-of-the-art and a
lot of different domains right in object
recognition detection texts which she
will hand written character recognition
speech recognition and a lot of a lot of
other domains so it's it's one of those
bizarre times when we were here and all
of a sudden we just jump in here and you
know we have this sort of crazy things
happening in academia when you build the
model you put a lot of effort into the
model you chip some results and then you
basically you know take one of these
models and then just up here right so
it's it's just you know there's been
rapid improvements you know it's very
hard to keep up
you know all of a sudden a year from now
a recognition technology image
recognition technologies jumped by a
factor of two people can recognize
things same thing happens in speech you
know and so it's a very fast progress
that's been happening in the last few
years and it's it's a very exciting area
to be and so and and and just to point
out the Toronto lab we collaborate with
a lot of different companies thanks to
see far for the support that's been you
know that that organization's been very
very instrumental in developing a lot of
these models fact see far I think was
the main sponsor of this whole framework
that was developed by by a lot of
different researches and a lot of
different companies and particularly us
are are collaborating with that so thank
you I just wanted to say thank you to
rest very much for this very interesting
talk I think we all gathered a lot of
insights in addition to a new
perspective on cats and boxes who knew
that was possible and thank you to our
partners at CFR if you're interested in
more talks like this and other events at
Mars please check out our website at
Marcy decom we have a great newsletter
where we announce and promote these
things all the time and it's my
understanding that we'll be sending a
brief questionnaire and some information
to all the participants of the event
we're all going to be here if you have
any questions about CFR or the content
or Mars so please feel free to network
shake hands and continue the
conversation thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>