<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Case for Artificial Intelligence Research Subjects Oversight Committees | Coder Coacher - Coaching Coders</title><meta content="The Case for Artificial Intelligence Research Subjects Oversight Committees - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Case for Artificial Intelligence Research Subjects Oversight Committees</b></h2><h5 class="post__date">2017-09-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m4OUitBEoiw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">a ronald sandler and john Bassel they're
gonna share the podium here and i hope
we're gonna get into the issue of sorry
we're moving faster and faster I hope
you're gonna get to the issue of how
capacities are relevant to a moral
agency but we'll see so Ron is associate
professor of philosophy and the director
of the ethics Institute at Northeastern
his research area is our environmental
ethics and the intersections of ethics
and Technology and John will be part of
this well is an assistant professor of
philosophy at Northeastern whose
workflows focuses on ethics applied
ethics and the philosophy of biology
thank you
so as the title indicates what we're
gonna be talking about today is we're
gonna be trying to moat we're going to
be trying we're gonna be trying to
motivate the idea or propose the idea
that responsible development of robust
AI would be promoted by implementing
some AI research oversight committees
similar to the types of committees that
are used in other types of research like
human subjects research or embryonic
stem cell research and then we're going
to talk a little bit about how that
might be structured how the the
committees might be tasked how they
might be constituted empowered and so on
now my job was to motivate this project
and then John was going to talk is going
to talk about the details of what the
committee's might be like but in a lot
of ways the last two talks were just
motivating the project so thanks Matt
Eric and Martha appreciate that because
the one thing to to recognize is if we
think that that artificial intelligences
that have capacities similar to humans
are going to have the same moral status
similar to humans they're gonna come
from somewhere there's going to be a
process by which they come about and so
and there's going to be some features of
that process is gonna make it that's
going to require some some sensitivity
to what's going on so for example as
Eric pointed out there's going to be
epistemological issues it's not going to
be always obvious that these things are
these capacities are emerging at a
certain rate another concern is or to
take Eric's point and Mars point look if
you have design principles that you
could put into them if they are going to
be morally considerable such that you
want to make sure that when they do
arise people are going to be able to
recognize them then you have to have
some kind of enforcement mechanism to
make sure that those design principles
are followed so we heard earlier today
that one thing we could do is we could
we could trust the AI researchers
wherever they're working on it that
they're going to be benevolent they're
going to design these things in and
they're going to take precautions but
there's lots of research I'm sorry
there's lots of research that involves
possibly emergent moral status
properties embryonic stem cells research
is like this and chimeric research where
you're mixing genomic materials between
humans and nonhumans in the research and
in these kinds of cases we do think that
there has to be some kind of oversight
mechanism some kind of review process to
make sure that that that if if issues
are arise then they're recognized and to
make sure that and this is to a question
that came up earlier that maybe there's
certain kinds of experiments that might
actually not be okay because the
experiments themselves or the outcomes
could be disrespectful of the moral
status of of the agents so of the the
research subjects so if if if we think
that it's possible that we're going to
have entities that during the course of
research are going to have capacities
that are going to give them moral status
and that those capacities are going to
merge throughout this process they need
to be recognized then we're going to
probably need some
kind of institutional oversight
mechanism to make sure that the research
is sensitive to this and that researcher
is designed in the appropriate protocols
and mechanisms of recognizing when this
occurs okay so then the question just
becomes how do you do this and in the
other cases we've seen that that a
committee mechanism is the best way to
do it and so that's the approach that
John and I were thinking about for this
and he's going to come up and talk about
the details about how that might work I
was told not the top mic camera here so
thanks Ron so if we assume that in the
development of artificial intelligence
sometimes they'll have capacities which
ground moral status and so therefore we
owe them consideration the question then
like Ron said is how do we make sure
that they're treated in a way that's
appropriate given that they have those
capacities and have moral status so so
what sort of oversight model should we
use to ensure their protection so
research subject oversight when it comes
to so there are different kinds of
oversight we might care about oversight
to protect us from AI but when it comes
to subject sensitive oversight oversight
that's designed to protect research
subjects we typically use what we call a
committee based model you have a group
of people with the relevant kinds of
expertise they're typically submitted
some sort of some form of a protocol
that describes the research it's gonna
be done they have a set of questions
they ask about that and then via some
mechanism they approve or disapprove the
protocol they have various different
powers common versions of this IRI are
B's institutional review boards anybody
who's had to deal with those they're the
bane of your existence I'm sorry then
the same is true for people who do
animal research that's we have a cuddler
aya cooks Animal Care and Use Committee
so this is a standard model and there
are reasons that it's the standard model
and there are good reasons why it would
be a good model in the case of
artificial intelligence research there's
a certain set of advantages it has with
the alternative the alternative kind of
model would be what we might call a
compliance model you have a set of rules
you have compliance officers they make
sure the researchers are following the
rules um but this is actually not a
really good model for AI research for
several reasons one is tied up with
different forms of epistemic uncertainty
when it comes to which capacities these
things are going to have
and how they're likely to develop I'm
gonna get into those a little later but
also because there are just many
different forms that AI research takes
you might have there's the machine
learning group and there might be the
brain simulation group and so you might
think that it's hard to design a single
set of rules that's going to be useful
in governing all these different modes
of research and also the technology
advances rapidly so you have this worry
about lag that if you set a set of rules
researchers are going to be slowed down
by having to comply with them and having
to eat them change when it's time to
sort of consider new ways of building AI
so the committee based model is more
nimble it's more adaptable it's less
likely to slow down research while still
being responsible to the ethical
challenges raised by giving rise to
morally salient capacities so how do we
how do we develop and structure robust
AI research subjects oversight
committees and that's what I'm really
want to focus on um here's sort of a
schema for developing oversight
committees so for subject sensitive
oversight so you identify some ethical
aims these are the important things we
want a committee to accomplish then you
identify a set of obstacles things that
are going to prevent us from achieving
those ethical aims and then you
constitute task and empower a committee
to resolve those issues so you say we're
the experts we need to resolve these
obstacles
here's the kinds of tasks we should
assign them we should got to design
protocols to figure out what kind of
information we need to give them figure
out what kinds of questions they should
ask and then give them a bunch of power
maybe not too much power you have to
negotiate these powers to see to balance
protecting research subjects when it's
appropriate and not slowing down
research too much and so I want to model
this show you how this sort of schema
works in a particular case so I'm gonna
talk you through how we can apply this
schema to an existing case of animal
welfare regulation so animal all
federally funded animal research in the
United States is governed by the Animal
Welfare Act and the way that that is
implemented is through these things
called aya cooks the institutional
Animal Care and Use Committee and here's
how sort of aya cooks fit into the
schema so what is the ethical aim so the
ethical aim as laid out by the law is
that we have an obligation to minimize
the suffering of vertebrate research
subjects consistent with realizing
legitimate research aims so it's not
that we can't cause animals to suffer
although maybe you think that's morally
appropriate that's not how our league
framework hasn't set up the law requires
that we minimize animal suffering
consistent with achieving legitimate
research aims so that's the ethical
obstacles there are some challenges
related to questions of how will this
protocol this proposed design cause
suffering what are the ways this animal
can suffer how are they likely to suffer
given the way things are set up and this
raises challenge that you need to answer
by appealing to someone who can tell you
what kinds of research subjects and we
suffer in which ways there are things
like so for example different rodents
suffer differently from different kinds
of captivity so I think I think that
rats mind or are stressed by not being
in group housing in ways that mice are
not I might have it backwards
and so you need to answer questions like
that which of these research subjects is
likely to suffer less you need to answer
questions about scientific legitimacy is
so not only are the animals going to
suffer but is this research worth
causing animal suffering to achieve
whatever the ends are um that question
is extremely hard to answer
whenever I assign my students a sample
research protocol they almost never
think the research is worth doing you
need an actual scientist tell you why
this little thing why these 50 my
suffering to answer this my case it was
always pigeons I was on a night I cook
for a while there was always a
researcher that wanted to know how
pigeons located targets on the ground
and the first question I ask is well
does this help us in any way is the
pigeon brain anything like our such that
this Vidal person answers no but but
then the scientist would say yeah but
here's how it contributes to this
broader research project and explain to
me why I should be okay with this and
then point me to the fact that the NSF
thought it was good enough so I should
think it's good enough to so we have to
be able to answer these research
legitimate questions and put these put
this research in broader context and
another set of challenges involves
biases and I'm not saying that
scientists are just gonna pass anything
for the iaccoca in my experience
scientists are very responsible members
of I cooks but of course they have a
bias towards minimizing the suffer of
animals lots hangs on getting research
done lots of money is in the balance
you could have worries about quid pro
quo schemes about one scientist
approving this because they might be a
member on the AIA coat or they might be
submitting a protocol next time when the
AIA cook has a different member things
okay so how do you address these
obstacles to achieve the ethical aim of
minimizing suffering well the way I cook
Sark imposed
they have at least three members
covering these four categories a
veterinarian a scientist a non scientist
and a non institutional member someone
that's not from the institution doing
the research the UC Lee Iacocca's are
much bigger they don't just have three
members there's not someone playing a
dual role some institutions University
Wisconsin for example as each College
that does animal research has its own
aya cook and there's a campus-wide I
cook and they're all they have 15 20
people and but you can see why those
that Constitution will help you achieve
overcoming those obstacles you have
someone that can live veterinarian they
can answer questions about how various
members of different species suffer and
under what context they'll suffer you
have scientists that can tell you about
the worth of scientific research and you
have members of the public and people
outside of the discipline to protect
against bias towards or bias against
animal welfare and so they're tasked
with they're given a set of protocols
they have this expertise they're tasked
with assessing the protocol to make sure
it's eves the ethical aim helping each
other to answer these questions from
drawing on their different areas of
expertise and then they have the
authority to approve the protocol or not
if not it goes back to the researcher
they try again
monitoring life during the life of the
protocol the veterinarian will typically
walk around the animal facilities and
they have the power to suspend the
protocol so there's some serious power
to make sure that we're achieving the
ethical ink so now when we transition to
the question of AI research subjects
oversight the question is how can we
sort of take this schema and apply it
and try to develop AI research oversight
to protect research subjects um so let's
just start with the ethical aims it'd be
nice it'd be really great if we could
just draw on the ethical aims of aya
cooks and say hey we're gonna do the
same thing we're gonna minimize
suffering consistent with achieving
legitimate research aims but there's a
problem with that we are not sure
exactly what kinds of capacities AI
research subject here and a have and if
they have robust capacities like us we
heard from Matthew and I think this is
consistent with what Eric tomorrow or
saying we're gonna owe them more it's
not appropriate to treat human research
subjects in a way that minimizes their
suffering consistent with achieving
legitimate research aims IRB protections
are much stronger than iacocca
protections so we need something that's
a little broader that's going to be
protective of the many different kinds
of individuals we could have given their
different levels I don't
a levels different types of moral status
they might have um so here's sorry
here's one stab here's one stab at the I
think a plausible stab at what the
ethical aims could be that would cover
this the variety of cases treat each AI
in a way that's commensurate with its
moral status if the if the AI has
cognitive capacities that are serious
similar to the cognitive capacities of
dogs we should treat it the way that
would be appropriate to treat a dog now
we might just adopt the AIA cook
standard and say what that means is
minimizing the suffering of that animal
consistent with research aims or we
might rethink what's commensurate
treatment given the type of being it is
if the AI ends up having human-like
capacities then we should apply a
different standard and because we may
develop things that are have totally
alien capacities to us we're gonna have
to think about what beings of that type
that have this cluster of relevant
morally salient capacities what types of
treatment our convention that type of
moral status or given those capacities
so that's I think the moral aim now the
the hard part areas of difficulty I've
divided the areas of difficulty into
three distinct parts I'm not naive
enough to think that I've covered them
all I'm not naive enough to think that
these categories have hard borders but
they're sort of useful ways I think
about these so consciousness challenges
so one problem that appears here that
appears I think less so in animal ethics
research is the question about the basis
of consciousness Paul Bogosian likes to
bring this up every third or fourth talk
and I've been really thankful for that
um we really don't have a good answer to
this question but it's an and that
doesn't matter so much in the case of
animals because they have behaviors and
physiology and they're tied to us via
evolutionary history so that we know
that those behaviors and physiology are
pretty good evidence that they have
capacities like ours if they have a
similar brain part this is very naive
science they if they have brains that
are very similar to ours and they behave
in ways that are similar to ours we can
expect that they evolved for similar
purposes so similar have similar qualia
you might be skeptical of that but our
inferences about minds that are like
ours and evolutionarily related to ours
are better than our inferences about
minds that are made of computer parts
that don't share an evolutionary history
and aren't physiologically like ours at
least not you can try to talking out of
that so that makes it important to
figure out like what is the basis of
consciousness
because that would actually help us
determine what capacities these things
have if you just if you knew if you're
an IT behaviorist this becomes very easy
but also very ethnically challenging you
can figure out oh this thing is
satisfying these naive behavioral rules
so therefore it's conscious right so we
can answer does it have these capacities
by applying a behavioral test if you
have a different view about the
basically the conscious basis of
consciousness you have to figure out
whether it has those bases but it might
be one of the most tractable ways to
answer which capacities that this a I
have is answering this question so
that's important but we also have a set
of empirical questions related to this
we might ask given that there's
different mechanisms for developing
artificial intelligence which on
different on different given different
technologies how likely are we to
realize those bases of consciousness so
if you're using let's say I don't know
direct brain simulation you might be hey
if this works we're pretty sure it's
gonna have these bases of consciousness
so that tells us something about its
morally relevant capacities if you use
an evolutionary algorithm you may have
no idea whether it satisfies the basis
of consciousness so you want to know
what's the probability of achieving
various bases of consciousness given
different kinds of ways of developing
any alright so those are some challenges
having to do with consciousness
epistemological challenges these some of
these are also related to consciousness
but we might be completely in the dark
about the capacities of developed AI
might be completely in the dark so then
let's say you do some research you
develop an AI and you want to try to
figure out whether it has moral status
we need a set of ways to test hypotheses
about for example alien intelligences
how do we come to make these inferences
what kind of behaviors should we look
for what kind of responses should we
look for so you need that and then
there's a set of normative questions one
of those normative questions is what is
the commensurate treatment given these
kinds of capacities like given that this
individual has these capacities what's
the appropriate way to treat it so
that's a straightforward normative
question that we we some moral
philosophers will claim to have settled
I often claim to have settled some of
them but we're gonna have to fight about
that in the context of designing AI
research oversight but there's other
normative questions like given that
we're not going to answer the
consciousness questions or the
epistemological questions in a
compelling way for everyone how should
we reason under disagreement and
uncertainty so the future humanity
Institute has a project for you how do
you reason under
cases where you have serious chance of
risk and high degrees of uncertainty and
you might think well you could be
morally cautious you don't want to harm
any moral agents and then maybe you go
the way that Eric and Marta do you just
don't do research unless you can be
really sure that it's gonna be you're
gonna be able to tell whether it's
conscious that will significantly slow
down research I suspect so you might try
to use some other normative principle
for how to make decisions in uncertainty
okay
so how do you address those obstacles I
just want to talk a bit about
constituting the committee and then I'll
be done we need experts from a wide
variety of fields
so to address the consciousness problems
we needed to talk to philosophers of
mind cognitive and neuroscience I didn't
put psychologists here but they should
totally be up here we're also gonna need
computer scientists and engineers
they're gonna be the ones so let's say
that the neuroscientist tell us hey if
you get a system that's functionally
like this it's conscious we're gonna
need computer scientists and engineers
to tell us whether that's been
physically realized in another system
right maybe the neuroscientist want to
answer the epistemological questions
we're gonna need philosophers of science
epistemologists but also I think we
should draw on ethology and biology
people who are equipped to make they try
to go out and find a new species and try
to guess what it's cognitive they don't
guess try to determine what its
cognitive capacities are so we want to
draw on them because they're good at
designing these kinds of inferences and
then for the normative questions we're
gonna need ethicists decision theorists
so it's a broad range of expertise and
that itself I think raises some serious
challenges first of all there are just
obstacles to interdisciplinarity that
Wendell Wallach was talking about before
we talked a lot about wanting to be
interdisciplinary but we're not but a
more serious problem is that there's not
there are not that many places in the
world that have that kind of diversity
of expertise and so you could worry
about this concentration of research or
like if you put these rules in place
that you have to be committee approved
to do this research you could worry
about a concentration of intellectual
resources in certain people being
excluded from engaging in these research
projects so to address that you have to
consider things that we don't do with
other committees which is decentralized
and distributed committees may be the
right model is to have there's a place
where these committees are formed and
they draw an expertise from lots of
different institutions and then
institutions themselves send their
protocols to these committees and get
feedback from them instead of having it
the way it is now with typical
committees which is every institution
has
and there are some remaining challenges
first of all funding you need say
they're gonna be administrative cost of
this they're gonna be cost to continuing
to develop developers think about what
should be in a research protocol there
are going to be challenges with tasking
and empowering how do you i've only I've
only hinted at how you solve one of the
problems who should be on the
committee's not what their job should be
and what Authority they should have
that's gonna have to be negotiated with
a our researchers who are gonna maybe
not like this probably not gonna like
this if they listen to their the people
have to deal with iock Oaks and IR bees
they're definitely not gonna like it and
then we saw that one so then we have
this other problem we have to secure
individual buy-in not just from
researchers that are doing the work but
from people that are willing to that's a
lot of work to review these protocols
but also from institutions and
corporates you got to get them to buy in
see that this is important or else
coerce them in some way so those are
remaining challenges thank you very much
I was wondering how we figure out what
systems fall under the scope of these
committees I mean the committees
themselves in all the in human and
animal cases have some subtle
distinctions decisions to make but we
have pretty straightforward rules for
figuring out which systems fall under
the scope I mean in one case humans and
the other case you said okay vertebrate
animals okay it's a bit arbitrary but at
least it's a it's a line it's very hard
to see what a comparable line or
criterion is gonna be for what systems
fall under the scope of an AI research
subjects committees at every AI system I
mean there is this and if you've seen
the website out there People for the
Ethical Treatment of reinforcement
learners every time you give that
negative reward signal there's a bit of
they could be a bit of suffering going
on someone's at least some ethicist have
to be looking into this and even if
skeptical about this it's very unfair
there's gonna be any bright line
anywhere to be drawn so do you have
ideas about how I thought about this a
little bit so a couple things to say
first on the AIA cook boxes there's that
the first box on our protocol says does
this experiment cause more than
momentary pain or distress to the
research subject that the boxes know
then someone from the AIA can just be
assigned to look at the protocol and
pass it off without it going through a
whole committee so you could have
something like that it's still not net
it's still not gonna do the narrowing
you need because you're thinking there's
gonna be tons of these research programs
which are AI research programs so what
do you do about that I think the answer
is well I'm willing to say this I'm
happy to leave it up to the AI
researchers because they're the ones
that oftentimes in their funding make
these promises about how robust their
machine learning capabilities are gonna
be and so I'm okay with hey if we fund
it and it's part of the research thing
that they think that it's gonna realize
robust cognitive capacities then then we
should evaluate that but if they admit
in their thing like we're just doing
this we're just doing reinforcement
learning we're not doing anything
different we're using traditional
computational components then it doesn't
go through a review so well I think we
make a judgement we come up with a
version of the check box and it's if
you're doing research where maybe what
we just we sit around we fight about we
say look no silicon-based computation
system of this type is ever gonna be
conscious I don't think that I don't
know if that's true but maybe we said
that so if your bot is this just silicon
based machine learning that gets check
box it doesn't go to the committee done
but yeah we're gonna have to sit and
fight about what exactly falls in the
boundary of what these committees look
at in full yeah
can I just add to that that this isn't a
special problem that would be to this
kind of case this comes up in IRB and
human subjects research as well right
there's the question of when you're
interacting with human subjects in what
ways or what other ways when does it
trigger in and we do give some
discretion to the researchers of the
people doing it but on the other hand if
if it's if it's a border case where it's
money it might be an over case and you
don't do it then there's a cost on the
other end so that's an incentive for the
researchers to be honest about when when
it's appropriate when it's not
so I was curious how how has buy-in how
is the buy-in process been enacted for
previous research oversight initiatives
like aia cook and how should and
shouldn't that inform this that's a
really really good question so the
buy-in process for IR B's and aya cooks
has been mandated by law which is nice
because you get by and just by coercion
but that is not true for escrow
committees so escrow Cooney's are a
really interesting case of the way so
escrow committees came about at sort of
the foundation of embryonic stem cell
research oversight which was at the
during the second Bush presidency and so
there were gonna be no government
regulations of this the only government
regulation was going to be we're not
gonna federally fund any research on
this insert a joke here about how the
good old days when Bush was the
conservative and don't sue me if you're
watching the live stream so esker
industry just got together they said hey
the public is concerned about embryonic
stem cell research we want to do this
research in ways that's going to be
acceptable to them as much as we can but
we want to reap the benefits so industry
and academia actually got together
collaborated and set these standards and
just opted to buy in so I haven't
studied that process enough to know like
what kind of incentives there were but
there was massive buy-in because
researchers really wanted to use stem
cell lines that weren't approved by the
government so they were willing to get
rid of the government funding to do this
research but they were sensitive to the
ethical concerns I don't know if AI
researchers I'm sure I'll hear about it
are is amenable to that but the s the
escrow case is a nice case that so one
of the bedrocks of human subjects
research is informed consent so I was
wondering you know why you sort of
didn't maybe touch on the idea of if we
did actually end up having AI that's
sufficiently similar to us they might
have moral status and then we might have
to ask them for voluntary consent to
participate in research but I think
that's right so the ethical aim is treat
subjects commensurate with their moral
status and so if the capacities end up
being the ones that are just like ours
and we think we deserve informed consent
as a constraint then they they're going
to get that too and so that raises a
problem that you might think that it's a
bad idea to spend money on research
where you might generate research
subjects will won't approve and then you
can't do the research anymore which is a
serious worry but it also speaks to the
design and constraint issue that Erich
and Marv we're talking about that if you
are intent if it is possible that you're
going to create this kind of entity you
might also have to ensure that it's
designed in a way that at that point it
could give consent or it could be
possible for it to understand enough to
give consent right then that's another
question at that point right then you'd
have to release it
whatever that means from the research
and and and and that and that's to the
question of does that mean you should
not start down this research path in the
first place or something yeah I have a
question about a certain PR issue that
could arise so what we have frequently
seen including AI research is that good
arguments get devolved into bad
arguments it's a certain instance of
Gresham's law that seems to be going on
now when applied to co-opting and
coercing industry into doing a research
how does one ensure that the argument
that the justification industry uses
actually corresponds with being on track
to creating a safe AI rather than some
similar incentive which will in the
limit still divert into horrible
consequences so we I'll just say this I
mean we refer to you know to coercion a
couple times by power in the state is
essentially what we were talking about
but there's other types of incentives
the embryonic stem cell case was an
example of a positive incentive and so
if you're talking about industry there
might be positive incentives I mean
they're already our industry is
interested in doing this in a way that's
not going to cause problems with you
know autonomous vehicles so they're
going to try to be proactive about that
right so trying to head off these kinds
of potential public relations problem
could be in a positive incentive for
instituting some kind of institutional
oversight to prevent it from happening
in the first place
when we get to human level III then
probably they will also want to be AI
researchers and do the research
themselves that's the question that
comes up Ron gets Ron gets excited about
this question um III don't know one of
the things I like about this
so like this relates to the question
about informed consent we have these
existing models of what the right way to
treat human subjects are but those are
actual compromises on the basis of hey
we're gonna do human subjects research
let's sit around and form some policy
about what's appropriate and so this
actually gives us a chance to sort of
think about in a new way what
commensurate treatment equals and how
informed consent plays a role against
other judgments and I think that that
whether we should allow AI on and what
role their consent is going to play is
going to be a function of those
discussions so I don't want to prejudge
it but I don't see any problem with it's
the last time we gave this talk someone
suggested that Watson there's already a
version of Watson that's like a
research-intensive version of Watson you
put it on research committees to help
you solve certain problems and they
asked about putting it and I don't have
any in principle objection to it for
sure I appreciate that you've you know
do you fleshed out this idea that's been
around for a while about whether or not
there will ever be AI are bees there's a
few questions that come to mind one is
you first of all you've talked is that
they're two different models are
compliance and an IRB model but actually
those are the same model in the way that
they are instituted in the United States
so the compliance officers prepare the
people for the IRB and prepare the IRB
for what the issues will be that they
will have to make a decision on but I
think the more important question was
alluded to by by Dave earlier and that's
really when does this kind of process
kick in yours has presumed that we have
a very advanced AI and therefore we get
into these questions about informed
consent or what its level of cognitive
capabilities are and therefore whether
it would be an appropriate subject and
therefore what level of you of rights
that would have
but there's a prior question that - that
about when do we start getting to an
area which we should even be concerned
about that so if we're gonna hook up
biological material to an AI for example
in order to check out questions of
emotions and pain regardless of its
cognitive status if it can give pain
responses should we consider that
something unethical to do a priori or
might that be as with animal pain
appropriate under certain circumstances
and then the other thing you alluded to
a little bit earlier was what about
cognitive pain just on the compliance
question that's I mean that's that's
right I mean there's there's a
compliance component to the committee
set up what we were had in mind is a
strictly compliance model that are
sometimes used in certain types of
research labs with hazardous materials
or other things we just have to you
don't have to go in front of a committee
you just check a list and then we
inspect the lab to make sure the the
things are so so that that was what that
distinction was about yeah and on the
other point I'm not exactly sure how to
answer the pain question but like when
do we start evaluating this if if we're
really so confident that current AI
research and machine learning techniques
are not gonna raise any of these
problems then the cost to having that
sent to a committee that didn't just
that just says here's why we don't think
we should be subject to evaluation um
you read their reason to say okay and
then it's over
it doesn't seem to be that much of a
cost to starting early um especially I
mean the thing is whenever I go to give
this talk or talk about the stuff I do
feel the silliness of it but at the same
time like Nick said yesterday this stuff
is gonna seem silly but then all of a
sudden it's gonna become controversial
and so I have that worry and so if you
trust what people and know about AI say
it just doesn't seem like it's that
problematic or that sort of silly to say
hey let's start thinking about this
about current AI just have a researcher
tell me why it doesn't matter that much
or why I'm not likely to we're have
ethical worries about this I check a box
and we're done so the cost seems minimal
to me to doing it broadly</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>