<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DeepMind - The Role of Multi-Agent Learning in Artificial Intelligence Research | Coder Coacher - Coaching Coders</title><meta content="DeepMind - The Role of Multi-Agent Learning in Artificial Intelligence Research - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DeepMind - The Role of Multi-Agent Learning in Artificial Intelligence Research</b></h2><h5 class="post__date">2017-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yE62Zwhmzi8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon great pleasure to welcome
you to the next in our series of Turing
lectures we've had quite a few lectures
from experts in government from
academics from all walks of life
I'm Andrew Blake the Institute Director
and it's my great pleasure to introduce
today Torrey grapple from deep mind
I've known Tory for a long time he
trained originally in Germany as a
physicist but rapidly saw the light and
went to the Technical University of
Berlin to do his PhD in neural networks
with Obermeyer and worked on large
margin classifiers and rancors he then
did a postdoc at ETH and another postdoc
here in London with John Shore Taylor
the renowned machine learning researcher
before joining the Microsoft lab where I
was also at the time where he did some
really very notable things one of the
one of the first things was working with
Ralph her brief and Tom Minka on a true
skill where they used approximate
Bayesian computation to solve a very
tricky open problem at the time which
was how to do ranking of game players in
multiplayer games and you know the
problem is how you adjust the the
ranking of a player at each successive
game so that in the future they'll get
good challenges they'll meet players of
the right standard to make great games
so that was the trueskill system 2006
then he worked on some rather
groundbreaking systems that were very
influential in the company on how to
match people doing searches with
advertisers that want to present
material in 2013 he was involved in a
very interesting and rather provocative
study where he along with some
researchers in Cambridge University
showed people just how much there
about their personality was revealed
from their facebook profiles and just
quite ordinary things that you put on
your facebook profile machine learning
can show you actually says quite a lot
about you also in 2013 I found a quote
from Tory where he said that he had a
passion for the game of Go and for the
quest of developing a go engine that
plays as well as the best human players
and this was a remarkable prophecy
because a few years later working at
deep mind he and his colleagues did
exactly that in the remarkable work that
was published in nature last year that
undoubtedly doubtless you've heard about
the alphago work so a very interesting
person Tory and therefore I'm completely
confident that this afternoon's lecture
on multi-agent learning is going to be
fascinating and I look forward to very
much thanks
good well thank you very much for the
kind words Andrew and for inviting me
here are we going to talk about a topic
that I'm very passionate about the role
of multi agent learning in artificial
intelligence research and if I'm
successful today after the lecture you
would hopefully agree that it is crucial
to look at multi-agent systems in order
to make progress in artificial
intelligence the work I'm going to
present is all in collaboration with the
fantastic team at deep mind so maybe
just to start about deep mind you may
have heard of this company we have this
beautiful to words very ambitious
mission of solving intelligence and we
would like to do that to make the world
a better place and it is really these
two things that have attracted me and
many other people to work for deep mind
and it is in this context that I would
like to talk about the role of multi
agent learning so we have the notion of
intelligence here and it might be
logical to ask what is intelligence if
we want to somehow solve it artificially
right and of course this is a rather
open question and there's probably no
one definition that will capture this
very well but a working definition that
we're working with a deep mind is that
intelligence measures an agent's ability
to achieve goals in a wide range of
environments and I hope it won't shock
you when I put this the most complex
formula of the talk right here in front
of you this is a the attempt of a formal
definition by a ligand Hatter of
intelligence and I would like to briefly
introduce you to this and explain a few
things about our methodology based on it
so the idea here is to measure the
intelligence of the policy PI and were
defining it as the value that that
policy PI achieves in an environment mu
and we average over all possible
environments mu in some set of
environments or little world's if you
like and there's also a weighting factor
which basically says that the weight is
2 to the minus the Kolmogorov complexity
of that environment and what that means
is that we really want to put more
weight on the simpler environments than
on the more complex ones there's a
technical reason this thing really only
converges if you have some kind of
intelligent waiting there but also it
emphasizes the idea that we want a
system that is general and to start to
be general you want to start with the
simplest tasks and have a system that
can cover a lot of or many most of the
simple tasks can then move on to more
complex tasks and that's what's
expressed here but this equation also
gives us a nice methodology because it
essentially says that we want to we want
generality in our intelligence
intelligence isn't just solving one
specific thing but it is solving a broad
set of tasks and for example you can
think of this as an example the set of
50 Atari games that people from deep
mind were able to learn with DP
reinforcement learning and then this set
would be those 50 games and the idea
would be to have a policy that can can
do well in all of these games but
eventually of course we would like this
environment to encapsulate many more
things so the most complex and
interesting environments that we can
find are multi-agent environments
because they're not just static and
follows a set of rules but they're
responding and that's really what
motivates our work in multi-agent
systems more specifically I would like
to emphasize three points here one of
the the first one is that multi ancient
thinking can really inspire interesting
architectures because often it's a good
idea to break down a problem into sub
components sub agents and that will
yield a more flexible system than having
a single monolithic system the second
idea that motivates this is that surely
intelligence didn't arise in isolation
there wasn't just one intelligent being
that suddenly became intelligent there
were groups of agents of animals later
of humans that through their interaction
came to be intelligent and finally if we
want to create artificially intelligent
agents that are useful and beneficial in
this world then we they will need to
understand agency they will understaffed
understand what we wish them to do and
they need to understand how to interact
with all the other agents in this world
be that people be that other AIS or be
that organizations for example so if we
dive into this the question of multi
agent designs of course has been
explored before for example by Minsky in
a society of minds but you can also
think of organization as a multi agent
architecture for example the US
government is composed of many
institutions that in turn comprised
people that fill certain roles and so on
so it really is a multi agent design and
there are certain advantages to this
kind of design typically multi agent
designs are more robust than single
monolithic designs because if there's an
agent failure that can often be
compensated they're scalable in the
sense that you can add more agents to an
organization or to an architecture and
and have it grow in that way you don't
need to redesign the whole thing and you
might also be able to reuse or
reconfigure the constituents in
different ways there's also challenges
of course one of them is that you might
for your system have some kind of global
goal yet the agents by to only have
local actions and local visibility of
the problem and so that's one of the big
challenges to overcome there's also the
problem of incentives and credit
assignment if you think about an
organization how do you incentivize
people and similarly if you think about
the multi-agent system
how can you give those agents the right
reward function such that together they
satisfy the goal or they reach the goal
that you would like the system to
satisfy finally if we think about
learning problems then of course if you
have multiple agents interacting and
each one of them is learning at the same
time that means that the environment
seen by one agent is constantly drifting
because the other agents are learning
and that's true for every one of those
learning agents so that also poses
challenges now the second point that I
made is that we live in a multi agent
world and there are just so many multi
agent aspects to it if you think of
these different systems that all involve
multi agency if you think about these
tasks here agents need to compete they
cooperate they coordinate with one
another they communicate they have to
predict each other's actions in order to
be successful and so on and just to give
you an example of how deeply embedded
these ideas are in our own minds I would
like to show you this little video it's
really just a bunch of geometric figures
moving around on the screen but if you
try to interpret what's happening I
think you will get the idea
so the story goes on and on
but I think there's probably nobody in
this room who did not project some kind
of agency on these simple geometric
figures that's just how we interpret
these these things because our minds are
made to do that this is a very early
study 1944 from Heider and Simmel so
finally there's this idea that human
intelligence or intelligence in general
certainly didn't arise in isolation and
there's that there's a cumulative
cultural effect at work here as well so
there's a competitive side to this say
humans would have competed with other
species within our species with other
tribes individuals would have competed
and so all of these this competitive
pressure created ever-increasing task
difficulties to which the mind needed to
adapt similarly there's of course the
importance of cooperation the other side
of the pole where the whole evolution of
culture can be understood as also a
cooperative phenomenon where people were
working together where groups of humans
were successful because they were able
to more effectively work together
leading to the beginning of institutions
of states and so on and and there's a
very fascinating direction I will not go
into this more deeply of cumulative
culture because if you think about it
there's not just the intelligence of the
individual that we need to approximate
if you like or to build but if you put a
human being into the woods alone what
can they really do it is all the
cultural context or the inventions that
have been made before the effect that we
can share work and special specialize in
certain tasks that that make us so so
powerful in this world ok so one thing
about this cumulative culture that I
wanted to mention because I just find
that so fascinating is that really the
things that
people discovered in the past in this
example certain mathematical fields or
ideas that happened over a long period
of time as you see here thousands of
years from the past into the present and
if you then look here at what age these
ideas are taught to children then you
see here this this linear trend so the
older an idea is the the earlier we
teach it to our children because these
ideas build upon one another and this is
just one one view on this idea of
cumulative culture that so powerfully
fuels our living on this planet good so
for the remainder of the time I would
like to talk about two topics the first
one is about one of the poet's learning
to cooperate and the other one is about
the other pole learning to compete and
the first one here is work with Joe Lee
beau Vinicius Lombardi Martin Lang - and
Janos maretskiy
and it deals with the notion of
sequential social dilemmas so let's dive
into that getting people to cooperate
isn't always easy and the way we can
study this is through the notion of
social dilemmas so the idea is that
social dilemmas are situations where any
individual may profit from selfishness
unless too many individuals choose the
surface option in which case the whole
group loses out I think we've probably
all been in this kind of situation
before and these situations exposed to
expose the tension between collective
and individual rationality because as a
group we would want to do one thing but
as an individual we might want to do
something else and there are a number of
examples here
maybe best-known the tragedy of the
Commons public good sharing Free Riders
you know people are going on the
Underground without paying resource
depletion the voter turn our pollution
all of these are problems that can be
described
this type of framework and that maybe we
can understand better in this type of
framework and and find solutions to in
the systems that we design and the main
question that we ask in this field is
despite all these obstacles essentially
human selfishness if you like can
cooperation emerge and be stable so one
way this has been studied in the past is
the idea of a matrix game social dilemma
and the idea here is to really get to
the Drosophila of this problem the
simplest possible game that exposes
these ideas and these games are
typically two player games a given
player can either cooperate or defect
and this there's a role player and a
column player here and this is the
so-called payoff matrix where this
number R here tells us for the row
player how much they get in terms of
reward if they cooperate and if the
other player also cooperates so we call
this here the reward this is the penalty
for mutual defection and then we have
the the sucker payoff which is the if
you cooperate with the other agent
effects and the temptation which is the
payoff that tempts people to defect in
the first place and there's certain set
of inequalities between these numbers
that need to hold in order for this game
to represent a a social dilemma and
there are these two technical terms that
will be important in the future one of
them is greed it is if this temptation
is greater than this reward then players
will be tempted out of greed to defect
and there's also this technical notion
of fear which is if this punishment here
is greater than the sucker payoff then
you would be tempted also to to defect
but this time or the fear and not out of
greed ok so that's basically the anatomy
of the kind of problem we're looking at
what are examples of such games here's a
game called chicken I will not go into
the detail
but the idea is basically two cars are
driving towards each other and either a
player could evade and go to the side
and if if neither of them does that and
an accident would happen but if they
both but if they that's this cell here
but if they both swerve then everything
is fine but it's even a little better if
you could go straight and the other guy
avoids you and so that then gives these
undesirable Nash equilibria that this
game tends to go towards and here's
another one it's called stag hunt the
ideas that people could cooperate in
order to find a great reward but they
might there might be a smaller reward
that they could go for that tempts them
to go there and they might do this order
fear that the other agent might do it
and that they would be better off to do
it as well if in order to avoid avoid
this zero and to get that one so this
one is an example of greed this is an
example of fear and the last one
combines the two and this is probably a
game that many of you will know it's the
well-known prisoner's dilemma where we
reserved them by both greed and fear so
essentially there are these two
prisoners and if they work together they
can they get to a positive outcome
because they don't act as witnesses
against each other if they both act as
witnesses then they will go to prison
both for a longer period of time but
there's this temptation to betray the
other one in the hope that they will
cooperate and and that's expressed at
least off-diagonal elements and so this
is particularly delicious as a game
theoretic scenario because there are
these two forces greed and fear that act
upon the agents okay so much for that
but this system as simple as it seems
these ideas have been used across many
fields to look at how cooperation arises
and there have been different studies
for example
how norms might arise within such
systems what happens in social networks
the idea of reciprocity how can you in a
repeated game make the other agent
cooperate and so on and it's been an
incredibly successful model however
there are severe limitations to this
model and of course it is mostly this
one-shot nature of the game that's not
really how our life works right when
agents make decisions these are
typically temporarily extended it's not
just a single decision it's a sequence
of decisions cooperation and defection
which are just single labels in this
game really our entire policies that you
need to play out in order to implement
them cooperativeness may also be a
graded quantity it's not clear that
there's just cooperate or defect there
could be behaviors that are in between
these two then if you model this as just
an simultaneous decision between the two
players you're missing out on all of
these behaviors where you can first see
what the other seems to be starting to
do and then adjust your own behavior and
so on so there's really a much richer
dynamics in these systems involved and
finally usually there's also situation
of partial information where the agents
do not have full knowledge of what the
other agent is doing and so the idea of
the work I'm presenting here on
sequential social dilemmas is to address
these shortcomings and the idea is
really to capture the essence of these
real world social dilemmas but maintain
this mixed motivation in which agents do
not have a clear path towards
collaboration or competition but somehow
need to find their way between in this
tension and the methodology that we use
is for these SSDs the sequential social
dilemmas is that we have we define games
sequential games and we train agents
using deep reinforcement learning to
behave within these games and to learn
to optimize some kind of reward and the
algorithms were using I will not talk
about them in
detail are very similar to the ones that
were for example used in the study on
atari games the dqn algorithm and we
will then be interested in using this as
a model in the social science sense of
how these learning rational agents
interact within these environments and
we will draw conclusions about about
such agents okay so here are two of the
games that we're studying studying here
the left one is called gathering and the
right one is called Wolfpack and the
idea of gathering is that there are two
agents and they're really just a blue
and a red pixel here and these agents
they get reward when they eat these
green apples and they can also attack
each other with this beam which might
take out the other agent and the other
agent that needs to pause collecting and
that gives some time for the for the
tagger to get more apples but there's no
reward for tagging you only ever get
rewards for eating these apples the
second example that we're going to use
we call Wolfpack and the idea is that
these red agents here you can think of
them as as wolves and they are trying to
capture this target here which is moving
about and the idea is that when they
capture it together they will give get a
higher reward because then it will the
the it will not be scavenged by other
animals because they're together whereas
if only a single wolf gets the target
then the reward will be less and these
two setups or games if you like they
differ in a crucial point that we'll
explore a little later because in this
game is actually easier to cooperate
because you can just ignore the other
agent essentially and just collect
apples whereas in this game it's easier
to defect it's easier to go for the prey
alone rather than to coordinate doing it
together
and we'll see the consequences of that
but clearly this is something that can
only be expressed in this scripture and
framework and would not be expressible
in a simple matrix game now in order to
make the connection to to the previous
literature we thought it was crucial to
see if these games actually correspond
to these traditional game theoretic
views on social dilemmas and so we took
for example The Gathering game here and
we did what is called a an empirical
game theoretic analysis of it we picked
two policies one of them cooperative and
one of them defecting and we let it play
against two other cooperative and
defecting policies to fill in a little
square matrix with the expected rewards
and that tells us how well does a
cooperative policy does do against
another cooperative one how well does it
do against a defective one and so on and
then we can analyze this little matrix
in the game theoretic sense and figure
out if it corresponds to any of the
known game theoretic social dilemmas and
what you see here is basically that for
gathering yes it is the case that there
are quite a few instances in which this
game corresponds to a prisoner's dilemma
and for the Wolfpack game we find
instances of the prisoner's dilemma as
well as the stag hunt game and the
chicken game in different configurations
of this game so we have a direct link
between this richer world in which we
can look at the sequential dynamics and
the matrix theoretic world where we can
apply standard tools from game theory
now I want to give you just a flavor of
what kind of result you can get here
because now because we're in this richer
learning environment
not just in a simple environment where
we pick one of two actions we can study
different factors that influence the
emergence of cooperation and so if we
first look at the first row here this is
the gathering game the one way you
collect these apples you
see that for example if you change the
discount factor of the reinforcement
learning algorithm the one that
determines our long term your planning
is that for longer term planning this
actually leads to behavior whether it's
more tagging in the in the action when
you increase the batch size you give
them longer memory it leads to a
decrease in the amount of tagging that's
happening and similarly in the wolfpack
game here we are looking at the lone
wolf capture rate so how much these
wolves have a tendency to go capture a
low and rather than work together we
find the same dependencies if we have
the higher discount rate then there will
be more lone wolf captures and if we
have the lower one there there will be
less and similarly for the Badger size
so here these games behave in
qualitative very similar ways with
regard to these learning algorithms but
if you look at this last plot you'll see
a difference because now here we're
varying the network size essentially the
computational capacity that we give to
the neural network in order to learn the
policies involved and here you see blue
is the larger network size and red is
the smaller network size so the the
agents with the larger network size have
the capacity to represent more richer
policies and you can see here that in in
this case this leads to to a higher
level of tagging for the larger network
case in this game of gathering but in
the other game the higher network size
actually leads to more teamwork and less
lone wolf activity so this is just an
example of how you can now study what
impact different aspects of the
environment or of the learning algorithm
have on the emergence of the cooperation
in these types of games
so just to conclude this part part of
the talk the these sequential social
dilemmas clearly have a lot more
interesting dimensions that we can look
at and in particular if we look at
learning because we're studying these
learning algorithms that we put into
these systems the interesting thing is
that these algorithms do not just learn
how to make those decisions they don't
need to learn how to efficiently
implement them so one of these wolves
cannot just say I will now hunt together
with my other wolf but they have to
figure out a way of actually doing it
and learning it in the presence of the
other wolf that's the second thing
effective cooperation policy is really
much easier to learn if the other agent
also cooperates right if they don't even
try to cooperate then it would be harder
to learn that and that's a phenomenon
you can only study in this situation
another point is that sometimes there
are coordination subproblems so we might
all be agreed that we want to work
together but now we need to figure out
how to do it together we need to
coordinate our actions and and that can
be studied in this framework as well
there's also the idea that there could
be different implementations maybe
there's different ways of working
together in order to achieve a goal and
I'll give you an example here from our
Wolfpack game which is the idea that
there's really two ways of for these
wolves to capture the target together
one of them is that they always stick
together and only when they're near the
target will they then surround it and
the other one is that as they both go
separately and when one of the Wolves
finds it it will wait for the other wolf
to come and then they'll capture it
together so there's different ways of
implementing cooperation and this
framework allows us to capture that okay
so we've learned how to cooperate and
also we have learned something about how
to cooperate and the second vignette
here will be about the question of how
we can learn to compete and again of
course we're talking about the multi
agent problem in this case
two agents a black player and a white
player in the game of Go and this is
joint work with David silver edge of
Wong and the fantastic alphago team at
deep mind so the game of gold
I actually wished I had an hour now and
could all sit you all down with little
go BOTS and teach you how to play this
game can I just have a show of hands who
actually knows how to play go see we
would even have enough teachers to do
this we have to try this next time so
now I just have to convince the rest of
you what a fascinating game this is so
the game of Go is over 3,000 years old
there's an estimated 40 million players
in the world and there's a staggering 10
to the 170 different goal positions now
we often say that's more than the atoms
in the known universe but the truth is
and I know this audience can take the
truth if there was a universe for every
atom in the known universe it would be
similar to the number of atoms in that
collection of universes that's how
complex it is okay so why is the game of
go so difficult what makes it so hard
it's actually not only difficult for
computers it's pretty hard for humans as
well well the most obvious point is
maybe the game tree complexity of the
game because in any given position
there's so many different moves you can
make in the game of code it is played on
this 19 by 19 grid with black and white
stones and so the first stone that black
places on the board which is empty
there's 361 different moves black could
make okay there's some symmetries but
roughly and then why it can make one of
360 remaining moves and so on and that
leads to this huge complexity so the
search space is huge and it is difficult
therefore to just enumerate all the
possibilities and do a min/max search
which would be the standard procedure
for smaller games the second point is
that it's also incredibly difficult to
eval
a given go position and say if it is
favorable for black or white this is
particularly obvious for beginners when
they see a physician I think but even
from for advanced go players it is a
very difficult skill to do this just to
give you an idea about the game space
complexity again this is an illustration
of the complexity of the game of chess
and there's maybe an average of 20
different moves at every stage that
needs to be considered at the game of
chess and then if you compare that to
the game of Go where there's you know
300 different moves to be considered
that's just vast and it's this vastness
of the game tree that really motivates
our approach to this problem and the
idea is that we want to use neural
networks and train them to reduce the
complexity of this search and we do this
in two ways
one network we call the value network
and it takes a go position and it gives
us an evaluation of how good that goal
position is from from blacks point of
view or from whites point of view
essentially something like a winning
probability and the architecture of this
neural network is that of a multi-layer
convolutional neural network I'm sure
you're familiar with the basic idea
these neural networks take into account
translational invariance of the of the
of the inputs by having these little
windows that they effectively shift over
the input and they have shared weights
and this is propagated up all the way to
obtain the evaluation so in other words
it's really a simple mapping from an
input the image if you like or some
representation of the board position to
a number between 0 and 1 that tells us
how likely it is that black will win ok
the second ingredient is what we call
the policy Network and the policy
network is also a mapping that takes a
position as input but as output it gives
us a probability distribution over all
the possible moves on the board and so
what this represents is in some sense
the intuition of the goal player when I
as a go player look at the board
I wouldn't consider all of the moves as
being equally plausible I would have
some idea that some moves would look
better than others and that's what we're
trying to capture in this policy network
and so we train it on pairs of positions
and moves and fortunately go players
often record their games and if you have
a recorded game then you have many pairs
of positions and moves played and you
can learn this as a multi-class
classification problem effectively so
how do we use these now coming back to
this vast game tree that we were talking
about earlier which has a fan-out of
roughly 300 so this is really just to
here because of the limitations of the
of the illustration here so we have this
vast tree that we would have to expand
in order to arrive at the truth of the
best move and now given these two neural
networks we can reduce the depths of
this search because while without those
newer networks we would have to play to
the end to evaluate if black or white
wins
now with the value network we can
evaluate much earlier if black has an
advantage or white has an advantage and
hence we can save all the time that we
would need to search this vast space
down here now at any move we also don't
want to look at all the possibilities
and that's where the where the policy
network comes in because for a given
move we might already have a pretty good
ideas of what the good moves are and we
can effectively make the number of moves
that we look at the branching factor
much smaller using the policy network so
a lot of work in this project then has
gone into training these newer networks
and I just like to show you how this
works and our starting point which was
absolutely crucial for this project was
a set of human expert game records and
their online servers that where people
play a lot of go and they logged the
games and we were able to tap into that
resource so we have a lot of game
records and the first thing that we did
we
we trained the supervised policy network
simply as a classification problem
position is the input move played by the
strong player is the output we learn the
mapping because it's crucial to have
generalization to be able to have a new
never seen position and to be able to
find a good move for that when I first
joined deepmind
Dave silver invited me to play against
the very first version of these trained
policy networks and I thought how
difficult can this be it's just in your
network right but you can imagine
surrounded by my new colleagues the
first few moves seemed easy but slowly
my position started going down the drain
and I will never forget the many friends
I made that day everyone knew me by then
right he's the guy who lost they'd go
against the neural network yeah ok now
comes a crucial part here because we're
looking at the multi agent scenario and
what's happening now is that we can take
this now very fast policy network that
we've already trained and we can let it
play against itself and because it's
been well trained we can we can now
produce relatively high quality games
and we can do it fast we can create a
lot of them and we can do reinforcement
learning on these now because we can see
for a given game if black or white wins
and then we can reinforce the moves that
led to the win and we can the moves that
led to the loss and in that way train
the system by self play and the beauty
here of course is in the multi agent
sense that we can really only do this
because the system can play against
itself and as it improves both sides
improve and so there's this beneficial
improvement through the self play
process so we then generated a lot more
soft play data I think on the order of
30 million games and then we were
finally in a position to Train the value
network because now if you have games
you can extract a position from a game
you
look at who won black or white and you
can now train this system again simply
as a classification problem is a
logistic regression problem to predict
what the winning probability in a given
situation is good let's look at a few
more details of how this training works
so for the supervised learning of the
policy network we have this twelve layer
convolutional neural networks we had
roughly 30 million positions from from
human expert games we filtered those to
get games from really strong players of
course we didn't want to imitate weak
players and then we're really just
maximizing the likelihood by stochastic
right in the sense so what this
basically is we have the probability of
making a move in a given position
parameterize by Sigma and what we're
trying to do here is we're trying to
adjust the parameters of the neural
networks such that it becomes more
likely that it would play the moves
observed in the training sample so the
second thing here is the reinforcement
learning of policy networks that's the
thing that I characterized as self play
so here again we use the same
architecture but now the the network's
is playing against itself and we call
the outcome of this game of a given game
set and so we're now waiting these
updates by this outcome so if black wins
the game then we will give more weight
to the black moves and we'll make the
the right moves that led to a loss will
make them less likely and so this led to
it to a very strong policy I think this
was the one that I lost against I have
to get myself some credit it wasn't a
purely supervised one and finally the
reinforcement learning of value networks
the idea here is that this is the value
network it represents the value of this
position s and this power
by theta and the nice thing is that we
know the game outcome because we have
the full game and we can use that game
outcome as the target and we can adjust
our parameters such that the value
network is more likely to give the
correct prediction of the game outcome
you can see here that many of these
things were quite compute intense and we
were fortunate to have those resources
available to us and this evaluation
function that we trained here was really
the first strong evaluation function
that has ever been devised for the game
of gold in the past they had been hand
attuned our people had tried to train
them in very simple ways but this
evaluation function really worked it it
can predict relatively accurately how
who is favorite in a given goal position
ok so now we have these two neural
networks and we need to understand how
we can integrate them into the search of
course the policy network alone would
already be able to just play because
given the position you can sample a move
from it or take the move with the
highest probability and it'll make a
move but that's not and we can do better
than that by combining the policy
Network the value network and Monte
Carlo tree search and just to explain
quickly how that works at every edge in
this tree that we expand we're
collecting information about how how
well this move did and that's a
combination of this queue and of our
prior which is provided by the policy
network so we have some a priori belief
of how good the move is but we will also
use evidence from how well it turns out
in the tree in order to see how good it
is and then we pick for our it for
expanding the tree the sum of these two
terms to figure out which position to
expand next and that gives us a next
position here and the first thing that
we do is we evaluate the policy network
on it to find the next edges or the next
possible moves that could be plausibly
made or
experts would made in this particular
make in this particular position we also
evaluate the value network on it in
order to figure out how good this
position actually is and we do a third
thing which is we play a random roll our
to the end of the game and record if
black or white wins that game and this
goes back to the Monte Carlo tree search
methods that had up to this point being
the strongest contender for producing
the best go programs and which had led
to two strong amateur go play okay so
we've now expanded this tree we've done
these rollouts we've evaluated our
networks on the resulting positions and
the last thing we need to do is we need
to back up all of this information in
order to figure out which of the
available moves in our root position is
now the one that we want to play the
best one and it's really a simple
averaging exercise in which we propagate
this information up the tree to figure
out what the value of this position will
be versus that position say to make that
decision at the root of which move to
play and that's really all there is
Monte Carlo tree search combined with
the value network to do early
evaluations and the policy network to
avoid expanding nonsensical moves but to
focus the search on the plausible moves
good another important element of such a
project of course is evaluation and we
put a lot of energy into good evaluation
and in order for you to understand
what's going on let me explain how you
evaluate the playing strength of go
players let's you typically done on this
traditional q-dance scale so at the
bottom here you have the beginners the
Q's this is similar to to a system as in
karate for example and as you become
better you will then advance to the
rundown level which is the first master
level and that goes up all the way to a
nine done and then for professional
players there's an extra scale from 1 P
to 9 P which are the professional done
and that's roughly here so that's the
scale we're using and we also use an e
low scale because that can be more
easily evaluated quantitatively and now
in some sense for going from right to
left here you see the history of
computer go because at the beginning
there were programs like Google which
try to combine search with some
handcrafted heuristics and other other
ideas but never really took off and
never reached a decent level here then
there were the first Monte Carlo tree
search programs for example Fuego or
Apache that eventually did break through
the one down level which is this strong
amateur level or maybe not that strong
but ok amateur level everyone down
that's where I'm bit careful and then
there was this new generation of
programs based on Monte Carlo tree
search Zen and crazy stone that reached
really strong amateur playing strength
and this is the level of of alphago as
we published it in nature the beginning
of or last year and you can see that it
is quite a bit above the computer
programs that were available at that
point and reaches into into the
professional than levels here and you
have to imagine that this version v13
that beat the other computer opponents
494 out of 495 times so this is this was
seriously better than those previous
programs and then later when we were
preparing for the match against least at
all that you may have heard about we had
this V 18 version which would then beat
the earlier version v 13 by giving it
several handicap stones these are moves
that the black the weaker player can
make on the board before the stronger
player even engages and
so this program was this version was
considerably stronger than that one now
the problem of course is that these are
just internal evaluations that we're
doing and really we needed validation
against humans as well and so whereas we
had this evaluation against these
programs and by self play what we were
missing is an evaluation against humans
and that's where our european go
champion fenway comes into the story and
we evaluated alphago against fun way who
is a too damn professional player and
the strongest player in europe and in
this match alphago beat fun way
5:04 anyway was a fantastic opponent he
later joined the alphago team to help us
improve it even further because he was
had just developed such a deep
understanding of of the problem and of
alphago in the course of this match but
at this point he definitely didn't want
to lose and it was very it was very
exciting and then finally in March 2016
we felt ready to challenge maybe the
strongest player of the game of Go
currently Lisa doll he's a South Korean
player and he has been dominating
tournaments over the past 10 years has
an incredible track record you can think
of him as the Roger Federer of goal you
know that kind of consistency and and
style and so we had this very exciting
match in March 2016 where alphago won
four to one against ISA doll which was a
beautiful result and cost us a lot of
sweat as you can imagine because when
that one game did go down it it was very
exciting for us of course we also feel a
great deal of respect for these players
and so we in some sense we were also
happy
that that they were able to score this
success but on the other hand you can
imagine that when you work so hard on
the project that it's it can be can be
very exciting to see this kind of thing
unfold okay and as a result by the way
alphago then received the official 9 dan
professional certification from the
Korean Association so I just want to
there were many nice and interesting
moves being made in this match and some
people even argue that some sense of
creativity of innovation really emerged
from from alphago's play and certainly
the professionals that we talked to our
really excited to learn from alphago and
and figure out new things about the game
of go here in this position here at game
2 alphago played this particular move
here and it was really quite interesting
to see we had a professional commentator
and he he saw the move he put the stone
down said oh no this probably wrong this
can't be true and so he then looked
again I said well no that is actually
what alphago played this move here and
he was a bit puzzled and eventually
through analysis he began to understand
what a profound move this is and it's a
bit hard to explain this here but just
to maybe the idea you see in go you can
work on the third line and if you have a
row of stones then this territory that
you make counts towards your final score
and it's a very good thing to do you you
get territory and you're making you're
using the edge of the board to get it
now you can also play on the fourth line
these stones are examples of that one
two three four and that gives you
influence towards the center and that's
also considered to be a good thing maybe
you can build a big territory in the
center but playing such a shoulder move
as we call it on the fifth line is very
unusual because you should be afraid
that white builds a wall like
this and gets a lot of territory on the
side and so go players would naturally
reject that move and say that can't
possibly be good probably because their
master told them that it can't be good
which their master told them and so on
so here comes alphago placed this move
goes on and wins the game and it was one
of the many innovations that that I
think human players are now picking up
from alphago to improve their own games
more recently we had a little adventure
because we wanted to launch some test
games online to see how much progress we
had made since then and so I Jeff Wong
from the team he played 60 games online
against different professionals mostly
the top professionals in the world most
of them have online accounts and they
also play they go online on go servers
and there's were they were relatively
short games with 20 seconds per move but
the remarkable thing was that alphago
under the name of master and magister
ended up winning all 60 of these games
and whereas at the beginning people were
didn't weren't quite sure if this was
alphago or some other program and so on
when when it came to 16 oh there were
thousands of people on the servers
watching these games unfold and so on
and it was beautiful to see and also
these 60 games are now a phenomenal
resource for the go community to study
the way the way alphago plays and for
example a grandmaster ghuli said that
together humans and AI will soon uncover
the deeper mysteries of go so they've
really taken on board alphago as a
possible tool where AI and humans can
work together to to understand something
like go in a deeper way than ever before
I just don't want to give credit to the
alphago team this phenomenal bunch and
recently some more people have all done
a fantastic job in working on this this
project
and I would also like to emphasize that
in order to solve such a hard problem it
also is necessary to have a lot of very
good people working on it so this is not
a small little project this was a major
effort and it's been an incredible
privilege to work with with these very
smart people on the project so some
lessons from alphago maybe so one might
argue ok this can really only solve one
problem the game of God and this fairly
specific problem so maybe in contrast to
what I said earlier but really we have a
very generic planning architecture that
combines the insights that neural
networks can deliver with the systematic
search that Monte Carlo tree search
gives us and this combination of
learning from data and planning leads to
these phenomenal results that some
people within this narrow domain might
even call creativity emerging the key
really is that we took human data at the
beginning and have the system learned
from those human data and then used the
self play mechanism in order to to
bootstrap that and get the player even
stronger beyond human performance
eventually and what we can see here is
really also from the reactions of the
professional players that now they view
this AI as a tool and they're super
excited to see what it comes up with in
the domain that they've all invested so
much lifetime into and and it was really
good to see that I sometimes you know in
the press you see remarks then okay this
is approaching artificial general
intelligence or something like that of
course that is all nonsense and I would
like to delineate a little more
precisely what has been achieved here by
saying what hasn't been achieved
of course ego is is a game where
everything is fully observable whereas
the world isn't such it's usually only
very partially observable in go we know
the model of the environment we know
exactly the rules of the game whereas in
the real world we don't
in the real world there's typically more
than just two agents not just black and
white so that certainly creates more
complex situations the real world is
definitely not a zero-sum game we're not
just all opposed to one another we can
work together to create a bigger pie and
so that's definitely something that
wasn't addressed in this work the
dynamics in the real world is typically
very complex continuous noisy and all of
that in contrast to the simple
simplicity of the game and typically
input spaces are much more high
dimensional just think about division
and our other sense organs and actual
spaces are much richer than just picking
a single move in a game and finally of
course there are all of these open
questions in artificial intelligence
that weren't addressed here yet it is an
interesting case study because in this
narrow domain this particular approach
has been so inspiring to those who know
this domain ok I would like to wrap up
by just giving you the bigger picture of
this kind of research direction a lot of
work a deep mind is on machine learning
and I think in general we can say that
machine learning has been one of the big
driving forces of what we now call AI
research but which got a lot of its
roots in machine learning when you look
at multi-agent system there's an
additional component which is the game
theoretic aspect because now you have
potentially cooperative or adversarial
scenarios and you need to take into
account the agency of others and game
theory is one of those frameworks that
are in a good position to address these
problems finally we can also learn from
cognitive science and maybe from the
social sciences because they've been
busy studying multi-agent interactions
for many years and certainly on the work
I presented on the emergence of
cooperation we've been drawing heavily
on insights from social science as well
so if we look at multi-agent and how it
can help us to achieve AI in
I better I think it can really help us
create better AI systems for example if
you think about personal digital
assistants they need to understand what
you want they need to perceive you as an
agent and treat you as an agent this
framework can help a study Collective
agent behavior if you look at traffic if
you look at the economy the environment
these are all challenges that can only
be addressed by understanding the
interaction of multiple agents and
finally there are these very important
aspects of AI safety and AI ethics as
the field moves forward and has greater
and greater impact on our world we need
to make sure that it is safe because it
is a powerful technology and we need to
pay put those safeguards in place and we
need to think about the ethical aspects
of AI as well ok thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>