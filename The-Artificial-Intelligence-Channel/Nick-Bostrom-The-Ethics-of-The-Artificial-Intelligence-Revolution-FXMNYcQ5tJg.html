<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Nick Bostrom - The Ethics of The Artificial Intelligence Revolution | Coder Coacher - Coaching Coders</title><meta content="Nick Bostrom - The Ethics of The Artificial Intelligence Revolution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Nick Bostrom - The Ethics of The Artificial Intelligence Revolution</b></h2><h5 class="post__date">2017-09-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FXMNYcQ5tJg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I've lost the title okay new title is
okay Nicki's gonna talk to us about a
matter which he will announce for us
please welcome Nick Bostrom
we have it yeah it changed a little
because I was asked also to try to say
something more general about this set of
issues before delving into some of the
issues I'm thinking about right now
so I put in this first slide some near
issues these are things that people like
to talk about when the topic of ethics
of AI comes up so self-driving cars
people love to debate that you have a
choice between running over three old
ladies or two young kindergarten kids so
what should you do you know how many how
many old ladies before you should run
over the kindergarten kids so I think I
mean it's it's it's it's interesting but
maybe not the most important it's that
mean I think there's 1.2 million people
dying on the roads every day and it
seems to me that from a practical point
of view that the main issue there is
just how can we roll out this technology
sooner how can we make progress on the
engineering so that we can bring down
that death toll algorithmic
discrimination so this is when more and
more automated processes are used to
make decisions where the tiler kid
credit whether to maybe give parole or
to do a lot of other choices then it
would be nice to be able to interrogate
these and to see what are the basis upon
which these judgments are made it might
be that we don't always think accuracy
is the only criterion we might think
that there are certain kinds of that
information that shouldn't filter in to
how somebody's treated so there's this
but that data ownership with privacy
will firms use the information you give
them to price discriminate against you
what what are some of our tolerance
thresholds for that virtual bias if
virtual realities become more and more
lifelike than are there certain behavior
even if they don't harm anybody that
would nevertheless be intolerable like
really vivid realistic a games
simulating rape for example you might
think that's wrong even if there is no
no actual victim there killer robots is
a big big topic obviously jobs and then
more sort of meta problems so some
people have complained about how
unrepresentative the the field of AI is
and among the people thinking about it
it's it's kind of very male-dominated
and it's a certain demographic so does
that shape the development of the field
if there are only certain types of
people who are participating in building
these technologies so so these are all I
say I mean as David said these are all
legitimate issues and I look forward to
hiring a lot of conversation about these
over the today and maybe tomorrow but
there is like an additional set which is
well I should say that these are all I
think can be kind of brought under the
rubric they are basically questions
about how to live well on how our
society should be organized now given
the current technological landscape so
how can we arrange we have given this
technology how can we arrange this so
that we have some decent outcomes but
there is an additional set of issues
which I guess I have tended to focus
more on so if one has some time scale
which I actually think looks more like
this there's a big part that that comes
kind of after this technology has
reached maturity and I want to tell more
about that but even that's not exactly
right to capture what I have in mind you
can kind of distinguish choices that we
make that have only relatively short
term consequences and so you get to
remake the choice a little bit later so
we can kind of increase the taxes this
year and then if we don't like it we can
decrease them next year
we
can change things around so what I have
in mind is not so much the kinds of
ethical issues that arise far into the
future rather what I have in mind are
ethical issues that might pertain to
choices if there are any such choices
that we might make that could have
long-term consequences for where we end
up where earth originating intelligent
civilization and up and it's really
those questions that are at the core of
the work that that we're trying to do at
the Institute so so one of these is this
AI alignment problem that I talked a lot
about in the book so in a certain class
of scenarios in in the class of sonars
we have where you have a super
intelligence becoming very very powerful
and there's been a lot of discussion
about this if at some point you have
that able maybe to shape the future
according to its preferences then how
can we ensure that the preference is
that this AI will have will be ones that
are aligned with what we want it to do
so that we get some good outcome
basically how you could control very
powerful AIS
and what we really are after here is
scalable control so there are a lot of
control methods that exist now and that
for the most part work perfectly fine
you have the off switch you have like
some other simple mechanism and they are
they are completely adequate and you can
propose more but what really would be
nice would be to have some set of
control methods that are scalable in the
sense that they would continue to work
even as the system that you're trying to
control becomes more intelligent ideally
they should work better that smartly the
system becomes and so some of the
current things that we rely on might not
be scalable in this sense the off switch
for example if you had a sufficiently
intelligent AI is sufficiently powerful
AI it might prevent you from switching
it off or might anticipate your action
and a reward button might be another
kind of control method that works fine
as long as you have control over the
reward button and you can understand
what the system is doing but it's not
not be scalable so so one approach to
this is the idea of
giving the AI goals that coincide with
others with ours so that the smarter
they I guess that the better it is able
them to achieve our goals and that in
principle a scaleable control method but
it done faces this challenge of how you
can specify what we really want
accurately and in such a way that it
doesn't have unforeseen consequences
when when really carried out to its
logical limit I argued in the book fire
I call it the orthogonality thesis which
is that there is no necessary connection
between intelligence and values that in
principle you could have almost any
combination you could have you know a
really really smart system that is
really nice or really nasty or have some
arbitrary goal or just as you could have
a really stupid system that is nice and
nasty and we see this even within the
human population to some extent you have
all combinations smart and dumb people
are nice and evil or whatever but this
is a much larger space and one thing I
would a lot of this is a technical
engineering challenge rather than the
ethics challenge there is still the
question of what what kind of goal
ultimately we should like try to put in
there and I I'd say that it's not at all
clear that we would want that to be the
goal of somehow optimizing the morality
of its behavior like do the thing that
is the best action to do or the most the
action you're most confident is the
right action because we don't really
know what what the true morality is like
suppose it turns out that it's some kind
of just desert theory and turns out that
what we all maybe were little bit rotten
so maybe we all just deserve a damn good
spanking and it's like you have the
future optimized according to this maybe
maximally morally correct or are some
ice bucket challenge or something like
that that just goes on forever I mean
who knows really what the sort of truths
about all these moral so if you think of
morality as maybe some way to organize
ethical intuitions in some reflective
equilibrium with other things like who
knows what would come out of that and if
you really put in
is unalterable goal in a very powerful
optimization process this might not be
anything that we really would want at
all
so maybe morality is some kind of
ingredient there but it might not be so
clear that we're really on reflection
would want this to do the morally
optimal thing so so there is this but
then there is this parallel challenge so
assuming you could solve the technical
challenge of how to put human intentions
human goals reliably into an arbitrarily
powerful intelligence there is still
then this political question that we
confront like which calls to put in who
should decide what is the overall set of
governance structures that should guide
our approach into this more AI enabled
future and so this is an area where work
is that an even earlier stage so the
control problem I mean a few years ago
there was almost nothing that were like
a couple of groups in the world who were
thinking at all about that and that has
changed over the last couple of years
and it's really amazing to see new
centers springing up and research papers
are starting to get published on that
but this is at an earlier stage this is
maybe where work on the technical
control problem was a few years ago so
so that there isn't anything very
complete or thought through to presenter
but I want to just talk a little bit
about some sort of work-in-progress that
I'm doing and together with Carrick
Flynn and Daniel Defoe I think Daniel is
in the audience here somewhere we
haven't had a chance to necessary sync
up on exactly all of these things are
shown on the slides but these are some
thoughts about how one might begin to
think about this so what I would like to
do as a first step is to think about
what are the so suppose somebody
proposed some submission for how this
should be governed some roadmap some
like what are the properties that one
would like such a proposal to have in
order for us to think that's a good
proposal that's something we could get
behind and in particular we can look at
these sort of normativity disorder like
what are these different rubrics
we'll look at some examples within each
one of these of types of features that
it'd be nice if our sort of proposal for
how humanity should before today's would
happen and I suggest that a useful focus
here is maybe not so much what's sort of
the end state but more we could try to
evaluate paths forward that start from
where we're now and then lead somewhere
into the future because one might have
preferences orientation is not just
about the destination but also about the
how how the decisions asked how to move
forward should go on the overall shape
of this we need to combine it at some
point sooner rather than later with
considerations about feasibility and so
we want some proposal that actually
could work not just something that would
be nice in in an ideal world and and
it's important there to consider second
best option so you might have a view
about what should be the best but you
might also want to have views you might
almost want to have a vector field as it
were with kind of views about what
should happen in any different situation
where you might end up or if you're
advising somebody and there is like the
best thing that they should do but for
whatever reason they are just unwilling
to do that you might still want to have
a view but where on the margin what what
might they should I go if you could just
kind of push them to do a little bit
more of one thing or another and we also
want to consider a commitment mechanisms
whereby early choices might lock you in
later choices so let's just briefly go
through some some thoughts on this so on
the efficiency we want to look at what
what is different in this context of
machine superintelligence so there are
obviously all kinds of considerations
that apply here just as they apply it to
all the other things we do but there are
perhaps certain distinct distinctive
issues or ones that have more weight in
this context so AI safety where it seems
like an important thing here and getting
an efficient outcome meaning one that
this kind of just increases the size of
the pie available particularly by
reducing the the the risk externalities
that could come from this so if if this
transition to the machine intelligence
era happens all all humans where we are
on earth even if you're sitting in a
little hut in Indonesia and have had no
voter input on this you you still if
they're gonna be risks you're still
gonna also face those risks so it seems
that one thing one can do is to mmm I
recognize an obligation there to put the
next effort on that timely development
and roll out in terms of the enormous
benefits that can come from this there
is a kind of arguably an urgency there
in particular insofar as we care about
people who exist now as opposed to
impartially about bringing new people in
existence to replace us but if the if
the goal is to benefit the 7 billion
people right now the clock is running
out like we're dying and and suffering
all kinds of ills and avoiding global
coordination failure so there's a
distinct that I don't really have time
to go into them but there are certain
there's a certain class of ways in which
things could go wrong and I think it's a
large chunk of of all the different ways
that things could grown that go wrong
because there is a failure at the global
level could coordinate that's that's why
we can have Wars and arms races and
destruction of the global Commons and
Malthusian outcomes and so forth in
considering this vulnerable world
hypothesis for want of a better name
would you could consider this hypothesis
that there is some level of Technology
Development at which often strongly
dominates defense in the sense that at
that level of technology it would be
feasible for a small group to take some
action that would lead to the
destruction of the world independently
of what other people do after that
action has been taken so we don't know
whether the vulnerable world hypothesis
is true but maybe this and then you can
think about if it is true what are the
kind of arrangements that would have
been put in place so that we can
maintain the the world intact even when
it enters such a vulnerable place and so
this ability to undertake some kind of
stabilization of the world if that
should be necessary some way - if there
is some really dangerous technology that
just would be much more easier to use to
cause destruction then maybe that would
have to be some way to prevent either
the proliferation of that or to very
closely monitor its use and that seems
like another important variable to
consider when looking at different paths
for us do they allow for a conditional
stabilization do are they such that if
it turns out that the world is
vulnerable in the sense that the world
could then be stabilized the control
could be centralized if that should be
necessary or is that if the development
just touch that there is no way for
anybody or any group of people to kind
of rein brainerd in or come to steer it
allocation so again yeah we all have
this risk externality also the there is
this kind of Bonanza that will have if
this goes well the the amount of wealth
and prosperity unlocked is enormous it's
this giant windfall you kind of open up
not just technological maturity here on
earth but also all the space resources
that our descendants can start to
colonize economic growth for a period of
time may become extremely rapid Robin
Hansen has a recent book where he you
know discusses the world economy
doubling every month or so for a period
of a couple of years and so you could
have this explosive growth in
productivity which also I think changes
the allocation or question somewhat so
it's it's more if you have such a large
abundance it becomes more possible I
think that if you could if you could
give everybody a really superb life in
great prosperity even by just carving
out sort of 1% or 1/10 of the percent of
the overall resource pie then it seems
to become much more of a no-brainer that
you should at least do that whereas
right now there are more kind of
trade-offs like some people would have
to give up significant wealth in order
for everybody to have a good life or you
might worry about economic incentives if
you tax people too much but this would
be an unique situation where you might
argue that there should perhaps be some
kind of basic allotment and another
interesting feature
here is that we are currently to a large
extent behind the veil of ignorance
nobody knows when AI when super
intelligence will be developed who will
develop it for what purpose is so we
kind of all have a shared interest if we
could in reaching an agreement now to
some kind of equitable reasonably
equitable distribution of influence and
wealth of the benefits it's like an
insurance scheme you would rather have
you know a certainty of getting you know
a seventh billions slide of this and
then having you know one in a seven
billions chance of getting all of it
because there are diminishing returns to
wealth and and power so you might think
that that could argue for some kind of
continuity that that if there are
pathways that don't completely reshuffle
the deck and just make some random set
of people the winners and some other
people are the losers but instead
something that was more continuous with
the existing status quo distribution
that that could be a desirable feature
that then we have this set of issues
that concern the creation of new people
our new morally relevant entities with
the possibility of of mine crime as I I
call them cities if you have so right
now we think of our computers as just
objects you could take a hammer and
smash them it doesn't really matter the
only question right now is how do the
computers affect us right now it doesn't
matter how we affect the computers
because they don't matter but you could
argue that at some point they might well
start to matter I've proposed a couple
of principles that include this
principle of substrate
non-discrimination so that if you had
something that was functionally and
mentally equivalent it wouldn't from a
moral point of view fundamentally matter
what its substrate of implementation was
whether it was silicon in a digital
computer or carbon in a in human brain
if they had the same functional property
using the same mental states that that
that would be no more relevant and
whether your skin is white or black
another would be a principle of ontogeny
non-discrimination so that holding fixed
what you are it shouldn't matter how you
were created at least I think these are
principles to consider it might well be
that we need to qualify them but but as
a starting point another would be a
principle of subjective time so if there
are cases where it it matters morally
how long something continues for say if
somebody is suffering it's worth if they
suffer for ten minutes than if they
suffer for five minutes and if you could
prevent either five minutes of one
person suffering or ten minutes of
somebody else's suffering you should
other things equal prefer to prevent the
ten minutes suffering in those cases
when you have digital Minds there are
two different senses of time that can
come apart because if you run the same
mind on a faster computer it's
subjective rate of time will go faster
even the clock time will remain the same
so the question then arises which of
these two senses of time are morally
relevant and I suggest that it's the
subjective sense of time so if you run a
an upload mind twice as fast and it kind
of accumulates more weight twice as fast
as well and and these are even if these
were correct it would only scratch the
surface there is like all kinds of
exotic minds with weird preferences and
strange properties that nobody has yet
thought about even what we what we what
we think would be there their moral
status so in addition to trying to
prevent mind crime or seeing it they
said this is Rotem of a path for that it
reduces the amount of mind and by the
way so this doesn't only rise once you
have sort of human level things living
in machines it it could arise a lot
sooner we think that a lot of animals
have moral status and maybe once you
have a eyes say with the similar
capabilities as a as a mouse or
something like that
you should start to think about whether
we should accorded similar moral status
as we do is if you
do medical experiments on a mouse you
have to go through an ethics from
committee and you have to like I need to
taste a mouse and it's not so much
whether you kill the mouse at the end
but it's like that it doesn't suffer
needlessly so that's that that can arise
a lot sooner and it seems like a silly
thing right now but I think it will go
from silly to controversial and at some
point in maybe the next 15 years or so
then then we also have this ease with
which you can produce more offspring or
copies so with humans it's the slope but
but here you could in principle
instantaneously copy yourself assuming
available hardware so you then have an
immediate conflict between two moral
intuitions we have this sort of
reproductive freedom everybody should
decide for themselves how many kids that
want to have and the idea that maybe
there should be some minimum welfare
standard at least for kids but but if
you combine these two then it just
doesn't work right because you have even
one person just likes to make more and
more copies they can sort of very
quickly eat up entire resourceful and
you have an immediate descent into
Malthusian condition about things
similarly if you could sort of just give
yourself more votes by duplicating
yourself kind of distorts the system so
maybe you would need to move to some
voting based on on your wealth or babay
you have each person has a suffrage
token that going to get split amongst
their descendants or something like that
process so in addition to these kind of
more outcome based properties we might
also care about how these outcomes are
decided and what happens along the
trajectory in particular one might have
intuitions favouring some sort of
legitimacy that you what you want
whatever may be you think I don't know
exactly how the future should be but I
would want it to be determined by
legitimate processes and exactly what is
legitimate here is can be complicated it
might not be enough decides I have
somebody consenting to it I mean suppose
it were the case that a sufficiently
advanced day I could persuade you into
anything by structuring a set of
arguments and stimuli in an appropriate
way then consent might not be sufficient
on you and
and finally we might have a discussion
about some of the methods so if there
are these different desiderata that
might not all be completely satisfiable
and we have to trade off different
degrees of satisfaction so how should we
do that I propose this parliamentary
model the way we kind of think of
suppose you have different moral
theories making different claims you can
think of each moral theories as a party
that gets to send delegates to this
imaginary Parliament of deliberation and
each theory gets to send a number of
delegates proportional to the
probability give to that moral theory
and then you imagine this Parliament
kind of deliberating and whatever that
Parliament would then decide is what you
should do and the idea here is that they
can trade among one another say if some
moral theory maybe you think it's only
thirty percent likely but it really
cares a lot about one particular choice
maybe that theory get its way on that
choice and maybe it kind of defers to
other theories on other choices and you
could expand this to include not just
moral theories but maybe you have
self-interest or different
constituencies you can kind of so it's a
sort of tries to be a robust way to
achieve compromise solutions that allow
for four wins and finally also in
addition it might become important what
the decision theory and the histology is
that that determines the long-term
future and then one might make some
proposal there that that should be based
on the actual decision theory that
humans have or that we would on
reflection have or something like that
so that's all that I have time for folks
look forward to the next two days thank
you
ok so now we have staff here because we
have time for a few questions just a
little bit about how this works we'll
have a three talks most of which will be
20 minutes each one will be followed by
10 minutes or so for a Q&amp;amp;A for the
individual talks after all 3 talks we'll
get all three speakers on stage and
they'll be a chance for a bit of
interaction among the group and
questions for from the audience but for
now we've got time for a few questions
for Nick we have people John will run
the run the mic looking for hands for
questions there's one there ah there we
go hi thank you my name is Heather
my question is you raised several issues
about government which is very
interesting to me because I used to work
for the United Nations and I'm a human
rights lawyer I've noticed that as
technology evolves government also
evolves and I'd love to hear your
thoughts on whether or not you think the
evolution of government is anywhere near
close to being able to deal with the
kind of challenges that the technology
is now posing and people think of
parallels with like the nuclear arms
race and was our government development
in line with what we were
technologically capable of
if it I mean it would be nice if it
improves and I think there are a lot of
exciting ideas for how you could do that
I mean it strikes me though that so
there are these I know some people
having to say these cryptocurrencies and
aetherium and they envisage sort of some
decentralized system that nobody has but
that often the very first use case they
think of this awesome may be the United
States could run like it seems to me
that that there should be a very long
trajectory between a cool new idea and
applying it to some kind of world
important system that you would want to
try it on small scales first and then if
it still looks like a good idea half a
century or two centuries later then you
know maybe you would really mmm one
that's like the plight to the things
that matter most I mean it's such a big
question how how technology can I mean I
think there is low-hanging fruit so one
one thing I've always thought would be a
cool is to have a prediction market a
well subsidized prediction market on
policy futures so you could see there
running odds of the the likely
consequences of adopting a certain
policy like if you if you erase the tax
rate 5% will economic growth be higher
or lower five years down the line you
know if you if you close the border to
Mexico will there be more of your
terrorist incidents and so you could
have speculators betting on these things
and then kind of read out from that some
some kind of market probability estimate
that could maybe discipline some of the
speculations but it is a hard thing to
do so so there's this technical control
problem and and there is this governance
problem you know arguably arguably they
are equally important in the sense that
they shaped the outcome equally but it
might be a lot more important to work on
the technical problem because it's just
harder to have a big impact on the even
if you even if you've thought about all
the answers
this kind of set of comments issues is
just much less obvious how that would
impact what actually happens and if you
come up with a really clever technical
solution to the control problem there's
a good chance that it will be
implemented so in that sense there might
be less important thank you well to draw
you out on something that a classical
angle to this whole thing which is that
seem to me that the interest of a lot of
the questions that you're asking is
premise on the systems in questions
being conscious so one of the things I
wanted to ask you is whether you thought
that was true or whether you thought now
as long as they're complex enough it
doesn't really matter whether they're
conscious and in the familiar sense of
course the question how you tell I don't
just mean the question of what
consciousness is but the question how
you tell whether a certain organism or a
unit is conscious is much debated but
one with no clear solution and so I was
surprised that it played virtually no
role in the way you were laying out the
landscape of the issues is mine crime
where it matters for whether we think it
it makes it difference morally whether
the thing in the box is conscious or not
but for the other purposes in terms of
thinking about how machines affect the
human condition I think one can abstract
away from from that yeah what's relevant
are their functional attributes and I
think it's an open question how easy it
is safe to build something that is not
conscious but still performs at the very
high level of intelligence we just don't
know whether sort of the easiest way to
achieve versatile general intelligence
involves something that we would own
reflection think probably is conscious
or whether the easiest way to do that is
through some very different architecture
that we wouldn't think is conscious i
i'm very uncertain about that hi so my
question is actually a little bit
similar to what his question was i was
interested in your principles of
substrate non-discrimination and on
tinnie non-discrimination i came to this
conference thinking the ethical
implications
of artificial intelligence more relied
upon the implementation of these systems
in society I didn't consider actually
having to respect these systems as
possibly being conscious or some form of
human intelligence
so my question first was how do you
determine if something is to be is a
mind to be respected and also what
should there be any difference at all
between say a human mind and a
simulation of a human mind and say an
experiment so you're right on the first
that for the most part the issue is
about certainly the presentations is
about how we humans use these machines
and how they affect us I was introducing
this SN a hare is actually a an issue
that has been very little discussed but
that act could could be important if
nothing else but because of just the
sheer numbers that it is very easy in
certain kinds of machine learning
training and experiments to create huge
numbers of these systems so if they had
some moral status then if you have any
kind of aggregate of intuitions well how
much something matters depends on how
many people are affected or how many
minds are engaged in it that it could
come - it could possibly come to
dominate the the sum total of say human
minds that have existed through history
at some point but this is a kind of a
niche issue at the moment and I gave it
some time precisely because it has been
neglected so I wasn't trying to allocate
my speaking time proportionally to how
much people have actually been talking
about these different things all the
things you've mentioned what is humanity
most likely to fail at yeah
I mean I think the Minecraft thing is
fairly likely to fail at that because if
you look at the difficulties we have in
extending empathy to animals we still
have a long way to go there I think and
animals have faces and they can squeak
but if these were invisible processes
happening in a microprocessor it would
be so much harder to see that that that
we might want to take that into account
so to give it more detail I so it's hard
because you would have to sort of
distinguish degrees of failing at
something and the real question is maybe
not what are we most likely to fail at
but we're on the margin could say the
efforts by one extra person or one extra
dollar most decrease the chance of
failing in a way that would make a
profound difference to the future and it
might be that working on something that
we're probably going to succeed at but
where maybe you could reduce the risk
that we will fail at it by a small
fraction might be better than working on
the most likely failure point anyway so
that becomes a more complicated
conversation thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>