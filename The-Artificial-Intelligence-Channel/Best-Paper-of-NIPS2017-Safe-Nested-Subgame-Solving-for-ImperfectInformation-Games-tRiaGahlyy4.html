<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Best Paper of NIPS2017 - Safe &amp; Nested Subgame Solving for Imperfect-Information Games | Coder Coacher - Coaching Coders</title><meta content="Best Paper of NIPS2017 - Safe &amp; Nested Subgame Solving for Imperfect-Information Games - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Best Paper of NIPS2017 - Safe &amp; Nested Subgame Solving for Imperfect-Information Games</b></h2><h5 class="post__date">2017-12-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tRiaGahlyy4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the first long talk is by know'm Brown
the title of the talk is safe and nested
sub-game solving for imperfect
information games and it's my honor to
say that this talk has been awarded that
one of the best paper Awards at the
conference so please join me in
congratulating norm and his co-author
Tomas and home
okay everybody I'm no I'm Brown this is
talking with my adviser Tomas antal and
I'm going to talk about our paper safe a
nested sub e I'm solving for imperfect
information games now this paper focuses
on imperfect information games these are
not games like chess or NGO or other
games like poker or more generally any
strategic interaction that involves
hidden information and I see this line
of research as being particularly
relevant to bringing AI into the real
world because most real world strategic
interactions involve some amount of
hidden information now when it comes to
these games poker has served as the
primary benchmark challenge for a long
time in a particular a very new Poker
called heads-up No Limit Texas Hold'em
No Limit Texas Hold'em because it's the
most popular variant poker in the world
and it's a huge game at 10 to the 161
decision points and heads up because
when you only have two players you can
determine with scientific certainty
who's better you don't have to worry
about issues like collusion or kingmaker
effects that might interfere
and although AI has made a lot of
progress in dealing with perfect
information games when it comes to
imperfect information games there has
been limited success and in fact no
prior AI has been able to beat top
humans in this game but earlier this
year our AI lab Broadus challenged four
of the world's best heads up No Limit
Texas Hold'em specialists in a 120,000
hand match that lasted 20 days there is
a two hundred thousand dollar prize pool
divided among the pros to incentivize
them to play well and the final result
is that our AI won by a lot this was
about three times the win rate that you
would expect between a top pro and in a
regular pro it was statistically
significant at about four standard
deviations and each human lost
individually to the bottom and this came
as a big surprise to everybody involved
there's a big surprise to the AI
community to the poker community even to
us and I think what really demonstrates
this is that if you look at the betting
markets there was a very active betting
market leading up to this competition
because their poker players and they
liked to gamble and the betting odds
against us were about four to one at the
start of the competition so when I talk
to people about this line of research
one of the questions I get a lot is well
why are imperfect information games so
hard after all we have deep blue we have
alphago why why can't we take those
techniques and apply them out of the box
to a game like poker and there's a few
reasons for this but one of the main
ones is that in an imperfect information
game the optimal strategy for a sub game
cannot be determined from information in
that sub game alone
let me show you what I mean in a perfect
information game if you take some action
and your opponent takes some action and
you find yourself in a sub game you can
forget about the sequence of actions
that got you here and you can forget
about all the other situations that you
do not encounter the only thing that's
relevant for determining the optimal
strategy going forward is the state that
you're in and all the states that can be
reached from this point on an imperfect
information games this is not true an
imperfect information games you take
some action your opponent take some
action you find yourself in a particular
sub game but now some other sub game
that you did not reach and in fact it
cannot even reach from this point on can
affect what the optimal strategy is in
the sub game that you are in and this is
kind of a counterintuitive idea but I'm
going to give you a concrete example of
it in a little bit now when it comes to
these games what we're trying to do is
find what's called the Nash equilibrium
this is a profile of strategy as one for
each player
where no player can improve by shifting
to a different strategy and in two
player zero-sum games in particular this
is extremely useful because if you're
playing the Nash equilibrium you are
guaranteed to not lose an expectation
and we're going to measure our
performance in terms of exploitability
this is distance basically distance from
a Nash equilibrium now you might wonder
why should we care about exploitability
and why should we care about Nash
equilibrium and I think if you look at
recent man-machine competitions it
really highlights the importance of
being robust to adaptation by the humans
for example the recent open open AI
competition in dota 2 they managed to
beat top humans and 1 vs. 1 matches and
over about three matches but once they
opened up the AI to the public and
invited the public to try it in and beat
the AI relatively weak players were able
to figure out how to adapt the to the
bots and find weaknesses that they could
take advantage of and even with alphago
and the original version of alphago that
beat Fenway they beat him 5-0 but after
several more matches phantom a was able
to find certain situations that the bots
had a lot of trouble with and they had
to address these weaknesses before the
release at all match but in our AI lab
Broadus we played against top humans for
20 days straight 120,000 hands of poker
and during that entire competition all
four humans working as a team to try to
explore and exploit the bots in any way
they could find and they couldn't find a
weakness in the bottom and I think that
the reason for that is our emphasis on
being
bust two opponent adaptation of
exploitation and having theoretical
guarantees on exploit ability now for
the rest of this talk I'm gonna I'm
gonna introduce this game called coin
toss it's a toy game an imperfect
formation game that's going to be used
as a running example going forward in
this game player one is going to flip a
coin that's going to land heads or tails
and 50/50 probability and player two is
going to try to guess the outcome of
this coin toss so once the coin lands
player one is going to observe the
outcome of this coin toss and they have
a choice they can do they sell the coin
or they can choose play now sell is
going to lead to a separate sub game the
details of which I'm not going to go
into but the expected value of that sub
game is going to be important if player
1 chooses playing when the coin lands
heads they get an expected value of 0.5
you could say the coin is lucky for
example and if the coin lands tails will
say the coin is unlucky and they have to
pay somebody 0.5 0.5 to take it away
from them so they get a reward of minus
0.5 by choosing sell on the other hand
they could choose this action play and
if they choose play then player 2 tries
to guess the outcome of the coin toss
without having observed how it landed if
player 2 guesses correctly that is the
guess heads when the coin actually
landed heads then player 1 is going to
lose $1 and player 2 is going to gain $1
and I'm showing the rewards here only
for player 1 because player 2 is simply
going to receive the opposite reward on
the other hand a player player two
guesses incorrectly that is player 1 if
those are the guessed tails when the
coin actually landed heads then player 1
is going to gain $1 and player 2 will
lose $1 and you can see there's a dotted
line between the two player 2 notes and
this signifies that there that the two
nodes are in what's called an
information set since player 2 did not
observe the outcome of the coin toss
they don't know which of these two
states they're in so I wanted to imagine
that your player 2 in this game and you
just observed player 1 chooses play
action so you know you're in this
imperfect information sub game well what
should you do what's the right strategy
well one option is to dis always gets
heads but if you do this then player 1
can adapt by always choosing stall when
the coin lands heads and getting $0.50
and always choosing play when the coin
lands tails and getting a dollar
so when average player 1 is getting 75
cents on the other hand if you're always
gets tails then player 1 could adapt by
always choosing play when the coin lands
heads and always choosing someone the
coin lands tails an averaging 25 cents
in the game
so it turns out that the the optimal
strategy is to mix its two guests heads
with 25% probability and tails with 75%
probability because if you do this then
no matter what player 1 does they can't
do better than an expected value of 0 so
this is the Nash equilibrium strategy
for player 2 in this game but now let's
say we changed this game a little bit
let's say we swap the payoffs for the
sell action so that player 1 gets minus
0.5 for choosing self when the coin
lands heads and plus 0.5 for choosing
sell when the coin lands tails well it's
easy to see that in this case the the
strategy for player 2 and the in the
place love game should actually change
we should be guessing heads with 75
percent probability and tails the 25
percent probability but you can see what
happened here is that by changing the
expected value for this sell sub game we
have affected what the optimal strategy
is in the place of game even though the
sell action is not in the place of game
and in fact it's not even on the path to
the place looking and this is this is
like that can only happen in imperfect
information games perfect information
games are a unique case where you don't
have to worry about this problem so when
you're dealing with imperfect
information you always have to consider
that the entire game is a whole when
you're deciding the optimal strategy for
a particular situation so we need very
different techniques to address this now
there is some hope one of the nice
things about this is we don't have to
worry about the actual strategy in the
play in the cell sub game we only have
to worry about the expected value for
that action and we could take advantage
of this and I'll talk about that a
little bit now you might be wondering
well why can't we just estimate what the
strategy that player 1 is playing why
can we just estimate that strategy and
try to find a response to it and this is
a technique called unsafe sub game
solving basically the idea here is we're
going to estimate a player 1 strategy
and then we'll determine a much better
strategy and the players in the play sub
game that responds to that that strategy
and I'll walk you through an example of
this you could say like after the coin
lands we know that we're in one of these
states with 50/50 probability each and
now 1 player 1 chooses play well we
would have s we've estimated that player
1 would choose play with 80% probability
when the coin landed heads and 30%
probability when the coin landed tails
so we're going to update our beliefs and
say that now we're in a 73%
there's a 73% probability that we're in
the left node and a 27% probability that
we're in the right note
and now we're going to determine an
optimal strategy that responds to this
belief distribution but in that case the
optimal strategy would be to guess heads
with a hundred percent probability and
we've already established that that's a
really bad strategy because player one
can simply adapt by shifting to a
different strategy so you can't make
assumptions about how the opponent is
going to play because they could always
switch to a different strategy
in fact unsafe sub game solving can lead
to very high exploitability both in
theory and in practice what we do
instead it's like called safe sub game
solving this has been an area of
research that has been developed over
the past few years the idea here is that
we're going to estimate the values the
opponent receives in sub games we're not
going to make any assumptions about how
they actually play so for example we can
estimate the value of the sell action
for player 1 and we have a theorem that
says if we are able to accurately
estimate these values that the opponent
the values of the actions the opponent
could take for example if these
estimates are off by at most Delta then
we can guarantee that our strategy is
that most to Delta Explorer has
exploitability at most to Delta using
safe sub game solving and in fact in
some cases we can actually do better
than this for example in this case
player 1 receives an expected value of
minus 1 for choosing cell regardless of
what state they're in and obviously
there's no way to make player 1 and
difference between the cell and the play
action in this case the best we can do
is force them to get an expected value
of zero for choosing play but if we look
at prior actions that the opponent could
have taken we could we might be able to
do better than this for example maybe
after the coin landed heads there was
some action that player 1 could have
taken that would have led to a reward of
$100 well if we find ourselves in the
place sub game then they would have had
to have given up this expected value of
$100 earlier on if they were in if
they're in the head state so by
reasoning about this we can allow the
value of player 1 to increase in the
case of the head state and in the
process lower the value of player 1 in
the tail State and still be safe still
guarantee that they're not doing any
better than before and in fact are
actually doing worse in this case so
reach sub game solving guarantees that
the that we can do just as well as
passed up the in solving technique safe
sub game solving techniques and in some
cases can actually do better by looking
at past actions the opponent might have
taken
now I'm glossing over a lot of the
details because this is a short talk but
the actual implementation of this is
very difficult to do correctly and in
fact two prior papers have tried to do
something similar to this but actually
did it in an incorrect way that allows
exploitability to be very high in
certain cases now there's another use of
safe stop being solving which is dealing
with large action spaces for example
let's say we're dealing with an auction
an auction game where players can bid
any amount between $0 and $1,000
well dealing with a branching factor of
a thousand is pretty is pretty difficult
what we can do is reduce the complexity
of the game by only considering for
example bet increments of $100 and the
reasoning here is that there really
isn't that big of a difference between
bidding $200 and 200 $1.00 but now if
the opponent were to actually bid
something like $250 then we have to
round it to a nearby size so for example
we were grant $250 to treat it as if it
were a bid of $200 but safe sub-game
solving gives us an alternative to this
rather than rounding it to a nearby
action we can come up with a sub-game in
response to that action uniquely and
solve it in real time and we could try
to make the opponent no better off for
having chosen that action than one of
the actions that are already in our
abstraction and actually we have a
theorem that says if the oft reactions
that is the actions that are not in our
abstraction are not much better than the
actions that are already in our
abstraction then safe slope game solving
will produce an approximate Nash
equilibrium and we can we can use this
repeatedly as we go down the game tree
in response to subsequent offt reactions
in an algorithm called nested sub game
solving here we start with a very large
game that's simply too large to solve up
front and we find an approximation for
how to play in the entire game as a
whole this is called the blueprint
strategy and now during actual play if
we find ourselves in one of these sub
games but come up with a much better
strategy in real time for that sub game
while fitting that strategy within the
overarching blueprint that we've already
computed and we repeat this process as
we go down the game tree when we find
ourselves in another sub game we again
find a much better strategy for that
particular sub game while fitting it
within the overarching blueprint that
we've already computed now in terms of
experiments we find that these
techniques it's a far lower exploit
ability we are not the first paper to
introduce safe sub game solving but the
techniques that we introduced lead to a
three
improvement over past slave sub-game
solving techniques and nests of being
solving leads to 12 times less
exploitability compared to action
mapping techniques that I described
earlier and also in terms of
head-to-head performance these
techniques leads to a dramatic
improvement there is a competition
called the annual computer poker
competition where researchers who work
on imperfect informations games get
together make an AI for poker and played
them against each other now the previous
best bot is called BB tarkanian a this
is our 2016 bot that won the competition
it beat the next closest competitor in
the competition by 12 middle big blinds
per hand this is a measure of win rate
in poker and that baba's slum BOTS and
it beat the 2014 champion by 25 millions
per hand there is another AI called deep
stack that came out around the same time
that also uses a flop game solving and
also has very low exploit ability but in
terms of head-to-head performance was
not able to beat maybe 13 enade but our
AI lab Broadus which uses a nested sub
game solving is able to beat baby
tarkanian 8 by 63 melodic lines per hand
which is a wide margin and this is due
largely to destined sub game solving in
fact if you don't use nested sucking
solving in the Pradas it's only about
tied with baby Tarkanian 8 so to
summarize the contributions of this
paper is that we developed domain
independent techniques for imperfect
information games with provable
guarantees on performance and
exploitability this is the first and
only AI this has led to the first and
only AI to be top humans in No Limit
poker and we introduced new aspects to
safe sub game solving that allow this
achievement such as considering previous
actions that the opponent could have
taken and doing nested sub games on and
responding to offed reactions there are
also techniques in the paper for being
robust to errors in the model which I
don't have time to go into and I think
what really highlights this achievement
is that if you look at the annual
computer poker competition between 2014
and 2017 none of the top teams using
that reused real-time sub game solving
but in 2018 which is the 2018
competition which is coming up in about
two months nearly every top team intends
to use real-time stop being solving in
their agent so I'm going to stop there
I'd be happy to take questions and just
one more thing there's going to be a
demonstration of librettist tonight at
7:00 p.m. and one of the top poker
players that we played against Jason
less will be there to demonstrate the
bots as well thank you
so I think we don't have any time for
questions but maybe you can ask gnome
questions at the demo tonight</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>