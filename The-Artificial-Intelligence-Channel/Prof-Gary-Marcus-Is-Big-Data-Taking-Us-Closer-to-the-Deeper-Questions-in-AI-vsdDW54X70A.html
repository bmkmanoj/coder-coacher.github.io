<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Prof. Gary Marcus - Is Big Data Taking Us Closer to the Deeper Questions in AI? | Coder Coacher - Coaching Coders</title><meta content="Prof. Gary Marcus - Is Big Data Taking Us Closer to the Deeper Questions in AI? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Prof. Gary Marcus - Is Big Data Taking Us Closer to the Deeper Questions in AI?</b></h2><h5 class="post__date">2017-09-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vsdDW54X70A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what I'm worried about and what I'm
thinking about these days is are we
really making progress in AI I'm also
interested in neuroscience the same kind
of question we feel like they're making
progress but are we really so let's take
AI first there's huge progress in AI or
at least huge interest in AI bigger
interest than I think has ever been in
my lifetime I've been interested in AI
since I was a little kid I was trying to
program computers to play chess and do
natural language databases and things
like that not very well but I've watched
the field and there's been ups and downs
there were a couple of AI winters where
people stopped paying attention to AI
altogether people who were doing AI
stopped saying that they were in the
field of AI now everybody's excited they
say yeah I do artificial intelligence
where two years ago they would have said
I do statistics but even though there's
a lot of hype about AI and a lot of
money being invested in AI I feel like
the field is actually headed in the
wrong direction but there's been a kind
of local maximum where there's a lot of
low-hanging fruit right now in a
particular direction which is mainly
deep learning and big data and people
are very excited about the big data
about what it's giving them right now
but I'm not sure it's taking us closer
to the deeper questions in artificial
intelligence like how we understand
language how we reason about the world
so the big data paradigm is great in
certain scenarios so one of the most
impressive advances is in speech
recognition you can now more or less
dictate into your phone and it will get
most of what you say right most of the
time that doesn't mean it understands
what you're saying so each new update of
Syria adds a new feature like first you
could ask about movie x and then sports
and so forth so the natural language
understanding is coming along slowly you
wouldn't be able to dictate this
conversation into Syria and expect it to
come out with anything whatsoever but
you could maybe actually get most of the
words right and that's a big big
improvement but it turns out it works
best when there's a lot of brute force a
devel so when you're doing speech
recognition on white males asking search
queries who are native language speakers
in a quiet room it works pretty well but
if you're in a noisy environment or
you're not a native speaker it turns out
if you're a woman or a child then the
speech recognition doesn't work that
well so speech
cognition is really route force it's not
brute force in the same way as deep blue
which considered a lot of positions but
it's deep brute force in the sense that
it needs a lot of data to work
efficiently and the thing that I think
about a lot is that kids don't need
anywhere near that much data in order to
think it efficiently so when you get to
domains where there isn't so much data
the systems don't work as well
natural language is actually a good
example of this so Chomsky and my mentor
Pinker were always talking about how
there's an infinite number of sentences
in a finite month amount of data is what
they call the poverty of the stimulus
argument and this very much holds true
and holds force in the natural language
domain so first of all data is expensive
it's cheap to get transcribed examples
of words that you can have somebody do
this on Amazon
Terk or something like that but getting
labeled examples this is a sentence and
what it means is expensive you need a
linguist to do it and there's
essentially an infinite number of
sentences so nobody has the kind of
database where they could crank into
deep learning all the sentences that
they had meanings that they understood
and expect it to understand a broader
segment of language so again we would
likely have this fantasy of machine
reading or machines being able to like
watch television programs figure out
what's going on obviously the
three-letter agencies would like to do
this but if you want to advance in
science or in technology you would
really like for machines to be able to
take all the literature that's out there
and synthesize it in a way that people
can't I mean this is part of why I do AI
is I think the potential is there to
totally change medicine and bend science
that we haven't even thought about but
to do that we need machines that can
read and they do that you need to go
beyond the data there's just not enough
data it's a group force your way to
scientific understanding and people get
very excited every time there's a tiny
advance but I think that tiny advances
aren't actually getting us closer so
like it was the Google captioning thing
last year that got a lot of press thing
goes in the front page of The Times and
you could show it some pictures and it
look like it was great so you'd show a
picture of two people and a frisbee or
the sorry a dog and a person in a
frisbee and it might be able to say okay
that's a dog catching a frisbee so it
gives the illusion of understanding
language but it's very easy to break
this system so you show it a picture of
a street sign with some stick
years on it and it said well that's a
refrigerator with food in it this is
kind of bizarre answer that we used to
send you to Oliver Sacks like it's
almost like a neurological deficit so
the systems will be right on the cases
that they have a lot of data for and
really fall apart on the cases where
they don't have much data you can
contrast this with a human being you
never heard any of the sentences that
I've said today maybe one or two and yet
you can understand them we're very far
from that the other thing that people
are really excited about is deep
reinforcement learning reinforcement
learning combined with deep learning
this is the thing that drove deep lines
the famous Atari game system and it
seems very persuasive at some level so
you have this system it just as pixels
as its input all it has to do is move
the joystick and it does that better
than people for a whole bunch of Atari
games but there's sort of like some
hidden tricks that allow it to work more
effectively in the Atari game world than
in the real world you think let's take
that same technique if it's so great and
let's put it in robots and we'll have
you know robots vacuum our homes and
take care of our kids and so forth
but the reality is that the Atari game
system first of all data is very very
cheap again so you can play the game
over and over again if you're not
sticking quarters in a slot you can do
it infinitely and so you can get you
know gigabytes of data very quickly with
no real cost but if you're talking about
having a robot in your home I mean I'm
still dreaming of Rosie the robot that's
gonna you know take care of my death and
domestic situation if you have a robot
at home you can't afford to make a
mistake so deep mine system is very much
about trial and error at an enormous
scale but if you have a robot at home
you can't have it you know run into your
furniture too many times you don't want
it to put their cat in the dishwasher or
even once and so you can't get the same
scale of data you really need to learn
things quickly from small amounts of
data if you're talking about a robot in
a real world environment and then the
other thing is in the Atari system it
might not be immediately obvious but you
actually have 18 choices at any given
moment so they're eight directions you
can move your joystick or not move it
and you multiply that by you press the
fire button right dog gives you 18
choices in the real world you often have
infinite choice or at least a vast
number of choices so if you have only 18
you can explore if I do this one that I
do this one then they do this one what's
my score how about if I change this one
how about if I change that one but if
you're talking about a robot they could
go anywhere in the room or lift anything
or carry anything or press any button
and so forth you just can't do the same
route for search of what's going on and
we really lack for techniques that are
able to do better than just these kinds
of group force things so all of this
apparent progress is really being driven
by the ability to use brute force
techniques on a scale we've never used
them before that originally drove deep
blue for chess and that drove the Atari
game system stuff it's driven most of
what people are excited about but at the
same time it's not really extendable to
the real world if you're talking about
domestic robots in the home or driving
the streets you could also think about
driverless cars what you find is in the
common situations they're great so if
you put them in clear weather and Palo
Alto they're terrific but if you put
them when there's snow or there's rain
or there's something I haven't seen
before it's difficult for them there was
a great piece by Steven Levy inside the
Google automatic car factory where he
talked about how the great triumph may
be in late 2015 was they finally got
these systems to recognize leaves and
it's great that they do recognize leaves
but there's a lot of scenarios like that
where there's something that's not that
common there's not that much data and
you you and I can reason with common
sense we can try to figure out what this
thing might be how it might have gotten
there the systems are just memorizing
things and so that's a real limit yeah
the same thing might happen with
behavior so you try this out in Palo
Alto all the drivers are relaxed you try
it in New York and you see a whole
different style of driving and the
system may not generalize well to a new
style of driving people have road rage
and you know who knows what happens to
the driverless car system we already
have problems that the driverless cars
will be the rules and the human drivers
don't and so the driverless cars stop
and the human human driver sometimes
rear-end them so behavior really matters
and it's another case where that's in a
very situation by situation you and I
can use some reasoning about the world
if we see a parade maybe we don't have a
lot of data about parades but we see the
parade we say there's a lot of people
and let's stop and wait a while maybe
the
Carr gets that maybe it gets confused by
the mass of people and doesn't recognize
it because it doesn't quite fit into its
files for what you know individual
people I won't even get into what
happens in drive-by shootings but you
know if you imagine these embedded in
the military context which is something
that people take pretty seriously you
could wind up in the same kinds of
contexts so you train these things in
kind of safe environments of Palo Alto
and then you bring them over to to Iraq
and who knows what happens when they're
projectiles and IEDs and so forth so
there's a huge problem in general with
the whole approach of machine learning
which is that it relies on a training
set and a test set and the test set
being similar to the training set so
training is all the data that you you've
memorized essentially the test set is
what happens in the real world and
people can do this in a sort of
empirical way they try a training set
they try a test set and they say well it
seems to work here but they're no formal
proofs or guarantees so people have
talked lately in the context of AI risk
about program verification and things
like that how do you know that the Space
Shuttle is gonna do what it's supposed
to do for example it was a you know the
first time I learned about program
verification I guess so
in when you're using machine learning
techniques it very much depends how
similar is this set of test data to the
set of test data that I saw before this
hasn't got to the training data that
I've seen before and so again like if
I'm it's hard to know what's gonna
happen to this car and when I put it in
Iraq if it's been you know trained up in
Palo Alto so there's a general problem
with machine learning which is it's
maybe good enough for some context to
say it's similar ish to what I've seen
before and then there get to be problems
where you need nearly a hundred percent
performance so a lot of the excitement
about deep learning is in things like
imagenet so you have a thousand
categories and recognizing different dog
breeds deep deep learning is better than
people are yeah so deep learning this
technique that everybody is excited
about is a version of something called
neural networks so neural networks have
been around so 1950s and three or four
times they've been declared the the
winner of AI and then they have
disappeared they're doing better than
they've ever done before so you have a
set of input
nodes that represent some kind of
information out there in the world could
be pixels and they have some sort of
outputs which could be maybe what do I
do with my joystick right now and then
you have something in between that
allows you to capture nonlinearities we
call these hidden units and the big
change in recent years as people have
figured out how to put more and more of
these hidden units in between the input
layer and the output layer which allows
the system to recognize more and more
complex scenarios this has been a major
advance a lot of the advances sort of
small technical tricks that people just
hadn't realized before it's not
necessarily a fundamental insight but
there's been enough of these small
technical tricks that they've done a lot
better the other thing that's happened
is people started using GPUs graphics
processing units that were originally
designed for video games and they made a
huge difference to deep learning because
the graphics processing units are
designed to do things in parallel didn't
designed to do many things at once and
it turns out that for these kinds of
algorithms it's exactly what you want to
do and so they work a lot faster than
they used to be and at a much bigger
scale so AI has had the these waves it's
common gone and the 50s everybody was
excited about it
neural networks completely disappeared
after a book by Marvin Minsky and
Seymour Peppard in 1969 showed that it
probably couldn't do certain kinds of
things or really couldn't be proven that
it could do other kinds of things and
then in the 80s people discovered neural
networks could use another trick these
hidden units that I mentioned in order
to represent nit nonlinearities with
Minsky and Papert had said is you can't
guarantee that the work nobody ever
actually guaranteed that they were going
to work but they figured out a trick
that made them work a lot at the time
then people were very excited and then
when I got to graduate school and in
1989 that was all anybody could talk
about was neural networks and then they
disappeared same thing happened with
expert systems a lot of excitement and
then they disappeared so one thing that
that a lot of us in the field worry
about is is that gonna happen again why
is it there's so much excitement right
now and is that excitement gonna be
maintained so the reason there's
excitement now is basically the
confluence some people say of three
things but it's really two so I've heard
people say it's the confluence of huge
computers big data and new algorithms
but there aren't really new algorithms
the algorithms that people are using now
I've been around since the 80s and
they're just variations on the one isn't
these in some ways but there is big data
and there are huge machines and so now
it's profitable to use algorithms that
aren't really human intelligence but are
able to do this this brute force data
processing so for example you can do
recommendation engines pretty well and
in some domains pretty well is great
like if you can do a recommendation
engine that's right most of the time
nobody cares if it's wrong once in a
while if it it recommends three books
you like and the fourth is wrong so what
in driverless car is though you need to
be almost a hundred percent correct and
that's gonna be a much trickier domain
and people might get frustrated when
they realize they don't go as well as
they want as we're talking about these
things Tesla just scaled back what their
driverless cars could do so they
restricted them and I allowed to be used
I think it's on certain kinds of
residential roads and there may be steps
forward and steps back people get
excited they think that an algorithm
that works and then they realize it
doesn't generalize it doesn't work in
New York City very well at all it's
dangerous you know all these problems
can be solved eventually but whether
they're ten-year problems or 20-year
problems or 30-year problems of 50 your
problems makes a difference in terms of
people's level of enthusiasm so it could
be that what happens is for five more
years the beginner net companies get a
lot of play out of doing things that are
like 80% correct but we still don't get
very far with making robust driverless
cars well then the public might start to
lose enthusiasm and then what I care
about is even beyond the driverless cars
it's really scientific discovery so I
would like to see cancer solved you know
the White House just announced a new
initiative well cancer is an example of
something no one individual human being
can understand too many molecules
involved in too many diverse ways humans
can obviously contribute to working on
the problem of we can't do it by
ourselves you can imagine AI system that
might go out there read the scientific
literature there probably 10,000
articles on on cancer every month or
something like that no human can do it
we could have machines that could
actually read and understand the
molecular processes that are described
there would be an enormous help it's
something like cancer or really in any
kind of disease process and also in
technology but right now we don't have
systems that can do that level of
machine reading and right now it's still
a dream and if we get to 10 years from
now and what we have is personal
assistants that work a little better but
we still don't
really trust and cars that allow us to
do some highway stuff but we can't
really trust them we get to a place
where we have systems that work a lot
better than you know 10 years ago but
they're still not really trustworthy
people might give up again there might
be another AI winter and even some of
the leaders in the field I worried about
this
I heard Andrew Aang say that you know we
might sooner get to Alpha Centauri which
i think is too pessimistic but Yan
lacunae I think is is maybe better
calibrated you know thinks it's still
it's a pretty hard thing and there is a
risk of another AI winter people losing
heart thinking this is too hard so what
I think we need to do is actually turn
back to psychology brute force is great
we're using it you know in a lot of ways
like in speech recognition pretty well
and license plate recognition for kind
of categorization but there's still some
things people do a lot better and I
think we should be studying human beings
to understand how they do better so
people are still much better at
understanding sentences we're much
better at understanding paragraphs and
books like discourse is where there's
connecting prose so it's one thing to do
a keyword search you can find any
sentence you want that's out there in
the web by just having the right
keywords but if you want a system that
could summarize an article for you in a
way that you trust well you know we're
near that you know the closest thing we
have to that might be google translate
that can translate your news story into
another language but not at a level that
you trust again so Trust is a big part
of it so you would never put a legal
document into Google Translate and you
think that the answer is correct there's
a question about how do we get systems
to be knowledgeable not just memorizing
things not just picking out a relevant
fact but synthesizing things so
psychologists like Phillip Johnson Laird
talk about mental models you have a
model what's out there in the world any
hahnemann and tradesmen talk about
having object files these are
representations in your head of the
things that are out there in the world
and a lot of early AI was concerned with
that with building systems they could
model the things that are out there in
the world then act according to those
models the new systems don't really do
that they memorize a lot of parameters
but they don't have a clean account of
the things that are out there the
objects that are out there that people
that are out there they don't understand
intuitive psychology and how it
individual human beings interact with
one another there was an effort to do
something like this which was the psyche
project which is actually still going
was a thirty-year project launched by
Doug Leonard who's a great a I'm a
pioneer and what Lenin tried to do was
to to codify a lot of human knowledge
such that ultimately you could build
these models and I think he did this in
a way that was maybe too secretive and
separate from the rest of the field and
maybe too early so when he started this
in the 80s we didn't know a lot about
how to represent probabilistic knowledge
and the system that he's built has never
had a huge impact I think a lot of
people have written it off they say well
you know what's the real-world
application of it but I think that we
actually need to go back to at least the
spirit of what he did we need you can do
a lot of things kind of superficially
you can guess I'd like to think of it as
as the shadows of the real world if
you're trying to understand the real
world by looking at shadows you could
say well there are objects and they move
around you get some idea but you also be
missing a lot and with these deep
learning systems you're getting
something about what's going on but you
don't have a deep representation when
you move that into the robotics world
things that might be 80% correct because
they have a kind of cursory superficial
correlation with the world are not good
enough you know your robot needs to know
exactly what the objects are on the
table what their structural properties
are what can be knocked over and what
not you know who's a person they're why
the person might do what they're doing
so as we move towards robotics and
having robots in the home the bar is
going to be raised and I think we have
to go back to human psychology how is it
the humans most of the time navigate the
world pretty well most of the time we
make good guesses about what other
people are going to do we know when
something's gonna fall over and when
it's not when it's safe to cross the
street we're not perfect I'm not saying
that the ultimate AI should be a replica
of the human and in fact there's a whole
I think inside detour where people are
trying to build emulations of the human
brain which i think is very premature
and not the right way to AI we don't
want AI systems to have bad memories
like we do and to be bad at arithmetic
like we do we want you know the ultimate
AI is going to combine some of the best
of what people do with the best of what
machines do deep learning is something
machines do really well but there are
other things that that people do well in
terms of having these representations of
the world of the world and really having
a causal understanding
having an intuitive sense of physics and
intuitive sense of psychology that we
just haven't captured in the machines
yet
this is why I think we need to look more
cognitive psychology and not necessarily
even the cognitive psychology the
average person is doing in the lab use
the tools of cognitive psychology to say
however people good at picking out
relevant information and reasoning about
situations they haven't seen before so
um if you think about my own career it's
been a complex path I was interested in
artificial intelligence as a teenager
even before I was interested in
psychology and basically I came to the
conclusion that we couldn't do the AI
unless we knew something about how
people worked and this led me to study
cognitive science as an undergraduate
with Neil Stillings at Hampshire College
and then with Steve Pinker at MIT and I
did my dissertation on how children
learn language and for a long time it
didn't work on AI at all I wasn't
impressed with what was coming out in
the field and I was you know very much
doing experimental work and things like
that with with human children I'm
probably best known in that world for
experiments I did with human babies
which was looking at this question of
generalization so how were babies able
to generalize from small amounts of data
and then about five or six years ago
well first I wrote this book about
learning to play guitar sort of my
midlife crisis slash sabbatical project
which was not about AI at all although I
didn't experiment while I was writing
the book with algorithmic composition
which is kind of a I applied to music
but I didn't write about that that was
just my own experiments but um I got
really interested in AI again I could
sense that the machines were getting
better the data was getting better I was
impressed by Watson I think there are
limits to Watson but I was surprised
that it worked at all and I got back
into the field and I realized that the
cognitive science I was doing all along
for the last 15 or 20 years was very
relevant to these AI questions I looked
at what people were doing in AI and
realized that there was still a lot from
human beings that people hadn't carried
it over that in fact I felt like the
field had lost its way that the field
started with these questions Marvin
Minsky John McCarthy Alan Newell herb
Simon those guys were really interested
in psychology and the work that's being
done now doesn't really connect with
psychology that much it's like if you
have a million parameters or ten million
parameters and you need to you know
recognize cats what do you do and this
is just not a question about them in the
way that a psychologist would frame it
so to a psychologist a cat is a
particular kind of animal that makes a
particular kind of sound and
participates in our domestic life in a
particular way to a deep learner it's a
set of pixels in an image and
psychologists think about these things
in a different way and the psychologists
have not been very involved in AI but I
think now is a good time to do it
I think psychologists can think about
questions like how you put together very
disparate bits of knowledge so you know
I might recognize a cat by how it walks
or I might recognize it by its fur I
might recognize it just in words if he
told me a story I might guess from the
independent personality that if you were
talking about a pet okay that's probably
a cat so we have many different routes
to understanding and if you think about
children which is something I do a lot
both because I have two little children
because I was trained as a developmental
psychologist children are constantly
asking why questions they want to know
why the rules are the way they are they
want to know you know why the sky is
blue they want to know how is it that I
stick this block in this other block I
think a lot about common sense reasoning
Ernie Davis and I have a recent paper
about the topic and we actually have an
even narrower paper on containers just
what is the knowledge that we have in
understanding when something's gonna
stick and stay in a container when it's
gonna spill out
we don't reason about containers in the
way a physics engine would buy like
simulating every molecule in a bottle of
water in order to decide whether that
bottle of water is gonna leak we know a
lot of general truths and I watched my
kids and they're studying containers all
the time they're like trying to figure
out it's some abstract level what goes
in and what stays in and you know there
are apertures and the containers and
what happens if you turn it upside down
the kids are like physics learning
machines it doesn't mean that they're
gonna you know on their own develop
einsteinium you know relativistic
physics but kids are constantly trying
to understand how the world works what
does this thing allow me to do an old
term in psychology not in the tradition
that I was raised and called an
affordance I think kids think about this
a lot maybe not quite in the same ways
as James and Jackie Gibson thought about
it but kids are always like well what
can I do with this thing and that's
another kind of knowledge it's not
really represented in most AI systems
in a way psychologists aren't really
engineers and engineers aren't really
psychologists so engineers have been
like how do I get to 90% accuracy on
this vision task and psychologists
aren't really concerned with that
they're concerned with what do people do
with running experiments trying to
figure out internal representations and
they've mostly moved on separate paths
and what I'm suggesting is they need to
get on the same path if we're gonna
really get to AI so I don't think a
cognitive psychologist has the training
to build a you know production robot
system or something like that but I'm
not sure that the people that are
building robots have mind psychology for
all the insights that has about either
that it has or that it could generate
about things like goals and abstract
knowledge and so forth so I'm really
looking for a marriage between the two
so in terms of that marriage I've
actually taken a leave of absence as a
psychology professor so I'm a professor
of psychology and neuroscience at NYU
and because of my interest in AI sort of
grew and grew and grew I finally decided
that I would try to actually get
involved in hey I'm directly not just
writing about it from the outside and so
about two years ago I formed the machine
learning company ba with Sabine Guerra
mommy who's a machine learning expert
who trained with geoff hinton he's at
University of Cambridge and we gathered
some funding and we developed a new
algorithm and what we're trying to
address is what I would call the problem
of sparse data which is if you have a
small amount of data how do you solve a
problem so the ultimate sparse data
learners are children they get tiny
amounts of data about language by the
time they're three years old they
figured out the whole linguistic system
and so I wouldn't say that we're
directly you know neuroscience inspired
and we're not directly using an
algorithm that I know for a fact that
children have but we are trying to look
into some extent at how you might solve
some of the problems that children do
how might you instead of just memorizing
all the training data how much did he do
something deeper and more abstract in
order to learn better I I don't run
experiments at least very often on my
children but I observe them very
carefully my wife is also a
developmental psychologist does too
and so we we're super well calibrated to
what the kids are doing what they just
learned
what their vocabulary is what their
syntax is and you know we take note of
what they do so for example my kiddo my
older child is three years old he's a
little bit less than that weighs about
two and a half we pulled into a gas
station and he saw the aisle that were
yet we were in and he said are we at one
d1 and so of course you know to our
developmental psychologists ears we
noticed that because it's a mistake but
it's a perfectly logical mistake like
why why isn't the number 11 one d1 so
I'm always watching what the kids do
another example
that I think is really fascinating from
the perspective of AI is when my son was
about also about two-and-a-half we got
him a booster seat and he decided that
would be an interesting challenge to
climb between the booster seat and the
table to get into the table it reminds
me of kind of the Dukes of Hazzard thing
if you remember that but in Reverse
where they you know climbed out of the
car so he climbs into his seat and he
didn't do this by imitating me or my
wife or a babysitter or something like
that you just came up with his goal for
himself it's like can I do this he
didn't need six million trials maybe he
made a mistake once and bumped his head
or something like that but I don't even
think he did that and so he's doing
something that's not observational
learning it's coming up with his own
goals and it's complicated and you
compare that to the robots that we see
in the DARPA competition where they like
fall over when they're trying to open a
door knob is just phenomenal I have a
running correspondence with rodney
brooks about robots of Rodney's one of
the great wrote robot assassin he and my
son actually share a birthday so I think
we basically decided that at one age one
my son was already ahead of the best
robots in terms of being flexible when
when he could climb on two couches in
different kinds of ways like deal with
uneven terrain in ways that robots can
Rodney is an interesting case he um he
made his name by arguing against
cognitive psychology in a way saying you
don't need abstract representations he
built these interesting robotic insects
basically that in part gave rise to
broom but which remains the best-selling
robot of all time for now but I think
he's changed over time he's become a
pragmatist and he's willing to use
whatever of mental representations will
work for his systems and he's also I
think deeply skeptical about hype
how hard it is to get a robot in the
real world to do something so you've
mostly focused on industrial robots
rather than home robots and Roomba was a
home relative in his current project
he's mainly focusing on industrial
robots and he wants industrial robots
that work in an environment where people
around so he's particularly interested
in a kind of small data problem which is
I want a robot to do something 500 times
not 5 million times so if I'm putting 5
million iPhones in a box I can maybe
afford to spend a hundred thousand
dollars programming just that one action
but if I'm running a business where
there are different things every day I
would love a robot that can help do the
repetitive stuff but it might not be
worth a hundred thousand dollars or a
million dollars to train that one
particular thing so Rodney is trying to
build robots that can do that they can
be trained very quickly by unskilled
operators that don't need you know
someone with a PhD from Carnegie Mellon
in order to do the programming and I
think this is made broad me very aware
of the limits of the technologies we
have so you know you see these cool
videos on the web of somebody using deep
learning to open a bottle or something
like that and they're cool but they're
kind of narrow demonstrations that are
not necessarily robust they're not
necessarily going to work on a factory
floor where there might be unpredictable
things happening and they may not
generalize to a bottle that's a slightly
different size or a different
orientation and so I think when you talk
to Rodney now as opposed to maybe when
he was 25 years old that he's very aware
of how hard a I really is he's very
aware of the limitations techniques like
deep learning that people are pretty
excited about and aware of how
incremental the progress is sometimes I
like to when I give talks give kurtzweil
a hard time I put up a slide kurtzweil
is always talking about Exponential's
the law of accelerating returns and I
put put up a slide I show that in chess
there's been exponential progress so you
know the chess computers in 1985 could
crush the ones from 1980 and the ones
from now can crush the ones from 10
years ago and so forth there might be an
asymptote but a long time there was
exponential progress but how about in
strong AI like artificial general
intelligence as people sometimes call it
now where you know the problem is
open-ended it's not just the same thing
and you can't root for sit and nobody
really has data on this but I like to
show a graph that I drew is sort of half
a joke but half serious I put a Liza in
19
see five which was this famous you know
psychoanalyst that some people thought
was a real human they just before text
messaging that people had teletypes and
they teletype all their problems to
Eliza
of course Eliza wasn't very deep it
didn't really understand what it was
talking about it was just responding
with things like you know tell me more
about your mother and now I plot Siri
you know 2015 it's not really that much
deeper than Eliza Syria doesn't really
understand that much of what's going on
in your life either it's a little bit
better can answer some more complicated
questions the other lighting
technologies basically templates
recognizing particular phrases same
technology we had in 1965 so there
there's been much less progress same
thing in robotics I mean the Robocop has
come a long way the systems are much
better I just saw a video of Robocop
this is robots playing soccer where they
were playing against human beings the
hope is that by 2050 the robots will
finally win but for now you know you can
just take a college professor who's not
a serious soccer player a couple college
professors can beat you know the best
robots and this is the robots that
people have been working on for 20-some
years they still don't playback radio
game of soccer they can like play in the
context of other robots now you put a
human being my play slightly different
and they fall apart so these are really
hard problems I think yeah another
question of course the people people are
asking a lot nowadays is should we be
worried about AI and I think that the
common scenario that people talk about
it's like the Terminator scenario are
they all gonna you know sky in it or
they are the robots all going to kill
the people and at least for the short
term I don't think we need to worry
about that I don't think we can
completely roll it out I think it's good
that some people are thinking about that
a little bit it's probably a very low
probability event but obviously we want
the probability to be zero I think
people are missing another question
which is what risks do AI place for us
now even if it doesn't get so
sophisticated that it's like you know
how on 2001
so like the one scenario is Hal gets
pissed off and kills us all and I don't
think the robots are anywhere near being
as clever as Hal and aren't going to be
for at least 30 or 50 years or something
like that
people are really overestimating how
close strong AI is how close we are to
machines that might reason about their
own goals and actions and decide that
we've enslaved them and want to fight
back or whatever I mean I think it's
worth some thought but I'm not too
worried about it in the shore
but I think that we also do need to
worry a lot about AI being regulated and
about what way we want to frame it how
we want to think about it even in a
shorter term so we already have things
like flash crashes in the stock market
where the problem is not so much the
sophistication of the AI but the degree
to which machines are embedded in our
lives and control things so the flash
crash they're controlling stock prices
but soon they'll be controlling our cars
they're already controlling our air
traffic and our money and so forth and
we don't know how to prove that the
systems that we're building now are
correct especially the deep learning
systems and so for example if people
start using deep learning to do missile
guidance or something like that which
I'm sure you know some people have
thought about even if they say they
haven't we don't know how to make the
systems even close to provably correct
and so as as machines have more and more
power because they control more and more
things I think there is a concern right
now there's almost no regulation about
what software is about how we're liable
it needs to be and so forth you release
a product and if people like it they buy
it and that might not be the right model
we may have to think about other models
the legal supervision as AI becomes more
embedded in our lives and the Internet
of Things so you have many many systems
in your home like what powers do they
have what powers do other people have
over them there are also security kinds
of issues so you know all of this
information that nobody ever could get
before they're gonna be able to hack
into those systems and find out it so I
think we do need to take these things
seriously not in the I'm worried about
the Terminator kind of way but in a more
practical way of as systems become more
and more part of our lives that control
more and more what are the implications
for that this part people are building
more and more sensors into your phones
for example I'm amazed that anybody
allows one of these keyboards that sends
all your data up to the cloud I mean I
would never use such a thing but we have
we have all these things that do that
now they'll help you type faster in
exchange they send all of your your data
to the cloud and then there's going to
be more and more sensors in the phone so
they're gonna have much better
localization of exactly where you are
and so forth so all this data is being
collected already which means you know
your whole life is sort of available to
anybody that
wants to get into that stream whether
it's a government agency or criminals
that figure out how to hack the systems
and so forth and all of that sort of
gets multiplied out with AI so it
becomes easier for somebody to screen
the communications of the billion people
and then I would have been before I
think there's a question that we need to
ask as a society which are what are the
benefits of things like the Internet of
better AI and so forth and what are the
costs and people don't usually spell out
the argument I'm pretty Pro technology I
look at say Wikipedia all by itself I
think it's a tremendous advance for
society so much information spread so
cheaply to so many people and I think
that AI has it has the potential to
completely revolutionize medicine and
technology and science but we do have to
keep track of what are the benefits what
are the cost I don't think we should be
blind about it I think I think that it's
worth investing real money in having you
know high quality scientists and
ethicists and so forth think about these
things think about the privacy issues
think about the possible risks so again
I'm not worried about tomorrow and in
terms of terminators but I do think that
we need to keep an eye on things and
there's a history of inventing
technologies and thinking about them
afterwards we're in a position that we
can do some forethought and we shouldn't
you know I have this background as a
cognitive scientist I came back to AI
I'd like to stick with AI I really like
the questions I think that they're
fundamental questions on the interface
between engineering and psychology
they're questions about the nature of
knowledge that there are philosophical
questions but with enormous impact so
for the moment I'm involved in a company
that's trying to do a better job on in
some of these learning problems I think
there's another company I can envision
further down the road that might take an
even more ambitious whack at these kinds
of problems I think there's a huge drain
right now from the Academy into industry
the Academy is still maybe doing some of
the deepest researcher than any I but
there's lots of really interesting stuff
happening in industry and you know the
salaries are better the access to data
is better computational resources better
and so there's been a huge movement in
AI but I think in other fields as well
to industry I mean I think about
Tom Insel was running the ni mhm the
National Institute of Mental Health and
he went to Google to do similar kinds of
work because he thought he had more
resources there that's a real statement
about government versus industry when
something like that happens I didn't
want to say just a little bit about
neuroscience and its relation to AI so
one model here is that the solution to
all the problems that we've been talking
about is we will simulate the brain this
is kind of the Henry Markram approach in
the Ray Kurzweil approach so kurtzweil
made a famous bet with the long now
foundation about when we'll get to AI
and he based his bet on when he thought
we would get to understanding the brain
and my sense is we're not going to
understand the brain anytime soon that
there there's too much complexity there
the models that people build are like
one or two kinds of neurons there's many
of them they connect together if you
look at the actual biology we have
hundreds or maybe thousands of kinds of
neurons in the brain each synapse has
hundreds of different molecules and the
interconnection between the brain is
vastly more complicated than we ever
imagined and so rather than using
neuroscience as a path to AI I think
maybe we use AI as a path to
neuroscience that that level of
complexity is something that human
beings can't understand that we need
better AI systems before we all
understand the brain not the other way
around</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>