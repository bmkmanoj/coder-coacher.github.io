<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Future of Artificial Intelligence - Prof. Stuart Russell | Coder Coacher - Coaching Coders</title><meta content="The Future of Artificial Intelligence - Prof. Stuart Russell - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Future of Artificial Intelligence - Prof. Stuart Russell</b></h2><h5 class="post__date">2017-12-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-8eChXXFB1c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">egg up and set up his machine so dr.
Russell has a wonderfully eclectic
resume as you saw so he's a professor at
the University of California Berkeley so
thank you for making the trek out here
you saw an excerpt from his textbook
which was pretty cool and you'll also
notice on his resume that he has worked
with the UN so he's got pretty uh pretty
eclectic background so with that over to
you okay thank you very much Fred so
I've been given the task of covering the
whole of the future of artificial
intelligence in 30 minutes so let me
begin by saying what it is and we all
know it's about making machines
intelligent and for some time maybe
about 30 years we've understood that to
mean machines that that do the right
thing that optimize the selection of
actions in pursuit of their objectives
or optimize expected value in economic
terms and I put an asterisk on that
because I'm gonna tell you later that
that's the wrong definition and we've
been barking on bucking up slightly the
wrong tree as a result of this and I
take some of the blame for that I do
want to point out that it has a very
large range of topics and some of the
previous speakers have pointed out the
the over emphasis on deep learning so
deep deep learning is one sub sub sub
field of machine learning and it's not
to to minimize the importance of what's
happened in the last 5 years or so but
actually to point out that you could see
similar advances happening in any of the
other subfields and having similar
magnitude of impact on the capabilities
of AI systems and I couldn't couldn't
resist putting in one of the impressive
examples this is a picture from a Google
blog post from Sammy Ben Joe's group
with oriole Danielle's and others and
this this task is producing captions for
images and here's the caption a group of
people shopping at an outdoor market
there are many vegetables at the fruit
stand and this is shown as one of the
sort of crowning achievements of the
deep learning community but notice that
there is no group of people that nobody
is shopping at all
and there is no fruit stand so so I
think that illustrates that that
sometimes you can take you can go a
bridge too far
you can try to to learn by supervised
learning a mapping that is that is too
complex that has intermediate stages for
example the recognitions of objects the
recognitions of spatial relations and
interpersonal relations in the scene and
so on and sometimes if you try to bridge
it all all the way from the image to a
caption you can go too far and now I
think the community is recognizing that
in fact it's helpful to have
intermediate relational representations
as a recent paper from deep mind on
relational networks that shows
dramatically improve performance from
explicitly first of all representing the
good ol fashioned logical content of the
image in terms of objects as spatial
relations interpersonal relations
grasping and so on and then on top of
that training to recognize higher-level
predicates or to give textual
descriptions okay so very briefly where
we are now we've seen as we see Earth
from the previous speakers very rapid
progress on several fronts and Android
has a useful description of where we're
at right now
anything that humans can do in one
second we can probably automate
reasonably well provided we have enough
label data we also have quite capable
physical robots they are agile for
example Boston Dynamics has incredibly
good leg locomotion and you know
clambering climbing stairs and so on we
have very agile flying robots we don't
have particularly good dexterity at the
moment this is a puzzle maybe a chicken
and egg problem that robot hands and the
algorithm is for controlling robot hands
have really lagged behind the rest of
the field but I assume this will get
solved quickly enough we have perception
it's not perfect but it's good enough to
navigate successfully in the real world
as we've seen with cars we have very
good short horizon tactical
decision-making as we've seen and go but
remember you know and go you can beat
the best human by looking ahead 20 or 30
steps but in deciding to come here when
you look at the one of the primitive
actions required for you to participate
in this meeting it involves something on
the order of a couple hundred million
steps for you to be here and be sitting
down listening to this talk okay and so
there's simply no way that you can scale
up what alphago did
looking ahead twenty or thirty steps to
two hundreds of millions or billions of
steps which are the kinds of activities
that humans engage in and of course
humans do that by operating not at the
level of primitive moves on the board or
move movements of their hands and
tongues and so on but actually higher
levels of abstraction like you know
booking a flight you know choosing to
take the train from from New York down
to DC for example you know taking a taxi
or an uber those are the things that we
decide about and that gives us the scope
to make decisions that cover billions of
time steps in the real world there's
also been progress in in complex
horriblest acknowledged representation
and reason there's much much less
visible than what's been happening in
deep learning and it's happened more
slowly the first papers in this area in
1997 so it's about twenty years that
we've been doing horriblest acknowledged
representation and reasoning at the
level of full the full expressive power
of turing machines and that's a that's a
really big deal i'll just give you one
example from our own group so many of
you familiar with the comprehensive
nuclear-test-ban treaty which bans
nuclear explosions and the UN
organization the CTBT o-- in vienna has
built a very impressive monitoring
system with hundreds of seismic stations
spread all over the world and then all
that data is is sent back to
Vienna and then interpreted to detect
the occurrence of nuclear explosions as
well as all the other significant
seismic events that take place in the
earth so with these kinds of expressive
probabilistic models we can actually
just do this as Bayesian inference so
what we do is we write down a model of
what we know about the geophysics and
the sensors in the parable istic
programming language and then we just
run inference and so we're not really
writing special-purpose software for
interpreting seismic data and solving
the nuclear monitoring problem we're
simply writing down the geophysics and
doing general-purpose inference on the
model and naturally these languages can
handle complicated uncertain
relationships for example the the
signals from seismic events are very
complicated and involve many different
types of waves with different velocities
and paths through the earth and the
connection between detections and events
is completely uncertain it's like
listening to a thousand extended
conversations all running at the same
time overlapping each other and then
trying to figure out who was saying what
when and where and but this is the this
is the model it took me about half an
hour to write and took a little while to
get the inference to run fast enough to
handle all the data coming from the
whole planet in real time and then these
are the results so this shows the
basically the the failure rate of the
existing UN system so between 30 and 50
percent of seismic events of significant
magnitude are missed by the existing
United Nations automated system and then
this is the failure rate of the system
that we built with the model that I just
showed you called net visa and so
between two and three fold reduction
here's detecting the 2013 DPRK nuclear
test we got a much more accurate
location than the combined efforts of
all the world's to your physicists who
produce that the green triangle on the
top left and
quite a bit closer to the true location
and then we've have a more sophisticated
model which you can't really explain it
has a lot more detail and in how we
write the geophysics of the structure of
the signal itself but just to point out
that in the in the low magnitude ranges
one to two we're actually finding 20
times more events than not only just the
UN automated system but actually the
expert to your physicists who work
full-time at the UN to analyze all this
data and we're even finding using just
10 seismic stations that are thousands
of miles away more events than the u.s.
array which has thousands of detectors
and was right on top of the region at
the time this data was collected so we
can see this kind of performance
improvement and it's not done by deep
learning we have very little training
data and you can think of what's going
on as a kind of a deep generative model
in fact if I I should probably stop
showing this right which is the model in
the formal language that we have and I
should show you pictures of hundreds of
thousands of random variables being
generated and and then deleted and
regenerated and restructured in real
time as the inference proceeds and then
you'd all be impressed and say oh wow
it's just like the brain right but it's
in fact it's exactly the same thing this
is this is a deep generative model with
about eight to ten layers and when
there's a lot of seismic activity you
can have 500,000 to a million random
variables that are dynamically generated
and then inference processes are
operating on those ok
show you a completely different example
this is a model in using the same
modeling language for for natural
language so the model basically says
there's a there's a real world in which
there are some objects and there are
some relations and some of the objects
are related by some of the relations and
then there's a way of expressing
relations in text and then you choose
some things that are true in the world
and you say them using the way of
expressing relations and text and that
produces the text so that's what the
model says
literally nothing more than that we
provide a bunch of text data from the
New York Times that was helpfully
pre-processed by Andrew McCallum's group
and then we just ask well what's true so
this is completely unsupervised so we're
just writing this completely generic
model like it has no knowledge of which
language we're talking about no longer
no knowledge of what's true in the world
no knowledge of what relations exist in
the world has to discover all of that
for itself and and so here's an example
of what it discovers so one it discovers
a couple of hundred relations here's one
of them number 46 and this is the these
are the textual patterns that are used
to express relation 46 when you want to
write down a sentence in English and
when you look at these these all turn
out to be that one object is a
subsidiary of another and so it's
discovered this ciliary relationship
under 16 ways that that's written in the
New York Times and then you can also ask
well what what facts hold for relation
46 in the world and it tells you all
those facts okay and these with more
than 95% accuracy with no training data
whatsoever okay so one of the questions
raised in the the the telephone call we
had with fred is where where are we the
US where is the rest of the world my
impression has been that as has been
true since the beginning of the field
that the US and UK are still far ahead
in basic research in terms of pushing
the frontier solving these fundamental
problems that we have the other
countries are not really contributing
all that much I guess I should add in
Canada - right Canadians obviously
Canada could contribute quite a lot to
the deep burning and China there's no
question that China sees AI as a huge
key technology for strategic dominance
and I visited horribly ten times in the
last couple of years
talk
to lots of different groups so there's
enormous level of interest and some very
very impressive capabilities
particularly in user-facing technologies
like speech recognition where companies
like $0.10 are doing some great work by
do the the Chinese analog to Google also
has some really impressive stuff and
hand ruing was contributing to them for
a while also their their quadrotors
they're sort of small consumer drone
capabilities are actually probably
better than the u.s. so DJI has about 70
percent of the mid-range market in the
world so so it's clear that and they
just announced a huge new program to
invest in research and development in
this area so when when China puts a big
chunk of resources behind something you
can expect that they will catch up
fairly quickly one of the things that we
see in China in Europe in the u.s.
everywhere
is that the rate of growth of the
industry is outstripping the ability of
academia to keep up - to supply highly
trained people and that's being
exacerbated by companies stealing
academics so that for example Stanford
has been kind of wiped out is not really
able to produce high-quality PhDs for
what I'll have to regrow their AI group
and CMU's had some losses we haven't
seen that happen at Berkeley yet but it
could happen any day and then finally I
think that there's too much focus on
deep learning at the to the exclusion of
these other areas that I think are
really important and actually will
contribute to solving the problems that
deep learning is running up against in
terms of its capabilities so in the
national security area there are lots of
things to be gained from AI I already
gave an example of monitoring early
warning systems situation assessment
these all help
and your defensive capabilities doing
intelligence analysis I'm not privy to
what goes on inside NSA in terms of how
they interpret their data but last time
I visited I knew I know that they were
doing our ballistic programming so
that's a good sign but presumably
particularly as we make progress on
natural language understanding and
behavior recognition we will see the
ability to to use AI systems to analyze
in detail the entire signal collection
capability output of the u.s. it's also
important and this was a point that was
made in a dinner I went to with Henry
Kissinger that if you can change the
nature of military tactics in ways that
have that no human has previously
conceived that could have a
revolutionary effect you could
completely change the strategic balance
without necessarily changing the aspects
but just the way you use them to
construct military campaigns we saw that
to some extent with this idea of getting
inside the decision loop in the first
Gulf War where we could make decisions
so much faster than the Iraqis that it
was really no contest but you could see
similar things happening with the
ability of AI systems as happened in go
for example where they just play in
revolutionary ways that humans never
thought could be right or feasible and
all of a sudden it turns out that we
were wrong and the machines figured out
new ways of doing things and just
looking optimistically if the AI
revolution comes to pass with robot
manufacturing and AI systems doing all
kinds of of work we should have much
greater productivity in global wealth
and less resource competition between
countries which can't be bad now on the
risks Rao pointed out there are many
additional risks coming from both AI
systems being used as weapons in
cyberspace and arguably some of the
malware already is simple kinds of AI
but you know particularly the techniques
people use to get through
classifiers and produce emails that are
misclassified but we will see more and
more of that and as more and more of our
economy is based on AI systems doing
things that it expands the attack
surface I do want to talk a little bit
about lethal autonomous weapons and I
put that on the nefarious side even
though of course some people might think
that expands us military capability I'm
going to argue that's a mistake the
economists now having poo-pooed the idea
of technological unemployment for
decades now seem to be coming around to
the idea that in fact this time it is
different and I've heard several the
world's leading economists say that this
is the biggest threat that the world
economy faces that we will see over the
medium term 15 20 25 years really
significant disruption to the economy
that could cause major societal problems
and then finally the existential risk
are we able to control super intelligent
machines that we build so in cyberspace
as you many of you know probably much
more than I do
the stuff that's going on is really
amazing I was having a conversation with
a cybersecurity expert I think it was at
the World Economic Forum and I was just
sort of tossing off you know the
speculative idea that you could build AI
systems that would read people's email
figure out what they were doing that
they shouldn't be doing and then do a
kind of customized blackmail of that
person based on what that person was
doing wrong and he said well actually
that's already going on and I actually
use reinforcement learning so that you
know when they make money from that day
that you know they get better at doing
it using using money as the
reinforcement signal and I think when
that starts to happen at large scale
then that's really sort of damaging the
quality of life of pretty much everyone
on the planet we've already seen the use
of fake news to change clinical behavior
and that's going to get worse as we for
example synthesize perfectly realistic
video of any human being doing anything
you want it's very hard to take that out
of your mind once you've seen it even if
you later find out oh that was a fake
right it's it's seared in your
consciousness forever and I really think
we need to do something quite serious to
put a stop to this just straightforward
stealing of cash is now a very large
business probably getting up there
towards the the scale of the oil
industry and talking to executives from
from very large international banks they
are seeing their customers no longer use
electronic data interchange for
financial transactions that's a real
problem when that happens so when a
company that does 10 million
transactions a year has to use facts
with voice confirmation for every single
transaction then you know something is
seriously wrong with our the security of
our electronic infrastructure and I
would say that our we're leaving our
populations unprotected and our AI
systems are also going to be unprotected
so for example if I have an AI system
that can go shopping on my behalf the
chance that it's going to get defrauded
is going to be a hundred percent I you
know this is not just you know ostriches
and buses this is just you know small
print you know fake goods etc etc etc
it's gonna be really hard for AI systems
not to be submitted in that way
so I think we need to have some rules of
the road and we don't have to start with
you know a law saying that everyone has
to tell the truth on the Internet and
the government's gonna dictate what the
truth is but we can have some very
simple rules and and part of it is
promoting a basic human right to mental
security we have a human right to
physical security and governments that's
one of the first duties of governments
is to provide that to its citizens both
from against crime and against invasion
but mental security meaning the right to
live and in
information environment that is largely
true is extremely important and you know
we right now the US has invested about
five million person-years five billion
person years in getting itself educated
and then we only allow it to be undone
by false information that seems like a
really crazy equation okay so only
briefly talk about least autonomous
weapons or weapons that can by
themselves locate select and eliminate
human targets and there are simple forms
already landmines and and they're so
stupid that they were banned so let's
make them listen make them more
intelligent so we have a sentry robot
which is kind of a landmine with a
machine gun
that can see people and then kill them
and then Israel has a missile that can
can loiter for many hours around the
geographical region when it finds
something that matches a target criteria
which could be a radar signature but it
could also be a visual signature for
example something that it thinks is a
tank like a school bus in Armenia then
it will dive bomb it and destroy it
so the real problem is not the Herot
missile which is about 25 feet long and
has a 500 pound payload but this right
this is something that you know
wholesales for about a dollar you can
buy them for four five or six dollars in
toy stores and the capability of those
systems to attack individual people
right to to identify humans and to kill
them how does it kill them well you only
need one gram shaped-charge to blow a
hole in nine millimeters of steel plate
so it's easy to see that that thing can
easily carry a 1 gram payload and land
on your head and blow hole and so it's
not hard to figure out that you can you
can make micro you of these that are
lethal you can put about 3 million of
those in one regular container and do a
lot of damage and it's a very asymmetric
kind of weapon it doesn't require a huge
military-industrial complex
you know with with a lot of standing
army and all the rest of it to have that
kind of of impact and in that way it's
completely different even though you
know Kalashnikovs are very lethal and
you could get three million clash
nogales it's a bit bigger than a
container you know a small ship but you
can't do anything with three million
clashing offs right unless you have
three million soldiers to carry them
which means you need to have 20 million
people to train and support and
transport all those people and you need
a nation-state but with the autonomous
weapons it's completely scalable and
doesn't require all the apparatus of a
nation-state so you're creating weapons
of mass destruction that are scalable
and I think you know if someone wanted
to put a serious effort a kind of
Manhattan Project into this it doesn't
require any breakthroughs in AI it's
easier than a self-driving car and could
be done in less than two years so the
u.s. is already working on defensive
technology against these kinds of swarm
weapons and I would recommend that that
be that we shared to reduce the
incentive to develop the offensive
weapons and that we need a treaty
banning at least the anti-personnel uses
of autonomous weapons coming back to the
the attack services of AI systems
another issue with autonomous weapons is
that they are hackable like any computer
system and so if your defense posture is
based largely on hackable weapons you
don't even know if you have any weapons
you might think you have weapons but in
fact they're weapons that now belong to
the other side and so this this may not
be the most secure way to build your
defense posture ok so in terms of going
forward looking ahead into the future I
think my list of what's really missing
in current AI technology is a little bit
different from rouse but one of the
things we lack is real understanding of
language but I believe this will happen
relatively soon because they the
commercial is said to be so huge and I
think we have many of the pieces of the
puzzle already an integration of
learning with knowledge
coming back to this idea or one-shot
learning or low shot learning how do you
how do you learn from few examples you
learn from few examples by knowing a lot
already about the domain in which the
examples are occurring I mentioned this
idea that we plan over very long
timescales at the moment we know how to
do that if the human surprise supplies
the levels of abstraction but we don't
know how to do it when the human doesn't
supply that we don't know how to make
learning systems construct those levels
of abstraction in behavior automatically
but again we have many of the pieces of
that and that progress could happen
fairly quickly so they require
conceptual breakthroughs but it's very
unpredictable to say how long it's going
to be before those occur and I always
like to give the example from nuclear
physics that on September eleventh
nineteen thirty three the the
established wisdom was that there was no
possibility we would ever be able to
extract atomic energy from atoms on
September 12 so sixteen hours after
Rutherford gave a very famous speech
saying it was impossible Leo Szilard
invented the chain reaction right so so
don't bet against human ingenuity and
it's odd so Rao brow was saying well
we've gone from you know AI how so sad
pathetic doesn't work to all we know
please don't kill us
right but within the AI community the
reaction has been the reverse right we
spent sixty years saying we're gonna
achieve human-level AI now somebody says
well you know if you do that'll be
really bad and then we say oh no well
we'll never do it right you know it's
impossible it's great you know it's way
way way centuries away it's way too
difficult we can't possibly get there
right which is really weird okay so
assuming that's true that we will
eventually make better decisions than
humans and I think that is true we can
certainly have machines that know way
more as soon as they can read they can
read everything the human race has ever
written right you know humans can't do
that we can't read everything we've ever
written but a machine can and you know
if we solve a couple of those problems
about levels of abstraction
have machines they can look further into
the future just as alphago can on the go
board they'll be able to do that in the
real world and this is not a new thing
right here's a quote if we could keep
the machines in a subservient position
for instance by turning off the power of
steam moments we should as a species
feel greatly humbled so this is this is
not Elon Musk or Stephen Hawking right
who arguably are not AI researchers this
is Alan Turing and if that's the problem
right so that this sort of vague unease
that you make something smarter than
yourself you're gonna be in trouble
right this is what we might call the
gorilla problem so their ancestors made
something smarter than themselves namely
us and here they are having a meeting to
discuss whether that was a good idea and
you can guess you can kind of guess just
from the facial expressions there
they're pretty sure this is a terrible
idea but then they go what's the
response to that well I mean you know
the response of that is we better stop
doing AI and I think that's neither
possible nor desirable because as I said
earlier there's lots of great things and
I can do so we have to understand why is
it a problem why is making better AI
worse right what's that
what's the reason and the reason is
we're stated quite clearly by Norbert
Wiener in a paper 1960 we better be
quite sure that the purpose put into the
machine is the purpose which we really
desire because I said AI is about making
machines that you know act optimally in
pursuit of their objectives if they have
different objectives from ours then
because they're more protectively more
intelligent than us then they get their
objectives and we don't write it becomes
like a chess match between machines and
humanity because we put the wrong
objectives into machine even if it's
something that sounds benign like cure
cancer right well you know there's a
quick way to cure cancer which is to
make everybody in the world into a
guinea pig in experiments with various
drugs right well that wouldn't
necessarily have a good outcome for the
human race but that was what the
objective was put into the machine so we
have to make sure that we don't put the
wrong objective into the Machine and
this is some
that goes back in human mythology for
millennia king midas is a perfect
example he said I want everything I
touch the turn to gold he got exactly
the purpose that he put into the machine
and then he regretted it because that
was not in fact what he really wanted
but what he said he wanted okay so how
do we get around this well I don't want
to go into all the math but this there's
basically some simple points the first
is that when you design machines they
should be designed only to optimize
human objectives not their own
objectives they don't have any
objectives because I don't care how
happy the machine is and how fulfilled
it is and how well it achieves it
objectives I only care about my
objectives now and so that should be the
the first rule the second point is that
the machine has to be explicitly
uncertain so if you like humble it knows
that it does not know what exactly the
human wants and this turns out to be
crucial but then it there is a method of
finding out what humans want which is
observing their behavior
so before you get all upset about well
whose values you know Christian values
Islamic values fundamentalist values no
none of those things right I'm not
talking about ethical codes I'm not
talking about touchy-feely notions of
human values I'm simply talking about
individual preferences over what life
you want
compared to lives you don't want right
so it's there isn't one set of values
and the robot doesn't have to adopt in
fact does not adopt any of these values
but it simply learns to predict what
humans want and then tries to help them
get it that's it okay so that's very
straightforward and so to learn from
human behavior involves what we call
inverse reinforcement learning which
basically means that by when you observe
the behavior of some other agent that
you assume to be intelligent you can
figure out from the behavior what is the
objective what's the underlying purpose
that's generating this behavior and
that's in fact the most
synched explanation for the behavior so
just to give you a simple example right
so this is me and morning all right so
you can tell what it is that I want okay
it's a very straightforward concept and
in fact there's a huge amount of
evidence that we can use to to learn
about human objectives from human
behavior so pretty much every book
that's ever been written describes
people doing things and then other
people being upset you even go back to
you know Babylonian clay tablets right
they say you know I traded five cows for
40 bushels of corn which tells you right
there something about the human utility
function but you know the corn was
rotten and now I'm gonna kill him right
so this is the kind of story that you
see in these clay tablets all the time
but you know we can also have the
machines watch TV or or just observe
humans directly so there's massive
amounts of evidence of what it is that
humans choose to do and if you can
invert through and the tricky part is
inverting through the human cognitive
structure which is very complicated to
get at the underlying preference
structure over lives that the machine is
good then going to use to make sure that
it doesn't do things that make us
unhappy so there are some complications
here so the first is that that the
standard theory of inverse reinforcement
learning which has been around for 20
years is not quite right
for one thing right if the machine
observes me wanting coffee I don't want
the machine to want coffee
that's not right right I want the
machine to understand that I want coffee
and then to learn how to help me get it
but also if I want if I have a robot in
my house and I want it to learn what it
is that I want then I'm not going to
behave the same way as I would if the
robot wasn't there right so it's really
it becomes a two-player game not a not a
one not one human being observed through
a two-way mirror by a robot watching the
behavior but actually a cooperative
process so you can set this up directly
in game theory in the human as it were
knows the value function right so they
act according to their own values
the robot doesn't know it but wants to
maximize it so that's the basic setup
and then when you when you look at the
solutions the optimal solutions of this
game have the right kinds of properties
that you want so the robot will ask
questions first it won't just act and do
something that may be quite risky well
she asked you whether this is okay
and the human will also have an
incentive in this game to teach the
robot to act in such a way that the
human preferences are more explicit and
in fact it changes the fundamental
assumptions of inverse reinforcement
learning the human will no longer act in
a way that would be optimal if the human
were were just acting by themselves and
therefore the inverse reinforcement
learning agent the robot in this case
should not assume that what it's
observing is optimal behavior in the
single agents sense let me give you one
example
remember that Alan Turing said wait
maybe we could switch off the machine
right so but you could ask well why
would the machine you know let you
switch it off right it's a super
intelligent machine it's not like it
couldn't think of that all right and
it's got some objective you know even
something like fetch the coffee and its
reasons to itself okay well I can't
fetch the coffee if I'm dead so I need
to make sure that no one kills me no one
switches me off right and this was a
point made by Steve on 100 years ago
it's one of the prime arguments as to
where the risk comes from that machines
will defend themselves against attempts
to change their objectives or just
switch them off and it seems like when
you put it that way well of course you
know no matter what the objective you
can't do it when you're dead so pretty
much every machine is gonna defend
itself against being switched off how
can we prevent that and the answer is if
the robot is uncertain about the
objective it understands that the human
knows more about the objective then it
does then it reasons reasons to itself
ok the human might switch me off but
only if I'm doing something that the
human doesn't like I don't know what
that is but I don't want it to happen
right I don't want to do something that
the human doesn't like so I will welcome
being switched off because that will
prevent a
Hastur fee and that's my goal okay and
so in fact who you can just do the
mathematical calculation and prove a
theorem that it's in the robots interest
to allow it and if you take away that
uncertainty so as the variance of the
prior over the human objective goes down
to zero
the robots incentive to allow itself to
be switched off also disappears so this
is a fundamental safety construct that
the machine has to be uncertain about
human objectives in order to be safe and
you can show that machines that are
built this way with this kind of
uncertainty and the ability to learn
from observing human human behavior can
be provably beneficial so you can prove
that the human is better off with this
machine than without it okay so the the
most fundamental consequences they said
we have to go back to the definition of
AI and say no we got it wrong it's not
about building systems that maximize
their objectives it's about building
systems that maximize our objectives
even when they don't know what they are
so you can build template designs for
safe AI and I think we'll start to see
these as we as AI moves into the real
world in cars in personal digital
assistants that can empty your bank
account by accident for example there
are smart homes that can freeze you to
death
domestic robots that can accidentally
trample on your cat or cook it you know
then you start to need to have the
Machine understand that you know the
nutritional value of a cat is less than
the sentimental value of the camera
all right that's really important part
of human value system so even if we
solve that problem I even if we know how
to build provably beneficial AI systems
there's no guarantee that humans will
follow those guidelines right they may
still want to take over the world using
AI as a tool to fulfill their evil plan
so that's sort of the cybercrime issue
in spades if we don't solve the
cybercrime issue then we're not going to
solve this either so I really hope
people will get gets it more serious
than we are already about cybercrime so
in conclusion we're making other
progress there are all these potentially
great positive consequences some really
bad negative consequences and I think
many reasons to coordinate the
activities at many levels to try and
make sure this comes out the right way
thank you thank you very much
so if somebody's dying to ask a question
why don't you do so otherwise we've got
a break and we can hold some of the
questions for professor Russell for the
panel but if there's anybody dying to
ask a question okay let's take our break
come back in about 10-15 minutes and
we'll have our panel discussion</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>