<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>OpenAI - Teaching AI Through Self-Play - Ilya Sutskever | Coder Coacher - Coaching Coders</title><meta content="OpenAI - Teaching AI Through Self-Play - Ilya Sutskever - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>OpenAI - Teaching AI Through Self-Play - Ilya Sutskever</b></h2><h5 class="post__date">2018-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BJi6N4tDupk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right HDMI to the rescue
well it's it's good to be here and
thanks so much for being patient and for
actually staying for the very last
session of the day and in this
presentation I want to talk about self
play it's one of my favorite topics I
think there are deep connections to meta
learning and now self play has also been
a lot on our minds after the recent
alphago's your result is chess and so um
before we begin diving into the
technical content I just want to remind
you a little bit about what it is that
you're trying to do with the open AI we
want to build a GI you want to solve the
alignment problem and also one way to
articulate it is solve the ownership
problem you want to the benefits to be
very distributed and an arms race of a
GI to be avoided
so now let's begin with a review all
this uh you know this is some online
learning you have right here in real
time so is this let's see let's try this
so I want to mention old work in self
play and that is TD gammon it's 1992 so
this is prehistoric ancient history and
what's really amazing about this work is
that it's just so modern
it's basically could have been written
in 2017 just with much much slower
computers and you can see the plots they
show the performance of their system
when the head of their neural net has 10
hidden neurons and in u.s. 20 hidden
units all the way up to 80 and they've
used q-learning
to train the policy train be self play
and to be the world champion and so like
the foresight is just amazing but this
gives you kind of the first instance
that I'm aware of
Purcell played if something really
really cool so in later on like I think
we've all seen alphago zero which show
that you can do self play to defeat the
strongest humans in go you can do big
large-scale self clay be the right self
improvement algorithm and if you do
that's right then you also can beat all
humans and go and not just in backgammon
so it means that self play can work at
least in principle it makes sense just
play against the carpet a system which
is a clone of yourself roughly the same
level and just keeps getting better as
you're getting better another result of
self play was our very own result in
dota 2 where we defeated the world
champion in the 1v1 version of this game
and this was also a pure self play
result and by the way so the DOTA game
not not everyone here may know what it
is exactly but it is the biggest
competitive eSport game and it has a
very large serious professional scene
and over 100 million dollars in prizes
were rewarded to champions last year and
so 505 is the main challenge and we've
solved 1v1 but again self play really
does work so that's encouraging it's
encouraging force of play but why why
should we be excited about self play do
we just want to solve games games aren't
real life what can softly do for us
that's better than this
so I think that the promise of self play
at the core is that it gives you a way
of creating really complicated
challenging for your agents out of a
very simple environment so fundamentally
in a selfless settings you have a simple
environment many of simple agents which
are powered by powerful neural networks
powerful learning algorithms
the algorithm is learning the
environment becomes more complex and it
just feels like a very natural place in
which you might want to train your you
know ultimate meta learner because the
agents will create lots of difficult
challenges for each other one other
thing that is very nice about subway
which is not particularly deep but it's
still nice is that you have a perfect
curriculum so when your agents aren't
very good the near opponent on isn't
very good so you still win about half
the time but then as you get better your
opponent gets better so it's always
challenging if it's set up for just
right it may be possible to just allow
this self-improvement post-process to go
on indefinitely so one really cool place
where this was explored was in
artificial life sort official life was
something which is a little bit like a
deep learning in the sense that it was
tried in the past and then then it
didn't work so and gave up on it and
this is this is like I think the coolest
the coolest work on this kind which was
done by Carl since there is a really
cool YouTube video if you just google
Carl seems artificial life on YouTube
like all they would they've produced
they've evolved a whole bunch of recom
of morphologies of agents doing really
complicated things and it's just amazing
that were able to do that only in 94
what people tried it and it didn't work
so of course it doesn't work well now of
course the computers were too slow so
one thing that we were motivated at open
AI was to see if we can apply self play
to something else which is not just the
game oops
yeah so here like we have a simple
environment you have this disk and you
just want to stay inside the disk it's
sumo you just want to push the opponent
out that's all the information that the
agents have and just from this
information just look at what they
figured out look at
look at this and this you know we
injected a very small amount of
information into the system it's just
that when the opponent gets good you
need to get good and so you generate
this rich and complex strategy but then
you don't just want complexity for free
what you really want all that this is
cool you should what you should see this
so this is not exactly symmetric self
play but it is still the case but as
they as the red agent becomes better at
kicking the green agent needs to get
better at blocking
now of course all of this is small okay
this is cool to see here so the the so
that the green agent was able to mmm
Daiichi under the arm of the red agents
so the balance is pretty good and here
you see we do a very very simple
transfer learning where we say okay
let's take the sumo agent and just apply
random big forces on it and see if it
can maintain its balance and here it is
maintaining its balance even when you
apply forces on it and so you can see
you can see what the promise is here
that if you create a bigger and better
multi agent environment with more agents
with completing goals they will create
their aging Society and they'll develop
social skills theory of mind and so one
of the big challenges with this line of
work is to find the way so that the
agents that you train with self play are
useful for an external task and I think
this is the very important research
question it needs to be addressed so the
kind of thing we was thinking about with
those wrestling humanoids was that okay
so they're going to be too good at
wrestling and then perhaps it will be
very easy to find unit to maybe cook a
simulated egg it's good at wrestling
it's good at cooking a simulated egg as
well but I think this is an important
part of this research the self play you
can see that it can produce a lot of
different challenging tasks for agents
which is what you need for meta learning
as well meta learning thrives when there
is a great variety of tasks and this
setup can produce it but it is also
important to do the next the final step
of actually taking these agents outside
the simulation and getting them to use
scenes which are truly useful and right
now the next step in this line of work
is to do that to demonstrate that it is
possible to take the agent out of the
simulation and get it to do something
that is very difficult to achieve by any
other means I also want to highlight one
very
important characteristic of self play
systems so self plays systems have a
property but if you set them up exactly
right you will experience a very rapid
increase in the competence of these
systems and this graph right here shows
the true skill which is essentially the
a low rating of our daughter bought as a
function of months so in April it was
pretty own owner this thing at the
bottom is not gonna go away oh yeah Rick
so in April it was totally bad and then
you know we scaled it up a little bit to
fix some bugs it became a little better
in May and then by late May is pretty
good then in June it beat an amateur in
July B the semi-pro and in late July it
was kind of a pretty strong Pro but it
wasn't the best
so you scale up more fix more bugs and
you see like it's then it's just the
scale in our pages keeps getting better
and better and better and better and
it's clear why because in self plays
sisters you can convert compute into
data this is one of very attractive
things about it in normal supervised
learning you're fundamentally limited by
your data sets your data set creates an
insurmountable ceiling of how far you
can go but it'll self play if you want
more better results you just put more
better compute and your result get
better now I want to now present a
hypothesis the hypothesis is that at
least in principle the Ã¡gi systems
there's a good chance that they will be
trained why this kind of self play and I
want to give some speculative evidence
for why it might be so so one thing that
we know from biological evolution is
that social animals tend to have larger
brains there exists at least one paper
from science which supports this view
one thing we know about our own
evolution is that we've experienced err
an intelligence explosion on a
biological time scale you know where the
volume of our brains have tripled in
size over the past two million years and
there's a good you it's definitely in
each hypothesis to say that it happened
because we were living in these social
tribes and we got so good at surviving
then suddenly our social standing in the
tribe and what other agents think of
what other humans are the prehistoric
humans think of us is the most important
thing and so you can clearly see how
such open-ended multi-agent systems
could produce theory of mine negotiation
social skills empathy and maybe even
real language understanding but that
will require some effort and I want to
finish with a speculative slide so if
you accept the claim that multi-agent
systems once done right experienced a
very rapid increase in performance and
you accept the hypothesis that first AG
eyes will be built in such competitive
multi-agent systems yourself play then
you should also see a very rapid
increase in the competence of early AG
eyes and this concludes my presentation
yeah very nice representation and I
fully subscribe to your idea that's for
true open-ended AI having the
environments evolve as part of the
training is probably necessary now
there's a there's a rich and long
history in the field of coevolution
which can so stop making machine as the
most simple form of coevolution where
the environments varies during training
right and one of the big problems
Akaushi that was found was that it's
hard to get the evaluation diverged
enough and not to focus on very specific
tests or opponents is that something
with your yeah so I think I think
maintaining em so the question yeah so
the question of how do we avoid a
collapse of your behaviors into a very
narrow low entropy lower entropy DM
subspaces and I this is definitely a
practical problem that we saw even in
the dota system and it feels a lot like
neural net training where it's something
that happens but if you add a lot of
variety to the environment if you
randomize if you add various all kinds
of em domain dimensions of variability
to the environments we've observed that
it makes the problem diminish a lot and
so I think that speed it will be
important to keep adding this
variability having not just one opponent
but multiple opponents and basically
having multiple types of opponents
I think this these kind of approaches
will be essential to make this robust
and stable hi so I very nice talk thank
you very much
but I was curious I love seeing your
presentation of the dodo BOTS but it's
my best of my knowledge you haven't
actually published anything about it
is there any timeframe word and we could
see something on archive yes oh so right
now we are working on the five five
version and I hope that Steen and not
not to not to find a future it's all
gonna be out there okay that's good
but it would be nice to see like you
know this is a contribution that I think
the community would like to see
I'm the surprise that something like
this where it's been presented and been
blogged about hasn't had a submission
for basic methods I mean I think I think
like this definitely makes sense but the
way would like to do it is we want to
finish the e5 e 5 F first okay
I had this saying like preparing the
Alpha go zero and T V game and it was
kind of an outlier that these are like
large-scale milestones but one has
nothing publicly available all the miles
the milestone is still work it's still
working progress okay
so it's obvious that the singularity
weighs heavily on your mind but you
admit that 20 years ago we just didn't
have the computing power to do what we
able to do now so do you have an
estimate of how much computing power we
might need to to get to that singularity
Aram well I think it could definitely
make so I wouldn't want to use that term
I think it has lots of connotation but I
think you can definitely make arguments
as to how much compute is needed like
you can run brain scale models much
faster in real time you're probably ok
but can we do that well not today
obviously so well Moore's laws ending so
how much silicon square meters are we
gonna need for this I mean for sure it's
going to be a large computer but these
but it's like you know it's okay so
that's a great question so I think when
people were looking at the feasibility
of putting objects into space
one of the counter-arguments we used is
that you gonna need something like a 500
you know something like 500 tons worth
of fuel into the rocket there is no way
it could it could happen but what
happens the Rockets ended up being
really huge and I agree that it's going
to be a big operation it's not something
small but these things are possible what
happen is that we're not in space
well so it is true that we are not in
space but the reason for that is that
they they haven't finished the job they
got their rockets with D be destroyed
every time they were used if the rocket
somehow were reusable
[Laughter]
hi I'm Ernest young thank you thank you
for the talk first my question is can
you give more color about when you said
more compute equals more and I forgot
the right-hand side but give more color
about more compute is it either you want
more parameters in the model to make
like the number of weights bigger or is
it just optimizing the mathematical
operations so they run faster I mean at
the basic level all it means is that in
a self play system if you got more
compute like in a circle a system at its
core allows you to convert computing to
data so what it means is that if you
have a lot of compute your agents are
going to get a lot of experience and
you'll have a lot more to learn from
that's basically all it means so if you
have more computing more experience
you've more learning sorry we've got to
stop there but we will there'll be a
panel discussion and we'll reconvene in
one hours time for the final session
let's let's thank our speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>