<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Pieter Abbeel - Policy Search for Reinforcement Learning | Coder Coacher - Coaching Coders</title><meta content="Pieter Abbeel - Policy Search for Reinforcement Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/The-Artificial-Intelligence-Channel/">The Artificial Intelligence Channel</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Pieter Abbeel - Policy Search for Reinforcement Learning</b></h2><h5 class="post__date">2017-09-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dEHyTBJ1pSc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah I got a couple affiliations it's
just three I'm a research scientist at
open AI
where do AI research um professor at
Berkeley I do AI research and Pai and do
some administration but not while I'm
gonna leave right now and then I'm also
one of the cofounders of great scope if
you ever need to do any grading this can
save you a lot of time we got tired of
spending a lot of time on grading
homework in exams and build a tool for
it now we've got a lot of data of people
having graded
now we build AI to learn from that data
to grade automatically and you can get
close to zero time spent on grading in
the near future okay reinforcement
learning let's see all right so Tyrell
all right already introduced this
formalism earlier today in reinforcement
learning we have an agent and we want
somehow to discover the right software
for this agent and this software will
take actions as a consequence of the
action the environment and maybe the
robot or something else will change
State a reward will be emitted that
indicates how good or bad the resulting
situation is and then the agent is faced
with the consequences of the action and
is to act in this new situation and this
repeats and that's fundamentally what
makes it different from let's say
supervised learning where you have a
one-off thing you make a decision and
then things reset and get a new one off
thing and it repeats here what you see
next depends on what you did before the
topic within reinforcement in that we're
going to cover in this lecture is policy
optimization impulse optimization the
goal is to directly find a policy PI
theta that given a current state emits a
distribution over possible actions and
then samples in that distribution and
hopefully that this version is a good
distribution that leads to good
performance underneath and everything
I'll cover here we can pretty much
assume that it's going to be a deep
neural net representing the policy but
in principle it could be any other
function that's parametrized with some
vector theta and as you change the
entries in theta you'll change the
policy the process that goes from
current state to action being taken so
formally what it looks like then what
we're trying to solve is it trying to
solve an optimization problem where we
maximize expected sum of rewards
accumulated over time
expected because well the environment
might be stochastic so might not be
deterministic what we encounter and
second often the policy will be
stochastic it is the case that the
optimal policy in MVPs tends to be
deterministic but for learning it can be
convenient to consider a broader class
of policies that is also consider
stochastic policies because that
actually tends to smooth out the
optimization surface so we'll be
optimizing over stochastic policies and
try to find one that performs well on
expectation even though I'd converges
maybe it'll end up being deterministic
to make this a low specific for robotics
for example the reward could be the
quality of a meal that a robot prepares
so a robot's busy cooking in a kitchen
after half an hour robot comes out gives
you a meal there's been zero reward all
the way throughout but then when the
meal comes out you eat it you say how
good it is maybe read it from one to
five stars and the robot will get a
reward from one through five as you can
see how this can be a really hard
problem because the robots been busy for
half an hour and only gets one piece of
feedback at the end of the whole session
about how good the cooking was and so to
actually learn from that it's really
difficult it might have to do this again
maybe it gets a different rating you can
tease apart what was different between
the different sessions and from that
somehow understand what is she do more
of what it should do less off and that's
exactly what policy optimization will
try to do now possible magicians not the
only way to tackle the problem so the
methods to the other methods are
important too we're focused on policy
optimization here so let me give you a
low pitch of why policy organization
might be the method of choice in your
practical situations often the policy is
simply to represent them the cue
function or the value function imagine a
robot wanted to grasp an object you try
to find the pause when you're trying to
find is a path for the gripper to go
around the object and close if you're
trying to find a value function you
trying to learn to predict exactly how
much expected reward you have as a
function of the state that you're in and
that's a much harder problem to
precisely solve and so it might be that
just learning the policy is simpler now
learning the value function if you learn
the value function you actually don't
know how to take actions once you have a
value function to then decide how to act
you need to have a dynamics model that
allows you to look ahead do one step
look ahead say okay I have my value
function for all the actions I have
currently available to me what would
happen if I take that action then what
reward do I get for the transition and
what's the value at the next state and
so you need to actually learn two things
to be able to act you can resolve this
by learning a q function step that's
even more complicated learning a value
function in many ways because now you
need to learn a function that
understands how much reward you're going
to get as a function of current state
and action which can be a pretty
complicated function to learn even for a
simple task where often the policy can
be much easier to represent here are
some success stories of policy
optimization that might help motivate
why you want to do it the top row are
successes from a little while ago and
they don't use any deep learning
underneath they are simpler policies
underneath but still parameterize
policies where the policy optimization
reinforcement algorithm allowed to
fine-tune parameters in a way that
humans couldn't fine tune them by hand
and as a consequence could get better
performance than hand tuning of these
systems so you see at the top is from UT
Austin the robot dogs that play robot
Cup soccer used reinforcement learning
to learn to walk or run faster than the
other robots that they're playing
against and hence be able to out-compete
them in soccer then next one we'll see a
little more detail later is inverted
helicopter flight by entering scoop at
Stanford I was part of that team then
next one is two legged locomotion
prosthetics PhD thesis at MIT this is a
biped Walker learn to walk from scratch
with reinforcement learning in the real
world and then the last one is one that
will also see a little bit about later
it's a task where you have a cup and a
little string and then a bowl attached
to the end of the string is supposed to
swing up the ball and catch it in the
cup the bottom are some more recent
success stories where there are deep
neural nets underneath what's being
learned the policy isn't network that
will map for most of them not all of
them know from raw pixels to round motor
commands
on the left you see some Atari games as
well as labyrinth which is a 3d
navigation environment that you navigate
with these solutions from first-person
vision then control of two-dimensional
robots in majokoi control
three-dimensional robots image oka
control of actual robots and then
alphago has a few stars here because
there's a lot of pieces to alphago and
only one piece of this policy
optimization but it is also in there as
part of how you restrict the branching
factor so your policy can help you
reduce the number of things to consider
as you search in the game tree through
plausible futures that you might
encounter in terms of the overall
landscape of reinforcement learning
policy optimization on the left here
dynamic program on the right enemy
program you'll see a lot more of later
today sure rich Sutton in the next
session will cover a lot of dynamic
programming dynamic programming relies
on a self-consistency equation that the
value at the current time is equal to
reward you get in the first transition
post value at the next time policy
optimization directly optimizes the
objective that you care about which is
expected expected reward and of course
then with actor critic Mattox you kind
of get in the middle between the two and
this session will cover these three so
we'll cover derivative free optimization
will cover policy gradients and then
we'll see how we can use value functions
inside policy gradients so that we get
actor critic methods which gets us
pretty close to the valley function
methods it's not that data ones aren't
important it's just only so much we can
cover in one lecture in terms of outline
for this lecture we'll look at
model-based methods first it's often now
what people think of when they say
policy optimization Metz's methods and
policy great methods but we'll see them
first and admittedly people have had a
hard time getting these to work as well
as the other ones but there's also
strong belief that model-based methods
in the long run could be more sample
efficient and in a session like this is
good to know what's out there rather
than just the things that have worked
the best of all things and then we'll
look at model free methods and at the
end we'll do something that kind of
them both together through the
stochastic competition graphs framework
okay starting with model-based what you
see here is also split into three types
of derivatives that we'll study will
study path derivatives will study
derivitive free methods and then we'll
study likely ratio gradients three
different ways of computing the
derivative if you have a derivative you
know which direction to step to improve
your policy different sets of
assumptions we'll start out with some
pretty strong assumptions here we'll
assume that F is f is a dynamics model
that describes next state given current
state of action for now we'll assume
it's known differentiable will assume
the reward function is known
differentiable and we'll assume that the
policy is known but that's not really an
assumption because we're designing the
policy ourselves we always know the
policy and then also assume that it's a
differentiable policy okay
so let's assume we have an MVP with only
two transitions that happen so very
short arisin just for making it concrete
and fitting it on the slide top probe
what do we have state at time zero
dynamics model and action we take as
described by the policy potentially
stochastic policy then results in next
state then policy size what to do next
state at time 2 and then we might have
another transition and this will
continue but we could also stop at two
time steps and then we have rewards at
the bottom that decide how much we got
how good this current trajectory was
well we try to optimize it some of our
rewards the first one here doesn't
matter too much because we can't really
influence it because we start in
whatever state we start in but I'm just
showing it for symmetry of the of the
figure so the notation reward function
are maps from state to reward policy PI
theta maps from state to action for now
let's assume deterministic and then the
dynamics model maps from state and
action to the next state then what we'd
like to do is optimize expected sum of
rewards so we'll call u of theta the
utility of a current policy PI theta and
so we're trying to maximize
uf theta so in this case what that means
is maximizing as some of our 0 r1 r2 now
we can actually compute a great
investment along this graph think of
this me a lot of you have seen a whole
week of deep neural Nets last week think
of the thing on the left that's just a
neural net that is a kind of special
structure it's a neural net with 3
layers with some connections within the
layer but it's very much like a neural
net and so in principle you could run
back propagation through this to find
the derivative of utility which is the
sum of the bottom things which would be
some of the bottom things would be a
loss function make it as high as
possible in this case you can just take
derivatives through this so you'd say
well I care about derivative of utility
respect to all the parameter parameters
theta which are the weights on the
policy connection then what is this it
has derivatives of reward to respect
each of the rewards respect to state
then third of our state respect to the
policy parameters we can expand the last
one this way the state depends on the
policy parameters through how you ended
up in that state which is dependent on
what state you were in at the previous
time which it also depends on the policy
parameters and depends on the dynamics
model directly oh this should be a you
hear that emic small directly depending
on the controls you action you took and
then how the control action depends on
the parameter vector theta or in this
case just entry theta I and then the
derivative of the controller action
respect to the parameter vector theta
depends on the policy but then also
depends on what state you were in and
just so there's another recursion here
at respect to state in a state respect
to the policy parameters that's just
back propagation spelled out in a
recursive way and so you could either do
that by hand or you could just feed this
computation graph that you see at the
top left feed
- an automatic differentiation package
we have feed in F you feed in our you
feed in a parameterised policy PI theta
which is the variables that you're
trying to optimize over and then just
ask it for a derivative and you're good
to go so to get derivatives policy
gradients we just do a robot which is
executing the policy then do back prop
which gives us a gradient and maybe we
do multiple rollouts of multiple initial
states to get a lower variance on our
gradients and we can do updates so that
gives us the simplest terms of a policy
gradient algorithm for deterministic
dynamics that monistic policy let's take
it a step further what if we have a
stochastic dynamics model so next state
is function of courtesy and action plus
some noise actually you can simply
consider these noise variables constant
once a robot has happened
so once the robot has happened you just
freeze the noise and you're back to the
to monistic model because the noise is
frozen is nothing stochastic about it
anymore you can still apply back
propagation just like you could do
before of course you got to realize that
you might have to do multiple rollouts
to see multiple instantiation of these
noise variables and it compute multiple
gradients and average those gradients to
get a lower variance estimate but
mathematically speaking you just do one
roll out and get estimate of the
gradient in exactly the same way so more
generally if you had a stochastic
dynamics model next state depends to
castle on current state in action you
can reap Rama tries this by saying it's
gonna be a deterministic function that
depends on current state and action as
well as some noise variable WT and the
function encodes the stochastic s'ti by
depending on that noise variable for
example in the case of a Gaussian
distribution maybe the next day is a
Gaussian distribution as a function of G
of s T and UT you can just split it out
as we did on the previous slide but it
doesn't have to be a Gaussian anything
where you have a continuous noise
variable you can do this and then what
you have is a computation graph that
looks like this you have you roll out as
you roll out happens your noise happens
you can determine what that noise is put
it in there
and then do a back propagation to the
graph in exactly the same way the same
is true when your policy becomes
stochastic you can when you execute your
policy see what the noise variables are
that you sampled insert them into the
graph freeze them for your back
propagation and same for the reward
function even though often the robot
function is deterministic if in some
sense some situation so you have a
stochastic reward function you can do
the same thing so at this point we're
able to deal with stochastic reward
dynamics and policy and just use a back
propagation path to get the gradient out
of course it still assumes that we know
the dynamics model and we know the
reward function we just have to fill in
the noise to be able to do this let's
look at a full algorithm so we know how
to compute gradients okay now get a
complete policy green algorithm we can
iterate going from 1 to over our
iterations then we do multiple rollouts
in each iteration doing a robot means
sampling an initial state and sampling
all the noise that will experience noise
in the dynamics noise in the policy
execution and noise in the reward
function anything is deterministic you
just can skip the noise sampling for
that based on that you can do your
fourth pass then you can do a backward
pass average all the great investments
and take a step in a great interaction
will do something fancier that's a
higher order to optimize this if you
have a real-world system you might not
have access to the noise yourself the
real world will sample the noise for you
at least for the dynamics and possibly
for the reward function then you just
back solve for it as you have had an
experience you say well SD plus 1 should
be equal to F of s T comma UT plus some
noise I've seen st plus 1s d nu T I came
back solve for the noise and that just
the environment provides it and you
solve for it yes
one reason to average it is to get a
lower variance estimate an alternative
would be to take one grade estimate take
a step and repeat either way can work
that's kind of up to you how you want to
implement this
if you don't average it you might have
to take smaller steps it might still be
advantages it could be that your
averages because it's easier to simulate
many things in parallel and the same
rate as you could simulate one thing and
then you'd rather average it rather than
doing one at a time but both can work
yes good question so is this for
continuous States and actions for the
differentiability assumption to be true
we need continuous States and actions
otherwise you won't be able to
differentiate through these functions
and will later see what to do when we
don't have that assumption but for now
that's where what we're assuming correct
yes
I don't think anybody has a real answer
for that yet in that it's it's still a
little bit of a challenge to get
model-based or elf to work well when you
typically well so typically this is
coming next on the slide here typically
you would not have a known dynamics
model F and you would have to estimated
from the data that you're collecting so
you'd have an unknown that AMEX model F
which which you're learning as well as
learning the policy at the same time and
there's a lot being learned at the same
time and people still have a hard time
getting this all to work reliably
and when you're trying to get something
to work it's often easier to work with
larger batches because you know if you
take an infinitely large batch you you
know what the behavior is going to be
whereas if you take very small batches
or only one rollout it's harder to
understand the behavior in hard to have
invariants that you know are guaranteed
to be true but I do think in the long
run people will figure out ways that you
go just one rollout at a time update and
go again in fact I suspect people will
do things and people have tried some
things like that were you actually
during the rollout you would update your
dynamics model F as you're rolling out
we aree optimize your policy during the
rollout based on the latest update of F
and adjust your policy during a single
rollout even to be even more sample
efficient
other questions so the full model-based
our algorithm then would be you do
what's shown on the slide as well as in
every iteration you estimate the
dynamics model F from the data as I said
it's been hard to get these things to
work there have been a few successes for
example the SVG algorithms from Nicolas
Hayes and collaborators at deep mind
have shown quite some success with this
model based RL approach so let's expand
this to a full horizon here so the full
accommodation graph I dropped the noise
parameters the noise in the dynamics
noise in the policy noise and reward
function just to keep a little more
self-contained us we'll play around with
this quite a bit but this is the
commutation graph that we'll be
considering
will and what happens is that we'll
compute a gradient starting from every
time step T so we won't just do one
gradient path starting from time zero
but we'll say from every time step what
is the gradient that we can compute the
relative to the policy parameters at
that time based on the entire future
okay so we'll have many copies of this
one for each time step T SVG infinity
does what I just described you lay out
the combination graph from each time
step and then do the back prop based on
what I just said back solving for the
noise back rap through it and get a
gradient and do it for each time then
add it together from all times and
that's your create an estimate now you
can have other variants of this the
issue with is SPG infinity is that it
can be high variance
if the horizon is long there can be a
lot of stuff as sisty in the process and
the estimate that you get could be high
variance estimate and there are ways to
reduce the variance one way to do this
is introducing discounting so you could
have discounted rewards for the future
rather than actual rewards that's a
pretty natural thing to do you can do
more you can just shorten the horizon
you can say I'm only going to look K
steps ahead and then to account for what
happens after K steps ahead you could
use the value function we haven't
covered in this lecture how are your
going to estimate that value function
but let's assume you can estimate a
value function you can have a value
function that
at the very last node so over here
instead of R we have V v55 because we
are parameterizing the value function
and we're gonna have to estimate these
parameters Phi and then once you put
your value function here you can do the
exact same thing as if it was a reward
we're still optimizing some of all of
these maybe this cannot sum of all of
these
that's our commutation graph back
propagation and we get a policy gradient
out and we do it again for all time
steps T and get more extreme scenario
SVG one we stop after one time step so
you look one time step ahead cap it off
with a value function there back row
through this small combination graph but
do it for all time slices T add it all
together
hit your policy gradient even more
extreme you could not even have the
dynamics model in this anymore you could
just say I have a current state current
action results in a Q value that's your
commutation graph back row get to that
which is very short back propagation and
you get a gradient of your policy this
way now you estimate a Q function q5
that's SVG 0 or DDP G depending on the
exact details of how you collect it and
represent some things in the last one
you actually don't need to learn a
dynamics model anymore
so some advantage there that you don't
need to learn a dynamics model of course
by learning it maybe you internalize
something about the environment that's
useful you can learn more efficiently
but if you have trouble learning a
dynamics model don't want to do it SVG
zero avoids you having to learn a
dynamics model let's make this concrete
so you can do this for all the verses
that I showed on the previous slide C
Mon okay good question what does it
stand for
d DP G stands for deep deterministic
Policy gradient an SVG I'm actually
spacing out on what it stands for
stochastic value gradient okay thank you
so SVG is stochastic value gradient
so let's make this into a full algorithm
I picked SVG one as a SVG zero as a SVG
zero
yeah picked SVG zero as a concrete
example but you can do the same thing
for the other ones actually I skipped
one then let's go back to SVG one I
picked SVG one is the first concrete
example if you do want this is a
competition graph this is what's
associated with that and then on the
right we see the full algorithm we do
rollouts for each row out what happens
is a forward pass we collect the data
from that forward pass we store any
noise encountered during the forward
pass either by back solving if it's in
the real world for what the noise was or
by just knowing what the noise is cuz
we're simulating inserting the noise
ourselves then we have three things to
do and it you can choose when you do
each one of them there's a bit of choice
there but you have to learn three things
you have to learn a policy a value
function and a dynamics model and so
maybe your first to a Polish a bleep you
don't have to a natural thing could be
to first to a dynamics model update only
then solve for the noise then do a
policy update then do a value update the
policy update is requires the noise in
the dynamics and then the back
progression pass which in this case is
the equation shown over here that gives
you a great investment for the policy
your value function very investment
could be something as simple as you have
a premises valid function which with
least squares you fit to a estimated
value which is estimated based on one
step look ahead reward you experience
plus gamma x value at the next state
based on the value function you
currently have typically this is
considered a constant once you get it
out of there and only this Phi is
differentiated through when taking the
gradient here and this is considered
constant computed based on what is the
previous iterations Phi and then we need
a gradient for the dynamics model and
can update the dynamics model so it's a
full organ that you can implement and
run pause grades with yes
can you go back oh yeah is it actually
necessary to solve for W cheaper because
then the that you're just going to use
so yes good point so let me clarify this
so what happens here when I say F of s T
UT WT f is a function that we have
available because we're learning it this
function f will depend both on s U and W
you want to make sure to evaluate your f
at the noise that you experienced in the
real world or in your robot you don't
want to assume zero noise here you could
but it would not be as precise a
gradient as the average over multiple
rollouts then if you use the actual
noise that you experienced so this thing
here is actually differentiate it
through oh yeah so notation wise what
yeah okay
so for this dynamics model here the way
to severe witness you have F SD u t + w
t there would just be a + WT living here
if you had a more complicated than Amex
mall where the noise comes in a more
complicated way they would have to solve
in a different way for W T and then
insert it in here correct yeah so think
of this as just a plus WT and you do
want to keep the + WT you don't want to
lose it oh the reason it will not
evaluate as T plus 1 is because we're
actually substituting this thing over
here the policy pi pi theta will go into
here for UT and suresh is
differentiating through this whole thing
same value but not same that not the
same gradient necessarily it depends on
how WT plays into it if it plays into it
the simplest way I believe it actually
will have the same gradient even more
complicated dependence on WT it's not
guaranteed that the gradient is the same
thank you thanks for pointing that out
SVG zero we don't need to learn a
dynamic smaller because the competition
graph doesn't have the dynamics model in
it we just have a cue function there so
we have a back prop to compute the
gradient of the policy and then a cue
function update which could be just like
with the value function we had before
just some kind of squared error
minimization where this estimate here is
usually frozen to be whatever the
previous parameter vector predicted for
it to be fixed here derivative here only
with respect to the first fire
appearance in the first term ok so we
have a few pause Green are games that
we've seen or whole family of them this
actually been used to solve some pretty
interesting tasks two-dimensional
robotic control image oka the different
versions seem to learn about equally
fast if you want to turn this into the
domestic policy gradients and for it to
work well you need to do something a
little extra
if you naively train SVG 0 you end up
very quickly with a deterministic policy
that optimizes the Q values because here
when you look at the policy update
equation has defined the action that
maximizes Q and that'll national make it
deterministic so you need to do
something to keep it stochastic so add
noise to the policy explicitly force
your policy be stochastic when you
collect data then your Q values learned
are learned off policy but that's fine
with Q values you can learn them off
policy if you do TD 0 so with TD 0 you
can learn your Q values off policy and
have a stochastic policy to collect the
data
then you to get this to work stabili
with a deep neural net you need to play
a few extra tricks which were proposing
a DD PG paper by Tim lilyc rap and
collaborators for learning the cue
function you just don't don't just take
the last rollouts you actually keep
around the replay buffer against the
replay buffer do updates on your cue
function then when you do this when you
compute your target Q values as our word
plus next Q value under the current
policy these Prime's here 5 Prime and
theta Prime what they mean is that
they're not the current setting of your
policy parameters and Q function
parameters instead they're the polyarchy
averaged which is exponentially average
past weights that you've encountered
during learning so far and so these
average parameters evolve much more
slowly than the parameters of your
current policy and current Q function
and will stabilize your target values
which allow your organ to be more stable
and hopefully converge and this actually
has them been applied to some more
complicated robotic simulation tasks and
including vision based control on
driving question so I'm not an expert on
the meaning of what is associated with
it but a crewed meaning that I've heard
people associate with it is it's like a
keeping track of a posterior over
possible values that your neuron that
should take on and this keeping track of
the sum mode of a posterior based on
everything you've seen in the past in
practice what it means is that you can
take larger step sizes and more some
more aggressive step sizes and at the
same time still have a stable
representation they keep around that is
the result of your learning because
effectively it kind of slowed the
average version is slower moving then
the thing that's actually moving ahead
but that way you can explore more you
can have more aggressive exploration
more aggressive optimization in the
landscape of that you're working in
while keeping around something that's
quite stable yes I believe just for the
q-value updates but I'm not on them
censure okay that's our first type of
gradients which you can use inside
model-based reinforcement learning or if
you do the su g0d d PG version even
within model free settings but assuming
a differentiable policy and/or dynamics
model if you have discrete actions what
I just described to you is actually not
going to work out if you don't have the
ability to learn a dynamics model if
you've discrete state space is also not
going to work out as described so let's
look at some other methods that are
applicable there simplest way to compute
Grange is just to do finite differences
right if you have an optimal
optimization objective utility which
depends on your policy parameter vectors
theta just preserve the policy parameter
vector theta one coordinate at a time
positive perturbation negative
perturbation see what the difference is
divided by two epsilon and there you
have your derivative estimate some
trickiness there if your function is
stochastic so at any point along the x
axis which is here optimizing a function
of X anywhere along the way when you ask
for a y-value there's attribution over Y
values that need to be careful because
you might end up with samples that are
somehow inconsistent in some way your
sample here you get this point your
sample here you get this point and now
the derivative looks the exact opposite
way than what you want so you need to be
a little careful about this if you have
control over the noise that's present
well if you don't have control you can
just average over many samples if you do
have control you can make sure that the
noise present in the sampling is the
same for the plus and a negative epsilon
perturbation and that way you can get
out a reliable derivative estimate so
fixing the random seed can help to get
those derivatives it can actually also
help in general whenever you're doing
policy optimization this way when the
dynamics small
is being executed if you control the
noses consistent that's the same as
intuitively something like oh my
helicopter is trying to fly in pretty
windy conditions but when I compare
performance of two policies I should
compare those policies under the same
windy conditions not each time randomly
sampled windy conditions and see which
one does better this one might have just
been luckier with the wind conditions
than the other one with this kind of
approach just finite perturbations find
a difference derivatives actually the
helicopter project that Stanford was
able to learn to control this helicopter
here so you see here is a helicopter
being stable to control such a very hard
control problem and in fact being stable
controlled to fly inverted now what
happened here is that to make this work
a policy class is handcrafted with
careful consideration of how the four
controls that you have for a helicopter
depend on the state resulting in only
twelve parameters I need to be optimized
over and we have only a very small
number of parameters actually these
simple methods can be made to work and
very successfully so to do this kind of
derivative free method if you go on to
optimize over policy with a hundred
thousand parameters might be a lot
harder it might take a lot longer but if
you do have a lot of prior knowledge
then these simple methods can actually
work quite well now there are other
great entry methods beyond just find out
difference perturbation something called
cross entry method the covariance matrix
adaptation and variance they're off so
let's take a look at those cross entry
method you're still optimizing the same
objective will view you the utility has
a black box so in everything we've seen
in the first half of the lecture we've
taking derivatives through the dynamics
and so forth understood that there was a
temporal process happening here we're
gonna ignore that there's a temporal
process just say we have a policy see
what happens just like in the finite
differences so that means we're probably
not going to be optimal in terms of
sample efficiency but we do have
something very simple to work with and
so cross entry is an example of a blue
method and what characterizes those
methods is that you keep around the
population of whatever you're optimizing
over in this case of population of
policies so we don't have just one
parameter vector theta with a
distribution of a parameter vectors
theta for example a Gaussian
distribution with a mean mu and maybe a
fixed covariance or maybe a learned
covariance this case let's assume a
fixed covariance so we don't have to
explicitly talk about it and we have a
mean mu I which says that for our
current population our current
population of policies mu I is the mean
and if I want the policy in that
population and sample from a Gaussian
with that mean and that gives me a
parameter vector theta cm then it rates
samples population members which is
samples policies from that Gaussian
distribution over policies once it's
sampled a policy it executes Roatan and
a policy it stores what parameter vector
it sampled as well as the utility it
achieved under that policy once it's
collected those it now checks some
subset of the policies that it's sampled
let's say the top 10% and then computes
a new mean for the Gaussian which
maximizes the log probability of the
policies sampled that we're in the top
10% so you're shifting your Gaussian
mean to where the top 10% was and then
repeat again and again and again very
simple procedure can actually work
embarrassingly well so for example for
the game of tetris which for a long time
was a benchmark in reinforcement I was
used quite a bit the numbers were such
tad cross entropy was actually
outperforming other reinforcement
methods by quite a bit for a long time
and it took till nips 2013 paper for a
dynamic pyramid method to finally
perform as well as cross entry method in
the game of tetris even though a dynamic
program methods use a lot more
information about the process than this
cost cross entropy method there's a
whole family of these some collecting
these on the slide the intent is not for
understanding each one of these just
right now when we show them to you but
later when you look back at the slide
you can see the relation close
relationship roid weighted regression
says instead of taking the top let's say
10% for each policy that was sampled we
take some function of the utility
achieved and the probability of sampling
that particular policy put that in front
of the log probability I maximize this
cross entry would be a special case
where you make that function 1 when
you're in the top 10% 0 when you're not
in the top 10% but there are other
functions you can take prescribed by
this paper over here path integral
method replaces the top 10% 1-0 scoring
by exponential weighting of the utility
achieved so it's a soft version of cross
entropy where the best ones get weighted
more and then if you're not as good you
get weighted less in the finding of the
new mean you can also find the
covariance not just the mean of that
Gaussian then you get a CMA method so
and then you can also do place in Texas
how you weight each one of them as a
function of their utility and then in
the power method there is actually some
additional e/m like ideas and important
sampling ideas that extract a little bit
of second order information in the
update which might make it more
efficient so all of these effectively
look at maximizing log probability of
parameter vectors of policy that you
sample before based on how well these
parameter vectors ended up performing in
their robots some examples success
stories often used in graphics for
animation of humanoid fly characters we
see here on the right is learn to
control for the ball in a cup game it
started with a demonstration a policy
was learned to match the demonstration
that was not a good policy we're seeing
that policy being executed initially
then the power method which is a cross
entropy method is being run to optimize
the policy and in just about a hundred
iterations it's able to learn to swing
up the ball catches in the cup this
particular case it's working based on a
motion capture system that tracks the
location of the ball so it's working in
state space in those location of the
ball knows the state of the robot and
then based on the stadium from
provides feedback control to swing up
and catch the ball the main caveat the
main advantage of this man is a super
simple always nice thing to do to do as
your first test when you try something
in reinforced learning just run across
and repeat like method or just cross
them to me itself see how it works
gives you a measure of how feasible it
might be to solve the problem not super
sample efficient that's the main caveat
especially of a high dimensional space
that you're optimizing over it could be
sample inefficient but the flipside of
all of that is that maybe these days you
don't care as much about sample
efficiency if you're working in
simulation if you can paralyze over
enough machines this method is extremely
easy to paralyze and scales extremely
well with parallel with parallelization
where you see here is a graph show as a
function of the number of course the
median time to solve in this case I
believe it's the 3d humanoid problem
what you see here is a linear curve
which means that as you introduce more
course it literally speeds up the method
by a factor of how many more cores are
introduced into the double number of
course you go twice as fast finding a
solution and so that's really nice so
actually you want to find a solution as
quickly as possible even though these
methods are not the most sample
efficient they might actually be the
fastest if you have a large compute
budget yes okay so in these methods it's
actually extremely simple what happens
you you have your current distribution
over policies you sample from that you
execute each one of them might be one
episode for each one of them might be a
few episodes if you're in a very
stochastic environment to get a better
measure of how good they are and then
you just look you don't do any
optimization on those executions you
just execute them as this so there's no
the first execution is not any different
from from the last one at that point
like when you when you played the top
10% like yes okay this typical simple
implementation just looks at the ones
from the last iteration now if in the
last iteration you ended up being
somehow confused because something weird
happened and you found the spot where
things look good and you're concentrated
all there and things all of a sudden are
pretty bad then you want to exactly
think about what you're thinking about
this well should I really research from
scratch or should I actually go back to
previous iterations see where things
were pretty good and restart from there
I think there's a lot of interesting
questions about how to what you might
want to keep around from past iterations
I haven't seen anything people might
have done it
where to keep around more you do a lot
of robots here so you you're probably
not gonna want to keep around everything
but it might be that there's a scheme
where you keep around most salient past
executions to them more effectively
update in the future yes
yeah so yeah you're right so the
assumption what I described is that the
posterior keep around is a Gaussian over
parameter vectors it's a very simple
assumption it's not necessarily the
right fit for most situations it's quite
effective and simple to to go with it
would be interesting to try other
distributions that might better fit what
your posterior looks like and resample
from those simple thing could be let's
say a mixture of gaussians which is also
very simple to compute them to sample
from and investigate how that does maybe
even more complicated distributions the
results have shown to you are all just
keeping a simple Gaussian distribution
around oh yeah if you if you use the one
of the latest methods in inference for
keep representing distributions maybe
use a personal Auto encoded to represent
your distribution or maybe use one of
the other aggressive models to represent
the distribution there's a trade off
because it would use some compute
involved in keeping that distribution
around and if you're doing everything in
simulation then ultimately compute is
what you're constrained by if you're
doing things in the real world then
maybe more complicated distributions
even if they require a lot of compute
are easier to justify because well you
want to reduce real-world rollouts and
you can do it by doing more compute yeah
be very interesting to play around with
I don't know how well it would work but
there is probably a trade-off point
there we're being a little smarter about
the distribution while not expending too
much compute somewhere well somehow
we'll achieve the best performance yes
so the entire this two is the
environment can change one way it
changes that you get dropped in a new
environment but there's a distribution
over environments in which case you can
think of that as randomness and which
ends so much you might not have to do
multiple robots per policy to get a good
estimate of how good that policy
parameter vector is and so that would be
a pretty good fit then another way the
environment could change is that your
environment changes during the robot so
there is change over time in that case
if you had a simple feed-forward neural
net policy let's say that your
parameterizing then it would not be a
good fit for your environment and so if
you want to apply this approach you and
you want to do it the way I just
described you would have to pick a
recurrent neural net the recurrent
neural net would then somehow store
information about how you because the
environment would change it would
somehow track how the environment is
changing and so then what you would
learn is you would learn the weights of
the recurrent neural net but in its
activations it would be able to keep
track of changes to the environment and
so then you would train a policy that is
good at adapting to a changing
environment which is a not with a lot
not a lot of people are testing on this
but it is in the real world it is what
it would be like you need something
that's good at adapting to a changing
environment not just a good policy for a
stationary environment so that's a good
question so one reason it's fast is also
because you don't have to do a back
propagation pass okay so that's nice
because that means you don't have to
store any information here fourth pass
and so if you had a recurrent neural net
policy you would have to store a lot of
information your forward passes and do a
backward pass and so for current illness
it would also be extra fast in some
sense that you don't have to do a
backward pass so there might still be
benefits in that regard the
parallelization one thing I forgot to
mention the reason the parallelization
works out so well is that all you need
in
the communication between nodes is
communicating the random seed of the
perturbation so even if you have a let's
say million dimensional parameter vector
if I'm collecting the data from all the
other workers all I need to know is what
random seed did they use and what was
the average performance under the policy
they have because if I know the average
performance which is a scalar I know the
random seed which is a scalar now I can
locally reconstruct what their
perturbation was and I can do the cross
entropy update locally so communication
is essentially well not zero but it's
extremely close to here and that's why
you see the linear scaling and that
would still be true if you had a
recurrent neural net Policy you still
would only need to know the random seed
that was used for the perturbation at
each of the worker nodes to understand
what update to make locally I think the
the real question in some sense what
you're asking about is what does it mean
to be acting in an environment that
continuously changes and I think it's a
question a lot of people are very
interested in but that we don't have
really good benchmarks for makes it
harder to to work on so you kind of
somehow you need a setting you need
settings where things are continually
changing on you so you can test those
ideas and I think right now we don't
have a good set of environments in which
to test them even though we know that
for the real world clearly it is what we
need
now we're going to look at for the last
1/3 ad what are the classical policy
grading methods so we're going to see
now is if somebody tells you policy
gradients and it don't say anything more
they'll likely refer to what we're going
to cover now even though we've already
seen two types of policy gradient
methods namely the BAC variation ones
namely path derivatives and we've seen
the kind of cross entropy evolutionary
style updates compared to the
evolutionary strategies will have the
same set of assumption that there's no
assumptions on the dynamics model no
assumptions on the reward function we
just need to be able to execute things
and know how much reward was collected
we can work with
discrete state action spaces but what
we're gonna want to do now is look at
the details of what happens during the
rollout to make the method more sample
efficient than a evolutionary method so
we'll change notation a little bit
temporarily to reduce clutter and we'll
introduce something called tau tau
stands for an entire trajectory okay so
utility under policy pi theta is
expected some of your words and we'll
write it as sum over all possible
trajectories tau probability of that
trajectory tau under the current policy
parameter vector theta times the reward
accumulated along that trajectory tau
same problem as we have before just a
change in notation to make it more
compact and so our goal is still to find
a policy parameter vector theta that
maximizes expected some of her words now
written this way yes yeah so trajectory
roll odds are the same thing so tau
stands for as 0 u 0 s 1 u 1 s 2 u 2 the
entire sequence of states and actions
so we'd like to do is compute a gradient
of this utility u theta so nebula theta
some of our old trajectories of the
quantity we just looked at and to derive
this will play some some tricks that
only in hindsight makes sense it's a
very short derivation I think it's worth
being aware of this trick and
understanding how this is derived so we
bring in the gradient into the summation
that can pretty much always be done when
you have a summation
it can also be eight integral can always
almost always be done to then now we
want to somehow compute that gradient we
know that the distribution depends on
our parent vectors theta but we're
actually going to play a little trick
first when I multiply and divide by
probability of trajectories under theta
then we move the denominator off to the
right then we use the identity that the
derivative of the log of X is the same
as DX over X so we replace it with
gradient of log probability of
trajectory on there theta so this thing
here is just applying derivative log in
the opposite direction and it's usually
used at this point where she are in a
very interesting situation there is this
interesting is because we have an
expectation again we use here is an
expected value of under the distribution
over trajectories of grad log
probability of trajectory times reward
we have an expectation we can
approximate it from samples and so we
can say well we're going to compute this
grandeur approximated by this quantity
over here which is same thing as what we
have over here but approximated from
samples that means that we can execute
our policy and for the royal ossetra
factors that we obtain we can average
that quantity for each row out to get a
great in estimate now we still have to
compute that coin I haven't told you how
to compute the grad log private
rectory but if we know how to compute
that then we can do this based on sample
estimate you can derive this in a
different way too so here it's very much
a hindsight type derivation where you
want it to end up with an expectation so
you multiply with the probability
divided by it and then work the other
thing in the back so you end up with an
expectation so you can compute this from
samples another way to derive it is to
look at importance sampling what is
important sampling it's a tool to sample
from a distribution well to generate
sample sum a distribution that you can't
sample from directly so you want to
sample from a distribution let's say you
want to sample from a distribution under
parameter vector theta but oh you can
sample from a distribution under
parameter vector theta old
now you want to still know what the
expectation is under the new parameter
vector theta what you can do is you can
sample from theta old and then re weight
your samples by the ratio of this
probability under theta divided by
probability under theta old if ure
weight your samples by this ratio you
get a good estimate of the expected
value under the distribution under theta
even though you sampled under theta old
so this tells you is that in principle
you could sample under your current
policy data old and evaluate how good
another policy is with new parameter
vector theta and then maybe optimize
this quantity to find a better parameter
vector theta can actually do this but
often it's not that great an estimate if
you move far away so actually the first
order approximation of this might be the
most meaningful thing to use so see what
the first order approximation looks like
if you first are approximate this
imported sampling estimate work through
it this expectation over here take the
gradient to see what it is then we
evaluated at the current theta which is
theta old will get the same thing we had
before and so we get the same quantity
as we had on the previous slide so we
see here is that from important sampling
seeing what the important sample
estimate looks like what the derivative
looks like in the imported sample
estimate we end up with the exact same
objective or exact same grand as we had
on the previous slide so two ways of
deriving the same thing very interesting
what we have here is a great an estimate
that works even if the reward function
is discontinuous so if your rewards is
zero one reward you can still use this
because we actually don't take a
derivative of the reward function even
though we're trying to optimize expected
reward we never take a derivative
through it
and a lot of problems are more easily
specified with a zero one reward
function success/failure and so for
those problems this can actually work
it's really nice intuitively what's
happening with these kind of gradient
estimates is that what you're doing is
you increase if you take a step in a
great interaction you increase the
probability of paths that have high
reward and decrease the probability of
paths that have negative reward or if
they're all the routes are always
positive you increase a lot the
probability of paths that have high
reward and increase a little bit the
path of the probability of paths that
have low reward now it can't increase
all probabilities of course it has to
sum to one
so there's a tree normalization and
happening and so your rewards are
positive all it's still gonna being that
the ones that don't achieve as high
reward ultimately will see their
probability drop to be able to increase
the probability of the ones with higher
reward so it's happening here if you
look at three rollouts if you have three
robots this update with shift
probability mass dorada that has better
reward it's very different from the past
derivatives we saw before with the path
derivatives from the very first stretch
those derivatives will look at those
trajectories and look at how should I
change my parameter vector to locally
perturb my trajectory to get a better
trajectory and then essentially update
the path for each of the rollouts
rather than shifting probability mass
from the not-so-good ones to the better
ones it's a very different way of
optimizing the beauty of this one is
that you actually donate derivatives
through the reward function in fact
we'll soon see you don't have any
derivatives through the dynamics model
to compute the
probability of path under parameter
vector let's take a look at that so what
is this grad log probability of path
under parameter vector well the
probability of a path is a prevalent of
initial state which I left out here then
times probability of next-day given
current state in action and probability
of action given State multiplied over
all times this is the policy we have
access to that this is the dynamics
model we might not have access to that
but look there's no theta here and
that's going to help us a lot so if you
work this out
grad log if a product is grad of the sum
of the logs then since there's no theta
in here
the grad respect to theta just
disappears and we end up with just grad
theta some log probabilities of actions
given State so we see here that we can
compute the derivative without having
access to a dynamics model okay so we
can do now is we can do rollouts for
each of the rollouts we can compute the
grad log probability of path under the
parameter vector this way multiplied
with reward along that path and that
gives us a contribution of that path to
the gradient spelling this out more
fully this how we're going to estimate
gradients and this quantity over here
we're going to estimate this way and on
expectation this gives us the right
grade you can do infinitely many
rollouts this will be the correct
gradient if you do finally many it will
be an estimate of the gradient as
formylated so far there are some issues
it's a very high high variance estimate
so I do need a ton of robots or you need
to play some tricks to reduce the
variance of this estimate so we look at
some tricks to do that baseline temporal
structure exploitation and they'll also
look at what we need to do to make this
stable until the step sizing / trust
regions yes
why is it high variance intuitively the
reason it's high variance imagine all
your words are positive then every path
will try to increase its own probability
when you roll when you roll out then you
have a path this graden says we should
make this path more likely even than not
so good once so that's not great that
means that you have all these
contributions fighting each other all
trying to increase their probability
instead of the bad ones understanding
they're bad and try to decrease their
probability that's exactly we're gonna
exploit but in the current equation
right now that's not exploited so when
the roads are always positive we try to
increase probability of all paths that's
not great so a really using could
introduce a notion of ranking or
understanding whether you're better than
average or not that's called the
baseline so the gradient becomes this
over here with subtracting a baseline
from the reward along a path now if
you're better than average at the
baselines the average you've got along
all executions then you increase your
probability if you're worse than average
you decrease your probability okay
there's some math you can do to show
that this will still give you a unbiased
estimate of the gradient in fact you can
even do some math to find a optimal that
is minimum variance estimate which in
practice people don't use that much but
it is what it is you can principle use
it simply thing to do is to just use the
average of what you got among the robots
that you did and just use that as your
baseline so another thing you can do is
exploit temporal structure right now
what we have is this equation over here
the grad log pebbles have a lot of
temple structure but the reward has
temporal structure too and well you can
exploit the fact that future pass
rewards don't depend on current actions
or otherwise phrased current actions
only influences future rewards but in
the equation over here current action
probabilities or grad log current action
probabilities multiplied with all
rewards past or future we should get rid
of the past and so then we get an
estimate that looks like this where you
only multiply
with everything that's in the future
here rather than also what's in the past
a good choice for B would then be
expected future return that's actually a
value function you can use a value
function if you can estimate one or you
can just use an average of future
returns from that time over your
multiple rollouts this gives us a full
algorithm so this is van illa policy
great algorithm you have some initial
parameter vector theta some initial
baseline may be zero what you think of
as average performance then you iterate
you collect a set of trajectories of
executing current policy in each
trajectory you compute the returns from
that time onwards you compute what we
call an advantage Testament which is the
reward you've got from that time onwards
- we get an average from that time or
state onwards that's the thing we have
multiplied with the grad log
probabilities you might also refit this
average estimate based on your current
rollouts and then you use this and your
grad log probably actually given state
times advantage how much you're better
than average to do an update to the
policy since the basic algorithm if you
draw enough samples this will actually
work pretty well now what remains I want
to give you a quick overview on extra
things you can do to make this even more
effective first thing is that you need
to worry about step sizing step sizing
always matters because you stepped too
far the grade is only a local
approximation step too far you actually
don't improve on your objective you
might do worse in reinforcement learning
however it's even a bigger issue than in
other optimization problems using
reinforcement learning if you take a
step that's too big the data you're
going to collect under the new policy is
going to be non interesting data it it's
not good data to compute grades from
imagine that you have a policy where you
sometimes get nonzero rewards sometimes
you don't but you sometimes do get
nonzero reward so you have some gradient
signal take a step that's too big you're
back in the regime where you never get
any reward you get no signal so you want
to be extra careful in RL about your
step sizes
in supervised learning why did why is it
not as big a concern well if you step
too far the next update the data is
sitting there for you waiting for you to
tell you to go back in the other
direction maybe it's a little less
efficient but the data will tell you
what to do
now RL you're too far you have a
terrible policy you don't get good data
you don't get any signal and now you
actually don't get to optimize in a
meaningful way simple step sizing would
be just to do a line search you have a
great interaction along the great
interaction you try different step size
see how well the robots perform and then
just take the best one and call it done
it's all naive but it because it ignores
more information you can exploit from
your robots it turns out that just looks
at first order information you can
exploit a little more information to
understand where your first
approximation is good where it's bad and
then within a region of where it's good
find the best spot so we're going to
define a trust range is a region where
you trust your first order approximation
and then once we have the trust region
we'll find the best point within the
trust region and we repeat will be a
good trust region we could say well we
want to stay in a region that's close in
terms of in terms of policy parameter
vector we want the resulting
distribution after we make a change over
to directories to be close to the
original distribution so we want to
measure in terms of distribution over
trajectories not measure in terms of
Euclidean distance in policy vector
space now we need to measure this Carol
divergence turns out we can expand the
probability of paths
computo scale divergences go a little
fast here but you can look at the slides
more slowly later it turns out it
simplifies and that all that shows up
ultimately is probabilities under your
policy the dynamics model cancels out
again so you can compute this trust
region based on just having access to
the policy that's nice so we have Cal KL
based on the policy which you can
approximate by looking at along the
rollouts at the states you visit what is
the KL divergence at those states
between the policy that you used to have
and a policy you have after
your update so with that we have a
constrained optimization problem we want
to keep the Cal small enough to be in
our trust region while optimizing the
first order approximation of our
objective we can second order
approximate this scale to make this
whole thing a little more efficient what
you get is the Fisher information
network shows up and what you actually
then get is something very similar to
the natural gradient except with a
constraint rather than just a natural
gradient step questions somewhere yes
sorry say again so jehad here is the
estimate of the gradient so we compute
an estimate of the gradient based on the
like liberation of policy gradient
that's living there and that's what that
is
yes so why do we measure the Cal in the
direction that we're measuring it it's
the competition ly easier one to measure
is the main reason yes so Y we'll use
the divers rather than a distance metric
the math works out really nicely that
that way I would say is the main
justification I mean the calibers in
general is seen as a pretty good measure
of how far distributions are apart but
among the measures you can use for how
far distributions are apart this is the
one where the math works out very
cleanly and is easy to compute with
okay so we have a constraint
optimization problem which can be solved
quite easily it turns out it is very
similar to the natural policy gradient
but by having a region rather than a
step size which is a natural pulse Green
would do you actually have a more stable
algorithm to find the actual update here
you can form the Lagrangian and do dual
descent on the Lagrangian the ground
wire to find Lagrange multiplier that
achieves the right Upsilon and find the
correct step that you want okay are we
done when you do a little bit of extra
work here turns out if theta is high
dimensional inverting that Fisher
information matrix can be expensive and
not practical so there are some tricks
you can play to make this practical
which is detailed in the 2015 ice ml
paper by John Schulman and collaborators
that can speed this up quite a bit
another thing you can do to make it even
more efficient is instead of using jehad
here you can replace this by something
else so instead of using the actual
policy gradient the first-order
approximation you can actually go back
to the importance sampling estimate that
I showed to you that allowed us to
derive this first-order approximation
you can actually plug back in the
imported sampling estimate of the
objective a local estimate which will
also be valid within this trust region
you make a trust region of the right
size and that way you can optimize
something much closer to the original
objective than just using the
first-order approximation to look at the
our LiBr pure implementation that's what
it does as the optimization objective is
the imported sampling based estimate
within the KL trust region and will look
a little more at this later so here are
some experiments that we actually did
with this what you gonna see here is the
final gates obtained with trusters and
policy optimization which what we just
covered in this case for 2d locomotion
jocco the reward function is actually
pretty simple reward function is just
about forth progress and impact with the
ground so you don't need to encode
anything about what walking looks like
the way this works is that initially
this robot will just fall over but over
time - thanks to different random
executions it figures out that some ways
of falling over take longer to fall over
than others and so are better puts more
weight on those trajectories and then
drifts towards finding a good gate that
maximizes reward here are some
comparison graphs with related methods
but that don't have all the machinery
that we just talked about and a shitness
to learn quite a bit quicker even on the
simple problems but especially on the
harder problems such as hopper and
Walker can apply the exact same method
to Atari games actually you can use
trusses and pause optimization to find
good policies on Atari the original
results of course were with q-learning
dqn and it turns out that typically dqn
remains the more sample efficient method
but in terms of wall clock time and ease
of use often the policy optimization
methods are easier and the current state
of yard is often achieved with something
called a 3c which we'll cover in a few
slides and that might not be a sample
efficient as DQ n but is walk up time
more efficient than the QN and very
similar to what I just showed you
let's take a look at that in the last
couple minutes we have here of ten
minutes for connecting with actor critic
we can do even better than what I just
described by bringing in value functions
okay so our policy grained estimate has
some of future Awards - a baseline and a
baseline a natural based on would be the
value function if we have a value
function because that tells us from this
state on average how well are you going
to do and then we can understand whether
the random action you took was better or
worse than average and what if then you
need to increase or decrease the
probability of that action so how do you
estimate the value of a state under
current policy well there's the bellman
equation which tells you
how the value of a current state s is a
function of you know policy probability
of our actions given state then probably
up next dating the Christian action
reward associated with the transition
plus gamma times the value at the next
state self-consistent set of equations
that you can solve for the value of the
policy in fact you can fill in your
current estimate on the right hand side
and it computes from that the left hand
side and repeat until this converges if
you have a large state space you can't
really do it by enumerating over all
states and repeating this so you need to
approximate things you might have a big
neural net that represents your value
function with parameter vector Phi 0
initially you collect your data on your
data you set up the objective above your
based on samples and so you want to move
you want to find a new value function V
Phi that is correspond to their left
hand side here and then it should match
this thing over here which is a sample
estimate of the right hand side and then
of course you don't want to move too far
from your current estimate otherwise you
might be overfitting to the last set of
samples and so this way you get a new
estimate of your value function and then
you might repeat this until you have
convergence we might just do one update
and do a policy grande update again so
we can then fill that in over here
announcement of the value function
what's nice about this is that we have
generalization now across states that
we're exploiting rather than just a
simple baseline them I just depend on
average how much do you get we have
another estimate that's high variance
sum of rewards when you do a roll out
again from the same state likely
different things will happen so clearly
this is gonna be a noisy estimate what
it really is is an estimate of the Q
value what you would like to have there
is to know what the Q value is of that
state in action and then use that
compare with the value and that tells
you how much better the action was then
other actions are worse than other
actions and tells you how to update your
policy so yes it's a reasonable estimate
to just take some of rewards it's an
unbiased estimate but it's not a low
variance estimate and you might need a
lot of samples for it to be good
it doesn't exploit any notion of
generalization that we might be able to
exploit from having seen other things so
we come first thing we can do is reduce
variance by discounting there
easy thing to do and then the next thing
we can do is reduce variance by function
approximation so my video dem reality we
care about actual sum of rewards but you
we use discounting nevertheless to
reduce variance okay
so gamma here then becomes a hyper
parameter that we optimize over that
maybe initially is far away from one
maybe 0.5 something very small so you
don't look far ahead you have low
variance but and as you do more learning
it might become higher you get you're
able to look further ahead inducing
function approximation what you can do
as well you can see it just like we saw
very early on you can see current reward
post value at the next time that could
be what you're using as your cue
estimate or you could use value at the
next next time or reward the road rule
would violate the next next next time
and so forth a three-c is a policy
grading method that follows the things
I've described - a trust region but then
use instead of sum of rewards uses some
estimate with some value there may be
five steps ahead and then the value
function of ten steps ahead then the
value function instead of just the sum
of rewards you can also wait all those
estimates that if you take an
exponentially weighted average it turns
out that the way you calculate things if
you do it cleverly it's pretty much as
efficient as using just one of them very
related to TD lambda and that's what
generalized advanced estimation uses and
then you have a instead of choosing a
horizon at which you cap it off you
choose the a lambda which effectively
says in a weighted way how far you
willing to look ahead in your roll outs
and so that's essentially using
eligibility traces which I imagine rich
might talk about this afternoon so what
do we have then we now have an active
critic algorithm we're going to learn
both a policy and a value function we're
going to collect rollouts and estimates
of the value from each state action the
simplest estimate here is just some of
rewards experience but more complicated
estimates will be the a3c estimate of
some rewards followed by value functions
or the generalized advance estimate
which is the expansion weighted
combination of rewards and value
functions at all times
once you collected those you can do an
update to the value function by saying
well the value at that stage should be
brought closer to what this quantity is
and of course some regularization and we
can get a policy grand update on the
policy based on the advantage Testament
over here
whatever you estimated as sum of rewards
which might be using value functions in
here minus the value of the state that
you were in times grad log probability
action giveth State so pretty easy to
implement and that it's going to be a
very effective policy grading method
there are many variations you can have
here you can imagine that for the
targets here that instead of using K
step look ahead you only use one step
look ahead which would be TD 0 imagine
you can use full roll as there's a lot
of tweaking you can do different choices
of lambda and gamma de you can use over
there and then you actually don't have
to use the same over here as you use
over here principle they could be
different in fact often you might do
something where here when you're
initially debugging you still just use
the sum of rewards because the sum of
rewards is unbiased and wants you to do
is value functions it becomes biased so
often initially you'll use some of her
words over here and still something with
a value function of TD style estimate
over here and then once things are
working well maybe you start using value
functions in this estimate over here too
okay
a3 see a sink advantage active critic
actually this is on a bunch of Atari
game so you see here is that a through C
outperforms Q learning methods DQ n
style methods there's a paper from leaf
about a year ago experience from a year
and a half ago that shows that a through
C outperforms DQ n methods in terms of
training time a through C was also
applied than actually to a pretty
complicated task what you see here is a
learned policy that learned to map from
first-person vision to actions on how to
navigate a maze define that things that
are higher reward like apples and
cherries and so forth so this has to
learn a vision system
3d vision system effectively and actions
all in one big neural net and was
possible with this approach we can also
study the effect of the choices of
lambda and gamma it turns out that for
lambda or here and then gamma here it's
good to choose something that's not all
the way at the extremes so lambda 0.96
gamma 0.98 seems to be best here what
that means is that there's a little bit
of a trade-off between how far you want
to look ahead gamma 0.98 means that
you're effectively looking at about
5,200 time steps and that that's in
these environments a good look ahead
while you're doing variance by not
looking too far ahead
lambda is determining how much you're
waiting the the pure reward based
estimate versus the value function
contribution when you top things off
with value functions and it shows that
you actually want to mostly rely on the
pure reward based Testament which is the
unbiased estimate but bring it a little
bit of the value function to reduce
variance with this you're able to learn
things like this what we're seeing here
is a humanoid robot in 3d now learning
to control itself the reward function
here is just the further north the
better the less impact with the ground
the better and this is a neural net with
about a hundred thousand parameters
going from mapping from the joint angles
joint velocities and center of mass
coordinates and velocity to torques at
each of the motors so it's a completely
from raw sensory inputs in this case at
the at the joints to Ross and Brock
controls at each of the motors and
viateur of course with reinforced
learning is that it's not specific to
the environment we should deploy it
traditionally if you want to you monoid
to walk maybe you would have spent a lot
of time thinking about humanoids and how
you should control them but with this
kind of approach you just deployed our
algorithm on the robot of choice you'd
switch your about of choice and you can
run the exact same code this in also the
Majorcan that is a simulator built by a
motor drive at University of Washington
and this robot is now learning to
with the same objective same algorithm
and to learn to control itself to run
actually quite fast you can also put in
different tasks it doesn't need to be
running here the reward function is
about how close the head is to standing
head height sitting is better than
standing but it's even better to stand
and what you see here is it invents
standing up there's nothing in there
that has told it about what you need to
do to stand up it's just measuring
distance to standing head ID as the
reward function and it learns against
that okay so what do we have left what
we've seen here is actually different
ways of computing derivatives you pop
one level up we've seen it all in the
context of reinforcement learning but
Ashley this is applicable whether it's
reinforcement learning or any other
setting in which you need derivatives as
in the slides what I have here is a few
examples of how you can compute
gradients for stochastic neural nets
using the same methodology we just
covered so it turns out that in this
notation a square node is a
deterministic node a circular node is a
stochastic node and now you can compute
gradients through the stochastic neural
nets by using the likelihood ratio trick
the same way we computed it for Policy
gradients what's really beautiful here
and what makes it so interesting is that
you don't have to do work by hand what
you can do is once you have your
stochastic computation graph you can
actually define a loss function so
instead of saying Oh has a stochastic
node that's being sampled I need to do
this grad log trick you can actually
define a loss function on that graph
whenever there's a stochastic node you
hang off a loss function that's the log
probability of the stochastic node x
what comes after the node once you do
that you can plug this into tensorflow
intensive look compute these likely
ratio grains for you with back
propagation just like it could do with a
deterministic Network so you can
automatically get these gradients for
stochastic neural nets by just computing
the competition graph corresponding to
this which means hanging log loss
functions log probabilities times
comes after off of these graphs some
food for thought we've seen very
different ways of computing derivatives
including in this general setting here
even when you have a stochastic notice
being sampled sometimes you can read
parametrize and bring it up bring out
the noise and you might have a question
what is the more effective way to
compute derivatives it's not always
obvious what the best way is to do it
and I think there's a lot of thought
that can still go into how to compute
derivatives when you have different ways
of computing derivatives
there's empirical things you could do
you can compute them both ways look at
the variance see which one has lower
variance I Periclean maybe go with that
but there might be other things you can
do think we're out of time so what I'm
gonna do is flash by you the five
remaining slides that you can look at
when the slides are put online and made
us set of slides I show you current
frontiers directions where you might
want to do research in deep
reinforcement learning as well as
pointers to recent papers that relate to
this there's one slide here's another
slide of current directions that I think
are quite important so a lot of
opportunity to do research in this field
and a lot of starting points in terms of
coding or more materials there are a few
classes that you can check out also be
linked in the slides there is code bases
that you can refer to to get started and
there's a lot of domains which can often
be the bottleneck if you don't have any
domains to test in that you might want
to try out to work with thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>