<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Creating a Python Mini-bot to Scrape Entire Website - Part 1 | Coder Coacher - Coaching Coders</title><meta content="Creating a Python Mini-bot to Scrape Entire Website - Part 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Chris-Hawkes/">Chris Hawkes</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Creating a Python Mini-bot to Scrape Entire Website - Part 1</b></h2><h5 class="post__date">2013-03-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RJNuqZii0-Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright guys in this next video I want
to show you how we can grab all the
links on a particular webpage it's not
really a full-blown like spider that's
going to go crazy and just started
scanning the web and all that stuff
because to be honest with you I've been
programming for years now and I'm trying
to maintain all that data just by having
my spider go from link to link to link I
find I spend more time trying to figure
out why the hell you know I don't have
what I'm looking for or I'm just trying
to sift through the massive amounts of
data they end up having that I end up
obtaining I find it's usually easier to
kind of go after one particular domain
like you know if I'm scraping Amazon for
computer stuff or all the PlayStation 3
games on Amazon you know I'll attack
basically you know it's one thing that
I'd be like ok with that that's what I'm
going to go after as opposed to trying
to do anything crazy I suppose if I were
you know Google or something like that I
mean you know they obviously have some
of the best programmers in the world
working for them and things like that
sorry I guess I'm not trying to be like
a Google or create some latest greatest
search engine or anything like that just
because I don't think I'm capable of it
but I can certainly make money by
getting certain bits of data that you
know specifically that I'm looking for
like I said I'm I'm going to show you
guys like how like I say if you wanted
to grab every single comedy music I'm
sorry comedy movie on Wikipedia like you
want to grab all the data for every
comedy movie in the United States on a
Wikipedia domain I'll give you just an
example of how I would go about doing
something like that and you know that's
not going to obviously solve your
situation on how to grab every comedy
movie from every domain out there I
would imagine that would probably get
you your IP address banned if you were
going to try to do something like that
just because if it or your even your own
internet provider might have a problem
with that type of bandwidth being used
but you know attack one particular thing
I think at a time is probably my
suggestion until you're a really really
good programmer and then
sure that you'll be a lot better than me
but for right now I'm going to go ahead
like I said and I'll just show you how
to grab all comedy movies on a Wikipedia
website so if you want to kind of adapt
the same type of thing to another
website or another genre of whatever you
may be looking for that maybe wikipedia
touches on or not
I'm sure Wikipedia has an API as a
scraper spider builder I don't like
messing with api's I just feel like I
feel like ap I suck because I have to
learn how to communicate with the API
and all that stuff and I think it's
easier just to build a spider to be
honest with you
plus you're kind of undercover doing
that but let me go ahead and I'll go
ahead and grab like I said the comedy
movie domains all right so if I go to
Google and I type in the list of comedy
movies you see the domain here and this
has friggin every comedy movie ever
going back to the 1890s which is insane
but um all these movies some of them are
links some of them aren't and well these
are links but they're just there's
nothing there but um like you start
getting most of these movies as we get
into more modern days our actual links
that have data that we would want to be
going after so this is a lot of movies
here and it would take somebody probably
years it would seem you know at your
with boredom and it's taking time to
sleep and keeping your sanity to conduct
compile all this data like to get it
into a database things like that it
seems like it would just take forever
and this is just an example with
Wikipedia but like say I wanted to get
every single movie that's that's
actually we're going to do number one
though we would identify and we would
have to extract every single link so
that would be step one
let's get every single link for every
one of these movies alright so first
things first is let's go ahead and grab
the URL here just copy that
and then put it replace it where we had
the band here okay okay so we can keep
all this to remain the same we're going
to open the URL we're going to read it
then we're going to bring it in the
beautiful soup and we'll remove this
part here and I'm just going to paste
this line of code here I'll explain it
but we're doing a for statement on every
link on the page to what we're doing is
using beautifulsoup to find it so it
will say like for link in soup they'll
find all a which is building a list of
all anchor tags in there and the
attribute I basically say if the href
address so if the URL address contains
the words wiki so there's links on the
Wikipedia page that would have that
don't have wiki and I don't want those
because the movie if you look at every
one of those movie pages they all have
this wiki address on the front of them
so in this that's what this carrot
symbol by the way for regular
expressions it just means that the URL
must start with forward slash wiki
forward slash so that hopefully I'm not
confusing you guys but bottom line the
only URLs that we're looking for the
ones that have the carrots let's start
with forward slash wiki so let's just
for right now go ahead and just print a
link and see what that happens you see
what happens there so I still have my
program named grab web page and as you
can see it's going through and just like
I wanted it to it's grabbing every one
of those forward slash wiki URLs for
every single movie on that page and
hopefully it'll stop soon it's a lot of
movies probably thousands alright so
that's how you grab every link and we
use beautifulsoup to do it so you'll
just want to store this little line of
code here some
and you're in your memory or preferably
just somewhere in some sort of directory
of commonly used code that you have
because that's a good way to grab every
single link on a web page and then you
can start doing logical operators on
each link like if the link contains
so-and-so or if it's longer than
so-and-so I'll do this do that
things like that but one second here
we're going to move on to the next step
I'm going to erase this now if you look
at the command prompt here you see that
the actual movie that we want and we're
grabbing obviously this entire thing now
I'm just thinking to myself on how we
can make this as useful as possible so I
could say maybe print link text I wonder
I wonder if that would just get what I'm
looking for here because we don't when
we have the entire anchor with yeah see
that'll get damn this is a big list this
here so you see we obviously just had
the name of the movies now we actually
have some stuff in here we don't want to
which is matching our URL our URL
capture which is like these um these
years and those are obviously links that
also matched so you could click on 1929
and get all the movies there and
everything so we wouldn't want that and
let me think here I'll figure out how to
to get rid of those the problem is
though is that I don't want to just say
if if it starts with a number get rid of
it because I mean I could do that I
guess I could identify four numbers but
if I mean there's like thousands of
movies in here so I'm sure some movies
start with a number and then I'd end up
getting rid of those too so for right
now I'm not going to worry about how we
should
rid of some of these links I mean
they're really not going to hurt us that
much I mean even if you were parsing
each one of these files and saving it to
a database and you could easily do like
some sort of database search based on
like you know 19 and then just you could
erase I'm sure that there's probably
what maybe maybe a hundred of those out
of here that you could identify real
quickly in a database search and delete
it or you could do some sort of regular
expression I guess on it but like I said
I would just be worried about maybe
deleting movies that might start with
some sort of a number that that wouldn't
be my intention and I'm sure that
there's not that many movies that start
with a number but like I said it's I'm
not really sure the best bet on
resolving that I think I would just go
about deleting it after the fact after
I've tried to scrape because even if you
try to request a movie page for 1955 and
it doesn't match the search criteria it
will just move on to the next item so
for right now let's let's move on and
I'll show you how to get each one of
these movies data automatically what I'm
going to do is go ahead and stop this
video here the next video we're going to
continue to move into this series on how
we would go about requesting each one of
those pages individually and do it all
at one time so we have basically one bot
working and I'll finish that up in the
next video thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>