<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High Performance Java EE | Coder Coacher - Coaching Coders</title><meta content="High Performance Java EE - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Adam-Bien/">Adam Bien</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High Performance Java EE</b></h2><h5 class="post__date">2016-10-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Nyq4vcEB6oc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hello to high load Def Con from from
Munich and today I would like to talk a
bit about Java II performance measuring
and what it can expect from average or
from stock application service so
application servers without any any
modification and let's start with a
short introduction so I'm just
freelancer working with Java since 1995
I'm working or I'm working I'm writing a
blog from time to time like a notepad a
few times a year I'm organized workshops
and this egg's good news is a mailing
list which announces very use various
events like for instance this one and ax
IO is a bunch of online courses so and
if there are any questions left you can
ask me questions whenever you like it is
at a xdv for instance this is the first
Monday of the month at 6 p.m. you can
ask whatever like should be Java and
Java related so and there's a couple of
virtual meeting spot and now start with
the content so this was the last slide
we have no time just about for half hour
40 minutes so let's start with the
content so what I have here is an IDE
with a stock application server it is
actually the reference implementation
called payara
and reference implementation this
GlassFish and payara is like hardened
reference implementation with lots of
back fixes and I just started the payara
and what I would like to do is to open
the admin console and the admin console
here is interesting to to see what's
actually going on so what I usually do
in in my in my projects I set up the
monitoring to high of all components so
I can see exactly what performance we
have what exceptions happened successful
not successful transactions and such
kind of monitoring actually all
application servers are able to provide
four years so you will see you know the
HTTP status codes problems you will see
transactions problems you will see you
will see slowest method of ejbs
show you in detail how to up to how to
fetch the information in a second what's
also interesting here we have the HTTP
service and as you can see we have two
listeners the one is 8080 and the other
one is 81 81
this is SSL and this is the usual one
and they are referring to a thread pool
so we have one HTTP fretful and so what
it what it means somewhere here here
there is an thread pool called HTTP
thread pool and you see how its
configured min thread pool size five max
read process five so what it means we
have at most five threats and if have
and if they cannot handle the load it
would be queue up to four thousand tasks
and then we will get five of five three
exception service not available so um if
you are really into high performance and
high load you will probably have to
change this default settings otherwise
your application server one scale for
slower transactions on that note you can
ask me questions in the chat so there is
a Houston chat or use Twitter just add a
B not a hex handle so adenine a hex UC
or the best would be if you would use
just the built-in chat okay so the
service started and and we look briefly
at the ID at the threads there's also
and and and slightly more complex
setting and this is called concurrent
resources so what you can do you can set
up thread pools and pick a finger at a
stretch pose and inject them and use in
your app so on what it basically is and
built-in bulkhead pattern similar to
histories from from Netflix but it's
built built into Java it doesn't provide
as nice dashboards but what it can
easily do just define a common set of
threats for our communication channel
like for for i/o channel with T with T
back-end okay this word the basics and
now I would like to develop us a small
application and see how it behaves so I
will call the application first I'll
need a
a project and I will call the project HD
for high load Def Con so let's create
that so what it basically is a simple
Java 7 app and the simplest possible
Java 8 7 app just comprises a single
class so go here and let's say load
resource we don't know whether it is
high load or not so HD let's do it right
away
boundary load resource so and now I can
say I would like expose you as a
resource so I will call it load and now
a basic thing I could say ok string
message and it should be for me and for
you
good morning and a little bit dynamic
output here so this is the simplest
possible Java 7 application and I will
build that and then run it immediately
on on this server here so say run and
choose the server and say go for it so
it's deployed what was it
resources and a thing it was load so we
have here a good morning load so the
first thing which we can do sometimes it
doesn't work so let's try this
it just does work so let's try this
apache benchmark 5 users and just
thousand requests and now do 10,000
requests because was a little bit too
quick
so Apache benchmark is a tool which
comes with Apache you can just use it
right away and what we see here that our
Connect time is two milliseconds
processing was one millisecond and total
three milliseconds and we achieved we
achieved where was it
four thousand transactions per second so
it's not very impressive but the
question is now what happens if we put
here fifteen threads and now you see
there was a little you know waiting
pause and the problem was possible the
EQ fill up with the requests so I would
just we achieved four thousand requests
per second it's probably with something
happen with the queue so we should look
at this so far any questions no
questions so interesting so um so four
thousand transactions per second and now
what we could do we can go here to the
2d chrome and say okay
application server show me your data and
we have here applications HD and we see
nothing here
so that's let's improve the situation so
we could go dig into into this so we
would say at least you know number of
sessions but nothing
application-specific so I would just go
here there's Jason stuff but nothing
application-specific so um I would like
to improve these situations I would just
go back nothing so what I could do I can
say okay let's do it let's make it an
EJB so now it's an EJB as you can see
something appeared so what means eg
beats are monitored resources and the
monitored resources appear and the
application service because all
applications of us on earth has to
monitor ejbs so what we get we get a
create count method ready count and
remove count so interesting situ
information so let's repeat the test
probably let's this was just go with
thousand so as you can see we got four
thousand three hundred transactions per
second so
the impossible is through the
performance I think improved a little
bit so now we have 6,000 transactions
per second - with an EJB 4,000
transactions per second without any gb
so and this is quite impressive so what
we could look now we have 2200 Ichi bees
were created current 5-methyl ready
count so what it means we there are only
5 EG bees active so what is the reason
for this the reason for this is we have
5 HTTP requests and and actually also no
5 hmm HTTP requests also we started 15
threats so what it means it could be a
bottleneck so what we could do for
instance we could go to the application
server to the console let's do this and
say here where is our pool HTTP thread
pool and max read pool
15 save that repeat the test and go back
here retry that and you see current 15
so what it means the bottleneck was the
pool of the application server now our
application now our application was was
paralyzed a little bit more we had 15
parallel parallel requests and if you if
we look at the performance we get 6,200
transactions per second so we could just
increase it at 25 so now it queues up
again so the performance dropped because
of the queue and if we look here nothing
changed because the HTTP read pool is
still set up to 15 so seems to be an
interesting information and again this
information is available as Jason which
is really easy to consume so what we
build with that is monitoring in in
systems were where we were the whole
night there were stress tests and in the
morning we would like to see the metrics
and this is actually great great great
way to do this if you are interested in
this look at Lightford
and lightish is a monitoring tool which
goes more into deep into desire github
projects or go to github and it will
connect with payara or light fish and
produce some nice charts so so far what
we did a little bit optimization and of
course you shouldn't play with the max-q
size too much because because if it is
too deep take you what you can get you
can get out of memory errors so okay so
we discuss the basic performance still
it's not very impressive so the next
question would be actually what we can
do here or what actually happens if we
create another class and the class we
call the class load manager this is the
business logic this is another EJB and
this EJB returns the actual payload and
this is enterprise good-o
bins are with you okay so plus system
come
Milly's again and I would like to inject
this class here load manager alum is
perfect and I would use the class here
so deployed I would repeat the test with
15 again
so we have 4,000 transactions per second
it's still queues so we just so we have
4,000 transactions per second now we get
some errors because it is overloaded and
what it basically means this this
injection has some impact on performance
but with EJB seer h EG b is created once
and pulled and sorry here and this
injection happens once per instance not
every time if this were request request
scoped so I just looked any questions
everyone agreed okay cool cool audience
what's about persistence so persistence
of Ida topic if you if you would add
persistence to the two for instance
Derby we can cut JP a so what will
immediately happen you will see
performance degradation because of i/o
why because of i/o well i always the
slowest possible thing so what I try to
do here to give you an impression what
application server can offer you
performance and with if you get the
database you will be as fast as a
database so I'm usually in real world
what happens you know application server
are not utilized and the databases are
overloaded so um so it really depends on
the on the database and really depends
whether you are accessing the database
in memory there are in memory databases
over or over the socket and over the
socket you will see performance
degradation of course okay so thanks
aunt Thank You Anthony for for the
question and yeah and this so we have
ten thousands transactions per second
now because the queue is empty so you
see the the performance really really
depends on the settings of the
application server so uh what happens in
in real world so what I did what we
there is one project I interviewed one
of my clients and they are using Java EE
for manufacturing and IOT systems and
they have in production for years
actually started with Java five 25,000
remote procedure calls per second and
1500 clans are connected and the latency
is 50 to 100 milliseconds but they are
doing a lot and what's also involved a
database is also involved in the in the
communication so if you are if you're
interested I will just paste this in the
chat look at this and this is this is a
really serious application 24/7 and and
highly available mission-critical
system so I would just put it in the
chat so I can pick it up so now how to
look into the application server so we
got a basic impression so I'm just we
can go back here and and let's go to the
duty console to this one where was it
four four eight four eight so you also
have transaction service and you see a
number of commits so we started in our
test 52,000 transactions no active
transactions is going on so let's start
the test again so now we just let's do
just ten threats and increase it a bit
still no active but 62,000 transactions
per second I'm too slow so I'm why is
this important because you are actually
seeing whether we are testing something
or not and again Jason view is also
available committed count and what's
very important is all by account
so what could happen we have crazy crazy
good performance caused by cost by
rollbacks and let's go here what you
also get we can monitor the connection
queue what overflows so what means here
we have zero overflow so far which is
interesting and HTTP listener one so we
can monitor here connection
so the health is still still okay and
the thread pool from the HTTP listener
is here and you can see coarse threads
is five and max Reds is 15 so it
actually the the setting was taken from
the admin console okay so now for
application developers what we do we
start with or J visual J visual VM I
thought it's gone
this will be terrible so there is a JVM
and I would like to connect to GlassFish
and what I see here is the memory
consumption so there are there are no
tails going on about the memory
consumption or application service now
what we can do we can perform garbage
collection and you can see it goes down
to 84 megabyte of RAM this is a stock
fool not not micro nothing this is full
application server and why why is the
important is to estimate you know how
much memory the application server
actually consumes and so not this so we
don't need this we need this as you can
see this is the spike and we have our
short spike so there is some some
correlation between the threads and and
and memory consumption as you can see we
have here 150 and 100 so there's like 50
make difference and it just disappeared
immediately so this is it is really very
important to watch how the application
server behaves on the load and there's
another tool what I use frequently and
this tool is called jmeter and with
jmeter it cannot generate as much load
as the apache benchmark because or it
consumes more resource more resources
but I can basically say I would like to
5/5 users and create a
sampler and the sampler is HTTP request
sampler and what we like to do is to
have a local hosts with 8080 and I think
the pass was HD resources load is it
true HD HD resources load and the only
thing we need is like a summary report
just to see whether we get errors or not
so I will launch that now summary and we
see we have 2000 transactions per second
this is actually not that interesting
what we see there are zero errors so now
why I'm starting this first now you can
see what is the true impact of of
performance you see or to to memory it
consumes a little bit more memory so it
goes back to 90 but it recovers
immediately so what will be interesting
you know we have just allocated 500
megabyte of / m2 GlassFish there is no
pressure to the garbage collector to
actually be efficient with the with the
garbage okay so what we also see here
number of threats this is interesting so
we can sort the threats by name and what
we should see is the I think this are
these red see where are the HDP threats
this is oh yes here are these are the
HTTP threats those are the ten threats
as you can see there's nothing to do for
them right now with with geometer why
not because I only started five of the
threats but here you can see you know
how overloaded is actually the front end
of the application server what's also
interesting we can start CPU profiling
and why the CPU profiling is interesting
so what we can look at this at the
actual overhead of eg B's or Java EE so
I would go to comm dot ax and look at
our ejbs
so we see here there is a boundary GB
this is it should be three one we have a
payload CDI bean and load resources and
what you can see is the the D
performance overhead is negligible so it
doesn't even appear here in the
Paige and what is the main overhead this
is Nile network of course is the
communication between the network the
yeah the the load balancer and the
application server so I will stop the
test for a for a second because
otherwise my machine gets hot and
meanwhile I will look at the chat so
this is where is the chat any questions
in the chat
no lazy chat got a private message or
don't write me private messages during
the session so you can write official
ones and let's see
HD contrib we are here some oh I see
myself this is nice but no questions and
you see you have to write in English I
have no idea about Russian language okay
so we are pretty far with that
now about the what we can do within the
application for high load applications
so the problem we have we have the HTTP
threads in one point of time they could
be overloaded with work so it would be a
good idea to pass as soon as possible
the work from the from the front end
threads to our own threads and maintain
as thread pool pair communications
channel so how to do this so thankfully
we have Java 8 so it is really nice so
we can say get payload a method so I
will try to do it a little bit nicer or
no you are a highly skilled audience or
forget about the the T nice things just
go to straight Java 8 so suspended
asyncresponse is the name async response
response and what happens now now the
HTTP thread is suspended so actually
pass back and the control is passed to
the to our thread pool we still don't
have one but we get some one and now I
can say ok the response is one
interesting methods and the method is
resumed
so and with the resume with the resume
we can pass the payload here
so this is an asynchronous call and so
we could just
of course my jmeter we could repeat that
with Jamie time so go for this
no so it started so now look at the
performance here so it will be no a lot
of difference what would be interesting
here at these threads what's the
behavior here and as you can see so that
these sweats there is actually they have
almost nothing to do because the control
is passed to another threat so um we
just stopped tea Lotus again so okay so
but this is not enough we would like to
have more control about this how to do
this well where is my ID orders I think
a Christian
Oh Alexandra Roma asked me could you
please tell me more about practical
aspects of using a baby on an jmeter so
practical I think what do you mean
practical is real world so for if we had
a little bit more time I would show you
jmh is a Java micro benchmark harness
this is what I use in real world to get
reliable numbers this is just to give
you an impression of the performance of
application server so I always use a B
Apache benchmark for for like no smoke
test to find out what the application
server is doing NJ meter for sustainable
load like this so I just generating load
and monitor the performance and if you
go to where a second Adam bean github
and a Java EE testing you will find you
will find oh wait a second so you will
find examples for my online course about
testing and what I did towards the ends
of the course
I created this is Java testing I created
this is the online course but I created
a benchmark with stress test and jmh and
what what what's interesting I use jmh
for this and and there is a test like
CDI versus you should be is it here CDI
CDI it should be stress and this is
based on jmh this is jmh benchmark so
look at this if you're interested so I'm
in one in one project client asked me to
prove that the application can sustain
several thousand transactions per second
in a cluster and so we created a
executable jar and we could performance
distress test whenever we wanted and
this is I was practical this is what we
did in the real world so and this jmh
gathers the performance data so what's
also interesting I measured here the
performance CDI versus egb and it turned
out that a job is a little bit faster
20% faster and then at github if you
search for github
see the CDI CDI versus EJB you will find
someone who is called saw some elesif
and this is I think a Russian IBM expert
and what he did he she just took my
example and create some matrix and what
he measured the performance impact the
performance a difference between spring
egb in CDI and unsurprisingly it should
be one because of the pooling could I
demonstrate here right now so um if you
would like to repeat the test at home
just clone his repository and just try
that okay
so practical means in in production are
used more jmh to have reliable numbers
you can specify in jama GG MH is like
maven plugin so you can launch and and
then it generates a jar which is
executable with all dependencies and you
can specify it whether you are
interested in throughput or whatever and
you're working with annotations and and
configured as they think with
annotations so I hope I answered your
question and and jmeter and Jamie tell
what you use for stress tests what you
can do with jmeter you can launch jmeter
I think with - C flank and and point to
configuration in a headless mode so if
you launch Jamie Tahir
Jamie term - help is that something like
this you will find these settings so and
there are some - me - n is no window and
there are property files there's a test
ai - these the test file so what you can
do you can launch jmeter and headless
mode what we do on Jenkins ok I hope I
answer your question but now this is at
least what I would like to show you is
this so now let's we are somehow high
performant but not very robust so now
change this so what we already did right
now now the thread is executed
immediately which is not a good idea so
therefore there is a nice utility called
calm
the future and this completable future
we can supply a task would supply
something and in our case this is the
EJB use again nice this EJB the method
payload supplies a string and then this
response consumes just drink and now we
built a pipeline and a nice one so what
we also get for free we can say ok hence
we are high-performance the tenth the
timeout should be one second and after
this the client gets the default default
HTTP status 503 service not available
what you can also do we can say said
timeout Handler and specify a timeout
Handler and specify your own response
code in case the server gets overloaded
so it's kind of handshake with
additional with additional with
additional header information now this
is still a little bit problematic
because this supply async uses uses
acing pool and the pool is based on
fork/join pool so let's improve the
situation and I will inject managed
executor service and this manage
executor service is a executor service
from JDK one five four one six seven
eight but managed by the application
server and what we can do here I can
pass the managed executor service here
so and now we created a pipeline which
is executed by managed executor service
and remember that our application server
here there was it where is our admin
console
not here here we have here
asynchronous resources somewhere here
and we inject it basically this managed
executor service there's only one this
is the default one and we injected this
one and it has quite a high number of
threats so you should also play with
this it's a little bit too optimistic so
what we could do right now so we just
cope go back to the two jmeter wizard
and launch it again
this was not very wise because we lost
we lost our our configuration but what
you will see here you see already we
have some executive service threats and
just very quickly created again and you
have now full control how concurrent
this channel becomes and for every
resource on end point you can create
such a pool and this is called Park
heads pattern so I will just keep that
so if I would relaunch this you will see
that the HTTP threat has nothing to do
anymore and the full control is in
Indian in your executor service so I
would just add just very quickly threads
forever and five and this was a simpler
I think exactly and this was HTTP and we
said we are local host 8080 and there
was HD HD HD resources load I think and
then we only need this summary listener
summary table and now launch it no
saving and go for it so it looks nice so
and what's the difference so where are
the threats these are the HTTP listener
threats
and somewhere should be the pool with
this is this pool threat this is the
threats here and and they are and they
are busier so what what what happened is
we pass control from the HTTP threat to
the 2d to our threat it does not change
anything at the performance but our
system because it becomes more robust
because we can we can just slice the
total number of threats and and dedicate
threats for a combination communication
channel so I get an a question here and
I'm what about scaling beyond a single
machine go to my youtube channel I
actually did it and what I did it you
can very easily with docker there's not
a dhoka dhoka course but would be very
easily to start a docker and what will
what will happen we use usually H a
proxy which load balancers to
application servers and but surprisingly
it also the performance is less because
the H a proxy consumes some where the
redirection some here's some performance
penalty but of course you can scale
better
so with dock it's not a problem if you
are interested in it I always stop this
wait a second so go to my YouTube
channel and I think and I'm being
YouTube exactly and one of the talks is
going to be load balancing load
balancing Java micro services so look at
this I think is five nine minute video
so from scratch and building a service
and load balancing to service ok any
other questions so HD conf if I add some
logging the performance will disappear
basically so a system out print line
will be the most terrible stuff and file
logging is slower and network logging
depends of course on your infrastructure
meanwhile is almost faster than then
persistent logging ok but what I
actually wanted to show you so how we
are the time is almost over so the very
last thing what
what I actually did created a very small
library called porcupine and if you are
if you are interested in hyper form and
Java it is already used in some projects
and what porcupine does I will use this
PI version so just edit so go to github
and search for porcupine and then what I
can do I can say add inject it's
dedicated dedicated means these are
qualifier and now I can just go with the
straight executor service and I will
call that HD pipe HD pipe
nice name and yeah just rebuild
everything and what it allows you it
allows you to specify on the fly Fred
polls without reconfigure the
application server so what it basically
means I can go here and say exactly and
now - II so what do you see I injected
statistics porcupine statistic HD pipe
pipeline MHD pipe with rejecting
execution handlers or what you can do a
rejected execution handler basically
means you can specify the behavior on
overload and this is standard it comes
from the standard comes from from JDK
1-5 so look at rejected executed handler
and you can specify different executors
handlers and it comes with defaults like
you see here completed tasks one core
pool size 8 currents thread pool size is
too large a thread pool size was 2 and
now we can could perform our little
stress tests so a B - C 5 threads - n
let's say I don't know 5000
so and now look at this and the C
completed task is 5003
largest red pool size was 8 so and
maximum pool size is 16 so the pool was
not overloaded so what you can also can
do you can you can of course inject
several such pools and each injection
will create behind-the-scenes and new
pool with this name and they will be
isolated and this is I would say a true
bulkhead pattern what what's interesting
because we have java ee the whole
dependency is just this is the
porcupines supply they are just a view
classes so this is actually one class
which cares about the dependency
injection and here the other so it is
empty or some dependencies where is it
yeah no so I don't get it as even empty
shouldn't be empty but the other one is
like three classes so wait a second you
just go this is the truth this is Deewar
there's a class lip this is the
porcupine zero far far four and this are
there are a view on rotations a view
classes and this is lambda so you see
more more than one so um so this is
actually used in production and you can
configure as well the the executor
service using it's a name HD pipe so I
think there is a question just logging
and go to HD contac okay no questions in
the chat we are almost over time so what
the only thing would like would mention
we used rest so far application service
comes with WebSockets as well and you if
you have to send a lot of messages
WebSockets come with far better
performance because the header is tiny
and the HTTP header consumes some you
know there's a so I would say HTTP
header kilobytes and WebSockets are is a
byte so there's a huge huge amount of
traffic going on for in smaller messages
so
just see whether I will able to
configure it on the fly and a hex HD
boundary custom confirm HD pipe
configuration extents execute or
configuration execute or configurator
not configuration sorry for that so and
what I have to do I will have to
implement override a method and this
method configures a pipeline so the
pipeline name they say if a HD pipe
equals name so now we know we are here
then we can do the following you could
say ok execute or configuration dot
default configuration dot and now I can
say you know whatever or I can build one
and say this I'm sorry new execute or
configuration dot builder and now say
dot for instance which which behavior on
overload abort policy color and policy
color on policy means basically back
pressure so the thread pool will block I
can register my own custom rejected
execution handler discard all this
policy and the interesting part are
bought policy colorants policy custom
rejected handler discard policy and so
forth they already come with JDK so this
is not
you know custom Java inject invention
keep a lifetime max bull size so what we
could do we can say let's say collar
runs policy queue capacity is 10 and
then say core pool size is 5
max pool size is 10 built and we return
this yet return so and I have to say
specializes so it means take this one
and so I think the GlassFish tight and I
will have to rebuild us so what it
basically means I have can have one
configurator or more configurators and
actually specify the behavior if I'm
cannot live with the standard so um is
it deployed run so and now if we go here
we see rejected execution handler
colorants policy and we see maximum pool
size is 10 active active count is 1 so
what it could do we could say queue
capacity max maximum pool size is let's
say 3 this will be hard is to free queue
capacity is 2 so there is current pool
sizes one core pool size so and now we
can say a B a B and repeat a test and go
back and you will see larger thread pool
size was 5 so now the thread pool was
overloaded there are no rejected tasks
so far so we would have to do to
increase it as you see remaining queue
capacity was 10 so just go with 15
just go with 15 and repeat go here
still no rejected remaining queue
capacity to stand so it's really hard
hard to overload to overload an
application server so I wanted to give
you actually a brief still so we are out
of time so thank you for watching and
see you on upcoming conferences probably
even in next X TV this is exactly the
same venue we use right now so if you
have any questions left go to X TV so
this is a first Monday of the month and
and what you can also do ask questions
in github so in github series you can
prepare a question that asked in advance
this is already a couple of questions
for the next X TV show and see on
upcoming conferences workshops or come
to Munich so the next workshops
micro-services and testing are in
december so thank you for watching and
bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>