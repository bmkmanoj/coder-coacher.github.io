<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Flume NG Basics | Coder Coacher - Coaching Coders</title><meta content="Flume NG Basics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Flume NG Basics</b></h2><h5 class="post__date">2013-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NA9fo4b0RBI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the first in a series of
webcasts designed to help you learn some
of the basics of designing integer
solutions today we're going to cover the
basics of loading data streams and they
do and didn't dupe using plumb for those
who don't know that Oracle we view big
data problems as often falling into one
of four categories acquiring and
organizing data enabling greater access
to data analyzing and refining important
data and finally making decisions and
publishing insights about data
today's topic falls under data
acquisition and organization while the
web is full of MapReduce tutorials one
topic not often addressed is how do we
get data into Hadoop cluster in the
first place
well they do provides a file system
utility that lets us copy files into TFS
it doesn't make the problem of
collecting huge amounts of constantly
generated machine data enemies here in
this talk we'll cover the basics of
solving this problem using form here is
a problem anyone who runs a web
application and once to leverage who do
can potentially understand on the left
you can see I have a number of
application servers they may be in the
cloud they may not always be a constant
number of them but they're all
generating log data I want to analyze
reps I want to analyze extremes or
shopping baskets however my Hadoop
cluster is back in the data center at
least a hop away from my application
servers so how do we get data into HDFS
flume provides a straightforward robust
way to move Xtreme Ian log data from
application servers into HDFS in fact
that's part of what motivated its design
in a very basic deployment flume agents
resident on each application server can
tail log files and write that data to
HDFS but what about our cloud 2.0
deployment I mean they will not want our
app servers to speak directly to HDFS
that's not a problem
flumes Avro sources and sinks allow for
building multi-hop deployments with both
fan and/or fan out simply by chaining
together agents we're going to build a
basic flume agent which listens for
input via that netcat sink and writes it
to HDFS as a text file but before we do
it's worth speaking to what flume is
glue is designed for distributed log
collection and movement it's quite
effective at moving data around a
variety of sources and landed in HDFS or
HBase
bloom offers a lot of desirable features
it provides durable transactions it
allows the writing of sequence files and
compress data to HDFS and it offers
in-flight data modification through
interceptors it's also worth noting that
firm has improved quite a bit between
0.9 the last og version and present no
longer requires a tsuki per installation
to function there is no longer a strict
master/slave architecture this really
extends our ability to develop rich
topologies for data movement here we
have a fluent configuration file
necessary to set up a basic agent it
defines three pieces per source how we
read data in the channel which is used
to manage transactions before they have
been written to the sync and the sync
which writes transactions to their final
destination or to another flume source
you'll notice we specify the name for
each at the top of the file the source
is set up as a net pet source the sync
is an HDFS sync and they are connected
by a memory channel which is just a ran
back transaction cue once we created
this configuration file we can simply
start the agent from the command line
once the agent is listening we can send
data to it by echoing data into the
other end of the netcat pipe every 30
seconds the flume agent will commit any
data received HDFS that's really all
there is to it let's look at a
demonstration of this code in action
we're going to use the configuration
shown in the slides to load XML encoded
data about customer activity in HDFS
this will allow us to use hyper Pig to
transform activity data to get a richer
picture of our customers for example if
we look at example data
you can see we have XML data that
includes customer information email
purchase timestamps and an array of
available flights we'd like to send this
record by record into HDFS using flown
look into the configuration file you can
see we have the same configuration
properties shown in the slides we have a
netcat source an HDFS sync which is set
to write text as a data stream with a
roll interval of 30 seconds and we're
using a memory channel which again is
just a simple ramped back transaction
queue we connect the source to the sink
via the channel and we're ready to run
the agent we have to provide a name we
use the same name specified here in the
configuration file HDFS agent and we
point it to a configuration file
after that we can start the agent if we
tail the log file for the agent will see
it processing the configuration file it
adds a sink it adds a source it
allocates a memory Channel and begins to
listen on the port to send this data
into indical we simply cat it through
the other end of a net cat pipe
the series of bouquets indicate that
each message went through line by line
looking back at the log we can see that
NH CFS temporary file is created and 30
seconds later is rolled to create a
finalized file that is time-stamped we
want to send another set of files it's
simply the same process
once we've done this we can shut down
the sink and examine HDFS to see what
files we've collected
you'll notice we have flume data time
stamp files for each of the time stamp
time timestamp commits that flew made
during the course of our experiment and
if we cat these
you notice that we've sent through
exactly the same records that were
stored in our XML file now the data is
loaded into HDFS the next step might be
to session eyes cut the customer
activity from there we can look at quick
frequent clickstream patters and
patterns and get a better understanding
of user actions with with our web
application
well flume does a great job providing
distributed data movement into HDFS
there are some other open source
alternatives describe a thrift based
solution which originated at Facebook is
lightweight and robust however it can be
tricky to build for various environments
and isn't specific specifically designed
to integrate with the do Apache Kaka is
a promising incubating project which
originated at LinkedIn kafka provides
similar features to flume inscribe but
it maintains a more traditional publish
and subscribe paradigm</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>