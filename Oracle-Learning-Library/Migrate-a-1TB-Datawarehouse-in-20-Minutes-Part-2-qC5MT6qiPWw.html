<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Migrate a 1TB Datawarehouse in 20 Minutes (Part 2) | Coder Coacher - Coaching Coders</title><meta content="Migrate a 1TB Datawarehouse in 20 Minutes (Part 2) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Migrate a 1TB Datawarehouse in 20 Minutes (Part 2)</b></h2><h5 class="post__date">2011-06-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qC5MT6qiPWw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the Oracle real world
performance video series this is part 2
of my great a 1 terabyte data warehouse
in 20 minutes with Andrew Holt worth
senior director of Oracle real world
performance team loading data is just an
overhead and is not the business end of
any data warehouse the business end is
when you run queries and make decisions
so when loading data we have to start
thinking about how can we speed up the
queries at a later point in time one of
the most important techniques of course
is compression but it will slow down
your loads and as you notice we were
incurring quite a lot of CPU cost
associated with loading because we were
decompressing data pipelining it and
then loading it compressed we had an
increased CPU time associated with the
load however the payback comes back when
we're doing scans as you just noticed
and doing backup and recovery it's the
best way to look at it is an investment
in the future
likewise there's another important
investment to be made in the database
and that's the use of partitioning as
well again partitioning does not speed
up the data loads it does increase the
CPU time associated with the data load
but again it helps downstream in terms
of queries because we're able to exploit
query pruning and then for manageability
if we want to rapidly exchange data into
the database and likewise exchange it
out and purge the data from the database
again in well designs data warehouses
where it's a write once to read many it
becomes a good investment for the future
to get back to external table loads a
very simple way to do this and there
should be things to note about the
syntax in an external table load to be
successful the first thing to note is
you define it type external the next
thing is you want to read from a
reference mount point or a directory
that is fast and capable of supporting a
lot of reads to enable you to reload in
parallel in this case we used a dbfs
filesystem on the database machine the
other thing to note is that we basically
used a preprocessor to our
to decompress the data on the database
machine so for those that staged data
external to the database machine and say
have an NFS mount point we're able to
pull across the compressed images of the
data and decompress it on the database
machine and effectively make yourself
CPU bound and not network bound it's
important to maintain attention to
detail and keep make sure that the
character set is aligned with the data
and then the last part is you'll note is
we've used for compressed files and in
that case the maximum parallelism we can
use when we only have four compressed
files is parallel for the parallelism
could be less than four but you won't
get it any higher when you only have
four files okay so let's just check
point where we are now we've loaded a
terabyte of data and it's all compressed
and in the database partitioned and
compressed optimal for queries before we
need to run queries now need to start
thinking about creating schema
statistics so this is pretty much the
real-world performance strategy when
creating new databases for projects is
what we like to do is create the tables
and if we've got a set of queries that
we'll be running against the data we
like to seed the optimizer by running
the queries on the tables it when
they're empty now these may not be the
final set of queries because remember
this is a data warehouse but it gives us
a chance to make a good head start on
seating the optimizer the other thing is
we also do for large partition tables is
enable incremental statistics and this
comes back and helps us later because as
a data warehouse gets bigger and bigger
we don't want to see our statistics
gathering get bigger and bigger each
month we then load the data as we've
just seen and we gather statistics so
let's initiate starting statistics
gathering here we are so we're going to
build the statistics on this database
and we'll be building it on the big
tables and immediately you can see we've
made the Machine extremely busy and you
can notice that there's reads from disk
and there's reads from flash
and eventually will actually notice a
little bit of rights to a desk as the
temp segments start to get formed to
building this index we go back to II M
we just take stock of what we saw you
can see the the four humps of the four
weeks of data that was loaded you can
notice a little bump
associated with the accounting that we
did and we'll check back in in with the
statistics building phase a little bit
later you can notice that there's a
spike just starting to form but let's
check back later so as I spoke earlier
incremental statistics is an extremely
important part of a data warehouse we
just can't afford to let the statistics
gathering process get bigger and bigger
as the database gets bigger and bigger
so in 11.1 and of course naturally 11.2
we have this concept of incremental
statistics this means that we basically
only gather the statistics on recently
modified partitions however the biggest
challenge with incremental statistics is
maintaining what we call the number of
distinct values this is the most
important statistic to keep in sync
other than the number of rows in a table
because it allows us to get good
cardinality estimates when optimizing
queries to solve this problem what we
have created is new structures that are
stored in the silk stable space called
synopses and what this is is a bitmap a
compressed representation of all the
distinct values of all the columns under
the tables are subject to incremental
statistics because really the challenge
we face and this is what happened in
older versions of Oracle is maintaining
a good ndv value on the global
statistics associated with a table in
previous releases is we would just add
up the number of distinct values in this
very simple case you can see that well
partition 1 has four distinct values 1 1
3 4 5 and partition 2 has five distinct
values 1 2 3 4 5 now if we were to add
the end v's together
we would end up with nine and this would
make us almost 2x wrong with our
cardinality estimates if we were to use
the synopses it would get the correct
ndv which is five now this is a very
simple case with just two partitions now
imagine that we have 40,000 partitions
the scope for error and hence bad
cardinality estimates and then
downstream bad execution plans is huge
so for this reason people need to be
aware of what's going on and why we
create those structures called synopsis
when we enable incremental statistics
the other thing to also note if you're
upgrading a database and you use
incremental statistics when you enable
them the first time you gather
statistics will take a long time because
we're building the synopsis the payback
will happen on the second third and
subsequent builds of the statistics when
you add a data into new time based
partitions now let's go back and see how
those statistics gathering is going as
you can see Enterprise Manager is just
starting to tell off the statistics
building process and you'll notice again
it's very much a CPU bound process in
many cases data warehouses of old this
was an i/o bound process this was
because the i/o was not sufficient to
keep the CPUs busy when gathering status
as you can see we've actually finished
the statistics and that took us a total
of three minutes and 20 seconds to
gather the statistics on what was one
terabyte of logical data</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>