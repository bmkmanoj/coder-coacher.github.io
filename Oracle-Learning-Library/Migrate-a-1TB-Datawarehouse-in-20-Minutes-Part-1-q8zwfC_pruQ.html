<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Migrate a 1TB Datawarehouse in 20 Minutes (Part 1) | Coder Coacher - Coaching Coders</title><meta content="Migrate a 1TB Datawarehouse in 20 Minutes (Part 1) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Migrate a 1TB Datawarehouse in 20 Minutes (Part 1)</b></h2><h5 class="post__date">2011-06-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q8zwfC_pruQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the Oracle rural performance
video series this is part one of my
grade one terabyte data warehouse in 20
minutes with senior director of the
Oracle real world performance group
Andrew Holdsworth hi everybody we're
gonna do data warehousing in 20 minutes
today for this exercise we're going to
use Oracle's latest and greatest X 2 - 2
database machine looking at the screen
in front of you you will see two windows
the top right window is a very
abbreviated form of the operating system
ace machine you will note the 14 storage
cells as the first 14 lines and below
that you'll see the eight database nodes
as they lines below that we'll be
looking at these screens from time to
time just to see how busy we're making
the machine as we migrate the database
in the other window you will note we
have the application which is a menu
driven system and the first thing we're
going to do is create the database and
the table spaces and the schema objects
I'm going to start that now you will
notice we're scrolling through on sequel
+ and you will have noticed we're just
starting to make the database machine
busy
well that is running I'm going to start
talking about the biggest challenges
that we see when migrating large
databases I love the challenges we have
associated with building databases
people come and tell me the problem is
data loading they say my problem is
always data loading and I always tend to
disagree with them I said your problem
is not your data loads it's your queries
data loading as we're going to show in
this video can be quite boring
in fact so boring it'll turn you to
sleep and many people say it's like
watching grass grow or paint dry or even
watching cricket but in fact most data
warehouse systems are actually detuned
before they've even started and the
reason for that is they took the wrong
hardware method you will notice we've
now built the database so now we're
going to load very quickly 30 days of
historical retail data more information
on what we're loading loading later
you'll notice we're going to be loading
four weeks of data in four batches think
really - notice very quickly as we load
this data is how busy we
the database nodes you'll notice that
they're starting to ramp up and very
quickly we've managed to saturate all of
the database nodes at nearly a close to
a 100% CPU utilization returning to the
hardware pathology most systems are
chronically under sized for data
warehousing a lot of this is historical
and aligns with corporate standards
associated with buying servers and in
many cases we see customers or teams or
organizations running with a machine
that's just way too small for their data
warehouse this may be as a result of
being told you get this number of cores
and this much disk space on the
corporates and and that is your data
warehouse unfortunately this isn't up to
the job when you may be moving millions
of rows in a query as opposed to five or
six or even 10 rows in an OLTP query so
in many cases we see the hardware is
just too small in terms of the number of
raw CPUs physically able to process the
rows required the next mistake that gets
made is they haven't balanced the i/o
with even the CPUs that they've got so
in many cases you have CPUs not being
able to be utilized because everything
is stalled in disk and in many cases
this means again another 10 X or an
order of magnitude of performances so
sacrifice and in many cases people can't
even run parallel query across nodes in
say a cluster because their interconnect
strategy was to use say gig e rather
than something like InfiniBand which has
much higher bandwidth and say gig gig e
Ethernet so everybody who's an Oracle
user they still know they have to get
the job done and so when working with
the incorrect equipment and the wrong
equipment they still have to get their
jobs done they still have to load their
data and they still have to run the
queries instead of being able to run the
equipment and the software as it was
designed to be used in a data warehouse
they revert to their old sort of
fashioned OTP tuning routes and start
building indexes all over the database
this has a impact on the size of the
database as well as the maintenance
stuff but it has a dramatic impact on
both the loading speed
and the ETL speed this means they slow
down and the database just gets bigger
and in many cases this is just the wrong
approach
and one of the simple ideas in terms of
maths we'll go through a little bit
later the fact that the machines are all
stalled on i/o and not using the CPUs as
well people tend to run way too many
queries just to try and make the CPUs
look busy so there's again we overload
the system both IO on a system is just
way too small and sized incorrectly
let's go have a look at our load and see
how that's going you can see the load is
progressing and it's still CPU bound and
we're pushing the load very well what
I'd also like to do is take some time
and have a look at this load process in
Enterprise Manager you will now see on
Enterprise Manager we have the
performance screen and you can see that
we've actually loaded the first week and
we're just finishing up the second week
of this data load you'll see the second
hump on the screen coming along ok let's
get back to talking about data loads and
so this data that load that we're doing
is something to think about we've loaded
a very simple schema consisting of these
five tables which is a subset of
Oracle's reference retail schema but
we've just used sort of more
conventional names to make for clarity
the most overriding entity or the
driving entity is the transaction table
this is somebody going to visit a
supermarket and as you can see for each
visits of the supermarket they have to
make payments on it and it has to be
done at a location for each transaction
of course there's multiple line items
all those grocery items being swiped
past and then naturally for each line
item there's a relationship to product
so let's look at the debt amount of data
that we're actually going to load on
this tape the first thing to note is
we're loading a terabyte of data and
this translates to just less than 8
billion rows and you'll notice that the
biggest table which dominates the data
volume which is the line items which is
just over 900 gigabytes of data is just
less than 7 billion rows it's worth
remembering these numbers because a lot
of people ask us how many rows can you
put in a database with all large
database techniques one of the most
important things is to learn how to
press the data down with Oracle from
version 10 onwards we had multiple
compression modes and this is the
default compression mode that we had in
version 10 or standard compression that
was available through direct path
loaders in version 11 is very often
called OLTP compression between the
thing to note on this we've made a
terabyte problem a third of a terabyte
problem by getting about a three to one
compression ratio but in fact what we
are doing in this load is a load because
we're running on a Database Machine
we're taking advantage of the hybrid
column of compression techniques and
we're in fact getting it close to a
twelve to one compression rate so we've
now made our terabyte problem a 64
gigabyte problem now this is quite
impressive particularly in terms of now
we have the ability to scan it extremely
quickly but for DBAs out there this
means a lot less data to back up an
archive so let's look at some of the
other data loading challenges we have
moving the data to the actual Database
Machine becomes one of the biggest
problems getting it off the legacy
machine and moving across either a
network or via devices
becomes the problem so learning to
compress the data becomes one of the key
issues and in fact in this demo we're
actually loading from gzip compressed
files so just to give you an idea if we
were to actually move a terabyte in one
hour
we'd have to be moving at things at just
shy of 300 megabytes a second and this
is of a higher bandwidth than most
networks by using compression we have on
this case via gzip compression a seven
point seven compression rate and we've
actually reduced the required rate if we
were to load within one hour to 40
megabytes a second now this is an
interesting number when put in context
of looking at the hardware opportunities
we have later the other thing to note
really is this is quite impressive and
we should all think is how long would it
take to get this out of your legacy
system if you were extracting say from
other database systems how long would it
take you to extract a terabyte out of
those systems so moving back on to
numbers let's think about that number of
40 megabytes per second well you can see
a USP dry
we'll be capable of doing about half
that rate if you move things on local
discs unplug them and reattach them you
may get close to that rate the important
thing to is then to think a little bit
further maybe you start staging data on
NFS servers and filers now this is
potentially infinite but it moves the
problem down to the network and in many
cases we find that we have to teach
database people how to be Network people
because how you actually hook up that
filer to the network to copy to your
Database Machine becomes a major
challenge and you find that database
people make the mistake of getting the
B's wrong when talking to network people
network people like to talk about
megabits database people like to talk
about megabytes remember there's a
factor of eight in there again you can
see that you have to do a little bit of
homework and work with your network
people to figure out how quickly can I
move data across the data center by the
network the last solution would be to
stage all the data on dbfs which if you
will say running an OLTP system on your
database machine and you were unloading
data into flat files for migration into
a data warehouse would be the fastest
regional together let's go back and
having a check in again and see how that
load is doing you can see from
Enterprise Manager is just coming to an
end and the last part of the fourth week
is finished to be loaded so that part is
complete let's go and verify and see how
long it really took on the other screen
as you can see the data is loaded we've
loaded and extended the size of the
database to 122 gigabytes and you can
see the actual load took just shy of 10
minutes to load a terabyte of data let's
talk about loading you will have noticed
when we were running it was all CPU
bound data loading is in fact CPU bound
a lot of people expect it to be IO based
but in fact if you think about data
loading what we are doing is parsing
input files which is a CPU bound process
and formatting database blocks and
writing them back out again and we in
fact spend more time formatting blocks
and parsing input data than we are do
spent writing so with it it's a case of
up
reading also your techniques and loading
because you may have noticed that we
were loading across 96 cause loading
across a lot of cause and into tables
that are heavily partitioned suddenly
the tools that we may have used in the
past such as sequel loader are not the
appropriate tools and in fact the tools
to use our external tables with just
simple either create table as select or
insert a select statements pulling the
data from an external table and placing
it into the database the reasons for
this are not multiple one it's been more
efficient with parallelism and the
second one is it negates the need for
huge amounts of memory for storing
metadata and it's just basically it's an
upgrade associated with having us moving
things from ones or twos of loaders
where parallelism is effectively in
single digits to enable us go up to
triple digits when loading data the
other thing you should also note about
your choices of columns have a huge
impact on the CPU load required to load
data the raw column of course is the
cheapest and the timestamp is the most
expensive however it's still important
to remember that you choose the
appropriate data type associated with
your column it's not worth compromising
the data type just to speed the loads
although some teams in some IT shops may
where the loading team and the query
teams are not the same team may do
things to make things difficult one of
the things we like to do after loading
the data is just count the data and see
that it's there so the first thing we're
going to do is just count the data now
it's important here to look at the top
right hand screen because what we're
going to do is basically scan the data
and what you will notice is considerable
amount of i/o from the disks in the top
left of the screen but at the same time
we will be populating the Exadata flash
cache so you usually write to memory on
the right hand side at the top of the
monitor screen here we go you can see
the 14 gig Aveo at least and you'll see
the right activity on the right hand
side and very quickly you'll notice that
we have scanned all the data in
approximately 13 seconds
okay the system is now gone quiet again
what we're going to do is scan it again
now the thing to note when we scan it
again you would expect all the i/o to
come from flash memory well let's have a
look
you can see a lot of reads from flash
but you're also noticing a certain
amount of reads from the discs you'll
also to notice to scan that data took
under half that time it took five
seconds
many people ask us why are we reading
from disk and it's very simple is we're
able to detect when the reads from flash
memory start to slow down were able to
take advantage of using the discs for
additional iOS this is a very unique
capability to Exadata</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>