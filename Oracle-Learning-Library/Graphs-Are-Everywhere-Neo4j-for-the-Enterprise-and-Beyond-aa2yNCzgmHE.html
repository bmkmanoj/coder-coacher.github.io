<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Graphs Are Everywhere: Neo4j for the Enterprise and Beyond | Coder Coacher - Coaching Coders</title><meta content="Graphs Are Everywhere: Neo4j for the Enterprise and Beyond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Graphs Are Everywhere: Neo4j for the Enterprise and Beyond</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aa2yNCzgmHE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright welcome everyone to this little
lunch session about graphs about graph
databases so I thought we'd actually
kick it off by checking the temperature
in the room a little bit how many here
have heard of no sequel hands up how
many here have heard of sequel ends up
I'm gonna hear have have put a no sequel
database in production how many here
have put a graph database in production
probably five six people something like
this alright sweet um so we have a tight
agenda today I'm going to start by
zooming out a little bit and talking
about the no sequel space after which
i'm going to dive into the topic of the
hour which is graph databases and talk
about the fundamentals i'm going to talk
about one specific graph database called
neo4j which is the one produced my mind
company and then we're going to dig into
some real world examples where neo4j and
graph databases are used today before
that though my name is MLA forum and I
have only one ground rule really for for
all of my talks and that is that i do
not want your undivided attention you're
all online well some of your online i'm
not sure how great the Wi-Fi works it
always seems to be a problem at javaone
but most of you are online I hope so I
do tweet about this the only thing that
I ask you about is that you use the
neo4j hashtag I monitor that one
religiously give me feedback let me know
if it's interesting or not interesting
if I'm doing good or bad also let's keep
it interactive feel free to just
interrupt me if you have any questions
if they require a more thoughtful
response I might push it to the end but
let's keep this as interactive as
possible so graph databases are being
lumped into this whole no sequel
category which is this big phenomena I
think ninety-five percent were you
raised raised a hand when you when I
asked if you'd heard of no sequel it's
definitely one of the buzzwords that one
can get a little bit tired of and
however I'm going to start by orienting
you in no sequel and let you know where
feel graph databases fit in this broader
scheme of things but before we do that
let's talk about the name so let's just
get the fundamentals down we all hate
the name the name is horrible the name
talks about something by defining what
it's not no sequel right and a lot of
people thinks that it means no to sequel
or never sequel and that it is a
replacement strategy for relational
databases that was never the intent it
most specifically it was never the
intent when you speak at oracle
openworld and it's of course never the
intent then we were all super good
friends with relational databases
because the whole point actually is that
no sequel means not only sequel and it
is the observation that the era of the
one-size-fits-all database is over we're
now at a point where for most
interesting projects where we can no
longer just assume that we're going to
take all of our data and just lump it
into one relational database but we're
going to look at the polyglot
persistence world where we're going to
look at our big data sets because old
data sets will be big and we're going to
see that this this this data were here
is very messy in complex let's put that
in a graph database this data we're here
is very key value oriented let's put
that in the key value database this data
over here is very tabular let's put that
in a relational database polyglot
persistence the back ends of the future
will consist of not only sequel
databases but graph databases and key
value stores so then why did no sequel
happened now this seems like a good idea
let's use the right tool for the job I
mean that's sort of what we're that's
our responsibility as engineers but for
some reason for 40 years like like
irrespective of the problem the right
answer has always been the relational
database and that's sort of how we as an
industry have have acted up until two
thousand nine when this term no sequel
was coined and now a couple of things
why that happened just now one of them
is what I think you've probably seen in
many presentations people talk about
this exponential explosion of data you
know big data is even more hype than
note no sequel I think Eric Schmidt went
on record by saying that every two days
we create as much information as we did
up to two thousand three I don't know if
that's true but
what is true is that there's an
explosion of data that most of us have
to cope with and handle so that's one
driver for why all of a sudden we're
looking at alternative databases another
driver is a little bit more subtle I
think and that's an increase in
connected data it's obvious in the
social domain social data people even
talk about it that's the social graph
but it's actually apparent in a lot of
other situations as well the third
reason is semi-structured data hands up
who here has heard of semi-structured
data yeah so most of you it's basically
data where you have a few mandatory
attributes and many optional attributes
and that's a challenge to model in a
relational database it can lead and it
often leads to so-called sparse tables
which are basically tables where you
have a lot of gaps in the columns of
many rows and the fourth one is a little
bit more subtle again it's a trend in
architecture I don't want to use the the
amazing so ah word here but we are as an
industry moving more from a word of a
monolithic Arctic architecture where the
you know there's a three tire
architecture where you always have a
presentation layer and a business layer
and a relational database and that's it
then every system looks like this two
more services enabled architecture right
where you say that hey this service over
here handles accounts let's wrap that in
in a domain oriented layer that speaks
maybe restful service over the wire or
something like this and if you
encapsulate your architecture in these
kinds of services all of a sudden you
can swap out the the persistence back
end whereas when I got started as a
programmer in the in the early 90s if I
had to change my relational to change my
database which was a relational database
always that cascaded out across you know
hundreds of applications at my at my
first job because all of these
applications were tied to the one
relational database of course happily
supplied by by Larry so that's another
reason why we can look at alternative
databases today so no sequel has sort of
this curse of being very hyped so every
project wants to call themselves no
sequel so that they can join the no
sequel
at conferences and whatnot and there
seems to be a new nose vehicle project
still announced every week but if you
squint a little bit you can see that
there are four broad categories of no
sequel I'm going to really quickly take
you through all four of them before we
zoom in on graph databases this is
usually a one or two hour talk that I do
so I'm going to go super super quick if
you have any questions though feel free
to hit me up either now or afterwards
even specifics about the other models
not just graph databases so the first
category is key value key value
databases and they were all inspired by
Amazon who published a white paper are
actually an academic article in the
mid-2000s called dynamo and they said
hey guys you know we've been able to
take our system and push it to lengths
beyond what most people have done and we
have done this not by throwing away the
relational database but by building a
new system that we argument the
relational database with and we call it
dynamo and it's basically a big
distributed hash table imagine you know
java.util.map but like on on a system
where you can add things over the wire a
big distributed hash table examples over
here is read this react project
Voldemort famous key value stores have a
bunch of bunch of strengths in
particular a very simple model which
makes it super simple to scale it out
there's no connectivity you have a key
and a value pair that's it nothing more
and if you have a very simple model like
that you can't take a complex data set
and put it in there but on the other
hand what you can do is that you can say
that hey this key value pair should
reside on this server over here and this
key value pair on this server over here
and it's very simple as as an
infrastructure provider as a vendor of a
key value store to build a horizontally
scalable solution the drawback however
is what I just mentioned the model is
incredibly simple I you know what I
typically the allegory then I'm ache is
that what if in Java the only data
structure you had available to yourself
was a hash table you know for in I know
there's no end you know for get
something from there you know from the
hash table and then you keep working
like that there would be no no data
structure at all other than this would
be very awkward to program and that's
what work
with the key value stories like it's
it's very painful but it has great
scalability so that's the first category
the second carry column families and
they were all inspired by a system
called big table and so google also
published an academic paper saying hey
guys we've also been able to push our
systems to the some scale and we did
this by not throwing away the relational
database there in an infamously huge
user of my sequel Google is but by
building a new system that takes over
some parts of the relational databases
previous responsibilities and they call
it big table and the data model here is
creatively that of a big table where you
have every row can at least in theory
have its own individual schema famous
implementations here kiss and I would
say is the most popular one age based
out of the Apache Hadoop project is
definitely rising in popularity and in
terms of strengths and weaknesses one of
the big things about this one is that it
can score the store this semi structured
information ie if you have one row which
has where you would want as a programmer
to add maybe 15 columns to it 15
attributes to it but most other rows
have only three you can easily model
that in a column family however there
are it's very challenging to do
secondary indexes you can really only at
least easily look it up by primary key
and it's very very challenging to do
connect the data and if you write object
oriented database object oriented
systems you know that a lot of data is
actually very very connected the third
category is by far the most popular one
in the most well-known in the no sequel
space here's where MongoDB lives I
wouldn't wage I'm not going to do a hand
racing in but I would wage that probably
ninety percent of you who said that you
put it no no sequel database in
production is using Mongo so this is by
far the most popular one and its
heritage is actually believe it or not
but lotus notes and while lotus notes
i'm going to show how many of you have
been had the pleasure of using lotus
notes as an email client it's it's
probably not the best email system in
the world
but it actually at a very interesting
architecture and it had a very very
fascinating data back in that Damian
cats the founder of CouchDB he worked on
that but he said that hey let's take
this thing and let's upgrade it to the
to the modern world which means that's
put a restful api on top of it let's add
MapReduce type queries written in
JavaScript and of course let's write it
in Erlang because that's language
service that was cool back then of
course now or like it's not cool at all
now Scala is the cool language or is it
go I don't know so I said that's that's
what he did and that inspired this whole
a thunderstorm i would say towards
document databases and the data model
here is that of simple documents and
what they call documents is basically
key value pairs a bunch of key value
pairs lumped together like a JSON
document more like a hashmap in in in
Java famous implementations CouchDB
MongoDB couchbase which is a fork ish of
CouchDB ask me more about that if you're
if you're interested but it's for kish
of of couch strengths it's a super
simple model if you understand jason
which you know I man I mean Java
programmer bye bye bye heart than by by
training and so I almost understand
Jason then but most programmers out
there do very natively understand Jason
then it's super simple just take a JSON
document you dump it into your database
and you're done it's very very very
simple it's also easy to get it to scale
they challenging to model connectivity
which is a good thing if you want to
build horizontal scalability however it
is challenging to model connectivity and
it turns out that a lot of data sets are
connected so in manga what you
frequently see is a bunch of key value
pairs where the value is an ID of
another document and of course that's
not a first class citizen document
database which means that for example if
you remove that document you're going to
have a bunch of ideas just pointing to
nothing else to Devin all right and
there's no tangible way to say there's a
type between you know this document in
this document they're connected via do
they know each other or do they own each
other or
whatever it is also challenged to model
connectivity and then we have graph
databases and I'm not going to go
through this at this such a high level i
will pay some Hommage to my to my lovely
competitors and friends in the in the
graph database space neo4j is the one
that I represent here today but there
are plenty others one called orient DB
which is Apache licensed out of Italy
infinite graph is probably the second
most well-known after neo4j I mean it's
based here in Sunnyvale allegro graph
comes from the Semantic Web RDF
community these are all fine graph
databases and i truly urge you to check
check all graph databases are well made
it all but but check several graph
databases out however for the remainder
of this talk i will be focusing on the
one that I know really well which is
which is neo4j so before we go on to
talk specifically about graph databases
here's one final slide that sort of
summarizes no sequel space because it
turns out that when you look at the no
sequel space a lot of people think a lot
about scaling to size but there are
actually two axes of scalability one is
scaling to size how does your system
cope with more and more data that looks
sort of similar and the second one is
scaling to complexity how does your
system cope with more and more data that
is increasingly semi-structured and
increasingly connected and you can see
how these four categories are
distributed at a very high level you
have the key value stores that are
awesome at scaling to size we talked
about the ease of horizontal scalability
when you have very detached small key
value pairs you have the column family
stores better at handling connectivity
because they can deal with the semi
structured information we have the
document databases even better at
dealing with complex stuff because they
can store nested documents in and of
every document and then graph databases
take it to its most extreme and is is
amazingly awesome at dealing and
modeling complex connected
semi-structured and hierarchical data
however it is the category that is the
most hard that it's the hardest i guess
to get to scale to size because
fundamentally the enemy of horizontal
scalability is kind of connections and
graph databases from fundamentally
embrace connections the
say that sure we can take a data model
which doesn't allow you to represent
connectivity but the world is connected
so why not embrace that and have a model
that that can represent that really
really well and then let us as the
vendor as the infrastructure provider
deal with the horizontal scalability of
course my very subjective and let
remember again I'm the graph database
guy but what we see very very clearly
when you're out talking to customers is
that ninety percent of the use cases
lives substantially substantially below
the scalability the size scalability
requirements of all of these systems key
value stores are great from if you want
to scale from ten thousand machines to
eleven thousand machines but most people
actually have maybe two to three
machines or five machines that's more
the the more high-volume inaudible or to
be the more the more common deployment
scenario is you know single-digit
machines rather than single-digit
thousands of machines and that's why we
choose to be a graph database so that's
a crash course on no sequel and no
question so far which means that maybe
it was super clear or maybe wasn't clear
at all you don't dare to ask any
questions but I'm again happy to take
any questions afterwards on uh no sequel
as a as a whole all right so graph
databases what our graph databases so
graph databases also sort of have a
little bit of a an unfortunate word
because when you say graph a lot of
people think of charts you think Excel
you know x-axis and y-axis that kind of
stuff right but that's actually not what
we're talking about we're talking about
that old thing that you'll Earth at old
bearded mathematician in the mid-1700s
invented in as a discipline of
mathematics so basically you model data
using nodes typed relationships between
nodes and then key value pairs that we
attached to both these nodes and to the
relationships and we'll get more to this
later yeah an interesting point is that
if you remember linked list entries from
your data structures classes the graph
database is the most general general
purpose data structure that at least
I've been able to figure out
you can model any other data structure
as a subset of graphs in fact if you
step back into into the database world
again it turns out that document
databases are a subset of graph
databases because every document in a
document database is just a node in the
graph database because the nodes can
contain key value pairs so they're
semantically the equivalent thing but on
top of that you also have these
relationships that tie the documents or
the nodes as we call them in graph
database lab together so this is how one
of my guys typically described the
difference between a relational database
in a graph database a traditional
relational database may tell you the
average age of a run in this room that's
a great operation for a relational
database but a graph database will tell
you who's most likely to buy you a beer
and I'll dig into a little bit more into
that specific example later actually
more specifically we're talking now
about a graph model called a property
graph and that one looks like this it
has three core abstractions the first
one is nodes and there's a bonus one but
the first one is nodes the second one
are these relationships that connect the
nodes together and then we have key
value pairs that you again can attach to
both the nodes into the relationship and
the most important point I think
probably the most important part of this
entire presentation is that the
relationships are first-class citizens
what the graph database guys like me say
is that hey how data is connected is
such a fundamental aspect that you
should embrace that completely represent
as a first class citizen that means that
if you code in Java there's going to be
a relationship class or an interface
actually but you're going to be able to
get in a relationship type back a
relationship instance back and on that
you have the same kind of java dot util
dot map like interface as you have on
the nodes on the nodes you can add
properties name andreas job talking
named peter job building key value that
you can attach to the nodes you can
attach the same thing to the
relationships super powerful express
that i've spoken at javaone and then add
the years i've spoken in 2005-2006 etc
very very powerful and in fact if you
zoom out a little bit and become
you know the level of philosophical that
i actually won't be until you pour me a
bunch of beers tonight but my
philosophical view and this is actually
that knowledge is all about how you
relate things to one another the fact
that my name is mo doesn't really tell
you a whole lot but the fact that i am
swedish that i live in menlo park that i
work at neo technology that I'm married
to Madeline all of those relationships
if what gives is what gives me context
what gives me caller and that is what
makes you understand who I am and that's
why we try to embrace this whole
relationships as first class there's
also bonus one which is indexes because
it turns out that in the real world
those sort of the fluffy description
right but in the real world when you sit
down on your program with graph
databases you need to get a hook you
need to say that hey give me all of
andreas as friends or give me all the
jobs that alison has had right well you
need to start somewhere and there's an
index to be able to do a lookup where
you go from the string allison down to
this particular node and after that it's
all fundamental graph operations like
traversals so just to really drive home
the point most of you are probably
familiar with a relational database and
what you in a relational database would
be a joint table so typically a one
table with two columns I d2.id or three
columns id id and type is what
fundamentally becomes the relationships
in in a graph database I think I've
already talked about this the fact that
the graph model is a superset of the
document model so anything that can be
easily represented as in a document
database can be equally easily
represented negraph their base which is
interesting so that's on the modeling
side however performance is also not
unimportant and I want to walk you
through one particular scenario we were
contacted a while back by a large social
network that you've heard about which
which said you know there are a lot of
no sequel databases out there we've
looked at a gazillion of them we find
them very fascinating but then we found
grafted
basis and graph databases are awesome
because they allow us to model world the
way the world is truly represent the
real world in our data so we really like
them however we're big social network X
so the only thing that we really care
about this performance so so they said
hey can you show us on benchmark so we
know how fast you are and us being the
the honest Swedish geeks that we are we
said hey guys we can prove anything with
benchmarks so how about you give us a
scenario that is relevant for you and
we'll show you how fast we are in that
scenario so that's what they did so this
one scenario they came up with was let's
imagine that we have a social graph the
thousand people 53 average friends we
grabbed two people at random and we
check if they're connected not even how
but if they're connected we limit it to
depth for because otherwise you know
Kevin Bacon erroneous is related to
everyone else right so it's limited to
depth for and we do this with warm
caches so we ran this with a famous
Swedish open source relational database
which shall remain unnamed with a
thousand people and we grabbed two
people at random it was a two thousand
millisecond operation okay guys
visualize the schema here in a
relational database it would be two
tables one would be tape persons with ID
and name the other one would be friends
which links I d2.id it's a bunch of
joints in that joint table this turns
out to be what I call the sweet spot of
suck for sequel it's an it's an
arbitrary path length query you don't
know ahead of time if you grab one
person over here and another person over
here you don't know ahead of time how
far away they are so you're going to
have to represent that in a really
really awkward I think it ends up being
32 joints of super awkward is four pages
of sequel code it was a horrible
horrible thing in the 0 for j this was a
two millisecond operation so we were
pretty happy about that a thousand times
faster sounds about right but it's also
what we're supposed to be good at right
this is connected data this is exactly
our sweet spot
not sweet butter sock it's sweet spot of
success right so we upped the ante a
little bit we added 1 million persons
this is running on my actually my
previous laptop the one laptop that
worked on projectors which my current
laptop doesn't do which is why you don't
get fancy animations here but just a PDF
view and which is with a three hundred
megabyte keep so you know JVM three
hundred megabyte hate megabyte II pretty
small added a million persons to it 50
average connections so 25 million total
connections grab two people at random
and we're still at two millisecond for
this one so when it comes to scalability
and performance on ever increasing
complexity of connected data graph
databases are pretty damn awesome so
that's sort of the runtime
characteristics how then would you work
with this from a programmatic
perspective well this is one example we
have the same social graph that I that I
listed before and what we want to do
here is we want to check the friends of
friends of this guy over here called
andreas friends of friends not direct
friends but two hops out and the first
thing we do is that we declare a
variable called n because I guess we're
poor at naming variables it should be
called Andrea's probably but anyways if
we do that by using an index look up and
then the second thing is really
important the second thing is what what
we do then is we declare a small pattern
that we want neo4j to find in the graph
and this pattern is of course
represented as ASCII art obviously for
those of you are old hackers you'll
recognize that this is obviously the
ASCII art of a graph the first one says
that it's a node which is why you'll see
the parentheses and then you see the
relationships indicated by an arrow to
an unbound variable another node and
then another relationship to a variable
that we called both friend or friend
right and that's the one that we return
so if you look at them dreis over there
you can see that
first variable will be bound to to be
some maybe this is too small for you but
the the closest friends and then the
second variable are bound to the ones
that are two hops away so that's a
simple way of expressing you know give
me my friends two hops away this query
language is called cipher and cipher is
the key way of how you want to interact
with the graph database or at least with
neo4j and then it's a pattern matching
query language so it's similar in
philosophy to sequel but it embraces
patterns you describe this mold parents
you draw them with ascii art and then
you tell me for jay to go find them in
your big graph right so a couple other
examples let's get the node with id0
super simple you know just to start get
the node and then return previously i
added a match in there and the three
fundamental operations or start match
and then return so if we want to
traverse from node 1 and 1 step out we
would do something like start from a and
then do a match which is super simple
from A to B hopefully you can see the
ascii art here and then return b or
friends of friends that's the one that
we just did start from a go to someone
that we're not going to return so we
don't need to have a variable there and
then another hop out pretty simple and
if you do this actually so the I talked
before about the sweet spot of sock for
sequel which is the arbitrary path
length queries when you don't know ahead
of time you just represent that by
adding a star here give me and you know
and you know give me the entire path
between a and B and done rather than 30
to know what was it f four pages of
sequel code so it's optimized with that
kind of scenario so neo4j in the real
world then water you actually use it for
but before that some background on neo4j
it actually got started way back in 2001
actually we wrote the first prototype in
two thousand so we've been at this for a
while we released the 10 version in 2010
took us 10 year to get to
no which is because we're a database so
we obsess about robustness that's the
one thing that we obsess the most about
many ways we prototyped it way back in
in the days in the early 2000s when we
worked at the media asset management
company which is like enterprise content
management it could released as an open
source project in 07 we got some seed
funding for the company backing the open
source project called neo technology in
09 we were only an embedded Java library
before a jar file basically a simple jar
five less than 500 k up until 2010 and
that's when we released near fridge a
server which is too old near forge a
sort of what solar is to leucine so puts
it out on the on the wire so that's sort
of the quick background and sonja is a
graph database and i think that part is
it's clear at this point i hope in the
presentation but what does it actually
mean so graph the graph potter graph
database means that it's a property
graph and we talked about this it's
awesome for complex highly connected
data but it's also a database and not
you know i would argue that there are
some of my lovely friends brothers and
sisters in the no sequel space who i
would challenge that they are a database
in my mind if you store data in
something and it can lose it after
you've stored it you can be many things
but you're not a database and for neo4j
we think that's that's absolutely you
know that must never happened that's a
showstopper so we're we have actually
acid transactions which as some of you
may know is pretty controversial in no
sequel land because in no sequel and
you're supposed to be base which is this
other acronym basically available soft
state eventually consistent it's a great
acronym and it basically means that you
want to relax consistency in order to
get to massive scalability which I by
one hundred percent but my counter to
that though is that you want to buy you
want to build an eventually consistent
system on the foundation of a very
consistent system if you build a
database you want to build a highly
transactional core that you then can
choose to relax once you get into hey I
don't have three machines anymore
I have 10 machines or 20 or 50 or 100
machines then it's always easier to
relax consistency rather than start with
with relaxed consistency and then after
four years you realize that hey the
mainstream use cases actually doesn't
require eventually consistent and then
you try to bolt on consistency strong
consistency so that's why we've chosen
to be completely as a transaction I'm
haven't as the transaction ality I'm a
java one so i can speak my native java
language we're at JT a compliant
transaction manager which means you can
you know use at transaction in spring
for example and it's just going to work
out of the box we can participate in
two-phase transactions we have really
big customers using oracle RAC on one
side and neo4j on another side put all
this simple data over here put all the
highly complex data cisco has a massive
master data management system up and
running using this in production and
they use two phase commit between these
two systems we think that's great we
don't think that's two phase commit is
usable in every scenario but that's
always the case in engineering so that's
an important point we scale up pretty
well you know 334 billion nodes and
relationships and 64 billion properties
we scale out using a master-slave
architecture which means that we have
replication so all the data you write to
one machine is going to get replicated
out to all the other machines we do not
yet an entity rim
synchrony the question is do we support
a synchronization between remote
locations well if you put up in here for
J instance here and one instance over
here and you ask them to join the same
cluster they're going to be synchronized
but if if you mean is like like from my
cell phone to to the deserts over data
centers we do support that but there's
not in this release there's no data
center awareness what you ultimately
want this saying that hey I run a
cluster which is globally across
multiple data centers you want to say
that these three instances that are
local to you replicate to them very
frequently and these three instances
which is which are in another data
center are not local to you so replicate
less frequently and we don't support
that yet that is coming in an upcoming
release though yeah good question so the
question is do you need less sharding if
you support so many nodes for instance
and shorting as probably most of you
know is when you partition your data and
put parts your data on one machine in
parts of it on another machine i would
say for size it's very infrequent that
you need to shard for size this is more
than a lot of people have but the
drawback of the master-slave replication
architecture is that you're right
throughput of the entire cluster is
limited by the right throughput of the
master because every right needs to be
propagated to everyone and it needs to
be in particular acknowledged by the
master and for that reason you may want
to short good question yeah go ahead
yeah yeah yeah yes if you would push it
to 26 dreams so let me put it this way
I've never seen a real world example
where people run into both the node
limit and the relationship limit at the
same time ya know exactly it's it's
usually weigh less nodes because the
notes are typically connected to other
nodes and you know this K like that but
that's miss 22 observation any other
questions on this
I can you rephrase that question I
didn't fully understand it
yeah so the question is how do you if if
you aggregate data and your group data
why won't you run into situation where
one note is here and another notice over
there particularly it maybe even on a
different machine that's the question
right yeah so currently we don't support
auto sharding so all machines look this
I all grow you know the graph on every
machine is identical to the graph on
every other machine but then however on
that instance and every day is super
smart about doing exactly what you said
so if data is very is very connected
very local it's very likely to be both
on disk and in memory be local so that
then those things will be will be really
fast all right um yeah so we've talked
about this there it's now a server
actually our native our heritage is
embeddable as a library on the JVM so we
have a say a very powerful and rich java
api in the embedded version and then we
also have a server version which uses a
RESTful API where you have client
bindings in various other languages and
we've talked about high availability for
for read scaling as well ok so then what
are graphs good for so the answer is any
situation where you have highly
connected data and it turns out that
there's an explosion of that today the
first one that got most people's
attention was social that's an obvious
one but recommendations is also a great
example in retail we have a lot of
applications in finance there's obvious
applications in intelligence in the
intelligence community in network
management in telecom etc we're going to
talk about three real case studies
actually notice that there are older
versions of the slides where I where we
were not allowed to talk about the
specific customer but we are now so I'm
actually going to mention the actual
customer names and I'd be happy to to
put you in touch if you if you're close
to the brink of picking neo4j into
production I want to speak with people
who are doing that or have done that for
some time
more than happy to put you in touch
we're going to talk about three okay
studies the first one is the ACL from
hell so it's around access control lists
the second one is about recommendations
and the third one is global
collaboration so the first one is a big
telco top ten telco in the world called
telenor based in the in the Nordics
which is where we're from originally
even though now we're based here and
what they wanted to do is that they had
a big system running on side base and
where they stored all of the users their
their entire subscriber database they
provided their IT at telenor the telco
provided a utility for their high and
their premium customers the premium
customers are big brands like let's
let's assume like he use a Swedish
example right let's assume IKEA is using
them IKEA is not they're not they don't
have three subscriptions with this telco
they have probably tens of thousands so
ikea then needs a little web app to be
able to manage all that and say that hey
the guys in marketing are not allowed to
make global phone calls but the sales
guys are allowed to do that you know
click click things like that and that
ended up being a super big complex
hierarchy you had subscription plans
connected to devices connected to people
people belong into groups groups belong
in departments etc and they were stuck
they had minutes response times in their
sybase system due to joins because of
connected data and so they plugged in
here for jiaying up millisecond response
time and they put that in production I
believe a year and a half ago now so
that's one example another quick example
is a big professional social network the
second biggest one after linkedin called
viedeo which is the french one they have
again this is all worse i think that
north of 50 million users today so
they're pretty huge and they want to do
basically the example that I talked
about before even though this was not
the customer for the benchmarking
scenario but let's assume that I want to
get in contact with someone I want to
figure out how can I get to that person
should I go through my friend John
my friend whatever if you want to show
that visually on their page it's a very
similar operation to what we just talked
about and their initial solution was
what everyone is doing which is they put
everything in their joint table once the
joint started being too costly to do at
runtime they basically pulled together a
small cron job which once every hour did
the joins precomputed them and put them
in a separate table so at one table
called one friends which are all my
direct friends and another table called
two friends which are all my friends or
friends and another table called three
friends and once every hour that got
regenerated no not that bad the problem
of course was that that thing just took
a minute initially and then they grew
and then it took an hour and then that
grew it into today and when we were
there it actually took I'm not sure if I
have that on here yeah two days when we
got in it actually took one week to
generate just those you know precomputed
tables right they swapped in here for
jay and can do it in real time
okay I can't comment on no no they're
not generating it they're doing it in
real time so you know if you and I
connect as friends on on video then that
millisecond you know the moment that
transaction commits and is in in the
relational database it's also going to
be in the upper J and you know if if I
immediately in a web browser if I one
millisecond after that query I'm going
to see the path through you it's
completely redone the third example is
Adobe one of Adobe's top-level if not
most strategic initiative right now is
the adobe creative cloud which is their
cloud offering of all of Adobe's
software and and part of it is a
collaboration aspect where they say that
hey we should allow our users to form ad
hoc groups and collaborate on content
and this is going eventually to be a
drop-down in every Adobe product you you
know should be able to go in from
actually don't know any other about what
other products adobe PDF readers
photoshop i guess is a good product
where you know you're working in
photoshop and you should be able to just
upload it to the Creative Cloud and form
an ad hoc group at someone so you're
going to have a lot of people a lot of
assets you're going to form a group
you're going to disband that group very
graph oriented system and it's all built
on neo4j across they actually have 9
production instances right now and they
have one data center in Japan one in
Europe one in here in North America so
it's a completely worldwide deployment
of this absolutely zero downtime
absolutely zero downtime not not a
second of downtime per year is is their
requirements which is actually led us to
the the 18 release of neo4j was released
yesterday or the day before yesterday
and it includes rolling upgrades which
was particularly a welcome surprise for
the adobe team and rolling upgrades is
if you run a cluster near for j17 you
can now bring in 18 on one slave and
then another slave and then on the
master you can roll roll
upgrades and never have any minute of
downtime no it's sinking all of them
together well why that is a synchronous
between the master and the slaves it's a
synchronous however so so our master
state replication works very similar to
my sequels which is that you write to
one master and then eventually it's
going to propagate out to the slaves
however a drawback in MySQL at least the
preferred architecture from mysql is
that you as a client will have to
remember that this fellow over there
that guy is the master because if you
write to one of the slaves you're like
all hell breaks loose when they start
shipping transaction log so it's a
really messy situation in neo4j you can
write to every one of the slaves you can
write every slave or or to the master
but what the slave will do if you write
to slave it's going to go over find the
master grab a write lock and do actually
a synchronous resynchronize right on the
master which is a strong consistency
operation like a small two-phase commit
although we don't use the ex a protocol
for that obviously because it's just
between to neo4j instances and then once
you've said once you've written the data
to the slave and to the master and the
master says that it's okay at that point
the transaction will return out and say
yeah acknowledged this was actually
written go ahead asynchronous
replication if the master work today you
would lose data which is why we
recommend people to write to the slave
because then it's also going to be at
the slave so then you have the
equivalent of N equals 2 in dynamo so
you have the day or at least in two
places okay so that was my quick crash
course and the big question then that
all of you are thinking is that holy
 how is this presentation so smooth
and awesome now the only thing I want to
do is screw this conference and just
download neo4j well
luckily enough luckily enough I have a
slide on that so what do you do if you
want to get near PJ well you have two
options and in fact I just realized that
I removed probably the most prominent
option but we'll get to that so the
first option is go to neo4j org and hit
the big download button you're going to
get the zip file or or a terrible and
you're up and running the second option
is you can try it out in the cloud where
available on the roku it's super simple
to get up and running I actually did a
presentation for a ruby group just a
couple of days ago and my usual slides
include a third option which is maven
but you know I didn't want to get into
the whole every Ruby programmer in the
world is not super thrilled about Java
so I actually removed maven because I
didn't figure it was so pertinent to
them but that actually that's that's the
the third option and if you use neo4j
embedded that's probably the preferred
one all this is obviously available on
the OPA org I can talk about that final
thing that I want to mention is that if
you think this sounds interesting if you
want to know more about graph databases
there's a conference coming up called
graph connect november five and six here
in San Francisco it's going to be an
amazing conference we have speakers from
twitter from square from adobe from
cisco etc talking about how they handle
how they work with graphs both tools as
well as use cases and and practices
there's there's managerial high-level
tracks as well as really hands-on
developer tracks I think it's going to
be an amazing conference if you want to
know more about it please chat to me
afterwards and then finally I want to
wrap up with with a with a quote from
it's a while ago now but it's from one
of the most I think clue full observers
of the no sequel space is done some of
the best writing I think about scalable
systems and it says that i'm going to
play with mia for j seems to me that
even after arguments about acid / scale
/ the cap theorem it's just more human
and agile to be graph-based and i really
love that quote so finally I want to
urge you all to go out and enjoy the
last I guess afternoon of Java
but remember one thing to be more human
and be more agile and be more
graph-based thanks yes I'm happy to take
questions here or or act after the fact
ya go how many one master it the master
is a single point of failure if it does
go down it's going to be a new master
will be reelected and what we encourage
people to do is to write to the slave so
data is at least duplicated on two
instances yes we do I can talk more to
you about that later if you want to next
year next year question in the back you
yeah are the best practices around about
the replication delay yeah other best
practices there's a bunch of people have
put it in production and almost all of
them use replication so yeah I'm not
sure if we crystallized any best
practices from that though
in my sake will you can do we actually
don't have that now but I it seems like
a good thing that we should probably add
I'm going to take the final question
then I'll take questions individually ya
go we ship with some graph database
graph algorithms but really small I mean
there's a gazillion of them right and
those are listed in the documentation
doc stopped near for Jo dorg then
obviously there's a bunch of them
floating around but there's no one
master list for that obviously Thanks
alright thanks everyone happy to take
questions afterwards</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>