<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Advanced JVM Tuning | Coder Coacher - Coaching Coders</title><meta content="Advanced JVM Tuning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Advanced JVM Tuning</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/o6qx_zvpOyI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the advanced JVM tuning talk
my name is David Keenan I am the Java
platform performance architect at Oracle
I'm here with Monica Beckwith who is our
lead performance engineer on garbage
collection and we're going to go through
advanced tuning now we have a lot of
content there's a good chance we're
gonna get through it we try to make
these slides as understandable and
standalone as possible so bear with us
as we try to go through this quickly
okay you've all seen this before alright
first isn't over to you alright so
basically what are we going to talk
about now we're going to talk about
advanced tuning strategies you know
these are these are ideas that we have
as far as new ideas for JVM tuning new
ideas for configuration 2 tuning of the
JRE things you should look out as far as
operating system tuning is concerned and
quite a bit on garbage collection and
methodologies as far as tuning for both
throughput low latency and footprint
take a message with tuning your mileage
is going to vary
not every tuning parameter is going to
help you not everything and sometimes
bad tuning can be hurt you significantly
with performance so one step one thing
at a time one flag at a time if possible
actually I cannot if possible one flag
at a time and you know make sure you
understand what you're measuring as you
go through turning the tuning process
lastly as far as our slides are as far
as our tuning parameters are concerned
hey you can go Google hotspot tuning
parameters and it has got to be close to
500 of them you know well we should have
been more careful to be very frank but
you know there's there are a smaller set
of flags that we highlight and we talk
about on a regular fashion than we have
on our VM options page try to stick to
those if they're not on our standard
pages you know not only may they not
impact performance at all
but also they may change so it you know
our development methodology has always
been hey let's try something new let's
have a flag around it you know because
that allows us to you know turn it on
for development testing turn it on for
performance testing and all those flags
are now public so just keep that in mind
you know we have a project for JDK eight
to clean up our flags and we you know we
understand it's confusing and and we are
certainly trying to fix that so let's go
right into tuning approach there are two
general methodologies of the four system
orbits two general methodologies that we
we employ for going after to performance
tuning and analysis the first of which
is a top-down analysis approach this is
the most common approach for software to
performance tuning mainly because you're
looking at your software application
down top down right so your largest your
applications largest contributors to
performance problems your bottlenecks
your algorithms that could be optimized
is really what you're doing after you
use this type of methodology when you
can change the code at the highest level
to stack your code alright so you're
basically going to start off with
monitoring your application under load
this is both OS performance monitoring
and JVM performance model mattering you
just we'll give you an idea of where
your potential problems lie you know is
it paging in the operating system am i
you know am i spinning on a lock is a
garbage collection that my problem is
once you get down the path that you get
past what you can monitor within the JVM
and the operating system you're then
looking at what's going on at the
application level and the kernel and
it's not just your application per se
it's also the idea to the other software
running on the system the operating
system and that kind of gives us an idea
of what we can do as far as oh vos and
JVM tuning both add a tuning parameter
level but also at changing your code
now I speak specifically about OS n JVM
I'm speaking how about our approach here
and you can apply these approaches to
your own to your own application spaces
but when we look at from a software
Percy it's software above the hardware
operating system JVM you have Java
Runtime virtualized environment and so
forth
it's very nice diagram showing this
notice on the left and your applications
under load during you do that while
you're testing it should be what you're
anticipating and what your test
environment a reasonable realistic load
start at the top at the OS you also give
you an idea you know that's the your
first space to look you're going to look
at the the the CPUs information that you
get from the operating system through
empty stat VM stabbed is if there's a
slew of tools to give you that
information also the IO operating system
iOS subsystem network and so forth
next is the JVM after that once you get
to pass your JVM as far as taking a look
at the information that's available to
you is you move to profiling your
application tuning and then code
modification and then it starts all over
again
second approach is a bottle of analysis
bottom-up analysis is very powerful when
you're trying to optimize for very
specific hardware platform and it's used
when you're not and also you use it when
you're not able to change the code at
the application level product
deployments that are already deployed
competitive benchmarks we might be able
to change the benchmarks this is it is a
where we find ourselves using a
bottom-up only approach you're gathering
statistics at the lowest level of the
stack both CPU statistics you know cache
misses Network statistics you know IO as
well and you're in and you're able to
modify the lowest levels of the code to
impact CPU efficiency so that's of the
JVM and you can't level you not able to
modify the highest levels of stack
you're stuck modifying the lowest levels
in stack and here's a nice graphic
showing us that approach start with
monitoring of CPU profiling your
application of hardware counters
identify your problematic problematic
instruction sequences tune modify OS JVM
code generation native libraries
basically everything under your
application now it's not one of these
approaches or the other you know I would
highly suggest are starting with a
top-down approach top-down approach will
will will yield the the largest
performance improvement initially your
largest performance improvements are
going to come from algorithmic changes
in your code so close that you know the
JVM and the operating system is we do
the best we can with a code that sanest
handed to us so you have the most powers
influencing your performance however
once you've exhausted of what you can do
in a top-down analysis eventually you're
going to you're going to be pointed to a
method where you're like wow I don't
know idea what how I could improve
performance in this method and that's
where the bottom-up approach comes into
play and the joining of those two
methods tells you hey that method that I
have no idea of how to improve the
performance actually it's a cache miss
as a cache miss because I'm accessing
this activist data structure
and that's what that's the type of
information you get from the bottom-up
approach with that information you can
make your code changes or at least try
it's not always that simple so again it
starts over again once you make your
modifications you've got to start
monitoring all over again to make sure
that actually you know influence what
you expected it to okay so that's our
general overview of approach now we're
going to go into general advanced tuning
so this is chasing the last 5% of
performance optimization general of the
areas that I'd like to cover our JVM
tuning and these are these are little
bits that you should be aware of you
know I'm not going to go down to
methodology as far as tuning advanced
going through advanced tuning
methodologies it's not this is this is
how how you you know these are the tips
of of getting that last 5% for you so
jr. random runtime configuration OS
tuning and deployment considerations as
well so starting off with JVM tuning the
first thing to consider is dynamic
compilation and and some new features we
have in tiered compilation I mention
this briefly in the talk yesterday
tier compilation is our new merger of
the of the traditional client compiler
which we can see we call the c1 and the
server compiler which we call c2 and the
interpreter we always interpret first we
would use the instrumented instrumented
code and the interpreter to make
decisions on what to compile in c1 and
c2 and in this situation with two
compilation that we interpret we we are
gathering information in the interpreter
to determine what we're going to compile
in the client compiler at c1 c1 will
generate instrumented code for us to
make decisions on what to compile in c2
so we're getting it much more much
better information as far as where our
true hotspots are because we are and we
are instrumenting and compiled code and
we're making your decisions with n c2
based on instrument at c1 code compared
in the past we would make our decisions
based on instrument interpreted code
this is not on by default and the main
reason is not on by default is that the
current risks that we have surrounded
the code cache are insufficient the code
cache is where all our compiled code
goes we knew that we needed more space
with tier compilation the problem is is
that there's much more space than we
anticipated we increase the size of the
code cache to 96 mega bytes from 64 with
your compilation and it isn't it it
simply isn't enough for very large
applications you know so we saw
situations where we are putting a lot of
pressure on the reserved code cache and
if you were out of code cache you
stopped compiling we do have you know
ways of cleaning out the code cache
kicking out code that is you know that's
old that doesn't need to be still
reserved but you can imagine the
situation where we're putting a lot of
pressure on the code cache and we've got
this new code that will clean out the
code cache and we're basically kicking
out code all that's high okay so we have
to we have to tighten that up so that's
not on by default at this point but I do
highly suggest you take a look at it
because we're a large majority of
applications actually most applications
you're going to see you're going to see
a performance benefit it's those corner
corner cases that we're concerned about
where you know in those very very large
applications I'm talking hundreds of
thousands of classes
it is its uses supported in jk7 yes
sound update for it is it's an inator it
is a native space it's not a menu and
that that would know that would continue
to be the case update for me yes
update for was I was I was a very large
performance change for us from seven we
we see most of it in and most of the
workloads actually all the workloads in
which we use tier compilation with a
properly tuned reserved code cache we
see performance patent it's just that
you know we don't have the heuristics to
in order to do that automatically yet we
will okay another thing to be aware of
is okay I'm sorry I'm using a view for
my display here that's a little
different okay I apologize
runtime tuning parameters always Preet
uh CH we allocate the heap upfront as
you start the JVM
it's an we ballot the heap but we don't
touch those those pages until you
actually need them so it's reserved
upfront and then we'll actually fetch
them from the operating system and
request them from the upper means as you
need those pages with very very large
heaps and when you're when you're rather
performance does it sensitive especially
at the beginning of the launching of
your application you don't want to take
that hit of faulting in those pages so
we we have a flag to pre touch them
upfront so all them be you will have
access to them you won't take that hit
from from from faulting the pages in
I'm sorry the question was no I'll cover
that I mentioned that twice here
unfortunately I make sure that they have
done yeah we'll go again
now look so the gist of finished
accountant always Preet uh chiz it's it
is worthwhile that if you're seeing a
performance hit and using a very large
heap it's worth it's a worthwhile flag
that to test because touching the pages
up front will avoid that overhead huge
pages or large pages is also something
you want to consider on Linux it's
here's here's the two flags that you
want to to not turn on I have another
slide that further on that has a bit
more detail okay
system dictionary the system dictionary
is a data structure in the heap that
stores unique loaded classes and it's a
hash table it's a locked hash table and
its default size is a thousand nine so
you imagine if you have more than a
thousand nine distinct classes loaded
you start to have a performance impact
from the system dictionary a large
majority of applications aren't running
into this problem however if you if
you're a if you're doing profiles and
you see something related to the system
dictionary increasing the size is
something that will be helpful to you
and I have a bit more information coming
up on that next next internal data
structure is is the string table this
again is a hash table it's the same type
of hash table that the system dictionary
is default size is also a thousand nine
intern strings are stored in the hash
table so if you are using more than a
thousand nine intern strings your
performance is going to degrade so why
is that so let's just take a look at
what a hash table is here alright so you
know that the array on the left-hand
side that's that's a vertical to the
screen is the size of the table if you
get beyond that size of that table
you start going building bucket chains
and that's as we move to the right
the critical region for access is right
there right so if your single entry if
you're accessing just a single entry in
the original array your critical regions
can be very small it's just a single
access in that vertical array that you
see there if you go beyond that you have
to walk the buckets within the same
critical region so you can imagine the
longer that that figure that critical
region is the more time you're spending
and the more of a bottleneck that is so
entry is greater than minus one your
performance is going to degrade so how
do we know and how do we know the
classes are loaded how do we don't how
many classes are loved it I mean the
rough estimate is to take a look at the
expert Mo's class and counselled
overloaded classes and that's that's a
simple command line allow you to do that
minus X repose class run your
application and then just count the
classes that give you a general idea of
what you're loading what that doesn't do
is reloads and unloads so that's that's
that that's the rough part of it
if you're unloading these classes she's
going to obviously going to reload them
again when you need it and that will
show up in for most class how do you
know how many intern strings you're that
you have loaded well that's actually
much simpler we have a flag for that
print string table statistics and that's
new and update 4 as well
I will run a large enterprise
application take a look at these two
floats at these two situations it's
worth I mean it's we've seen when we had
an artificial environment that we were
testing just to see what the performance
impact would be with a 70,000 in turn
strings we saw a 7 to 10% top-line
performance improvement just by just by
setting that in turn string table to the
correct size so that's just it's if
you're running into that problem it's a
quick fix as time goes on we will do as
time goes on in Java 8 and Java 9
because we have this flag it's not as
not a critical performance fix for us
but we would like to be adaptive and
something that you don't have to
manipulate okay code cache I mentioned
code cache default size is 64 96 running
to your competition okay I'm going to
hone that how to check the code cache
well if you apparently have a slide
versioning issue that I filled that out
hi
all right I didn't feel that after me so
basically what you're looking is
additive for for code cache I apologize
I didn't happen in there is you're
looking about you want to know what it
what is the capacity of the how many how
many classes or methods you've compiled
and the quickest thing is to look at a
print compilation and do account that
will tell you an idea if you a general
idea you want distinct classes that
you're loading they're distinct of
blocks that you're that you're compiling
but it gives you a general idea you can
also take a look at using keep dumps
take a look at the at the code cache
itself and find out where you are it's
not necessary to keep dump but it does
dump the code cache if you do if you do
run out of the code cache compilation
does stop it says compilation failed in
print compilation output and it's not
compiling anymore
that does not head down so that's
priests update update two of seven
before update two is seven it would
simply just stop now it starts kicking
things out makes it more difficult
frankly so it's I'd rather have it
stopped but it's not you know that's why
tiered compilation is not on by default
okay permanent generation says okay
there are two there are different 32-bit
and 64-bit defaults the 32-bit default
is eight megabytes to 64-bit default is
32 bytes so that's the starting size
so how do you know if you're running out
of perm space the full GC events and
print GC details have used the print GCD
50 tails flag and if you take a look at
the add a full GC event it shows you
what the size of your perm gen is
because the perm gen was only collected
at full juicing so that's well that's
where you're going to see the output and
you can set the initial perm Jen size
with perm size and the maximum of
maximum size just to know you've heard
us talk about this another other talks
the permanent permanent generation will
be removed in JDK 8 if you moved into
the heap
so this perm gen sizing is a sumo be a
thing of the past
okay we've talked about compressed dues
a lot so compressed dupes is essentially
using a 32-bit offset to reference into
a 64-bit heap but there's actually
several phases of compress tubes so I
want to give a little detail on what the
differences are okay if you're running
less than a four gig heap with a 6450
JVM basically you're the heap is in it
is moved to the verge low virtual
address space which we can directly
reference without encoding or decoding
so essentially it is you know we don't
have to do a shift or anything along
those lines the fact that we rate by the
line allows us to assume that we have a
zero zero base for starting and then
it's just a direct offset so there if
you're running less than 42 yeah it is a
direct opposite essentially it's the
direct reference if you're running less
than four gigabytes the performance is a
64-bit JVM is no different than 32 bits
clearly against it's it's the same if
your team size is between four gig and
26 gig that's where our full zero base
compressed UFO comes into play and
basically we know all we have to do is
just shift shift to decode the address a
shift is very fast on x86 and SPARC we
see very little performance impact
beyond 32 bit when you're up to step 26
cubics but there is a slight difference
it's not it's not this on it between 26
and 32 gigs is regular compressed tube
so so we don't have we can assume that
we have a zero base so that it's a bit
more of a decoding operation we have to
do an ad and then a shift essentially
now if you're running up to 64 gigs it's
the same of a button the same as above
same as the 32 bit 32 gig case but we
have to set this object alignment in
bytes to 16 you can see the trick that
we're doing right we're because we know
we're 8 byte aligned we're able to use
that assumption doing ought to do a jump
up basically
- don't offset into the heap - fine -
fine
the address if we set it to be 16 bytes
online we can do the same thing up to 64
gigs fragmentation tends to be a bit of
an issue you have to be aware of that as
you as you start to manipulate
manipulate alignment we have found that
up to 64 gigs the alignment impact I
mean the fragmentation impact when
setting at the 16 is not it's not
dramatic now if you wanted to say hey
let's set it to 30 to 32 and go up to
128 gigs yeah it's it's problematic at
that point that's a you'll find yourself
rather fragmented okay so I apologize I
have two sections where I'm mentoring
mentoring large pages in the sections a
bit more detailed so that's why it was a
bit confused when I get to the last
slide
okay so OS tuning we get to that now
large pages again so you mentioned is it
only on Linux now large page is actually
fully supported on Linux Windows and
Solaris Linux and Windows you need to do
OS configuration tuning and you need to
specify the flag for the JVM we don't do
it automatically so if you're running on
Linux Windows Linux and Windows the
first thing you need to do is is I have
a blog that I wrote points on time ago
that tells you what you need to change
on Windows and Linux Linux has gone back
and forth as far as what is required so
you know that what the suggestions I
have on this blog are are pretty tried
and trued for hotspot however it is the
old methodology I mean as far as the
automatic coalescing of pages it's a
different term I don't know the term but
essentially that's what they're doing in
Linux is let's let's just say it's not
fully baked so we just use it this is
the better way to the use it on linux
solaris it's been on default since
solaris 7 which is a long time ago and
what we do on solaris is that we we use
the first specified large page size when
you're running x86 degrees only one
large page with modern Intel systems is
2 megabytes you know
as far as the initial you know my let's
say medium-sized page so we always
choose two megabytes on Intel now that
you have the one gig page possibilities
and so on
you can certainly specify that using
large page size invites but running on
solaris we choose the two Meg or the or
the four mag of the Meg depending on the
platform that you're running on and if
you want to go higher you can specified
but in looking at performance Delta's
and differences between Linux windows
and Solaris that's that's a good thing
to note because you know we are trying
to take advantage of the default
function function the functional
defaults on the operating system that we
can and you know large pages has been
handled automatically on Solaris for
quite some time so that's why we've been
doing this for quite some time this is a
TLB pressure and that's what your levy
adding by using large pages is that is
the hardware TLB and pressure it can
have a significant impact on your
performance I saw a talk last night with
a folks from Salesforce and they indeed
saw a significant benefit by by using
large pages okay next for OS tuning is
manipulating the scheduler the Linux
Newton to did so that the completely
share it was a fair scheduler or I'm not
forgetting actually with a secret what
the acronym fully stands for but once
that came into Linux that was actually
made it a bit difficult for tuning it's
very it tends to be a bit more client
focused then then TAS sharing and
time-sharing focused as on the server
side one of the things that you can
manipulate are the scheduling policies
and using groups with CFS groups with
CFS is you can have a lot of success
with that so that's something that you
can certainly look at look into next out
as far as scheduler tuning a Solaris the
effects scheduling class is a scheduling
class that tries to you know give equal
time to all threads and it is just equal
time to all threads for the most part it
does manipulate the quantum as far as
what you're what you have but each
thread has the same same time available
on proc and we have found that that is
something that certainly is worth trying
for enterprise type applications ok
memory allocator so I'm just mentoring
Solaris here because we have a live you
mem that is worth trying libya ms is a
heavily put it is a it shines greatly
with highly parallel allocation and you
use the environment variables LD preload
and LD preload 64 depending on the
fitness of your application to take
advantage of that other deployment
considerations is you know multiple JVMs
with it's the classic you know if a
single JVM can't handle it maybe maybe
more than one can and the idea is that
you're you're reducing the work of any
single JVM it is very successful in
reducing overall maximum pause times
especially when you using some of our
parallel garbage collectors the pause
time that you incur where the garbage
collection is directly directly related
to the amount of live data that you have
so your if you're able to distribute the
live data across many JVMs it's just
simply going to be faster on those
smaller JVM and that's that's a I'll see
I got the five so I made a mistake in
this slide actually on the default
thread count is not five-eighths it's
5/16 now prior to seven update form was
five minutes
so I'm skipping ahead but another thing
to consider with multiple JVMs if you
are indeed running multiple JVMs is is
that those the parallel GC thread-count
we don't do we have no awareness of
other jaebeum's running on the system so
if every JVM is running 5 5/16 of the
hardware threads you can you end up
having a boatload of threads running
that you may not want to so our general
suggestion is that the total GC threads
should be less than or equal to the
number of Hardware threads that you're
at that you're running and you have the
specify that by hand using the parallel
GC thread count and there's the flag
right there
ok now into Jerry configuration just
just just a couple tip that's as far as
configuration is concerned so so
updating the crypto provider on Solaris
doing this will allow you to take
advantage of some of the hardware crypto
optimizations that we have with the
later SPARC and x86 CPUs that Solaris is
supporting so certainly if you find
crypto and you're in your in your
profile then you might want to certainly
consider this and lastly this is just
another one is if you're using temp the
default temp for Java is not typically
the correct temp for your operating
system so setting it to the actual temp
directory of your operating system might
help performance if you're happily using
that application set that do dynamic
compilation of byte codes use temp quite
a bit so if you want them that's that's
our our compilers use the temp is quite
heavily so that's where we found setting
it to the the operating systems temp
directory is an advantage again you know
as I all of these advanced tuning
parameters that I've went through are
really you know you're talking about the
last five percent of performance for the
most part so of course keep that in mind
ok that brings us to the GCTF section at
this point so I'm going to break over to
and have a monica carry on and cover GCC
from here out so here's Monica thank you
so I'm going to talk about GC tuning and
I'm going to cover three GCS Oh Lord of
course sorry
so I'm going to talk about GC tuning and
I'm I'll try not to go fast but there's
a lot of lot of lot of matter that I'm
going to cover and so let's start what's
the number one goal for GC tuning try to
collect as many short-lived objects and
the young collection itself right and
and then there are others and I'm going
to talk about this more in detail later
but I'm just going to summarize it first
right here so you size your young
generations to try to age the objects so
that you can collect it in the young
generation itself sorry then you size
the old generation to try to fit to your
live data set so what's your steady
state live object size size your old
generation to contain that so our
recommendation is that if you do have a
heap we would say anywhere from 1.5 X -
4 X right and you should be very careful
about not spilling into swap space the
swapping build kill performance what
other things you can do you can increase
the survivor space so that so so yeah
you took care of your short short-lived
objects what about your medium live
objects if you're promoting medium live
objects into old generation prematurely
guess what now you need an old
collection to take care of that right so
try to size your survivors to age those
medium lived objects as well so they
collect them in the young generation
itself so you're not promoting them as
much but when you're but there is this
word of caution when you're sizing the
survivor spaces be careful that if it's
too too big a space then some space can
go unused so if you have footprint
issues try to think about that in mind
so depending on your application goals
and the type of collector that you
select keep sizes play a lot of
have a lot of impact on that so when you
when you have your goat so so so once
you go through pain throughput latency
footprint right so and then you will
select the collector accordingly
so keep selection of the collector keep
that goal in mind and also remember the
young the old generation collections are
more expensive for some over the other
so so choose your collector wisely so
I'm talking about the latency and
throughput trade-off right and I'm think
of a parallel collector as you as your
throughput and think of g1 as your
latency right so what are the
differences nan not incremental more
incremental you know what does that mean
it means that it collects the entire
generation in one in one chunk right and
that's not incremental more incremental
you have real generalized heap so what's
the benefit or downside right so you
have long pauses when you're collecting
the entire generation in one chunk on
the other side you have more uniform
pauses because you are collecting
regionalised heaps right but then you
have minimal overhead on the application
thread and what that means that you're
getting the best throughput pretty much
right on the other hand over here you
have more overhead on the application
threads so you get best Layton sees
throughput may suffer so so there's no
one-size-fit-all GC so let's kind of
deep dive
so I'm tuning for throughput so what is
my suggestion right what will I do I
will let their economic work for me and
that's my suggestion to you that the
urban Amex work for you and I'm talking
about right now the parallel GC which is
the troop of GC in hotspot so it is by
design it's it's supposed to have low GC
overhead right so for example the GC
time ratio which is which provides a
throughput goal so let's let's go back
quickly so what are the three goals the
goals are throughput latency low latency
minimum footprint right so for parallel
GC we are it's designed to have low I
meant to have maximize throughput so
that's number one priority and and
that's evident by the switch over here
which which defaults to 99 which means
that your throughput goal is 99 right
that means 99% of the application time
and only one for GC also the use
adaptive size policy which is enabled by
default it helps maintain the throughput
goal by adaptively sizing your
generations there is something called a
target survival ratio which defaults to
50 which means that only 50% of the
survivor capacity will be collected and
GC and and it also helps with having
those spikes object spikes and the
surviving object spines so Pyke's sorry
again how is it how is it designed to
have low overhead you have something
called a chain earring threshold which
helps it I was talking about the aging
of the objects so with the help of the
stranding threshold which is we
calculated at every collection it tries
to try to maintain or reduce the copying
cost and to reduce the overall cost of
the GC so for example if you're doing
more work in the young young generation
and then in the old generation and guess
what the tailoring threshold is lowered
to more to promote more objects into the
old so you're doing less work in the
young kind of so so yes it's it's it's
designed to have low GC overhead
now what happens when the ergonomics
don't work for you so well we say tune
by hand so how do you do that you
maximize your heap space because
throughput is your goal then you have to
put foot print on the back seat right so
you maximize your hip space and we've
seen and and I have done this myself too
and we've seen that we people do keep
their initial heat and the maximum peep
to the same value how does that help
well whenever you need to grow your heap
or even shrink it for that matter you
will be doing a full collection so to
avoid the full collection is what people
do that but then I'm also so my
suggestion is this if you again if you
are through putting true but minded so
and and footprint is not the issue allow
that little little in just in case
situations allow them keep your maximum
peep at that and whatever heap you think
that your application needs set your
initial heap to that so so guess what so
so you're doing good but but then you
won't be having an out of memory
exception right yeah you will get a full
GC but you know at least you're not
having that acceptance whether it's just
in case situations which can happen my
second suggestion is to study your GC
logs that's what I do daily basis I do I
I make sure my the outputs have cringy
see details and fin GC timestamps and I
size the generations by hand if you do
need those the last five percent or two
percent or whatever you need I do that
so let's quickly look at the output here
we have so I said the print GC timestamp
so that one shows right there the
timestamp it shows a number of seconds
since Allah launched of the process so
it helps you compare different logs if
you made some tuning changes you can see
you know the timestamps right there this
area over there shows you the young
generation information so what's the
occupancy before the GC occurred
occupancy after the GC and the total
size of the young generation
this one shows the Java heap before
garbage collection Java heap after and
the total size of chopped Java heap and
then the time that it took for the GC to
happen right sorry I'll just go back
very quickly they okay so talking about
sizing generations so this um there's a
search called new ratio it helps you set
the young to old generation ratio and
smaller and value indicates that you
need more space for the young generation
so for example right here if you have a
new ratio of two that means you want two
third of the total heap for your old
generation and one third for the young
if it's three it's 3/4 1/4 something to
keep in mind so I've also seen and I've
done this myself too I'm keeping a fixed
value of nursery and higher higher fixed
value of nursery and then letting it
grow from new to from the minimum
nursery to a maximum nursery and why do
people do that it's because in most
cases young generation has the most
effect on your throughput but when
you're being too aggressive you know
with generation expansions it can be
careful try to maintain again GC
overhead directly relates to throughput
so try to maintain your GC overhead
lower than 5% in fact it later on I have
an example in the low latency case that
the overhead is also important for
latency and people it takes time to
connect those two together but it is
true so how do you find to the young
generation yourself it is a disabled to
use adaptive size policy and tune by
hand so usually a application with
steady state behave study behavior
usually don't need it anyway and one
word of caution heap will not grow from
its initial size so if you set the heap
at initial size and you disable use
adaptive size policy your heap will not
grow so now you disabled you adaptive
size policy what do you do next
look at the output of adaptive size
policy too
the behavior and then you find to unit
by yourself right
so what does it show what is this is an
outfit of adaptive size policy right
here with panel GC and what does it show
it shows you the survived how many bites
survived how many bites got promoted and
the overflow did any of anything
overflow that means it got over flow the
survivors and have to be promoted now
because your survivor spaces were not
sized properly so in this case it's
false which means nothing over
overflowed but then if the switch is
true that means you have a problem or
you know you need to tune in basically
you probably don't have a problem yet
but you need to tune it second another
suggestion is to look at your
engineering distribution to see how the
JVM does its you know calculation of the
ages and then fine tune it by yourself
so there's an example of a French onion
distribution for parallel GC it tells
you the desired survivor size it tells
you the new threshold which is the age
threshold and what's the Meher maximum
threshold so and of course the same
French young GC outputs that we've seen
before okay so when you look at the
output of continuing distribution what
can you do
you can initialize your initial failure
threshold or your max and you can change
the value of your max in your interest
you can lower or increase it based on
the applications distribution now you
know that in steady-state my aging is X
and then I can I can I can adjust these
two so that I don't start at a low
initial value or I don't need to go to
up to the maximum so you can you can you
can tune that yourself and you can also
tune based on the other the last two
facts that I mentioned you can also tune
survivor ratio as such so what what is
survivor ratio so if survivor ratio
determines the size of survivor spaces
and how is that calculated the survivor
size is equal to is equal to your
nursery divided by the survivor ratio
plus 2 y 2 because you have two survivor
spaces the from and the two
and and when and why and why do you why
would you use this
so remember any overflow it gets
promoted to the old generation which can
overload the old generation right and
large survivor spaces are needed for
transient data in so it's not your live
data set that's that's going to remain
with you it's just transient it's medium
lived objects do you want to let just
age them they're each age them in the
survivors filter them out don't promote
them right that's the goal so I'm going
to talk about g1g see as a throughput
collector yes so I know G 1 G C is known
to many as a low latency collector but
yeah so let's see how we do that first
my first and I'm going to repeat this
again most important thing understand
the defaults g1 does have these defaults
and they are there for a purpose try to
understand them so there's the maximum
and minimum nursery default which is 20%
and 80% so you're basically nursery will
go from 20% of your heap to 80% of your
heap and there's a maximum pass I'm
target which defaults to 200
milliseconds so note that you know when
you're doing this tuning and you're
setting on XML I know you did this for
through purchased about two slides ago
right but doing that for xmn for g1 has
implications
first of all everybody should know that
when the Java heap expands your you are
you have just fixed your nursery on the
command lines your nursery is not
expanding your heap is expanding in
nursery still so most like I said most X
most of the applications you Nursery
plays an important role for throughput
right another thing is that just we
talking about defaults g1 is new to the
throughput game we all know it so for
example okay right here the GC time
ratio that was
earlier which was 99% of application
time for parallel parallel GC guess what
it is here
it's 90% so so yeah we don't care about
it we don't care nine for that nine
percent right it's funny but it's true
that's what that's what the default is
so what do you do after you understand
the default you try to manage the GC
overhead how do you manage the GC
overhead by expanding the heap but but
again too aggressive and expansion will
hurt throughput so so be careful about
that
another thing with g1 is that so again
remember those priorities again g1 is
designed with low latency in mind so low
latency comes first
throughput footprint right so don't set
really aggressive possum targets if
you're if you have throughput in mind
then let your pastime relax a bit
but why is that because when you set an
aggressive pass on target it indicates
that you're willing to have more GC
overhead more GC overhead means that
you're okay with lower throughput okay
managing the GC correct again try to
tame your mix GCS I have some examples
in the low latency sections about this
so these are the flags that I use
there's there's more but these three are
really really important for everybody to
know
the first one is initiating heap
occupancy percent what does that do it
help it it starts a marking cycle when
the heap the total heat not just the old
generation the total heap reaches that
threshold which it's the default is 45%
the second one g1 mix GC count target
what is that it's the numb it's a
maximum number of mix GCS that would be
performed after a marking cycle the
default here is four and what's a third
flag the g1 old
that region live threshold percent it's
a mouthful so what does that do it so
July the threshold percent so there's a
live pressure percent of acting default
of 90 which so that many regions which
have 90 percent of live data will be
considered in a in a collection in a mix
GC collection so guess what you can
reduce it if you think those are getting
very expensive makes you see counts
makes you see times are getting more
expensive so reduce that drop that to
lower sixty five you know experiment
with it we're going to move down to
tuning for low latency and I'm going to
talk about CMS tuning because that's
that's the low latency collector right
now the people are using for hot spots
low latency collector so first thing you
need to know about tuning for low
latency is know your passes you should
know your average pause your maximum
paths time and the frequency of classes
so and monitor your GC time you know
what you know what what the young
collector is in for CMS its perineum
collector pardhu collector so you having
stopped the wall puzzles so so monitor
your GC times maybe that is the problem
right resize your young generation it's
basically similar you know partner is
similar to parallel throughput collector
right so resize your young generations
based on the passes that you see young
generation too long to crease your young
generation it may not work for all cases
young generation to frequent increase
your young generation with CMS you have
to be really careful about premature
promotions because promotions are in CMS
are very expensive because we have free
lists as compared to tea labs and the
more often we promote and then reach
claim guess what we are setting us
ourselves up for fragmentation so it
will creep in
so I've seen applications and in fact
the on Sunday was talking to somebody
and he said that they do that to the you
CMS as a safety net so so they're doing
mostly young collections and for this
just in case you don't guess what what
should I do oh and we'll just use CMS
and safety net and some people also
schedule full GCS at non critical times
CMS the important issue decrease the
likelihood of fragmentation
sadly you can never eliminate
fragmentation but you can decrease how
do you do that you can decrease
promotions into the old generation
usually large objects of various sizes
are the main cause of it
so be warned you can you can reduce the
CMS cycle duration by tuning parallel
CMS threads but you will see more
concurrent overhead but again I guess
you had low latency as your past Act
number one past time goal anyway so CMS
will automatically trying to find its
best best initiating occupancy what does
that mean that it first tries to do a
CMS cycle early enough to collect the
stats then it tries to start the cycles
as late as possible but early enough not
to run out of heap before the cycle
completes it keeps collecting stats and
adjusting when to start the stands
sometimes it can be start too late
unfortunately these are the flags that
you need to know the first one is
initiating heat initiating occupancy
fraction which is the size of the
occupancy of the old generation not the
full keep the old generation that
triggers a SEMA cycle the second one is
telling CMS not to uses or goad which I
just mentioned a slide earlier not to
use that and use what you just said on
the command line up the first one excuse
me and then the CMS initiating perm
occupancy fraction which is the
occupancy of the permian that triggers
the CMS cycle and class and loading has
to be enabled for this
now talking about g1 as G's one g1
tuning for low-latency again understand
your g1 g faults use them to your
advantage
and help and use that to minimize the
command line options
remember that XML I was talking about
earlier just by setting xmn guess what
you did you you you are no longer caring
for pass time target you telling that
this is my XML and my pass on target is
up at the door right so be careful about
that
if the defaults don't work for you then
you set your pass them targets yourself
you can also set your pass them pause
frequencies and look and look at the GC
logs and see if you're getting the most
of your past time goals I have some
examples next so in this particular
example the heap restrictions or so
basically they have a footprint trying
to contain their footprint they were
stretching the past time go so we
working with the heap space of 2.5 gigs
and g1 was not able to accommodate the
live data set so guess what people have
and we were having promotion failures
and evacuation failures and we were
having full GCS as a result and that's
how it looked on the left hand side so
what we did first I to understand the
life to calculate the lab Tina said and
we made sure that all Jen can
accommodate that and then that's how it
looked afterwards now same example after
we tuned it guess what it's still not
meeting its fast time target which was
the default of 200 milliseconds you see
the young generation also so I'll take a
step back the red is young generation
collections and greenish blue are mixed
GCS so mixed GCS were blowing the pass
time target and the young generations
were all nice and good right so then we
what we do we chain the next GCS as I
mentioned a couple of slides earlier and
this is the slide I was talking about
about the overhead so GC overhead is
above 5% and guess what it was doing it
was affecting of course it's affecting
your throughput right we know there's a
direct relationship
it was also affecting the average
response time and the 98th percentile
response
so we work with taming the mix GCS as I
mentioned earlier and we brought it down
so and it kind of helped them a lot
actually just bringing it down below 5%
getting the getting the most of your
past time goals this is the switch that
you use to set your past time goal if
you don't want the 200 milliseconds
default it provides a hint that past
time goals of n milliseconds are desire
or less are desired g1 will do its best
to meet the past time goal how does it
do that it adjusts its young generation
remember the 20% in the 80% default for
the young generation it'll try to adjust
that it'll try to size your heap so if
it's increasing the heap or whatever is
taking care of the young generation the
young duration 20% to 80% is going to
move with the size of the heap and now
there are the related parameters to
start to try to stay within the goal
don't let mix GCS stretch your goals
Lots a lot of old regions can make mix
GC pauses longer you should understand
what triggers a marketing cycle if the
defaults don't work for you you should
modify them I have about eight minutes
so this is how you start select the
start of a marking cycle remember the
initiating keep occupancy percent which
defaults to 45 so sure you can you can
manipulate that right here with that
command line option if you start your
marking cycle too early you'll have high
concurrent overhead and frequent mix GCS
will incur if it's too late
guess what you're going to do it
yourself for evacuation failure and a
full GC down the line and set it to a
higher value so that you can hold your
steady state like sighs in the old
generation we'll talk about tuning for
footprint this these are these advices
are applicable to all GCS a no your live
data set we spoke about that briefly no
your promotion rate and no your
allocation rate crane so like I said you
should
fit comfortably in your old generation
if not guess what you're going to have
expensive full GCS down the line
promotion rate why do you need to know
that so you can age you know if you know
that it's just you're promoting more and
you're promoting mediums that lived
objects as well so you can satisfies
your survivors accordingly allocation
great you should you know need to know
is it just steady or we have allocation
bursts or something like that and then
adjust your heap sizes them accordingly
how to calculate allocation rate so look
at so you have to do this with the log
you know staying
once you know your steady-state window
I'm destroying two so what do you do is
you look at your current allocation
basically whatever objects were before
the GC in the young generation and the
and the previous GC post objects you
subtract that and you in divide that by
the time the time difference and that's
the allocation rate right here promotion
promotion rate right here so what is
promotion rates whatever is getting
promoted to the Elgin so basically
you're subtracting the old gen occupancy
now to the kolchin occupancy before and
since you're the princi see details
output only gives information about
young generation and the total heap so
basically we trying to total heap is
equal to young generation and old
generation and that's what we're trying
to do we're trying to subtract bad
values just so that you know what got
promoted into the old generation and we
use time stamps to to get the frequency
so for this particular application is
actually very good right we were we were
doing three six six six seven megabytes
allocation and 7.9 six megabytes per
second we're promoting only that much
how do you know you're live data size
just look at the origin occupancy after
full GC so you see after a full GC or
the total heap and the origin occupancy
R is the same value it's one point six
cakes right there because you're young
generation is zero gets collected fully
so there could be other contributors and
GC is not always the issue and we should
note we should be aware of that
check the footprint of your entire
process think of native libraries thread
stack so many things right and then
sometimes you may have to rework your
application you know you're working with
footprint restrictions but your object
allocation and attention is is not
helping that cause so you'll have to
have to collect a heat profile and
understand your allocation and attention
and then work on reducing them use most
space efficient data structures check
your input size so I want it all and I
want throughput along with low latency I
want minimum footprint
unfortunately guess what you you have to
sacrifice one in fervor the other two
unless of course you have limitless
resources which we all do but the good
news is not that that I have seen this
that not all not three are always
important and you are the judge in and
you know what's important for your
application so maybe this recap will
help you choose so remember when you
have large your heat which is footprint
it means that you may experience fewer
GCS but based on your object lightness
it could take longer to collect so you
have passed time right there you have
the throughput consideration and your
heap which is the footprint control
consideration so we are about three
minutes left and we are the summary
slide a alright so we have a showdown
today at 4:30
Dave day will be there we have a g1 GC
performance tuning talk tomorrow at 8:30
I'll be there and Charlie hunt will be
there and then we have talk and
Wednesday at 10 a.m. are your GC logs
speaking to you Kirk will be there and
these are the aliases I want I want
everybody to note these down and if you
have any questions suggestions please
feel free to use or at least monitor
them you know you want to understand how
- Do It Yourself there are people using
these aliases there are people replying
I replied many of my colleagues reply
developers will apply on these so you
understand the process so monitor them
at least well that's all I have to say
maybe a room we get time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>