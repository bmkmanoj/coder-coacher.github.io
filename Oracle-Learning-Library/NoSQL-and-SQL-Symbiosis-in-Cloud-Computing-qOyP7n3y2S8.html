<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NoSQL and SQL Symbiosis in Cloud Computing | Coder Coacher - Coaching Coders</title><meta content="NoSQL and SQL Symbiosis in Cloud Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>NoSQL and SQL Symbiosis in Cloud Computing</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qOyP7n3y2S8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone welcome to the
presentation about SQL and NoSQL
symbiosis how we can see it in our
software my name is Pavel Kuchera and
together with unit on and spin yank we
would like to present you some use case
how we deal with kind of complex data
store on the right side of this slide
you can see is the final diagram of the
auto symbiosis and we will go through
the description of the surveys go inside
the data analysis different database
architecture and we should end up with
the final implementation and deployment
let's get started our software is the
creator of the most spread antivirus
solution in the world we have more than
160 million of users that's why we are
facing heavy load on our back-end
services and we need to think you know
in how to do them you know pretty low
respond Lincoln and Link tons of
requests the seconds we found out that
the understanding of the distribution of
the executable files around the word is
kind of important for antivirus analysis
so for example we would like to know how
many users owns the particular
executable file
also the other statistics or important
data is when the file was first seen
this is the reason why we introduced
file reputation service so how does this
service works we start the information
about the executable files we saw on our
clients around the world can anyone
guess how many executables files we saw
in last couple of months around the
world and guess okay ten millions and
other guess 10 billion the answer is in
the middle is a 1 billion yeah but we
have t-shirts for you both so the key
the key for us is the SHA hash so
through this key we can identify an
executable file in the world yeah so so
this would please would be our key in
our in our database solution
so we had been facing a couple of
challenges so as as as mentioned we need
to be able to store more than 1 billion
file information also we need to respond
100,000 request a second be very fast in
response so the user is not bored
because of lighting before the file
execution and finally be able to respond
we do I create data ten seconds all the
data should be enough for us so this was
this was kind of the requirements from
our virus lab team so it looked that we
should probably use disk storage for
persistence in large volumes the
application should run on more notes be
hosted on different location and
dislocation need to be synchronized
quickly we could not suit the optimized
solution without more understanding the
data we had be we had been expecting
something like what's described on the
on the picture and on on the x axis that
is number of file owners and the y-axis
there is number of files on bound by
specific number of users let's take two
examples for better understanding of
this picture if you create some unique
piece of the code compile it as for
example my HelloWorld dot X I you would
be the most probably only one owner of
this file so there are half of billion
such a file or represented by the left
bloop
the other example can be largely
spreaded file like Java exa it has many
owners so it belongs to the right side
of the graph it's pretty difficult to
deal with different data characteristics
in in such large volume let's try to
split them and have slightly different
approach to each of them we name the
first part loners the files which around
by less than 1,500 users we can see
4,000 executions in second so it means
four thousand requests about loans files
we are interested in this kind of us
more so we require maximum 10 seconds
delay accuracy and we know that there is
variability our billion of them the rest
of the files we called top stars many
many users have them
many users have them at least 1,500 the
request for the information of top stars
are about 10 times more than loners we
don't need so much fresh information
about the files so we found out that one
hour should be should do the work for us
and we know that top stars are 1000
times less in volume now we know our
data more
and we can split them in kind of
separate groups so we define these three
groups on the top left they are loners
we can probably stop them in no SQL
because the the row is not so long and
the number of rows can go to the
billions the rest we have some homemade
solution we call distributed merger
which is slow in in response but can
store millions of rolls with very long
long cross many columns there and
finally on the top right of the slide
there are some reduce data we don't need
all the information which are in
distributed merger and this part would
give us kind of accurate information
with quick access
so I will ask you not done and he would
talk more more about going more deep
into the database no techniques yeah
hello so as we saw our data is divided
into two worlds each was distinct
characteristics and each suggesting
different approaches different
technologies we have these loners which
are huge number talking about billions
of rows very narrow rows each row
containing just a few columns hmm for
these we must distribute real-time
statistics for all our clients around
the world we may however tolerate here a
short time short term inconsistency we
don't mind if after 10 seconds the
prevalence of a file will be a futile
yeah larger by a few few units on the
other hand we have the top stars these
are few in number we're talking about
one hundred of thousands but they
generate huge traffic we're talking
about 50,000 requests over the cluster
now this we could not handle this on a
small cluster we could not support
random random writes random raids and
such for such traffic on a small cluster
here though we tolerate delay for a
statistic we do not require immediate
update of statistics for these records
so we're dividing these parts of our
data set into different technologies
let's see the considerations behind
choosing these technologies so according
to the cap theorem a database
distributed databases that may not
guarantee all three of these
requirements at the same time you may
pick two of these and left some some of
their I other three so we had to
compound our system from three sub
components which complete each other
each fulfilling these requirements which
we need them all so our emphatic
emphasis was on availability of
real-time statistics we cannot tolerate
that our clients would not be receiving
a response because one machine has
failed or or it's overloaded or because
connection timeouts our clients in
Brazil would have to be wrote it to our
data center in America I not be routed
to to Prague or Frankfurt this this
would take some 40 hundred milliseconds
so let's see how do we pick the specific
technologies here we have examples of
existing technologies and how they stand
in this diagram we have dynamic
Cassandra coach DB which are
high-performance column base data stores
which put the emphasis on availability
on the other hand we have our DBMS
system which
persist the data in a consistent form
down there we have HBase big table which
shammy scale very well and also support
some kind of transactions and ulta misty
we hoped we could do that with its way
HBase we liked very much the elegant
programming model and the the fact that
operational pune row are atomic it has
you may inject your code in using
so-called coprocessors which are
operating on the back end where did that
data resides so you may make some kind
of transaction using a single single
trip it works very well at the start but
as we scaled up the de load on the
system we started to have machine
failures and response connection timeout
and so on so we decided to move to
Cassandra we realized that we simply
cannot we simply must tolerate the
inconsistency and put the accent on the
emphasis on availability cassandra has
it's very extensive tunable options you
may choose your replication strategy
computation strategy you may dynamically
change the ratio between consistency and
replication by choosing their
consistency level with which you read or
write also it has a distributed cache
which HBase could not give us this now
this is very very useful for us because
the clients one client is writing and
this is being distributed to them
memory of another host across the ocean
so Cassandra proved to be very stable
and this is the current current use now
let's see how how this is tender the
overall system looks like the system is
composed from three subsystems each
capturing an edge of this triangle I was
speaking about cassandra cassandra is
dealing with the loners these are the
random raids random writes immediate
response of fresh statistics now what do
we do is only huge traffic generated by
the most common files there these
generate ninety percent of the traffic
oh so yeah so these are being streamed
into our distributed merger which is a
system implemented by avast it's
specially Jays possession is sequential
reads and writes this is it's optimized
for for sequential read/write it's
distributed we may scale it up add more
nodes partitioning as the data is
streamed through this system it's been
analyzed and reduced to a scale which is
which may be handled by standard
relational database system so this
system is swallowing all huge data and
reducing it to postgresql where our
first lab team may analyze it using ad
hoc queries where we may persist it in a
consistent form so on so now we seen
within the components of our system
let's see how they cooperate together
in more detail and now they are
implemented okay so in order to provide
you with a better understanding of the
application let me delve more into the
guts of the system so I'm in the
floating minutes I'm going to talk about
about the implementation details of
therapy application so on this picture
you can see the data flow of the
application there are in basically two
main loops which is fed by by the
request coming from the clients at least
requests are consistent sister of SH á
s stems along great user IDs the front
end server also responds with references
which is the number of files spread
around the world and the outer loop
satis for a little insurance exchange
top stars data between the front-end and
the backend servers the front end server
feeds the back end server wait top star
requests and the fire epic and with
which is running demerger to distribute
the manager mentioned by own upon it
performs some analysis that computes the
top star prevalences and these top these
top star preferences are sent back to
the very front end servers in order to
update their caches on the other hand
the inner loop is responsible for the
persistence of the application the front
end server stores top stars parkland
loner runners request to the cassandra
directly and at the same time it reads
loners provinces and returns it back to
the clients
developed back end the major reads from
the Cassandra some data that is used to
during the analysis for example
calculation of off top star prevalences
the dress out of this analysis is
written to to postgresql explore data
bases from where it is read by front end
servers to during startup because
because it contains image of the cash
because something must be loaded at the
back in the beginning in order to
provide provide initial response respond
to advances for top stars so now now let
me zoom in to provide a front end server
in order to reveal its structure the
front and service endpoint is
implemented on top of Nettie now we are
experimenting also in grizzly and the
handler accepts best request encoded in
Google protocol buffers the same system
or this the same format is format is
used for encoding the responses by the
way we found these this system and it's
a framework for encoding the google talk
about is very handy because it is both
condensed and and also it is legible
across various systems the arab annular
sends sends the messages to to the
diamond which is right below the handler
and this this diamond makes a very
important decisions because it it
decides whether the incoming requests is
a request for for a top star or if it is
a request for a loner if the request
bears some data concerning
loaner a lot loners it goes directly to
Cassandra at the same time of you read
data from from Cassandra that consist of
for example prevalence of the owner
being asked if the request is a top star
the things become is becoming more
complex we assume that the prevalence of
the of the top star being under question
is already stored in the top star cash
this disturbed our cash is initialized
Roma the SQL database from the process
and it is continuing it is continuously
updated by back-end systems by back-end
mergers and at the stratum we send the
top star requests to the partitioner
host ask is to distribute to send the
top star request to the backend measures
but the partitioner doesn't doesn't send
the incoming top star request to the
back end instead it it images the
request into some kind of chunk or
pocket and it sends this Patek a packet
after some threshold is exceeded so now
let's move on to the back end side again
at the end point of the back end is
receiving the chunks I spoke about
before and these chunks are coming from
various from all from all the front end
servers but each measure is responsibly
only for 41 range of SH a so so there is
kind of a partitioning and every every
part every major has a sign at some
range of sha keys the request handler is
again implemented on top of naughty org
ously or alternate alternatively and 2ds
handler
sense or in it unpacks the chunks or the
pockets of of SH a user ID spares and
this data goes to go to the local
partitioner host task is to do to
further partition the data among among
local mergers because there are more
that can be mowed managers than one
running honor on a single instance of of
the very back end a merger is an
emergent responsible for merging or
combining incoming data of it with the
data that are locally stored and once
applicate or some data appears in a in a
merger skew the merger starts copying
data from the from the source disk to
the destination this and the file which
is being copied can be very large it it
has about hundreds of hundreds of
gigabytes I think and these and these
data are already are order already
sorted and the measure is responsible
for combining these sorted big data with
the incoming requests so the result of
this merging process is the original
original data combined or merged with
the incoming packets with the unique new
data so it's kind of streaming combined
with with it a big big persistence so
maybe I have to mention that we're under
these matching processes on the owner on
a separate drives it is very important
because we read sequentially and we
write sequentially so it is very
advantageous to read from a different
drive then to write so I think that the
the normal setup of our service is eight
independent drives Soviet so it means
that there are four for marriage for
independent measures each measure is
responsible responsible for a certain
range of the range of the of the node
and it is very it's really very fast
because we can we can copy that up in in
the full full speed of of of the derives
but it is also important to say that we
can attach so-called magic extensions or
how to say it is or bigger its side
effects these are side effects are
extensions that can be plucked onto onto
the matching process on to the copying
process and these extensions extensions
can perform some data analysis one of
such analysis is comp calculation of top
star so this video shows the schema of
calculating top star balances as a as a
side effect analysis dimension feeds
this extension with the key value pairs
where the key is sha and the value is
user ID the reducer on the picture is
responsible just for counting the user
IDs belonging to a certain sha and which
turns out to be the prevalence of the
top star represented by the sha so the
reducer emits SH a prevalence pairs that
are going to to the distributor and the
distributor is responsible for
broadcasting these these new key value
pairs these prevalences to to the front
end servers where I used for updating
the gashes also but but we do not send
only the bare very prevalent says we
attach some more information to these
key value pairs and these in front these
this information is thread from
Cassandra
also the result of the of the analyzed
analysis is written to 22 pues es to the
SQL database from where it is read by
front end servers when when they are
booting and now let's go back to the
front end to close up the loop now the
receiver is again an endpoint receiving
the top star prevalences and these
parentheses are our hearts and buy buy
the distributor from the back end and
the front and this front and handler
just receives the data from from the
major it unpacks the data the chance and
it updates the local top star cash sent
an whole cycle takes about I think one
hour so
stop star boom this the inner one in the
in the loop in a robe will deal with
these way to owners I understand
yes it it doesn't is it is that it's in
select from cash it is the this because
the top star cash to the ellipse is fed
by by by the back from a pat on the back
end so we regularly update this top star
cash from from the back end and yes it's
a memory kits in Marrakech
yeah this is a very interesting question
and it it is under under analysis now
because this is a very fresh system and
now we are experimenting with observing
the diner the dynamics of of the system
I am exclaiming the dis comic now it but
it's very difficult to say now but it
may be very quickly and also it may be
very slow it's no we were experimenting
with a loop because first we used HBase
as a no SQL storage and also we try to
write some some Hadoop jobs and butter
now if we do not use or your after use
it to use it for other projects but we
used to use it but now we do we do not
use it anymore but maybe in the future
the cash no it is just just a hashmap
because we do not need to distribute the
data because this is there is the
distributor on the back end whoo-whoo
broadcasts do that data to to the gashes
there are five hundred thousand rows 4 5
500 500 thousand entries so 500,000 top
stars just in one GM
it doesn't take too much memory it's
about I don't one gigabyte on to two
gigabytes it's only maybe bubble shoot
shoot answer this question because he is
a guy is the database guy yeah we played
with different kind of our databases and
we found out that postgres we can we
can't you okay we can we can yeah this
is good question we can also put this
the top stars data into the Cassandra
but we are also using the postgres for
doing some analytics stuff to drink the
query and and then using SQL as the
select zone on them it is good yeah but
but it cannot contain the whole row of
data that's right it's reduced its
reduced enough so we can do the analyzes
and and and select on on top of them
yeah okay so in the end of this of this
section I would like to mention some
implementation details so first of all I
would like to mention something about to
do request handlers because I I've
already said that we implement these
request handlers or not top of Nettie
org ously but we do not write it
directly or not epic or naughty naughty
or easily instead we have developed our
internal / phone call to reactor and
there is there is an obstruction of the
niño core and there are two two
implementations of this niño core one
is written in grizzly and the other is
written in a naughty so it is exchanging
exchangeable we can switch switch decor
according to our request requirements
and there is one important point
regarding this this and I owe a
synchronous processing or synchronous or
asynchronous because it allows us to
choose dynamically whether the incoming
requests
are to being processed a synchronously
or asynchronously because for example if
we read top stars from the cash it's
it's an in mum in-memory cache we can
really shoot processes we show the
process or execute the request
synchronously so it means to process it
process it in the same same thread as
the fed which receives as she receives
the events on the other side if we for
example read some data from Cassandra or
overwrite to Cassandra or to a president
we have to we have to perform the
operation are synchronously so we just
indicate in some callback method that
this request should be executed either a
synchronously or asynchronously also one
note regarding Cassandra because we use
Cassandra connection pool which is an
open source library that you can
download from from the URL mentioned on
the bottom we found this cassandra
connection pool very flexible and robust
and it also provides us with different
other policies and it's based on a
tomcat JDBC pool and last but not least
i have to mention so called co co
routines these co routines allow you to
suspend a method execution and return to
the color the color continues in its
operation and it can decide later on to
return back the execution to the to the
point where the execution in the method
was was suspended it resembles kind of
do a multithreading however running in a
single thread we use this technology in
reducers allowing us to transform the
push even system to move on and use
loops instead instead of callbacks so
it's very convenient for us because we
do not do not have to write some
callbacks because all the system i I've
been describing is
event-driven so there are events Amit
Amit being emitted from the major and
there are various extensions that are
receiving the events so in order to
write some annelid analysis procedure
this procedure should write many many
callbacks on at least at least one and
the disk or back would receive the
events and but sometimes it is necessary
to remember some context for for this
processing so in this event driven
programming it is kind of it is kind of
difficult to preserve the context but if
you try for example vivre if we write
the code kind of like a reducer in in a
MapReduce paradigm it's much more easier
and convenient to preserve the context
because you use iterators and you just
loop through these iterators and euchre
you can preserve the state on the logo
stack so these coroutines owes us to
write the ADIZ MapReduce code and it
behaves as if it were a traditional
MapReduce thing but in the background it
is an even driven even driven system so
now i would like to hand to hand over
the microphone to bubble and he is going
to to say something of about the
infrastructure so how does the
deployment look like in a main in the
reality in the end so we have two
geographically different location they
are on the other side of the globe we
are now adding the threat location so on
the picture you can see left and right
the requests from clients are
differentiated based on DNS response so
so we know where the coins are and we
are sending them to to appropriate part
of them of the world
in each location there are eight
Cassandra notes which creates the Geo
cluster then if you can see the top
stars request can be answered directly
from the notes cash so it's in a memory
cache and if it is not in the cache it's
not it's not top star and the cassandra
is asked for loners data so for doing
operations with the way the top stars
there are distributed manager merger and
postgresql but it it has the delay delay
video with the accuracy so it's the it's
the blue requests okay that's about it
thanks for listening any questions you
have
ok the question is how many key spaces
we have and I'm column families yeah we
have we have two key spaces and each has
one column family the the one key space
is just for storing data and then we can
do some dump of Cassandra and then do
analytics for that the other key space
is is actual one which is click for for
reads and writes we have one key space
which is a we we said we set up row
caching so it is actually also a
distributed cache this is the one that
gives the answers the real answers the
other one is it's like for persisting
huge data
no no transactions no no locking we're
doing what one right one Ridge and one
run per request otherwise we would have
been risking a race condition any other
question yeah
okay the question is what happened if
the data center split yeah this is the
information then after so that if the
data center are actually split then the
data are not propagated around the whole
world so it actually like two pieces
half of the globe knows only about the
files on the half of the globe but when
it gets connected again then its nature
of the cassandra is being updated you
know or we can repair the notes or
something like that and also it gets
updated the data from the from the
distributed manager so for some time
there would be split system but but it
will convert gate to to the to the
stable status later on yeah if you let
it split like for one day then you can
see that there are different top stars
on different location yeah and different
files you know and but but when it's get
connected it will it will it will be
balanced again thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>