<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JVM Performance BOF | Coder Coacher - Coaching Coders</title><meta content="JVM Performance BOF - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JVM Performance BOF</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XltfWhVExww" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to Java one this is the Java JVM
performance Bob and we have a few
analysts here that we're going to
introduce I guess I'll start off I'm
David Keenan I am the Java platform
performance architect then working with
performance for more than a decade to my
maths right but yeah anyway so this is
just an open forum for answers you know
the answer your questions and so be
looking forward to it my name is Michael
Vince Ted I'm working as jbm architects
with the hot spot and to some extent i
guess the j rocket JVM have been working
with JVMs for 12 something yourself I'm
Monica Beckwith and I'm a child of
performance engineer i work more
recently i'm working with g one g c as
well as garbage collectors in general
hey anything lists also have other I
know sessions here at javaone my name is
Gerry Driscoll I'm with Oracle I've been
with Sun for 20 years prior to oracle
i'm currently the director of java
performance next slide because this is
this is the typical disclaimer that that
you know only say things that we're
supposed to say and why we are here so
basically what they just said is the
reason why we're here is to field
questions on anything about java
performance I mean we have the panel
here who is very intelligent
knowledgeable about the JVM but we also
do performance work throughout the
platform stack and they are very
knowledgeable about not only the JVM but
core libraries and other performance
components within the platform so with
that what we would like to do is to open
up
room two questions if you would raise
your hand and ask your question we might
have you either come to the to the mic
on a frontier to ask your question so it
can be heard by the whole audience and
and that way we won't have to repeat
questions yeah it's too bad we don't
have the mics in the middle the room
like we used to have right so you know
as you ask the questions will try to
repeat them if you're comfortable coming
up please do so because you know you
frame your own questions always better
than me trying to repeat it so we were
stand right here very good ok my
perspiration will be about you know box
it types list of integers etc etc since
1.5 we sold this in a language but
having solved perform problems yet so
they're multiple social tect pointers or
specializing collections for different
types of women just what is there
anything going to that direction so
fondly enough with the dynamic languages
and with the invoked enemy 292
implementation we realized that we've
seen that in many cases the you know the
new languages and to some extent you
know obviously job as well method
handles and all these are using boxed
primitives for sure in many cases and
the work were actually you know starting
to work on now hard after so what we did
was to basically rewrite parts of the
290 to invoke dynamic implementation
over the last year or so and in the next
step now with that work done we're
moving over towards doing more
optimizations like the one you're
talking about boxing elimination there
is also it's combined with inlining to
large extent as well so we are working
on it we will improve it over the next
few months and yeah
directions like are going to eat anytime
soon taking ownership like that like
like what was what specific things you
write so short term it's mostly around
making sure that the boxing elimination
we kind of have today is more
generically applicable and long term we
also have so if you were to if you
attended the jvm language summit John
Rose had a presentation on what he calls
arrays todaro which is essentially
trying to take data representation in
Java to the next level it's not only
about making arrays larger it's around
making the representation more flexible
and providing you know better the data
locality and layout you know allowing
you to express data in new ways and as
part of that also looking at value types
so being able to you know dynamically
change I guess the the nature of data
during runtime which is I'm hoping what
to you know the question you have
basically were you aware of what we have
now for autobox you listener yeah I know
what you what would it have now but it's
limited to I believe mining so oh yeah
pattern yeah hi Wes my question was
about the dangers are right so so I for
some of you probably heard us announced
the product Sumatra very recently
Sumatra is trying to leverage GPUs and
the enable offloading of data paralyzed
workloads to the GPU as part of that
it's very natural you know we're it so
clear there's a clear overlap with the
whole data representation problem GPUs
tend to want to have data represented in
another way and obviously to enable the
bridging between Java and GPUs that's
clearly an area we need to invest in so
short term it's around make you know
improving the optimizations we have but
more long-term it's it's completely
revamping I guess the data
representation and making it more
flexible
hi so at present hot sport doesn't have
particularly good ways to see a lot of
detail about what's being done at
compilation time so it compilation time
obviously rather than jalak so other
JVMs have the ability to dump code by
default at runtime I mean with hot spot
that's not possible in a product build
only with a fast debug build so what's
what's the story about getting actually
more information out of the product
builds yes learnt be able to see more
walks in the code cache and actually the
dynamics how that works is it would be
very helpful right so we have actually
worked going on in this area and part of
it is being able to control the compiler
in a better way so if you you know if
you're trying to support the JVM for
some plus we do it helps to be able to
represent kinds of optimizations and
exactly the way that jbm did them
there's a jet for this that was
published two days ago or something
called compiler control and logging I
think I do not have the number in my
head it's on openjdk java.net / Jeff's
in the long list and and so what that
product consists of two different parts
one is you know enabling users to
control what's actually going on in
which optimizations to apply but the
other part is also to visualize and and
you know gets improve the logging I
guess of what's actually going on so I'm
curious what I've in it what type of
information specifically are you looking
for like types of optimizations well I
mean to start with just dumping the
assembly that's actually being produced
okay not currently that's like that you
need and I just plug in for all that I'm
nots I spread up to bigger issue too is
that we are sleep sometimes
need to know what's being compiled with
file and currently all the way to do it
is really win compilation it's very but
print compilation is several orders of
magnitude too little information now
that there are you know there are
several profilers that give you this
level information as far as what the
generated code is and and it doesn't
step through the decisions that we made
as far as optimizations and we add we
you know for considering such things are
you aware of those like the studio
analyzer detune things like that um yes
but there were significant problems with
using vtune against the JVM agreed so
try studio analyzer Oh in Linux and the
next oh is it elitism yeah and yes none
the next year it's been less it's
actually working quite well Alexis right
yeah it was on linux for quite some time
and i wouldn't suggest it until you know
last year or so so yeah okay cool thank
you okay but if we need to in our own
julie then we still need some sort of
accessing not greedy i mean well hurry
we understand that's it that's a
certainly it would be valuable and as
far as our we have our own internal
requirements along those lines as well
yeah okay we got one over here hey guys
i mean i have very much appreciate you
coming up but you know if you want to
yell mouth this would take time okay so
regarding this this question
particularly the last one is there any
way to actually know when the compiler
decide to not do any optimization that
will be also interesting like why it's
not in lining things or not optimizing
things because for example methods too
long or too many loops portable so i
mean in situations where we choose
specifically like the counters tripped
and we were going to compile a me chose
not to that information for the most
part i mean there's some of that
information available now in print
compilation and I mean using the deep
fast debug builds you get more
I'm going to Pat I would have asked
Michael if there's more that I'm not
aware of right i mean the the obvious
problem is what you know there's a
number of optimizations we apply in some
cases they there are reasons for them
not you know triggering on that specific
part of the cold and clearly it's it's
hard to know what it is that isn't
happening it's easier to describe what's
happening then what's not happening
clearly so it's a challenge but I think
you know it's it's not like an area
where many people are you know having
problems or are want to see improvements
I'm hoping that again the compiler
control the logging project can help you
know at least take the first step
towards simplifying that it is so so the
information that is available is that if
we were if you were eligible to compile
and we chose not to there's a certain
amount of revision it's not incredibly
detailed but it gives you some idea that
the two large code cache full things
like that and related to this as well
and do you have any tips that one can
implement in actual code and to actually
make it more jit friendly like for
example not doing switch because you
know switches are not easily optimized
or if you can lie it's better to I don't
know for example useful if you have a
very hot piece of code maybe just dump
everything to a single class because if
it's multiple class then it becomes some
megamorph recall and it's not optimized
and stuff like that I mean our general
suggestion with with this type of level
of optimization is to the to generate
you know right clean human readable easy
simple Cobras which can be you worry i
know i understand it but you're only
going to your only achieving the last
five percent of optimization at best buy
doing him to any of that along those
lines now we and you might trip over
what we're trying to optimize and your
next release so in the past absolutely i
would say that there were opportunities
in which you can manipulate your code in
a fashion that would be significant
performance improvements those
opportunities are less and it's double
that's to the point now where you may be
impeding your future performance by
doing such things so you know if you see
a situation saying that that hey if I
put it all in as one day class and you
know there's a
a morphic you know your but it's not
mega morphic and you're seeing a big
performance benefit then tell us because
most likely we can fix it rather quickly
you know in those situations so that's
general guidelines I understand you know
I live in a world where I'm optimizing
the snot out of things right so that's
different it's easy for me to say that
to you so I understand if you know that
that problem but in the general sense we
want to you know we don't want to that
you know the better the code you can
hand us the better optimize we're going
to think can I abuse for last one sure
you said before that the jsr 29 to
rewrite has been landed somewhere is
there any plan to replace the JK 71 or
back party to jdk 7 you have any
estimate for the youversion that's not
will happen so the goal is to to have it
going to seven as well yes the current
plan is to have it go into seven to 12
was taken so my question is about auto
boxing again so bring back and escape
analysis because I did some tests in
java 6 and i found that if your item
method we constructing new integer
objects all the time it was about eight
times faster than using auto boxing
because of escape nalysis which doesn't
kick him in java 7 it's about 25 times
faster and so making a new integer which
looks like it would be slower there's
actually fast and java 7 that actually
is the bath the same speed as just using
primitives for the same calculation so
um and it's one of the reasons I believe
is because of the integer cash inside
integer now is there any possibility of
getting rid of that integer cash so the
internet cash is there specifically to
aid our our previous implementation of
auto box eulogy for that code pad
specifically I mean it's one of the
hottest stimming new wins you know take
a look at your class profile you'll see
that chances are there's an awful lot of
them so that's why we specifically chose
that and our previous auto box collision
you know implementation was really a
pattern match more or less we had
several patterns that we knew we wanted
to go after and that's what we went
after and one of them is is right there
so as I think I mean I'm not going to I
won't be specific in this answer but any
as we enhance our escape analysis as we
enhance our auto box elysium those
structures that we had that we may find
to be an impedance of the new
implementation will most certainly be
removed yes so if that's it and I think
that type of situation you know why when
we're concerned about footprint and you
know I think that's a pretty ripe
opportune thought invitation to get rid
of that extra the Large Array example to
that which all the seventies about an
assertion inside the body operator which
increases the science to be on the DS
435 bytes
right
right so yeah so exactly the ovum on the
topic of limiting optimization based on
byte codes that's one of the things
we're looking into right now to revisit
and update and in many cases hopefully
remove completely these fairly
artificial limits on inlining on
different kinds of optimizations in many
cases they're not even based on live
data if you so well in the compiler you
know in so we have nodes internally
where in some cases basing the limits on
the total number of nodes we've ever had
and it's not necessarily the fact that
they're still there so I think in many
cases where we're going to relax and
improve things like that and the
hopefully a few of them the artificial
limitations you now have to work around
will no longer be there you know a
problem
you're going to chanel question where do
we send a feedback on performance like
it we see some piece of code that does
not like look at sm its link should have
been ap- much better wait so this is a
question we've gotten a few times now
and we will make steps to get an OpenJDK
performance alias it's not there now so
if you can take the step of determining
which sub sub component is your problem
and talk to that Elliot's that'd be
fantastic but we will have a departments
alias so meanwhile what's your phone
number Monica I'm sorry may look it up
quick so just a general comment which is
kind of a follow-up to that one I'm when
people are submitting micro benchmarks
and things can they please submit a URL
which has a complete link to their
entire test harness the number of times
that you don't have to deal with with
people that claim they found a
regression or a thing in the benchmark
tests on us so right a lot of those
lines that I shouldn't overstep now I'm
going to do it anyway we have micro
benchmarks suites that we we would like
to get into the open it and we are doing
the best we can to get that there as
quick as possible so we hope that that
is effective and helpful but you know
there's bringing things open source is
not a small in short process if it
involves lawyers
so i should like to move to locks yeah
go ahead so what I've noticed in the
field is that about the vast majority of
locks that you see actually used in
applications if you look at thousands of
thousands of thread dumps which
unfortunately I've had to do that you
notice that the vast majority of them
are just absolutely could be aletta
Dawei almost immediately or the vast
majority of them at least could be
biased almost immediately and without
actually doing any analysis and it
actually looks like it'd be much better
to actually bias greedy do a greedy
biasing upfront and then do an unbiased
later if if need be yeah what's what
that's that that is our gentlemen I
don't know if you've done any like auto
bottles yeah so single studies or
anything like that but well as your bias
locking is on by default but you have to
lay there is it fully apply up to me and
that's because there was no it's it is a
delay because of startup impact right
that's what we're most concerned about
there is that that during the startup
the phase of start up there are indeed a
lot of replications that we would have
to happen so we delay the effect of bias
lossing is locking until we know that
we're past that phase that we know we'd
have to set that we need to provoke
that's essentially applications
expensive we do then to do an epic basa
present revocation scheme which is
helped quite a bit however you know so
that's a heuristic adjustment that's
good feedback so i might want you know
biased locking is kind of as you get
into newer newer me cpu views as if not
that the cost of that unk intended lock
is less and less we've actually find
situations now we're bias locking
because the potential of having a
revocation vias locking is actually an
impedance to to some workloads so you
know we have to make that decision is by
as locking something we want to keep his
bias locking something we want to adjust
the heuristics and perhaps to the
feedback that you have here is something
that we want to to hear more about so
that's we've had that same observation
the largest large large majority of
ox and synchronization events are R&amp;amp;D
done contented and you know there's bias
locking and then there's extending the
scape analysis well right so enhancing
our escape analysis will help us there
as well because we are flood we have a
flow sensitive implementation now that
limits our scope greatly right going to
flow and sensitive in Java is daunting
right so we're working on that as best
as we can but yeah we have two avenues
approach I think biased I think a long
term if done ready to scape analysis has
a lot of benefit in that situation and
that's really the best way to do it just
get rid of the lock we know we don't
need to use it is it right done having a
heuristics around how we handle the lock
is is with anything any heuristic
there's a pitfall and sometimes those
for those those cliffs can be brother
ratted certificate sure did you have
more to add my not a whole lot ok we're
looking both you know at UNK intended
locking and removing those but also
contended locking so we're you know
continuously improving the the locking
implementation and it's good feedback
for sure actually the young get the
contended locking improvements they are
are they back in yungay this pointer
that's so it's work we're picking up
from Oracle labs it's slowly making
itself into the cold basin will usually
changes there yeah Dave diets the same
you know engineer it's been working on
our content locking schemes for the
longest time is still continues to do
that from the labs thank God
oh yeah the green okay so I'm gonna add
this gentleman the back at his hand up
for a while so okay so the question is
you know can we change garbage
collection collection tuning parameters
during run time within the same
collector framework is that supported
you know this better than I do well so
can you give examples all parameters
that you would like to change like an
acute sizing Franklin's the size of the
all generations
population generation in the permit
right and then time the police can see
this to be all the younger or the young
generations and things like that I don't
know all of them but I am session
tomorrow actually and I heard that there
is a year to great adapted
self-optimizing
je ka and the idea is to use some kind
of
sample data of running application on a
specific hardware and then to learn from
them and to judge the pragmatism I be
interested is it possible to check it
and during run time to the judgment
parameters or it requires the restart
right so I think so basically you know
first of all I want to say that the
permian is gone you know MJ decade you
will no longer have to specify permian
sizing other limitations they're not
gone that's right babe we've shifted the
maximum size problem is they're actually
worse because some of that into native
code so we have no tooling actually but
so I think that's both the yes or no I
think we've obviously moved to some of
the data and obviously we need to look
at this is again a good feedback you
know we we realize that things have
moved around and in some cases we need
to provide tooling to support finding
you know the new issues or you know
figuring out where memory goes as part
of that we've also done the native
memory tracking feature which is
tracking them memory on the native side
so you know some of the tooling already
exist but I'm sure we'll find more cases
more primitives or tools that you you
will want to find things but the reality
is that i mean the peerage and was in a
large majority of applications not the
ones that we're trying to tune you know
heavily but a large majority of
applications was just simply wasted
space so I mean having this separate
region where there was a there is a
fixed size where you know a large
majority of the time we just simply were
using the size and it's an environment
where we want to cram as many jvms into
a you know your cloud box in the sky
that and that's that's something that
you know we just don't want to do
anymore all right so there you go
yeah so oh sorry exactly so moving out
over to the more general case I guess I
think you know what is hard is to
completely change the garbage collector
framework for example going from CMS to
serial all of a sudden that that's so a
very big impact and you know but what
we're you know our strategy on the GC
side very specifically is to use g one
as the framework of choice going forward
and some of the tuning you're mentioning
all those you know 10 or 20 or 100 flags
that you have to specify today our goal
is to make sure that you don't have to
specify them even to start with that the
g one collector actually Tunes itself
using heuristics and obviously you know
realistically as I think I mentioned in
my earlier session today there will
always be cases where you can squeeze
out a few more percent by manually
tuning things but our goal is to make
sure that for the common case 95%
whatever even more than that you
shouldn't have to specify these
parameters right so if it's be very
specific so I'd say the answer is yes
and no so I mean Michael was talking to
do with the difference in the framework
so shifting from CMS to parallel GC no
you can't make that change shifting from
you know any of the frameworks to each
other that doesn't that's not supported
if you have your your defaults as far as
the maximum heap size and a minimum heap
size and the same thing for the young
generation yes you can manipulate those
on the fly using tools you know you know
thread counts things like that some so
it's a yes and no question but as to
Michael's point you know once we are
fruition when g1 happens you know not
only should you not have to do it but
but most likely you know you can change
what you want because be one unified
framework
ok so right I'm sorry so do you think I
put a lot of my coat or a search
statements and finally keywords both on
local variables in class to member
variables do either of those have any
impact on performance and these getting
production where a searcher turned off
no no in fact right and the only
organically well if their turn up the
only exception to that would be again
the artificial limits on the inline
sizes and things like that which right
it was short but again it's like yeah so
in some cases you were running to those
weird bastard cases where a bite of the
one more bite code will actually make a
difference we're working hard on making
sure that that's not the case so hold
your breath paralyze you know
something's find the final that yes
not really no uh you know in the normal
case final total finals yeah we can make
certain levels of assumption then we can
make but but that is you know from a
from application code level there's no
we incur keys absolutely right finally
stand finals jasso days after that
unfortunately so really we have a couple
questions on the other side of the room
Oh
to a very curious space where I came up
with a set of parameters supposed to a
very good man in memory use white pages
and use CMS better performance for
thirty percent was and check into the
final plans I notice that some flat said
did not specify the change and once I
compare those that respect those because
so so when you change those parameters
back did you see the level of
performance you were expecting yeah
let's bug all right no I think he
changed the collector is what he's
saying he chose stage so there is a
there is a so in JD case you know yeah
there is a book there's a bug that so
this is used Newman that you assess if I
did that mean you said I'm sorry it was
large pages that's on redhat linux yeah
yeah it's there's what there was a bug
in jeddah case X surrounding that so
that the workaround is to disable large
pages yeah we ran into that our go-to
what's the update Charlotte update 33 or
34 has it fixed thanks to our friends at
sales forth yes yes yes yes running snow
have a question
there are some well-known patterns that
indicate there is a performance problem
run
both change frequently as any
consideration to build into the vm any
kind of
a things are going really bad
we're just
like a self-diagnostic diagnostic yeah
yeah and and I'm not sure it needs maybe
to be in the JVM itself but it clearly
would help to have tools that could help
you find these kinds of problems right
yeah I think Kurt's got so much so so so
in many cases if you look at it what
we're trying to do in the GBM is not
necessarily solve the whole problem but
exposed suitable primitives so that
other people can take that to the next
level by building tools on top of it and
you know complex which is you know maybe
one example of a primitive that we could
choose to expose if we don't already do
today I'm not sure if we have the always
beans and things like that well you know
right okay and then have again tool
tools vendors help developers optimized
code based on that I don't know not sure
if that answered the question but that's
my best and we had to take on DJ scripts
that did something somewhere
this one away oh you're next go let me
get this viscous guy first I mean you
can grab here so I'm in Java states
using the CMS collector and I allocated
a big a lot of memory that doesn't
change ever left out of my program I've
noticed that seems to make garbage
question incremental once I take longer
should that be possible or should that
the general sense is that you're going
to spend more debt and then the more
live data you have the more time you
spent garbage collection
I mean so the two you're talking the
concurrent phase us in CMS is that and
it's increasing over time is that what
you say yeah ski we got I mean you got
to go through the live data I mean in
the more live data you have the more
time you're in spent unless you're
running g1 ok we have a job so g one
actually so the idea of g1 and there's
there's two talks that are coming up we
have an advanced tuning talk tomorrow at
ten in this room and then there's the g1
tuning talk which will you know give you
more information that you can possibly
imagine about g1 and i'm sure that we
address there as well so but but that
particular problem increasing live data
set CMS in particular with a concurrent
latent garbage collector not only so
there's two problems strategist g1 that
situation great very huge heaps we want
to be able to to guarantee the low
latencies but also as you get into that
situation eventually you're going to
have fragmentation and eventually you're
going to have to use so how do you
handle the fragmentation CMS is not
going to handle the fragmentation so
that's why we actually wanted
fragmentation
yep yeah and you're talking exactly it's
the race right given time yeah okay good
grant yeah with all the new I oh stop
are you paying attention to performance
of byte buffers because I feel there's
still select different performance from
Y to race so yes I mean that delco stats
are highly and heavily used in a lot of
enterprise applications so in general
absolutely the regional combinations not
working as
okay all right do you so I guess an
effort to improve things in general if
you have Lily if you have a micro
workload that shows that easily we would
love to see that yes we have a gentleman
over here that's been had his hand up
for a while if you don't mind go ahead
who's John rolls around so repeat that
one more time re implementing the invoke
interface on top of them both dynamic is
what you said it spread that sounds very
interesting I don't know what the GC I
mean it sounds it's almost like we're
doing the opposite today we're kind of
implementing invoke dynamic on top of
him both the interface I guess I yeah I
have to admit failure here and you know
so that's I have to come curious where
the question came from so as you know if
you want to let us know on the aliases
and we can certainly try to dig a little
deeper but well yeah if you have numbers
you met you if you see the differences
that mean obviously not gonna see it but
maybe if you have idea surrounding this
certainly willing to discuss it it's
tough to know I just don't got you on
the open forum right okay so gentlemen
the back
especially after G so
um yes so to some extent you know so
this is an area where you know we're
ramping up the effort in right now it's
not as if we don't have anything in this
space already so we have class data
sharing today already in the hot spot BM
it was you know before the permian
removal project it was limited to the
cereal collector but one of the nice
fall outs of the permian removal project
was that it now actually kind of works
on all the collectors trivially
automatically the whole you know dynamic
environment that you know clouds come
with no that is clearly something where
the JVM is in a unique position to help
again we are already virtualizing things
for the application we plan to continue
doing that and clearly you know if we
want to adapt to the environment we're
running in to slice up the things up and
down and I've gotten what the question
you have actually lost sorry
we're doing rocky top runners in a limo
I resources cpu and memory write
applications inside of that keen
observer avian can deal with them
because multi-tenancy we have a too
sterile job external area
so yeah so basically I think there's the
interesting trade-off here is between
sharing our memory and isolation so the
more you share the less isolation you'll
have the more isolation you want the
less you'll be able to share and it's
not only on the data side of things it's
on any kind of resource so if you think
about it you know it's everything from
network I owed to cpu utilization to you
know whatever and I si you know my if
you had an infinite amount of time I
think you would basically you could
start in either end and you'll end up in
the middle you have basically the same
thing in there somewhere you're where
you're sharing the maximum amount of
memory possible well while still having
some kind of insulation in there so the
interesting part is really from where
which n do you approach this class data
sharing to some extent starts in the end
where everything is isolated first you
only share well what is reasonable to
share and in this case it's the class
metadata you share it transparently
using the OS level functionality to do
that you can also approach it from the
very outer end so one of the products
that Oracle lab labs again have been
working on for many years is the MVM
product implementing the the isolates
jsr I forget the number which is
starting in kind of the other end where
you have you you're sharing everything
to start with and then you're figuring
out what do not share and again the
trade-off there is a complex one in many
cases it's okay to sacrifice some
isolation in order to gain more
shareability in the other cases you
really want that secure isolation and
and yeah so it all depends on what your
end goal is we have solutions in both
spaces and we're constantly improving
these as well and yeah and we're still
you know working on on both ends of the
spectrum or the scale
drive here until we get your ex go
through yes in general there's a lot of
gem collectors
no spaces like this is this what is that
there's no answer
look at the mint final flags you see
that what's the default values for this
in the documentation but i'm not sure if
in some cases the logic behind that
people why this is this in this platform
why this is 16 or 32 that can roll a
song in some cases help us decide which
one to use so you can talk about it look
at the source should i say something one
of the things i can say is that where we
are aware of the fact that the
documentation around or command line
parameters is there's room for
improvement and so one of the products
that is currently ongoing is to clean
that up and to be you know be more clear
about around what parameters we expect
people to actually change and to make
sure that those are both documented and
tested and supported and you know the
works the ones we're not documenting is
our command line options we're hoping
that people either won't have to use to
start with or you know or so esoteric
that it doesn't really make sense
document them so that's the first step i
would say you know just cleaning up what
we have there today on top of that and
we like to come back to this is d1 where
you know hopefully we can reduce the
number of parameters you have to specify
to start with and then there's clearly
you know as you point out in some cases
it says 16 in there and it's not exactly
clear why and i think in many cases it
isn't clear exactly why that is 16 and
what drove that decision the code base
is you know 15 years old I'm sure you
know people made experiments and wrote
16 in there for reason that may have
changed since and I think that's a yeah
but I put a lot of these fights
some of the you know application
behavior ideally this sort of
application is sort of behavior then we
know that my application doesn't fit
that profile my voice
I thought well I think you hit you just
hit the last point if there is actually
an explanation behind it we will
certainly get that explanation in this
next wave of fixing things but we just
weren't at the be honest with you we
created flags for free debugging our
development and weren't very careful
about what was left left open and left
out there so now you have you no more
flattered that you could possibly
imagine and a lot of them don't really
do anything is this where say that there
are 504 there's some pot and that's
that's we you know where we grab that
situation and then we're trying to fix
that I mean we went open source without
fixing that problem when we probably
should have not done bets oh to be very
frank but and internally i have the same
problem too so what I do is I look at
the source and try to figure out myself
if I don't find any engineers that can
help me so I have to have similar issues
so but we can tell me with that's it
should be it should be documented if it
means something and and and for 4g one
we try and you know through blogs and
stuff like that people are trying to get
more information out there especially
the phases of g1 and just understanding
the g1 logs and Serena
get involved that for the last six
months is not really up to date at the
meeting or the behavior
that's weak but these are engineers who
are writing the blog's too so you know
you rather have them working on product
and to update the blog every day right
no yeah right now nobody I mean
referring to the source code you know
crude as it may be in some case is the
only way to get there I mean a garbage
collector for example is a really
complex things so in many cases though
you know it's really hard to describe it
it's really hard to predict how people
are going to use it and how applications
behaves and we're trying to provide
saying defaults for our flags in some
cases I'm sure we chose approximately
the right numbers in some cases we
didn't and and yeah it's a it's a really
hard problem I mean we could remove all
the parameters as well but that would
clearly you know make it harder in many
cases to tune the product some people
would probably be happier because you
know there are less things it's like my
iphone I know that it can do some things
well and those things it does really
well other things it can do and I know
there's no way I'm gonna get it to do
that so it is what it is that is what it
is sorry okay we've got maybe two
questions okay go right ahead so I'd
like to go back to g1 and please take it
right well on my experience with g one
so far i've been using it in since it
was an experimental collector is that
it's still a little bit behind
especially in stop the war performance
we respect the CMS and especially
compared to the deterministic collector
of jrockit so i wanted to know well if
you have a like a plan where you can say
g1 performance you know arrived on par
with CMS and
you know that's from their own you can
actually say okay this is the collector
of the future because I understand it's
the replacement the long-term
replacement for CMS but I feel it's not
there yet no this isn't fair you know
yeah it's only there in the good cases
and I'll just take it yeah I was the
plan I think they can tell you but I'm
working on it every day so yes we're
trying to get there and and I would like
to know your specific case and I can I
would like to help you yeah yeah yeah we
exchanged a couple be Muslim oh what's
inning there oh right take it off right
now I think this is really you know if
you have problems with you one or if you
have an application that you would like
to you know for us to focus on this is
really your channels like g1 we
understand that it's a new collector and
that means that you know the range of
applications that you want performs well
on is fairly limited right now our goal
is to widen that you know as quickly as
possible but we really need your
feedback to help drive at work so so
please work with Monica and the
performance team to you know provide
feedback on where g one isn't really
prepared the defaults for g one as a
stand now spending on the to your
application behavior it may not match it
and it may perform quite poorly and the
drones were there for a reason and I ran
the test to say that twenty percent and
eighty percent for nursery is what we
want to begin with because we have to
start somewhere right and then try to
get most of the applications now
depending on your heap size we'll talk
about this in the g1 talk and a little
bit tomorrow as well but sure I mean I'd
like to I'd like to know your
application and is there any plan to
port the deterministic collector of Jay
rocket into g1 or as an alternative
algorithm collection algorithm available
there's no planted port deterministic DC
specifically from Jay rocky too hot spot
but there are we do have you know in the
pipe work to implement low latency or
post less GCS in hot spot again on top
of the d1 framework right so open source
or
that is a product management decision
that I happily hand over to product
management yeah that would work work
we're going over now right so we're done
thank you very much thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>