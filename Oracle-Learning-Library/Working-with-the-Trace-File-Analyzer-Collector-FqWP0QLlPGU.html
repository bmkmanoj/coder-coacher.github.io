<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Working with the Trace File Analyzer Collector | Coder Coacher - Coaching Coders</title><meta content="Working with the Trace File Analyzer Collector - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Working with the Trace File Analyzer Collector</b></h2><h5 class="post__date">2014-05-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FqWP0QLlPGU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">working with the trace file analyzer
collector as database engineers you face
many pain points when it comes to
determining which trace files you should
be collecting and sending to support
what files do you collect for a problem
it's not very easy to determine what is
needed sometimes the logs get
overwritten or removed before you can
collect them for an issue some of your
tools do not allow for easy collection
of logs from all nodes in a cluster
often you decide it is better to collect
everything the large collection results
in massive uploads to support and these
can take a long time to upload and
analyze we have a solution for you
trace file analyzer collector trace file
analyzer collector is a utility for
collecting Oracle diagnostics TFA
collector collects Diagnostics based on
the time of the incident it prunes
larger files also based on the time
specified with the goal of making the
collection as small and as relevant as
possible using TFA you can collect
Diagnostics modified with a specific
time frame with a very simple command
TFA collector understands clusters and
all the various components from which
Diagnostics might be required
TFA will communicate your directive to
all nodes in the cluster in this
training we will discuss the challenges
around collecting Oracle diagnostics and
the solutions to those challenges
provided by TFA collector we will
discuss TFA collectors architecture
installation security how to take
collections trace analytics available
from TFA collector and the resource
footprint let's take a look at how TFA
collector works the customer initiates
diagnostic collection using the
collection command TFA signals
collections on other nodes collections
are written to local TFA repositories in
a normal state TFA periodically stores
file and resource metadata for use at
collection time
it also tails the crs ASM and DB alert
logs watching for significant events
like aura 600 or a 74 45 or a 40 31
evictions and instance terminations
local collections are copied to the
master TFA node the customer transfers
collections to Oracle support TFA
collector knows about all the standard
diagnostic destinations for Oracle
products and the operating system and it
discovers them automatically at install
time thereafter it periodically
inventories those directories by making
note of new files and modified files TFA
probes the files to determine the
earliest and most recent timestamps in
the files and it stores that as metadata
for later use TFA categorizes trace and
log files by component when a collection
is triggered and inventory is performed
to sweep up the most recently created
and or modified files based on the time
specified and the metadata stored TFA
works out which files are candidates for
the collection which is cluster wide
larger files are pruned to reduce the
size of the upload while preserving the
relevant Diagnostics alert logs are
always preserved to the last startup
regardless of size the files are
archived into a compressed zip file each
node will produce a collection unless
the collection directive specifies
certain nodes in the cluster as stated
earlier TFA collector is always on
continuously monitoring alert logs and
doing periodic discovery and inventories
and if auto collection is configured
doing collections as needed the TFA
control is the command interface for TFA
collector and it is used for all TFA
management tasks root privilege or
pseudo control is needed for performing
most configuration tasks since TFA
collector runs this route so that it can
access any file that might be needed for
support and since many files that are
typically needed are owned by root as of
TFA version 3.1 non-root use
are allowed to take collections and run
other basic commands including using the
analyze feature for analyzing alert logs
system message files and OS watcher data
here is a listing of the command syntax
for TFA Control the commands most often
used by non-root users will be diet
collect to take collections print to
print out various information like
status and configuration analyze to
perform analytical summarizations of
what is going on with your system and
directory to add or otherwise manage
directories that are accessible to TFA
collector the commands most often used
by root users are access to setup access
control lists and manage user access
directory to add or otherwise manage
directories that are accessible to TFA
collector set to change to the
configuration settings and behavior of
TFA collector such as repository maximum
size and location etc purge to manage
space in the TFA repositories and host
to add or remove hosts from the TFA
collector configuration here is a
listing of the options for the print
command now let's talk about
installation the supportive platforms
are listed TFA collector is installed by
default on 11 204 and will be installed
by default in future versions of 12c TFA
collector version 2 5 1 5 is installed
with 11 204 and it is recommended to
upgrade those installations to TFA
collector version 3.2 or higher in order
to get the latest features like the
non-root collection and the analyze
functionality during the upgrade TFA
collector detects earlier versions
distributes the new version to remote
nodes and upgrades on those nodes for
other versions it is recommended that
you download TFA collector from the my
Oracle support document ID 1 5 1 3 9 1
2.1 and install it to upgrade the
collector regardless if you are
upgrading or installing the instructions
in the
are relevant to you this example shows
the options available with the install
TFA lite command here you can see that
to install you invoke the install TFA
lite script specifying the location
where you want TFA collector installed
referred to as TFA base note that the
install must be performed as root or
using sudo control
also since TFA collector uses Java you
need to specify the location of a 1.5 or
higher Java Runtime environment referred
to as the Java home the Oracle Grid
infrastructure includes a JRE 1.5 or
higher and if available that would be
the best choice for the Java home note
that the Java home must be in the same
location on all nodes if there is more
than one node in the configuration if
you are installing in a clustered
environment then you would typically
want to do a cluster wide installation
but doing local installs on each node is
supported this may depend upon your
organization's security policies as
doing a cluster wide installation
requires temporary password lists ssh
user equivalents for the root user
if this temporary setup for SSH is not
allowed by policy then the best choice
is to do local installs on each node if
TFA has to setup this temporary password
list ssh for root it is removed once the
installation is accomplished
note that this password lists ssh user
equivalents for root is only required
for the initial installation and that in
the future when upgrading TFA is not
required in this example you see the TFA
installer informing you of the
requirement for password lists ssh user
equivalents for root for doing a cluster
wide installation and offering to set it
up temporarily for the duration of the
installation if not already configured
if policies prohibit this then you can
cancel at this point and proceed to do
local installations on each node as root
next the Installer will proceed to
discover the nodes involved if this is a
clustered environment
in this example with four nodes finally
the Installer will check the SSH user
equivalents for the root user on each
remote node and temporarily configure it
if necessary TFA collector will discover
any running databases registered in the
cluster where and determine there are
diagnostic destinations once finished
checking the status of the Oracle stack
TFA collector will confirm if you want
to proceed with the installation the
Installer next discovers the well known
diagnostic directories on each node this
slide shows a truncated listing of
diagnostic directories discovered for
each node next TFA collector will be
installed on each node in the system a
summary of the configuration and status
is displayed the port is the port used
for secure communications between the
TFA collectors on each node the port's
used will not necessarily be the same on
all nodes and also note in this case
version 3.2 was installed an inventory
is started on each node assuming the
defer discovery flag was not used when
invoking the installer the default
access controls for non-root users is
configured a summary displaying the
install and repository locations and the
status of the space usage for the
repository is presented for each node
finally at the very end of the
installation you should receive a
message stating the installation
succeeded and that TFA control command
syntax usage is displayed at this point
you may want to explore the commands and
the options for each now let's discuss
security as mentioned earlier TFA
collector runs continuously as root
since it needs to be able to access
files that are owned by root to minimize
problems getting all the Diagnostics
needed for support issues however TFA
will only accept commands on its Secure
Sockets and then only built-in commands
that it understands as of version 3.1
TFA collector non-root users can take
collections by default the owners of the
Grid infrastructure database homes and
verse of the OS DBA groups for any
oracle homes are allowed to take
collections
at the discretion of the customer access
control list can be set using the access
command and its options this
functionality allows the customer to
control exactly which users should be
allowed to take collections without
having root privileges here is the
syntax for the access command as you can
see you can list add remove block and
allow specific users you can disallow
access by non-root users altogether you
can reset the access for non-root users
to the default users mentioned earlier
in other words the access controls are
flexible enough to meet the requirements
of the customer while not overly
restricting access to the needed
diagnostics this slide provides some
example commands for setting up access
controls for TFA collector you can add a
user add a group remove a user block a
user and remove all users note that
remove all requires collections being
taken by a root or under pseudo control
which was the case prior to version 3.1
by default all trace directories
discovered by TFA collector are marked
as public meaning anyone able to take
collections is able to collect files
from them you can optionally configure
directories as private meaning only root
or users who have privileges on the
directories are able to collect files
from them so in role separated
environments access controls can be
configured if required if desired non
root access to files can be disabled
altogether and collections can be taken
by root or by other users under pseudo
control now we'll discuss collections
taking collections is the primary task
of TFA collector the command for taking
collections is diet collect and there is
a lot of flexibility built in
collections can be limited to specific
notes or sets of notes collections can
be limited to specific components or
sets of components collections can be
limited to specific times or ranges of
time the more precisely the time of
incident can be identified and collected
for the smaller and more relevant the
collections can be the simplest form of
the diet collect command is TFA control
diet collect this command will collect
diagnostics for all notes for all
components that were modified in the
last four hours so if an incident is
known to have occurred within the last
four hours this would be the simplest
way to obtain the relevant collection
although if an even more precise time
could be specified it would be even
better
such as using TFA control diet collect -
since one hour this command will collect
everything modified within the last hour
potentially making the collection quite
a bit smaller and more relevant as can
be seen from the diet command options
TFA collector knows about the operating
system cluster where ASM ASM clustered
file system database database workload
manager install and configuration
utilities and cluster health monitor in
addition to well-known directories that
the TFA collector discovers and collects
from you can also configure it to
collect from other directories that
would not ordinarily be discovered one
example of this is to add what could be
thought of as a utility directory to the
TFA directories using the TFA control
directory add command and configuring
that directory with the collect all
attribute then when performing a diet
collect with the - collect alters option
all files found in that directory will
be added to the collection regardless of
time or file type in this way you can
include files in your collections that
would not otherwise have been collected
another way to upload miscellaneous
files is to place them in the default
utility directory named Diagnostics
underscore to underscore collect under
your TFA base directory any files placed
in that directory will be collected the
next time a diagnostic collection is
executed with the - collect all ders
option TFA will empty that directory
once the collection is complete examples
of the kinds of
files you might include in the collect
all directory would be screenshots of
error messages AWR reports that have
been run in connection with the problem
time third party tool output and so on
you can also specify the collector
option with the comma separated list of
directories otherwise unknown to TFA
collectors configuration to sweep up
files into a collection irrespective of
the type or time constraints specified
for the collection these two
capabilities allow you a lot of
flexibility to include whatever is
needed in your collections that might
assist Oracle support in solving your
problem using a tag specifies a
user-defined name for the directory that
will be created in the TFA repository to
contain the files from the collection
make the directory name meaningful and
easily identifiable to the user taking
the collection or to other staff who may
be engaged at a later time otherwise TFA
uses a naming convention which includes
the date and time of the collection
finally you can see the time modifiers
for collections since is a number for
hours or days from the current time from
n2 for a range of time for for a certain
date here you see an example of commands
for collections for all nodes all
components for the last four hours from
the current time including cluster
health monitor and OS watcher data for
all nodes all components for the last 8
hours from the current time including
cluster health monitor and OS watcher
data for two specific databases only for
the last one day from the current time
for the CRS and OS modules only for two
specific nodes for the last six hours
from the current time these examples
demonstrate collections for ASM only on
a specific node from midnight on March
4th to 2100 of March 5th all nodes and
all components for a specific date all
nodes and all components for a specific
24-hour period CRS component only for
all nodes and including any files found
in the collector list of
directory's note that in this example
diag collect a non root user is
initiating the collection the collection
is for a thirteen hour period for all
components and all nodes as the
collection proceeds you receive feedback
to monitor the progress of the
collection you can see that a cluster
wide file inventory is conducted to get
the most up-to-date metadata for files
that might be of interest for the period
specified you can see that the
collection directive is communicated to
remote nodes the list of relevant files
based on the time specification is
identified TFA start zipping the files
trimming down the size of larger files
if necessary some metrics are summarized
at the end of the collection process
indicating how many files were
considered how many qualified how much
space they would have taken and how much
space was saved the timestamps at the
beginning of each line can be used to
determine how long each step in the
process took and the overall duration of
the collection finally the compressed
files that need to be uploaded to the SR
are identified note that the compressed
files from the remote nodes are copied
to the local repository of the
initiating node so the main takeaway
from these last two slides is that with
one simple command you can quickly and
easily collect a precise and small as
possible set irrelevant diagnostics from
an entire clustered system this should
limit the need for additional request
from Oracle support for first failure
Diagnostics
and it should remove a lot of
uncertainty on the part of the customer
about what exactly should be uploaded if
the diet collect command is issued with
the no monitor flag the collection will
be run in the background and the user
will not receive the typical progress
feedback in those cases the TFA control
print actions command can be used to
monitor the progress of the collection
as seen in this example you can see that
the inventory is running on my host 1
and the collection is running on my host
1 the data here are truncated for
brevity but similar output would be
displayed for each node involved in the
collection the status will be updated
over time
the command can be run periodically to
monitor now let's discuss trace
analytics a powerful feature of the TFA
collector is the analyze command which
can be used to analyze and summarize the
following files CRS alert log ASM alert
log database alert logs system message
logs and OS watcher data analysis can be
limited to component time specific
search patterns and error types here are
the options and time modifiers to the
analyze command search is used for
string pattern matching comp is used to
specify specific components type can be
used to specify error warning or generic
messages currently the most significant
use for analyze is summarizing messages
by type such as error warning and
generic searching for and summarizing
search patterns summarizing OS watcher
top data summarizing OS watcher slab
info data these examples demonstrate the
command syntax for each of these options
this is an example of the output from an
analysis of generic messages similar
analysis can be done for errors and
warnings here is another example of
output for an analysis for generic
messages example of output for a pattern
match search Oh RA in this case over the
past week this is an example of the
analysis of the OS watcher top data note
the following data provided for each
metric first sample highest and lowest
sample with their x average count of
non-zero samples third to the last
sample second to the last sample the
last sample and trending data this is an
example of the analysis of the OS
watcher slab info data the metrics are
summarized in a similar way to the top
data first sample highest and lowest
samples with their x average count of
non-zero samples
third to the last
sample second-to-the-last sample the
last sample and trending data now it's
time to discuss the resource footprint a
question that is often asked is about
the resource footprint of TFA collector
the answer is that it is small TFA
collector runs as a java process and as
stated earlier TFA collector is always
on and has never really idle at minimum
it is constantly tailing the crs ASM and
database alert logs looking for
significant events this activity doesn't
require much CPU periodically TFA
collector performs a file inventory
during the duration of an inventory you
might notice some CPU utilization in top
when a collection is performed you will
notice some CPU being consumed by TFA
collector but we have never received any
complaints about TFA utilizing too much
CPU for the most part you won't even
notice it running as you can see in the
example TFA collector would be tailing
alert logs and no Java process is even
evident this is a test system and there
is not much activity on this system if
we were observing a system with a lot of
activity then the TFA collector would be
an even smaller portion of the CPU
activity in this slide TFA collector is
performing a collection which involves
doing an inventory and then copying and
zipping a lot of diagnostic files you
can see that a Java process is the top
consumer of CPU at this point in time
this CPU utilization will fluctuate in
this particular system there are 6 CPUs
and as you can see from the load average
the load on the system is not really
significant in the previous slide before
the collection began the first bucket of
the load average histogram was 14 so the
load average roughly doubled the load is
still not significant TFA collector is
not stressing the system at all in this
slide the following command was used to
display more detail about the CPU
utilization for the Java process of TFA
collector pea
d11 953 top- p11 953 again this sample
was taken while a collection was in
progress this output displays the CPU
utilization of the process spread across
all six CPUs well percent CPU is
indicated as 68 percent at this point in
time the CPU utilization is spread
across all six CPUs predominantly on CPU
zero and CPU for the CPU utilization
during the collection will fluctuate on
each CPU but on the whole the load on
the system is not significant at this
point the TFA collector collection has
completed and as you can see the CPU
utilization for the 1:1 9:53 process has
subsided to practically nothing another
aspect of the resource footprint of TFA
collector is the storage this slide
displays the result of the TFA control
print config command which shows the
impact of having done two collections on
a freshly installed system the default
size for the TFA collector repository on
each node is 10 gigabytes as can be seen
after taking to cluster wide collections
the result is that forty four megabytes
out of 10 gigabytes has been consumed on
one node the reason only one node has
space consumed is because as the
collections from each node are copied
over to the initiating node which in
this case is my host 1 the files are
deleted from the other nodes it is
important to note that you can initiate
collections from any node and the TFA
collector configuration it is up to the
convenience of the user should the
repository fill up by reaching its
maximum configured size TFA collector
simply stops taking collections until
space is cleared in the repository or
the maximum size of the repository is
increased this behavior protects from
inadvertently filling a file system you
can use the TFA control purge command to
purge older collections from the
repository and free up space in order to
manage the repository storage you can
also change the location of the
posit or e to a shared file system for
instance on a filer where there is a
large amount of free space this slide
shows the collections that were taken
and the files that were created the zip
files are the ones that would be
uploaded to an SR in this case there are
four nodes and there is a zip file for
each one the size of collections will
vary based on many variables such as how
many nodes the time window specified the
components specified the number and size
of trace files having been generated and
so on in a database consolidation
environment care should be taken not to
collect files for every database as that
is bound to take longer occupy more
space on the file system and result in
much bigger collections in that
situation collections should be targeted
at the database or databases having been
affected by an issue rather than all
databases the point is that you are in
control of each collection and the more
targeted the collections the better they
will be for many reasons it is probably
not a good idea to leave all collections
laying around taking up space and you
should periodically purge your
repository so this has been an overview
of the trace file analyzer collector
installation and collection features see
the related documentation at my Oracle
support for more information look for a
document ID 1 5 1 3 9 1 2.1</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>