<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oracle Social Intelligence Part 2 | Coder Coacher - Coaching Coders</title><meta content="Oracle Social Intelligence Part 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oracle Social Intelligence Part 2</b></h2><h5 class="post__date">2015-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pHlMNnx59Q8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">basically had to fit into their existing
hacker system not to from terrain vent
everything from the scratch because
obviously this wasn't possible so
starting with this car ability this is
the presentation by the way which is
scheduled more or less for one hour we
need to do this in 30 minutes so I'll
try to compress this content as much as
possible but I think we can share the
slides later on with you so you can go
through the for the content in your own
pace but i think it was Steven who
referred yesterday to this to this
benchmark which was done I think
September in 2014 with a 1 trillion for
a triple stored in in Exadata and that's
that that answered our concerns so
basically we were a little bit afraid
whether we are able to store the
relevant information when in the right
way and process this efficiently and
with right performance and this sort of
answered all our doubts so first of all
one trillion and I think that Steven
also mentioned that we can easily exceed
this this this threshold up to three I
think he mentioned something like that
so again this this leads us huge space
for 444 data to store and process them
efficiently on the database side and as
we have more than 20 k of events per
second these pieces of communication
little business of communication this
number is going to grow and the number
of data which we are storing this will
grow and will grow really fast obviously
we are not willing to back up the
internet because that's not the purpose
here but we are we prepared a series of
filtering mechanism based on risk based
on content based on other stuff to limit
the number of of the content but still
it's going to be a lot so with this kind
of approach we are more than sure that
we we can handle this secondly what was
I think very important for us so how we
are going to prepare Anna I think
already answer to this to the same to
this question with the previous slide
saying that when you were already
starting from the beginning we're
thinking about triples so we were
thinking about graph representation and
graph based storage because we thought
that classical
and outs a relational data
representation it's not mentally I would
say close to the way how those
relationships look in real life
obviously you can do this but this
column representation it's hiding a
little bit the relationships so we were
thinking about and what is important
it's with this kind of model you have
very strictly specified format so you
need to think about the model you need
to think about how you structure the
data from the very beginning obviously
you can add number of flexfields but
this makes all the scheme a little bit
not performance at some point in time
and it's making this really hard to
maintain at some point in time so we
thought that there must be something
some better way of doing this and that's
how we how we found out that basically
those relationships are better or at
least better represented when for when
we do have graph representation
underneath so and this all started to be
even more important when we discovered
how much we can get out of the of the
messages that are exchanged so obviously
you're already had a location name of
the person maybe date of the message
where it was sent these are outside
basic ones but we can really get out of
those messages much more than that
coordinates device information IP
address potentially sentiment from
extraction of the entity so for example
time mentions locations hashtags on all
this stuff can be extracted and should
be extracted from the from the messages
and should be stored and used for
further analysis so this all clearly
proved a task that we need a really
performance and fully blown NLP engine
for doing this dis dis live let's say
inspection of of the vortex enrichment
of the of the messages and I've already
mentioned about security aspects so
maybe i will quickly skip through this
slide but obviously we had to make sure
that we are going to separate the
data in the right way and we are going
to protect the data in the right way in
the places where we store the data but
also everywhere else where the data it's
it's transmitted and application
diversity that's what I've already
mentioned this is something which I
think it's obvious to everyone and
collaboration that's the last but not
least which I'd like to point out here
we do have many tools for doing already
some sort of analytics on top of graphs
and on top of data that we store but we
very soon found out that if I may refer
here for social relationship management
which is Oracle product by the way in
the cloud who provides an ability to to
subscribe to certain topics of
discussions which are exchanged in the
internet or which are posted in the
internet and SRM it's collecting all the
data and provides us basically basic and
functionality to analyze the data but we
soon found out that it's too generic
this is more for marketing purposes so
if you are I don't know I McDonald's you
would like to further further Fatu to
know what people are thinking and and
that we think about you this is a tool
for the job so you would like to
probably get this kind of reports with
this kind of platform and on the other
hand on the on the on the on the
opposite and you do have this advanced
analytics oh really hardcore a set of
functionalities for for data analytics
for doing all this complex stuff but
this is not some on the other hand
useful for those officers for those
analysts working in the intelligence
surgeons because these guys don't have
this kind of skills so this is something
which is a little bit a little bit
difficult for them also to learn so
we've clearly found out that we need
something in the middle that which is
providing enough analytics for them that
they they would use but it's also simple
enough and able to use the data that is
coming from the external work very
efficiently
and now before me maybe we go to the
solution itself this last slide because
those kinds of questions may pop up in
your hand in your heads in few moments
how we calculate risks in the system
risk are at the moment this is this this
is over simplification but soon 2324 to
a little give you a little bit of taste
how we are doing this we are taking into
account the distance of the people we
are taking into account content and this
is something which might be important
for you as you are as your background
hit send in Special and location enable
systems we are also considering a
location-based alerts and location-based
risk assessments so basically if someone
it's within two kilometers that's why I
also took the slide because this might
be important for you that we are using
this kind of mechanism then we might
create geofences where someone is
entering this this is geofence the alert
is triggered and particular person it's
highlighted as if as if as a potentially
dangerous object who should be monitored
and obviously historical activities so
for example if someone was quiet for the
recent several months and now he tweets
like crazy and what's even worse he is
using all the terms like jihad and
everything else what it's a little bit
scary for us this might be a person who
who should be taking to some sort of
different level of inspection let me say
like that so just just basics of the
risk assessment module and how it all
works so basically we do have a pipe
first of all we are starting with
natural language processing maybe other
way around we are starting with
collecting the data so we are starting
with getting the data out of the
internet space we do have a major media
proxying from component for for doing
this job out shouldn't and show you this
in a moment and then we are putting the
data in sort of a funnel which is trying
to reduce the amount of data but also do
the right job for the for doing this
text enrichment so first of all natural
language processing language linguistics
sentiment analysis and entity
instruction
there we put this in the cross cross
domain secure database which is use used
by other systems through the integration
layer or by real-time monitoring
existence or by user driven analytics or
visualization don't be misguided by the
word database we are using here a little
bit of the hmm i think it was called
from on monday a polyglot model so we
are using different tools for four and
two different types of storage is for
storing different different type of data
so we are using big data appliance for
storing this low-density data then after
the data refined and after the natural
language processing it's executed we are
storing the data in the graph format in
the graph data store and this one it's
based currently on two verses we have
two versions two flavors let me say of
the social intelligence one semantic
graph-based and one its property
graph-based i will show them in a moment
so another um slightly more detailed
view on the architecture so how it looks
in the details again on the left hand
side we do have those external data
sources here we have media pro</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>