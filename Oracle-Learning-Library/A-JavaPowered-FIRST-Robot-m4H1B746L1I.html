<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Java-Powered FIRST Robot | Coder Coacher - Coaching Coders</title><meta content="A Java-Powered FIRST Robot - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Java-Powered FIRST Robot</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m4H1B746L1I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to talk to you a bit about how
we've used Java in programming our robot
this year so here's a picture of our
team wearing appropriate attire and
there you can see this year's robot and
we were fortunate enough to win one of
the regional competitions this year with
that robot how many of you here have any
experience of the FIRST Robotics
Competition okay some of you great so
for those that don't then I'll do a
little bit of explanation first robotics
is a program for high school age
students and it brings mentors from all
different kinds of backgrounds but
primarily focused on on technology into
student teams and each year you have six
weeks to build a robot and this is not
like a little tiny lego robot this is a
big big robot that can weigh up 220
pounds so they're trying to reach there
trying to create real situations where
you have too much to do in too little
time with too much too little money and
all kinds of other constraints and and
then you you compete with your robot and
this is not a battle bots type thing
this there is it is a contact sport as
far as the robots are concerned but
there are a lot of rules about that
contact and you're not out there to try
and destroy the other robots you're out
there to try and to try and win the game
this year's game was essentially a
basketball game played with a few little
twists to it so it's played with 8 inch
foam balls rather than actual
basketballs and there are actually four
hoops at each end of the field and it
played on a field that which is about
the same size as a basket
all field tennis court that kind of size
there are obviously a lot of rules and
the exactly what the game is changes
every single year so you don't know
until like the first weekend of January
what the game is going to be and you
then have six weeks about technically
about six and a half weeks to build your
robot and seal it up in a plastic a big
plastic bag and then take it off to
compete the plastic bag thing is all
about giving giving people the same
amount of time to actually build and
work on their robot regardless exactly
when that your regional competition is
so robots must weigh less than a hundred
twenty pounds by the time you put a
battery inside the robot added some
bumpers that makes the thing weigh close
to 150 pounds which i think is the ocean
limit for a two-person lift so I think
that's one of the reasons why they chose
wondering 20 pounds and it must be it
must fit in a box and they actually
check this at the tournament but the the
start of the match it has to fit inside
a box that's 38 x 28 x 60 inches so the
robot can be like this tall and the
match is played three robots against
three and i'll show you such a short
video a little bit later you see so that
you can see and each match is two and a
quarter minutes long the first 15
seconds of that the robots are all
autonomous so they're doing whatever
you've programmed them to do and then
you've got two minutes of what they call
teleoperated time so that's where
students are controlling the robots then
you normally have two students who are
controlling different parts of a robot
another student who can actual actually
handle the game pieces in this case the
balls and then you have a dr coach in
that the discretion of the team can
either be another student or a coach /
mentor so this is kind of what the field
looks like you can see half the fields
here you can see one end you've got
those for hoops
in a diamond shape there so that you've
got you know you get one point to the
bottom hoop two for the metal hoops and
free for the top poop and then you've
got these some strange things sitting
over halfway line firstly there's a
little stainless steel barrier at the
halfway line of the field that's
basically four inches square and it's
just that so that to make the challenge
you're moving around the field a little
bit harder and then you've got these
four bridges or they're kind of like
teeter totters in the middle there and
they play two roles in the game firstly
there an alternate to being able to
cross over that barrier there but
obviously you need to be some way of
pulling that bridge down so that you can
drive up onto it if you want to cross
over it and then at the end of the game
you can get bonus points by balancing on
the bridge with another robot and within
with the bridge in the middle of the
field is kind of unusual in that you get
bonus points for balancing on that
bridge with a robot from the opposite
side from the opposing Alliance and
you're awarded what they call
co-optation points for for doing that
and the competition points play quite a
lot of influence this year on the
seeding at the end of qualifying matches
so that's the picture of us balancing at
the end of the match with one of the
opposition robots and you can see one of
the one of the foam balls down there and
on the floor too so this year we made
our robot drive in the you know
backwards and forwards in the y
direction so you want you want to be
able to drive around and pick up these
basketballs off the ground so you want
as wider a hole in the front of your
robot to put balls in to make it as easy
as possible to to drive into over the
top of balls so we we made the front of
our robot the the 38 inch wide part and
we we had a 28 pneumatic wheels which
gave us enormous grip on those
on the bridges and came in very very
handy for balancing with and it also
meant that we could cross over that
barrier that I talked about as well and
we actually made the robot symmetric so
that you could harvest from the back or
the front just meant the driver had to
be able to drive the robot both both
ways around but she got used to that
pretty quickly and we had we built a
ball shooter that was controlled by a
camera and we had a very simple way of
being able to tip the bridge down so
that we could drive up onto it or a
partner robot could and if we get time
I'll talk about some of the senses that
we used and how we used them so on to
the software well the software is
written in Java and we used a library
called WPI lib j to do the software that
runs on the robot and this is an open
source library that's written especially
for the FRC competition by Worcester
Polytechnic in Massachusetts and the
Java that we're running here it's
actually the the Squawk virtual machine
which is the Java that runs on the
little sun spots looking third of those
and we did we worked to port that to the
Power PC hardware that that is used for
the first competition and essentially
it's headless mid Pete so meet Pete
without the graphics and then on the
driver station one of the things that we
did for the first time this year was
that not all of our software is actually
running on the robot so you have a
pc-based control system that you attach
joysticks and gamepads whatever you want
to use as your control devices too and
that sends though basically communicates
with your robot code so that the state
of those control devices is communicated
to your coat robot code 40 times a
second a new thing that we did this year
for new for our team at least was that
we also ran additional code on that
driver station and took advantage of the
fact that you've got compared to what
you have on the robot you've got a
honking big
PC sitting there really not doing very
much and this year we used it to do
image processing RV video stream now
this was not a brand new laptop so very
small it's actually smaller than this
thick one here that physically smaller
than this one here that I'm using to
present with and it was about three
years old when we started using it and
we as far as we could tell we only used
about twenty percent of the CPU so the
stuff that we were doing was not
actually you know that complex or CPU
intensive and the robot and the field
communicate using Wi-Fi so you've got
basically IP connectivity between the
robot and the driver station and they
open up certain ports to allow you to do
that kind of communication so the way
that WPI lip is structured there are
really three concepts that you have to
think about when you're designing your
code the first is something called a
subsystem and that's just a you know a
functional group grouping of components
within your robot and I've listed out
the subsystems that we had this year the
drive base well that's the part of the
robot that actually you know holds the
wheels on has the drive mechanism the
motors the transmissions the chains or
this year we used drive belts saved a
few pounds of weight so we have sensors
on the gearboxes so we can tell how each
each side of a robot is moving and so on
the ball harvester well that's the thing
that can pick up balls off the ground
lift them up store them until they're
needed and then deliver them deliver
them into the shooter at the point when
you actually want to shoot a ball the
breach tipper will answer little
mechanism that just pokes out of the
front of the robot so that you can drive
up to a bridge and tip it down and drive
onto it the shooter I just missed out
well it's pretty obvious what that does
the autonomous subsystem well that
doesn't have any sort of physical
analogue on the robot it's just there to
manage
the selection of which autonomous
program we want the robot to execute in
this match so it's kind of a virtual
subsystem so not everything that you do
has to have be physically related to the
way the robot is and then finally a
camera subsystem so that subsystem
managed all the communication to and
from the software that was that was
running on the driver station and
basically a substance system class you
have to extend a subsystem class that's
provided by the library but other than
the fact that you extend that class
there are no requirements on a subsystem
class so you really have to think what
are the operations that I want this
class to expose don't don't want to
expose the ability to drive straight or
go backwards or whatever and then then
you can start thinking about okay what
methods do we want to implement on for
this subsystem class another key concept
is something called a button so
typically I mean a button is essentially
represents a control input to the robot
and normally then this would be a real
button on a real joy stick or a real
game pad but it doesn't have to be it
could be a virtual button so for example
if the harvester motor current has been
high for more than a second then let's
consider that that button is pressed
that way we can make something happen
within the robot when when we spot that
situation occurring and one of the goals
of the the way that WPI lib has been set
up is to try and avoid having to expose
high school students who are just
learning to program and just learning
java we're trying to avoid having to
expose them to multi threading and
synchronization and semaphores and you
know it's hard enough for them to
actually be at a program without trying
to explain to them the you know exactly
what the java synchronized key word
means
and all the things that you really need
to know in order to use it correctly and
so on so using these buttons and buttons
get mapped to things called commands
which you'll see in a moment and they
basically cause a compact come up
there's the associated command to get
scheduled having that mapping BB dynamic
is actually very nice because if you're
at a practice session and the drive team
tell you are we really I really wish
this button was over here then you can
say okay give me two minutes and you can
fix that so that's the way of making the
drive team very happy than very
productive oops so very quickly this is
what so a command is basically a way of
encapsulating an instruction to a
subsystem and there's a thing called a
command group that lets you form groups
of commands that into something that you
can schedule in one go now we actually
created 65 commands this year I was
really quite surprised when I went
encounter them up at the end of the
season that includes a lot of debug and
calibration type commands and all of the
different autonomous programs that we
came up with during the season as well
let's just go back here and take a very
quick look this is probably a bit of an
eye chart for you guys I'm sorry about
that but essentially this is a we
created a button called a disabled only
joystick button so we wanted a a button
that would only be activated all when
the robot is disabled so when when it's
sitting there on the field that before I
match even starts and you want to be
able to tell their over I want you to
run autonomous program number three by
pressing the plus one button three times
so this basically all you have to do is
implement a get method that returns
whether or not the button is pressed and
you can decide what you know what what
it means what calculation you need to do
in order to determine whether your
button real or virtual is actually
pressed obviously if you're talking
about a real joystick button then this
cloud there that already does that so
you just have to instantiate the right
class with the right parameters to the
constructor one of the things that help
to grow the number of commands that we
had was the fact that we we wanted the
robot to be as functional as possible if
we're when things start to break you
know when a sensor wire breaks or
something like that we put a lot of
emphasis on making on automating the
software so that there's little room for
error in what the dr team does is
possible but things happen sensors break
wires break motors burn out and it's
important to have a manual backup and to
actually train the drive team have them
practice using that manual backup system
so that when something does go wrong you
know in a real match then you know how
to deal with it it's not scary you don't
waste the rest of the match trying to
work out what happened and you can
actually you actually have a way of
working around it okay sorry so a
command always excuse me always extends
a class called command base so you
provide a constructor and there is an
initialized method that's called when
the command is scheduled there is an
execute method that's called on a about
one about 40 times a second while the
command is active and then there is a
method called is finished and that's
called by the framework to ask the
command are you done yet so in this case
it's a very simple command all how to do
is essentially set a state variable in a
subsystem so it just returns true
obviously if your command may involve
you know moving something until you get
a certain sense of out sensor reading
then your is
next will be a little more complex and
then there's an end method that that's
called once the command is complete and
then there's also an interrupted method
so commands are associated with one or
more subsistence if when you schedule a
new command that needs one of those
subsystems then the the existing command
that was using that subsystem will get
interrupted and you may we didn't
actually have to implement any
interrupted methods this this year but
there is a mechanism there to deal with
that so I said that we that we built a
shooter that used the camera well each
of those hoops has a rectangle of retro
reflective tape that's the kind of tape
that you find on children's coats or
boots or whatever that reflects car
headlights straight back in the driver's
vision this incredibly directional in
the way it reflects you can be
illuminating it and if you stand a few
feet off to one side it won't look any
different the moment you line up line
your eye up with the the light that
you're illuminating it with and you'll
really see it so we put some green LEDs
around actually around the the camera
lens and you know made good use of the
the reflective properties of that tape
and so we mount today we had a fixed
camera that was mounted on the on the
shooter the shooter could move in
azimuth so it could point in different
directions relative to the robot and it
has what's effectively an elevation
control and we can also vary the the
wheel speed of the shooter the shooter
is basically a single wheel and as it as
it shoots the ball through it puts a
vast amount of backspin on the ball and
you know I'm not a native basketball
player that even I know that back spins
a good thing when you're playing
basketball and we found that through
experimentation that when you put that
much backspin on there if you hit the
back door behind the net
ball is going in so so we found a way of
getting that back spin very naturally
just from the design of the one we all
tutor so the the camera that's on the
robot is just they it's actually a
rather old webcam it's probably
state-of-the-art about four years ago so
maximum vga image we actually use 320 x
240 because we found that that was
enough pixels to give us the resolution
that we needed to be accurate with our
shooter so the video stream that's
coming from that camera goes directly to
the driver station it actually bypasses
the computer on the robot entirely the
software on the driver station processes
those video frames and I'll talk talk
more about that later it identifies the
target coordinates in each frame and
sends those target coordinates back to
the robot so the robot then receives
basically an X in a Y and that's the
coordinates in the camera frame of the
center of the of the division target so
we can use the x value if the if the
shooter is pointing in the right
direction azimuth wise so left to right
then the vision target will be quote in
the middle of the camera field of view
now I say quote in the middle of because
you never quite make that camera point
directly down the center line of the
robot and directly down the line where
the ball is going to go when it comes
out of your shooter so that's just a
parameter that you have to calibrate to
say what what are you going to regard as
being the middle of the field of view
and then the Y value if you think about
it the further away you are then the
lower down in the field of you the the
vision target will be and that means a
higher number I will be great a higher
number and we we used a linear
regression we'd a lot of calibration
work and
a fitting and use linear regression to
basically workout for a given position
on the field distance away from the
target I given Y value in the in the
camera what wheel speed and what else
shoot relevation do you need in order to
get the ball in that target to go in the
hoop so we basically derived some 4th
order polynomials and that we use to
that and it'll thank you to Wolfram
Alpha for being so easy to use for that
kind of thing if you haven't if you're
not familiar with that and you do any
kind of manipulating numbers I'd highly
recommend it and we we then use PID
control for all of the things that you
can adjust about the shooter in order to
get accurate control and so this graph
here shows the polynomial that we used
for converting from y-coordinate down
the bottom there to the wheel speed rpm
number that we needed on the the y axis
there so you can see and there's a
there's a separate you know curve going
on at the same time for adjusting the
elevation and lovely some interaction
between those which is why this is the
slightly strange shape but you can see
that as you get as you start to get
further and further away then the the
RPM makes an increasing amount of
difference or you have to adjust the the
shooter wheel speed more and more as you
get further away from the target but
using linear regression like this is
nice because you're not you're not doing
sort of look up tables or if it's
between this and this then use this
value kind of thing which you're very
laborious and hot and easy to break and
it's really a very simple computation to
do and it works really it's a technique
that we've used for several years and it
works really really well so azimuth
control well so when you receive target
information then you may be on target
or the target may be to the left or to
the right of center in which case you
have to move the move the azimuth of the
of the shooter in order to account for
that so we have a encoder that allows us
to know the position of the vasa 'the so
we can then convert the the pixel offset
that we've got from the from the camera
picture to an encoder offset you know
when the encoder changes by this notch
when that when you move the camera just
enough so that it moves one the target
moves one pixel one thing that we found
that we needed did need to account for
is the time lag involved in all this the
time lag between when the when the
photons actually hit the sensor in the
webcam to the JPEG being encoded inside
the webcam to that being transmitted
back to the driver station being
processed and then the the target
coordinates coming back to the robot and
and you then actually controlling the
mechanism to do something so we kept we
call it an azimuth history so that we
could when we receive new data from the
camera we could actually look back in
that history to see where was the
azimuth pointing and we use the number
200 milliseconds where will be pointing
200 milliseconds ago and that's where
that's the number that we need to use
the offset from the camera on in order
to work out where we really need to be
pointing now so once we got the the time
lag thing worked out in the azimuth
history working nicely then we found
that the even when the robot was driving
around the PID loop for the azimuth
control converged very very quickly and
you could drive past the target and have
the and watch the shooter actually
follow really quite accurately as as you
were driving
so this is this is what a camera image
looks like the observant folks among you
will tell me that doesn't look like that
those don't look very green yes this was
taken using blue LEDs but you can see
you know the way in which the division
targets are reflecting the the incoming
light so in the driver station software
then we convert the image to HSV and
then we apply a color threshold so that
we turn that the the 24-bit image down
into a one bit binary image which we can
then further process so we were using a
google cloud project called Java CD
which is basically a Java interface to
the open CD image processing package and
ran into an interesting issue with one
of the one of the threshold encontra in
there in which basically wouldn't let
you have a have a threshold of there was
no way of specifying a threshold of 256
and it it only applied it applied a less
than to the upper threshold which meant
that if your image was kind of over
exposed or you know well saturated you
actually missed a lot of pixels out of
your I'm out of your fresh holding so we
ended up having to jump through some
hoops and basically process each of the
HS and v channels separately and then
kind of end them back together to get
the to get the final binary image rather
than using the function you ought to
have been able to use to do this in in
opencv I think that's actually been
fixed at least in the C++ API for open
CD I think that's now been fixed but it
tripped us up for a while until we
realized what was happening so as you
can see so that this is the thresholded
image and the threshold has been tuned
and we continue to tune the thresholds
over the course of the season for
different lighting conditions and so on
but you can see that there are sort of
gaps
it and if you look at that top one there
you can at the bottom of the top
rectangle there you can really see where
the the hoop and the net kind of get in
the way and make a make a nice hole make
that rectangle not non-continuous for
you so we then use what's called a close
operation on that image which basically
means that you you kind of spread out
the white area which will tend to close
up small gaps and then you selectively
erode that back so that you get an image
it's more like what you started with
only with some of the gaps closed up and
you're basically counting the number of
white pixels of the white neighbors that
a pixel has in order to determine
whether you should keep it as white or
will turn it to black when your ear
oding it again at the end and we deal
with the rest of the gaps so this is the
same image after it's been dilated and
then eroded and you can see that it most
of the gaps are gone but not all of them
so we still had a little more work a bit
later in the process too so what we then
do is contour detection so this gives us
a very very complex set of contours
which you then you then approximate get
open to eat CD to approximate those
contours down to a resolution that will
allow you to tell is this thing us does
this thing look a bit like a rectangle
or not so we convert the we approximate
those and convert into polygons
basically which we can then work with
and in order to deal with the remaining
gaps we do some heuristic at this point
where we're approximating the outline of
that polygon in order to complete broken
outlines and essentially we when we get
to the stage of having a set of polygons
that are candidate targets we then say
well we know what kind of thing we're
looking for so we know something about
how big or little it ought to be in the
field of view so if the area of the
polygon is less than a certain amount
or bigger than a certain amount then
we're going to you know you're not the
target we're looking for you really
ought to have four corners if you're a
rectangle and you're the angles between
your corners ought to be you know
roughly 90 degrees obviously as you with
perspective that will change so it's not
exact but you can still put some limits
there and again get rid of some more
false positives and we also know that
we're looking at a rectangle with a
specific aspect ratio and we know kind
of based on where we can be on the field
how much that aspect ratio can seem to
change when we look at it so again you
can put more heuristics in to allow you
to select the right target and not get
confused by false positives and then
finally so this is this is basically an
image that's marked up with the the
final rectangles that we spotted and
you'll see that we've got a nice set of
88 rectangles here for which are
contained within the other four so
obviously you throw out any rectangles
that are contained within one of the
other rectangles that you found and that
way you're you're only looking at the
outside edge of the tape rather than the
inside edge of the tape now this is what
the drivers actually see on their driver
station screen so they can see that the
they get a nice visual indication that
the targeting system is actually working
and the communication with the camera is
good and so on so all the targets that
are seen are get highlighted in yellow
and the one that with the driver has
actually told the robot that's the one I
want you to aim for is highlighted in
red so they can then now there's no way
you may not always see all of the
targets of course so there might be a
robot in the way or you may not be
looking at all the targets because some
of them are outside of the field of you
so there's no real way to say both
always go for the the one that's in the
middle and on the left
really all you can do is say go for the
leftmost one that you can see so in that
case then you know obviously the purpose
of this display was to give the drive
team a visual confirmation if they
needed it that they were that what the
robot thought they were targeting was
the the hoop that they that they think
they're aiming for so this is kind of we
um we actually capture quite a lot of
images and store them on the driver
station during matches so that we can
continue to debug and when the drive
team comes along and says halfway
through that match we were right there
and it didn't shoot then we can go take
a look at the images and try and try and
see what happened whether it was an
image processing problem or something
else that was happening on the robot so
this is a slightly blurry image because
it was taken while they were driving
away from the goal on having having just
scored so most of the time then we well
all the time where possible we run our
shooter in what we call automatic mode
and the fire button that the student is
pressing is really a permission defiant
or fire when ready button and the robot
will actually shoot the ball when it
thinks that the shoot is on target so
that means that the the shoot is
pointing the right direction that the
elevation is correct and the wheel speed
is correct based on the camera
information that we have the there are
we can see the targets from further away
than we can actually shoot particularly
when it comes to that top goal so we
also put some limits on that tart the
the y coordinate and then we also want
the robot to be more or less stationary
and we didn't this was something that we
didn't really experiment too much with
we just when we have encoders on the
drive base so we can tell whether the
robots moving or not
and we didn't get as far as wanting to
do the extra computation to take account
of a robot motion when we're trying to
work out how to shoot the ball maybe an
offseason project okay let me see if I
can play some match video
ok
I'm guessing that quick time is not set
up on his laptop so maybe we're going to
be able to use that
doesn't look promising I will so suffice
to say that doing all this in Java
worked really well for us it enabled the
students to be surprisingly productive
and excuse me we we had a you a very
enjoyable and very good season and we
had we found that offloading all that
processing from the robot to the driver
station really helped us do well any
questions so far yes sir
at last
no so that the the sunspot reference we
the virtual machine that we're using on
the robot is the same is the same
virtual machine that runs on the sunspot
we weren't actually using any sunspot
devices so that there is a there's an
embedded controller that's donated by
national instruments that that sits on
the robot that's powerpc-based that bats
about powers that controls the robot and
then there is a laptop which the which
the drive team who are actually
controlling the robot arm that's sitting
right in front of them yes yeah we were
yeah so well the squat vm was was a good
choice for the for the actual robot
controller because it's a fairly limited
powerpc-based piece of hardware and you
know sse embedded is not available for
that for that hard way so for writing
robot code then then you know robot
control code then it's fine obviously
having foot full se on the laptop was
very nice yes no we we we never we never
experienced anything that i would put
down to to a garbage collection cycle
clearly you can write you can write java
code to generate more garbage or less
garbage we we made a conscious effort to
do all of our you know explicit object
allocation or explicit object usage in
you know as a shin time and you know
that obviously there's a certain amount
of garbage that you can't help producing
just because you're running java code
but we we never we never experienced
that you you don't have to make this a
lot run for days or weeks with our
garbage collection it only really only
has to run for a few minutes without any
significant garbage collection going on
see and we
why
maybe I mean this was we we we generated
about seven seven thousand lines of Java
code this season so it's it's a
non-trivial application yes so we had a
variety of things for autonomous ranging
from so the robot can start with two
balls inside at the beginning of
autonomous so obviously the you know one
of the things you like to do in
autonomous is to actually score those
balls because you you get double the
points actually get it you get a three
point bonus for each ball that you that
you score during the autonomous period
there are also balls just go back to her
okay you can see this is actually a
picture of just after a match has
started and you can see that there are
some balls sitting on top of those
bridges the the red bridge of the the
bottom bottom left corner of the screen
belongs to us with the red Alliance and
the blue Alliance is not allowed to
touch that bridge we can't touch the
blue bridge but the middle bridge there
we can we can compete for that so in
this in this particular instance then
our robot has deployed its bridge tipper
you can kind of see something poking at
the front of it and is heading for that
that bridge to go and get those other
balls so that we can then try and score
those balls in in autonomous and deny
them to the opposition as well as
scoring the two that we actually start
off the match with so we had a variety
of programs that would go to go there or
go and we had a program that would come
to the the ramp the bridge at the bottom
of the screen and tip that to get the
balls off and so on so we had we had
probably about five or so programs that
we used regularly during the season okay
two minutes to go
no we we actually didn't worry about the
size we just went by the the y
coordinate of the center of the
rectangle that we found because that as
you as you as you walk closer to it then
it's going to be you know it's going to
go up in the field of view of the camera
now the way the coordinate system works
then that means that Y is going down so
why less means you're closer Y increases
as you come further away okay then you
don't actually need to calculate how far
away you are you just need something
that's a proxy for that distance we were
running the camera at 20 frames a second
so from the point of view of the drive
team then you basically get video
on-screen live video from the from the
robot with the with the annotation and
with the overlay added to it
no that we we found we were I think we
did actually measure the time it takes
to process each frame it was on the
order of about 12 or 15 milliseconds I
think
we used a lower resolution image so
primarily because we were concerned
about the bandwidth on a competition
field and we had no work we we neither
using less bandwidth was better than
using more and we had no way of testing
that outside of competition so we went
for the lowest resolution that we could
use that gave us enough resolution to to
be accurate in targeting okay well thank
you all very much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>