<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Exercising Java 7 Features in Enterprise Applications While Avoiding Pitfalls | Coder Coacher - Coaching Coders</title><meta content="Exercising Java 7 Features in Enterprise Applications While Avoiding Pitfalls - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Exercising Java 7 Features in Enterprise Applications While Avoiding Pitfalls</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-TLUqFpM5w0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming for this
presentation about the jdk 7 features
just a quick introduction about myself
anilkumar i have been working at intel
in the server java performance area for
10 or 12 years and with respect to the
jdk 7 features what i can promise you
there will be no hardware slides there
will be no cpu mentioned it will be all
about java and jdk features i'm sure in
telemarketing won't like it but that's
how it is today so quickly going to we
have lot of material first i would like
to give a huge credit to Sergei Catco
who is the integer performance team
member and he is the brain behind many
of the implementations and the things we
will talk today I will show you without
his contribution I wish she was here but
due to some of the things he couldn't so
just a bit agent that we have a lot of
things to cover but first I would like
to probably around five minutes of time
to go over the there is not much mystery
about the enterprise benchmark you will
see it very soon so around five or six
minutes about the benchmark which we
have worked on and it's pretty complex
benchmark you will see a bit later and
then what jdk 7 features we have used in
it the one of the thing i would like to
highlight that it is not a toy benchmark
the jdk 7 features we are talking here
are not based on some hypothetical cases
or some small examples it's a pretty
complex benchmark and we have tested
them for this scaling and I will cover
them a bit later too so we have several
successes just from the get-go and there
were other cases it was not that good
and what week we have to do in them and
event summary and I leave some time for
the questions so first around five or
six minute for the benchmark so if the
benchmark name is spec JB 2012 is going
to come soon so I just want to get a
quick idea how many people here have
heard about the
expect Java benchmarks just raising the
hands very few okay it will help i guess
i'll go a bit so there we have right now
from the in the spec of for java
benchmark active and those are expect a
BB which have been very g to run but it
was almost like a toy benchmark which
was respectively 2005 and that's the one
we are replacing from his brand brand
new benchmark written from scratch
everything and making it more enterprise
java level benchmark we already have a
enterprise very complex benchmark all
respect gentle eyes 2010 but that is too
complex it's so complex that probably
only 20 to 50 people in the world run it
there are publications but it's closest
to the real world production environment
so we try to make something which we in
between simple to the complex but at the
same time is still easy to run the
current benchmark it emulates a
supermarket supply supplier head quarter
where we let's say you are running in a
cloud or in that and clustered
environment and your everything the
point of sale the ITA employed as the
data mining on it everything running on
that I t so it tried to simulate the
complete IT system getting exercised the
benchmark we have tested a already from
single node 264 nodes we have tested it
and we have make sure with with the
scaling that it should scale even up to
thousands of nodes but we have tested
ourself up to 64 nodes it uses the
latest technology which are available
security xml jdk 7 features so that when
the benchmark comes out and the JVM
vendors are optimizing for it the many
of these technologies should be
optimized by default and another new
thing is it has two matrix called mexico
j offs for the throughput and it has a
response time metric too big that have
been the first time being introduced so
you could have some idea from 0% load as
you increase two hundred percent how the
response time changes so that credit is
also there now let's talk about a bit
more about this struck
so I have two more slides on this first
is you can run in many configuration one
is it has three component controller
transaction injector and the back ends
so controllers just control the complete
phase of the run and evaluate at the end
to generate you report transaction
injectors are the load injectors which
actually sends the request to be
processed and the back end is the
business logic which process was
required and send the response back so
you could run it in the single
configuration as on the top single
application set or you could run it as
multi-application set deployed in
clusters or in the cloud mode the one of
the new thing which was not there before
in other benchmarks was that there is a
back-end to back-end communication the
inter java process communication because
we are seeing many customers using it
and one of that is one of the component
JVMs were not working to optimize that
hard so we hope to use those features it
will help JVM vendors to optimize the
inter process communication now a bit
more detail about the back end business
process logic as I talked about it
simulates the supermarket company so you
have multiple supermarkets within the
same Java process running you have
supplier interface and then you have the
headquarter so most of the work is as a
point of sale goes to supermarkets where
we they are maintaining their inventory
they when they need a customer
information they pull from the
headquarter and then they send the
receipt after the after the check out
this end there is received back to the
headquarter and had quite a store the
information how does the JV a remote
customer remote interaction happens it
is example let's say some you are in a
costco or a supermarket and you are
visiting from one area to the next then
they are being mean to like canada
entity and the US entity bill will be
maintained separately and if a customer
from canada visiting us their account
information need to be access from the
other country database so that's kind of
like example we try to do the so a
customer could visit any supermarket and
information could be it's in local Head
Quarter or could be in the remote so
that will cause the transaction to
complete go to the remote process get
the information and finish it so that
type of remote process information will
be there and it's being a benchmark or
so we have said so many properties in
the benchmark that you could increase
that remote interaction from almost from
zero to big amount as well as there are
many other features which you could
change a different type of cues I if I
have time we'll go at the towards the
end I have some slides there so so that
was about the workload that it is a
scalable workload is pretty complex it
does the inter process communications
and it has many entities which could
maintain their own thread pool because
supermarket has their own data structure
with their own thread pool possible and
head quarter could be their own thread
pulls various threadpool working and you
could deploy it in a cluster distributed
mode or you could run on the same
hardware system so the various ways of
deploying them now let's move to the
mean areas which we want to talk today
are the jdk 7 features which we used and
some success stories and some failures
so this is the list I could get from the
jdk 7 site about the different features
and we used all the jdk 7 features in
this benchmark very possible no that was
just a kidding part week otherwise we
want to be completing it so here is the
real list and I will be respect to
setting some timeline expectations the
font size is small means i will try to
spend less time on those features and
the bigger means that is where i will
spend more time so for example the quick
feature new i/o java xml try with the
source catching with multiple exceptions
underscore numeric literals and type
interfaces and the thread local random
those are used I will go very quickly
over them but the main discussion I want
to happen to us the more complex part
which is using for join so I think the
first slide we should be able to done in
seven to eight minutes and then most of
the part would be about fork join that's
where I want to have some main
discussion feedback so let's talk about
very first feature were the new I oh so
spec JV to hunt well by nature we try to
keep its CPU centric and the memory
centric and the processing centric so I
o is the exercise some I oh but it's not
it doesn't influence performance we use
the i/o part in generating the report
section where it need to hand a lot of
files and in different directory
structure and create different links and
images etc and what we found during the
use that it was very very convenient way
of using them and we did not find any
issue with the new IEP once again we did
not stress it to the benchmarking level
or so or did not measure the performance
but our reporter is very complex it's
almost need to handle one to two gigs
size of the binary log process it create
more than ten thousands of files which
it does in 10-12 minutes but we are not
putting any performance metric on it but
we did not see any scaling issues it
actually worked pretty well the next one
is the Java XML and that happens to be
when we are doing inter-process
communication or in the entities then we
use encryption or sometime compression
and some on some format to maintain the
XML or jst so we have used these
features what we did not look what is
new and the GX be so we did not so that
part we are not sure but the XML part in
the using jdk 7 works fine there were
some cases where we have seen
serialization or D serialization issue
and on some architecture I if you try to
go many many cores with in one single
Java process it could hit the scaling
water leg as far as it is clustered we
did not see the issue but if you go
within one huge java process with 500 or
more thread etcetera you might start
seeing the issue
but we did since other benchmark could
go Mort was the cluster side at for the
node once we did not try to really
stress it within the single JVM a lot
and see what causing the bottom leg
issues the next feature of jdk 7 views
you try with the resource and it is
again we use for write and read file
operations this new feature also we use
it because it's very convenient and one
of the good thing is you don't need to
remember about closing the stream once
you are done with it the stream get
close is by itself so that feature we
found to be very convenient and we did
not see any issue it works very
flawlessly the next one is the catching
multiple exception types and in this
case also there are the way we are doing
communication and there are two type of
request when we want to talk from one
Java process to next and I will cover
later the some requests are blocking
where I need to wait a thread to get the
response back so those are synchronous
type there are other a synchronous type
and the communication need to almost
communicate between thousands of nodes
it need to keep track of so that part
was very complex vu using the Grizzly
there and it could throw a different
exceptions whose answer is same so for
that part it actually worked very well
for us it makes the code Presley simple
and it allows actually much less code in
terms of cleanliness and we did not find
any issue in this area also it works
pretty good and we also use the type
interface for generic instance creation
so we have lot of generics and a lot of
Jenny collections and the diamond part
of it actually makes the code very
simple or readable so it was very useful
and it also helped us any time
refactoring of the code result if we
could do it much easy and we did not
find any issue with respect to scaling
also in the time interface for
generating
the another interesting one is
underscores in the numeric literals and
we have in this benchmark where we are
maintaining the response time in
microseconds and many time due to
accuracy the constant value to compare
where the SL is our etcetera you could
find several constant of this type with
a lot of zeros and we are using it so
that if the it's easily readable so that
part worked well too now coming to the
concurrent utilities the new feature
also there is the ran the thread local
random and it worked very well compared
to the old we were using Java util
random it was not scaling that well the
new one actually skills really well and
there was one issue we encounter there
was a bug where it gives you the same
sequence each time so it's random number
but it was giving same sequence and we
reported that bug also and I don't know
if someone else of the reporters a at
the same time but it is fixed now in
update to now I think we will talk about
the last feature which I have lot of
discussion going on in terms of folk
join so the first question I would like
to ask that how many people are using
for joint framework right now it gives
some idea could you please raise your
hands five or six Wow okay I think it
might be interesting discuss and after
that that what are their experience with
that money and let's see going through
what we found in using for joint and
what is our experience so the flow i
plan to go is just some quick
introduction or fork joint and what we
found what all changes we did and couple
of issues we have to do because it's not
very straightforward I guess that's the
reason I see like five hands being
raised people are concerned a bit out
there once it is what it works it works
great with respect to scaling and I
think for the new multi-threading
multi-core it it should help your
application to scale well but yes there
it took us months
I will say three or four months or more
to actually tune everything and a couple
of design decision we have to do in the
in the benchmark so let's just quickly
talk about the foam joint pool a bit so
what happens you have a submission queue
and you have workers so there are 2 q's
worker they have their own individual
queues and the mean is the submission
view any work comes into the submission
queue and worker pics could pick the
work from the submission queue or from
the worker cue the when worker is doing
the transaction it will take the
transaction from its only own worker DQ
will talk about in that space but the
other workers so worker takes the work
from the head and the all other worker
could steal it from the tail that's what
the fork joint is the work stealing part
now let's see how it works so you have a
worker what the new task is always
submitted from the worker into its own
cue let's say worker is split it or
worker does create a new task while
executing it that task always goes to
the worker q and a new task from outside
the thread pool outside the JVM process
when it comes in it always go into the
submission q so there are 2 q's when the
internal workers creates it it always
goes into their its own q but if it asks
come from outside it goes into
submission give so that part is pretty
important you will see in many instances
how it impacts the decision-making how
it impacts how you are going to
architect it in your application the
what are the consequences of it so
that's how it works number one a thread
when it is looking for a walk it looks
is there a task in my queue there which
is d its own qdq if it find the task
they are perfectly executed if it
doesn't then it tried to go can I steal
it from someone else so then it looks in
the all other thread dq's can I steal it
from the tail side of it if it doesn't
find anything to steal from there then
it goes to the submission queue and get
the task from there so that is the
sequence in the form
and work mostly now let me just took the
alternate what happens in an alternate
scenario case when we have a thread pool
so in a thread pool all the task are
submitted to the submission q so this is
a normal case example and we had this
implementation first and the region was
jdk 7 was not coming there we were not
sure when we started on the benchmark
and each of the entities supermarket
headquarter all were using your own
thread pools it is only when the jdk 7
was confirmed we moved to the fork joint
and that was pretty good incentive so
the task goes to a submission queue and
all the workers need to go to the
submission queue and that is where the
content that happens and we notice it
immediately anytime you need to scale up
there was a big contention on these cues
and it was not easy to resolve it so
that was our motivation why we decided
to move from thread pool to the fork
join so now let's see what happens in
the fork joint pool and what we ideally
want what we really want that the worker
often should get the task from its own
cue that is where the task should be
there most of the time very rarely a
workers should try to go to steal from
the others because if they stealing is
too much happening then also you will
see issues start happening and rarely
you needed to go to the submission queue
to get a work because if this in the
fork joint if that is not happening and
I will talk about later how you can
monitor it whether it is happening or
not there are API to monitor it I'll
talk later if that is not happening then
something is not right in your the way
you have architected it or the way the Q
and the task are getting created the
being generated but that is the ideal
case to have to focus on to work
properly most of the time you should see
work from the worker q very rarely
stealing and very rarely going to the
main queue so though that is what the
ideal goal will be and let's see a
couple of scenario what happens so these
are just a sample let's say 1 million
size of task comes to the submission q
so these are a couple of approaches now
we are
talking about implementation of the fork
joint which we tried and then we have to
change and what we ultimately did so
when a 1 million size comes you could
implement the approach which is very
naive that means you immediately take it
from the task queue you now put it into
the each worker queue and what happens
when you try to do that you would have
the N thread contention on that cue and
it is no different than the thread pool
so we really don't want this approach of
the fork joint because otherwise is no
different than the thread pool approach
we we were trying to get away from now
the there is a little battle which is
approached one and that could be that
when the work comes in the main batch 1
million submitted the thread one goes
which get a notification that so it
works it task comes in the bad
submission Q and a notification gets
sent and the one of the worker will come
and pick it so it out of 1 million it
picks the one task and it puts the rest
of it its own q and now what will happen
is another worker looking for the work
they all going to go now at seat tail of
the worker one and they will try to
steal from there one task to do now
suddenly what you will see in this
approach it will work fine if there are
only two threads because then only the
second thread need to go to this queue
and get it from there but if there are a
lot of threads then again you will start
having the contention point just moves
to the tail of thread one the contents
and will start happening here in this
space for them so this will be this will
work when you have two or four thread
but this will be totally broken if you
have lot of threads because the contents
it will happen on the tail so you do not
want this approach the reason I'm giving
example in for joint for joint does all
this work where the task come to the
batch I should have little bit into
sorry about the so the fever on so the
work come in the main task you and for
joint does all these things notification
it does notice again for you it does the
how often the worker need to do the
polling how does the work is stealing
need to happen it does all that
framework what it doesn't do for you is
how are you going to divide that work
how do you want to submit from the main
queue to the worker Q so that part that
is where you're a bit architecting comes
into play based on the workload and that
is where most of our learnings came from
so the approach one did not work because
again contents and move to the tail of
the thread one now let's go to the
approach to what we did so a batch comes
of 1,000,000 the thread one picks it up
and it divides it into two so it picks
the half of it it will do and it puts
the rest of the half in its q now
another worker will come and it will go
to the tail it will take it it will
divide again by half so out of 500 k
chunk it got the 250 and it puts the
remaining into the other q and the next
thread will come and it will again
divide it so it's the vice by sex and
approach where whatever is in the tail
of the other thread I pick the thing I
take the half of it and so this one will
work pretty good because very soon the
work is divided pretty quickly and since
we are dividing it in the big enough
chunks there are no contention so this
approach actually works pretty good what
we have tested and it scales very well
the we try to do two approaches which we
did actually some analysis some
performance results on on the benchmark
and those were if in one approach i can
divide the work by half so whatever work
I picked from the beginning I / half I
take half put half in my tea q and the
other work able to keep dividing we call
it by sex and approach other one is I
could have just divided by number of
workers because I know how many are the
default for join workers in the pool the
minimum one and we could have / equal
size
them so that approach is divided by
number of workers and what we did we did
runs with both of them and we have the
batch size around of 1,000 tasks when it
comes at a one-goal a batch and we did
not see any performance impact in either
of the approach and we scale them into
even in the single node to pretty large
values so they actually work pretty well
both the approaches so I like to
recommend here you could try either of
the approach by section or divided by
number of workers and they work pretty
good the only thing we could say that
what we have not done is the extreme
small batch size or a really huge batch
size let's say the batch size is really
10,000 1 million or if it is only 10 so
we could go back and update the data
with those things we did know but at
least for the medium size batch sizes
either of the approach work fine with
respect to splitting the work now let's
try to I talked about the monitoring if
you are seeing some issues on your fork
joint pool what can you do about it so
there are these six or seven API is here
that which tells you get parallelism is
number of target workers pool size is
number of live workers so and I will
cover that part what the joint
approach it has at least two approaches
in one case it can increase the number
of thread pool workers a lot so you
define a minimum the default size and
then it can grow to a lot it can sink
back grow lot means infinite and then
string back and we'll talk approach
another one is fixed you don't allow it
to change so that is why you have the
gap pool size how many worker at that
time for joint has created another api's
are about how many submission q count so
that way let's say some task are coming
and you want to monitor your queue size
for the load balancing purpose so you
have the 2q because in one case it gives
you how many tasks are waiting in your
main submission queue and another API
gives you how many tasks are waiting in
the total in the worker queues those
dq's because the task could be in either
of them so
by having some idea of the total number
of task there you could try to do the
load balancing on that and we try in the
benchmark we have two modes we call them
intrusive mode where we are doing lot of
profiling and we use these api's there
too to debug the issues because they
were several issue came that is how we
did our approach of submitting batches
and they study that what works best and
which did not work and why and in the
regular mode of the compliant mode we
didn't want to access the API so we did
not use but what we have done we try to
invoke these api is every one
millisecond to that frequency that means
1000 times in one second and we did not
almost see no impact on the performance
of the full system so that was pretty
interesting we thought there might have
been reasonably overhead and with
respect to the number of task number of
thread your results will not be like
accurate at the time because there is a
parallelism going on the workers of the
stealing but approximately they are
pretty close and they are able to give
you idea when you are designing
different approach of submitting into
the queue which approach works for you
how your cues are growing and
particularly for the load balancing the
purdy approximate enough so I at least
in our experience looks like these api's
are pretty lightweight and could be used
now if it about what happens so I'm
running one JVM process should i be
using one for join pool or should i be
using many for example in our case we
have supermarket each supermarket having
their own data structure on inventory on
checking out process and there are
headquarters and then there are
suppliers also i try to show here only
supermarket and head quarter they are
the one main entities which exercise a
lot so the another decision was should I
go with one formed when thread pool or
many thread pool for we call them
entities so should I go for one type of
entity one thread pool so there were the
two approaches we have tried what we
found in the beginning we tried the 14
join
threadpool per entity so the way by
default for join threadpool create the
number of default thread is total number
of logical process are available and as
a result you would have an
oversaturation on the left side here is
when you have 14 clan thread pool /
entity then load balancing becomes a
problem because in that case if any one
of them is hogging all the resources the
other one will be starving and there is
no way for you to balance it out you are
at the mercy there the and so in that
case we found that the load balancing is
easy with 14 join pool / JVM process and
that's the one we are using in the
benchmark the other problem was with
respect to using several folks on thread
pool / JVM was the communication so
let's say supermarket need to interact
with the head quarter to get the
customer information that it need to
come back process the transaction for
checkout it need to go to the head
quarter again and then it need to send
the receipt for a storage so it is
possible and we have a local caching in
the supermarket well another information
comes and go so it is possible that when
these we call them remote communication
when the one for Jen threat will need to
talk to the other one so we call it
remote communication it is possible you
could have a circular deadlock and you
need to then make sure that's not
happening when you have multiple thread
pool in the single thread pool that's
not going to happen so if you could
implement we advise going with 14 giant
thread pool / JVM process but there is
another spin on it let's so another one
is this is just another example what
happened now we talked about that our
benchmark has enters our communication
process so and it is also possible we
could deploy supermarkets in one JVM
instance headquarter in the another JVM
instance so now they are two separate
shower processes and each java process
as we just talked has 142 and thread
pull each what happened there is a
remote communication now because one JVM
need to get information from the other
and they are put equal type it might
have to get some information from the
jvm one so again you need to know your
flow graph here and it is possible that
you could have a circular deadlock again
and your application will hang because
so you need to make sure in your flow
chart that you are not having the
circular deadlock one depending on the
other and you can't so one of the
example would be lets say JVM one has
the eight threads and we will talk about
the approach a bit later and another 18
thread these aids are waiting for some
work to be done by the JVM too and these
eights from the JVM to get waiting to
get work from jvm one that lock nothing
will happen you will hang system will
hang so let's see what happens actually
in the for joint case of the remote
communication next two slides talks
about what really happens so we have a
task from the JVM process one it need to
be sent to the another JVM that thread
is blocking so this approach is blocking
approach so thread is blocking for the
response and the folks on thread pull
knows that it is waiting on the work to
be response to come from the another JVM
so it puts into the parking and it is
just waiting for the response from the
other and what happens meanwhile when
you have a managed blocker approach in
the fork joint so that's what they call
it managed blocker up managed blocker
approach so when the full joint checks
oh my one of the thread is waiting the
response from the another day another
process it actually creates a new thread
and that thread goes and picks the walk
to be done because otherwise what will
happen you will all wait and system will
have almost very low cpu utilization of
the work being done so that is how the
fork joint does this handling that it
when it detect the thread is waiting for
the other work response to come from
other it creates a new thread now what
could happen so we have remote customer
I we are doing very heavy customer
processing in the supermarket these
customer need to get some information
from the head quadrant which is in the
another JVM and it is taking time for
whatever reason it's slow the GC GC
pause might have happened on the other
JVM run
old parallel no response coming suddenly
my 2 30 40 or 48 64 threats are all
waiting for the response nothing will be
processed locally even though i had the
work so for joint actually start
spinning new thread new thread and most
of the time works but what happened
cases happened where the remote
communication was ultimately several of
the threads hundreds of the thread just
waiting from the response on the other
JVM because it was taking longer and
longer and this one start creating more
and more thread and we saw 35,000
100,000 thread getting created by the
folk joint pool there is no control on
it you cannot control it so that is one
of the drawback of the managed blocker
where it infinitely grows and it could
actually kill or hang your system or
process if you don't control it so and
we saw that behavior inspector 2012 when
we enable the remote transactions yeah
it was going more than 32,000 threats
and system was hanging so then we did
the alternate approach that's a bit our
solution there was we created a code to
have a bottleneck on the manage blocker
so it's a we could put that code the way
we made it walk into if you send me mail
we can give you how we do so in that
case we put a maximum limit and the
managed thread block or checks it have I
reached my max limit and if it has
reached then it will not create the new
thread so that way you are guaranteed
that it will not go infinite and your
system will not crash so it solves that
part but the another problem is now you
cannot grow infinitely and your M number
of threads which you put maximum all
could be waiting from the another
process and another process could be
wait so in terms of architecture you do
need to know a bit am I again causing
circular deadlock it doesn't solve the
circular deadlock problem it solves your
application crashing problem but it
doesn't claw stops your circular
deadlock so you could still and that
happen so we did the manage blocker
approach and now deadlock happy
system wouldn't crash but deadlock so
what we did for that one so here is an
example we call it TR approach so
remember eyes advised that we would have
only one for joint thread pool / GBM
process yes our main thread pool the
level one tr1 let's call it is one JVM /
process but what happens the way we
solve this approach was if the from a
JVM one let's a supermarket the tier 1
need to send any request it need by the
JVM to the request will not go in the
tier 1 it will go in the tier 2 and if
any request from a tier 2 from the other
GBM let's say because the transaction
requires subcomponents it need to come
to the other JVM it goes to the next
level tier so a bit in this approach you
need to know what is the highest
dependency is on your flow and it was
logic was simple if any TRN need to send
request remote request it goes to n plus
1 tier so there is all and we make sure
based on the dependency at maximum
dependency we have enough tears and that
is how so most of the time we operate in
the tier 1 for gin pole and very few
times we have to go to tier 2 tier 3
based on how much the more traffic is
happening and we can see from the
monitoring API etcetera so this approach
works best for us in the in the
benchmark application and it scales fine
and we did not see any more contact
switch or going to because we in this
managed blocker approach we are keeping
the thread fix in the folks and third
Pole we are not letting them grow
infinite because that's really bad
problem for the products and environment
it could actually shut down or hang
that's what it don't want so we rather
have the tier approach and make sure we
have enough tears so you will get a
response back you will not have a
deadlock happen the worst case here
could be you could be slower as far as
you have enough tears so there might be
some context switching rate higher if
you have higher in more traffic sometime
but they do come down the one of the
good thing about the for joint thread
pool is if you are not using them they
almost zero you don't have over
so the extra tears only become active if
there is a work if there is a request
other way they go down no overhead on
the system and system is operating most
of the time so that's the approach we
found in the benchmark which worked
really good for us so let me summarize
on this one so yes we have a
recommendation of for Jen thread pool /
JVM one load balancing nice but we use
the manage block or limit otherwise it
can grow to infinite which is not good
and in that case you might be in the
deadlock and that we just make sure your
flow how many tier you might need and
that's the message somebody so now as I
talked earlier there are two type of
request could go from the thread for Jen
thread pull one is blocked type where I
am waiting for the response i go into
the parking i will get the response back
same thread will pick the work when
response come back and finishes it
another is a synchronous approach and
you could have based on what you really
need to do these two different things in
the a synchronous approach I send the
task to the another GV m and the thread
goes pick something else it's a producer
consumer approach so that also there in
the folk join pool where I could send
the essen goodness task to another JVM
what was happening there we thought the
common sense was telling us they're one
of the way we could read make it better
was there making it buffering and I
think I'll talk about that one that we
could do when the task are coming all
these tasks go to the main queue as we
talked about in the 4gen thread pool you
have the worker thread and the main
queue all the task whether it is
requests or messages they go down to the
main queue so the here I was just trying
to summarize that the one was blocking
and another synchronous messages let's
go to the asynchronous message slide so
this is we call it task buffering and we
thought on the common approach it would
be good idea to buffer them because
otherwise the small amount of work and
going to the other one and we keep
interrupting each time so we thought it
will be efficient to buffer them and
then put it in the buffer queue that way
it's a bigger chunk of the batch and you
can easily process it but it we saw a
bottleneck and we were surprised like
what's going on it wasn't scaling and
what we found was that in terms of
execution order the way the different
tasks are coming without buffering they
were actually intermingling well between
each other but when we start doing
buffering the way they were collapsing
you have the same type of task coming
together next to each other and when
threads were picking them and start
executing it what we found that many of
the tasks were going to the same data
structure to the similar thing and a
very heavy contention was happening on
those that thread so I think this one is
a bit about your access pattern in the
way it happened in our bench in the
workload there and the our learning
there was then we decided not to use
buffering because we found that the
tasks are going to the some same hot
spot at the same time so for us in this
case without buffering actually work
better than the buffering would have
been but if these task with buffering
would have been mix and not going to the
hot places probably the buffering would
have been better so what we learned from
this was if similar task in the remote
JVM going to the very same hot place you
might have a big continent contention
issue and you may want to watch for it
because remember the buff without
buffering you might have even some more
time line let's say I'm going to buffer
for 200 milliseconds if I wouldn't have
then we would have been tasked much more
gap between them but when we buffer then
they go just boom all of them together
much bigger contention so it's the
coalescing effect also in the same
structure was pretty bad for us now a
bit about the for joint another feature
of for joint is the join part so what
happens a bit about the joint part you
have a task a task a that thread one
actually takes some of it and rest of
goes to its tail and from the tail that
a threat to could come it could take
some part of it and it could another
thread comes and then at the end they
will become joint so if you have the bad
side where it need to be broken and that
it do be joined it work actually really
well and where we exercise this part is
in our reporter what happens in our
reporter we have many gigabytes of the
data coming out of the benchmark and
then at the end we want to correlate
several things produce report and
several component need to be combined
and they have some dependency so the
part here work really well is the
dependency the the way the join works
it's very easy the way you can define
the joining of these tasks and it it
actually scales very well and it the
challenges were you need to know your
flow so it doesn't do the dependency
checking for you so let's say you have
some blocks and you need to join them
when you're defining you need to do the
good job of joining the dependencies if
you make a mistake in the dependency
your jvm will hang so that does require
a bit making sure that you are careful
when you are writing your code for
joining them but once you do it right it
actually really works super fast our we
cut down the time by like 10x when the
reporter was working without the
dependency in a serial manner to process
that two to three gig of the information
took us in some bigger cases the detail
one was like 30 40 minutes and it cut
down to five minutes with the join and
dependency part but there is a risk of
you do need to know the flow and make
sure you're not causing deadlox
otherwise it will hang and it did
actually when we did one mistake now
talk so those were the success stories
where we were able to use both type of
messages solve the blocking problem a
bit summary use one for join pool with
the tier approach and use the masses
identify the problem with
acid buffering and we did it find the
one case here for the forms one we
couldn't solve and that is do we have
one type of transaction let's say in the
supermarket where the the complete list
need to be bought online and it need to
be bought from several instances so
let's say customer is buying 20 items
and each and he is online and each item
need to be scanned in any of the
supermarket any of the jvm and you let's
say you have 512 notes running so for
that 1i and I need to process 20 items
for each item now I need to go quickly
ask each other jbm which could be
hundreds of them do you have this item
and who can give me so what happened
that transaction almost require huge
amount of interactions from the other
jvm processes but the work done was tiny
by the time that item goes to the other
jvm process the work is take that one
compare it to the concurrent hash map to
find the item if it is their reserve it
and tell the other JVM I have it so the
work done was tiny the overhead was
pretty good for the transport to come to
serialize DC lies marshalling and
marshaling so one transaction was
requiring lot of interactions and the
work is very tiny what happened when we
unable this transaction because we have
all properties where we could reduce
size how much time to active what is the
request mix when we even put one percent
of the transactions out of all this of
this type the system tanked the
resources CP utilizan came down from
ninety percent to blow thirty percent
performance drop 10x prolly and we need
to get the benchmark out and this was
not our we solve enough problems we
thought we have done enough jdk 7
feature use the due to lack of time we
did not try to debug the issue but we do
think that if you have a very fine grain
inter-process communication then either
there is a contention happening or
there's a lot of network for that many
requests or it is blocking on some load
unbalancing but we couldn't debug the
issue so our benchmark worked fine when
there was a little more coarse grained
interaction
and the fork joint and thread pool
another thing worked very fine but when
it was a very fine grain very often
communication it do not scale and we do
not have the solution to that one you it
probably require more into the network
stack and doing it better load balancing
but that was our failure case where we
do not have a recommendation or solution
for that space now going to my question
again so before I saw five people
probably raise their hand about the
joining a fork joint the way it has
worked pretty well after we found a
couple of cases and we have scaled them
almost up to 64 Nord no performance
issue very nice CP utilize and etcetera
my question is is does it give
encouragement or more and to a more
people to use folks want or how anyone
feeling any guys who are challenging to
use it hey i got my worth of the
presentation at least 10 guys interested
so feel free if you have like questions
i will put the email at the end and the
spec benchmark code depending on your
work if you have license then the source
code everything is free it's available
with a kid and otherwise you can come
talk to me and we can see you what
examples and think could be shared for
the full joint we might try to put
actually a white paper if that will help
with more examples on in this case
because we do think that it it does when
it works it worked great but it does
require some tuning it's not like outer
box it unless it is a toy case you
create some easy matrix multiplication
those type of things it does it's
actually does require some tuning does
require some riorca tech ting does
require knowing your dependency part so
now there were some other things we did
so those are not jdk 7 related i wasn't
sure how the time will go either we
could go in the question answer or in
two or three minutes i could just show
you the couple of other things we learn
from the benchmark and those are let me
just go through two or three minutes and
then we'll go questions so we have on
bounded queues or the sampling buffer
load of them being used in other
benchmark for like message buffering and
voice installment purchase or oshiro
data mining and what we found that there
were scaling issues those things were
not scaling other than once the for
joint was working several other thing
but not scaling and so we in our
benchmark we have just buy property we
can change by using the factory method
what type of q's we want in those
structures so we tried the concurrent
link c lq concurrent link you we tried
the link blocking cube we try the
striping of them and we found that if
the transactions are big so we have many
report on Jekyll sums are very light
others are very heavy so if the
transactions are light then you will
find that concurrent link you or the
stipes are better the otherwise
otherwise we didn't see any difference
in them with respect to the concurrent
collections we could have the queue
sampling buffer which it blocked and you
pull out of it or so or for other
buffering we are using circular array or
we could use a random array what we
found that there was no visible
difference between any of them unless
your transactions are light then random
array works off with the best the next
one is circular array the queue sampling
buffer becomes terrible it becomes a
bottleneck on the performance if your
transaction is very light whatever item
you're putting in and out very often q
sampling before was terrible in terms of
scaling and we have to go actually to
the either the circular arrow random
array so don't think it would once you
start implementing with a fork joint
pool etcetera is always the folks on
pool fault many time is actually these
arrays and buffers who are not scaling
now just the summary that we try to use
I think most challenging one with the
fork join to use it and we i think i
would give ourselves a great just we
didn't solve the very thin lightweight
when the interaction is very often and
high frequency one it might be important
for some and we will try to probably
some other
time try to see what's going on there
but I think we did pretty good job on
solving in a pretty complex environment
pretty real case communicating in a
enterprise level java using not just a
toy and it is skills well when it need
it does need you to be at one medium to
advanced level programming the because
you do need to know your dependency you
do need to know your you're not causing
that lock and how to do the manage block
because it did require us almost a month
on the complex problem to debug and go
with the alternate solution on the part
I think with that this is my email and
you feel free if you have questions and
we do plan to put white paper in this
area looks like but interesting at the
rate people want to use it we want it to
scale well for the multi multi threaded
architectures more and more coming and
we do want it to succeed thank you
everyone and I think I'm open for the
questions or</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>