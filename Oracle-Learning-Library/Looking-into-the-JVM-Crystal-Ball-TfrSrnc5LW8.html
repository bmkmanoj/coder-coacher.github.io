<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Looking into the JVM Crystal Ball | Coder Coacher - Coaching Coders</title><meta content="Looking into the JVM Crystal Ball - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Looking into the JVM Crystal Ball</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TfrSrnc5LW8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone and welcome to Java one and
the session looking into the JVM crystal
ball we're going to talk a bit about
sort of the vm strategy and what
technical things we were working on now
and in the future so it's taking the
keynote yesterday and drilling down a
bit further into what it's how it
affects the vm and our roadmap going
forward some stuff on freeway I'm the
JVM product manager at Oracle so I'm
focusing on hotspot and on J rocket and
with me i am michael whitsett who's the
JVM architect all right we're going to
talk about the future so we need to have
this slide and I'm gonna start off with
a sort of a brief introduction in what
we see as the vm strategy and then
meekly is going to spend most of the
time talking about the technical roadmap
and and what actual projects are we
doing since this is a technical project
that's or technical conference that's
where we're going to spend the most
focus and I'm going to try to make
myself scarce hear the vm strategy and
one thing we talked about we started
talking about this basically and right
after this on our concession at the
first shower one was vm conversion some
sort of focusing resources and we were
working on that with jay rock and hot
spot our goal is to have a single
best-of-breed vm and the way we're
aiming of doing it is moving features
from Jay Rockets over into a hotspot and
the main focus has been or are or is on
serviceability features and a
performance and performance wise we feel
like we're there today with seven you
for update we had a talk earlier today
one about why to switch to jdk 7 and
what we see with the later seven updates
from 74 and laters and all the tests
that we are running jdk 7 compared to
jdk six and j rockets are so faster in
basically 90 sort of more than
ninety-five percent of the test cases
were running so
we're pretty pleased with that I'm going
to drill in the mid photo what we mean
about serviceability features and what
we're doing in that area and during the
last year we're also sort with another
convergence project which is sort of for
the vm but also in a greater scale for
Java ME and job SC we want to make sure
we're bringing these two areas closer
together making sure you have sort of
the right round right ones run
everywhere type of cross device
development here and today Java ME is
142 and you don't have all the features
you have in your etsy and you have
libraries and Java ME that you don't
have an SE and we want to blend those
more so drilling in a bit further into
the embedded space and one what to do
there and what I'm basically was saying
we won't feature parity between Java ME
and java SE what does that mean for the
vm is sort of we want the vm features to
be available in the embedded systems
that we have on the SE side we want a
tooling the management capabilities to
be available so a lot of those things
are areas that we're looking into how do
we sort of scale those things down
because a lot of these have been
developed for the client space or the
service space and the embedded space
things are your smaller we need to make
sure we have a wish efficient memory
utilization with already internal data
structures how we're managing code etc
more feature specific there are also
similar in this CD CD C or Java a meat
we have features available there that we
want to bring over to the hotspot
embedded in the replacement one of those
is the Roma sation of application code
and you can view this as similar as
ahead of time compilation but taking a
step further you're basically binary
packaging the vm together with all the
application code and the idea is to
avoid sort of the overhead of compiling
and also the size of things
so serviceability is the word we're
using internally and we have a specific
team within within the jvm group that
are focused on serviceability and it's
basically the way we the part that we
call when you introspect or do analysis
on the vm how do you extract information
how do you visualize that information
and get access to it and today you have
verbose logging in the vm it's really
powerful and we recommend using it we
recommend basically using it in
production environment with the overhead
is pretty low for it and the downside we
have today is we don't really internally
in the vm have a common infrastructure
for it so you have different
capabilities depending on and different
flags depending on which part of the vm
you're logging like the GC has its part
the compiler has its own so we want to
make sure we have a unified system there
we want to make sure we reduce the
number of flags that you that you need
to memorize and then simplify the
options that you need to use to enable
part 2 to enable logging and also by
request people's they are using splunk
or other automatic tools to parse
verbose output file and we want to make
sure that we're providing a format
that's easily possible automatically as
well native memory tracking is a feature
we haven't yet rocket that we're
actively working on now to get into hot
spot it's basically allowing you to
understand what's where is the native
data structures or the internal data
structures of the vm what's being
allocated how much is being used and for
what so you can actually see like the
class class data part of the JVM the
heap and other data structures and then
we grew up the main into into different
categories
another part of the j rocket product
that we really proud of is what we call
j rocket flight recorder and j-roc
admission control where those are
getting renamed into java flight
recorder and Java Mission Control as we
port them over and it's basically sort
of a low-impact way of continuously
collecting fine-grained information in
production so the performance impact the
goal there we have is less than 2%
impact and using this detailed
information rather than sort of let's
say you you're running your server you
have an ulta transaction takes longer
than you expected it what can you do
well you start up and start doing some
profiling of your application the
problem is you're starting your
profiling after the issue cards so you
might not have the issue while you're
profiling and with fight recorder since
it's running continuously what we want
you to be able to do is actually look
back in time see what led up to my
longer transaction and actually sort of
see what happened during the longer
transaction and before and we provide a
lot of a complete tool chain around this
and this was a license feature in the a
rocket it's going to continue to be a
license feature in in the Oracle JDK and
it's going to be part of the license
that we call java SE advanced
in the enterprise space and also in the
Clarion space but call it out here is
specifically for server Java one
products is always been on our mind is
basically just say Java we want to make
sure that you don't need to tune in sort
of as many cases as possible one there
are multiple things we're doing here I'm
going to highlight a few of them one
part is removing the permian removing
the artificial limitation that we you
today have around the native data
structures in the vm that end up on the
permanent generation and this is
something that we're targeting for JDK 8
and it's on display their maker will go
into more details into that project
we're also looking at how can we help
reduce the complex tuning of the GCS if
anyone here is running CMS it quite
easily start filling up so if not the
sort of a full paper with command line
options you yourself at least halfway
there quite quickly when you start
tuning it and we are working with t1
we're hoping to have that as GC
framework going forward simplifying it
by allowing you to set the post target
instead of doing fine tuning of this all
the different aspect of the GCS and
letting the ergonomics and the JVM
handle that for you and today you also
have a different JVM if you're running
on the client machine or if you're
running on a server machine we would
like to see having a single binary and
having a multi-tiered compiler that
allows you to optimize the code in
multiple steps so we have a part of this
today with third component air
compilation and we want to make sure
that we're moving for with that and
having a single binary ranging from
client all the way up the server
other things in the enterprise that
we're looking for for is instant
performance if you're running your
application today you're usually
launching it every time you relaunch
you're basically resetting everything
and starting from scratch again have to
sort of optimized re optimized methods
and we would like to make sure that
remove the need for a warmup period so
today you start with interpreter you run
it a couple of times we optimize the
code and you were able to run at full
speed we want to make sure that you can
run at full speed or as close as
possible to full speed instantly without
doing a warm-up and we will do questions
at the end is going to be easier with
with the sound sorry we're also looking
at low latency garbage collectors and
this is sort of what we see is sort of
extremely low latency it's generally
more in the financial secretary in some
cases the telecom sector where you're
basically after sub-millisecond poses
for most applications especially if
you're writing a web application 100 or
200 millisecond doesn't sort of really
hurt you but if you're in the trading
space for example even a millisecond
this can be crucial in when it comes to
sort of winning a trade or or being sort
of within the guarantees that you you
need to be able to be efficient on the
market another aspect in the GCC space
is big data requires big heaps it's
growing today sort of for different
reasons you do clustering and you want
to do clustering because you want to
make sure that you you have a way of
spreading the workload but you don't
want to have we don't want you to have
to run too many JVMs you like there are
limitations one part is the instant
performance with Hadoop for example you
have the way of restarting and starting
a new JVM each time you do a query well
it helps with the respect to sort of
reducing the
risk of having a DC force for example
but it's going to cost you in
performance wise because we'll have to
rewarm up everything so you're going to
run into the top here as well and so we
want to make sure that we can handle
larger heaps without having to long
process and allow you to run with the
fewer JVMs reused to JVMs and have a
simpler setup for your big data and
another big thing sort of people are
talking about today and then both both
up here and down at open world is cloud
and virtualization and in other word we
tend to talk about the multi-tenancy and
the way we see this is basically you
want to make the most of the hardware
you have you want to make sure so if you
can maximize your application density
you want to maximize sort of the the
usage of the resources you have
available and from a user perspective
you want to be able to scale dynamically
you want to have dynamic or so condom on
availability if i have a new application
let me deploy it in here and if i get a
lot of users on it i have sort of the
ability to grow at the same time you
want to be able to do this the other
part is we want to make sure that you
continue to have high availability and a
complete isolation so there are a lot of
things if you are in a public cloud and
everyone is running their application
there you don't wanna leak information
to your competitor or someone else
running in the cloud and you don't
probably don't want to see their
information either with with the risk of
that entails so those are sort of
conflicting goals that we're looking to
and as i said earlier mika will go into
ideas we have for for all the different
areas that i bring up here
developer experience work continuing to
improve in this area multi-language
support is one of the bigger errors
we're working on we see that running on
the vm or running with java there is a
need for specialized languages there
there is a need for having new and
innovative languages java is a sort of
amateur platform we want to be
innovative we will be innovative them
but we will also do it at a slower pace
than what a complete a new language with
a sort of smaller user base can be so we
will in a sense cherry-pick the great
ideas as we're doing with lambda and
pick that up and bring it in once we see
that it is something that is getting a
good use and we can figure out how to
with fit it in to the platform without
breaking things for for old applications
or existing applications the other
benefit we see is with hot spot you have
a stable you have a high performance we
vm so we hope to be able to sort of make
sure that people can use the hotspot vm
instead of reinventing the villain wheel
in the sense and rewriting and rewriting
or writing a new vm and we're also
working on language interoperability and
it was shown a bit with with javascript
and also on how we can debug in
javascript and then switch over
debugging in java code and dynamically
switch between the two in the
development tool chain we're working
closely with ID developers with netbeans
two and an eclipse flap as well to make
sure that they as soon as we have a new
language feature a vm feature that they
can pick it up they have it available in
at the same time basically as we release
the feature we're also looking
specifically as how can we make
development more dynamic development and
debugging today you have for example the
capability of replacing the inner part
of the method and we're looking at how
can we extend that how can you actually
add fields and new methods and just as
have your application running and add in
these on the fly and continue to develop
and see how the changes just get
automatically picked up
so this is a bit of a high-level
overview of some of the bigger areas
that we're looking into and I'm gonna
head leave it to me Kel to talk a bit
about the technical things that we're
doing in these areas like hi everybody
so Stefan said I'll try to break down a
few of these requirements into the more
technical aspects of what we're actually
working on or are planning to work on
one thing I would like to mention first
though is the concept of a jab you'll
see throughout this presentation that
I'll be referring to jabs JEP s it's
short for the JDK enhancement proposal
and if you want to have a mental picture
around it it's kind of like a roadmap
item now it's a document and process
that we use in the openjdk context to
document the upcoming features the ideas
we have and all it's also a way to you
know form a basis for discussions around
those ideas this is something that is
used for the whole openjdk community or
project i should say on including hot
spot and it's not only something we are
oracle do so if it helps you can think
of this as the why and the what so why
are we doing this why are we proposing
this feature and what is the feature
actually around the Japs are public you
can find them at this URL and as I said
you'll see a number of these being
referred to throughout the presentation
so you can go read more about it every
Jap by the way has a discussion forum
attached to it so it's actually an email
list so if you're interested in you know
discussing that dig up the Jap find the
discussion forum and participate so if
you look at a couple of you know maybe
the biggest bucket of features we've
been working on the last year or two and
are still investing in its all around
convergence so Stefan mentioned it's
both in terms of convergence between the
j-rock jvm and the hotspot a vmware
where a cherry picking over
functionality from j rockets and now
porting it over to hotspot but it's also
convergence with the embedded systems or
the embedded jdk
our JVM is now able to scale everywhere
from as you see on the left side here
the Raspberry Pi device a small produce
oil for those of you who were in the
keynote yesterday you saw it it's a
credit card science to you know circuit
board it was blown up in on a big screen
so it may be look bigger but it's a
credit card sized thing ronson arm v6
chip it has a couple of hundred makes a
memory on it USB port and the network
it's a pretty powerful small device and
the nice part is that it can actually
run java SE as well and it runs hot spot
in the other end of the scale we have
systems like exalogic or for that sake
sparc supercluster where you have huge
machines lots of memory and you're
trying to run applications with lots and
lots of users so it's not you know you
can always ask yourself you know one of
the goals with our convergence project
was to make better use of our resources
internally and it's a fair question to
ask I think whether or not these devices
actually share the same kind of
requirements at all or are we just
trying to make the same poor JVM scale
to these extremes well so looking at a
few of the requirements we have the
maybe d biggest or they are the most
complex part of an embedded port or an
embedded JDK is around footprint and
making sure that we squeeze out the last
few bits and bytes out of the memory
we're using and it's obvious that that's
the case on the embedded side but if you
look at what's happening on the
enterprise side it's not necessarily you
know the machines have lots of memory
that's sure but the thing is that what
you're doing instead is trying to
squeeze in you know really push this
these systems with lots you know
thousands of users huge applications and
so even though they have a lot of memory
compared to embedded systems they are
actually memory limited as well it
really is important to make the best
possible use of the memory that is in
there as we go down the list here you
can see that you know almost any feature
you point to that you start looking into
you realize that whatever we do to help
the embedded space will actually benefit
the enterprise and vice versa and I
think it's not too far-fetched to think
that you know if it helps the i'm going
to call them extremes here the things
that are in the middle will benefit from
it as well so client desktop systems you
know the smaller applications so these
extremes aren't that different after all
and almost as if to prove that somebody
built a let's call it a mini exalogic or
something using raspberry pi devices and
lego pieces i I seriously like this
project and it's yeah it's amazing if
there are a number of Raspberry Pi
devices in there but take away again is
that you know or convergence projects
are not only making sure that the JVM
can run in these different environments
but also actually bet you know
leveraging the resources we have and and
it turns out that most of the
functionality most of the features we're
working on will actually have a benefit
both on the end price and the embedded
side it's drilling in a bit into the
footprint aspect and I think this was
covered to some extent in the keynote
yesterday as well we have worked on a
few different things to try to get the
footprint down and one of the things we
have been looking at this is the first
reference to a job so 147 in this case
one of the teachers is reducing the size
of the the structures we're keeping
internally in the JVM as you you know
I've add more and more classes you know
the lower big numbers tells you that at
some point that you know every single
bit and byte will actually start matter
muttering so what we have done here is
to leverage the the knowledge and the
skill set that are embedded team has
internally and made sure to apply that
to the hotspot code base so a few
examples here include things like taking
our class structures and looking at the
fields that we have internally in some
cases you realize that fields are used
in a mutually exclusive way as you know
either one field is used or the other
obviously in that case it makes sense to
merge those
because you know otherwise it's dead
weight it's around move you know moving
around re sorting fields to reduce
padding and in many cases is also a fact
that the Java specification and other
you know requirements or specifications
of different kinds make it possible to
reduce the size of the fields so you
know for example how many exception
handlers they can be kept / method and
things like that so it doesn't sound
like it much you know every single
optimization but it's when you combine
these and when you scale it up with the
number of classes that it actually makes
a difference another thing we are
looking at and working on is dynamically
sizing a lot of the structures we have
internally many of these are vm internal
and not necessarily easy to explain but
I think one of the things that people
hopefully can relate to fairly easily is
the string in turntable so if you call
string in turn the goal is to really you
know take your string and return unique
cure I should take common instance so if
you know one string object which you
know you will get the same string back
every single time you call this clearly
that needs to be efficient so internally
it's implemented much like a hash table
and the fan you know today in hot spot
the signs of that table is fixed in size
you can specify it during startup but it
won't change dynamically in runtime as
you can imagine on the you know embedded
side we want to keep that table very
very small to start with as a matter of
fact in general you want to keep it
small as long as you have good
performance on the enterprise side the
the string table may not even be big
enough like we over time you will have
so many interns strings that you want to
add this structure be bigger instead so
that's an example of a structure where
you know we can really benefit from
making it dynamically sized and working
with the system we have around us to
figure out how big that size is supposed
to be system dictionary the system
dictionary in hot spot is the structure
that keeps track of or maps a class
loader and a class named to the
short class object same thing there it's
a structure that is currently fixed in
size but which should be varying
depending on the application you're
running and we also have a number of
different caches internally like the
code cache for example where we store
our compiled code we obviously want to
keep that to a minimum while maintaining
the performance and on the embedded
Titus is clearly you know very very
important you have limited memory but
you still want to have the most hot
methods in the cache so cash management
of that kind and we have other catches
like that internally as well any bite
really counts in this world and again
we've done a benefit from doing these
optimizations not only on the enterprise
side but also on the unlimited and
everything in between sticking to to
memory as Stephan mentioned one of the
convergence projects that we have been
working on is the thing we called native
memory tracking this product where this
functionality was part of or is still
part of the job j-rok jdk and we you
know we before figuring out what the
memory is used for on the Java heap you
have a number of different tools I'm not
going to say that the problem is mostly
solved but at least there's a plethora
of different options and so you can use
H prof dumps and analyze the news in
Eclipse m80 for example or using Mission
Control and other tools like that but
the Java heap even though it's you know
hopefully the big part of the memory
there's still a lot of memory that is
being used elsewhere in some cases this
is internal to the JVM and you know you
can tune around things to free up some
of it in other cases it's implicitly
allocated by the JVM by your application
so you know class has some footprint on
the heap but it also has some footprint
in native memory native Native memory
tracking is all around keeping track of
what the JVM allocates internally that
is not on the Java eep and there are
couple of different reasons why we have
this functionality well what drove it
from the start I guess was
supportability and making sure that when
you run into cases where the vm for some
reason is using a lot of memory you just
want to drill down and see what so what
is it actually using this memory for is
it a you know a a case where you need to
rewrite your application to some extent
or it can I do something about it or if
they're actually a memory leak in here
somewhere but this functionality
actually turns out to be really useful
when you're humping footprint in general
so you can use we can use this
functionality to look at what the
biggest consumers of memory are and in
that way drive the footprint reduction
work in the vm so switching gears a bit
one of the other big features that made
it into jdk 7 was the invoke dynamic or
jsr 292 implementation and that was kind
of I'm gonna say the first initial
version of that work what we have done
over the last year since jdk 7 was
released is to two very large exempt i'm
going to say rewrite that implementation
it was very heavy on assembler code
usage which is not necessarily portable
as you know we'll know it was also
pretty complex and I think we learned a
lot by doing the first iteration of that
functionality what we've done now is to
rewrite a large part of that and
actually instead of having it in
assembler go the very opposite way and
have the largest part of that
functionality implemented in Java with
only a few intrinsic to actually help
you know implement it in the JVM a
number of our dynamic language
implementers have also reported problems
with no class found errors in the first
implementation and the new
implementation is has solved that
problem so I for those of you or working
in that space hopefully the problem is
gone if it's not please tell us
we are continuously working on improving
this area on the belly especially on the
performance side I'll go into that in a
slide one of the things that may be
worth mentioning here is that even
though we've oak dynamic in the jsr 292
work is positioned as a dynamic language
feature our evil cunning plan somewhere
down there is actually to make use of
this for Java as well so for example
project lambda targeted for JT kate is
actually making use of the invoke
dynamic functionality and we expected to
make use you know even more use of the
invoke dynamic functional functionality
going forward so it's not not only for
involved for dynamic languages other
than Java but for Java as well one
feature for again for people that are
generating bytecode and are having
problems understanding why the JVM isn't
accepting it that is worth pointing to
here is the enhanced verification error
messages it's not a big piece of
functionality in itself but what it
tries to do is to just make the error
messages that the verifier spits out a
bit more user-friendly pointing to the
actual problem and hopefully helping
people that are again generating
bytecode worth picking up and we're
looking at if you're generating bytecode
in general I guess and one of the
products that we are working really
closely together with now is product not
sworn the JavaScript engine that is
implemented on top of the JVM where you
know in the same way that we're working
together with the Ruby people and the
iphone people it's good to have a few of
these different kinds of languages to
drive the both the functionality work on
the invoke that I'm excited but also the
performance work there so we were
getting really good feedback on things
we need to improve in the compiler and
then in the JVM some of which are listed
on this slide a lot of it is on the
compiler side not surprisingly perhaps
currently we're investing in three areas
i'm gonna say in the comp
whether to improve this this is not only
for multi language but it's been you
know driven by multi-language we were
expecting this to have impact on you
know good performance impact on job
applications in general as well the
first one is in lining I'm going to
argue that in lining is one of the most
powerful optimizations that the JVM does
which is also very no significantly
harder to try to do in a static language
like C and C++ our in liner today is
somewhat limited in that it does all the
inline work upfront and then the rest of
the optimizations are applied to the
code what we want to do here is to turn
that around or slightly and enable the
compiler to do incremental inlining so
you know running a series of
optimizations and as we see the the
method you know changing if it's you
know small enough we can add more in
line to it run more optimizations and it
kind of been a feedback loop make sure
that we produce the best code possible
we also have a few hard cut up cut off
limits on the in lining in hot spot
today which were we want to revisit and
update so when you have in line the next
step is trying to do you know best the
best possible work on the code you have
one of the other powerful optimizations
which is harder to do in Java for sure
is escape analysis and so you know
analyzing how objects are used in making
sure that if they're threadlocal
essentially you can remove
synchronization on them you can you know
do all kinds of different things you
know moving things around to produce
better code and the last part which
happens to be very commonly used in
dynamic languages is boxing elimination
so this is when you typically wrap an
integral you know a lowercase int in an
uppercase integer in many cases those
are actually not you know there's it's
only way to pass parameters or return
values what
we want to do there is just to explode
them remove the the allocation have the
primitive represented on stack in the
CPU without that object completely
transparently and another area i want to
point to on the compiler side is the
compiler control and logging what the
goal is here is basically to enable you
to very deep in a very detailed way
control how the compiler actually
compiled code you know directives
pointing out that i want this
optimizations to run in this exact case
again driven by 22 from two different
angles the supportability signed and
making sure that we can reproduce
problems and the issues in the compiler
but also to I was going to say something
else really cleverly there but I'll
leave that for another time a couple of
other things I want to point out on the
performance side is that we're
constantly picking up new intrinsic new
hardware instructions in you see views
and things like that lately we have been
implementing vectorization leveraging
the simdi instructions in newer cpus and
we're also picking up you know the crypt
of instructions in your chips making JVM
transparently use those in in an
efficient way on the locking side we're
picking up some work from Oracle labs on
the content locking side so if you have
many threads competing on the same lock
we're going to improve the performance
there it's a collection of small
improvements things like improving the
spin back off mechanism where we're
threads are trying to take the lock they
back off waiting it for a while but also
taking advantage of the fact that if
you're trying to notify a monitor and
there's no waiter there then we can
shortcut that instead of taking the slow
path
on the garbage collection side our
investment currently is on the the
garbage collect the garbage first or d1
garbage collector side I'm you know this
is a topic you can spend hours on in
itself but let's say that for simplicity
it's a region-based collector we for the
longest time been riding on the concept
that most objects in Java die young and
therefore we've had essentially like one
part of the heap where the newly
allocated objects are and then another
part where the ones that survive for a
long time you know are stored the
problem we're seeing now with big data
and so on is that heaps are growing
tremendously big heaps are you know
being you know people are deploying
applications or Java processes with tens
or gigabytes of heap and at some point
the garbage collector will have a hard
time keeping up with that so what we
have done is to implement this using
regions you can think of it like you
know instead of having two pieces or one
piece of heap it's hundreds or even
thousands or smaller regions and the
garbage collector will automatically
based on heuristics choose which parts
of the heap to garbage collect at a
certain time what this does is that it
you know what we're trying to do is to
make sure that you don't have to tune
the garbage collector and that it tries
to do all this work on its own which is
much easier if you have a more fine
granular way of choosing what garbage
collect realistically they will which be
cases where you can outperform whatever
the JVM tries to do but you know our
goal is to make sure that in the common
case you're not you don't have to or
that there's only a you know a a single
dial you have to turn our initial focus
on g1 has been on large heaps and sixty
jake heaps and up to 32 or even beyond
that with some reasonable post times so
in this case reasonable is defined to
approximately 250 500 milliseconds what
we're working on now is to widen that
set of
use cases we believe that there are
cases already today where g one will
outperform the other collectors but our
goal is to make sure that that set of
the applications is wider and wider the
long-term goal here is to try to cut
down on the number of garbage collector
we're supporting in hot spot we have a
number of them today the first one we'd
like to you know look at is CMS the
concurrent marking sweet collector
clearly we understand that there's a lot
of work to be done here to identify
which workloads where you know CMS today
outperforms the other collectors and
make sure that we have a story in place
on top of gee wonder so we will take
some time until we get there we're very
interested in your feedback now so I'll
keep keep coming back to that as well
please help us pick up builds that we
have today since of jdk 7u40 one is
supported you can run it today
specifying the XX used g one g c flag
and we we love we'd love to hear your
feedback on what works and where we need
to improve think a couple of things
worth pointing out where we are looking
at implementing the new malware
allocation path for d1 and g one is also
the collector framework will be building
our low latency GC on top of
so on the memory side still I guess one
of the biggest project we've been
working on is the perm gen or permanent
generation removal somewhat simplified
the memory memory in the virtual address
space looks like this in a JVM or in hot
spot pre this product so you have the
Java heap you have the permanent
generation and you have some native
memory on the side the problem with this
is that the permanent generation is
again static in size you can specify
cheering startup it's not changing once
you've started up and in many cases that
has you know users and the operators are
seeing this java.lang out of memory on
the permian space side well what we did
now is to realize that it's really hard
to tune three different pieces so we
have removed permian it's the code has
been checked in now so it's only a
matter of getting it out in the release
the data that used to be in the
permanent generation is now either on
the Java eep or in native memory so at
least one less thing to tune there are a
couple of flags that are now with
essentially no ops the perm gen flag and
the max perm sighs flag and those are
ignored in jdk 8 so this product is
fairly big the nice part is that it's
too transparent to the user no new API
snow weird things you know you have to
change your application to make use of
it or is it well the problem here is
obviously that it's a big project we
have changed a lot like things have
moved around and we noticed in jdk 7
that in some cases things that we
changed ever so slightly that's still
fulfill the spec did actually you know
I've were visible when it came down to
it so again we need you to help us you
know find the problems the potential
problems in this area by picking up the
builds trying them out and just getting
us feedback on if it worked or not I'll
give some pointers later
so another convergence project that
stuff I'm pointed out is the Java flight
recorder functionality keeping track of
events that are happening in the JVM and
ask things you know as your essays are
breached getting that log and being able
to analyze what led up to the problem
internally this is implemented much like
a i'm going to say cyclic buffer we
store these events in the JVM memory and
you can optionally actually store these
events to file as well the goal of the
whole project is to make sure that the
overhead of having this default on is
negligible that you should have this on
in between production we are fully
supporting it and you know you can dump
things out when you need them so the
internally in the JVM we have that but
the log itself isn't really worth
anything you clearly need a way to
visualize this and analyze what happened
and so here's where Mission Control
helps we have very powerful presentation
of this in the Java mission control user
interface so on project that we're
really proud of as stuff and pointed out
on the developer simplification or
developer story side one of the things
we want to make sure to simplify here is
the the redeployment or actually kind of
development testing cycle if you're
writing your you know a big application
today or big you know suggs changing
some code somewhere in many cases you
will have to really reboot the whole JVM
or redeploy some whole you know the
whole stack somewhere some of the
functionality that exists in java.lang
instrument and JB I'm JB MTI today
allows you to change the code inside of
a method but it's very limited in that
that's kind of the only thing you can do
and what we'd like to do here is to
enable that functionality to change
things that are associated with the
types for example classes adding
removing methods whole methods not only
the body of the massive methods but also
adding and removing fields
adding interfaces to existing classes
and things like that and being able to
hotswap this during runtime very very
cheaply very very quickly also work that
is being that has been developed by
Oracle labs and that we're now looking
at adopting one of the areas i
personally feel very excited about is
the heterogeneous computing this was for
those who were at the keynote yesterday
we have a you know a new product in
place to see how we can leverage the the
more and more diverse computational
units not only the CPU but the GPU and
things like that going forward in Java
one of the things I think has changed
somewhat over the last 10 years is that
we've been blissfully you know unaware
of other kinds of devices we've been
focusing on cpus and felt that that's
what runs java but with the sudden
explosion here of GPUs you know
basically I you know you have GPUs all
over the place only they're very
powerful and they're more available than
they've ever been in the past and I
think we are also getting to the point
now where some kinds of data parallel
problems or data parallel computations
actually benefit from being uploaded to
the GPUs and what we're trying to see
here is what we can do to make that that
story work well in the java space
project sumatra is the name of the
product in the open data context where
we're working on this and tightly
associated with this product is the
concept of our race todaro these the
name of the product or the name of the
concept is attributed to John Rose I
encourage you to go look at his
presentation from the JVM language
summit on this topic somewhat simplified
it's around realizing the fact that GPUs
may not want to treat data in the same
way as CP us do so we have to have a
common story on data sharing or an
efficient good story on data sharing
between CPUs and GPUs
and also subtlety is like agreeing on
the memory model which is really really
important we want to understand what
happens when a GPU actually update
something how's that visible to the CPUs
and also on hard problem to solve
complex problem to solve that's
something we think we can do almost
completely transparently in the JVM okay
as Stefan mention also one of the the
things we'd like to improve on is the
the logging framework in hot spots so we
have a number of different login
frameworks today one of them is you know
the most powerful and maybe most used
ones is on the DC side but there are
also logging this logging functionality
on the compiler side as well and so on
what we want to do is to merge all these
together to have a one unified login
framework for all the components in the
JVM and to also make sure that these
logs can be easily parsed by logging
frameworks like splunk exactly thanks
and that they come with you know a log
rotation think things that you kind of
expect out of login frameworks I guess
another goal is to make sure that we
don't have any interleaved messages it
turns out that combining all these
requirements yeah I see people cheering
in the back there combining all of these
requirements is really hard but I guess
that's why we're paid a good good money
so please give us feedback on what you
would like to see out of logging there
again there so Jeff there's a discussion
forum we need to hear from you
going over to clouds so the both good
and bad part about clouds is that
they're making things way more flexible
the goo you know you can imagine the
good aspects of that you can also
imagine the challenges of trying to make
good use of that i we I think we all
know that the JVM isn't in a really good
position to help out here we already to
some extent virtualized the environment
you're running on and even though we're
now in many cases adding a new layer
with more virtualization that's exactly
what a JVM is good at helping out with
so suddenly everything can change
everything from how many CPUs you have
how powerful the CPUs are how much
memory is available and even the network
bandwidth depending on where your vm has
migrated our goal here is to make sure
that the JVM can pick up these changes
change things internally in itself you
know that cache sizes that number of
threads that are garbage collecting
things like that but also to to help
inform the rest of the system about
these changes and at the same time one
of the things we want to you know one of
the challenges of doing this is that we
want to maintain the isolation so if you
you know one of the reasons why people
use clouds I guess is to get more and
more users on to the same hardware to
really make good efficient use of the
resources but there's a trade up there
you know either you get sick you know
fully isolated secure environments or
you you have maximized sharing without
the isolation those are the extremes
clearly we want the separation but we
need we want to share as much as
possible as well so you can imagine that
if you had an infinite amount of time
and resources and you spent that on you
know you'd end up in the kind of the
same place in the middle question is
which end you startin and what you know
how do we implement that so we're
evaluating the different options here
and some of which we already have today
it's not like we're starting from
scratch in the cloud space
we already have class data sharing in
hotspot a way to store the the
initialized or you know class metadata
onto disk and for subsequent JVMs that
start up they can reuse that same data
and transparently share it between the
JMS it the class data sharing
functionality is currently limited to
the serial garbage collector in the
released versions but one of the nice
outcomes of the permanent generation
removal project is that it now suddenly
automatically actually works on all the
collectors so even though we you know
change that was not the primary purpose
of the permanent generation rule project
it happened to be a good effect of it
and it's also currently limited to
system classes which is a thing we were
looking into relaxing going forward and
I think the second to last slide is on
have ahead of time compilation so this
is tightly coupled with I guess class
data sharing there are a few different
things we want to do here one is to
reduce the startup time of your
application one is to reduce the warmup
time of your application closely related
but different one inside here is
obviously that if you are redeploying
restarting your whole application your
week you have to recollect or you know
collect all the profiling information
again clearly would be better if we
could recollect that so we're looking
into this evaluating different options
and together with class data sharing
again we you know that's where I think
we have a good story in place sharing
both the class memory and the code
so as I mentioned one of the things we
would really appreciate your help with
is on helping us you know with feedback
on how things are working and also just
trying things out both on the garbage
collector side on the permanent
generation removal side we now have a
fully functional implementation but we
are aware of the fact that in some cases
your your use case will you know be
different from what we expected so if
you you know one thing to take away from
here is that if you could pick up try
out just report back any issues you find
or you know ideas on on improvement that
would help us tremendously there are a
couple of different email lists you can
use hotspot Deb is for general
development discussions we also have a
list for garbage collection feedback
hotspot ECU's we are publishing as we
said in the keynote yesterday as well
jdk 8 builds on a weekly basis those
builds i think since of this week or
potentially next week will contain the
permanent generation removal so please
pick that up and if you want to help us
develop it we're looking for developers
we're hiring so either work on the code
base outside of Oracle or for that sake
join join us in the jvm team that
concludes the session or presentation
I don't know what what time yeah I'm
okay five minutes so few questions
party application
right so the question is the permanent
generation the data there has been
moving into either Java heap or native
memory and what what are the where did
we move what I guess so somewhat
simplified we actually have the
structures the most the key structures
that we have are actually represented
both on the Java eep and in the native
memory but different parts of it
obviously so for example a class you
know the part of a class that is used by
the JVM for various purposes is on the
native heap and the things that are were
used on the Java level to do you know
instance of checks and client you know
type checks of different kinds how a lot
of that has actually moved on to the
Java heap there's so we're what we are
going to do by the way I think we
actually sent out a mail last week about
this as well which describes a bit more
in detail what the permanent generation
removal actually implies and what which
pieces you need to think about when
picking that up so look at the I'm gonna
say the hot spot that list for an email
on that and will also follow up on
documenting more about this before we do
make the release
oh so the question is when do we have
when can we get rid of XM acts I guess
right that's right we we would love to
get rid of that parameter as well I
think we can you know slowly slowly get
there the biggest problem you have as a
JVM is to figure out how much memory
you're actually allowed to use in some
cases you know we're hoping that the
memory that is on the machine is ours to
use and we make the best use possible of
it in other cases you have tens of JVMs
or even tenths of other processes that
are also competing for the same memory
so the challenge here is clearly to get
some you know get make use of
information you don't have or for that
take invent something that you know the
you're kind of on the operating system
level where we can get some help in
identifying which parts of the memory
we're free to use so where we want to
get there it's part of the just a Java
mantra gets short term I think we'll
still unfortunately have to live with MX
for some time but we're getting there
and in many cases I think you should be
able to to do without it
hockey
Oh
right do you want to take that so take
technically speaking we probably want to
take the term a CTC and move it into hot
spot but we're definitely looking into
the areas as i mentioned of low latency
DC and as we do that since it's they're
going to be a fairly specialized feature
targeted for a niche market it's we're
most likely going to have it as a
licensed feature g1 and and sort of CMS
or replacing CMS with g one that's not
something we see as as doing as a
closely to our license feature but if we
spend effort in time that's benefiting a
niche market or specialized market then
we'll probably have then will most like
to do it as a licensed feature project I
wanna load
different jars for
standard in the world
so is there anything is standardization
happening around modules as acid ace
today with jar files etc so the thing
that's going on now is mentioning he
noticed a jigsaw product that's sort of
what we're looking at in the jeddak a
space and that's targeted for for JD k9
as yesterday yeah this
so I could be warned so where the
question is the EAA es and I crypto
instructions is that targeted for seven
or eight I actually don't have the
answer to that today where where yeah
we'll have to get back to you on that
now I
so I guess the question is is tiered
compilation tying into the story of
instant startup I think it can be where
again looking into what exactly we you
know how we want to implement the quick
start up the warm-up time all of these
things and tiered is definitely you know
the some kind of teared is definitely
part of that story and as you probably
know tiered we we have it in the code
base you can use it today if you want to
we have chosen not to turn it on by
default yet but that's something we're
looking into as well
so when's to fully converge jvm going to
be available stuffing thank you know so
it's it's an iterative process we some
things are already in we're already in
the jdk 7 GA a few things I've come in
during seven and more things will soar
targeted 447 leases and we waiting to
hamids on in the jdk timeframe adequate
yep so it's something that we're sort of
investigating currently trying to scope
what the effort is and what the
requirements are so it's it's in the
early stages of figuring out how do we
solve this problem because it's it's far
from an easy problem trying to get down
to sub milliseconds right thank you so
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>