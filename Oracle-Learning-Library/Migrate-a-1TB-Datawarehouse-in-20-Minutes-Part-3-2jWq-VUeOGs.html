<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Migrate a 1TB Datawarehouse in 20 Minutes (Part 3) | Coder Coacher - Coaching Coders</title><meta content="Migrate a 1TB Datawarehouse in 20 Minutes (Part 3) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Migrate a 1TB Datawarehouse in 20 Minutes (Part 3)</b></h2><h5 class="post__date">2011-06-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2jWq-VUeOGs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the Oracle real world
performance video series this is part 3
of my great I one terabyte data
warehouse in under 20 minutes with
Andrew holdsworth senior director of the
real world performance team we're going
to focus next on the resource management
and actually doing the daily load and
transformation of new data as it comes
into the data warehouse one of the
biggest issues will be faced in a big
data warehouse and on a big database
machine is learning to use the resources
effectively and one of the most
interesting first tools to use when
managing resources is the use of
database services to isolate different
workloads on two different nodes so we
may have a database machine that has
effectively got all eight nodes
dedicated to queries and then in your
maintenance period you may want to
allocate specifically one to data
loading and leave the rest for querying
this is just an example of the type of
resource management let's talk about
validation and transformation of the
data we'll go back to the screens we
showed before and we'll start initiating
the load virus service on to a database
machine you can see the previous run
that we went through on enterprise
manager you'll notice that the data load
and the statistics are gathered and
we're back to the menu so let's start
off the load of one day's worth of data
straight away using service d2 on to the
database machine and if you look at the
top right window you'll very quickly
notice that no two is suddenly becoming
extremely busy and as we perform the
data load what I'd like to do next is
talk about validating the data and
transforming the data and really
important part of this is to learn how
to use set-based techniques and
parallelism and use the horsepower of
the database machine when we staged the
data into the database we can do very
simple data integrity checking looking
for columns that should be not null
specific data types and rules etc
however having got the data into the
database we need to do some more
classical checking looking for
uniqueness checking that they're not
foreign keys that are dangling orphaned
records and some business rules and then
finally we would like to transform some
of the data fairly quickly and do
business real world type transformations
on the data now traditionally many of
these processes have been done by ETL
tools where they pulled all the data out
of the database manipulated the data and
pushed it back in this case we're going
to show basically referential integrity
checking being done as a set based
operation and likewise transformations
of data being done as a set based
operation done in parallel so let's go
and see it as you can see we've already
loaded the data and this was
representing we've loaded quarter of a
billion rows on one node of a database
machine in about one minute 28 seconds
or just say a minute and a half to we
have this quarter of a billion rows
staged into staging tables and so the
next maneuver for us is to validate that
it's valid so what we're going to do is
run in a series of queries that
basically count the number of rows to
check that we loaded things correctly
we're going to look for duplicate
transactions and then we're going to do
a series of outer joins checking that
there's no dangling records on the
foreign key references and we will be
doing this very query rather than
imposing a constraint you can see very
quickly we're making the Machine busy
and you can see the counts really take
no time we're basically doing a query to
check for the duplicates and now we're
straight into the parent-child
relationships you can see again we're
scrolling through the machine is getting
busy and with literally checking quarter
of a billion rows what in what will be a
very short period of time and really
what I ask you to think is we've just
done referential integrity checking for
a quarter of a billion rows in a total
execution time of about 27 seconds how
quickly
you validate your data after doing this
of course when we can put the
constraints on the data with them being
trusted because we check them the next
area we want to really look at is
transforming the data having this data
and staging tables it will be good to
copy them across into new partitions
prior to them being exchanged into the
main database and what we'd like to do
is a series of simple sequel functions
that are really quite valid in every
case so what we're going to do is run
three transformations one in it on each
of the big tables that we've just loaded
the first one will be a simple sequel
function which is basically just upper
casing the first character in a field
the next one will be be doing a case
statement on the select statement and to
effectively enumerate the payment type
into the database and then the last one
will be doing a transformation with a
lot more complexity on the biggest table
of the lot the line item table and to
normalize out local tax codes such that
the database and the data warehouse has
a uniform value of what everybody has
been paying for the products in the
supermarkets and normalized out in terms
of local sales tax codes which may
change depending on which zip code you
are in etc so let's put that in motion
again the important thing to notice is
we're making the Machine busy all over
again a brief period but the really
important thing is to notice it's done
as an insert as select into a new table
and notice the number of rows we've just
modified 15 just shy of 15 million rows
in 11 seconds we've got the second
transformation you can see that well you
we have a case statement based on cash
check etc and again 15 million rows
transformed in just shy of 15 seconds
and now we've got the last sequel
statement which is basically normalizing
out the local tax codes or v80 depending
on which part of the world you're from
and that'll be working on
a just shy of quarter of a billion rows
thing to notice at this point notice how
busy the CPU is on the database nodes
we're fully utilizing the whole machine
at this point it's now winding down and
the rows have been copied across and
you'll notice that we took us just under
30 seconds to effectively normalize all
the tax on the whole data warehouse for
that day and we've now done that so we
spent a minute modifying the three big
tables again I ask you to think how long
would it take you to modify quarter of a
billion rows in your daily data cleaning
on cleansing process so the final part
to do this is to then just gather the
incremental statistics which we spoke
about in the previous presentation and
put the data into production and you can
see that the statistics gathering again
is making the machine busy for a very
brief period of time because we're
running on such a reduced amount of data
than we run before and very quickly you
can see the statistics are coming out in
a matter of seconds and by the time I
finish this you'll notice that we've
spent a total of 19 seconds and updating
the statistics on all the tables on the
last quarter of a billion rows loaded
okay now that we've transformed and put
that data into the new new database as
it as today's daily load I just like to
talk a little bit about this concept
asset based processing versus row by row
processing in fact one of my colleagues
Tom kite doesn't call it row by row
processing he calls it slow by slow and
in fact just to give you an idea of the
sort of speed ups available when you use
set-based techniques correctly and when
transforming data and large sets of data
remember we changed every row in every
day every table we loaded and also
validating the data we have a simple
case we ran the in fact the store
validation and the product validation in
the sort of more conventional ways
we see people running serial PL sequel
programs or Java programs where for each
row read they then go and do their own
look up to see if the story is valid or
the product is valid so it's very often
a PL sequel loop and they take the the
row and then they basically initiate
another query using the contents of that
row to validate it we actually wrote
this up and we actually even created
additional indexes on the stores and the
products and we measured how long it
would take on exactly the same piece of
equipment and the point to notice you
will see that for the store validation
instead of taking 1.9 seconds it took
seven minutes and the second one that it
becomes more extreme if we look at the
product validation on the line items
remember there was quarter of a billion
Rosa it took two and a half seconds to
execute as asset base query but as a
serial query where you're effectively
doing your own nested loops join inside
PL sequel it took an hour and 46 minutes
again this is the same technology being
used to run it it's just a difference in
approach and when we have a very
powerful parallel database machine set
based processing is the key to handling
large sets of data for both validation
and return for transformation so it just
gives you an idea of the numbers that we
can be achieving through this</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>