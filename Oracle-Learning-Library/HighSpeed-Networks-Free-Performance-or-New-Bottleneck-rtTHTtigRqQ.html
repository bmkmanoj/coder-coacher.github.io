<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High-Speed Networks: Free Performance or New Bottleneck? | Coder Coacher - Coaching Coders</title><meta content="High-Speed Networks: Free Performance or New Bottleneck? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High-Speed Networks: Free Performance or New Bottleneck?</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rtTHTtigRqQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so first of all thank you for coming I
have to admit when this topic went in I
thought that I might speak to a room 220
people now hopefully by the end of this
talk it won't be down to 20 people we'll
see though I'll try my best but may feel
a little bit off topic and as I get into
it you'll see that it's really more
about and certainly in the context of
high-speed networks but it's really more
you know ways you can screw things up
and ways you figure out that you screwed
things up and how to make them better so
and I'll talk about that in a second so
legal might the company I work for IBM
would appreciate it if you did not hold
anything that I said against them so
first a little bit about me I talked a
little bit about some of my speaking
disasters for some of you that came
earlier I've actually been a JVM and
well interpreted languages guy for about
15 years now working on a wide variety
of things at ahead of time compilation
you know byte code verification
interpretation mainly garbage collection
mind you although most recently I've
also been looking at things like
multi-tenancy and whatnot you can see
here that clearly I was just coming off
vacation because I look all smiley and
happy and well-rested and everything
whereas this is the usual working
working Ryan and as I told some people
earlier my name is Ryan Shambo Coney
that's how it said I won't quiz you on
how to say it at the end and I'm going
to claim that I say it right just the
way my family happens to say it so what
what should you expect from this talk
well it's it's kind of there's really
two parts to it there's sort of an
introductory part that really lays out
the current state of the world where
we're at what you're looking at in terms
of high-speed networks it's not really
an in-depth thing you know I'm not gonna
go really deep on on networks in general
what I am going to talk about is really
you know a lot of the concepts what's
available out there various speeds and
things to kind of keep in mind then I
want to switch gears and say okay well
in the context of these high-speed
networks what happens when you actually
start to deploy software that tries to
exploit them and some of the things that
that can
happen to you so you might have guessed
this from my my bio page beforehand but
I just want to lay out the disclaimer
here right now I am NOT a network expert
I am NOT a hardware expert not even
close on either one of those counts I'm
a guy who with teams of individuals who
had a job to do had to ship working
software these are the roadblocks we ran
into and this is how we got over them
all right so that's really the context
and the view dope coming from so if I
happen to say things or say well you
know we kind of ran into this and we
didn't know what to do so we went around
it and you think you know the answer or
you've got you know more insight than I
do on that go topic come see me later
and be very interested to hear about it
but this is really me sharing war wounds
and you know of different you know heads
slapping moments when you look at your
code realize there's no way this could
have worked or performed very well so
most people who've done a computer
science degree of computer engineering
degree have seen this quote before and
always makes me laugh because I tend to
think about a station wagon full you
know complete with the wood paneling
down the sides you know hitting a big
speed bump and off into the air with you
know tons of tapes flying out the back
and there's sort of that really actually
you know there's a real connection
between that and the experiences I'm
gonna talk about here I mean how many
people have actually been on a system
where you've maybe been you know working
on single core or something like that
your codes work great and you know
something says oh well what's written on
a four core system or something that you
think no problem it's going to be
fantastic and it's terrible like it's
worse than single core anybody ever had
that experience yeah I ate crow once -
it was like terrible so you know the
funny thing is right now is that
networks you know are just a thing for a
lot of people it's like it's there it
you don't really think about it right
there's there are differentiators when
land wireless wired but but that's the
way that people think about it they
don't think about it in terms of speeds
or the problems that various speeds can
cause or say some of the api's or
facilities that are available to use
these networks what how do those get in
the way what do they do
I mean it's just a thing for most people
I need two computers to talk to it let's
open up a socket and that's the end of
so like I said earlier I mean you know
there's there's more to it than that and
you discover this very quickly once you
start to actually use higher and higher
speed networks that the bottlenecks
excuse me may well be in your code and
it may well be you know core design
decisions that you didn't realize that
you were making has suddenly affect your
performance in tremendous ways so you
know can it really the bottom line here
which unfortunately I apologize people
the back can't see yeah you know
basically look things are getting higher
in higher speed but is there something
that we can do to actually exploit it
and not just use it okay so there's a
difference right you know how do you
actually exploit something really make
use of it alright so just a little bit
of history first before we get going
network speeds have actually pretty much
been proceeding or getting faster and
faster over the years at a pretty steady
rate I mean if you go back to 10 megabit
Ethernet back in the 1980s or 1973 if
you really want to be technical about it
but you know we started out with a
certain speed that was like great right
you know anybody who used anything like
dial-up or so on just knows that this
was fantastic
and over time things have actually
progressed and gotten faster and faster
until we've gotten to the point now what
you know people in homes or what have
you you know you're typically
experienced nowadays it's a hundred
megabit or or one gig e rights to your
typical experience you at the office
maybe 10 gig II depends your office but
now there's something you know
InfiniBand which is actually even faster
now Ethernet is getting faster but now
that there's there's also InfiniBand
which is just you know miles or light
years light speed fast equivalent to
ethernet in some cases and so you know
networks are coming a long way and it is
actually changing and I'm actually lying
to you with this chart a little bit
because that's a logarithmic chart and
this is the reality of the situation now
so I apologize for people behind the
pillar
I mean if you look at the bottom that's
a huge leap for InfiniBand right that's
that's and I'm not even showing you the
fastest InfiniBand here I've actually
set up not numbers in order to prove my
point I've actually scaled things back a
little bit like
said Ethernet right now you know 10-gig
II you know hits out to about here
comparative to InfiniBand they are doing
things to improve it but InfiniBand is
just you know ridiculously off the
charts compared to ethernet now what
does that mean in the context of the
real world or everything else right one
thing you always think about is you know
disk spinning speed right never go to
spinning discs because it's just way too
slow right that's always kind of thing
anybody had the experience of taking
their existing laptop and shoving in an
SSD instead of their spinning disk yes
it's like a new machine right yes
especially with file based IDs and so on
so this you know and I haven't used the
numbers run such a wide gamut that I I
couldn't really pick one so I sort of
did a sawed-off between a bunch of
averages and kind of a commodity
hardware that you can say buy
off-the-shelf or what have you you know
InfiniBand is really interesting right
if you if you look at you know you know
one GigE right it's basically not even
on the map
but if you start to talk about ten Giggy
sort of you talk about InfiniBand right
I mean we're talking about SD speeds
here basically right not more at Penn
CSSD and so on and it's actually in the
picture of something as simple as a core
i7 right all of a sudden your memory
speeds versus your network speeds
they're not the same but they're pretty
darn close alright
this gap is getting smaller and smaller
and smaller so like I said before this
isn't a situation where you know you
just want to use it you just I say well
it's just another network it's a bit
faster whatever we'll throw our stuff
down and it'll just work the way it
always has we're getting at a point
where you want to take a look at how
your systems are currently running how
you're using networks and are there
better ways to actually use it right
Network distance isn't as far as it used
to be right so that's gonna change the
game right this is this is something
completely new and you know if you're
coming from an old you know you old
enough background like something like IP
X or so on right and you move on to
tcp/ip you'll you know things got better
but they're still actually pretty
terrible if you really look at it and in
the context yeah
oh okay it's actually been around so
I'll get to that we got a small history
chart when we when we move down right
now for boat it's about you know twelve
years old now at this point so where is
it deployed you know you're typically
gonna see it big warps right
IBM for example right Oracle Oracle does
some stuff with it too so on so yeah if
you do so sorry I should repeat the
question I'm like you know what you know
where is InfiniBand deployed so you know
it's not some general for some general
network that you just you know kind of
plug in at home it is expensive and it's
typically used by the corpse although is
starting to push down right Sartain get
pushed down so the chart should read my
own notes maybe I so this was a number
of different competing designs that end
up merging in nineteen nine now I'm
gonna take a very kind of quick overview
of it but you can think of it as it's
basically a cable so I kind of I joke
about this it's a cable between two
machines if you will right like you know
it's a network okay but it's different
it's a lot faster and the way you use it
is a bit of a game-changer
so it actually features a number of
really important things really really
high throughput okay you saw the chart
previously throughput was just
ridiculous
extremely extremely low latency and one
of the bigger things here that you end
up seeing it aside from the scalability
aspects is that it actually guarantees
delivery and quality of service okay so
you know you send something on
InfiniBand it's guaranteed to arrive
okay unless somebody of course you know
cuts the cable or something like that
but you know it's guaranteed to get
there it handles a lot of that machinery
another interesting aspect about it is
that it offers RDMA right remote direct
memory access what that means is that
you can actually have a chunk of memory
on your machine you know write a bunch
of data out to it and then say oh yeah
I'd like to write this in this other
machines memory space at this location
as long as both machines agree that
that's okay to happen you can basically
have to a transfer like this as
basically I wrote here and it's going to
show up on some other memory computers
memory
really cool really interesting you know
different way of thinking about things
now you can well imagine that you know
because of this and that's the really
the last point which is going to lead
into a couple of different charts here
is that how you use InfiniBand on you
know at the direct level is
significantly different than how you
might use sockets sockets are more
stream based right you're right stuff
and basically it you know it's a pipe
that basically still gets sent down if
you want to think about it that way
InfiniBand especially we start to talk
about RDMA is basically like no that
piece of memory over here is really
local to me and i'm gonna write to it
it's a bit slower than traditional
memory but it's really really darn fast
again as evidenced by by the chart so
there's no actual standard for using a
pin band there's a bunch of defect as a
de facto standard and there are ways for
upper-level layer protocols like TCP IP
and so on to use it and I'm actually
gonna get into that in these next charts
so you know these pictures here are kind
of generalizations and you if you want
to surf the web and kind of read up a
bit more about this you can actually get
far better detail and far better
explanation that I could possibly give
over the course of the next few minutes
but for InfiniBand if you wanted to use
it directly it really starts with you
know your application at the top and
then you basically use a bunch of
InfiniBand services okay
and this isn't like using a socket you
actually have to modify your application
if your architecture deals on a
streaming type basis you have to change
the way that works it's basically you
know write a bunch of stuff to memory
and then say transmit the way it works
is not through a socket API there's a
bunch of things called verbs that you
basically give actions to the InfiniBand
you have to handle your own cueing and
so on and so forth but it's all right
there for you so the services are really
kinda like this thin API to basically
talk directly to it and there's nothing
in the way this is actually really
interesting the kernel libraries and so
on are completely bypassed when you use
InfiniBand the really cool thing about
that means that when you're transmitting
stuff when you say transmit it now costs
you zero CPU except to say transmit
right there isn't a lot of work and
Machinery underneath to make sure things
got delivered divide it up into packets
and so on and fin aband of that for you
you say transmit and you walk
right basically zero CPU burn okay
extremely extremely low CPU cost that's
really real important when you're
talking about you know big you know big
software that has lots of things to do
spending a lot of time transmitting
stuff is not what you want to spend this
time doing just want to have it happen
and then basically just goes directly to
the device driver layer and so on so
right through very low CPU cost now you
can use tcp/ip on top of InfiniBand all
right this is called IP oh I be so IP
over InfiniBand it basically allows you
to use a socket layer API okay and it
you know this sits on top of TCP IP
which basically in turn then talks to
some IP o IB stuff that then says oh
sorry about that that's really
InfiniBand I'm going to do a bunch of
magic hide all the details of InfiniBand
from you make it all look like sockets
and basically use InfiniBand right so
the plus is is that you could
potentially get some of the speed
improvements from within event array
except that now you have all this stuff
and goop in the way that's faking things
out and not really realizing that if in
advance underneath you're getting you
know guaranteed of delivery services
from the software layer which is already
being done by the hardware layer and
there's a lot of CPU burn going on here
when you transmit something it's just
like using sockets for all intent and
purpose
all right just at the last moment that
goes oh sorry InfiniBand and switches
over now there's a third one SDP and the
SDP actually you can look up on the net
just look up Java SDP and there's tons
of tutorials on how to get this to go to
goo I think it's like a first hit right
away same idea right socket API soak a
traditional code might you know you just
swap on traditional code except that has
much thinner layer in between that
actually knows how to map the API from
here directly down to the InfiniBand
core and it you know although it looks
like a streaming API and so on it gets
rid of all the nonsense that goes on
here that's unnecessary for InfiniBand
and just patches you straight through
not as good as this but it's pretty good
trade-off and this diagram lies a little
bit SDP does make use a little bit of IP
like just in case you want to get
technical
but it's not really important at this
point right the important thing to
really know here is that SDP sockets
direct protocol should have said and
again look it up if you're interested
it's a very easy concept and simple
simple reads to kind of get up-to-date
it's a nice trade-off between you know
not having to deal directly with
InfiniBand and verbs and all the cueing
stuff and so on that goes on and being
able to use traditional socket API right
so I did want to cover just very quickly
before I move on to some of the case
studies throughput versus latency and
the reason why is that I often find that
you know you ask people hey do you
understand throughput versus wait and
see if they said yes and then you get
them to explain it and they almost do
and I admit I'm guilty of it too
sometimes I will say things that oh yeah
well you're late and it's like
throughput and I just that's cuz my
brain is like a spinning top but but you
know it's a little bit of remedial those
who're you who definitely know I just
you know bear with me for a minute or
two so imagine you've got a pipe okay
and you've got a bunch of data units
that you want to transmit all right so
each of these orange squares represents
a data unit all right and you want to
get that data unit down the pipe now
that could be anything from a bit or a
transaction or anything of the like okay
so we'll just talk in terms of data
units now those things have to actually
travel down the pipe to get to the other
end all right the time it takes from the
beginning of the pipe to the end of the
pipe is called latency perfect right
good so a lot of you already know this
so something an example of wait-and-see
might might say well it takes 10
milliseconds right for something to get
down failure into the pipe all right now
at the end of the pipe a bunch of stuff
is coming out all at once
all right bunch of data units all at
once the rate at which the data units
arrive per unit of time is called the
yay all right perfect maybe I didn't
even need to go over this so you know
for example the throughput is measured
as 10 gigabits per second per second
here okay simple enough all right if
that was a little bit too much for you
just take the shower analogy okay this
is usually one of the common ones right
you've got a showerhead like a pipe and
water comes out of the shower don't make
the time takes for a drop of water to
get from one end of the pipe to the
other is latency the amount of water
coming out at any one time is your
throughput so throughput versus latency
you know the famous the famous quote of
you can't always buy latency sorry you
can't buy latency but you can buy
throughput it's important to sort of
note these types of things
latency I mean the length of your pipe
is length of your pipe right is very
difficult to sort of short nothing but
if you think about sending data down a
wire or something you can buy more
throughput you can buy another cable to
plug in that Cable will still be the
same length right take the same amount
of time but you might be able to you
know push more more data side by side
down the pipe and if you go back to the
diagram that's sort of the same idea but
your motivations for either going after
more throughput or more better latency
are often going to be defined by your
own goals so take for example
transaction processing system right it's
gonna care about throughput typically
how many transactions who can squeeze
through per second whereas the one user
trying to get his trade through cares
about the latency right so it's gonna be
different motivations all right but when
you have something like InfiniBand right
or these super super high speed networks
you can start to think about things
differently this is going to lead into
my some of my studies studies examples
there's something you know the last go
here for something like our DMA right if
you've got such a huge amount of
throughput available to you it starts to
change the game a little bit for you
right instead of being really smart
about things honkies are going to
compress the data to make it smaller or
I have to be really careful about which
data I send or you know I wouldn't want
to prime the cache now the other one
because it's going to take too much time
to send all that data you have to worry
about it right now you can do things
like well just send the to whole page
yeah I sent a bunch of extra data that I
didn't need to you but I had that I had
the throughput available why not just
send it and then if it needs that other
piece of data that's lying right next to
it it's already got it right this
changes potentially the way you think
about your own local memory layout and
the type of data that you're
transmitting if you actually transmit
more than what you are right now maybe
makes you re-evaluate how you know how
you lay your data out to make it as fast
possible to transmit the thing and then
the next likely thing all right so
networks are fast and getting faster so
I'd appreciate it when I talk about
these examples if you didn't laugh too
hard at us but some of these things are
just going to seem ridiculously obvious
but they always do in hindsight right
you know you look at your code you're
like wow who put that lock there it was
you you know we've all had i've actually
stood up and criticized going like god
who wrote this
ryan it was you so the first the first
one's a really simple experiment so we
you know I sort of toned these down a
little bit and kind of boiled it down to
what the experiments were and maybe not
necessarily what we were doing but it
kind of gets to the root of things the
first one was really just a simple right
okay it was how long did it take to
write a bunch of data kind of in a loop
all right extremely simple it was a
single threaded test we were using 40
gigabytes per second InfiniBand and
really we were just looking at network
speeds we were saying look we have we
have this program we know what it runs
how it runs on regular networks but
let's see how it runs on InfiniBand and
actually let's see what some of the
upper layer protocols the overhead you
know incurs on these types of things so
when I talked about s DP versus IP OID
and directly using InfiniBand for our
DMA we just wanted to see the difference
we didn't know right we just we wanted
to figure out like what's what happens
here when you actually push a bunch of
data down the pipe and we did say okay
well what job is gonna have some
overhead no matter what so let's write
equivalent C programs write native we're
just to see what the difference might be
and in case Java itself is getting in
the way and being VM folks we're
actually in position to be able to fix
those types of things too so it's really
important for us to be able to see the
differences all right please tell me you
can see that at the back mostly you see
lines at least yeah cool Thanks
so the blue line so first of all this
chart the payload size of the bottom
here it runs from 1 byte all the way up
through 2
not listed here but 512 megabytes that's
actually just the amount of data being
written at once in a loop over and over
again throughput is upwards so up is
better the dark blue line solid line is
the C version using IPO IB the dashed
line is the Java version using a it says
DB B here what I mean by that is direct
byte buffers we basically and I have
socket channels and because we wanted to
avoid J and I'm marshalling cost so we
used a direct byte buffer that's writing
directly to native memory from Java and
then sending it that way
all right so this is just using IP IB
and them the these diagrams actually all
use the same format solid lines or C
code dashed lines are Java code so I'll
I'll just sort of talk about it that way
the first thing you notice okay not
surprising is that the C code is
initially faster than Java code no
surprise right I mean there's a
transition between Java and C in order
to get down you know to the kernel you
know the C code was written specifically
for this one thing so it's a Java code
but it's really tough to get that
optimization right down to the kind of
this the the C coding all it's such a
simple program that's it's it's really
difficult but he almost went down you
can see in a certain point after about
128 K or so that things sort of start to
even out and then it sort of sticks
together and nothing changes so the
first thing that we ended up observing
here and you know through some of our
research research right you know looking
up googling trying to figure out what
why it's evened out is it turns out that
the IPO IB layer that you end up using
actually after 128 K stopped sending the
data on mass it actually will start to
carve the data up in 128 K chunks and
then send it that way okay so at that
point the overhead is so high the Java
and C end up eating evening out excuse
me right so that was like the first
describe it's like okay well IPO I be
sort of has limits and then it's you
know it kind of evens out at that
certain point so that ends up being a
bit of a problem or you know something
that you need to be aware of we had to
be aware of all right let's try SDP
right SDP does not have the same level
of overhead as the IPO IB stuff you know
can we derive any can we you know
we get anymore benefit and we do until
we don't right so you you can see both
cases C and Java right I mean it just
Rockets right up we're like her ray
things are great and then they're not
right it looks like stock market line
basically right it's like it's like
everything was great and then not right
it was just it was really disappointing
I mean really disappointing to see that
so um we weren't sure what to make of
that all right so more work more
discovery waiting lots of different
settings what does it mean what are we
doing wrong you know it's basically
using socket API over you know
InfiniBand how does InfiniBand start to
really be terrible at that point all
right
brief little interlude so I'm gonna call
this an interlude in this is titled zero
copy 64 K boundary just ignore that for
the moment it will be made clearer very
shortly so when you're actually
transmitting data from Java okay if
you're using a classic you know java.net
package typically and I'm gonna you know
kind of generalize a bit here this is
how it happens right you've got a byte
array you write data to that byte array
alright data appears you then copy that
data over through j'ni to the native
space okay to in order to be able to
transmit it then after that you end up
copying from that native space into the
kernel so that they can so that the data
can actually be transmitted okay it's a
couple of things happen here first of
all your data is getting copied at least
twice alright that's really terrible the
other thing is is that we've basically
tripled the amount of memory required to
transmit that data all right now you
might say well not really
right there's you know this is on the
heap right we've already paid that cusp
of stole that's that's three times I
count that as three three times all
right there's a lot of CPU burn all
right copying memory is not free not
even a little bit right
GC back I'm believe me right I wish it
was free not so there's a lot of things
happening here in the traditional system
now all right
but I did say we were using direct byte
buffers so directly writing to native
space with SDP so what was actually
happening here well you know the obvious
shortcut we have some our own array in
native space
or writing the data directly from Java
to the native space and then that's
being copy then transmitted all right
one copy instead of two not bad
I've cut the memory used down to two X
instead of three X that's okay and
there's a lot less CPU burn all right
pretty good stuff so that's great Ryan
but that doesn't explain why there was
this huge drop-off on SDP ah well here's
the difference right there's this thing
called zero copy threshold and SDP all
right if your payload is greater than 64
K when you use SDP you write data say
128 K and then that memory is registered
registered okay with InfiniBand
for our DMA purposes so what it says is
it says I'm not going to copy this to
the kernel and transmit I am actually
going to register this memory in place
okay and use our DMA to transmit that
directly sounds fantastic
it's exactly what we want right right at
once and then it just appears in other
machine space sounds excellent but it's
not okay because SDP on every transmit
will actually register and unregister
the same memory and this is extremely
slow to do so slow that it ways the
benefits that InfiniBand our DMA would
actually give you okay and actually
craters your performance so you go to
transmit this and then of course there's
the super expensive unregister as well
which is not quite as expensive but
still now you know I say I apologize for
the bug in my slide here that you know
there should be zero copy you know no
copies before it's transmitted but the
register and unregister is extremely
extremely expensive here and kills any
of your performance so going back to the
original chart you can see here what's
happening is that at 64 K everything's
great you're getting tip-top performance
we seem to have plateaued which is a
little disappointing but still you're
getting a lot better than IPO if I be
the zero copy threshold hits as we move
228 K threshold and boom your
performance drops there's a certain
level of overhead you end up getting
covering a bit of that performance but
not to the point that it actually really
matters all right
so something to be aware of like these
are the types of things hey look faster
is better maybe not maybe you're dealing
with different problems right so there's
something I want it oh sorry yes one
more thing I jumped a jumped a bit ahead
here so the last thing we tried was just
using our DMA directly so same idea use
a direct byte buffer right straight to
native memory and use our DMA ourselves
so we've got a chunk of memory that
we're gonna write to we just register
that once at the beginning and we reuse
that to write the data back across
really fast faster than STP and there's
no degradation and quite frankly
although the graphs make it look even
see is slightly ahead of Java but only
slightly okay so this I mean this is a
really nice chart to see write scales
nicely right we keep adding payload no
problem right so it's really good now
the problem with our DMA I started using
the our DMA is that we had to write a
lot of the infrastructure ourselves it's
not particularly hard we're systems guys
not a problem but for the average person
right now are the average Java developer
this is still like a little bit
inaccessible to them okay so this is the
kinds of things that we're like well
okay we've got to figure out a way to
fix this and make this a bit more
accessible to people just a little note
about that zero copy threshold the 64k
you can actually adjust it on STP so you
can actually set it lower or higher if
you want so we wanted to find out well
okay well what happens right can we
actually improve this situation or not
it turns out you really can't okay even
if you set the copy threshold lower you
know you end up with this sawtooth
pattern and and really you know if
you're if you set it to a lower amount
you know you'll get this you'll hit your
peak you will drop significantly then
you come back up and no matter what
threshold you set it at it seems to
always return to about the same point
now we have an explanation for that
and you might oh you might actually say
well look why why is there zero copy
threshold anyways right like what's the
point you have to think about it if you
have a one gigabyte buffer that you're
writing to and you want to actually
transmit you certainly don't want the
kernel try and allocate a gigabyte
within itself right typically not even
possible so
you know it gets the point where SDP
says look I'm not going to force the
memory allocation to occur I can't so
instead I'm just going to go ahead and
register and unregister it for safety
purposes but in the end it turns out so
we played around with this for a little
bit and it turned out that look 64k the
default setting actually turns out to be
pretty much the sweet spot you peek gets
in tend to plateau you know it we didn't
have much more than that so something to
be aware of anyways again not an expert
just telling you what we ran into and
what we saw what we observed be actually
one experiment with this but probably
the same thing all right so yeah the
beauty of some of these things of course
you know like what what do you see here
it was nice you know IPO IB SDP is
actually relatively simple to set up I
mean again just Google that super simple
to do it's really easy to set up but
you're certainly not getting the
benefits and there are certain
limitations as well you have to watch
for you can get increased speed of
course as we saw from the graphs you
just have to watch out what you're doing
your payloads and so on what your
expectations are if you really want to
go hardcore you can certainly get huge
advantages but you've got some native
stuff too right I wouldn't call it non
it's not trivial it's not non it's not
you know rocket science either you just
have to be aware it's a different way of
doing things
and of course there are no this is the
first indicator that there are hidden
gotchas and that's purely from the
network point of view all right all
right we're 30 minutes in I got about 15
minutes here so all right we think we
know everything right we ran some tests
great we got it no problem all right
that rest should be easy so the next
experiment we did was we took the orb
and you know using RMI and whatnot and
we said a little look how does this
perform over InfiniBand okay this is
sort of a different different approach
this isn't you know we're not gonna be
writing data just just writing data I
mean we're gonna be doing stuff like
marshaling data and transmitting it and
so on you know what are the problems so
the test was really we just want to send
different sized packets through the
different sized payloads rather through
the the orb we're just measuring the
time required to complete this test it
is a multi-threaded test we had 500
threads doing
I saw it once we were just measuring the
time to complete not the most scientific
thing but it was at least a measurement
that was sort of interesting to us and
what we wanted to do is we want to say
well look what's the comparison between
Ethernet to SDP and IP oh I be using
InfiniBand you know I've got the listed
conditions here we're using just a byte
payload that actually changes things a
little bit for those that understand the
orb but still it was an interesting
experiment you know the real expectation
here and bottom line I'll read that for
people at the back at this point we said
well look we've got the InfiniBand thing
figured out so you know we're
realistically realistically expecting to
discover a whole bunch of performance
bottlenecks in the orb and this side of
it is this is how your code can get in
the way of performance particularly on
high-speed networks and talk about
InfiniBand a lot here but I mean this
can also apply to you know even you know
10 gig e networks right this is the same
sort of thing and we actually do see
benefits as well from some of the stuff
that we discovered
so here's Ethernet here was the
experiment we ran it okay you know
pretty standard looking graph down is
better this is time to complete right so
up is more time therefore down is better
not the most scientific like I said
and again payload size what's the size
of the bytes that were actually shoving
down the pipe okay standard graph run it
with SDP and a better but not what you'd
want right that's a lot of money for not
all to win so okay what what was the
problem right and what were some of the
problems that we discovered and here's
where you're gonna go oh my word you you
got you don't write code this bad do you
and the answer is yes it yes we do and
so does everyone else and it's okay well
I'll meet by the bar later and discuss
our problems all right so what's the
first ridiculous thing that we end up
find and again you know this may not be
you but it's the kind of thing that
you're gonna find you're gonna have to
look for so just you know just kind be
aware be be ready to to laugh at
yourself cuz you know who else will
other than everybody so we have a byte
array we want to send we want to squeeze
it through the orb all right so
you know really you know bunch of bytes
now let's write the data all right
so how it actually works though and you
know generalizations is that the orb has
an internal buffer that it actually uses
for transmission so it takes the data
and it says okay well I need to send
this across so I'm gonna do is I'm gonna
push that data into my internal buffer
and then I'm gonna send it across okay
very general answer you know
blurring some lines here but this is
basically what happens well I've
actually drawn this kind of to scale
because the internal or buffer is one
kilobyte and let's say we're sending a
tubular to kilobyte byte array down the
pipe you can see the problem right we
end up having to take that byte array
and you know you split it conceptually
into to one kilobyte chunks and then
basically pick this thing up and then we
copy it into some internal transmission
buffer that we then go and transmit okay
completely ridiculous right right and
and this you know oh okay you don't it's
just I didn't see there so you know you
will see this kind of stuff all the time
when you look in your code I see I see
some smiles in the back and hopefully
it's not like wow that guy's an idiot
it's actually more like wow we've been
there too right
so you know you you end up seeing like
really simple stuff like that and it's a
byte array folks just take the byte
array and start sending it directly
right that's simple
all right so these are the kind of
things that we just weren't optimizing
for and it was like simple fixes in
order to get derived better performance
now you know you you can actually you
know that was byte arrays but speaking
in kind of general terms about you know
marshaling the data across what we end
up finding was that okay this 1 kilobyte
thing was actually getting in the way of
the ethernet stuff and you know 3 to 4
kilobytes was actually sufficient to
kind of max at your performs anything
large really wasn't getting you anything
right now when we for SDP does anybody
want to take a guess what the best
buffer size was oh I think I heard it 64
K exactly right the zero copy threshold
anything larger right we started just to
go max it out so it was just a lot
better to avoid you know the hairy
situation of having the memory
registered and slowing you down so if we
kept that buck our 64 K and transmitted
64 K chunks
performance was excellent all right
that's the big one
um garbage collection I I'm gonna admit
it I was a GC guy beforehand so you know
if you want to throw vegetables at me
later it's fine so you know and this
isn't the GC talk I'm gonna try to make
this as quickly as possible imagine this
is your heap okay
and you have a bunch of allocated memory
that's the orange spots and the way
memory ends up working is there's a lot
of fragmentation holes throughout so
it's free memory here here and here and
so on to accommodate different types of
allocates well one of the things about
you know increasing the buffer size as
well that we had to sort of balance oh
it's nice that the 64k one ended up
working out you know well for us in this
case too was that you can actually start
to get into situations with large enough
buffers that that thing is actually
bigger than any of your available free
memory so you can have enough free
memory if you summed it all up in order
to allocate it but unfortunately you
don't have one contiguous chunk to wedge
this thing in guess what that means
a premature garbage collect right you're
gonna have like you know 30% free memory
on your heap and a GC is gonna kick in
and worse perhaps a global compact so
when you start to start you know scale
out your buffers and so on and worry
about what's gonna you know what should
make the most sense one of the things
you have to account for is how big your
system is is your test environment
mimicking what's actually going to
happen in the real world and are you
going to be incurring a ton of GCS that
you didn't necessarily have to do is
that gonna hurt your performance more
than shrinking your buffer a little bit
so that you don't incur these types of
problems
all right last ridiculous problem so
these are all cumulative and there were
a lot more but these are sort of the
ones that my management felt fit for
public consumption as a really simple
diagram here so we actually have you
know from the client the server side we
actually have you know it's 500 thread
tests right like I so we have a bunch of
threads running as a test like I said
earlier there were 500 of them 500 these
guys are actually trying to communicate
with the server via the orb well guess
what the initial configuration was
one connection sir stop laughing
yeah I told you see are you laughing at
me or with me is that I can't tell so so
you know these are the types you know
you look at this and you're like well
what were we thinking
right and we've been living with this
for how long so just ridiculous all
right so you know super highly contented
resource and to being a nightmare you
just couldn't saturate the channel he's
got 500 threads with one little straw
that our ways trying to cram their stuff
down through ridiculous all right
so let's play around a little bit right
so we said okay well look me up I'm into
threads
let's give it 500 connections right so
that was terrible yeah yeah hey that'll
work
no you know just a disaster left and
right it just this did not work okay
they're a resource nightmare completely
context switching nightmare and so on so
ever did not work what we ended up
finding actually and you know to quickly
code it to kind of zoom through this
timkin it sorry 10 connections or about
anywhere from two to five percent of our
actual number of threads turned out to
actually be the best sort of number it
was a the best situation for this test
to be clear for us to be able to
saturate the pipe and you know minimize
the amount of context switching that was
going on - basically delay transmissions
and so on have the right level of
contention at a lock I just said that
the right level of contention of the
lock in order to maximize our throughput
okay
this these are the kind of things that
you end up sort of looking for it's sort
of weird how it some of this math ends
up being kind of fairly common in a lot
of cases depending on what you're doing
that two or five percent is certainly
not a rule of thumb for everything but
for a lot of things that we ended up
doing an experimenting on that seemed to
be the one for us so you know you kind
of end up looking for these types of
things you see them in they actually
help you pick better defaults to start
with - a lot of your software alright so
a bunch of those you know silly things
and more occurred and we you know went
and hammered them out of the system and
so you know going back to the original
graph this is what we had and I
apologize for the shift it's just the
legend got bigger and this is the SDP
with the new stuff and all the changes
now I won't lie to you some of those
change
also affected the Ethernet thing which
unfortunately don't have on this graph
but you know the point is that we got
significantly better I will also point
out that that 64k threshold and
increasing the buffer was really the
bulk of the win okay but there still
were other things that actually you know
gave you that extra you know a few
percentage points that that really
helped out right and and you asked any
performance person who's done you know
gotten a system in place one percent is
gold okay you get one percent out of a
system that's one that's actually had
all the dumb things cranked out of it
one percent is worth absolutely gould
okay so a lot of these little changes
are really really worth it in the end so
good know it's stupendous but a lot
better than it was right and that's just
by fixing you know silly thinks right
because we're an IP o-- i be as well
just for grins just to see where it was
just to make sure everything was you
know the way we expected it I apologize
for the light blue line you know it's
about where you would expect it to be so
you know from that experiment it you
know it's quite simple right you know we
don't all call him into race cars and
drive to work at 300 miles per hour
we love to accept that you know the odds
of us actually getting there in one
piece is is pretty low and I'm not
saying that you should do things slowly
here but it's a matter of you know just
stepping on the gas right is not you
know it's not that simple right you wish
it was but you have to watch out for a
whole bunch of other things that are in
your way before you can actually go that
fast all right that that's basically the
end of the talk you know I think I
really said a lot of this throughout and
so you know conclusions are a lot about
just sort of repeating yourself there
are faster solutions out there but a
network is not just a network it's a lot
more and there's a lot of ways that you
can go out there and exploit it and and
use it to go back and find the problems
in your code right like I said it's it's
the old problem of you know you ran on
one CPU and then you ran on four and
your code got worse and performance this
is the same idea it's getting to the
point where people just don't think
about the fact that their networks
that's the same problem okay networks
are reaching the point where that's the
same problem you have problems in your
code likely that you need to ferret out
in order to really maximize the
advantage that you can have none that ah
so I will take questions what I will say
though is that that's the end of the
talk
thank you very much for sitting through
it I'm glad I don't have 20 people left
like I said at the beginning so thank
you for enduring this if you want to
leave now please feel free just do it
quietly while I feel some questions and
thank you very much for your attention
preciate it sorry question upfront
can i elaborate on the so okay so the
question is can i elaborate on the
internals of the of InfiniBand uh I'll
maybe start over I'm not a network
expert so I can talk to you offline
about what I know okay but it's what I
know
alright I you know quite frankly if
there's somebody in the audience who
knows more I'd be happy to turn you
right to them and say talk to them you
know for us it was just hey look it's
cool noon as fast let's try it out and
it wasn't so I you know but that having
that said you know a quick Google there
are tons there's one simulator
you know most change email addresses
there's there's one PDF it's like 50
pages long and if you want to learn
anything of a better thin Amman you
don't know a thing it's 50 pages you'll
have it read in like an hour and it you
learned so much right not composed by us
but it was it was well worth the read go
backward slight hope 51 okay in the
meantime while mr. performance yeah go
ahead okay sorry who sir somebody else
had their hand up right there go yeah
direct like direct use right yeah so if
you if you get rid of SDP the the reason
why you want to use say something like
SDP is all your stuff is socket
and you don't want to change your stuff
but you want to use InfiniBand so if
something like IPO ID or SDP is
basically just keep using sockets
acceptable use InfiniBand and try to
exploit InfiniBand for you so at the top
no actually when I said that that was a
so InfiniBand services they're not a
java thing it's just a very thin see
where that basically knows how to talk
to the InfiniBand all right so yeah
there's no as far as I know do you know
anything that's InfiniBand direct uh
InfiniBand based protocols from java at
this point i don't know of any No okay
so right so so yeah but so the short
answer is nothing available yet but you
know we have our via we have a VM
Oracle's gonna be em you know you can
expect something in the future but it's
it you know this there's no true
standard at this point other than how
InfiniBand how you talk to InfiniBand
which you know it's unclear if that's
how you want to talk to it from java i
would say it's not but it's one of those
things where you know exercise for the
listener take a read the conceptual
points about how it works how you have
to end up watching queues and so on it's
very easy to pick up and there's some
very simple literature out there that
really explains it kind of at the level
that you want without becoming a Network
expert go ahead so you know all I can
say we didn't really investigate that
much all I can really say is that
there's such there's a huge hit but then
the non-coffee aspects start to gain you
back right because you've pinned the
memory effectively right at that point
so it's it's a huge cost to pin the
memory but then as you transmit more and
more you can start to gain that back but
then that the weight of that ends up
hitting a max that's that's that you
know I'm I'm really blowing smoke here
at that point that's what we believe was
happening but I'm not necessarily
hundred percent certain
no actually we didn't so that's yeah
that's weird another another oh my gosh
yeah
yes up there you were talking about this
here right though okay that so that's
what I was answering there but yeah at
this point yeah if you could yep hmm
that's an excellent point yeah this is
not a real you know may may or may not
yep right absolutely I was gonna say
that the Java is memory which causes a
real problem for Java right right good
points anyone else
oh sorry over there yeah yeah not
anything that I have here though
unfortunately does not really um I
actually not to said anything that I can
legally give you unfortunately apologies
on that one idea I my company won't let
me actually divulge this is why I have
to strip through put numbers off the
side yeah I'm sorry
I write because because somebody's gonna
read something into it and you know
legal action or something it's I I'm
sorry I know I know I know
hey hire me whatever company you work
for and then I can you know oh oh is it
still being recorded
well they'll actually be I'm if they
aren't yeah Chris do you have the answer
for that okay they'll be on SlideShare
okay
yeah so you have my name or you can look
it up on that you know if you don't end
up seeing them just drop me a line and
you know I'm happy to hook you up here
sure it's quite rare maybe I maybe
didn't understand the question then are
you sick
sorry is the question right so you know
I might say there that you know you can
derive winds from high-speed networks
okay it's it's pretty clear that things
can be better they will not be the best
that you can possibly get so you need to
evaluate the cost benefit versus what
it's you know it's going to take for you
to put that in and that when you do use
these upper-level pro protocols that you
have to be aware of some of the
limitations that exist so I showed IPO
IB and SDP and some of the gotchas that
you've got the the point is is that as
much as it may look the same because
elliot's sockets it's not alright and
just so be aware of that if that's how
you're gonna exploit it you think it's
beneficial great be aware that there's
things to be aware of know so it's it's
all--it's sorry
InfiniBand has its own transmission
guarantees a service and so on protocol
you can slap down tcp/ip on that like IP
ib but it's pointless okay
yes absolutely and there was one more
I'll take this my last one folks I gotta
clear out so right so the question is
this who's going to use it
I can't I'm not going to get into what
you know from a customer point of view I
know I'm not gonna start to divulge you
know who customers are doing what but
you know anything where it's you you
need you know you can think of any
transaction system right that requires
you know incredibly incredibly high
throughput and we'll wait and see so you
know things like trades or so on are
huge parts so this isn't use absolutely
but you know from a Java point of view
not a lot of people are really you know
they're not at that point yet it's just
sort of starting to enter into the
consciousness even though SDP tutorials
for Java have been around for a number
of years at this point you can look it
up it's very easy to kind of look up but
but yeah you know there are certainly
you know use cases for it not sorry poor
choice of words is certainly in use by
our customers absolutely okay thank you
folks
preciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>