<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Application Design Practices to Avoid When Dealing with Sub-100-Millisecond SLAs | Coder Coacher - Coaching Coders</title><meta content="Java Application Design Practices to Avoid When Dealing with Sub-100-Millisecond SLAs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Application Design Practices to Avoid When Dealing with Sub-100-Millisecond SLAs</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BziFX9eUaNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good good afternoon everyone and
welcome to java application design
practices to avoid when dealing with sub
100 millisecond service level agreements
so this is a presentation done in
cooperation with intel and IBM and my
name is Daryl Meyer I work for IBM and
for the past 12 years or so I've been
working on the just-in-time compiler
component of the j9 java virtual machine
that runs under many of IBM's products
I've worked with several customers on
doing performance analysis I've been
benchmarking stuff and recently I've
been focusing on Java benchmark
development as well my colleague Neil
Kumar is from Intel and for the past
decade he's been working on working with
customers improving java performance on
intel's fleet of hardware also with this
presentation is Alanis I Pina with Intel
but she unfortunately couldn't be here
today and she she does java performance
as well so we've got a lot of material
that we want to cover today so this may
be kindly request that if you have any
questions maybe you can hold them to the
end because we do have a fair bit of
stuff to get over through through today
so what we'd like to do is to would like
to share a lot of the insight that an
eel and i have from discussing java
performance with customers really care
about response time workloads and really
in part a lot of the experience you've
had developing a new benchmark for
measuring java business logic that
actually uses a throughput has a
response time component to it as well
we'd like to really educate you and what
really contributes to higher transaction
times in applications sorry transaction
response times and applications and also
importantly states to figure out
actually how to measure that response
time as well because if you don't if
you're not sure what you're measuring
then you know you might have some issues
when you're trying to identify where the
problems are
and then starting at the sort of the top
of the stack is the job application
itself so stuff that Java developers
would actually write and this is the
thing that you perhaps have the most
influence over and there are some things
you can be doing to improving your
response time there's also things that
you shouldn't be doing as well I guess
that's maybe more the subject of this
presentation to avoid so that you can
actually get better response time in
some of your transactions but looking
beyond just the Java code itself java
the java code runs on top of an entire
stack of of components you know from
from middleware down through the JVM
through the operating system and so on
so to really get the best possible
performance you may want to tune
different layers in that stack as well
and even if you think that you're
getting a really good response time with
your application perhaps you can
actually be doing better so we'll be
giving a few tips to see if there are
some telltale signs there that you can
actually be getting a better response
time and woven throughout this we're
going to be giving lots of practical
examples and these practical examples
have been have been derived from the new
spec GBV 2012 benchmark that does have a
response time component to it so I think
there's a lot of stuff that's relevant
so the title of the presentation
mentioned service level agreements and a
service level agreement is basically
some kind of a commitment to provide
some sort of service at a prescribed
level of performance and there's lots of
different ways of different kinds of SLA
s but the one that we're going to be
focusing on mostly today is is is
response time we're going to touch on
throughput a bit as well but any
response time is really the thing that
that we want to focus on here these can
be informal SOAs so things like your
application must respond to a certain
event within a certain amount of time
because it otherwise I won't function
correctly so or the other way would be
if it was contractually obligated you
actually had an agreement with a
customer that you provide a certain
service within a certain amount of time
then but what we're really looking at
here are informal ones but I think that
a lot of the techniques
we prevent present today or can apply to
more than one environment response time
itself can have a number of different
definitions and as we'll see in just a
sec depending on how you measure things
your response times can actually be
quite different so really what we're
talking about here and a lot of
information that we're going to giving
today is really in the transaction space
so really how much time do you really
need to provide to to complete a
transaction in response to do that work
and in the general sense lower response
response times are better and it's
really a goal that you really want to
achieve if if you want to get good
performance and make an customer
satisfaction as well and with response
time there's often different
perspectives that you can take on it so
for example if you're a user and you
just entered some information and you
click Submit perhaps what you really
care about is that you get your data
back or whatever information you're
requesting within about a second or so
or even let or half a second something
like that so your response time is
actually quite high but if you are
developing like a real time trading
application where you have to respond to
some kind of a market event then the
response times in those situations are
actually much much lower and they are
probably in the order of like 50
milliseconds to 100 milliseconds
something like that one thing that we
hear from customers as well when we
start talking about response time and
tuning response time is well I have a
transaction that's that's not achieving
the response time that I think that it
should be so I think the way that I'm
going to fix that is to actually just
increase its throughput and and
therefore I'm going to get a lower
response time well that's true in many
cases and in fact if you actually do
improve the throughput of a particular
transaction you have to be aware though
that it will have or may have effects on
other transactions that are occurring in
the system as well so for example you
can throw a lot of resources at
improving the the throughput of a
particular transaction but it may come
at a cost and that you'd see that your
response time on other transactions will
actually increase
and then for their for your overall
response time will actually increase as
well so you really want to keep an eye
on a lot of different things there when
you're when you're looking at response
time and not just focusing on one at a
time so how do you measure response time
and it's actually a fairly simple
question but it's also one that will
find that different customers do things
a little bit differently so it's really
important to nail this down because if
you really want to focus on on improving
your response time you really got to
know what you're measuring and how you
can make how you can influence that
particular response time so up here we
have a particularly sort of contrived
system where we've broken out the
different components of a pivot rands
action request into into different
components here so a request comes in it
gets marshaled into some kind of a
request queue and then we have a after
after we have this transaction mechanism
which basically pulls a request out of
the queue and formulates a transaction
and then submits it into this
transaction queue and then we've got a
thread pool of some sort that has a
number of threads in it that when
there's an available thread it'll just
go grab the next thing out of the
transaction view and start executing it
the transactions take different amounts
of time they may complete in different
different durations and then when things
are done the the responses are sort of
marshal through a response or a
retirement queue and then ultimately
delivered back to the requester so in a
in a model like this there are a few
plate there there you can actually
measure different kinds of response
times in here so for example you can
actually measure the response time from
the time that the request is actually
submitted to the time the response is is
delivered and you know from it from a
user point of view this is really what
they what they really tend to care about
but another way of looking at this as
well is measuring the response time from
the time that not from the request is
first made but by the time that a
transaction is formed and it's actually
submitted for you know queued for
submission it's ready to run but
nobody's running it yet and then perhaps
another way of doing this is to actually
once the transaction actually does start
running measure the duration of which
22 to complete and call that your
response time so you really have to sort
of narrow down here what it is that you
want these are just three examples but
there's other you know permutations here
as well that you can go through other
important things to remember about
measuring response time is to make sure
and this is actually this actually
happens more often than one might think
but make sure that that when your timing
response when you're actually measuring
response times make sure that the timing
aspect of it is not part of the response
time so what tends to happen sometimes
is somebody wants to take a snapshot of
the current time but they'll also want
to log that to a file or they want to do
some tracing with that sort of thing or
send it out a socket something like that
and often the setup for doing that extra
logging is included as part of the
response time so you want to be careful
that when you actually do take your
snapshot all the time you're not
including anything else with that thing
that you don't want even though it may
not seem like it's a big overhead
overhead over time those little those
little bits of delay tend to add up and
they may cause your overall response
time to to increase Java provides some
fairly handy methods for doing timing so
these are in the system class so system
nano time and system.currenttimemillis
you should be aware these are these are
perfectly fine methods to use but you
just need to be aware that there are
accuracy and precision implications of
using these particular methods so for
example system nano time specifies that
the result that's going to come back to
you is going to be some sort of a
nanosecond it's going to be a nanosecond
precision so you're going to get a
nanosecond value back however it doesn't
say anything about the accuracy of that
and in fact many operating systems do
not actually have a nanosecond granular
timer that will actually hand you back
something that actually is in
nanoseconds you're going to get
something let's say that was in
millisecond or so yeah milliseconds that
was converted into into into nanos in
nanoseconds so you have to be a little
bit careful about the accuracy and the
precision of these timers and don't go
nuts with the timers Iver I mean even
though they are relatively cheap to do
if you use too many of them there will
be an overhead there as well
virtual environments response time in
these environments is can be a little
bit tricky as well what you have to
watch out for when you're measuring
timing stuff on virtual environments is
is clock skew so if you're running a
virtual environment that's getting its
clock source from the host there might
be some skewing the time it may not
actually be the time that you think that
you're measuring so if you if you
suspect that there is some kind of a
clock skew happening there or you should
suspect that there would be then what
you what you should try to do is to
actually measure your clock source on
some kind of an external system that has
a much more sort of consistent
consistent timer on it and then one
example here to kind of drive some of
these points home so what this chart is
showing is a bit of a sneak peek at some
response time throughput curve that's
coming out of this Specter abb 2012
benchmark I just wanted to sort of show
a couple of points on this thing to show
that you really need to focus on that
you need to be aware of what exactly it
is that you're that you're interested in
when you're looking at a chart like this
so what this is showing here on the
x-axis are different degrees of
injection rate so this is these are
different throughputs and it's and
what's showing on the y axis or the
response time at transactions injected
at that at that throughput and the way
that this sort of works within the
workload is that at a particular
injection rate let's say three thousand
in this in this case of three thousand
operations per second two thousand
requests per second we're going to
measure the the response time of the
transactions that occur in that
particular in for that particular
injection rate and you can sort of
characterize them based on you know you
can characterize the spectrum of them so
for example if you're if you just look
at sort of the median response time in
that entire range you can see from the
blue the the dark would be blue dots up
there that's the median response time it
actually looks pretty stable and a lot
of systems you know a lot of people
really only care about the median
response time and that's that's
perfectly fine but if you start to
increase the the size of that data
that the the group you're going to start
to see that your response time can
actually change quite substantially so
the purple ones here the the sort of
violating purple purpley once represent
the 99th percentile and you can see that
the response time even at 3,000 here is
actually going to be up closer to that
gets a couple of order magnitudes larger
so if you start looking it so if you
care about most of the response time of
most of the transactions then you have
to be prepared that the response time of
those is actually going to be maybe
actually quite larger than the other so
the tips and techniques that we're gonna
be talking about here some of them apply
well to median response time others are
probably well to hire percentiles as
well like 99 so as I'm as I mentioned in
the intro there the influences on the
response time themselves is not just
from the code that you write it does
play a big part of it but when you when
you look at the stack as a whole there
are a number of different places that
you can tune in order to get really good
response time and often cases you
actually have to look past your
application and sometimes this does work
against you know java's philosophy of
write once run anywhere kind of thing
but in in some cases it would make sense
if you actually did look at the entire
stack and and and tune your way down
through it and we're going to be talking
about each of these different layers
here as we go through this but before I
talk about that though I do need to
introduce this benchmark because it will
serve as a as really the the case study
that we're going to be used for a lot of
examples in this particular in this
particular talk so spec Shea BB 2012 is
the next generation Java business logic
benchmark develop i spec the standard
performance evaluation corporation so
this is an industry core a consortium of
companies like Intel IBM Oracle AMD ulit
packard things like that that really
define workloads that are really trying
to encourage you know fair measurement
of performance of performance on systems
and software and that sort of thing
and up until this point you know the the
Java business logic benchmark has been
spectrally 2005 something you may be
familiar with that but you know we've
come to the realization that that
particular benchmark isn't really
addressing a lot of the needs of users
these days it doesn't have a response
time component to it it isn't using
modern features of the JVM that
customers are doing it's not structured
the way that that customers like
yourselves are actually writing code so
we decided a few years ago that this
needs to be revamped and and we actually
rewrote the entire benchmark to make it
much more relevant to to to to customers
and and for the next you know five
perhaps ten years with this with this
workload the good thing about a
benchmark like this is that it tends to
drive a lot of good stuff into hardware
and it tends to drive a lot of good
stuff into software it forces JVM
vendors in particular to innovate on on
key technology that would actually
benefit everyone and so thats you know
really one of the one of the focuses of
this kind of workload as well the
business model of this workload is a
supermarket supply chain so we've got
headquarters you've got supermarkets and
suppliers and customers and customers
buying things and customers returning
things and buying stuff online that sort
of thing it's a transactional workload
it's very scalable there's multiple ways
of that this workload can actually be
configured it can be run under a single
JVM you can be run with multiple JVMs it
can be run with in a distributed mode
and it's also self injecting so it
basically figures out on the system that
you happen to be running on beat a small
system or a very large scale server it
figures out how much work that system
can do can do and then sort of Tunes
itself to inject just the right amount
of work on that thing to try to test it
and get it to fall over and and it forms
the workload from that it does use very
customer relevant technology very modern
stuff so a lot of Java 7 features it
uses Java util concurrent fairly heavily
at
l stuff security which is really lacking
in a lot of benchmarks and so you know
hopefully this is going to cause us to
do a lot of innovation in this area and
then perhaps the key thing is well
that's that differentiates this from
other spec workloads as well is that it
actually has two metrics of one of which
uses response time so it has the more
traditional throughput operations per
second but it also has a response time
component as well which is good and like
I said we're going to be using this as a
case study in this in this presentation
so starting at the top of the stack is
your application and the way that you
write your application and some of the
ways that you can design your
application so really in order to
improve response time one of the things
that you really want to be focusing on
is reducing the amount of latency that
you have in your in your and your in
your code and because you really want to
try to minimize as many pauses as you
possibly can there because that just
works against you for response time so
one of the first things that you want to
think about is making sure that it's
actually scaled and there's a lot of
code out there that's not and
scalability by the way when I'm what I'm
defining this as is really the ability
to increase throughput as you apply more
resources to something so for example if
you happen to be running your
application and it's running fine on a
little eggle for course are four core
processor and all of a sudden you run on
the latest you know westbury X that's
got a ten core processor there do you
see at least a 2x speed up do you see
any speed up at all and and if you're
expecting to see a speed up and you
don't then that might be an indication
that your your application really was
written that way so really what you want
to be doing is to look at your
application and whatever soft your
writing and make sure that it's prepared
to run on modern hardware because
hardware these days you know it's
starting to see more and more cores it's
scaling horizontally it's not
necessarily it's not scaling vertically
by throwing more you know gigahertz at
it you're really going to really take
advantage of the hardware you're going
to have to start writing code that
actually does take advantage of you know
the multiple cores
this isn't necessarily possible in all
cases depending on your your application
depending on your problem domain it may
not be something that's easily paralyzed
abul so it may not be something you can
do but you do want to look at creating
more parallelism in your application to
help you with this Java the JDK actually
does provide some help for this and this
is in the form of in the java.util
concurrent package you know there's a
task executive framework for very large
tasks and managing those and and having
those tasks completed and sending
responses back and java 7 actually
introduced much finer grained
parallelism in this Fork joint framework
for doing tastic decomposition so you
want to look into those and I've already
mentioned the java.util concurrent
package but if you've never looked at
this before and there actually is a lot
of customers out there that haven't even
heard of Java util concurrent before not
even aware of what it can do I certainly
encourage you to look into this because
it's a very very useful package in the
JDK and it was actually introduced back
in in Java 5 and there's been sort of
incremental improvements made to it in
Java 6 and Java 7 and really what this
is it's a very rich set of building
blocks for helping you develop scalable
applications and it's got a lot of cool
stuff in there and it does a lot of the
hard work that some people try to do
themselves but they just don't get it
right so for example if you want to
write your own kind of funky lock or if
you want to do you know if you want to
do your own concurrent collection you
can try that but in a lot of cases you
may have some bugs in there it's very
difficult to get a right to satisfy
java's memory model and and threading
model and that kind of thing what this
but this package basically does is takes
care of all that for you so you've got
things in here like I said it does
really good job with concurrent
collections it's got task management
stuff on fields it does there's
operations that you can have on like
atomic operations and fields those are
pretty cool really want to encourage you
to use Java util concurrent as a base
class for new stuff that you happen to
be developed so don't write your own
stuff but bright you can write
on top of you can write it on top of
this extend the collection classes for
example and take advantage of a lot of
the underlying features here and perhaps
one of the best reasons to use juc is
that it has actually ma optimized pretty
heavily by modern JVMs there's actually
some operations in here that you don't
actually see as a developer but actually
the way that these classes are actually
implemented that jvms can basically take
and in many cases just transform into it
it in many cases it just transforms into
a single instruction on a particular on
a particular architecture so really want
to encourage you to use this because we
can do a really good job optimizing this
another thing that sort of stands in way
it stands in the way of increasing or
decreasing response time is extra
synchronization and you know this is
necessary I mean Java is after all
multi-threaded so you are going to need
some synchronization there but you don't
want to you want to make sure that you
don't have excessive synchronization and
you want to get rid of anything that's
not really necessary to have Java
traditionally has got some fairly heavy
weight synchronizations i'll call it
it's the the synchronization that's
built into the language you with the
synchronized keyword monitors and that
kind of thing but unfortunately that
kind of synchronization does inhibit
scalability and it is in some cases
difficult to work around it is useful to
use that kind of synchronization when
you know you really do care about mutual
exclusion you got to keep all the other
threads out but there were lots of cases
where you can actually get better
performance by bending those rules and
relaxing those rules a little bit jvms
can help you here we do try to recognize
when you are you know when there's
redundant synchronization and
eliminating them but you certainly
shouldn't rely on the g a vm for finding
these opportunities and getting rid of
them juc is maybe where you want to
consider when you when you when you're
developing your own locking for example
you want to start looking at the things
like this or it's got a much finer
grained
and it also provides building blocks for
together your own locks so there's some
cool stuff there contended locks or
another problem so if you happen to have
multiple threads that are all try and
get the same walk then you may have a
problem because the ones that don't get
the lock have to do something and many
cases in the JVM those the java runtime
is basically just going to put those
threads to sleep eventually and that's
not good for your response time either
if your if your thread is basically
sleeping instead of doing work it's not
it's not helping you and sometimes you
don't even need synchronization at all
you want to look into using volatile
fields where it makes sense because
volatile fields really use the Java
memory model to enforce you know the
ordering of certain operations so these
are very useful for things where you for
example you have one writer and multiple
readers you may get away with being able
to use a ball tough field in that case
which end up doesn't which doesn't end
up using a lock but has pretty good
synchronization characteristics or as
well you want to avoid excessive object
allocations and we're now starting to
get into the we're starting to get to
the point where garbage collection is
starting to come into it into this
discussion and really the more objects
you end up creating the more strain you
place on the garbage collector and you
know it while it may I mean while it may
seem that you may not be creating a lot
of objects in practice your heat can
actually grow quite large and it does
have and we'll see some some examples of
this and shortly it can't have a fairly
negative effect on your application in
particular your response time if you can
don't put allocations and loops unless
you really need that kind of a semantic
try to share the same object within the
loop and hoist the thing out of the loop
the allocation out of the loop consider
using weak and soft references when when
you can
a weak and soft reference to be or a
weaker and a soft reference if you if
you're not familiar with those is
basically a reference that's not really
strong enough to prevent another object
from being garbage collected so for
example if this thing if this week
reference or sorry if you have a weak
reference that's the only thing that's
pointing to an object that object can
still get garbage collected it's not
like the sort of traditional way that
the garbage collector works where it
looks around the heap and tries to find
references to objects if it finds a
reference from one object which other it
still assumes that that object is live
but if the only thing pointing at an
object is a weak reference for example
the GC can still get rid of it in fact
it will on each cycle software
references are exactly like that except
that they tend to persist for a little
bit longer they have a little bit looser
rules and when they must be must be
cleaned up so if you haven't ever looked
into weak and soft references before
please do where they're useful are for
caches in particular where you want to
keep some information around that let's
say isn't isn't being written a lot or
you just want to keep something close by
you can use weak references for that if
you want to supplement information about
an object let's sort of a side table of
some sort you can use weak references
for that anyways you can you can easily
rima to utilize the data if you don't
care that if I asked for some
information it comes back as no and then
I can recreate that data find this might
be something you want might want to look
into another thing you want to keep an
eye on as well when you're talking about
object allocation are immutable classes
and some of the side effects of using
immutable classes so for example bigoted
bigdecimal is a good example of this
where it's an immutable class but it
provides a number of operations like add
multiply subtract things like that but
the but the interesting thing about
those is that every time you do an
operation you're getting a new object
back so if you tend to do these in loops
you have a lot of this code then you're
going to start to see a lot of
bigdecimal objects being created so just
be aware of immutable classes and and
the side effects so they might have okay
so into some real data here so i think i
have to preface this data with a
with some information about how we
actually obtain these so when we looked
at measuring certain things to basically
present here we're all we're looking at
a benchmark that's been the application
itself has been tuned quite a bit but we
had lots of problems early on while
developing this workload with thread
pools and numbers of threads there so we
ended up fixing a lot of those problems
and in terms of caching data and that
kind of stuff we fixed up all fixed up a
lot of those issues so what we're really
seeing here is is whatever's left and
what we're going to see from a lot of
these examples as whatever's left often
has a the fingers often pointing at the
garbage collector not always but it's
but it also often shows they're
interesting characteristics in the curve
that point to garbage collection so what
we're looking at here is an example of
of doing accumulating too much data in
your in your workload and this is
talking about receipt storage in the
benchmark and what that really means is
that the way that this particular
benchmark works is that when a
supermarket does a completes a
transaction it sends a copy of the
receipt to its headquarters for storage
for some reason I mean it's it's it's
therefore that the headquarters for
archiving or whatever it happens to be
but the point being though that it's
actually sending something over and over
and and and this list is ending of
growing quite large and what we're sure
what we're showing here are two
different response times that we want to
care about so we're looking at needing
response time to the transaction and
we're looking at the 99th percentile and
what we did here was you basically
varied the number of receipts that we
end up storing in the headquarters for
for processing later and what we can see
from the 99 the top the top chart there
is that as we increase the number of
receipts that we end up pertaining
there's the median response time doesn't
really change all that much but there is
an impact on on throughput the x-axis
here the J ops is actually the
throughput so the farther you go to the
right the the more throughput you're
actually are able to achieve but if you
look at the 99th percentile as you
increase the number of of receipts being
stored you can see that your 10,000
receipts is ok 50,000 you're starting to
get a little bit of bumpy
eat bumpiness in your in your in
response time a few spikes there but but
you know if you all go all the way up to
100k you your your your career is
starting to get pretty spikey there and
a lot of that is due to garbage
collection and and just the way that it
actually has to spend time cleaning up
those receipts that are no longer needed
anymore so it does have a fairly big
impact on on the 90th percentile but if
you only care about the the median stuff
you know it looks it looks decent
another example here is another
component of the workload where the
headquarter component is able to do
mining of some data that comes into it
and it can actually use this data to
influence the benchmark so for example
it can figure out customer buying
patterns or which products were
purchased together and it can use this
information to influence future
purchases by customers and that kind of
thing what we found when we were
developing this workload was that the
amount of work that we gave the
headquarters to do the amount of data
that I'd have to crunch through to do
this data mining had a fairly
significant impact on performance and on
response time and in the in the median
response time here you know as we
decrease the amount of work the blue
line here is a decrease in the amount of
work as we decrease the amount of work
that the headquarter had to do the the
response time the median response and
you didn't really change all that much
but in the if you look at the 99th
percentile and unfortunately this graph
is a little bit clipped here but the the
response time for the 99 percentile
actually had some fairly high spikes in
it it starts to get fairly spiky there
at the end and you don't see how high
those go but you know if if you look
there the the peak there is now a
thousand milliseconds so some of those
pauses some of the response times are
actually taking you know well over a
second so you definitely want to you
know key keeps in those things in mind
another aspect to look at here in your
application is to reduce the overhead
that there is to access data and this
very common issue in client server
workloads and a very very common way of
getting around this is a basically cash
some data locally on a certain system to
avoid actually having to talk to
somebody else to get data and this
actually is a fairly useful approach
especially if your data is we're going
to change that much but as but you know
you have to be careful with this right
because if you start to cash too much
then again you're going to start pushing
putting pressure on your garbage
collector and this is also an example of
where if you cash too much you can do a
really good job improving the
performance of this one thread by
caching all his data and he's like
zipping along getting really good
response time but if that's now causing
garbage collections that could actually
be harming other threads as well and
eating into their response times so you
have to consider the system as a whole
and not just you necessarily focus on
single transactions you have to look at
the at the bigger picture other ways
that you can actually reduce your data
access latency is to use Java new I oh
so introduced in in Java 14 and there's
lots of cool stuff in Java 14 for tying
you know Java I owe to the operating
system and some of the features of the
operating system provides so for example
things like buffering and channels and
selectors and that kind of thing so it
actually does tend to leverage a lot
more the performance of the operating
system kind of gets it out of a jab and
let the operating system do its thing so
use that if you're not if you're not
using it and another thing that we have
that we that we really found while
developing jbb is in terms of your data
structure choices if you choose blocking
data structures so if I come up with a
queue of work and it's a fixed size it's
got 100 elements let's say and if
anybody else tries to add you can add
elements up to the point when when it's
full but if anybody else has tries to
add it sits there puts up its handling
blocks that wasn't really good for for
response time either because your threat
isn't doing any work it's just sitting
there waiting to add something to it to
a work you so the way that we worked
around that is to serve a structure it's
such that we could get away with using
non blocking data structures
and you know the job util concurrent
package does provide non-blocking data
structures as well so for example
connive got their concurrent link to you
so you might want to look into into
those as well you got to be careful with
those though because again you're going
to get into the point where you want to
accumulate too much data but you know
just so you know that it's that it's
there and something you want to look
into so again back to jbb this is an
example of a super mobile supermarket
cashing some data so it doesn't have to
talk to the headquarters and the example
in this particular case here is it
caching information about customers that
isn't likely to change his name is
credit history and things like that it
keeps a bunch of information closed so
it doesn't have to do the talking all
the time and as you can see as you as
you increase the amount that you cash
the throughput really increases and if
you again if you're just looking at the
median response time it doesn't really
have that much of an effect on on the
median response time but again if you
look at the at the 99th percentile you
start to see that adding keeping more
and more data local is starting to incur
some fairly large spikes there in in the
at the 99th percentile which may be
unacceptable to you so you do have to
kind of tune the amount of data that you
you're retaining their or look into ways
of fixing your garbage collection or
tuning your garbage collector in order
to get decent response time at the 99th
percentile so these days many java
applications are not written you know
from scratch as sort of stand-alone
entities a lot of them are but but many
of them are not and oftentimes you're
built into like some sort of an app
server or you're using a lot of third
party packages or using like some Apache
stuff and really the message here is
that if you're developing with other
packages or on top of other frameworks
you really need to understand what
they're doing as well and how they could
possibly be impacting your your through
but in response time and there isn't
really any sort of
very specific advice that you can get
but the more general advice depending on
the package is to really understand how
those packages are using thread pools
how they're doing thread management how
they do caching how they do connection
pooling things like that and then
understand it how to tune that for your
environment for your application to get
the kind of behavior that you need next
layer below is the Java Virtual Machine
and it's a fairly important layer it
tends to you know it does run everything
and there's lots of things that you can
do to improve your performance lower
your response time by tuning the Java
Virtual Machine this is just sort of a
typical example this is jay nines
internal kind of structure it's a very
layered kind of approach and it's
typical of a JVM these days but it's a
point here being that it's fairly
complex and there's lots of little Aires
in here that you can use to to tune but
I'm going to focus on the most important
ones and the most important one is is
garbage collection and especially when
we're talking about response time and
it's it's been something that since
we've been developing the specter of
eerie 2000 wirkkala 2012 workload the
the theme that comes up it's been coming
up for years has been garbage collection
and how we need to tune garbage
collection in order to attain the
response 71 this is worth a presentation
on its own on garbage collection tuning
but really what you need to do is to
understand what your application is
doing understand what your application
is doing with its data to decide what
kind of a garbage collection policy you
should be choosing for your application
and oftentimes you are trading
throughput versus response time and the
thing that tends to work against most
applications is the fact that when the
GC does do work most of most DC policies
have us what they call stop the world
phase which means all threads have to
stop while the GC does a little bit of
work and that really kills your response
time for sure so what you want to try to
do if you really care about response
time is to try to minimum
stop the world pauses that happened so
there are variety of different kinds of
garbage collection policies out there
and so for example throughput ones are
ones where you really don't care about
response time that much you care more
about doing a maximum amount of work you
can incur longer pauses but you know but
you know if you're sorry you can incur
longer pauses when the garbage collector
has to kick in but you really want to
get your a lot of throughput down there
most GC policies these days are
concurrent which means that the garbage
collector actually does a little bit of
work while your application is running
such that when the garbage collector
does kick in it doesn't actually have to
do as much work as it would have if it
wasn't doing that working concurrent and
what this tends to do is to lower your
average pause time for garbage
collection that's good for response time
these days we're starting to see more
and more balanced policies which you
know they tend to carve you know the
carbs your heap up into regions it
improves parallelism it goes after
collecting areas that have got the most
garbage and that kind of thing so I
think that that's going to be sort of
the next area where garbage collection
is moving into where we're going to
start to see some innovation there
another message here is to actually tune
your heat parameters you the maximum
size the your nursery size your old
space size that kind of thing a lot of
that is really dependent on your
application and how it's using data
again but but JVMs are getting better at
doing this automatically but you know to
get the best possible results you still
want to do that yourself right now and
there are you know there are verbose
logging features available to help you
correlate what's going on in your
garbage collector was what's going on in
your application to see if you know I
got some issue with my response time
here maybe there's some GC event that's
that's causing that so just a couple of
comparisons here of performance using
different garbage collection policies so
this is just basically just a comparison
between a concurrent mark approach and a
non-concurrent mark approach and as we
can see the the one that sort of favors
through
actually does get better throughput at
the in the median response time and the
response time isn't really all that
different in the medium medium response
time but if we start to look at the 99th
percentile of transactions you see that
the the more throughput based policy
really tends to again it has better
throughput but there are often some huge
spikes that are occurring there when the
application is to stop and pause and if
you really care about not having really
big response times this is going to kill
you because you can see that some of
them are well over one second there but
if you look at the mark the concurrent
mark one the performance is much more
stable much more predictable and on
average it actually works out better for
response time these days most systems
are 64 bits and you know more and more
applications are starting to take
advantage of the larger address space
and so we often see this with you know
in heat databases are in-memory
databases grid kind of applications
things like that and that's fine I mean
it's great to use it sort of thing but
the one thing to keep in mind with
64-bit architectures is that they're not
as efficient as 32 bit in terms of
utilizing your system and the reason is
that your image ave is a very reference
a very object based system and 60
representing 64 bits is a lot less
efficient than 32 bits because your
pointers are twice as why you're getting
less utilization out of your processor
caches you're getting just less
utilization out of your memory and
that's cut that's starting to stress the
hardware so JVM these days have a
solution to that and what they basically
do is to say well we can actually
represent up to a certain point we can
actually not use 64-bit addresses we can
actually just use an offset into into a
certain amount of memory range we can
have it as a 32-bit offset and so we can
get all the sort of space benefits of 32
bit but sorry the space the blue the
pointer space benefits of 32 bit but at
the cost of a little bit of runtime
overhead for expanding that that offset
out into a real address
and that can be enabled with with other
you know using compressed oops you may
have heard of that before so that's
something you want to consider when
you're using 64-bit and if you are
running with probably less than a 30
gigabyte heat something like that I
don't have a data I don't have data on
that particular i don't think i didn't
know i don't i don't have data that
actually shows that but this sort of
empirically what we what we tend to see
with some applications that your
throughput is going to increase by about
ten to fifteen percent if you use
compressed groups and your response time
may increase by something like five
percent that's what it did in inspector
review 2012 the next layer here is the
operating system and there's a couple of
things i just want to point out here so
one thing that is particularly good for
performance and good for you know
eliminating bottlenecks in your union
transactions these days is to optimize
the memory that's actually being used
and where that so most ball actually all
operating systems these days use some
kind of a paging approach and when you
have a physical virtual addresses in
napa physical address the processor
internally uses these things called
translation lookaside buffer is to help
with that translation to cache the
effect to cash that translation and that
buffer is actually affected by the size
of the pages that you use the bigger
pages you have the more you know the
more reuse you can get out of those
buffers so we like to see is if the
operating system allows it is to is to
use larger physical pages so for example
going up from the standard 4k page and a
lot of operating systems up into the two
megabyte range for a particular page so
if you tend to use a lot of data you can
actually reduce your three you can
actually reduce the overhead of
accessing that data by using large pages
and you have to enable these in the
operating system but but to use those on
the command line of the JVM su you can
spent you specify that you want to use
large pages
and just a quick effect here of what
happened on jbb 2012 is that you know
when you enable large pages you know
throughput doesn't increase median
response time again doesn't seem to have
much of an effect but again on the on
the on the 99th percentile the larger
pages while you're getting more
throughput it's trying its best to keep
the the response time down as much as it
can there and the last two points that I
want to measure mention here are about
thread scheduling so you do want to keep
your eye on contacts which is because if
your threat isn't running then it's not
doing anything right so that's just
eating into your response time and you
know there's a couple of different
flavors of context switches there's sort
of voluntary ones where you give up the
processor yourself you're saying I'm I'm
going to park my thread because I'm
contended for example there's a
contended lock here I'm going to I'm
going to give up the I'm going to park
by thread that's not necessarily good
because you're basically you're giving
up an involuntary ones we actually have
this problem with this particular
benchmark early on where we had too many
threads and they were causing in each of
them one or two do work and they were
causing other ones too to be constantly
swapped out so if you have too many
threads you don't want to keep an eye on
that and then maybe something actually
Neal is going to show a little bit of
data on this in a bit but you want to
watch out for threads moving around if
you're running on one core then it's not
and he's happily accessing his data on
that core it's not really good for
performance if you suddenly switch to
another core the OS schedules you in
another core and you're you're doing
remote access to your data and that's
just going to cause extra latency there
so you do want to watch out for that use
the facilities in the operating system
for looking at further migration ok so
that was covering most of the software
stuff I neo is going to come up and talk
about the hardware itself and different
aspects of that thank you know I'm the
hardware gay and with that I do want to
ask one quick question that when we are
dealing with 100 millisecond or lower
response time most of the time you might
have
no your hardware so my question is most
of the time what I have seen in the
customers your programming and in that
space you really don't care for the
hardware but at that lower response time
you go on 100 millisecond or 10
millisecond there is not getting away
from the hardware part of it so how many
people do really know their hardware
where the thing going to run some idea
pretty good at least I knew one hand
will come that is guild from Azul sway
so I think on the hardware space up that
could be one whole presentation on each
of the topics once you go in the detail
and particularly i have seen those
questions when we are talking about sub
1 millisecond response time but i do
want to highlight quickly the things we
have seen very often like for example
power management most of the time it
works very well you will not see a
impact but if you have a financial or
the high transaction type response time
and you are at very low utilization
where the response to a request could
come you need to quickly do the work and
go away you could see fairly long
latency and in those cases people don't
like to leave the power management
unable though there are trade-offs so
you do want to aware at a high
utilization it's not going to make a
difference but if you have any phases of
low utilization you could expect some
strange behavior so you may want to
check is that causing it the other ones
are not going to detail about the
physical memory or if you have memory
swapping very common space you could see
long latency there they won't be from GC
but you could watch it by using your
operating from OS level monitoring
itself is it members same thing for the
network for example you could have we
have even in this application if it's a
two-tier application where the request
is going through if you don't have
enough socket connections you might see
long latency and the last part is about
the bios where you have to go when
you're talking about 10 millisecond one
millisecond latency and they are hyper
threading in particular cases when you
have most of the time you will see gain
performance gain throughput as well as
post time but if you have barely
contented locks then the hyper threading
can actually increase your latency and
you may want to check by disabling it is
that causing it if you either you find
first do I have contended locks or then
HT could be causing yourself trouble
there prefetching JVM is smart enough to
do prefetching and what we have seen
nowadays actually we tune that working
with both the JVM JVM tend to do their
own prefetching and the hardware dead
prefetching in the past they were
colliding we have make sure that by
default those type of things should work
fine now next part would be when you are
talking a response time there is
particularly very low response time you
do need to know what kind of and this is
just one example I can give you hear
that there are so many processor family
just at the high level for example e7
series are for sock admission criticals
then there is a five series there are
three type and the e 3 series entry
level if you are looking for the best
response time then the highest frequency
parts are going to give you the best
response time and I try to highlight
here like 'if I've 2600 series and is
the family which can give you the
highest frequency part for the response
time so you do need to know there is no
going away from if you are going to go
into the e 7 series then their
frequencies are tend to be less than the
e 52 socket series so you might see
better response have it to e5 2600 even
in the 2600 series look at the middle
column you could almost be from the top
frequency part 2.9 gigahertz to very low
1.8 and this frequency is going to
directly impact your response time and
many times we have seen volume usually
are from 2670 or 2665 which is almost
203 bin down but if you are very
sensitive than there is not going away
from the top in parts it all depends how
critical it is the I would like to make
one more comment I have seen the turbo
frequency how many people have heard
about the turbo boost or turbo
frequencies okay so that I
testing one nowadays we are doing like
fifteen twenty percent higher so that
means you could have got that much
better that's most time what really
happens the power management I talked
about many time when people are
sensitive they go disable the power
management because they want a constant
frequency what happens with the constant
frequency you lose the turbo benefit but
there are a methodology where you could
have both you could let your system run
at constant speed and put benefit from
turbos so if you are really care for the
response time you may want to be look
into it let me share one data point here
that so this is a part with eight core
and we try to see the scaling on the
response time what happens when we are
using all eight core versus fork or so
as Darrell mentioned earlier the
benchmark is scaling very well that you
could see from four core two eight core
you do twice on the throughput almost
one of the interesting thing you will
not see in the median almost no impact
on the response time on the median but
look at the p99 the eight core are able
to hold the response time below 200
millisecond for almost 50 puts up to
fifty percent of utilizing their of the
total load while with the four core in
the very beginning itself it goes above
200 milliseconds so once you're unsure
is scaling with the more poor you will
see the more benefit of the more core
here on the response time p 99 again if
that is what you care for the next one
is leveraging your topology with the
Numa many of the nodes are Numa that
means your each processor has their own
memory and many times you are able to
affinity other case times you are not
able to affinia ties with due to
different reasons but there is a very
clear benefit of the affinity you can
show on the top part you can see more
throughput and median time again almost
no impact but if you look at the p 99
there is very clearly for much longer
you could keep it below 500 millisecond
and actually almost up to twenty-five
percent below 100 millisecond range so
there are once again if you care for the
p 99 and we assume at the higher
response time you do when you want below
hundred millisecond then i guess the
thing you need to look for is p99 values
or even higher that
thank ok so it's all right so just a
couple of points to close off here so
when you're when you're looking at your
system that's say you're happy with it
the way that it's running right now are
there any signs that you can actually be
achieving better performance than you
actually are and getting better response
than they actually are so one sign would
be like we first talked about you don't
have your not using very many threads
there so you're basically have a fairly
single-threaded application so if you
think that there is some parallelism
there you can exploit then by all means
do and that may actually rely you know
pay out as an improvement on your
response time if you have a lot of
contended locks that's something to keep
an eye out for as well through our
locking tool lock monitor tools
available that you can use for that look
at your cpu utilization if your if you
think you're running full out and you
look at your CP utilization it's only
sixty percent there's something there
that's that's preventing you from we're
fully saturating the cpu and you got to
get to the bottom of that because you
know you're not you're not using all the
processing power that you can and also
if you're spending a lot of time in the
colonel there's got to be some reasons
for that too it could be paging it could
be locking you know other other other
reasons as well but if you're let's say
more than ten percent in the kernel then
there's some issue there that you should
probably get to the bottom of lots of
tools available as well I just listed
some of the ones that that IBM provides
here so you know we certainly provide
IBM health center their equivalent ones
from from Oracle as well can figure out
what your jvm is doing how well you know
where the hotspots are in that
particular application how your heap is
performing the garbage collection
analysis tools as well ways of
visualizing what's in your heat what
would constitute the objects in your
heap you can find out for example if you
do have a excessive object allocation
problem you look at your paws times find
out when they occur and what else is
going on at that time and then of course
there's some offline tools as well that
you can use to basically dumped contents
of your heap at a certain time and then
just pawed through it to your heart's
content and find out what's what's going
on there so
so that's that's all we had to say so
any questions on there
well I think it gets back to I I think
it kind of depends on the on the other
situation that you're in there it really
like I was saying about the frameworks
right you really need to understand how
to tune that particular framework and
how your applications fitting into that
right and I I don't know if there's
actually a specific answer to that
question I think it really depends on
its really a kind of a case by case
maybe you can come up here I can hardly
hear you now maybe you can come up here
afterwards I can be hard to hear you now
actually yes sir the g1i is it defo
junagadh dear Motty one do you are you
talking about g one g one garbage
collector i'm not sure with see oh so
there's that so a question is about
there's a java 7 update for there's a GC
that does not cause pause times ok so
these type what do you want I don't know
anything about g12 Godsey they were very
very little about it but Dave up here
anything else yes sir hyper-threading in
GC that's so hyper threading and DC
should work fine because we have the
most of the JVMs that made sure there is
no contention with respect to garbage
collection part so you will not see a
issue with the hyper threading and the
garbage collection mostly it is
somewhere else in your code if you have
lot of content at log then you could see
the problem
so you if there are any special case we
can try to run and see so you can send
my contact was there anilkumar at
entercom or if you have a card feel fig
i can me but no i'm not aware of the
concurrent with the hyper threading
having any issues but you never know
this is what a very sure if you write to
us we can try to replicate it and see
what can be done this should not be
called me you said you can't go any more
than 1.5 game I using the hot spot j9
that will be your guy there miss that he
is the hot spot guys to talk about
whatever your particular domain whatever
your system is actually doing we can
discuss that after if you like
anything else okay thank you everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>