<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Caching to Data Grids: The Past, Present, and Future of Scalable Java | Coder Coacher - Coaching Coders</title><meta content="Distributed Caching to Data Grids: The Past, Present, and Future of Scalable Java - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Caching to Data Grids: The Past, Present, and Future of Scalable Java</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yhpSMy-9N2Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everybody so quick show
of hands how many of you are here for
the first time at JavaOne this is
amazing
I saw this in another presentation as
well like a lot of first time people
yeah that's great well welcome to Java
one I've been coming to Java one since
the second Java one because I couldn't
get into the first any of any of you go
to the first Java one all right I didn't
see any hands but yeah it was completely
sold out they had no idea that this this
thing for programming doorknobs would
take off and was the original joke with
oak which was the precursor for Java
they had no idea how how much the demand
would be and it just took off was was
incredibly a successful language and
platform and as as hopefully you're
seeing this week there's a lot of stuff
going on in Java I work for Oracle but
Oracle obviously bought Sun but a lot of
the stuff going on in Java is happening
outside of Oracle - thanks to open JDK
the transparency work we're doing in the
JCP and things like that and so let me
just start with because I am required no
matter what I talk about to put this
legal disclaimer in although I'm not
talking about the futures of specific
products but this is a statement that
says if I talk about anything in the
future please don't hold me to any of it
so there you go let's see so my name is
Cameron pretty and I was years ago at a
startup called tango Seoul where we
created a product called coherence which
is one of one of the first if not the
first in memory datagrid products and
now they're probably about 50 products
in the space some open source some
commercial incredibly exciting area in
the industry distributed computing is a
lot of fun so how many of you work with
distributed computing today makes sense
why you'd be here
you just think it's really interesting
that's so it is it is actually a lot of
fun and so my background you know I'm
not a computer scientist I actually
studied economics and political science
which is one of my one of my honours
roommates you know I was talking to him
the other day and you know what he asked
me he said you want fries with that in
other words it's a fairly useless set of
degrees and but I love software I got
into software a software company when I
was in school and been doing it ever
since and it's a certainly a great field
in terms you know sky's the limit in
terms of you know we're creating things
that people never imagined you know even
twenty years ago so so I joined Oracle
in 2007 and been here ever since
so quick overview of the agenda we're
gonna take a look back and then look
back through kind of evolution and a lot
of what I'm going to be talking about
today is gonna be colored from my own
experience I'll try to keep this you
know kind of vendor neutral or something
I'm not gonna you know talk a whole
bunch about specific product features or
something like that but I'm certainly
gonna explain it from what I saw in
terms of the industry and by the way I'd
like to put a special thing so I don't
know if he's here but Rob mysic actually
was kind enough to help put this deck
together for me so thank you Rob if
you're listening so gonna take a look
back through the evolution how did how
did this thing called in-memory data
grids how do these come come to be also
gonna look at them good to see you by
the way congratulations on your release
speaking of data grids the we're gonna
look at some of the core concepts like
what what makes us work why did we why
did we choose the things we do in and
these are core concepts that are
incredibly useful regardless of whether
you're building a data grid or using a
data grid or even none of the above if
you're just building systems that have
to scale and should be highly available
and you know understanding why those
trade-offs are difficult and how to
maximize what you can achieve
in in a scale environment and then you
know take a look at kind of where we are
now and hopefully a brief look of what's
what's coming as well so brief history
of time of course is a play on the the
book of the same name by what does that
famous scientist named Steven Hawkings
yes so so we started if you go all the
way back to say 1999 era did j2ee back
when it was called j2ee before Java EE
was just coming out
was a huge specification I read the
whole thing didn't understand any of it
and you know what you had was a couple
early application servers most notably
WebLogic certainly still exist today it
was probably the most popular at the
time but there are other ones as well
Kiva I think they got bought by Sun and
net dynamics I think was another one
there were several very popular ones
that you probably haven't heard of
because they disappeared fairly quickly
but what we saw was that these Java EE
was designed Jonathan j2ee at the time
was designed for a scale out
architecture and the idea was you know
you put everything on the server and
then you have a second server and a
third server and a fourth server and you
somehow load balance across the servers
and it would scale whose scale
beautifully and of course it did scale
beautifully unless you had data right if
you had to say talk to a database and
the reason why is because the
application server every time you ask
for a piece of data it would just go to
the database and get it and you know
every and and the granularity was so
small that if you asked for two pieces
of data guess how many times you enter
the database twice and even more
insulting is that if you only deployed
one server so say in the case of
WebLogic at the time if you deployed one
server everything ran really fast you
like well this is great if I have more
users all had a second server so you'd
add a second server and what would
happen it would run like a horribly
maimed dog it would just come to a
complete halt so you'd have two servers
and the database load would go from
three percent to 100 percent it doesn't
make any sense why would two
servers crush the database if only one
server you know left it alone and the
reason is is because the app servers had
a with only one server running they'd
cache locally
they'd cache all that data in memory so
the EJB calls are really really really
quick everything was really fast and
then as soon as you add as a second
server you had to turn off that caching
or you get data corruption so you turn
off the caching and everything every
request now is going to the database so
instead of scaling really well what
people found was when they really needed
it to scale it didn't and this is this
is the certainly for our company this is
where the opportunity came in we were at
the time a very small company and you
know we hadn't built a better mousetrap
we didn't know what kind of mousetrap we
were we were we were building and we
just knew we were gonna build something
eventually and we kept on seeing this
problem with with customers that we were
working with where you know they'd start
to scale out and everything would grind
to a halt you know since we were doing
consulting as what we called our keep
the lights on strategy in other words
don't go bankrupt we figured if someone
has to have solved this right so we went
to Google and you know at Google please
tell me has anyone solved this problem
and of course we didn't actually write
the query that way but we were looking
for all sorts of words that would
describe this problem and all we found
was companies having this problem
and no one had no one had really solved
it and so this was back in 2000 2001
timeframe and this is when you first
started seeing this concept called
clustered caching well I actually it was
only called that because we called it
that because there was no word for it
there was no marketing jingo for it
sometimes if you look way back there's
the distributed caching is a long time
term has been used since we didn't know
anything about computer science we
didn't know to call it that so we called
it clustered caching because app servers
like WebLogic called it clustering which
of course is also a horrible misuse of
computer science terms but when we came
out with it we thought well this is
great we could put the data in memory
and keep it and sync across multiple
servers and then every time it needs
data it doesn't have to go
database' just grabs it out of memory
and this was great what we did was we
started with replication hey you say
he'd stick some data in memory and poof
it was there on every server so if you
had two servers the data was in memory
on two servers if you had ten servers in
memory on ten servers now at the time
does anyone remember like Java JVM 1 and
1.2 and so on the G C's were horrible by
today's standards I mean back then you
know you didn't have enough memory to
actually run into that problem anyway
because it was too expensive so most VMs
were running like maybe 64 megabytes
server VMs maybe 256 megabytes whoo that
was big you know in a big server at the
time was a Sun efore 50 or something
like that my memory is going so high if
I get somebody's facts wrong please
don't hold it against me but a sunny 450
maxed out at 4 gigs of RAM like I've got
4 gigs of RAM on my phone I think I I
mean just think about it like the I'm
lying of course but you know it's not
much memory by today's standards back
then that was the big server that was an
$80,000 machine which seems almost
ridiculous today but you know you
weren't gonna have a cache of 10 gigs
because you could only have four gigs
max and by the way you couldn't run a VM
and we're gonna go that size so you can
see where replication starts to break
down because if you're sticking
everything in every server you're
limited by how much free space you can
get in the in the heap which is pretty
small so luckily we had a meeting with a
customer who basically said well we love
your product but it can't work because
we got a store like 10 times that much
data and we thought well how would we do
that
and well actually I I miss predicted the
the slide so ignore my previous
statement I'll come back to that so you
started to see things emerge around like
replicated models and things like that
like session management so HTTP session
management there's one of the things
that in a clustered mode would allow an
application server to have for example
high availability where you'd kill an
app server and none of the users knew so
what was really nice about this when we
started seeing systems like this so this
was built into things like web logic
we built an implementation and our own
product as well to do things like this
and there were several others I think
there was one done by spirit soft which
is a another messaging startup at the
time and there was one by JBoss by Bella
bond from JBoss who wrote messaging
library called Jacobs and you know so
what was really nice about something
like this was was you could have an
application deployed on ten servers and
you'd have you know a tenth of the users
on each server and you kill a server
accidentally and you know no impact to
the users because the session
information that they were relying on
was replicated across multiple servers
now the really nice thing about this was
it allowed companies to start using Java
EE for pretty much for non-stop systems
where they would deploy an application
to one server start it back up taking
the next server down deploy it and start
it back up and this is what we called a
rolling a rolling upgrade so you started
to see how some of this in memory
technology was helping not just the
performance aspect of it but also
helping on the availability side so the
way we solved the the space problem was
partitioning and you've probably heard
of the word sharding as well charting
and partitioning are interchangeable
terms I was actually reading a debate
online this week on Quora in terms of
what to sharding mean what does
partitioning mean but like all marketing
words they mean please buy my software
so at least I'm honest so what we did
with partitioning is we said ok if we
have 10 servers let's just put 10
percent of the data on each one and find
a way to do it as automatically as
possible so you don't have to write code
to figure out what data goes where and
the way we implemented it as well was we
automatically backed the data up so we'd
put 10 percent on one server and then
we'd slice that 10 percent into 9 pieces
and put each piece on a different server
so we basically I guess it would be
called like striping or something in
raid terms we'd stripe it out and so now
you'd automatically route all requests
to whichever of the 10 servers had the
data you were looking for and if you
added an 11th server would automatically
load balanced all that data management
so it would kind of spread out or bring
it in contract in if you kill the server
things like that
so it was like automatic failover and
the nice thing about this was now you
could not just put more data in but it
started to introduce new new concepts in
fact one of the greatest concepts of
partitioning is what we call ownership
and what this means is that when you
partition when you split up the work or
the the state management or whatever it
is across multiple servers each server
owns the responsibility for slice of the
data for for partition of it and by
owning that responsibility it means that
you have a place and we'll get we'll get
into the details of what this means
later but you have a place to go to for
ordered operations against something and
this is actually the hardest thing to
scale so by partitioning if you have a
what we called definite ownership
semantics meaning at any given time
there's at most one owner for a piece of
information by having that you can
achieve order in a scale out system so
think about a counter for example if I
have to go from 0 to 1 to 2 to 3 it
sounds really easy
except you know if you're in a scale out
system you have to use something like a
mutex or a semaphore or something so
let's think about how we would do this
we'd like lock the piece of data so that
no other server would mess with it then
we'd go get the current value we'd add 1
to it we'd put it back in then we'd
unlock when you have ownership instead
of instead of doing that you can
actually send an operation to the data
and say I'd like you to add one
to that piece of value and because it's
owned somewhere definite inside the
scale out environment all those commands
if you will can be cued in a definite
order by the owner so it's completely
transparent to the to the consumer of
this but all that information and all
the operations against it are being
managed in a very predictable manner
ensuring that you know it doesn't go
from 0 to 2 or 0 1 1 2 or something like
that so that you know the you can
achieve very very predictable outcomes
also things like persistence because
there's exactly one place where that
data is owned you can read and write
from a database for that piece of data
from that one place you can read and
write from disk for that piece of data
so you're assigning ownership and by
doing so you have a definite location
within scale and environment for
performing certain types of operations
and so on top of this you started to see
an emergence of data management features
things like let me think of some good
examples so I gave the counter example
where work a counter example but the
counter example public speaking is it's
always interesting to hear yourself talk
and hear all the mistakes you make when
you're when you're doing this but you
know the counter is actually a like data
management it's no longer I'm not just
caching data or reading data now I'm
actually relying on that data to be
correct and so because you have a
definite ownership within a scale at
environment you can now start to send
operations to where the data is to
actually do mutating operations save it
to a database things like that and then
this is where we started to see that
line drawn between you know people
started to say why is this called
caching you know I understand it's in
memory but what you're doing is more
like data management than caching so we
had to make up a new market
so we could charge more money for the
software shall we called it the datagrid
and you think I'm joking but actually
that was serious so we actually kind of
in in our own company we split our
product into two editions one was was
more for the caching use cases and one
was for the the data management features
and then you know then what we saw was
you know as as companies were putting
more and more information into these
systems and by now by the way there were
probably a dozen vendors in this space
so let me name a few of them you
definitely don't wanna use any other
stuff come get ours but companies like
VMware bought a company called gem stone
they have a gem fire I believe the name
of product very very popular product
often is used sold what sold by the guys
who do the spring work at VMware so very
popular there JBoss has a new data grid
called infini spam is uh let's see their
data cribs well eh cache is a more of a
caching than a date a good product hub
but it's also very popular it was called
the easy hibernate cache that's where
the name came from there's a product by
IBM called used to be called object grid
I think it's now called extreme scale so
yeah that's it's another marketing name
so and then there's there's other
open-source ones there's a Giga spaces
yes Giga spaces which is based on the
jinnee
distributed computing model there's also
another open-source one that's still
fairly still fairly young but expanding
its called hazel caste any others
terracotta terracotta I wouldn't so
terracotta is the age cache product but
I wouldn't consider it a data grid but
it's a definitely a very strong very
popular distributed caching product and
they're probably moving more in that
direction so Greg luck from from eh cat
who wrote eh cache is also co spec lead
with us on jsr 107 which is the caching
standard that is ten years late so as
companies put more and more data into
this they had to suddenly had to have
ways to get it out right so they put all
their their crown jewels of their the
runtime systems into this
into these fairly non-standard you know
or emerging standard I'd like to think
of it as but they put all their data in
now they're like well how do I get it
out you know I have all these other
things so we started having to create
ways to access that information started
very early on with things like you know
see clients or dotnet clients or Java
clients to get in and out of the data
more recently what we've seen is a move
toward restful interfaces and of course
I should have mentioned one other
product open-source memcached I'm sure
you guys have heard of that and there's
a whole bunch of commercial offshoots
from that as well so memcached one of
the cool things about memcache is that
it has clients built in perl and c and
c++ and brain and you know like various
as a language nevermind
so lots of different languages and so
it's almost become a universal wire
format for simple caching use cases
because a number of products actually
talk memcached even though they're not
memcached in the back so memcached is a
fully partitioned model it's not highly
available and it's often not elastic in
any way but it's a it is very popular
because it has the right price which is
free and so let's see parallel
processing so once all that data was in
there and you were able to get stuff in
and out what we started seeing emerging
and from a couple different perspectives
as well depending on on the background
of the product was different ways of
manipulating that data sometimes for
what we think of today is like big data
use cases where you're doing like data
mining out of big data sets aggregation
work like that and sometimes you know
just queries like running queries
against that data building an
application using queries things like
that
and because everything was partitioned
it was possible to spread out the work
across those partitions since every
server was owning some slice of the data
instead of like going to one server and
getting that data and going to another
server and getting that data you could
send a command out in parallel and use
the aggregate aggregate memory and
compute capability of the scaled
environment to process the data
on speaking of scale-out environment i
just realized there's another company
that makes a product i think it's for
dotnet called scale-out software so
aptly named so there are a lot of
definitely a lot of companies in the
space now and then we've seen more
recently an emerging area called no
sequel databases and so no sequel
databases fall into a couple different
categories but one of the most common
categories it's used is what we call a
key value store and most of most of
these data grid projects and products
that I've been talking about are based
on that key value concept and in fact
the one I mentioned earlier gem fire
actually was evolved from a database so
it actually has a lot of kind of no
sequel roots long before there was a
word no sequel so so what you're seeing
is kind of a merging in two general no
sequel part of the market of a lot of
the technology used in in-memory data
grids alright lastly you know we've
started seeing the use cases expand
beyond kind of a traditional in a data
center use case we've seen quite a few
products now having ways of providing
redundant geo redundancy or redundancy
across multiple data centers where the
information even though it's in memory
it's it's often considered business
critical so you know if they lose the
power like actual data gets lost
sometimes and so they want to have
redundancy across across multiple data
centers you might ask why would someone
store something in memory if it's that
important and the answer is well they
wouldn't if they had a choice it's you
know if you need that level of
throughput or latency for operations
then
some of these some of these solutions
that you wouldn't traditionally consider
become your only possible solution so
most of these most of these solutions
also provide like I said earlier
persistence of capability to start a
disk or store to a database and most of
them also provide both synchronous and
asynchronous options for persistence so
even though you'd lose the if you lose
the power you could lose data you might
only lose you know the last smidgen of
updates so a relatively relatively small
impact which for some applications is
completely unacceptable and for others
is quite acceptable so I want to look at
some some core concepts and these are
the ones I was talking about earlier
where you know I want you to think about
you know why these systems were built
the way they were why did they evolve
the way they were and how can you take
these patterns and apply them to
application development in general
things like scalability and high
availability so we talked about
scalability like how do you achieve
scalability and I'm not talking about
scalability within a particular server
I'm talking about scalability that has
to go beyond a server and there's
there's two aspects for this you know
the first aspect is you want to be
answering the question as early as
possible so for example the most
scalable way to build a web application
is to push as much of it down to
something like Akamai as possible and
the reason is that Akamai has done all
the work of going around to every
University every ISP every this that and
the other thing that has a connection
somewhere in putting a server there so
that when you get on from home and you
know the page you're requesting can come
from Akamai it doesn't go anywhere you
know compare that to if you know you put
one server somewhere and you put a
website on it and then everyone goes to
that one server you know it's it's
you're using you know long paths if you
will through the internet so you're
creating a scalability problem where if
you can push it out to the edge or
beyond you're actually kind of solving
the scalability problem before it ever
exists now the problem is is you can't
do much easily with Akamai or solutions
like that content
content delivery networks so they're
great for static content they're great
for content that can be generally
out-of-date but even though they have
features to support some dynamic stuff
it's a diminishing returns it gets much
more complex for much less return so
then you know what's the next way to
solve scalability well our tician and
again we talked about why this works for
data but partitioning in general says
you know I have to find some way that I
can take anything that has to be done
and break that set of things that have
to be done up across multiple resources
horizontal scalability of course is what
we're talking about here and once I
spread out the ability to manage those
things I have to be able to balance my
load across them which is of course
fulfilled by the role of a load balancer
so what we're talking about here when we
talk about partitioning is we're talking
about designing applications for
elasticity being able to have an
application that is capable in its
design of being run in a scale-out
environment and in doing so you may be
building an application that runs fine
on one server but if you don't design it
to scale out it won't scale ability
isn't something you glue on to an
application it's not a you know it's not
something you tack on after you're done
so typically you you know you have to
rewrite applications to make them
scalable because you have to actually be
architected and then the last point here
I say only buy what you need and what I
mean here is that scalability is hugely
expensive particularly as you add
qualities of service to scalability so
what I mean is if you want like data
consistency like absolute data
consistency and absolute durability and
transactionality and things like this
each one of these qualities of service
comes with a big price tag in there I'm
gonna play liberties with math here
they're exponential right so if I pick
one it's not that expensive but if I
pick two
and put them together like I want
transactionality and durability yeah you
know all of a sudden those costs
compound significantly so why is that
well because the only thing that doesn't
scale is data consistency that is
literally the only thing that doesn't
scale if you want something to be in
order it doesn't scale if you want it to
be and by the way because you can think
of it kind of like a Turing machine
concept but in a distributed system you
can define all state management by
messaging and all messaging by state
management which means things like a
little theoretical I guess but things
like ordering and consistency of
information are actually the same thing
in a distributed system and this happens
to be the only thing that that that
limits scalability in a distributed
system so when we talk about you know
scalability and why it's hard think
about it this way like if I'm building a
system and I just whip it up and throw
it on a server and it runs really fast
kind of like the the early example I
gave of application servers yeah it's
really fast I've got RAM and I've got
CPU these things are fast right and then
I start I start by adding something like
persistence
well SSDs are pretty fast but they're
still they're not they're not like Ram
you know I have a spinning disk
well it's local you know I've got
aggregate i/o if I scale it out you know
each server has its own local i/o so it
scales well but you know by the time I
start talking on the network I've
already lost right you know so I go
asynchronous stuff on the network yeah
it's scales pretty well or if I
partition meaning I'm using synchronous
requests on the network I can see the
scalability dip a little more you know
and at the worst end of it we see things
that are centralized which is for
example a traditional database model so
I don't database to scale well no
because they're at the bottom of the
list because they're centralized because
everyone has to go to one place to get
the right answer so and by the way it's
interesting if you think about
today went to the wrong Stein no sorry
guys
what I was gonna say is it's interesting
if you think about databases why they're
hard to scale it's because they start
with an incredibly high quality of
service like they guarantee you know
some sequence of transactions will take
you from any previous state of the
database to any current you know state
of the database and that right there is
what I was talking about earlier in
terms of ordering anytime you have to
guarantee ordering you you lose
scalability so what about high
availability something else that
typically is needed by applications well
the idea behind I availability is really
simple don't have any single points of
failure like be able to shoot anything
in your data center and not have the
system go down well it's pretty obvious
but how do you implement high
availability well let's look at three
different ways so the first way is to be
stateless if you don't have any state I
mean you can cookie-cutter deploy
something that's stateless because you
can send a request to any of them and
it'll work the same way because it's
stateless it doesn't manage any state
now the problem is with stateless things
is is they scale well because they put
the load somewhere else okay because
there's stateless they delegate state
management somewhere else that's why
stateless applications which was such a
great thing supposedly when Java EE came
out originally and I just dumped all the
load on the database and even though
it's tempting to consider it a
conspiracy theory by a vendor that sold
very large servers in the vendor that
sold databases tress on an Oracle you
know I think it was more just a
reflection that that something that
worked well in academia didn't
necessarily translate to the real world
so statelessness was considered oh
that's the up-and-coming thing in
academia but when they actually applied
it in the real world they realized all
they were doing was moving the
bottleneck to somewhere that was more
expensive which of course is why Sun did
so well and Oracle did so well at the
time because everyone was buying giant
Sun servers and putting Oracle on it
let's see
so stateless how about cached why do I
put cached here
well because high availability also
means it's not just that the servers
running they said it can answer a
request within some period of time right
so if I can cache data and I don't care
if it's up-to-date I can answer the
request on that server without going
somewhere else
so that makes the system intrinsically
more highly available because each
server that I'm sending load to can be
caching information and servicing
requests in lastly replication so if I
can't be stateless and if I can't deal
with data consistency issues with
caching I have to have some way of
replicating information now the database
replicates to disk makes sense so that
if you lose the database ie the running
process you can always go to the backup
which is on disk itself now what we're
talking about in a scale-out environment
is replication and memory typically so
these two things though turns out they
go together it's kind of like peanut
butter and jelly so scalability and high
availability end up often being both
requirements of systems but it turns out
that they're actually solved in the same
way
in other words scale-out is redundancy
so if I'm scaling out a stateless server
to handle more load it turns out I'm
also creating a system that's more
highly available because it has multiple
instances of those servers
load balancing is failover the fact that
I can send a request somewhere and it
makes sure that it gets to one of those
servers that I've scaled out to that
means if one of them dies it's going to
make sure that the next request goes
somewhere else to another server and geo
distribution is again it's a form of
scalability across geographies and a
form of high availability so stateless
like I said it's always H a cached it's
H a if I can tolerate the consistency
and the only thing that's really
challenging here is replication state
management and the reason is is because
first of all you have to partition it
that's the only way you can scale state
management man is to partition it and
secondly
nice replication particularly over a
wide area network is hard so so why
don't we talk about performance like why
are we talking about scalability I mean
isn't isn't what we're actually after is
something that performs well and the
answer is that performances is is the
red herring right I remember early on
there'd be you know companies with
disastrous problems with their web
applications and they they'd have an
engineer oh let's profile it right well
look you could make the code a hundred
times faster and have no impact on
performance because what you're doing in
profiling is you're looking at you know
single thread going through that system
the real problem the reason it looks
like it's performing badly is because
it's not scaling okay if it's scales
then you know the thousandth user gets
the same you know the same performance
is the first user okay so if the first
user has poor performance then you start
profiling right but the real thing you
have to focus on is scalability because
it's something that has to be
intrinsically designed into an
applications architecture it's not
something you can add later while
performance if you go back to was a guy
in the Navy that said the root of wasn't
anything the Navy the root of all evil
is premature optimization or something
like that who said that was a Donald
Knuth or all right hmm I'm not sure
youth okay so the root of all evil is
premature optimization so solving
scalability first let's you later try to
address things like performance so what
we talked about is scalable performance
not not performance scalable performance
is knowing that basically if you boil it
down as concept it says look if I can
achieve this much with one server and I
add another server and another server
and another server
I want my third server in my fifth
server in my 500th server to be a cheat
to be providing the same amount of
throughput and from an end-user point of
view the same latency on on requests so
basically it says I'm going to design
systems such that as I add resources
I've designed the systems
as I add the resources I understand how
those resources are going to translate
to additional throughput
okay and that's why partitioning is
absolutely essential so let's talk about
where are we now so what's what's going
on with with this with this part of the
market today well it was a very rapidly
changing field for for a number of years
I this was probably one of the most
competitive areas in the industry like I
said there's a lot of different products
and open-source projects attacking this
area and so what you saw for years was
that you know the features were coming
so fast and furious that you know
products generally didn't keep up on on
some of the the other aspects of what it
means to be a product like being able to
manage and monitor it and you know we
had entire new classes of problems that
we had never never faced before like you
know you have one server generating a
gigabyte a log a day but what if you're
running a thousand servers right now you
say ok mail me the logs we've literally
had you know customers send us hundreds
of gigabytes for one log dump like wow
thanks what do I do with this so you
know you've actually seen companies
emerge around these problems that's like
Splunk I don't know if you've heard of
them you know they do log mining across
multiple servers and things like that
integration like how do you plug in
other things with these technologies
because early on you know these
technologies were used as kind of point
solutions to solve some problem that no
one else could could solve in a
conventional way so to speak or the
alternative is that people just found
the technology so interesting that they
plugged it in even though they didn't
need it we've seen a few of those but
generally speaking I mean these were
technologies being used to solve very
hard problems but you know after it was
in place you know what about you know
what about the next generation of
developers coming along they have to
plug into it and you know products that
have to work together around information
you know that's high velocity
information and typically massive
massive datasets and an area that we're
involved in investing in quite
significantly in our own products is
this
concept of engineered systems so making
sure that the hardware and the software
are optimally designed for each other so
I just give one one quick example there
you know we've been working a lot on low
level messaging anybody know about 0 mq
ring a bell
one two three excellent it's a sea
library for a very low level messaging
there there are other products in their
projects and project products in the
space but if you think about it the
first thing you do is you're building
distributed systems whether it's
something like this or or something else
is the first thing you do is you create
messaging so that you can do message
passing as a basis for building
distributed basically distributed state
machines and your look
okay so I need to do messaging though so
what do I have in Java to do messaging
with JMS well that's hate to say it's
not low-level JMS may be hard sometimes
but it's certainly not low-level it's
it's fairly heavyweight so what you need
as an infrastructure common
infrastructure for all distributed
systems of some form of you know very
low level messaging I mentioned J groups
earlier by Bella bond as with JBoss very
popular one and there's others but what
we've done is we've actually created new
api's that we hope to eventually push
into Java for explicitly for low level
messaging and these are asynchronous
api's meaning the API itself is
asynchronous not just asynchronous
messaging and so on our ex illogic
hardware we now have reliable delivery
and I'm not going to give exact numbers
but well under 10 microseconds reliable
delivery and those are you know those
are 4k payloads under 10 microseconds so
Java you know I expect Java to become as
it has in many other areas I expect it
to become one of the go-to platforms for
for low latency systems in fact I can't
give any names for NDA reasons but the
largest most successful financial
services company in low latency
actually all their work is done in Java
already and they've built their own
InfiniBand based software for doing this
so it's it's just funny you know five
years ago you wouldn't have thought of
Java as being the low latency language
but today it is I mean it is basically
it's I'm seeing it everywhere and
hopefully we'll have this coming into
the JDK before too long so that's a so
which technology am I talking about
so I'm talking about InfiniBand based
messaging not part of Java yet it's
something that we're going to propose to
to become part of job
so the question is is this something
that's a product that Oracle has today
it's not actually a product it's kind of
baked into a couple things that we have
today but we're trying to prepare it so
it'll be separable from Oracle products
and we can push the technology actually
into the JDK so first we got to prove it
out that everything works and we love it
and then then once we're once we're
perfectly happy with it then then we'll
push it push it out as a proposal for
for Java so and that's by the way that's
also in terms of throughput
we're actually limited only by the
hardware bus in the servers so we're
actually pushing it pushing data so fast
that the only thing limiting data flow
is the the actual backplane of the
server the PCIe bus so we're over 30
gigabits per second sustained throughput
pretty pretty crazy all right let's take
another look at it so you know what are
the things coming up and around this
technology so first of all I'm pleased
to announce if you haven't already heard
that jsr 107 which I'm a spec lead for
and I should have finished 10 years ago
it's finally it's finally coming to
fruition and will be in Java EE 7 so and
I will tell you my experience with this
is that it's really hard to do a jsr
from start to finish it's a lot of work
so I hope you guys are willing to to
pitch in obviously the JCP is the java
community process and it's free to join
you know and while oracle definitely
contributes quite a lot to the
development of java you know this is a
it's also a platform that's you know
shared across the industry so we welcome
we certainly welcome the involvement of
the community and we've been working
very hard to add transparency we
actually have the first step in the
transparency process which was jsr 248 I
think so that's done so now all of the
work going on in expert groups is public
if they've adopted the latest version of
the JCP and
we have a second phase already underway
in terms of the transparency project so
so also the in finis fan guys from JBoss
led by Minister Thani started a new jsr
as well based on infinite spans so
they're looking at standardizing some
api through four data grids I'm not
actually aware of the current status of
that project so I I can't really comment
on it other than the fact that it does
exist another exciting area of
development no sequel like I said
earlier you know a lot of the concepts
in no sequel actually came came out of
this space and you're seeing definitely
some blurring blurring of lines I won't
go into too many details there but you
know a lot of the times no sequels being
used for caching so in-memory data grids
are obviously ideal for that and a
number of them have already also added
or already had disk based persistence as
well for the for the data grids so even
though they're running in memory they're
fully fully durable as well big data is
another area that we've seen a couple
vendors focus on recently so big data is
a little bit hard to define because like
a lot of other terms that we use it's a
it's a marketing word so it gets VC
money but it doesn't necessarily doesn't
necessarily translate to the real world
so sometimes big data means a gigabyte
sometimes big data means a petabyte and
there's a big difference between a
gigabyte and a petabyte at least four
letters and six orders of magnitude if I
understand correctly or nine I don't
know math wasn't a strong suite for me
but so when we talk about Big Data we're
not necessarily talking about the size
of the data as is as we are talking
about we're trying to do with it and
basically concepts behind Big Data are
you know how do I go through lots of
data very quickly how do i scale out the
processing of datasets so get big data
might you know I don't might only be
working with 10 terabytes and so you
know by by your definition that might be
tiny data
or it might be huge data but it's not so
much how much data you have but it's how
you go after it so things like MapReduce
for example so a number of products have
MapReduce functionality in them now in
the data grid space so parallel you know
parallel execution and things like that
then lastly a cloud so cloud is actually
a culmination of when we talk about
cloud computing it's actually a
combination of many things that we've
been trying to do for years in our
industry you know which is you know the
democratization of the data center you
know how do I take the resources in the
data center and make it available when
needed to the people who provide value
to my business by having those resources
so traditionally that meant going
through IT you know I need a server ok
I'll order one never be nine months
actually at Oracle it takes longer than
that it's probably you know fifteen
months from the time that I need a
server to the time I get it unless it's
an emergency so on the other hand I go
to Amazon and I have a server in 12
seconds that's the democratization of
the data center and if you can tie that
concept in with you know supply and
demand if you will if you know for
example it's charged to you when you
have it like Amazon does of course
because they make money on it you know
then you're not gonna be wasting it like
if you look in a typical data center
most the servers are doing nothing
they're literally the utilization
depending on the study you read is
somewhere between one and two percent
just they're just burning dinosaurs that
is wasting electricity so what cloud is
about is finding a way to turn the
resources of our data centers into you
know rationally consumed resources and
by pushing you know pushing the these
resources out as as self-service for
example infrastructure as a service but
within the cloud it creates new
requirements and in one of those is that
you know I want to be able to have build
applications that I can deploy into a
cloud environment and you know and then
I want to go home and sleep I don't want
to be like monitoring you know
a dashboard from my system if I need
more throughput if I have more user
requests and I expected I'd like it to
start up another server and another
server and another server I'd like it to
scale out and so what you see is that
the concepts that we talked about today
are absolutely essential for this type
of system because every application
should be built from the ground up with
these things in mind when I say every
application I'm not talking about what
you play on your iPhone I'm talking
about what sits back on the server so
every server-side application should be
built from the ground up to be scalable
and to be highly available because I
don't know about you but I very rarely
meet any application developers who are
building something that isn't considered
at least by them to be mission-critical
require high availability and they'd
like to believe it's going to be so
successful that it's going to have to
have lots of servers to write so anyone
here not build applications that need
high availability and scalability so
either everyone's too embarrassed to
admit it or or it's true that you'd like
to have high availability and
scalability so when you think about how
we're pushing these things for example
into the standards process and things
like that what we're trying to do is
we're trying to take these these things
that used to take rocket scientists to
build and we're trying to democratize
them as well if we can build them into
the standards such that they're easy to
consume so that you get these things not
for free but by understanding you know
common design patterns and the API is in
the platform itself that you'll be able
to build every application from the
ground up such that it can scale and it
can be highly available so that's why I
think I think this is a pretty exciting
area obviously I have a preferred
preferred implementation of all this
because I hope to build one but it's
still an incredibly exciting an
incredibly exciting field with just so
many so many things going on every day
it seems there's there's new inventions
and new new new approaches to solving
these problems so just a little bit of
look in terms of some of the areas that
we see emerging so
let's see I don't want to talk about
some of these the one of the projects
that we're doing it's called elastic
fabric none of this is visible yet but
it's a concept that everything that
we've learned in terms of distributed
computing and how to make things
reliable and scale well is is that
basically the network is not your friend
like everything that works well doesn't
work on the network in fact the network
only works when you're developing if you
like put something in production that
always fails the network's unpredictable
the machines aren't everything there's
so many unpredictable things it's like
it's like trying to build a house on top
of a bunch of lily pads everything's
moving up and down you know it's like
you're sinking you're the networks very
unpredictable how do you build a stable
platform in a distributed system so you
can build on top of it right and the
answer is is really simple actually it's
it's a concept of a state machine like
if you want to eliminate edge conditions
and software to go back to computer
science 101 which is as far as I got
and and you'll see there's this concept
of a state machine like I have a state
you asked me to do something I have a
new state right and no matter how many
times if I had that state and you asked
me the same thing I'd end up in that new
state it's like there's a predictability
to it it eliminates edge conditions by
design
hey and these you know these concepts
are built on you know ordered messaging
and state machine concept and there's
number I've been trying to read myself
to educate myself on some of these
topics you know what's been done in in
academia in the past on all these topics
and they're there a number of papers on
this quite quite interesting so when we
talk about elastic fabric what we're
talking about is how do we push down all
these things that we learned about how
to build state stateful scalable
architectures how do we push them down
so that they just become natural parts
of everything that we do
and basically what that means is you
know having I already talked about it
you know low level messaging support in
in Java and on top of that having you
know having that concept of being able
to have scalable scale out
partition about state machines as well
so that's that's something that we're
also designing we see a lot of use cases
around exit what we call XML I call it
mail processing grids but XML grids so
grids where I'm trying to take all this
data and make a consumable in fact more
recently we've seen a move toward rest
restful interfaces XML and JSON now
particularly with a push toward html5
how do I make this information you know
be able to push all the way down to a
browser client and how do I make the
information consumable by by any
language in any client and then you know
from a management perspective moving
much more toward autonomic computing so
fully lights out you know computers that
manage themselves so I've always wanted
to have a project named Skynet to do
something like this but you can imagine
that you know that's that's what we are
trying to build we're trying to take
things that we used to have to do by
hand and push them down into what we
think of traditionally as an operating
system I don't know if any of you did
like programming years ago with like
multi-threading or symmetric
multiprocessing back before we had
things like synchronized keyword and
java or something like that and it was
it was really hard right and in fact you
know back before there were threads how
did you do multi process you know if you
had two CPUs SMP what were the if you
didn't have a thread as a concept how
did you do SMP and you know these things
they just seemed obvious now it's like
well yeah we have threads and we have
Atomics and we have transactional memory
and we have you know synchronized
keyword java and all this stuff like so
we pushed down into the platform all
these things so when we talk about you
know multi servers how is it different
from multi threads right we think Oh big
deal I have a hundred servers today well
you don't say big deal I have 100
threads today when's the last time you
had an app server that had less than 100
threads running I mean well don't answer
that question you probably have but
there are you know quite often you look
at a server and it's got hundreds
hundreds of threads running and you
don't think anything about that now but
that was huge ten years ago
so how ten years from now how can we
look back and say remember a stubrag I
had a clustered system like the word
cluster should go away because it should
just be the normal right scale out
should be the normal elasticity and
dynamic elasticity should be the new
normal you know so to do that yes we
need innovation in the platform and
everything else but we also have to
drive it down and make these things just
absolutely bog-standard where every
application we build looks like this and
by the way then you will be able to
sleep a lot better at night because you
know you get unexpected load you get
unexpected outages you know your systems
are gonna just keep running and that's
that's you know that's what our job is
our job in some sense is to obsolete
ourselves but not because we're trying
to put ourselves out of a job but
because there's gonna be something even
more exciting over the next hill some
some some next problem that we're gonna
have to solve all right and with that
I'd like to just say thank you there's
some other sessions of interest perhaps
so I'll put that up there but thank you
very much I don't know how we're doing
on time but if we do have any time I'd
be glad to take some questions so you
had a question yes so the question is
you have caching and you have no sequel
and they're they're performing the same
purpose how do you choose one or the
other so the answer is pretty simple
which is you know every one of these
products or solutions has a sweet spot
for what it's trying to solve so you
want to look for something that has a
sweet spot that matches what your
problems are because basically it's like
turing-complete you can use any one of
these products for any problem right
golden hammer approach but you know
you're gonna end up with something
that's quite suboptimal so look look for
the types of products that are built to
solve the problems that you're facing
specifically that's what they specialize
in and then within that regardless of if
they call it a datagrid or no sequel or
new sequel or next sequel or I mean
there's all sorts of big data who cares
what they call it is the problem set
that it's focused on solving the
problems that you're facing
you know narrow down the list to a small
number of solutions and then evaluate
them see how they actually apply to the
problems that you have in your domain so
and it's like I said it's a very rich
field so there are many many products
out there so it's almost guaranteed that
no matter what your problem domain is
there's at least two or three products
focused on solving your problem
excuse me can you get the slides they're
gonna be posted right okay so the slides
will be posted on on the Oracle website
okay and that's the same place that
they're posting all the Java one convoy
okay one minute
so if you already have everything in a
database and you want to if you will
refactor we architect around at the
start to bring some of that out how
would you do it well applications tend
to fall into two general categories so
I'll try to answer this from a fairly
high level because it's easier and did
you know every application is going to
be challenging in its own way but
generally the applications go after data
by in a relational manner or it goes
after it an object manner so if you look
at an application is it using hibernate
is it using toplink or eclipse link get
JPA
code oj do one of these products if so
what it means is the applications
already has a layer in it that's
designed to bring the relational model
abstract it up into an object model and
in that case it's actually fairly easy
to adopt technology like no sequel or a
data grid or distributed cache because
the information is already being managed
in a granular form that at an object
granularity on the other hand if you
open up the application you start
looking like oh this JSP has some sequel
pasted in it and this one has some
sequel pasted in it that's a bad sign
that means that basically you're gonna
have to change every component in the
application because they're all going
directly against the database right so
it's it's a it's a very wide front if
you want to think of it in the military
you know they have it's one man deep and
it's you know 50 miles 50 miles long so
that when you see that you're you're
probably in trouble and there are you
know there are certainly solutions in
that space as well how you accelerate
database connectivity how do you shard
databases etc but if you're looking to
adopt this type of technology and you
find sequels sprinkled all over you're
in trouble
so his Oracle focused on doing that as
well within the database group
absolutely so they have a couple
different solutions for this a of a
times ten database which is an in-memory
database it's lickety-split fast there's
also solutions from like IBM and others
as well in the in-memory database space
and Oracle's probably number one answer
for this is rack and Exadata which is an
engineered system built around rack and
custom hardware it's hugely expensive
it's a very high margin product because
it's because there just isn't any
competition and then massive like if you
look at the numbers of transactions I
don't have them offhand but I've seen
slide presentations on exa data it's
like how many million transactions per
second like it's it's amazing but yes it
is expensive and that's that's the
reason you want to design for
scalability because you don't want to
get trapped where your only answer to
get out of the scalability problem that
you're in is to buy some you know 10
million dollar machine although work
we'll be glad to sell you one with 22
percent maintenance all right so thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>