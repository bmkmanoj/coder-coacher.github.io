<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Utility Industry - Outlier Detection | Coder Coacher - Coaching Coders</title><meta content="Utility Industry - Outlier Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Utility Industry - Outlier Detection</b></h2><h5 class="post__date">2017-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eP0TiTtP3iU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in the world of utilities the ability to
monitor outlier usage activity on the
power network has become a major
requirement this ensures that the power
generation companies can easily
determine the dwellings across city or
town districts that are consuming the
most power and can dynamically adjust
the power network accordingly to satisfy
demand or price charging rates
in this use case scenario the assumption
is that in every apartment and house
there are small plugs which monitor and
send the current wattage usage as
streams to the Oracle stream analytics
platform
the shape of the streaming data is shown
here in this case the initial properties
relate to the usage values and as we are
only using and interested in the load
measurements we will filter out the work
related values during processing the
remaining shape properties relate to the
unique identifiers of the plug and the
house
so in summary the goal of this
demonstration use an available sample
simulation data is to calculate the
percentage of plugs in each dwelling
over the last hour that have an average
load greater than the average load of
all of the city or town plugs
to explain the analysis performed by
Oracle stream analytics to identify
outliers here is a graphical
representation shown how the time series
energy usage data is plotted on the
x-axis with the load value depicted on
the y-axis
the white line shows the town or city
average and the remaining lines relate
to each plug in a specific dwelling for
each hour a percentage can be calculated
for the number of dwelling plugs that
are consuming energy above the town or
city average at that point in time
you
here we have the Oracle stream analytics
catalog with some predefined connection
artifacts for the Kafka messaging layer
and the Oracle database let's start the
simulation data
we now define the SmartPlug stream with
the event shape dynamically detected
from the streaming payload
you
okay artifacts created and finished
let's create our first pipeline
application
let's kick off with the outlier
percentage pipeline using the plug
string
you
the live apple stream with the
identified shape arrives on the canvas
from the various smart plugs
our first query stage will cleanse the
streaming data
you
using the analytical power of stream
analytics we can easily filter out
unwanted measurement information and use
only load related data that has an
energy usage above zero
the live output stream columns can be
easily renamed for clarity
we now use expression builder to create
a new composite key column from the
household and plug ID
now we have a unique identifier for each
plug for later downstream query usage
you
now we create a group stage to calculate
the average loads
you
here we will define free aggregates
firstly the town or city average the
second is the plug wise average load and
the third is the total number of plugs
in the dwelling over a 10.1 hour period
you
the live output stream columns can be
easily renamed for clarity
you
let's now create another stage to
compute the outliers
you
firstly we filter on the plug average
load with the city average load
you
next we calculate the number of plugs
whose average load is greater than the
city average load and we can rename the
resultant generated column
you
Cullum's can now be removed as they are
not needed for the final analysis
a next query stage will calculate the
percentage of outlier plugs
you
once again the power of our expression
builder is exposed as we create the
required algorithm column and we name it
accordingly
you
now we have all of the information
required analysis to satisfy the goal of
a use case
so now we can add some very cool
visualizations the first is the
percentage outlier plugs analysis bar
chart
you
we can also add another bar chart
visualization for outlier plug counts
you
so this completes our first pipeline
vacation
you
and here is a quick review of each stage
you
now we will add a target for the event
shape results that will be sent to a
target Kefka topic
you
this will be used by our next pipeline
application
you
we now publish a pipeline to send the
results to a Cathcart topic
you
now we want to add some geospatial
analytics with this new analytical data
firstly we create another stream that
will consume the outlier data and call
it outlier stream again notice how local
stream analytics dynamically infers the
event shape from the streaming kathcart
topic payload
to get all of the locations of the
dwellings we can create a database table
reference for that persisted information
you
again so artifacts created so let's
create another pipeline application
called
outlier geospatial analytics
you
the live APIs stream with the identified
shapes arrives on the canvas for the
house outlier event data
now we had a query to enable the
classification of each house consumption
you
here we can add a new column to set the
required load consumption level with a
default setting of unknown
you
now we will create a rule stage to
classify the levels for each dwelling
you
based on our consumption levels we can
set for category level rules low
moderate high and very high all now
represented in the streaming data
you
our next query will be used to retrieve
all of the dwelling locations
you
we joined to the database reference
based on the house ID value to get a
specific geospatial longitude and
latitude positions
you
we can cleanse the live output stream by
removing unwanted event data columns for
this use case
you
with this information at hand we can now
create a new geo spatial visualization
called outlier map for apartment complex
we map the required column attributes
the longitude the latitude and the house
ID and then we save the visualization
slice
you
on our map we can drill down to specific
locations
you
and we can customize this chart for the
various consumption levels using color
coding
you
now in our drill-down spatial map for a
specific apartment dwelling location we
can easily differentiate between the
various levels using the various color
coding
you
we can publish our application
you
here we can see we have our two pipeline
applications running
you
and we can create a new operational
dashboard artifact to view the created
visualization slices
we add the various visualization slices
to our dashboard canvas
you
and we are able to rearrange our
dashboard slice views perspectives as
needed for the best effect
you
the first view is the percentage of
outlier plugs for each dwelling next we
have the count analysis and finally a
geospatial color coded representation of
consumption for a specific area of
interest so there we have it a new
creative powerful utilities power usage
of outliers perspective solution with
multiple pipeline applications from
streaming analyzed data relating to
relevant house smart plugs which
provides a valuable energy monitoring
spatial and action management experience
19 days or months but in minutes
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>