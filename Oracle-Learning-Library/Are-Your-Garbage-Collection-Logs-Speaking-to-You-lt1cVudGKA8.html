<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Are Your Garbage Collection Logs Speaking to You? | Coder Coacher - Coaching Coders</title><meta content="Are Your Garbage Collection Logs Speaking to You? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Are Your Garbage Collection Logs Speaking to You?</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lt1cVudGKA8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so unfortunately when we do these
GGC talks we have to do the obligatory
here's how the GC thing is laid out so
bear with me with that at the beginning
hopefully you know we have a case study
at the end which i think is probably for
some of you would be the more
interesting part of the talk for those
of you that don't have enough of an idea
of the structure of what's going on in
Java heap then the the beginning will be
you should be happy with that okay so a
little bit about this talk here right do
your GC log speak to you the name really
comes from an incident that I had many
years ago when I was working in
Luxembourg actually at a bank and there
was a we just got a brand new
performance monitoring tool in and it
was really cool because I had all these
like squiggly lines going across the
screen just like all good performance
monitoring monitoring tools are supposed
to have and I was looking at the single
in the wild this is really cool and I
had this architect behind me he was
looking at it and says you know he's
French so I think I got some slavish
translation from some frantic French
expression writing saying is that
speaking to you and I said wow that's a
really interesting expression and I
looked at him and I said and it's also a
really wonderful opportunity I said yes
why of course
and I expected the next thing he was
going to say Isabelle what is a saying
at that point I would have said well I
don't really have a clue but you know
but anyways and I guess that's pretty
much how we look at garbage collection
logs right we just look at this massive
blob of numbers and then we just say
okay I'll just give the heap 10 gigs
because that sounds good and you know
let's work out the rest later on and see
what's going on because when I look at
this stuff is like yech ok anyway so
hopefully we can clear something up
today so a little bit about me so I have
a consultancy where I do training and
and tuning and have worked you know on
with performance tuning since about 1998
quite accidentally there's other stuff
but I think we can just use Google if
you want this is where we're very close
to where a friend of mine Heinz lives
he's over there in the corner and we
offer our a couple of times a year we
offer performance tuning course at his
house somewhere close it over there so
you can imagine there's a little bit of
beach time in there and stuff like that
but it's kind of fun and most recently I
started this tooling company called J
clarity so all of our tools and stuff
are actually going to be are being
developed there and being released
through that particular entity so
standard disclaimer at the front
any resemblance of an opinion
recommendation or anything offered
during this presentation that resembles
performance doing advice is merely
coincidental
in other words advice given at a
conference is not a substitute for
common sense so everything we say go
home and kick the wheels on it before
you take it as being golden right okay
why should I monitor do you see well
easy right juicy logs contain critical
information it's the information you
need to know how to tune the heap spaces
in your system without this you're
essentially just guessing and you're
probably giving up on a lot of
performance in your system I spend a lot
of time reading GC logs and I spend a
lot of time helping customers try to
tune their GC it's generally a
multi-step process that can take a long
time it doesn't take that much effort
but it can take a long time because it
just takes a long time to collect the
data that you need in order to do a good
analysis right what is the performance
impact of logging GC and production well
in my experience no one has ever felt
the performance impact of logging GC and
production unless their GC logs have
filled a disk or something like that
right and if your GC logs are filling a
disk that means you either have
incredibly small discs or far too
frequent GCS
how do I get a GC log well here's a
whole bunch of flags and there's dozens
more that you can use and you know the
more you use the more information you
get but the more complicated the logs
get so I tend to stick to just a couple
like the print juicy details and
printing the tenure and distribution
right and make sure that the data goes
to a log file instead of being written
to stood out which is what the verbose
GC does all the other things can give me
interesting information under different
conditions but those are the ones that
I'm going to stick with primarily okay
so what do you get okay for those of you
faint at heart close your eyes now there
we go right okay it's not quite that
ugly so if we're gonna understand what
this thing is saying and this is where
some of you might want to bear with me
for a second we actually have to
understand what it is we're trying to
describe here right so the first thing
we want to look at is say okay you know
what happened so we start up a Java
Virtual Machine and then we're basically
going to reserve a chunk of C heap and
we're gonna call that Java heap and then
we're just gonna lay out the data
structure right right right into this
big chunk of memory so we can set the
size of this by using this particular
flag here - MX and size and that gives
us our you know maximum heap size then
what we're going to do is we're gonna
create a generational space this is
similar to what you saw being discussed
this morning and during the advance JVM
tuning sessions we're gonna make a
generational which means that we're
going to subdivide this into a young and
old space now there's a couple of ways
we can do this we can use this we can
divide this by using a ratio or we can
divide this by just saying what the
young size is going to be and the
leftover is going to go to old
I prefer ratios because then I can take
advantage of adaptive sizing and what
adaptive sizing says it's going to like
let's use some heuristics here to try to
decide what's the best internal sizes of
these spaces in order to minimize
is the impact of garbage collection on
Iran our applications throughput now
what I'm going to do next is I'm going
to further divide young generational
space into these three space that's
called even survivor zero and survivor
one now you might notice on the slides I
have N and a one and A one right there
that's really gets to you know how do i
size that well I'm going to set this
thing called survivor ratio and the
survivor ratio is going to take all the
space in young generational you know
what if what I've allocated for young
generational space and it's gonna say
okay whatever the current size is right
now and that's determined by adaptive
sizing I'm going to apply now I'm going
to divide that into n plus two spaces
even gets n of them and each of the
survivors bases gets one okay and really
what we're going to see is that eden is
going to be consumed by our application
threads or garbage collection people
call them mutator threads so like you
are the aliens coming into our space
here and eventually we're gonna fill
that space and when we fill that space
we're going to get an allocation failure
and then these things call garbage
collection threads are going to run and
they're going to do something right now
this is a shared data structure it's
multi-threaded that means we have
concurrency concerns how do we manage
that well we're going to go into this
activity called safe pointing what's
safe pointing is is we're going to take
our heat our sorry our Java code and
we're gonna sprinkle it with these
things called safe points and when the
garbage collection is going to be
triggered the first thing that's going
to do is touch a safe point then says
mutator threads when you actually touch
a safe point you have to stop and then
when all of the mutator threads are
stopped then the garbage collection can
proceed right and that gives us our full
stop the world pause that we've all come
to expect love and embrace right right
and so you know if we need to get
information about safe pointing then of
course there's another flag we can set
and that gives us even more
Meishan about what's going on in this
particular case so the garbage collector
is going to run and is going to recover
all de-reference memory it's gonna do
this two ways either we're gonna have a
scavenge we're gonna have a full GC or
well okay so that's really the two types
right so the scavenge means I'm only
going to collect one pool I'm either
gonna collect young or tenured and so we
have a pärnu collector which will
collect young generational space and we
concurrent mark-sweep collector which
will scavenge through tenured space
right the full GC is going to collect
all memory pools both young and tenured
space so that's really the difference
between the two to do that we're going
to use this thing called mark and sweep
the mark and sweep starts with well
we're in a safe point and then we have
mutually exclusive access to everything
we're gonna do a scan for routes which
means I have to go through right you
know for example let's say Eden we're
gonna collect Eden well then I have to
go through survivor spaces and I have to
go through tenured and I have to go
through you know my my thread stacks and
I have to find all of the all of the
things in those spaces that are our
references pointing into Eden right that
becomes part of my GC root set and from
the GC root set what I'm going to do is
I'm going to start and trace and I'm
gonna find all live objects thank you
it's probably annoying the hell out of
everybody sorry
okay so I'm gonna find trace all live
objects and in the case of Eden what I'm
going to do is I'm going to evacuate all
of those live objects into the inactive
survivors base and then I'm going to go
and I'm going to do the same thing for
the survivor space and I'm going to move
all of the all of the live objects from
the active survivor space into the
inactive survivor space and then what
I'm going to do is I'm going to make
that inactive survivor space active so
you can sort of see something here right
mutator threads are mostly allocating
objects while they're allocating objects
in Eden most of the time
right with a few edge cases and all of
the other spaces right the allocation
happens via the garbage collection
threads right so we say this sweep in
this case from the markets we're in a
back way all the objects are right well
it's safe well sort of there's some
certain things that can happen here
eden is obviously larger than the
survivor spaces so if we have too many
people's too many objects surviving the
attempt to collect god I sound like a
mass murderer with that slip-up right
Freudian if they survive the attempt to
collect then what happens is that we get
this condition called we're gonna
actually if we move everything into the
survivor space we're gonna flood that
memory pool obviously we can't do that
so we have to do something to clear that
out which means that might be a direct
promotion into directly into tenured or
we might roll back the tenuring
threshold so hot spot will come along
and say well I know you said the
tenuring threshold should be four but I
think it should be too in order to make
room so we're gonna get rid of you know
anything that's older than age two and
put it directly up into tenure space and
that's a premature promotion event right
so when I speak at inuring what I'm
talking about is the age of an object
and the age of an object is determined
by the number of collections that it
survives okay so with that you probably
think that okay frequency of collection
is probably important because if garbage
collections happen more frequently well
you sort of have these orthogonal events
you have the garbage collector running
right and then you have your application
determining a lifetime of an object and
these sort of things you know they run
sort of orthogonal to each other right
if the garbage collector runs faster
more objects survive because your object
your application has isn't finished with
them right so you say you have an object
in you takes you a second to process it
right if the garbage collector is
running at five times a second that
means it's going to get to age five
before it actually gets collected if
you're running at once every two seconds
then you know is going to be collected
after you're finished with it so you can
see that the frequency of collection
here actually has some bearing on how
objects actually travel through these
spaces what we want to do is we want to
work hard in this case to make sure that
we collect everything in the young
generational space because when when we
get the collection in the young
generational space that's that's the
cheap one by the time we get to tenured
as I mentioned earlier for the people
that intended the other talks right
there's a lot more work that has to be
done because that's an in-place
collection we cannot compact with a
concurrent mark-sweep collector we have
to have a full parallel GC in order to
do a compaction right and and and and we
have to maintain free lists and there's
a whole bunch of extra work that we
don't have to do when we can evacuate
because when we can evacuate what we end
up is the situation where okay after we
evacuate all the objects out the whole
space is empty week it's free to use
again right so you know where's the free
list well it's the space itself right
allocation into Eden is a pointer bump
right there's no nothing else that we
need to do in that particular case right
so you know we need to look after free
list management fragmentation possibly
compaction stuff like that okay so
here's the actual code from the from the
open JDK that determines which
collectors you can actually use so I've
already mentioned something right so we
can either have a serial or if you use a
concurrent mark-sweep which is which is
a partial GC and tenure we can use that
with a pärnu the reason why is some of
these are paired is because you know
Java heap is a data structure and we
have to agree upon a common you know
shape for that particular data structure
in order for these algorithms to
actually work be able to work on it and
these things evolved over time and of
course the shape of Java heap has been
optimized for each of these particular
collectors so if we can use a parallel
GC in old then we can use the then we're
going to
this old parallel GC for young
generational space and of course we have
this new collector called g1 GC which I
won't focus on so much okay so that's
our choices for collection strategies
okay so back to our GC lot let's fit all
of this information into what we just
learned and let's just focus on this
particular log record here so if we look
at the colored bits here what we
actually have is well the first one is
number seconds since the JVM started
that's when the collection was called
for now sometimes you get another time
inside the little square brackets there
and that timestamp usually refers to the
time when that particular activity
started and sometimes there's a lag
between when the GC was called for and
when the GC actually gets gets going
when that happens then that's when I
might want to look at the safe pointing
behavior in the application
maybe there's something preventing the
threads to actually get to a safe point
the green bit is the actual stop the
world pause time so one of the things we
want to do is we want to pull that green
bit out and we want to add them all up
and then we're gonna divide it by the
run time and then it's going to give us
our Jeep GC throughput number our GC
efficiency right obviously we want that
percentage to be very low the other bit
the blue bit says the algorithm we're
using this is using a PS young gen which
means you're using a parallel scavenge
which means we'll be using a parallel
old collector old gen collector right so
there's a bit of structure here so we
like I said we have these these square
things that sort of define where try to
define where records begin in and
unfortunately when you're running a
current current a concurrent mark-sweep
collector with a para new since both
collectors write to the log at the same
time and that write is not synchronized
things get funky
shall we say okay so what we have on the
outside here is in the in these yellow
bits is well the this nice record that
says what's happening globally in heap
okay
so what we're gonna do is we're gonna
look at that when to say okay our
configured size which is different than
a reserved size in this case so it what
are configured sizes is what adaptive
sizing is going to tell us that we it
needs to be to try to maintain high
efficiency of collection then you know
if we look at this one here we're going
to see that that's our occupancy before
then we collect that's our occupancy
after and then all of this black stuff
here I purposely use white gray and
black because of the in garbage
collection nomenclature grey means I
don't know if it's dead or alive white
means I know it's alive and black means
I know it's dead okay so in this case we
have occupancy before occupancy after
and then we have recovered sorry and if
we take the before after subtracting
then that gives us a recovered value
right
and of course the last value there as I
said before is the is the configured
size okay hold on sorry sorry the slides
are in slide Sheriff you don't have to
take those you want you can just
download them from there okay now are we
lost in slides okay let's go to the next
part which is this part that inner
record which describes what's happening
in young generational space so for PS
young gen our configured size is two oh
six seven - okay our occupancy before is
one four eight zero six nine K right and
our occupancy after the collection is
five eight five nine K so we got a whole
pile of objects actually being recovered
but not really being recovered some of
that one some of that recovery might
actually be things being promoted into
tenured space
right what basically that number that's
left behind is a residual number that's
left in one of the survivor spaces like
Eden will be completely empty so again
we can do the same thing configured mine
at our - occupancy and we can see
recovered is 1 4 - 2 O which
surprisingly looks like total heap
recovered which means that we've had no
promotion into Tanyard space okay now we
don't have the values here for what's
going on in tenured space but as you can
imagine what we can do is we can take
the total - young and then we get
whatever is left over is whatever has
happened in tenured space right so now
it's just getting into simple arithmetic
right so you can see as we predict it
the promotion is going to be 0k in this
particular case for survivor spaces so
this is a print tenuring distribution
unfortunately with PS young gen we get
some very terse information but it's
still pretty useful right because this
is saying that the configured size of
the survivor space is that yellow value
up there right and what we're actually
you know and in this case we can
actually compare how much was left in
young after the collection with the
desired survivor space to give us an
idea you know like our is our survivor
space too big or too small
there's also another number we can use
which is the sink all tenuring threshold
right so we have a max of 15 in this
case so when we started up the VM
because I probably set some funky memory
settings or something it automatically
says ok you seem to know what you're
doing so I'm just gonna set the tenuring
threshold to 15 if it needs to be
something else you better go and
configure it yourself right in other
words it's up to you now if you don't
try to configure heat too much then
it'll default to 4 and that'll be the
tenuring threshold right so how this
works is hotspots going to come along
and it's actually gonna calculate how
you know what the expected survivors
number of survivors are from the
collection of Eden and it's going to
take that number and it's going to
compare it to the size and
if it's bigger than what we have to do
is we have to roll back or recalculate
the tendering threshold in this case the
canoeing threshold has been recalculated
two three and this represents a proven
premature promotion event in other words
I couldn't I didn't have enough space
for all fifteen generations of objects I
only have space for three so from 4 to
15 off to tenure do you go okay that's
bad that puts more pressure in tenured
space which means that either we're
gonna wear that means we're gonna either
run CMS more frequently or we're gonna
run full GCS more frequently so we want
to do something to try to stop that from
happening which means that we need to
resize our survivor spaces in contrast
this is a full GC record so it's
basically laid out here with all the
sections it looks very similar to the
young generational collection and in
this case we're reporting on power old
gen so that's our parallel collector
running null gen and it's the same thing
before occupancy before occupancy effort
after configured size in the brackets
right and in this case we're also
reporting on perm gen and again perm gen
is before after configured size of perm
gen note that when you do a full GC
young gen completely empties so that
means it goes to size zero our occupancy
of zero I should say which means that
the full gen also the full GC also has
this extra added bad effect of taking
everything that maybe shouldn't be
promoted and pushing it up into into
tenured space this is what a deaf new
record actually looks like with print
tenure and distribution and as you can
see I got a breakdown of the number of
bytes by age and this is really really
useful because if I sum these things up
over time and create a graph I can use
that information to calculate precisely
what the tenuring
threshold for this application should be
yeah and if you look at it pretty much
everything else is the same I have
before/after configured sighs pause time
statistics right so I can just parse
through these and get whatever field I
want out of here or do the calculation
that I need in order to you know figure
out what's going on this is a concurrent
mark-sweep phase now I don't want to go
into the intricacies of a concurrent
mark-sweep I'll just point out the
obvious things which is the red bits are
stop the world phase so that's your
initial mark and your rescan the white
and yellow interleaves are basically
pairs of records that indicate when a
concurrent phase starts and when it ends
and it gives you an idea of how much CPU
time and how much wall clock time it's
consumed right there's no sizing
information in a CMS record only pause
time information because the only time
you get an idea of size is when you
actually do the concurrent reset now
intermingled with all of this stuff will
be our new collection records so this is
really like condensed right because
what's happening is that as the
concurrent mark-sweep is is running your
application is running it's filling up
edan and then it's going to promote all
of these things up into tenured space so
you know as these promotions happen then
tint the concurrent mark-sweep
collection actually has to now
recalculate or you know reset its
position based on all of these new
objects coming in right so what we want
to do is we want to try to use these
logs here to try to improve the
performance of our of an application so
we're gonna I'm going to do this as a
case study so I have this application
I'm gonna run it at a whopping five
transactions per second it's fast right
and here's some goals right want a
reduce response time improve
transactional throughput or you know the
requests there are requests retirement
rates and reduce hardware consumption
and you know make sure that we don't
have any stalls due to the collection
right so here's our application I'm
gonna run it just with default settings
which means that my heap is going to
be configured to 1/64 of all the
physical RAM I have on my machine right
we all knew that right put up your hands
if you didn't oh yeah you're the only
one apparently or the others are lying
okay okay and likely the default heap is
too small and well you can see lucky we
got a Java laying out of memory error so
you know that's a problem so what does
that look like right so I'm going to
pull those values out I'm gonna dump
them in a graph a Nega see you know okay
I blew his partials red is full the
black line is being heap size and you
can look at all Sundays just consume
heap I did great in too frequent full
GCS and basically I'm dead right
heaps too small you can see that in this
particular view this is a visual VM
plug-in I wrote you can get it from Java
net and okay so let's put a yellow line
on here right so that particular
collection event what we can see is in
the bottom graph right that collection
event completely fills the tenured space
so by the time I get to the next time
then when I get an allocation failure in
Eden I got no room in tenured space I'm
done
game over so let's go to run too so
let's just make the heat bigger I'm
going to estimate I need a gig and here
I have a graph of my response times
because you know my goal is to improve
response time certainly going from an
out of memory air to this is a vast
improvement in response time at least I
would hope so and I can also look at CPU
utilization as part of my study of the
hardware so if we look at this
particular thing here that's at our
primary user transaction here we can see
that well okay my average response time
is 145 milliseconds but my maximum
response time is two point eight six
seven seconds so I got a huge spread
here right now if I look at CPU
utilization it's like really spiky
really spiky is
generally not good and it's spiking
between 25 and 70 percent in probably
averaging somewhere around 60% as an
eyeball average right not good okay the
other thing I want to look at is
frequency remember I mentioned frequency
earlier on well you know in this case
well we wanted we want to try to
decrease the frequency frequency means
we have to context switch context
switching is expensive we don't like
systems context switching so this is
effectively garbage collection lock
induced contention they're induced lock
contention I should say okay first
that's our throughput number so that's
the sum of our pause x divided by
runtime fifteen point five six percent
we definitely want that below five
percent so that's not very good
that's our GC frequency the number of
collections per second you know and
that's that's a fairly reasonable number
for this particular app by that I think
we can do a lot better here here's the
premature promotion picture this is a
number of times that we had to roll the
tenuring threshold back from its max
tenuring threshold that we set on the
command line right eighty three point
three percent that sucks okay we get a
bit of breakdown here so we can
understand what the curve looks like
right here's what it looks like over
time and and so this is this is a graph
that's obviously constrained by
something right because it's just got a
flat top normally we're not gonna see a
flat top and a waveform like this yeah
so that this waves being clipped very
badly and that's an indication again
that our survivor spaces here are way
too small right so let's increase this a
radius of our space so now we sort of
have to do this a little carefully right
because well if if we just increase you
know so how we going to do this well we
need to increase the size a young
generational space overall because like
if we just increase the size of survivor
spaces we have a zero-sum game which
we're gonna steal from Eden if we steal
from Eden we're not going to we're only
going to increase the frequency of
collections not decrease them which
means our objects is going to age faster
which means they're going to flow
through the system faster which will
have probably a devastating effect on
our overall throughput so we don't want
to do that what we actually want to do
is we want to give young generational
space more space right so that we can
actually make the survivor spaces bigger
without reducing the size of Eden and in
this case we'll probably should maintain
the same rate of collections in young
generational space we should have a
knock-on effect of decreasing the number
of full GCS if that makes any sense
there's also another problem with this
with the strategy and the strategy is
something called the initiating
occupancy fraction that's about 69% in a
in it with a concurrent mark-sweep
collector plus a safety factor right so
what that means is when tenured gets to
be 69% full then what I'm going to start
doing is and start running this
concurrent mark-sweep collector if I
steal memory away from tenured then my
lie set right is going to be actually
closer to where that initiating
occupancy fraction is okay now hopefully
I don't get so much promotion not as
much promotion because I'm catching dead
things like in young generational space
but but you know since my at the top of
my life set is closer to the initiating
occupancy fraction I'm certainly going
to potentially have a problem there so
it might be it might be advisable just
to make heap slightly bigger overall
right to take that problem into account
in this case I'm not going to do it what
I do is what I'm going to do is I'm
going to set the new ratio to one which
means I got 512 megs for young and 1205
makes for a tenured I don't want tenure
to be smaller than young because that
could create some other problems okay
and then what I'm going to do is I'm
going to set the survivor ratio to 1 so
new ratios one survivor ratio is 1 in
this case sorry I'm just said fixing the
new size to 512 so I'm just gonna do a
fixed size here and I can set the
survivor ational 1 which means that Eden
survivor 0 survivor 1 are all
approximately 170 megabytes
okay that's so that was run one there's
run - here's run 3 look at that
we went from 145 milliseconds to 81
milliseconds
moreover look at our max time right drop
by well from - 867 - 107 - so we should
all be happy now all right that's really
cool
moreover look at our CPU picture instead
of you know basically we've knocked
about 20% off the average utilization of
our CPU by calming the collector down
moreover we've completely wiped out
premature promotion finally
we can see our frequency is down by a
you know factor of four something like
that and our efficiency is now like
running around three point one three
percent which is really cool too right
all we did was change a couple of things
on the command line based on some
numbers we saw okay next question can we
improve the memory footprint well here's
the new tenuring over time picture right
so this looks a lot different yeah and
what it suggests to me is that the
survivor spaces only need to be about 60
megabytes in size now that might induce
some premature promotion but that's okay
I don't really care you know I can
tolerate a number of boat 15 upwards
around 15% premature promotion and the
system seems to cope quite well with
that in in most cases this is our
tenuring distribution remember I said it
took the age of bytes add them all up
and did this there so really what we're
seeing here is a tailing edge of the
week generational hypothesis right and
what is saying is like oh man look at
how many objects were having died
between the time we did our zero
collections at the time we did our first
collection and then we get a little bit
more at two marginally more at three by
four were flat you know since I don't
have any air bounds on this particular
graph you know I probably suggest that I
want to set the tenuring threshold at
five and what that's gonna do is it's
gonna say well anything after age five
is going to be promoted anyways so
rather than copy it back and forth
between the survivor spaces let's just
promote it now we're going to reduce our
coffee costs
right so here's run for what we're gonna
do right configure Eton to be 180 Meg's
alright we've had it set 270 I'm gonna
do 180 just to be just for the mass
right that's gonna maintain our GC
frequency overall hopefully or at least
not make it worse
I'm gonna set s0 s1 to 60 Meg's each so
what I'm going to do is go through this
calculation here have 180 plus 60 plus
60 that's my n plus 1 plus 1 right that
gives me three hundred Meg's for a young
generational space size I'm going to
make tenured the same size which means
my new heap is now 600 Meg so that's a
four hundred Meg's savings nice right
and to do that I'm gonna have to set the
survivor ratio equal to three yeah set
the tenuring threshold to five good
before but let's just use five for
safety and then when I run this it's not
nice like that
so basically slightly maybe slightly
faster 74 instead of 81 our max is still
like 105 five so and but our 90s
percentile look at that 161 to 146 so we
have a marginal improvement a very
marginal improvement but we've done it
with much less memory CPU unsurprisingly
stays about the same
frequency it's about the same pause time
goal you know it's it's been reduced so
the I mean these are slightly smaller
but you know nothing to write home about
the big big gain is in the is is in the
memory footprint here we're looking at
the heap usage after collection and as
you can see it looks like there's even
more possibilities for reductions here
but we don't want to over tune in the
heap we definitely want to give it the
you know some room for safety right so
you know the other was what happened
here right so GC adapts to a
configuration the configuration we
calculated here so we just need a
slightly more heap so it just grabbed it
right there and look at tenured we
hardly need any tenured now right so the
performance profile of this application
changed very dramatically with just a
few settings and it's so it's really
interesting so last question I could ask
here is like could the JVM have done
this on its own right instead of us
having to go through all US mental
gymnastics to get here and you know
looking at all this ugly de dents and
doing these well strange calculations
well the answer is yeah but if we set
the maximum heap side to one gig and the
new ratio on the survivor ratio to one
will get identical results to what I've
just tuned the application to do right
but only after warm-up so in other words
the system has to collect data and after
its collected that data it will adapt
eventually to the settings you've given
it but only after it's been put after
pressure so what we tend to see is we
have a warmup period where the
application is going to put pressure and
it's not going to behave well and then
if adaptive sizing works and I do stress
if because it tends to be a little
temperamental and you do get this a you
do get the system to adapt to the
configuration it needs to be and then
we'll get identical results now it's
just in real life we found this to be
very temper
mental so sometimes it works sometimes
it doesn't
one thing we found which I've talked to
the you know hot spot engineers about no
one has a good explanation about it yet
is that JVMs that lie in low memory tend
to adapt better than JVMs that are
running in high memory anybody have an
answer I'm all ears
so your results can vary from run to run
that's why I didn't do a demo is because
once I start running all of these other
you know PowerPoint and everything like
that well or keynote then I have no idea
how the demo is going to work so demo is
great
so that's 1042 and that's exactly how I
want it to end this right is because
that means that we have how much time we
have a lot of time for questions which I
really and I know Hines was commenting
that he really enjoyed the Q&amp;amp;A sessions
here at JavaOne so what I'd like to do
is just turn it over to you guys to ask
questions and hopefully at least one of
us in the room can answer them yes in
the back Martin
yes okay so that's how the memory pools
are actually laid out so they're
contiguous and they're basically
reserved all in one chunk and then what
you're using is actually just allocated
out of reserve so there's this buffer
area so that we can you know make the
spaces bigger there's a buffer area
between young and old so that we don't
have to move old right we don't want to
move the bottom of old because we move
the bottom of old it's like not good
that's good that's gonna be a lot of
pain any other question yes
do these techniques work for very large
heaps well mm yes the techniques
themselves work the results might be
less than spectacular given that the the
heap is large as we all know as we get
into larger and larger heaps we're at
tend to be dealing with larger and
larger live set sizes and the cost of
garbage collection one of the factors in
the cost of garbage collection is the
size of the live set right the bigger
the live set the longer takes the scan
for roots the longer it takes to do the
copies that longer it takes to do
everything right because we only deal
with live objects we don't deal with
dead objects right I mean and you know
more live objects more work larger heap
more live objects yes sir
going in the back first and over here
yes
yeah how to tune the the JVM for when
you have applications that are behaving
differently from each other
any ideas it's it's really it really
becomes a problem I find is in in some
cases if we actually can't find a
compromise between the two applications
we would recommend that you don't go the
multi tendon tendency route because if
you have to have two completely
different GC strategies there really
isn't a good way to manage that in the
JVM today you would actually have to
have separate memory spaces you know and
as I said the current way to do that is
to JVMs unfortunately and we've run into
cases like that we're just saying you
know gave if we could split this out
that'd be good I mean if you look at
like how some caches are set up like can
I mention Oracle products without
getting fruit thrown at me rotten fruit
coherence for instance right it has this
idea of like near cache well all that is
saying like I got these long-lived
objects that I rarely want to collect I
don't want to mix them with an
application that has all these
short-lived objects so I you know I'm
going to have to separate them somehow
so I'll just put them in a separate VM
and you just you know we'll just do
tricks to actually move the data from
that VM that's running parallel to me
you know maybe on the same machine to me
you know closer the the other thing is
that you know that particular situation
looks like one where G one seems to be
well fitted right so as soon as G one
becomes usable did I say that no you
might want to consider you know testing
G one I mean there are people actually
using G one now so it certainly might
want to consider it because the G one
seems like it's should ignore data that
it doesn't need to collect much better
than CMS does right and and that's
really the trick is like you know how do
we get these systems to ignore things we
don't want them to look at cache data
seems something like something I want to
ignore there was one over here yes and
then you sorry
what can you do to improve well I have
to understand why I was getting a one
second response time there and there
might be other effects when I was
actually running these benches I was
playing with the thread pool sizes right
and when I mean when your plants start
playing with thread pool sizes and the
the the number of variables goes you
know yeah as to what to do
so really outliers like that you have to
study them specifically to understand
what's going on in the yes you in the
blue
I would go to a Java 7vm but even so
what you what you tend to do with
latency-sensitive applications these use
a lot of like direct buffers and
pre-allocate and then basically use the
number of these tricks that just turn
the collector off entirely right and
other people actually try to smite size
so that they never get a full GC so we
have applications that never have a full
GC during their runtime and that just
means that you over tune the young
generational collector to try to you
know just try to make sure things don't
escape into tenured space that can work
but I think mostly what people do is
they just use you know basically
primitives right and then allocate
buffers or primitives and then wrappers
around that and then and then what
happens is that you just end up with a
situation where you just never have to
cut have to the collector never has to
run yeah yeah the young
right and that's why if you pre-allocate
buffers for these things then you can do
the allocation directly in the buffers I
mean it's not the way you're supposed to
program in Java but all of the low
latency high frequency traders do it
anyways right they're very very good at
not making sure the collector never runs
by by these so we say alternate
programming techniques right don't try
this at home for the other people okay
this is not a very satisfactory answer
sorry
yes tenure yes mmm
well you know that's why we have g1 and
balance and things like that right
because it doesn't the longer the larger
that Eden gets the more that you're
looking like a now a single space
collection right and the more it looks
like a single space collection the more
you go back to like Java one dot X
performance right maybe not quite that
bad that's a little bit of an
exaggeration but certainly when you get
it you know we have deployments on 256
gigs right it's it's just like you know
even a young gent on this in in that
size of space is like not desirable
right so which really means that we need
well I mean some of the banks are
actually hiring people who know how to
write garbage collectors so just for
that particular reason you know they're
trying to look at alternate I mean
there's also things like what's known as
like pause lows collectors that have
been implemented by people like Azul
right so I I suppose I should mention
them in all honesty since I just
mentioned IBM as well as open JDK
open JDK right and they tend to do
things quite differently when they had
hardware support for it they actually
put some of the harder things that you
had to do actually you know like if you
move an object right you have to
basically trace all the pointers to it
and Swizzle all the pointers so that
everybody finds the new object otherwise
you end up with the point same
your problem you have with see you know
Azul used to do that as a hardware fault
so you'd make a reference to an object
that had moveed it would set a trap in
the hardware and then it would Swizzle
the pointer for you like on on contact
you know and there's tricks like that
that people actually use to create these
like pause lists or more to put more
concurrency into the end of the
collectors I think we're seeing that
like in g1 also yeah yes sir
right okay no it's actually a breakout
right the bet I mean the way you can
tell all that is just take all the
timestamps and add them up and then if
it's greater than the next time stamp in
the log it's like you know okay that
can't be right yeah sometimes simple
maths are the best thing to do to help
sort things I mean the logs I showed you
are really clean then you can get some
really ugly ones like just got a new one
like two days ago that I hadn't seen
before it's great I had to scratch my
head for a couple minutes to sort out
actually what that log was telling me
because they missed a tag right so like
for instance concurrent mark-sweep does
not report on sighs this was a
concurrent mark-sweep that was reporting
on sighs so obviously it's not a
concurrent mark-sweep it's a concurrent
mode failure which I didn't really talk
about right and a concurrent mode
failure is a full GC so it's like okay
got it not concurrent mark-sweep full GC
but you know it took a little sorting to
sort that out yes we have one up front
here
memory fragmentation okay
so memory fragmentation is really a big
problem and with the concurrent
mark-sweep collector because you don't
have compaction which is the best way to
resolve memory fragmentation just
compact it away so if anyone doesn't
understand this problem it's like you
know what happens with your disk when
you start deleting files right you know
you eventually you end up with it's all
a fragment and you go to a defrag which
basically brings everything together
again and that's what a compaction phase
is with a concurrent mark-sweep
collector what we've seen is we actually
lower the initiating occupancy fraction
and give the tenure space bigger because
the fragmentation has a tendency to work
itself out if it's given enough time by
giving a heap bigger tenured space right
we're saying okay we'll make the heap
space bigger right so we'll try to give
this thing more time however I don't
want to decrease the frequency of the
concurrent mark-sweep collector right so
we have to consider frequency as well as
size in this case right so I'm going to
reduce the initiating occupancy fraction
so that it's sitting at the same level
that it was prior right so if it's at
70% and that means it triggers at two
gigs well I'm going to reduce it to you
know whatever percentage it it needs to
be in order to sit at a two gig trigger
threshold right and that way I maintain
the same frequency of concurrent
mark-sweep collections but I'll give it
more space overall which means that I
have more time to work through the
fragmentation issues which means I
probably won't end up with concurrent
mode failure right and you know and
degrades and that degrades into full GC
there was one was it over there yeah I'm
not looking over here so sorry yeah
okay
got some guidelines okay rule number one
out of memory air increase heap rule
number two
too frequent full GCS that's really a
sign that you might want to increase
heat but you also to consider what's
putting pressure on that right and those
are really the only two rules is really
you know when you start you're getting
situations where you're full GC counts
are too high so I actually do a measure
of full GC 2 GC ratio and you know I'd
like to pound that thing down to zero
right you know we have one app here I
was looking at it and the guys saying no
GC is working fine I said yeah your
Jesus told you see the GC ratio is now
sitting at 87% so I you know I think we
can do better in that case right in that
case probably a reasonable target is
somewhere around twenty to thirty
percent but even that seems like a
fairly high number you know full GC is
expensive you know the partial G C's are
much much cheaper so we want to try to
get rid of full GC s in that case yeah
yes sir
yeah okay okay
here's the bad answer to that you can
write a slab allocator that uses direct
IO to do a memory buffer into C heap and
just put them out there but be careful
because you might end up writing your
own garbage collector the Cassandra guys
are well on their way to doing that
other than that I'm afraid there's not
much you can do it with a concurrent
mark-sweep collector aviv we know that
these things are there we try to get
them into heap we try to get them to
compact it we try to get them in
someplace where they're like out of
trouble and then just basically adjust
initiating occupancy fraction for them
and that's pretty much unfortunately all
you can do in that case there isn't a
space that says you know don't touch
this you know because every time people
have tried to do that I mean I I did
this in a small and a small talk VM I
put in you know don't never collect this
space type space right or have this
thing persist after VM shutdown and
stuff like that and you know the
developers always got it wrong and you
always just blow that that memory pool
as opposed to you know blowing regular
heap so it's I think there's a lot of
resistance to do things like that but if
you can get them outside of the VM when
you're not using them that's probably
the best thing to do like serialize it
disk memcache something like that right
just get them out of the VM when you
don't actually need it if you have a
latency concern then you know you're
just gonna have to take the hit
unfortunately any questions from this
side of the room
yes howdy
he's so quiet
so the question is is there any
statistics that you can get that says an
object has been sitting in tenured for a
long time so so no no I know there isn't
actually there's there's no statistics
kept for that way those are very those
are you generally kept by profilers and
you know if you want to profile your
production system for that type of
general information right next yeah it's
a good question though but unfortunately
there just isn't I mean the generational
counts are kept for in Eden but by the
time they get promoted then you know
that's just profiling data so you have
to instrument for that sorry yeah okay
just randomly I think yes sir yes
implicitly that was a graphs I showed
some of the graphs I showed were from
visual VM and I think they're fairly
accurate you know yeah it's okay like
you're the mbeans are reporting what the
collectors are telling them and the
collectors are reporting fairly
accurately as far as as far as I've been
able to determine right in the back do
stand up and come closer if you like
yeah and every time they do I make a lot
of noise so they don't like me anymore
jep one five eight read it I just beat
on the guy who wrote that specification
last night Tom because it's show you say
butt ugly sorry Stefan if you can hear
that but um that's a new logging
framework that's supposed to go into the
JDK into the JVM and really we're trying
to get them to shall we say make that
better once that gets into play then we
should be able to move the GC logs over
into that framework and that hopefully
will resolve the concurrency bug for the
moment what I do is I just outsource the
problem to somebody over there okay
you're welcome okay
JEP one five seven or one five eight
this butt is one of those I think is one
five eight one five seven okay it's a
number okay and we have about one minute
for about one last question anything
I use our own tool that's written by Jay
clarity called sense 'm that's our
that's our tooling company guys right
and there's a couple of my colleagues
are back in the corner over there we you
know if you want more information on
that you can talk to them i think they
know more about it than I do
what's that
they're slides slides are on SlideShare
okay so you can download from there okay
I think we're gonna vote at a time so
thank you very much for attending and
have a good rest today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>