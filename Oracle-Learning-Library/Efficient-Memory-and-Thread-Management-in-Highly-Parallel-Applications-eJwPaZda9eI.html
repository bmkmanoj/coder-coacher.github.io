<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Efficient Memory and Thread Management in Highly Parallel Applications | Coder Coacher - Coaching Coders</title><meta content="Efficient Memory and Thread Management in Highly Parallel Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Efficient Memory and Thread Management in Highly Parallel Applications</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eJwPaZda9eI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Philip koza i'm a senior
software engineer at IBM I'm not a JVM
developer I'm just a guy who never being
responsible for the memory usage of our
application everything we're going to
discuss here today has been implemented
and frankly I can't imagine supporting
our application without it so here's
what we're going to cover today first
going to go over where Java uses memory
some of the reasons you can end up using
too much java heap memory I've estimate
the memory usage of your objects during
testing then how to estimate the memory
usage of your objects during runtime how
to track and control your memory usage
at runtime how to minimize the amount of
memory you're using how to efficiently
manage your threads in the presence of
uncut exceptions especially out of
memory error and a lot of summary again
alright so there's there's many
different forms of memory usage that the
JVM has the first is your OS and your C
runtime now obviously your C runtimes
only if you've got a JVM written C but
most are that are typically there is a
see runtime then you have your native
heat the native heap is for things like
direct by direct buffers drip byte
buffers and ji calls there's your method
or where your class objects are stored
there is your your jvm stack and then
there's your heat and this is where all
your instances of objects y'all liking
the new operator are and you can get an
out of memory error for suspending the
memory usage of the JVM has except for
the OS and c runtime memory usage but
typically if you get an out memory it's
because of your heap usage and that's
what this presentation is going to cover
we're only going to be covering how to
monitor and control your your eat memory
usage some of the benefits here you can
get better performance and then higher
availability of your app so how does
java how to job keep objects use memory
well the memory usage of a job jake is
composed of a number of things the first
is there's a certain mount of overhead
all you aren't even aware that this over
it exists but it can range from eight to
twenty four bytes to pay on the JVM
the store is a clear reference to your
class object various flag fields and
things like that there's memory for your
primitive fields reference fields
alignment bikes almost all the jvm seem
to want to allocate objects on
boundaries of eight so typically all
your objects are rounded up to be a
multiple of 8 and then arrays have some
extra bytes for size okay so that's how
Java uses memory so what are the typical
reasons your your app can use more than
you expected it to well the number one
reason is just you just don't understand
how your objects are using memory
another reason is a just over use of
delegation you just have too many
objects objects referencing object
referencing objects and so forth too
many threads in this context we're
talking about the the actual the Java
heap usage of a threat almost all
threads are you do some amount of Java
heap memory and so even if your threads
you're only using a little amount if
you've got hundreds and hundreds of
threads you can run out of memory and
then another reason is not using
collection classes properly this is
basically there's no there's many
different kinds of collections list heat
map heat table and they all have
different memories that you said case or
characteristics and it can be hard to
know which is the right collection for
you there's a really good talk later
this week on wednesday from 8 30 29 30
by chris bailey it's called from java
code to java heap he goes over in great
detail the memory usage of each of the
collection classes and how to pick the
right one so i highly recommend you
attend that talk so then there's memory
leaks and then just not cleaning up
often enough and then a final thing is
finalized errs if you've got an object
with a finalizer it's not going to get
garbage collected until the final image
run and there's the JVM doesn't
guarantee when a finalizes get run if
it's ever going to get run so if you've
got finalized as you can end up having
problems that are similar
memory leak dereference the object but
the memories still being held so what
are the indications your app is using
too much memory well the first you're
going to find is due to get degradation
in your performance this means both your
responsiveness and your throughput or
suffer and allocating memory takes time
but it's fairly cheap compared to the
cost of garbage collecting it so if
you're you know garbage collection costs
are high you're basically you're it's
going to be because your your garbage
picture is running more more frequent
there's more memory you got more memory
or garbage protector or more often
that's a sleepover and analyze all that
memory and when it gets to about
eighty-five ninety percent are testing
has shown that your junior and really
starts to thrash it basically just gets
to the point where it's getting harder
and harder for it to find memory free
and eventually you get to the point
where your JDM sustained all its time
doing garbage collection and then
eventually that leads to out of memory
are so you really want to avoid this you
know most jvms when you get it on memory
error it's only going to kill the thread
to actually got the hair it was just the
one that was trying to get memory when
it wasn't anymore it may may be the one
that was using a lot of memory and maybe
you that i was using hardly any memory
at all but it's just the poor sucker
that happened to get the error so when
that happens you can end up with other
threads just hanging and behaving really
strangely i call these zombie threats
and your jb a mic crash but it might not
happen right away so you know typically
when you got on a memory error it's just
too late so you really don't want to
wait to start thinking about how to
control your memory usage of your
application until you get it out of
memory so now I'd like to discuss the
impact of performance on long-lived
objects in most JVMs the default of
memory allocation of garbage collection
algorithm is generational it's the
default in oracle hotspot it's the
default in IBM j9 from build 2.6 and
later in general GC your objects are
initially allocated into a young
generation
and then if they live long enough they
get promoted into a thing called the
older tender generation and GC and the
tenured edition is a lot more expensive
so if you can reduce the amount of data
do you have that gets stuck in the
tenure generation you can really
increase your performance quite a bit so
it's hard to say for sure you know what
objects are going to become tenured but
likely candidates are things that are
stored in static structures pass between
threads or stored in collections and by
definition an object that's long-lived
it's going to tie up memory for a longer
time and it's gonna increase the odds of
you getting a memory error it's you know
it's still possible that you can run out
of a memory if you don't have any long
lip objects but it's unlikely and in
order for that to happen you have to be
allocating some very large objects or be
allocating a large number of small
objects in a short amount of time so how
can you minimize the amount of a
long-term memory you use well the first
thing that is delay allah ki near
objects of tea and you know you know
lazy allegation make sure you clean up
your objects as soon as you're done with
them and it's sure you're not going to
eat them again you could try converting
them to a more memory efficient form if
you're not going to needed for what you
know like stick a convert into a byte
array and of course you should always
try to design your objects to be as
memory efficient from the start as you
can so so you're going to want to
control your memory usage so but before
you can do that you have to be able to
figure out you have to determine how
much your objects are using so then you
know memory usage objects is going to
depend on a lot of things first and
foremost is who your jvm vendor is and
next the biggest thing is whether you're
32-bit or 64-bit and if you're 64 bit
you have to determine if you're using
compressed references or not I don't
know that much about what exactly a
compressed references but my
understanding is it's basically the JVM
pixel area memory as a fixed address and
then it converts your references into
offsets from that base address and it's
it's enabled in hot spot by the
Alton version 6 version 23 and later and
Java 7 and IBM you have to explicitly
enable it with this compressed reference
as a option so there's there's two basic
ways that you can estimate the memory
usage of your objects sure there's other
ways with these are the two most widely
used ways the first is using the the
runtime package total memory and free
memory of methods and the second is
using the instrumentation package that
the jvm provides we're going to talk
about each of those in turn so so the
classic way to determine your memory
usage and this is the only way prior to
Java five was use the runtime memory
methods so basically what you do is you
first see how much memory you're using
before you start then you allocate a
bunch of objects of the type that you
want to measure the memory usage of and
then you could be the difference between
your before memory usage and after and
that's how much memory used you know
sounds sounds simple right well it is
but there's a few complications gotchas
here you know unfortunately it can be
inaccurate and it's mainly because
because of the garbage collector right
in order to put to be accurate you've
got to make sure your garbage collector
runs before you has to make you remember
use and you know you really can't force
the garbage patch around you and you
know even if it's runs you might not
free up all the memory that's it might
not collect all the memory that's being
used at that time so so typically to do
though is you you called system GC a
bunch of times and you hope that that's
going to increase the odds if the
garbage patches could get run you can
you can minimize some of the impacts of
this by allocating a bunch of objects
and take any average of them and then
declaring the tax than our message for
this object another you have to keep in
mind is this thing called escape
analysis this was added in Java 6 and
what that is is the JVM can can look at
your allocations if they're inside of a
method and if it sees it that it's
there's no references to that memory
outside of that method if the references
to that object doesn't escape the method
it can decide to just stick them on the
stack and if that happens and you're
it's going to look like they're not
using any memory at all when you're
measuring the heap usage so you want to
make sure that doesn't happen you may
have to join your chassis and have a
reference make sure you have some phony
references to the objects outside of the
method so here's a here's a code sample
of this it's going to go over this
briefly basically you have a factory you
declare a factory interface which is one
method make object you declare a bunch
of implementations of that that are
going to allocate objects of the type
you want to measure here we can see we
got object integer long float double so
forth and then you've got your main
method where you basically just call a
method that's going to allocate objects
of that type and measure their memory
usage and here we can see that the
method that actually does the memory
usage you can see that it's calling this
a run GC method to run the garbage
collector a bunch of times then we get
the amount of memory we used we allocate
our objects we get the after memory used
we we allocated a bunch of objects so we
would take the average of them we round
up and that's our memory usage well
here's a here's some of the those
utility methods are being used here you
can see run GC we're running it 25 times
in a loop and sleep in a little each
time I found that this does get the
garbage which would've run pretty pretty
reliably but you know every JVM is a
little different you may have to tweak
this a bit okay so then the other way is
to use of the instrumentation package so
to use this package was added in Java 5
you have to allocate a method called
preemie it's going to have this exact
signature it's gotta have this exact
signature pre main string args
instrumentation and what that
instrumentation is parameter is the JVM
is going to pass and object it
influenced the instrumentation interface
to this pre main method then you can use
that instrumentation interface of method
get object size to pass the object that
you want to measure the memory usage of
and it'll just make them
Rajon ok so this further little details
you have to deal with here when you're
using this package you have to make sure
you have a manifest file with this exact
line in it and also you to run a program
that's going to use to get object size
method you have to use specify this java
agent option on your command line and
one thing to keep in mind is that you
can't get deep man reversed estimates
with this if you want deep referent
estimates you you're going to use
something called a reflection so here's
an example of using the instrumentation
package we define an agent class that
defines the pre main method then we have
our own get object size method that
invokes the instrumentation interface
object to get object size method then we
chart up and then we can use our agent
class to get memory estimates here we're
doing it for an integer array so I've
got a little chart here that shows the
usage of these techniques to estimate
some some common objects here's chart
I'm not going to go over this too much
one entry thing to notice is act for
64-bit uncompressed references the IBM
IBM JDM uses about 48 bytes more than
the hotspot JVM that's a because the IBM
JDM has a few extra flags in the
overhead area but if you use compressed
references both the the IBM 64 bit jvm
and the hotspot are almost exactly the
same ok so so using one of these two
techniques you can you estimate the
memory usage of your your objects during
testing so which is better well the
runtime methods are the simplest right
instrumentation does seem to be more
accurate so I would say it's preferred
if you don't need a deep memory estimate
if you need a deep memory estimate
instrumentation and reflections probably
ultimate solution but it's more
complicated and it's a lot slower
so so now you need to translate this
this knowledge into methods igitur
efficiently guesstimate the more users
objects while your app is running so
your goal is to write a method that's
going to escalate the more you serve
your objects for every class you get
attract the memory usage of right and
any classes that are referenced are
getting a separate method so since
you're going to need a separate method
for every object anyway you don't really
need to get deep memory estimates during
your testing and to make this perform
really well what you want to do is you
want to have utility classes where you
pre-calculated all the memories which of
things like primitives and standard
objects and so forth and then you just
they're just static finals or you just
invoke those and you get your memory
estimates so your method what do they
need to monitor well you need to have
the mess to make your object overhead
you got to take into account your
alignment policy size of a reference
size of primitives size of primitive
arrays and then a basic objects using
like string and integer and bigdecimal
and so forth and then for collections
you want to measure the size of an empty
collection and you want to make sure you
determine you know the overhead for any
entries here to get out of the
collection and it's really important to
always if you can't get it exactly right
overestimate rather than underestimate
because you really it's better just use
a little bit less than you otherwise
could then to run out and get an out of
memory and you can always refine your
estimates to make them better using the
techniques we've discussed and and most
likely the JVM is using more memory than
you thought it was anyway so so how do
you calculate the memory usage of your
applications objects well one way to do
it is is to have an interface where you
just have one method called get memory
usage or it can call it anymore and it
should return along and then every class
if you're going to track tomorrow usage
of you're going to come with this inner
fence and then each implementation is
going to ask memory usage of its
primitives and object references and so
forth and as I
said earlier you can do that as a static
final so you don't have to keep doing it
all the time and one thing to keep in
line is that a null pointer reference
uses the same amount of embers a real
pointer if you have any non mole
references that you just follow the
pointer and invoke the get memory usage
a method for that class and and this
includes a raise
now you want to make sure that if your
alignment policy is that things have to
be on a multiplier is made that you
round up the final result recursos you
have to watch out for recursive
references if you got to remember
whether you've already estimated this
off you come back around the object
again in the middle of your estimation
you can remember that so you don't get
stuck in an infinite loop if you got any
shared objects something that's just
used by all your objects you should just
count at once or not at all it may just
be insignificant probably flyweight
objects such as interred strains catch
integers and so forth unless you're sure
that it's really good to be interred or
the IBM the GM is going to treat it as a
as a flyweight its privacy to dis assume
it's not but if you're sure like it's a
frequently use filename for example
something you know that you've used
before then go ahead sure treat it as a
flyweight so you're probably thinking
this sounds like a lot of work well it
really isn't and and the benefit really
can pay off first of all you're going to
find that most of your custom objects
most of your applications objects
quickly devolved down to primitives and
your standard objects like strings and
so forth and you don't need to do this
for every class in your application
right you're only going to do for the
ones they're going to track the memory
usage of and as we talked about earlier
its long-term memory that really is
what's going to kill you in your garbage
collection costs so which we recommended
don't don't bother monitoring the trying
to estimate the memory usage of your
short-term objects just the ones that go
to live a long time okay and you can
always you know check the accuracy of
your estimates against using the
technique we discussed earlier to
constantly keep improving them getting
better but in the beginning shoot you
know I always make sure you overestimate
rather than underestimate and we found
it's very quick right we've never had
any performance problems you have using
this methodology but we've had a lot of
reformed problems with not doing it
because all you're doing is is you've
got these static finals and stuff you're
just doing some magicians you've got a
few method calls if you make if you've
got object reference and objects but
it's it's really
quite quick so here's an example so
we've got a work class called working in
it implements the memory usage interface
it's got some static files for our
primitive usages and our overheads we've
got a this this class has a string
reference it's got a reference to
another custom class called request and
we have this ma'am usage field what that
mem usage field does is it if we've
already estimated the usage of this
object once we're not going to do it
again if we haven't already estimate
computed it we factor in our fixed
overhead we get the memory usage of our
string we get the memory usage of this
request object by calling it kemarin
usage estimate method and then we round
it up and have our total so here's a
this is basically the same thing it's
just the the request class that it was
referencing showing how it's doing its
memory usage it's almost exactly the
same oh and there is it a governor usage
method and then here's some of the
utility the utility class methods get
string memory usage you got a factor and
of course the length of the object and
then we have a rounding that that round
up to a multiple ok ok so so that's how
we calculate the memory of an object and
so now we can use that to track and
control and rum usage at runtime
so so it's recommended you just have one
logical pool of memory where you're
going to keep track of your memory usage
you want to avoid separate pools having
separate pools gets gets complicated and
then you end up with situations where
one pools got a lot of memory left it's
not using it completely and other
threads may want memory and then they
can't get it because their pools all
full so it's recommend just have one
large pool now you can control your
minutes there's two kind of basic ways
to control your memories there static
and then dynamic control we're going to
talk about each of those so control your
memory usage statically means your
reserves memory up front when the app
starts or when the thread starts you
don't actually allocate the memory you
just you're just reserving it right
you're making sure that it's a reserved
ahead of time so it doesn't it all taken
by other prints and then with dynamic
tracking you you're going to have a
total amount that you keep track of for
all your threads and then every time you
allocate an object that implements this
get my message interface that we talked
about you're going to increment this
total amount use and then when you don't
need the object anymore you decrement it
and as we said before you're only going
to track your long to remember usage so
you have to do this for everything you
you don't need to actually pay for any
memory dynamically if you've already
paid for it statically and then you want
every thread to kind of maintain a local
total the tracks its memory usage that
it's just using because that's going to
be helpful and we see later what you do
if you there isn't enough memory so so
this global total it's just it's just a
globally accessible wall and you got to
synchronize all your increments and
documents of it but you can buffer your
your inks and decks of it that's what we
do we just buffer them up into 32k
buffers and of the increments and
decrements it's not actually storing any
memory in these things just you're
actually waiting delayed when you're
going to increment the global total and
that can that really just cuts down on
synchronization costs it's not really
even an issue
ah now you know the absolute limit on
your memory is of course what you
specify with your x MX parameter when
you started your your jvm and your
dynamically track memory well it can't
have all that it only you can only let
it have what's left over after you've
accounted for everything else so one
example of everything else is your
static loser reserve memory so when
should you allocate memory static well
some good rules of thumb war it's for
things like your application just has to
have it you can't wait have the your app
for this particular usage go and ask for
it and then be told it there's no memory
left so it should be something that
there's not very many of shoot something
you going to use a long time and should
have some kind of static sighs or static
upper limit so what's in it what are
some good examples of that well some
examples of that are like IO buffers
right or our life old page cache those
are all good examples of memory you
could allocate statically and you have
to the reserve a sermon a member for
everything else you're out it's going to
use that you're not going to be
accounting for right so all your
short-term objects that you're not
tracking just your basic overhead of
having thread running and so forth and
you can do that as one lump sum or you
can have a per trade value then you
multiply that by the number of threads
you have our both we use both so and
then one final thing is you want to
reserve some memory to keep your jvm
from crashing you don't want to go past
about eighty-five ninety percent of your
memory utilization so recommend i
recommend you reserve about fifteen
percent of your jvm max memory for just
to keep your jvm from thrashing alright
so so the amount of member you could
have available for your dynamic track
memory is is your maximum memory minus
all the other stuff you just reserved
for your overhead and your statically
allocated memory and so forth and so so
every time you know you have a thread
increment the global total it just
compares it against this limit and if
succeeded the request is denied so okay
what are you gonna do right well if it
gets denied the simplest thing is the
requesting thread has to handle it so
how can it handle it well it could wait
try again it could reduce some of its
own existing memory usage so it can
throw away any objects that it doesn't
need any lower priority objects it could
write some of its data to disk they
could give up their own exception and
you know typically what we do is we
basically have existing threads when it
asks for more memory it goes and tries
its first choice thing as to reduce some
of its own memory usage and that's why
you want threads to keep track of the
memory usage so that they can know how
much memory they're actually using and
you could have get a thread to give try
to get another tried to give up
someone's memory and that's possible but
it's it's more complicated and we're not
going to discuss that here so now if a
thread decides to deal with its denied
memory request by recent memory usage it
doesn't have to actually need to
resubmit the request again you actually
don't want it to do that right because
it's just basically it just treats it as
a swap of one of its memory usage for
another visit or usage it actually went
and freed up that memory usage
decremented the total ask for remember
users now again it may not get it
somebody else may have already grabbed
it so this way you just have it used so
let's swap out some of its existing
memory usage it's like a swap it
guarantees it gets the memory and not
some other bread
so let's that's you know one way well
what are some other techniques well soft
references that's built in jbean
provides it why not use soft references
well you know soft references how their
their uses they can be really good for
controlling the research of things that
you can always get again or you don't
really need them right an example that
would be a web page image right if it
gets reclaimed by the garbage collector
you just read them from disk again right
but soft references don't really work
very well for objects where you know the
only copy you've got of it is what's a
memory if it gets garbage collected what
are you gonna do it's gone right and and
you just have no control over what you
don't win these when this is going to
happen and and what priority that the
JVM is get it aside you know which
objects is it going to take and you
really want that low of control so so
tracking and controlling REM usage is
useful but you know the best thing of
course is to avoid allocating memory in
the first place so what are some ways
you can do that well should always you
know prefer a raised over complex
structures if you can use an array
instead of a list do it it's much more
memory efficient on when it comes to
collections you know avoid empty
collections you really want to have lazy
allocation not allocated collections
you're actually ready to stick something
in there right you should sighs your
collections appropriately right and in
some collections if you just use the
default a size 4 like I think hash maps
100 and you're only going to stick to
objects in there you can end up using a
lot of memory for no good reason and
choose the correct collection there's
many different memory all the questions
have different memory usage
characteristics so pick the right one
and again there's that talk on wednesday
at eight thirty by chris bailey that
goes over that in detail and keep them
on that you're in a class structure your
class structure how that impacts your
memory usage right you want to you know
avoid storing data in a separate class
if all instance of that class need the
data and it's not needed outside of
class and conversely you want to store
info specific only to a subset of cloud
since this in a drive class or in a
separate class
so another thing you can do is you could
convert your objects to something like a
byte array right so you know you're not
going to need something for a long time
or if you're about to write it to a
string you're gonna write something
that's stream you're going to be turned
it into bites anyway might as well just
do that earlier and save on your memory
usage and then here's an example one
little technique you could use a few
guys stick a bunch of things in a
hashmap and most of them aren't going to
be used for for a while or maybe never
used right you could convert it to a
bite o ring stick it in the hash map and
then when you go to get it you convert
it back and you could have a little
little rule or you say well if once I
fetch it once that shows that it's it
was needed by at least somebody so i'll
leave it converted an object form and
this can really cut down your music she
got these hash maps that have all these
objects in ER that are probably never
going to get used of course if you do
this you can't use generics because
you're going to have a mixture of
objects and by terrains inside
detachment oh and you should do clean up
as soon as you can right and one thing
to keep in mind there is this you know
optimized for your main line processing
not your exceptional processing right so
you know for example if you've got some
info you had to read from something like
a sequel database or something for a
user request right just delete it as
soon as you send it back to the user
right if for some reason it fails and
you just go get it again right you don't
have to keep it in memory sitting there
just in case it fails now of course this
is only a good idea if your failures are
rare fail your traffic all the time it's
probably not a good idea and of course
you can only do it if you have a way of
getting that information again and
finally use finally blocks as much as
you can right you want to make sure that
any memory you allocate gets free and if
you're a dynamically tracking it right
it's being incorporated into this
variable your tracking the memory usage
of you want to make sure that gets
decremented if the bread dies because
otherwise you can end up with what looks
like a memory leak your gothic track
your variable that tracks the limit is
going to be saying there's this memory
venues but it's not really being used by
anybody
and then of course there is object fully
right an object pooling confuse dirty
word at times but there there are some
limited cases where oughta clean can be
can be worthwhile right it you know
ultimately doesn't actually reduce your
memory usage but it can reduce the cost
to allocate garbage collected so it's
really only worth it for simple
multi-purpose objects they're going to
be able to be reused in for lots of
different things so byte arrays are a
great great candidate for this because
they can store any type of any of any
object and its really easy to have a
pool of them that you can then find
something that's a close sized what you
want to use but the only good object
full interpretation I've ever see that
was worthwhile was one that dealt with
Paige caches from a database right
you've got these database pages they're
all 4k they're all exactly the same size
it can really pay off with something
like that where you're gonna have all
everything is going to be exact same
size and you don't want to have to
constantly keep allocating byte arrays
all the time to explore these pages but
if you ever find your you're pulling is
getting complicated just to stop you
leave complex memory management to JVM
because you know it's going to do that
better than you you could for sure all
right so so now we gotta remember you
said your control and we're going to
switch gears here a bit and for the rest
of this talk we're going to talk about
how to keep your threads under control
so first thing you probably wanna do
with your threads is you're going to
want to keep them balanced and when I'm
talking about balancing here I'm going
to talk about balancing in their memory
use because just which is talk it's
focused on is memory use so so one way
to do that is to it'll feed the work to
your threads be an array blocking cue or
your own synchronized by furnace game
you've got some size limit on this right
and then you know if a thread straight
simple kid gets empty or topic it gets
full thread has to wait so key surcharge
from getting too far ahead of each other
and then the capacity of this this QR
buffer is what's going to determine how
far have your threads can get relative
to each other and to keep track of to
balance to keep track of whether you've
got a good the right proper of threads
here excuse me you can keep track you
have some statistics to keep track of
the producers and consumers and the
number of times they have to wait to put
or take and if you have a lot of
consumer weights you need more producer
prez gala producer weights need more
consumer trips now there's an issue with
the the standard array blocking key
limitations yeah the capacity of the
queue is a number of objects right so
unless all objects are sticking in there
have exactly the same size the memory
usage of this q Can Can just vary widely
and you can run out of memory if you
don't somehow limit or keep track of the
memory usage of these of these cues and
that's actually happening to us so what
can you do about it well one solution is
to layer a logical q over the top of
your blocking queue and this logical Q
is going to be a buffered Q so you're
going to have a list and you can store
objects days list and then what you're
actually added the key or these lips
right and so and the list are going to
be considered full when they're
estimating memory usage reaches a
certain amount so you're getting it with
a queue of Liz and they're each going to
use the same amount of memory okay and
then the maximum amount of memory you
can stick in the queue is your your
buffer size that was actually declared
for the queue times are the buffer size
times the key capacity which was
actually declared for the queue so so
then the max amount of memory
it's going to use your is this going to
be you want to statically reserve it
when the queue is created and then
release it when the thread stopped you
know this has one of the nice things
about using buffer cues is it has
additional benefit if you reduce the
number of puts and gets online queue
because even though it's not using
classic synchronization awready blocking
keys that the block and cube java
blocking cues do have a synchronization
cost and so this can help reduce that
because you're not doing an actual put
or get to the queue as often as you
would if you're just sticking individual
objects in there and another advantage
of it is it basis the amount of Q to
work you have for any given thread by
the amount of memory that work consumes
and it's probably true that larger
requests take longer to process so it's
probably a better metric than just a
number of requests and now if you're if
you got a buffer queue one thing you got
to keep in mind is it you got to make
sure you flush the queue if it ever gets
idle it has to wait there's always you
can end up with things stuck and they're
not getting out to the user or to their
next destination and so speaking of
waiting and waiting indefinitely on
anything is dangerous so you want to try
to never wait on anything forever right
because if you do and there's another
threat needs you to do something like
stop for example right you're going to
be waiting forever and so you really
want to use loops and timeouts whenever
you can that way you can wake up see if
anybody wants to stop or maybe there's a
more urgent priority request to deal
with and if not then you can just go
back to waiting again but i can't
recommend this enough it's really
important you never wait on anything
forever
alright so despite everything you do you
get it out of memory here alright so why
would that happen well maybe you
remember used adjustments were we're too
low maybe all that memory you decide you
weren't going to track use the more
memory than you thought it was going to
or maybe there was just some short-term
bursts that overwhelm the garbage
collector and it just couldn't handle it
or maybe you got a memory leak are
there's too many threads there's just a
lot of reasons why this could happen so
so if an outofmemory error occurs you're
probably going to be heap dump to see
where you know what was using the memory
so the the IBM j9 JVM enables them by
default you can disable it of course if
you don't want that the Oracle hotspot
does have them disabled by default so if
you want he dumps with using hot spot
you got enable it and i highly recommend
that you do this if you're only going to
get a dump if you get an out of memory
you're not going to get a dunk for any
little thing that goes wrong you'll only
get a 4 out of memory error and it's
really useful but you know of course if
in the field in production you're
getting lots of memory errors and it's
overwhelming the customers of servers
you can you can turn it off so when in
our memory care occurs as I mentioned
earlier it's only going to its loan
guarantee to stop the thread that
triggered it so you can edit with zombie
what I call zombie threads right the
threads that were dependent on that
thread that died can hang and you know
the death of the thread that got out of
memory exception it might be enough to
free up enough air a memory to allow
things to perceive but maybe not
probably not so so if this happens you
need some way of bringing down all your
related threads so one way to do that is
with stock request objects so and we'll
talk about that in just a moment but if
you've got what i call a critical thread
it's a thread that eat us your
application just has to have this thread
goes down you might as well you're not
getting done so if that happens
you probably wanna bring the entire JVM
down and we'll talk a bit about how to
do that and now if you're having lots of
problems with zombie threads just them
you know everything if when you get out
of the rear is everything just doesn't
seem to work you might just want to
bring down the entire JVM all the time
but you shouldn't have to do that all
right so stop him with stock request
objects so what does that mean well it
means just have an object what's called
stop request object that that's going to
indicate the thread should saw you'll
have all your classes regularly check
this thing right and then of course
always before waiting on anything okay
and it can work this it works the same
whether the class is running as a thread
or not which is one of the nice things
about it and you could actually also
have different types of stop requests
maybe you want you know the thread to
stop right now I don't care what you're
doing just stop or maybe you want it to
stop gracefully you want to tell the
thread finish what you're doing first
then stop so you could have just one
single stop request object for all the
related threads but that only allows the
mall to be stopped at once so probably
what you want is a stop request object
that's got for the group of threads that
are related and then an individual stop
request objects for each thread so you
can stop them individually now setting
it you know you'd have to be
synchronized to set it but stopping
doesn't happen that all and once you do
it it's spot and then reading it of
course it's going to have them
frequently but that doesn't need to be
synchronized because it's not critical
you see the stop request right away JDM
isn't going to synchronization isn't
going to guarantee that anyway and it
should be a boolean items should be
atomic you shouldn't have any issues
with the non atomic updates oh so that's
how you can get your threads to stop so
how do you know when they've actually
stopped well you could use thread
daughter's alive right but to do that
you have to have a handle to the three
and it presumes the class is running as
a thread what if it isn't running as a
maybe sometimes you run a zone sometimes
you don't right so this doesn't really
work for hire abstraction so what can
you do well so it's better to just make
that abstract and one way to do that is
to separate your runnable class from the
class it's going to do the real work
have a runnable class invoke the work
class then your runnable class has a
flag called israelian or whatever you
want to call it that keeps track of the
run status and and then you have all
your thread to use this common runnable
class another advantage of this by
having a common runnable classes you can
guarantee have a common mechanism for
handling uncaught exceptions which are
just about to talk about so using this
technique if you know you have a
graceful stop if the stop request was
graceful that you want your threads to
wait for all the producers droids to
stop before they stop so that allows
hanging in flight data to get processed
and if you have an uncut exception of
some other unexpected thread termination
you just can set the stock request to
break down all the related threads so
you know finally you gotta when you stop
you have to return control of the user
you want to make sure you don't return
control of your user until you're sure
that all the threads have stopped
because if they decide they want to or
resubmit the request again have it start
up again it's really where things can
happen if you try to bring up a new set
of threads when the old threads are
still running so what can you do how do
you do that well you could call bread
join wait for the threads to actually
physically stop but to do that you have
to have a handle all the threads and
while that's happening you can't be
responsive to any other requests so it's
probably better to just have your
controller query or is running flag and
waits them all to be stopped so here's
an example not going to go into detail
on this it's just here for a reference
alright final thing we're going to talk
about today is uncaught exception
handling so it's really important you
have proper hailing for your uncut
exceptions you know the leading cause of
unexpected thread termination and if you
got any threads that are dependent on on
your threads and one goes down
you're gonna have hanging threads and
hanging threads can be really hard to
detect right are they hung or they just
in between births of traffic you know
you can't proper you can't be for proper
statistics to monett that that can help
you detect it but it can take a while
before you're sure that these traits
really hung and then if you've got some
kind of Ottawa board policy on unhung
threads it's inevitable that you're
going to end up auto boring them too
soon so what typically ends up happening
if you end up if you have hung dreads is
it requires a human to come in there and
do something about it and as we all know
that's that's bad so so by default you
know if you don't do anything you're
uncut extensions are handle by the
default on cut exception handler and all
that does is write a stack trace the
standard error so that's rarely you know
what you want right the stack trace make
it lost and if you're if you did have
your classes tracking your run status
it's going to be wrong it's going to
still say it's running because you never
had a chance to set it to not running
and then what if you have any other
threads that are dependent on that
thread you really you really just can't
do anything so you know don't let the
default happen to you so you want to
make sure you have Han electron cut
except panels and with uncaught
exceptions you want to make sure you
just do what you have to do keep it to
this simple just do exactly what you
have to do and no more because if you
got really complex uncaught exception
handling it could trigger other on
cotton exceptions and it gets really
hard to debug because most likely what
you're interested in was the original
uncut exception if you get an uncaught
exception your uncut exception handler
that's what you're going to be seen the
original is going to get mashed in most
cases and and it's also probably true
that your own cut exception handler
doesn't get tested your prizing to test
as much as your other classes so you
don't really want you don't want a
complex on cut exception handler all
handlers at a minimum you know should
log the air and stack trace to somewhere
or your get actually able to find it
like a trace file or something and you
should let the user know that this
occurred if you've gotta run status you
should set that the stock and error or
some other equivalent and and to do that
though you gotta have access to the
thread class state
so I ideally the handler should also
delete and any dependent threads you you
could have your handler alert the
controller which could restart that
thread but that's not recommended
because any related threads you don't
know what state they're in so the best
thing to do is safest thing is just
bring down all the related threads and
to do that you don't actually have to
have a controller or if you do have one
you'll have to get involved if you've
got the stop request object you just
when uncaught exception happens the
uncaught exception allure sets it to
stop a board stop and then the threads
all the related Reds go down and you
don't have any hanging so we're to
handle there's three primary places you
can have a custom uncaught exception
handler you could catch throwable and
the run method or you can have a finally
block in your run net and we're going to
talk about each of those in turn so if
you got a custom exception handler just
three different kinds you can have one
per jim iam you can have one per thread
group you can have one per runnable
class / JDM we mentioned this earlier
about all you can do is is law of the
air and alert the user that's about all
you can do because you don't have access
tending your specific drug classes or
three group state if you do this you
said it'd be the static thread method
set up default uncut exception handler
another way is to have a custom mount
cotton calendar birthright group so this
can guarantee you've got snared handling
for a whole group of threads this is a
good choice for a default uncaught
exception and you have your custom dry
group class extend that the right group
and override the uncut exception method
and now this allows you to have some
common code for your related threads but
you still don't have access to the
individual runnable classes for your
dreads so you could have Co here to set
the stop you could register the stop
object for those related threads with
your with your thread group and then
have it set that and then you said it
true if an uncaught exception handler
uncaught exception occurs and then you
can have a uncut except tyler per thread
class
so this gives you complete access to the
state of your ear your class you can log
anything you want about it you have all
the information there and it's
guaranteed to be in vote and to use this
one you don't use a static version of it
set on text me I'll use the non static
version and then you could catch
throwable in the run method well this
isn't recommended in most cases if
you're going to do this you want to make
sure you do it at the highest level
possible because you do it down inside
any loops you catch let's see you catch
out of memory error now that occurs you
catch it and you're still inside some
loop the thread isn't going to die but
it's not really gonna be able to do
anything the JVM just you're just going
to be in States gonna be a zombie you
could have a zombie thread and it's you
just don't want that there is only
really one good reason to do this that I
can think of and that is is that you
know can be useful if you've got a
finally block that's got some complex
processing in it and you wanna if an
uncaught exception occurs you want to
remember what the original one was so in
case an uncaught exception like Heller
uncaught exception occurs in your
finally block you'll know what the
original exception was so you could
catch throwable log the original error
there then re throw it or do all your
other handling there so that way if one
occurs in the uncaught exception
handling for the finally block you'll at
least know what the original exception
was but this is a probably not worth it
with your having this happen to you a
lot that what else goes back to what I
said earlier keep your uncut akhir
finally blocks on kite exception
handling to a minimum so then another
way is to have a finally blocking your
run method and if you're going to use
this technique what you do is you have
some kind of flag a handled flag that
you set to true the end of every try
block every catch block so if you ever
get to the finally block and handled is
false you had an uncaught exception so
one of the answers of this is that you
you have handling that's customized just
for that class you have complete access
to its run state and everything
disadvantages are every catch block has
to set this handle flag to false or it's
going to be a said to the true excuse me
or it's going to be treated as an
unhandled
uncut exception and then finally the you
still need an uncaught exception handler
that's going to log the exception
because the finally block doesn't have
access to the exception and one final
thing to talk about is is critical
threads so these are threads that your
application has to have and so if they
stop we mentioned this before you're a
pitcher is not going to function so you
probably wanna bring the entire JVM down
and wait to do that is the call system
exit and one way to make sure this
happens is to have a separate thread
group for your critical throat and that
uncaught exception handler for that
thread group a call system exit and your
thread group on collective chillin for
non-critical threads doesn't call
systematics and that way your critical
threads will bring down the JVM but your
non-critical dreads won't all right so
here's an example of using finally block
not going to go into this in detail look
we're a little over and so in summary
we've seen that excessive memories using
too much memory can really have a bad
effect on your performance it's if you
can estimate the memory usage your
objects yeah it's the first step banal
to control your memory usage and then
tracking and controlling at runtime can
really pay off to avoid out of memory
error there's a lot of ways you can
reduce your memory usage up front you
can limit your threads input queue a
topic you sighs you can balance the
normal usage of your threads and then
having a good framework for a stopping
starting and handling on cut exceptions
is really critical if you want to
properly avoid hanging and other
unpleasant behavior when out of memory
error occurs okay that's it thank you
and I'll take questions now</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>