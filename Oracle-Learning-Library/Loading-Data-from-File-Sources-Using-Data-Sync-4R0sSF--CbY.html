<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Loading Data from File Sources Using Data Sync | Coder Coacher - Coaching Coders</title><meta content="Loading Data from File Sources Using Data Sync - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Loading Data from File Sources Using Data Sync</b></h2><h5 class="post__date">2015-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4R0sSF--CbY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so you want to use data sync to upload
customer fulfillment and Revenue details
from an on-premises flat file to a
provisions cloud service let me show you
how the data file root directory that
contains the flat file has already been
defined in data sync defining a default
directory enables data sync to easily
configure the repository when porting
metadata to other environment prior to
importing examine the file to view the
delimiter x' ensure that each row is on
a separate line and determine if there
is a header row and if you want to skip
any lines data sync estimates the data
types and other characteristics such as
type and length of a string before
creating the new target table in the
cloud service and before it registers
the columns into which the data will be
imported when data sink registers a file
it records the files metadata locate the
file and view the import options accept
the specified delimiter as this file
uses commas to delimit the columns by
default data sync samples up to 10,000
records to identify the data types
because this file is so small use a
minus 1 to sample all rows next create a
new target table into which data sync
will import the file the source file
column mapping dialog identifies the
attributes for each column within the
fulfillments file inspect the column
attributes for the customer fulfillment
table if a table contains columns that
are not needed for analysis deselect the
load check box for the specific columns
if the type attribute is incorrect
select the proper characteristic for the
column data sync estimates the length of
each string field during the import
process and computes the length based on
increments of 50 if you want to remove
duplicates or perform an incremental
load while reading from a file select
the check box for update rows on match
to keep data in the target table for a
period of interest rather than
accumulating data during subsequent
loads select the rolling delete checkbox
data sync will prune the data every time
the table is loaded note that the
selected column must be a date field and
on the target tables tab you must
specify the number of days to retain the
data before data sync performs the purge
finally define a load strategy for the
file the load strategy tells data sink
how to handle subsequent operations
against the file such as load
incrementally append data and replace
data when you select never delete data
the target table is never truncated
select a user key column that uniquely
identifies each record for update and a
date timestamp column for rolling
deletes this is especially useful if
you're populating the table by means
other than data sync now you know how to
upload a file source create a rolling
delete and define a load strategy for a
file source using Data Sync thanks for
watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>