<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Enterprise Search in Action | Coder Coacher - Coaching Coders</title><meta content="Enterprise Search in Action - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Enterprise Search in Action</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bS7oqOlBkIc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everybody feel thrilled
to have here at the last session you
know the Java one and I must start with
making an apology to all of you because
in this session there won't be any java
code sorry for that yeah but to
compensate that we have some groovy code
and some sparkle so hope that helps my
name is holding us and the fellow with
blue minish almost turned here in the IT
business right now next to me is a
marketer drink also very experienced RT
architect also working at a tremendous
this talk is going about enterprise and
enterprise search is not about searching
enterprises but it's all about finding
information it is a very broad subject
and we had a lot of thoughts about how
to address all these subjects what you
saw is that one of the biggest subjects
of this java one is modularity I'm all
dirty I'm very old I think from the 70s
concept so we thought okay it's cool to
bring back old stuff nowadays so we are
going to use the four plus one view it's
a very very known way of describing
architectures for addressing all the
issues concerning Antipas church and we
made a slight modification to the
four-poster on fuel it's called the 5+1
field because we need an extra few let
me first try to explain what if 4+1 a
few ways i said as i said justice that
enterprise search has a lot of different
aspects like architect service has also
a lot of different aspects all they
going to do is give this talk and
address issues
using a diffuse of the focus one I feel
and we start the logical view what you
do in logical view is that year describe
the decomposition of your system in
logical abstractions example if you're
having a object-oriented system Yoshio
fuel and compose the the object model of
your system the next few we're going to
use in our case the process view and
process views all about processes
tourettes concurrency and has a lot of
impact on the scalability of your of
your system which is of course a very
important in Maine and processors then
we're going to talk about the physical
layout often typical Enterprise Search
application in your development
environment which technologies are used
which libraries are used and how they
put together to form the solution once
you have the software you have to deploy
it somewhere infrastructure and the
mapping of the sofa solution to the
infrastructure is what is described in
the deployment view and of course an
inspired solution is solution for users
so all the user cares about is the
functionality which is offered by them
and add functionality is a distracter in
the use case view and if you look at if
if you look at an enterprise search
solution a key component and key aspect
of course content is the information and
that's why we've added another filter
for the focus on fuel and we call it the
information for you just to describe the
information aspects of the solution and
it's all about meta data structure
content structure and how it is related
to touch on the Munich stalker I'm going
to describe for your the the u.s. guys
feel any information field which are the
boring aspects of this talk then we'll
go into the
dual solution and Mark will take over
and describe how an enterprise such
solution is built so from a functional
perspective of what it said message and
I had to answer this question a lot of
times because if you're talking about
enterprisers the first question we
always get is what makes it different
from google search because we all know
and love google or bing but i think most
of us is no noble and yeah part of a
search solution is not searching with
finding information so what you see over
here is a typical UI for foreign
enterprise search a solution and yeah it
has google like google like results
that's what you what you see in the
bottom left it's a the results of a
query they are they're all that small
description of what is found and you can
click on that and then you go to the
document which is found in the virtual
set so nothing new but there are some
other pieces of information on the
screen that is the grouping what you see
over there are facets and it is a
grouping of the results found based on
the search query of them and it is a
domain-specific so what you typically
can do in an intercessor application is
that you have a heaven model of the
information and you can group
the results into specific clusters of of
information which can be defined by
yourself another aspect of integers is
something because if you have a large a
lot resultset you want to influence the
ranking of the results your fans
sometimes let let me give an example if
I'm looking if I'm looking for for java
for example i can get a lot of results
and sometimes i want to sort them on
data fabrication sometimes I want to
sort them on author another way I had to
Sodom is the subjects which are treated
in there so having a domain-specific
salt functionality also which is very
important in an end process and of
course the last thing is the filtering
so sometimes you want to exclude and
also based on the demesne definition you
have you want to exclude results from
your query so what what it what makes
enterprise search different from from
google is that you can add your own
domain knowledge your knowledge about
the information to the search engine and
use that information and providing users
with better better results that's the
whole mean but that's not the only thing
what distinguishes and pressures from
google search what you see over here is
a typical layout of all the information
which is present an enterprise so what
you have here is this web pages
but also behind the firewall we have
enterprise content management systems we
have a lot of databases with useful
information and we all know the famous
directory structure where all the
departmental documents are located and
if you look at at Google search it can
only index it can only search for the
Internet public addressable information
but from an enterprise search
perspective there is a lot of a lot more
interesting information available in
your enterprise and that should be found
by the by the enterprise resolution let
me recap so enterprise searches is all
about finding the relevant information
of all the of all the documents in your
enterprise very document is a very broad
term it isn't just the word documents
there's also PDF files databases content
management systems fast and good and
having good results where it's also very
important that it is integrated within
the security system of your of your of
your enterprise because it's only the
results but you're authorized photos
should be shown at all public this is
from a functional perspective the
biggest difference between Google 70 and
enterprise sensors
ok
now I want to shift from the functional
perspective to the information to the
information but perspective and as I
just said is that all you want to do
with enterprise search is that we add
domain-specific knowledge tutorial
documents so very important in
enterprise such a solution is that we
have knowledge about context in which
the information is created used and
unsearched so what do you do and by such
a solution is that if we look at this
this picture is that all the information
is indexed our solution is based on the
leasing solar so all the information is
an elusive solar index but before it is
placed into this this this index it is
enriched what we do is that we add a
domain-specific knowledge to the
document if you have if you mentioned
and knowledge you have you need some way
to model that that knowledge a mulling a
lot can be done in a several ways we are
using a semantic technology for it so
you can describe the knowledge in in re
f but you can also use all our other
forms of notes modeling so what you said
before we indexer the documents its will
pass our knowledge repository and elliot
will be enriched let me give an example
if we encounter a Jaffa in a document we
can Adam we can add metadata to the
document about Java but what is java is
it an it's an island or is done
programming knowledge of programming
language it can be very difficult to
decide which of these two others but
because we know the context in which
document is a curated we know for
example the author or we know the
company where this is matt is very easy
for us to judge whether it is and
programming language are at this edison
it is not so by applying this domain as
specific knowledge we get better meta
data about the document which we can use
in our search queries on the other side
what you can do is if there is a search
placed within the within each such a
solution we can also use the same
knowledge for enriching the search
request so if you're looking in your
search query for java we can add
programming language imperative language
to the search query and we can return
documents with no java but imperative
programming language which you should
also find in the return asset and the
last thing we can do about providing
better search results is that if a user
is logged in into the system we have a
profile of the user so we know the
interests we know the department is
working for the projects he or she is so
looking for and we can use that
information while creating a the search
request yeah yes okay okay
so you basically build on solar and you
see this correctly into the index if you
look at the at the indexing sex but we
also add it at Clarion time so if you
fire a query to leasing solar we can
enhance the query before it sends to
sing solo
try to understand the difference in what
you have
had a box from Seoul
we'll come to that later Marco all just
that it's a it's a very good question
but it will be adjusted in procession
it's okay okay
okay I'm going to hand off tomorrow okay
yeah thanks yeah it's all hands told
every about everybody about the use case
of the system about the information few
a now it's time to see how we turn it
all into software constructs so I'm
going to talk a little bit about the
logical few and in order to do that I'm
going back to the information few from
hounds where we have on the left side
the web pages this is an example it
could be more sources as well that is
going to be indexed inside the
enterprise system and that part is what
we call the collection process on the
other side of the equation we have
something called the publication process
if you have something inside the index
then you can search through it and
that's what we call the publication
process so there are two different
processes that we're going to address
with this in the back of our minds this
is the logical architecture that we are
using on the left side you do see the
the collection process and on the right
side there's the publication process so
let's let's go through all the
components from the collection process
first a tails it all starts with sources
you know in an enterprise search system
we we don't have any limitation on
sources they could actually be anywhere
behind a firewall in a database in a
semantic store in a Content for foster
II web pages whatever you know they can
be anywhere and it can also be in any
format it can be a word document it can
be a PDF it can be a keynote document
but it can also be you know a texture an
unstructured or a binary document like a
video or an audio file there is also
metadata metadata inside an audio file
as well and last you know it can be a
compound document it can be a zip file
containing other formats as well and we
all have to cope with that and the other
thing is that you know just like handset
in the the picture that the Internet
addressable stuff you know we'll have to
cope with security as well sometimes
humans are behind a firewall are behind
any security boundary and that will have
to handle that as well so from all the
sources how do we actually get
information from sources there are
actually three ways to get them first of
is what we all know from google you know
the crawling the spider ring and that's
where we unleash a robot on I for
example an internet or something and
they will crawl through all the
documents you know traversing all the
associations to other document and
honest path taking all the documents
with it and that we at the end we have
all the documents from a complete
website or complete directory structure
whatsoever that's what we call crawling
the other thing is what we call
harvesting and horse ting is actually
periodically pulling out of for example
a Content repository or a database or
whatever it's still a pool system but
it's not crawling the last thing that we
see over here is push we have to cope
with you know with with companies or
third parties that that do have
information for us that is very relevant
for us and they're going to be able to
push him into into our system so we are
going to give them api's to do that now
you can think about you know sold web
services rest services or even Java
api's we have all the information if we
haven't downloaded with we retrieve the
information we have to validate whether
the information is correct so we have to
look at the documents and we have to see
whether they are syntactically correct
whether they know the structure of the
document is what we expect it to be and
on the other side we also have to look
at the the content itself we have to
validate the content from a semantic
point of view you know it we got some
metadata properties from a document but
the combination of the metadata
properties are they valid in on example
could be a zip code and a house number
you know if you have a zip code and
house number we'll have to check whether
the weather on that zip code that house
number does actually exist for a certain
property and it doesn't you know the
document is invalid and we'll bet
index that as well so very important for
validation point of view what we what do
we do with the documents that we
actually reject so it's very important
that we either bring them back to the to
the reduction or the editors from the
original document or any third-party
whatsoever so that they can actually you
know take action and all the things that
we found when the documents have are
valid and then we are going to enrich
them we are going to add information to
it which we call enhancing so if we if
we have a set of metadata properties we
are able to actually add metadata
properties for example based on third
party announced repositories inside
either a semantic repository or a
database or an XML document or an XML
database whatsoever but before that you
know we do get a set of properties but
sometimes you know there are many more
properties inside the document and
that's also an enrichment step it's
called extraction so for example if you
get an HTML document or then you can
actually use XPath for example just to
extract even more made a data from the
from the from the document and then the
third enrichment is what we call
filtering is actually either taking out
properties that we think are not
relevant or will Club they're all the
information or it's actually for example
if you have a web page you know there's
all kinds of information on the web page
that are definitely not relevant for for
searching for example the navigation
part of the document or the ads we just
want to filter we just want to take them
out and we only want to have the
relevant pieces of the documents so that
we we may have better search results
when we have all the documents we have
validated the documents we have enriched
the documents we're going to index the
documents in other
is we are going to send them to the
search engine and the search engine will
actually analyze all the documents for
me Father for us and it will make them
searchable and they're storing it in a
searchable format inside an index and
also at that point of view we are able
to tell them this is a more important
document than other so we can actually
already do some boosting there as well
when all the documents have been indexed
inside the indexes from from the search
engine we can actually go and try to
find whatever we want to find inside the
index so we are going to actually send
in search requests into the search
engine and we get search results back so
the first component inside that
publication process is how do we
actually issue search requests well
actually there are many ways to issue
search requests and so it can be a get
interface HTTP GET interface it can be a
post in the face like in web servers or
reserves it can be api's from all kinds
of stuff you know there are hundreds of
way to issue your search request when
we've issued a search request we have to
check whether the search request is
valid you know coming back to the zip
code and house number if we have a
search request where we explicitly fire
a want to find information for a certain
zip code and a certain house number we
better check whether that combination is
still valid and if that combination is
not developed we're going to be sure
that no documents whatsoever going to be
found so in that's the case we have to
check whether not only are the search
requests coming in with the proper
structure we also have to check them and
validate them semantically and there's
the other step that we find very
important it's if we have a search
request and we know the person that is
issuing the search request we know the
context of the search request we can
actually enrich the search request we
can actually look at the several classes
from the search query and we're going to
recommend for example you use misspelled
your your filter or no its you you
trying to find the terms
Francisco inside a free text document
but you know what we do have explicit
properties called city do you want to
search on City San Francisco instead
because that would actually be resolving
to better search results and you know we
could also do that under the hood so if
we haven't stored you know zip codes
inside a document inside the search
engine we can actually in the enrichment
face we can actually look at the zip
code we can turn it into a coordinate
and will then we can use geospatial and
from search as well and we can all do
that under the hood and the enrichment
part you know when we have a proper
search request we are going to unleash
it on the search engine we're going to
tell the search engine go and filter all
the documents out filter out that is not
relevant based upon the search request
so it's going to look at the entire set
of all the records inside the search
engine and it's going to take out all
the non relevant leaving it at the end a
smaller set of search results and from
that smaller set of search results we
can actually apply grouping to it so we
can say look from a from a domain
specific content trying to tell me how
the explicit set of results are being
grouped so how many let's say we have a
thousand results how many of them do
apply to the city of Los Angeles how
many do you apply it to the city of San
Francisco how many did you apply it to
the city of Amsterdam if we have that
and we can handle the information to the
the interactive user for example you
know he can make a better choice you
know he can look at that grouping and
say okay oh yeah I want to only do the
documents from that specific group and
the other thing is that sorting you know
we have to apply specific ranking where
we where Google is only capable of of
ranking on either day or a very you know
very small parts of the document we can
actually rank on anything we can rank on
specific metadata properties we can rank
on domain specific metadata properties
we can actually use
domain-specific repositories to to to
you know to calculate new ranking
algorithms so we can actually be more
specific and ranking all the search
results coming out of the search engine
we are going we are able to look at the
search results and we are able to
actually tell them okay these are the
search results but you know based upon
the search results we think that the
other search results might be also of
your interest or you know they're there
are more documents that that might fit
your search query so that's also a
recommendation redirection based upon
the results and the other side was do a
recommendation based upon the request
and the other thing is that we use that
enrichment step as well just to add in
extra information you know normally you
get some properties that are indexed but
we can actually go out put more
information in there as well from other
3rd power outside repositories and we
can even put in editorial information
for example if you know you get results
and it's almost the end of the year you
know we can even put in an editorial
comment saying always by the way to the
end of the year you have filed in your
eye or your for the IRS you have to file
your tax return statement whatever you
could also put that in if you want if
that is relevant as well the last step
of that equation is that when we have
all the results you know we are we have
to be able to to present them to the to
the user that is using the system and
that is where we actually differ from
google searches well where Google is is
more about you know selling ads and it's
trying to to have very minimalistic
general istic search result screens you
know with a lot of ads there we can
actually make very rich user interfaces
making a possible for the fodder for the
users to find the information even more
quickly so it's all about finding
information
yeah I have an example of that later on
we actually use a kapala inside Lucy and
solar to do geospatial search and we are
using google maps api and that's the
example to actually take the zip code
out turn it into a coordinate and then
also index the quarters as well but
there's an example in that that's the
groovy coat that we're talking about
what we noticed from the logical view is
that there's a lot of enrichment and
validation everywhere inside the logical
architecture and we would like to have
to reuse a lot of that so we would like
to to make components you know
enrichment components that we can reuse
in the inflection process but also in
the various publication processes as
well and also across multiple tenants so
we said okay we have to think about you
know the the logical view of the richer
component and looking at the enricher
component we found that it's actually
following a pipe and filter architecture
so actually we are going to feed the
documents into a flow and you know it's
going to execute actions on the
documents and the outcome of the actions
going to be put back into the documents
are going to do the another action and
the outcome of you actually going to be
put back into the document so it's
actually traversing a spear certain flow
and in that flow it's actually doing
enrichment actions extracting filtering
enhancing but even validating is also
part of that and what you see here for
example is it's a very easy example you
know document coming in with three
fields you can remove a field yeah you
can look at the field and then based
upon the concert field go either this
way or that way during the enrichment
that's content based routing as well so
look okay looking at from at that point
of view oh sorry yeah sorry we could the
enricher what we did create is a an
xml-based the main specific language
where we actually specify
domain-specific enrichment actions so if
a search request coming in we can use an
xml-based language to turn the search
request into a domain-specific search
request and there's some example about
that later on what what we normally do
is that the configuration of that is
being done by doc by the company you
know by the enterprise and we are not
actually what's not the search engine
it's the people that are configuring the
enrichment actions but you can choose
whether you doing a filter or a ranking
so there's a lot of choices there that
you can do but the choices are all up to
the to the enterprises that are behind
the search solution you know we can talk
about a later on maybe it's going to be
answered by some of the query of some of
the examples that we have so again what
we did is that we we build an xml-based
domain-specific language just to
orchestrate the flow of the enricher and
inside there we can do if then else you
know switch case else we can add remove
fields we can do we know we can execute
expressions and Richmond actions and we
actually already built some richer's
that we that we can use inside the flows
one of them being HTTP client where we
can actually extract information from
from a URL based on a URL if it's
restful
coming back we can use OG NL just to
parse the rest and then to extract
metadata from the rest response we can
use xslt ex parte xquery to to you know
to go to xml-based repositories and
extract information of that we can use
jdbc we can use Sparkle their Sparkle
here which is a query language for the
semantic repositories and we can also go
out to another search engine for example
then just to do another search request
and the response of that search request
being fed into the original document as
well and the last thing is what we can
do is apache tika and apache tika is a
library for extracting metadata you know
from a huge amount of different document
types it's pretty good there's an
example of the xml-based the
domain-specific language that we have
what you see over here is an N richer
and you're richer goes from top to
bottom it adds a field a adds a field B
it adds a field another add another
field B and there is adding something
with with this yeah it's a velocity
replacement where we replace the the a
between brackets with the actual content
of a so we're going to create new fields
with the content of the other fields and
you know there's an if-then-else add
field we can remove fields that's the
basic part of it but it can also be a
little bit you know more more difficult
than that for example you know we can
add a field that we can add a property
called URL we can use an HTTP and richer
you know to grab the contents behind the
URL put it inside the website property
and then we can use an XPath expression
for example just to get the h1 element
to create a new property called title we
can actually filter the website by
taking out all the things that are idle
relevant or not relevant making
a new property called filtered website
and then we can use tika just to take
out all the individual terms from that
fill that website and then later on we
can actually use switch test just to
look at the various fields to create new
patterns new properties so it's it's
it's a very powerful but also complex
dsl that you can actually use in all all
the parts within the architecture as
well in the collection process as well
as also in the publication process and
the request side but also in the
response side so you can use it for
example to enrich your search request
there is an example of the use of
sparkle just taking you know creating a
field called place and then you know
adding executing a sparkle statement on
a certain Sparkle semantic repository
you know using the information from
another property that we already have
and the outcome of this sparkle query is
also going to be inside inserted into
the document as well and this is the you
know the code that we have for the
geospatial search so that might be your
answer to it this is a very funny thing
what we do over here let's see whether
this works you can see this ok we have a
field called fq location zip code FQ
stands for filtered query which is a
construct inside lachine solar what we
do is that all the individual plans of a
search request will be individual
properties inside the document so we are
actually saying a search request is also
a document and then if we if we know
that it's a document with fields with
properties then we can also enrich it so
we can actually do the all the ratio of
things with it and that's what we did so
we're looking at the filter query clause
if it you know if it complies to a
certain pattern regular expression
pattern then we know it's a valid
zip code and if we know it's developed
zip code we can actually execute some
you know some groovy code scripting code
and then the outcome of this scripting
code it's actually going to call a
Google Maps API and the outcome of that
scripting code is going to be put back
into the document or in this case the
search request as well which means that
your search request is not going to be
enriched with with the coordinates that
you get back from the google maps api so
instead of searching on a zip code
you're going to search on a coordinate
and if the pattern of the zip code you
know is it doesn't comply to the
certitude irregular expression pattern
let's go back to push the button if it
doesn't comply to the regular expression
over here we are going to reject the
document in this case we're going to
reject the search query so this is a
very declarative way of enriching your
sir your documents for the collection
process your search request but also
your search responses and you don't
actually need to code anything it's just
a declarative way to do that let's go to
the process view of the of the solution
that we do have let's look at the
collection process first from a
collection process point of view what we
did just to make the system more
responsive we actually split up the
collection process and the publication
process which looks very logically to
certain people but then again it's it's
it's very important to do that you know
the collection process all the
information coming out of the collection
process is going to be indexed into
Lucien's solar but it can also be on a
very different machine and and then on
another machine you know the publication
process is going to use the indexes and
that's where we split up the collection
process and publication process what we
also looked at is that you know from a
collection process point of view we
noticed that not only
enrichment is a flow but the entire
process of the collection process is a
business process flow and a business
process flow in our in our opinions you
know we could actually we are looking
towards a concept of an enterprise
search service bus in this situation and
why do we choose to use an enterprise
service bus for the collection process
well it's very simple all the concepts
within the ESB actually map perfectly
under the business process that we do
have so we can actually use channels and
pipes just to just to get the coupling
between the different surfaces between
the different components of the
collection process so that we can
actually have the we can actually make
the the entire collection process
asynchronously so if we finish the
collect content inbound we can
asynchronously validate it
asynchronously and rich it and that will
speed up the things as well so it's good
for first performance of scalability as
well we can also make it parallel so we
can actually do the content enrichment
in parallel a lot so that will actually
speed up the entire thing as well
content based routing is also very
important if we have split up all the
components you know we could use content
based routing that looks inside the
documents we can use it to to map the
unity to wrap the information to
different components and even two
different machines different notes in
your cluster as well which makes it more
distributed and scalable as well you get
all the things like hot deploy you know
we can split it up in services getting
more modular modularity we can do reuse
of all the components over more than one
collection process for so different
collection processes for other customers
you know it's it's it does fit perfectly
and the good part of that you know using
an ESB you get all the nifty things that
you normally would would be we have to
program you know you get them out of the
box so you get all the components like
the Transformers the validations the
splitters and and the filters the
routers everything you know it's out of
the box its nearing inside an enterprise
service bus and
from complete my point of view you know
Enterprise Service buses do have a lot
of transports available already so web
services smtp you know Jay JMS sometimes
jcr everything is there so all the tools
that we need you know to just get the
information out of the sources is there
and to be leveraged so it does have a
very good fit there what we what I'm
trying to do in this picture is that the
collection the collection process and
I'm going to express it using enterprise
integration pattern kind of a kind of
way so what you see this is not going to
work what you see here is that you know
the components from the logical
architecture we you know we created
services from them so this is the
content inbound service we have a
validation service and enrichment
service an indexer servers and they all
are you know decoupled by channels which
means that they can all run
asynchronously in parallel on different
machines distributive so this is really
this really helped us getting a very
responsive scalable collection process
from an end from an implementation point
of view let's look at what we what
components that we use that we use to do
the implementation of the several
components within the logical
architecture going back to the logical
architecture what you see over here
let's put something on top of that now
we'll use apache Lucene solar for the
search engine and for the index we used
on the left side we use a tool called
Apache notch to do the crawling the
spidering thing and apache nudge is an
Apache Hadoop based crawling crawling
utility which is very scalable very very
performant works perfectly and we're
using apache tikka to do part of the
extraction of documents that we normally
can't do that like a word document or
any you know the doc documents or the
dot PDF documents you you have to do a
lot of things there and apache teak i
can do it for us and the entire
collection process we are using an
enterprise service person in this
situation we have used mule ESB but you
know apache camera works works as well
and might be a better solution you know
a better solution in other situations
from a UI point of view you know you get
the results back from solar and it
doesn't really matter what kind of tools
you use there and so we have used all
kinds of tools we have even used PHP
with smarty templates we have used
groovy grails just to create the
presentation we have used all kinds of
tools here from an enricher point of
view the enricher that we talked about
with the xml-based DSS we actually build
our own and richer frame x framework and
that a richer framer is going to be
talked about later on what we used over
there going back to the publication
process what we said is that we have
leveraged a tool called up
jealousy and solar which is a very very
scalable fast full-text search engine
completely written in Java it has a lot
of function of features or that we can
already leverage out of the box so
there's hit highlighting there's
facetted search which is grouping and it
has cashing it has boosting
functionality it is very good in scaling
distributed the replication of indexes
so there's a lot of things that is that
is there already out of the box and it's
open source and the good thing it has it
is very very extensible they really gave
a good thought about how we are going to
split up the entire solar solution into
a good api's good modular system so
that's what we leverage has leveraged
very well so the thing is what we did
with your publication process we just
embraced it we said come on it's it's
you know building your own search engine
doesn't make sense you know just pick
one and this is the best search engine
open source java based search engine
around so we could actually use the
cinnamon in handling the stop words you
know the stemming the more like this the
spelling for said there is a huge amount
of functionality that we can already
leverage out of the box and we did
leverage that out box of course we did
you know from an API point of view it
already had a huge amount of API is that
we can use you know it has Jason a piace
that XML API is a PHP Java API there's a
lot that you can actually leverage so
why why do it over again you know
reinvent the wheel that doesn't really
make sense from a customization point of
views for each enterprise that we have
we had to you know tune it to the
enterprise itself tune it to the tenant
itself so we have to create a specific
schema for that specific enterprise and
we have to do some configuration of the
runtime of solar for that specific
enterprise but you know and the other
thing is that there's a lot of
enrichment already inside the solar
stack but we wanted to get our and
richer you know where we can
leverage and reuse all the enricher
action actions that we have and the
xml-based dsl we had to create extends a
solar so that we can actually use that
for a for enriching search request but
also enriching the search responses and
another thing that we did add was a
little bit about security because you
know if you have if you are in an
enterprise and you have indexed all the
information it's not said that everybody
within the enterprise has access to that
information no it could be that a
certain you know that the the director
has more access to other documents than
someone on the working floor so we had
to extend the solar system which
security features as well coming back to
the the enriching framework that we have
you know looking at from a point of view
we we already see it's a pipe and filter
architecture it reaches you know with
their traversing a certain flow doing
all kinds of richer actions in a certain
order so it's extracting and enhancing
filtering validation and we try to find
a muddler way of of doing that so what
we did is that we use osgi to build and
richer's and we made it capable of
plugging in osgi bundles inside the
enricher framework so that it
immediately is extended with new and
richer's so from a point I'm going to
back blader on why that is so important
as well and then from a orchestration of
a flow we have actually build an engine
based using sex event handlers to to to
you know to handle the the
domain-specific language and to actually
call the enricher bundles as well and
using osgi gave us a lot it was hot
pluggable it was updatable it was
modular
we could actually reuse richer's and we
didn't need to restart the entire system
when we make changes to all the other
enriches and even make changes to the
configuration of that and it's very very
extensible which means new enriches just
create a new bundle and just install it
inside the OSGi container and you can
use it immediately without restart
without downtown down time anything and
if you have if you're looking at from a
deployment point of view which which
comes back later on as well but from a
deployment point of view using osgi and
we have a tool called Apache A's we
could actually you know deploy or or we
could actually was that called you know
provision the the bundles the young
richer's and we could provision also the
configuration of the flow but also the
configuration of the schema of leucine
and the configuration of the collection
process the mule ESB flows we could
provision the configuration using a tool
called Apache age over many of the nodes
within the cluster if you're deploying
it and on a cluster as well so the last
thing I want to talk about is deployment
you know of course you know if you have
a system and you're going to use it
inside an enterprise it gets bigger
bigger bigger bigger bigger which means
that uncertain moved in time you have to
think about clustering adding more notes
just to get more scalability just to get
more performance and if you have if you
are able now with the content based
routing and the collection process to
distribute the components on different
nodes you have to think about that so
let's take a look at a one of our
typical deployment ways of deployment
what we did is that from a collection
server just we we took one big server to
do all the the the collection process
work and we store them in in indexes
from solar and we had a couple of slave
we call them slave a couple of
publication process servers and we could
just add
of that and there's a load balancer on
top of that and looking from the left
side all the deployment so all the new
changes to the to the search solution
which are we could all provision them as
bundles because we have leverage now we
have embraced osgi said well so that
works pretty well it's very flexible
which means that if you want to have
more collection process service we just
add more collection process servers and
then we'll work together and on the
other side you know so Lucy and so is
very good at for example index
replication and making you know a
cluster of search engines so we can just
add more search engines to it
immediately getting more scalability and
more performance so that works pretty
well as well so what we talked about now
is we we talked about you know the
search engine we talked about how we you
know how we implement that the
collection process we talked about how
we implement at the publication process
we talked about the enrichment
functionality the enriches that we have
and what we did to it to make that
possible how we deployed it and that
actually concludes the presentation
there's some more to read for that what
we the solution that we created so the
entire collection process and the entire
publication process and the enricher and
all of that has been given to the m dot
o guys and just take a look at i'm da da
da dork which is pretty good framework
for building very rapid cloud-based
applications osgi based more or less so
all the solutions that we build they're
all part of the UM data stack some more
read about the search engine there's you
know just check out the leucine solar
webpage is pretty good and on the last
part to read is about the company that
we all work for it's called luminous
just check it out so this does conclude
the presentation if you have any
questions just let me know
the question is whether we have
implemented field level security yes we
did for a customer yes we did and we did
that by adding a specific field where we
have added you know the ACL stuff we
just added in that field and then based
upon the profile of the the user that is
locked in or we could actually extend
the filters using the N richer's we
could extend the filters under the hood
we're getting giving them with with
adding more security based filters to
the search request that is how we did
that yes it is the question was whether
it's late binding it is late binding
it's done at runtime
part of clearing days yes bunch of
configuration that you do you support
the canary
it is a the question is whether we
support the addition of dynamic
attributes where we don't know the
attributes up front that's the question
right so yes we actually do you know
normally what happens for example if we
use apache tika you know with Apache
tika you know you can just extract all
the properties that it can find which
might not be which might be properties
that you don't know about so we we use
that as well is that that's for your
question yes ok the question is what do
we do when you get more properties on
the way you know when you have it in
production it the configuration of the
pipe is XML and the XML can be
provisioned by the paci a stuff and if
it's as soon as it readies provision and
it's right and it is stored on the
servers it will actually use the server
the enricher framework will use that XML
so you can do that when end production
which sometimes might not be a good idea
but you could
the question is whether all the
attributes of the enricher this sit way
in this case is controlled by the
configuration of the xml-based
configuration yes there is more for
example a leucine solar and if you use
all the features part of leucine solar
then you know a restart might be needed
sometimes but from a enrichment
framework point of view there's no
restart necessary how the question is
how do we handle multilingual documents
in the same index if they if they if
they have a different key or URL then
there's no problem whatsoever so that's
what okay all right now I understand
when you look at the enricher the normal
solution in solar would be and text file
with stop words with synonyms and stuff
like that but looking at a enrichment
framework you see that we can actually
go into a semantic storage we can go
into a semantics semantics repository
and that's what we already that's what
we also did you know we have some escos
libraries for example stored outside of
the application server and based upon
the search requests coming in we can
actually do a specific sparco query
which can be language specific so we
could actually based upon the language
in the search request we can actually
add other synonyms than than the ones in
the text file and ask us is really good
at defining synonyms and
and.and a broader and narrower terms and
stuff like that see more along the lines
of the actual process of hot and just
when those essentially coil everything
down and you apply those rules to what's
coming in mm-hmm if you have multiple
sets of rules right you know for each
different language it could be it's not
gonna necessarily boil things we could
looking at the enricher framework there
is a way to do content based routing
towards different indexes so you could
you could but you could store it as well
as a different document as well inside
the same index with other properties
there that's possible if you separate
index if they're a separate solar has
functionality just to do some further
it's the federated search now searching
through two indexes and then combining
the results but I already also did a
project where we did federated search on
on more search engines not only solar
and that's why we use an ESB as well you
could do that as well more questions
mention find you say obstacles
intentions very able to integrate with
Joshua sounds what we just simply the
term we list of search results okay yes
yes the question was using a federated
search functionality were we able to
combine results coming back from the
different search engines into the
respond yeah that's that's the hardest
thing in federated search because they
have different architectures they have
different relevance relevant irrelevant
algorithms so you know so there's also a
trade up there as well and it's not only
the relevance but it's also that you
know that how fast are the refill cells
coming back do you are you going to be
waiting for all the search engines or
are you just going to be you know you
know you make it a asynchronous and then
put a listener there to get all the
results back and then as the results
coming in then you know just using Ajax
functionality adjust the UI like that
you know federated search is really
really interesting technology let's call
it that way but it's it's extremely hard
and especially the relevancy so there
has to be because we have the enricher
we what we did is we try to we try to
map the different relevance scores the
different scores coming back because
they are just metadata properties we try
to map them on a generic relevance yeah
we try to normalize them as well which
worked pretty well but it's still
difficult
some
let's before is like smacks or
salt
okay the question is how can we how do
we handle complex things within the
solar stack the the dis max handler
stuff like that most of it is not done
fighting richer so we leveraging the
standard stuff from solar there is that
your question answer yeah we just you
know we embraced solar the only thing
that we did with solar is that we use
its extensibility way of extensibility
to add plugins on the request side and
also have a plugging on the response
side and through that plug-in we had
access to the generator framework and
because we have actually bundled the the
solar solution into a OSGi container
inside the indata framework we have
access to other OSGi bundles and so we
have access to the end richer's which
are osgi bundles the question is does
the extension support date ranges
actually yeah we do
any more questions not with you okay
last question then ok the question was
how do we search PDF files and word
files actually we use the tool called
Apache tika for that what Apache tika
will do it will open up word files and
it will take out all the relevant
properties from that but it will also
take out all the free text all the terms
inside that document so we get all the
properties and all the terms and then
we're going to continue in the
collection process and enrich it we do
the same for pdfs so just remember
apache tika is very nice technology ok I
want to thank everybody</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>