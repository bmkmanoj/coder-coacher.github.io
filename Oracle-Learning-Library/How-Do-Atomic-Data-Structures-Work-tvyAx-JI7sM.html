<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How Do Atomic Data Structures Work? | Coder Coacher - Coaching Coders</title><meta content="How Do Atomic Data Structures Work? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How Do Atomic Data Structures Work?</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tvyAx-JI7sM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Tobias I work at a company
called neo technology what we do is we
build a database management system kind
of like the company that sponsors this
event
they're called Oracle they build some
kind of database been doing that for a
while I don't know if you heard about it
it's not very popular our database is
much better
it's completely different
no but it's great fun working there so
the thing with our database is that it's
it's not a relational database it stores
data as nodes and relationships sukhino
to call this a graph database if you
want to know more about that I have a
talk on Wednesday on various so-called
no squeal databases or no sequel don't
know how do you pronounce that I like
pronouncing its squeal just because it
sounds fun
I think I think rc-- CEO is make its
doing a talk on on our database tip in
particular at this conference as well I
don't know when that is because he
doesn't talk to me anymore I'm just
kidding all right so I'm gonna be when I
submitted this talk I submitted with the
title how do atomic data structures work
then I when I started making the
presentation I I thought what the hell
was I thinking atomic data structures
are simple because all an atomic data
structure really is is a data structure
where you have where all the operations
are atomic and achieving atomicity is
simple see see you can do that with
synchronization that's not interesting
what I really wanted to talk about is
how do non-blocking data structures work
because synchronization as we all know
implies blocking so actually I think I
have a slide on what I'm talking about
now I'll have a slider like next
so I'm gonna be talking about how to do
none blocking data structures workers
that's more interesting so how do we
achieve atomicity on our data structures
without walking and just as with with my
coding at neo technology I don't
remember all these things like how this
works I just remember how to do it and
I'm gonna do the same thing with this
presentation I remember how to give a
presentation
I remember how these things works I'm
essentially gonna do as I do when I work
I'm gonna wing it so I've made up a few
algorithms and I think they work the
thing with with presentations at a
conference like this is that it's not
the actual content that's relevant
because you're not going to learn
anything at the conference what you're
gonna what you get from a conference is
you're gonna get a taste of an idea and
then you're gonna go back home or to
work you're gonna look at it this was an
interesting idea
I'm gonna read up on this later so the
actual learning happens later so I don't
think it's too bad that I'm standing
here you just don't really know I do
know what I'm doing I but but
essentially winging the algorithms and I
try keeping them really simple more
simple than than what you mmm
when you office see from from an actual
algorithm from my actual data structure
that that you might use and for example
the ones that are in in the Java
collections library let's look at what
we're gonna go through here so I'm
talking now that's essentially this
introduction and then we're going to go
through three examples of non-blocking
data structures that's gonna be the main
part of this talk and then I'm gonna
just end it say a few things so first of
all we're gonna start with a
non-blocking queue and we're gonna do
another number kinky so we're actually
gonna do actually this is this is we're
gonna do two non-blocking cues here and
gonna ball
- this one so the other max disruptor is
a cool thing from a company called L max
the London multiple asset exchange and
they made a big splash like last year or
year and a half ago something like that
with this disruptive technology which
was some a library they wrote in house
that they released as open source
essentially as as a marketing thing I
think to say look at what cool stuff we
do at this company you should come work
for us and I ain't sure what school I
didn't go work for the locust I already
have a cool job and but some of you
might have gone there I don't know for
some people at least did so that's
actually a bit more than a than a
non-locking q there's more to the
disruptor than than just the Q part but
essentially what it's meant to do is
replace queues with a particular
workflow so it's a completely different
API than than what you normally see from
the java.util Q interface but
essentially you use it in the places
where you would use a Q but you it does
more than the eschewing but we're gonna
focus on the Q part because I'm going to
keep this simple and also as I said
earlier presentations are free for
getting you interested in things and
thinking what if I'm going to read read
up more on this and so now you know
about this thing you can read more about
it or go to one of the persons I don't
know if they were doing presentations
this year they did two presentations on
this last year talk to the guys as well
they're super cool and friendly and then
finally a lock free hash map where I'm
gonna take a really simple approach
you've never seen a hash map this simple
because I'm gonna like skip a few things
that are hard with hash maps but it's
gonna be
completely non-locking and it's gonna
perform quite well mmm
at least then the the when I've tried it
so we get back to to why I named this
presentation wrong
what is atomic mean as it comes from
from the Greek word that spelled like
that eighth Thomas which means
undividable so when we talk about atomic
data structures means that or atomic
operations on data structures it means
that the entire operation either
completely happens or none of it happens
it's like acid transactions in databases
the a and acid is of course right
obesity either the entire transaction
happens you can see it in its entirety
or dn't or you can't see any any of it
it does it didn't happen at all
so an operation is a series of
operations dieter occur all occur I'm
Tom this is I didn't write this slide
yesterday so I don't remember it yeah
it's an operation Oh an operation or
series operations that either all occur
or not on occur right
it's just as I said and this
specifically applies to the visibility
of states so either so when you only
have a multi-threaded environment which
is where this makes sense or is
interesting at all when one thread
updates performs a an atomic operation
on the data structure other threads when
they view the state of that data
structure they either see it as if all
of your changes in your atomic
operations happen or none of it did
and as I said earlier
atomicity can be achieved through
locking synchronization but that's not
very interesting so we're gonna focus on
how do we do this non walking Li right
and some level of atomicity is essential
for for multi-threaded programming so if
you do it through through
synchronization or if you do it through
a means of non blocking actions having
atomic operations is essential for for
having great programs this so what does
it lock free data structure then or a
non blocking data structure am i talking
into the mic or rather than mic this is
a data structure doesn't block any
threads when performing operations so
even if I write to the data structure or
update the data structures other threads
can still read from it so that's the
rules out synchronizing the entire data
structure so as I said a synchronous
data search is still atomic but we're
looking at lock free ones and perfectly
they don't even spin way to spin waiting
is is when you read the state and you
see hmm this isn't really what I
expected I'm gonna wait and retry in a
while so that's not block free that's
non blocking you're spinning is also a
kind of blocking the good thing about
this is the improves CPU utilization by
making sure that a CPU is never waiting
for something to happen
so if you have multiple threads running
on multiple CPUs when you read from a
data structure is never gonna block and
wait for data to be available can return
immediately say it was not available or
even if data is available it's not gonna
wait for some other thread that's
currently also reading it or
adding more data and also the non no
spin waiting also imperious even
utilisation because we don't hike our
CPUs do you sit sit idle and just wait
for things to happen and I realized that
CPU cycles are much cheaper now than
they were 30 years ago but still it's
it's something it's something if we want
our programs to perform well we should
still consider making sure that we make
the most out of out of the hardware
available so I start with the first
example when I start with the building
blocks that we're not going to use this
synchronize regions we all know them and
they have two responsibilities first of
all they created a sterilised path to
the code saying that only one thread can
be here at any given time
no never two or more could I'm not no
threads in there but at most one but
they also which is more interesting
guarantee safe publication this is the
key so what this safe publication safe
location is that is the notion of making
sure that the data you write is
consistently visible to all other
threads and how this synchronization
ensure this so everything written before
the end of a synchronized block by some
thread is visible to another thread upon
entry or the synchronized block so when
you have a synchronized list for example
you modify the content of it and the the
and read the content on Donna Reed so
when you enter a synchronized region you
are ensuring that everything that was
written last time
someone was in that syncronize region
you're gonna be able to see that even
the things that happen that was written
before that synchronized region are
gonna be visible to you so it's not just
the things that happen within that
synchronized region it's the things that
happen all the way to up until the end
of that synchronized region there's
still so and that's important because
that means that you can create an object
outside of the synchronous region they
just added to the collection inside the
synchronous region otherwise that
wouldn't be possible
the other interesting thing is that this
actually works on on all the
synchronization the the visibility of
data in post by synchronization is
actually global but it's hard to reason
about it globally but that's important
for having multiple synchronous regions
on the same monitor so if you have two
synchronized methods in in a data
structure one add and one remove for
example if you have a queue they are two
different methods to different singers
region that's synchronized on the same
monitor so you can know that you only
have one thread in either of those
synchronous regions because you only
have one thread holding that monitor at
at once so you know that the data that
was made visible by the ad is visible to
you in the remove
so we want something that gives us this
kind of data visibility oka stats cubes
we know the data visibility is important
for for data structures to work or for
atomicity but we don't want that the
blocking part we don't want the
serialization that let's synchronize
imposes um and the key native thing in
in the java model that gives you this is
the volatile keyword which applies to
fields the volatile fields are give you
the same kind of safe publication that
that's synchronized to us but without
the without the synchronization without
the serialized region so volatile fields
guarantee publication of reference and
the post what's called memory barriers
which is the same thing that the
serialize the the synchronized regions
do so there are two kind of barriers of
the there's a read barrier and the write
barrier and the read barrier is what
happens on entry to a synchronous region
where you make sure that everything I
read after here is gonna be okay and
write barrier is what happens at the end
of a synchronize region that says that
everything that's written up until here
it's gonna be okay when someone hits a
read bury but volatile references our
core is a single field only so in order
to but still these memory barriers are
global but it's hard to reason about
them globally so it's possible to write
stuff to other variables outside of
synchronization and that's not part of
all at file the volatile reference and
have them be safely read after sync
nice block where after reading a
volatile but it's freakin hard to reason
about that it's much easier if if
everything is contained within within a
single reference or as with synchronized
if all updates are done within within
the synchronized block and all reads
within the synchronous block as well I
don't know about you but when I learned
Java I was lied to about this people
told me no you have to do everything
within the synchronized block but
actually its global but it's just super
hard to do too recent about the memory
barriers on the global scope so what is
this memory of everything memory
barriers are a way of establishing this
happens before relationships as I said
we have two threads here thread 1 and
thread 2 they both synchronize on the
same thing let's say that this is the
volatile so thread 1 does stuff here
right state to something then publishes
a reference through through a volatile
field or through synchronization then
later thread 2 reads that value so it's
readable volatile field how do we know
it read that value well it's the same
value of UF to have a reference it could
have I mean it's perfectly possible that
this happened before here and then we
wouldn't know anything about the
relationships we've seen here but my
rambling now when we when we know that
we get the very same object we know that
we have a happens before relationship
when we talk about a volatile references
so that's what we can say that if I
publish something through of all of our
reference if I if all of my state that I
want to make visible to that thread one
want to make visible to visible to other
threads all of that state is reachable
from the reference that I put into the
volatile field I will know when I get
that object through the volatile field
that it's safe to read it because all
right
that happened before I wrote the object
to the reference so our gonna be all
rights I did before I wrote the volatile
is gonna be visible I know they're
visible after after I read it
these are called barriers because of
that reason because no right or these
are called barriers because of how this
works under the hood and that the
compiler and the CPU are not allowed to
reorder any right instructions prior to
the right barrier and it's not allowed
to reorder any read extraction
instructions between to move them before
the rate of the barrier so they're a
bunch of instructions going on thread
one and the bunch of instructions going
on in there too some of the ones in here
update memory some of the ones in here
read from memory the ones that update
memory here are not allowed to cross
this barrier when CPU or the compiler
does instruction rescheduling and the
ones that read are not allowed to cross
this barrier here sim similarly this is
essentially how it works then the
building block that we need in order to
make things work with volatile fields is
atomic updates
so this is our low-level building block
builds on top of all the tiles fields
you botha field is the only thing in
java on which you can do atomic
operations so almost true
let's assume that that's true and it has
the ability to conditionally set the
value in a volatile field and this is of
course what allows you to atomically
change the value so the the most the
easiest most convenient way of getting
access to these atomic updates is
through the java.util concurrent atomic
atomic reference it's so atomic that it
contains the word atomic twice and the
main of the most fundamental fundamental
operation here is to compare insect
operation commonly referred to as lazily
referred to as Cass or CAS no no one
says CAS Cass you guys a callous or
comparing set or you write CAS and say
compare and set but on this we can build
the get and set operation which is kind
of handy which is essentially a swap
operation I have this value that I want
to publish and as I do that I want to
get the previous value I got this
glasses
this coaster is my volatile field I've
got a value on it
and I do an atomic operation where I
swap the value that's how getting set
works the that builds on compare and set
which is um-hmm excuse me trap I love
using physical things for metaphors but
I guess it breaks down a room like this
so comparing so getting set I got a
value on it this is my volatile field it
has a value I want to change the value
so I set this and I got this back that's
how getting set works compare and set is
I have a reference to the value that's
already here and I got a new value that
I want to set and I do a check here do a
comparison if it was the same I don't
get a reference to because I already
have a reference if it was the same the
comparing set operation succeeds however
if someone else changed the value before
me I'm gonna do comparison and it's it's
gonna it's not gonna match so so I see
the operation fails and I still have
both of these references I don't have
the new reference but I know that it
isn't the same as the one I expected it
to be that makes sense
or
so it's re for not concluding that as a
video I should have had that as a video
and obviously getting Seth can be
implemented on top of comparing sets by
just as I said compares that you have a
reference to the value and if the
comparing set succeeds I can just return
the value that I have a reference to
because I know that that was the
previous value okay so that's
essentially what atomic reference gives
you then there's something that I think
is super cool and I think everyone
should use instead of atomic reference
and that's atomic reference field
updater which is a complete mouthful and
when you say atomic reference field
updater the the code that you're right
is even more verbose than that because
you you you write this word twice in
your code both in the in the field
declaration and in the invocation to the
constructor and has two generic
parameters so if you're not comfortable
with generics maybe maybe this is
something you should just okay and no
it's about like this and then once it
works you don't touch it anymore
it gives you the same functionality this
atomic reference but with the difference
that it works on a volatile field that
you declared in your class so you
declare a volatile field and an atomic
reference field updater
for that field the benefit of this is
that it gives you lower memory overhead
which is why I use it because working
with a database I'm concerned about
memory a lot building a database and so
I'd care about having as few object
headers as possible so I'm really in
love with thumb of reference we all have
data for that reason but also which is
even more important for for most of you
is gonna be that it has a better memory
locality because with atomic reference
don't reference you have your object and
it references the atomic reference and
that reference is the actual value with
atomic reference field of data you have
your object and that references the
value immediately see so you know that
they're on the same cache line I talked
about a hardware said 30 words some
hardware we're software guys okay um but
so you know this on the same cache line
which means that it's already gonna be
in memory when you've read your object
and perform the atomic operations on
it's on the atomic reference field of
data through the through the atomic
reference field updater which means that
the that the CPU is not gonna have to
stall and read from main memory in order
to perform your operation so you save a
few thousand clock cycles on that which
is cool
okay this that was the background for
the tool set that we need we were gonna
do a an EQ and non-blocking atomic you
there are two approaches they can take
the queues and these are available in
the Java collections library one is a
linked queue and one is again an array
base queue you typically you can't make
an array values queue that isn't bound
and making a bounded so that has a limit
so array base Hugh's always has limits
it's hard to make one that doesn't have
a limit link queues are hard to make
with limits because essentially making a
bounded linked queue means that you have
to to maintain the size in order to say
I know this is full now the benefit of a
link queue is of course that it it never
blocks at all because there's it can
never be full the benefit of an array
based bounded queue is or an array based
queue is that it does block because you
actually want that in some situations
when you have multiple threads doing
work you want some way of limiting the
amount of work and a queue it's actually
quite a good thing abandoned queue for
limiting the amount of work that you let
your computer do it's a good thing
because it do you do have finite
resources you only have this machine
only has four cores for example the
machines I typically deploy on us 48 of
course so there's a limit set of threads
that I can run so I don't want too many
threads doing works I have bounded
queues for for limiting that
okay I like relating things to real
worth this is that's what these things
are about for understanding how a cue
works
think of a cue in a in a supermarket
with a link you you remember who is in
front of you so I stand in queue here
and there's some guy in front of me and
all I have to do is remember to stand
behind that guy and I'm gonna be fine
with an array based cue I just remember
where I stand as I stand in the same
place all the time and then eventually
the the bank the the cashier will get to
me instead of instead of me consistent
continuing to walk behind the same guy
all the time as I didn't the link you
for making non-blocking implementations
of a linked queue there are two
operations that we do that we need its
NQ and DQ so adding things and removing
things it's actually surprisingly hard
to get this algorithm right and
traditionally so if you build up this
with two two pointers one that points to
the beginning of the list and one that
points to the end I can never remember
which one is which do I add things to
the beginning or take things out to the
beginning so I rename them to how to get
where so add is the pointer to where I
add new things and get this where I read
things from much more at least I think
much more convenient than head and tail
so never just it kind of makes sense to
read things from the head but it kind of
makes sense to write things to the head
so which one is which I never remember I
called him Allen games that
few years ago there was a talk by Cliff
click at this conference he's a great
guy
on a non-blocking hashmap that's way
more complex than the one I'm gonna show
and much better this is essentially what
my notes from that talk said now it's
all about reasoning about the states of
of your data structure what state is it
in how do I transition how to transition
to another state and how I make sure
that states are always valid right so in
our non-blocking length queue we have
these two pointers in the actual key
object the get pointer and the add
pointer or the head in the sail
whichever switch and then we have Q
elements and these have a next reference
who's in front of me who's standing in
front of me in the line and also some
payload data that we don't really care
about from the human perspective it's
some object and we have two operations
that we need to do removing and adding
and we need to reason about the state of
this so when I remove an element I'm
gonna do this by first reading the gets
field and I'm gonna get the value out of
that I'm actually going to remove the
value at the same time we're using this
getting set operation so I'm the only
one removing this value that one slide
in duration here so get the value out of
the first thing in the cube then I
compare and set the this elements
yeah the get pointer to the next of the
of this element
take whatever is referenced by this
which is this and repoint this pointer
here to point to that instead and I do
this with a comparing sense so if it
fails if someone else has already
changed this pointer to point to
something else nothing's gonna happen
here nice is to value equal to null
the value that I got here is it null
what does no mean here no mean what does
not know value mean no value means that
someone else already read this the value
out of this element so someone else
while I when I got in here and when I
read the value popped the value out of
this someone else had already read the
same object out of the gets pointer and
ID popped this value before me but have
not updated the this pointer the get
pointer before I read it right
so I read something that someone else
had already said this is not valid
anymore and since this is an atomic
operation the get then says only one guy
is gonna be able to change the value to
null and get the get the actual no non
null value all other threads are gonna
be sending it to null but they're gonna
be getting no back as well okay so if
it's null I read something that was
stale it was old so I'm going back to
two and retry yeah no at first I get a
new next I get the next from this so I
could continue traversing the linked
list and I go to back here and go to
doesn't exist in Java in Java you would
write a loop it's just easier to write
code to in an enumerated list in an
enumerated list like this so adding an
element then I create a new element that
I want to add and I read the ad pointer
I call that L for last it was the thing
that was last when when I checked it and
then I tried to set the next pointer
this from its previous value which is
the same as this thing we'll see why in
a few minutes to the new element I I
created if this succeeds fine this
lesson succeed
someone else has updated this while
while I was from before someone else
updated the value in here before me so
I'm gonna read a new value out of here
I'm gonna retry sorry go back to two and
and retry and then when it succeeds I'm
gonna update this pointer to point to my
new element and you might see if you're
awake
you notice that okay it's possible that
I can someone created a value
I got a Miss on this cast it failed
which means that someone else updated
next pointer but since we update the
next point before we update the add
pointer it's quite possible that I go
back here and read it very same thing
that doesn't matter because eventually
the guy that managed to update this is
gonna get here and it's gonna update
this so we might retry two or three
times but eventually we're gonna get
there I'm actually here we don't care
about that when we update the add
pointer we don't care about failures
because if failure here means that
someone else had already updated the the
list to get longer and what was someone
had already include elements after the
element that that I inserted then we get
to the initial state which is the witch
which is where we get to why we do
comparison on the next value expecting
it to be itself and updating it to the
new and also why would that there so we
create an element that is that can in
the initial state we have an element
that is the reference is nothing
it's a sentinel object and its next
point of points to itself in the values
and all to track the fact there's no use
to remember from removing the elements
if it's if we get null from here we
can't return any things where you're
gonna so actually if we remove from an
empty list we can see that this is
actually gonna block so it's not super
optimal or it's not gonna be it's gonna
spin no it's not gonna lock any other
thread but it's gonna wait until
something as a real low by just looping
around here and
yep and we point to itself also and we
do the same thing here when you create a
new element we make sure that this next
pointer is itself and we do that to make
sure that this thing always does the
right thing so when we change the gift
pointer here from its current value we
change it to the next value of the
previous one here and if this is the
last element we're gonna change it to
the very same element again so we make
sure that we that we maintain a
consistent state in in the list as I
said it's all about the States that's
pretty much it
very simple fish but there are a few
things to keep right keeping your head
and as I said this is overly simplified
in that it does actually block if you
look at yo the implementation of Java
util concurrent concurrent think queue
it does quite a few clever tricks in
order to not have to retry any of its
comparison operations does this by
marking objects similarly as we marked
the value as null Mars objects as this
has an invalid next pointer and when it
finds one of those obviously in by the
next pointer it's gonna reread the I
think it's head which is the get pointer
here so I remember
but it's gonna reread the get pointer
and finds one of those because that
means that well someone else has traded
a list that is valid by unlinking this
but I read it so I got an inconsistent
state and I do encourage people to
actually go in and read the source code
of the JDK is it super helpful they're a
bunch of cool stuff in there and a bunch
of the things that you wish were cooler
all right so how about a non-locking
boundary I found a key then implemented
by an array so we typically think of an
array as a straight line of boxes but we
can take that line of boxes and sort of
bend it around by by this lever
mathematic thing called modulo which is
super convenient on a computer if you
have a size of the array that is a power
of two because modulo a power of two is
a simple truncation of the value so what
we do with the with an Iranian cue is we
have to these sound pointers anymore
these are R if R integers referencing
positions in this array and all we need
to ensure with space in this is that the
it's called mod and get pointers again
the add pointer is always greater than
or equal to the get pointer if they're
equal the queue is empty and also that
the add pointer is never greater than
the get pointer plus the size of the
area so the difference between them
should always be less than the size of
airy which in this case is 16 which is a
bit power too
and when we add all we do is we update
dad pointer we update the ad calendar
and when we get things we update the get
pointer and the reference here is no
longer valid and the reference here
it's about it and this is essentially
how the how the disruptor works right
with this is in the wrong place
so the disruptor is essentially a one of
those erase or ring buffer stuffs they
call when you treat them as a ring with
a read mark and a right mark and writers
what they do is they actually has a few
more things to it
so you get a who has a writer what you
do is you allocate this lot to say I
want to write things to this to this
buffer and you get a position sure
you're welcome to write here's the
position at which you should write your
value and then I do stuff and write that
value in there and I say okay I've
written that's gonna update the
available the mark for for how for which
point is available for reading next yeah
that's so the available pointer is
updated marking right actually has an
available more as well
so we'd write unavailable and the right
mark is always same thing as with the
mounded cue the right mark is never
greater than the read mark plus the size
of the area the available mark is always
between the read and the write mark so
the read mark is never greater than the
available mark and readers read from the
read mark so what they do is they say
they actually they probably another one
here so you get a position when you say
I want to read from this get a position
okay read from here and then you get the
value out there and then you say okay
I'm done reading this position so
there's another mark for that and the
nice thing about this is that the
readers contain on the read mark where
it's the writers contain on the content
of the right mark so there's never any
contention between the reader and the
writer as it is in the linked queue for
example with the link queue we have a
few states where they actually contend
on the same object whereas with the
disrupter the writers are always only
looking at the right mark and the
available mark and the readers are only
ever looking at the read mark and then
of course they've finally tuned this
code by padding the code looks awful if
you look at the source code for this
looks awful but it is amazing but it
looks bad because what they do is they
add a bunch of fields to do their
classes that aren't used ever and they
do this in order to get padding in order
to avoid what's called false sharing and
that's when you have
multiple objects sitting in the same
cache line so in the same position in
the CPU what's bad about having things
in the same cache line is that the cache
line is the level of abstraction of
which the which multiple cores
communicate between with one other so if
you have if you want to sort of these
read and write queries again if you can
read something and there's a read
barrier you have to make sure that what
you read is actually from the most
recent version of that cache line and
when you write to something but the CPU
is gonna do is gonna is that it's gonna
say I now need to own the most recent
version of this cache line so if you can
avoid that kind of if you have objects
that are smaller than a single cache
line they can you can fit multiple
obviously within the same cache line you
get false sharing so even if I write to
one of you can you write to another
you're gonna get we're gonna hit the
same cache line because they happen to
be close to one another in RAM and even
though I read Maya bit and you write
your object your rights are gonna affect
my reads that's called false Jeremy and
they do padding to make sure that that
their objects are always at least the
size of a cache line to to never have
more than wall out between a cache line
but they talked about this effort Rica
this was what I you said finally a final
example an atomic map our non-blocking
hash map and the most important thing
with maps in general is how do I deal
with collisions and this becomes more
even more important with non-blocking
maps so to normal approach to
traditional or popular approaches for
this one is to rehash that's essentially
I when when you write to a position you
find it whoops there's all or try to
write to a position you finally whoops
there's already something here then you
do some operations to find a new
position it's just the same no so you
what you do is you get the hash code for
the for the key you get a position for
that and then you find that it's already
something there with a different key
same hash code with different key that's
how hash curves work and then you find
that then you need to rehash which means
that you find a new position that
corresponds to the same the new hash
code corresponding to the same key
usually the simple way to do this is
just take the current hashcode and
incremented by one the way that this the
reason this works is that when someone
wants to read that they go in to that
position that they get from the normal
hash code of the key and then you
they see that opes the key here doesn't
actually match the one I expected I'm
gonna do the same kind of rehash and
search for the search port the actual
key that I wanted until I find something
that has a different hash at this point
I terminate and say that well it didn't
exist in this map which in the worst
case is scanning through the entire what
the other approach is to on each
position instead of having an actual
value is to have a linked list of of
elements so I hash to something and I I
think my key hash to position if there's
already something there I create a
linked list of values that contain both
of the values with their key of course
both of these suffer from from hash
collisions in that if you have a bad
hash code algorithm you're gonna get
either you're gonna get a linear scan
through the entire hash or through the
entire array for free getting things or
are you gonna get a linked list
containing all of your data instead of
instead of the instead of a good order
one hash algorithm so the presentation I
talked about there was a Java one a few
years ago by dr. Clegg he did a
rehashing algorithm and then a replace
or a rebuild of the entire entire hash
table at when when the when the rehash
became too expensive so Alan because
what's important to know to to think
about when you do this non-blocking lee
is that you know when you remove
something you never get the opportunity
to say well actually now I remove this
I'm gonna stand and see if the if the
values that come after here should have
been in this position instead because
that's too complicated of a state to
deal with you can't do that without
walking Christie can't have the same
thing in multiple locations so so what
what cliff did in his one was then he
started something called tombstone so
when something when he removed from
position instead of removing both the
key and the value inserted a tombstone
instead of value saying that there's no
value here but there is a collision
which means that of course that you can
override it if you write in the about
you but for reading you still have to
traverse through and rehash these
presentations online so if you want more
details on that are really recommend
reading that or his source code that's
on SourceForge there is a concurrent
hash map in the java standard library
and this is good but it's not completely
non-blocking so for for the academic
purpose of this talk mmm not so
interesting but it's a good
implementation use it so we had those
two approaches with for rehashing or
linking and but are there any other ways
well there's something called a hash try
and that's all almost as good as the as
a hash map in terms of getting a one
axis to your elements but not quite so
what it is is a combination between a
hash table and a tree so instead of
having a linked list when I get a
collision or rehashing when I get a
collision boring
is that when I get a commission insert
here and there's already something here
I'm gonna replace it with another array
in that place
so if I just have if I have no
collisions I just have values in this
top level re if I get a collision I
create a sub array and I rehash into
that so I take a different hash code or
a different part of the hash code so I
get a collision on the low-low bits over
the hash code for this position I might
not get the get a coalition on the high
bits for it so I can have a higher order
bits hash code for the position into
this array I get a different location
and we can do this with simple
compare-and-swap operations or compare
and safe operations and this is where I
said earlier that comparin set only
works on kampala type fields and then I
said that was a lie
here's where it was a lie there's
actually the Java library contains
something called in the atomic array
which can do compare and set on a race
there's no atomic array field update or
something like that which could have
been cool but it doesn't exist but
atomic array atomic reference array is
it's pretty good and I am pretty much
out of time
you know how cast works now so you can
probably figure this one out
let's move on to completion head and
tail eradicates are freely updated by
with not by the same thread which means
that they you're gonna get into contain
you can contention on by by ship false
sharing because they're gonna be in the
same cache line Chris they're in the
same object Banach use frequently
perform better because they throttle
your throttle your workload but they
will block when they're full l mister
software is cool its high-performance
check it out and yeah hash Tristar also
cool
Rick hickey of closure fame there's no
is Scala its Scala that frequency uses
hash trice check out the Scala source
code if you're fun carefree about iced
rice or they published papers on it and
that's it I got 10 seconds left
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>