<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kinect Open Source Programming Secrets: Hacking with OpenNI, NITE, and Java | Coder Coacher - Coaching Coders</title><meta content="Kinect Open Source Programming Secrets: Hacking with OpenNI, NITE, and Java - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kinect Open Source Programming Secrets: Hacking with OpenNI, NITE, and Java</b></h2><h5 class="post__date">2013-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_Nj23vTxUVI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Andrew Davison I'm gonna
be talking about the Kinect how to
program it using two libraries open ni
and night
using their java binding so before I
begin how many of you have got to
connect and how many of you have
programmed with the Kinect Oh anyway why
we you should all be programming it's
very lots of fun all right so I'm gonna
start by just sort of you know
explaining the general hardware that
connects and now I'm gonna be looking at
the two libraries open Ni and night open
ni is the basic library which gives you
access to things like the maps which is
the open ni name for the cameras also if
open and I you can access skeleton
information about users who are standing
in front of the Kinect
so you can access the joints will see an
example then I'll move on to a second
library night ni te which is on top of
open ni and that gives you extra
functionality in particular it allows
you to track her hand points over time
as you move your hands about and also it
gives you a set of gestures as well so
you can use night detect things like
waves and circular motion of your hands
then I'll finish off with two sort of
examples of the Kinect in sort of
unusual domains one for computer vision
and augmented reality okay I can use
this
all right so here's a sort of the inside
of the connect there's a lot of
interesting stuff here but my talks just
going to be focusing on the two cameras
there's a ordinary RGB camera is this
thing here but the more interesting one
is the infrared camera there's actually
two parts to this there's a source which
sends out in a pattern of infrared spots
over the scene those are reflected back
and detected by the infrared sensor and
depending on how the pattern the pattern
of dots have been reflected transformed
the Kinect can then transform that
information into depth information so it
can calculate how far the user is away
from the Kinect how far the background
is things like that so the infrared
camera is used to calculate the depth
information there's other stuff too like
four microphones a tilt motor
accelerometer I'm not going to talk
about those in this talk okay all right
so here's the sort of basic camera stuff
that you can access
that's the RGB camera as well the camera
that's sort of in the middle of the
Kinect here's a picture of the infrared
camera the data coming back this
infrared information can be used to
calculate the depth now what gets back
in the depth calculation is a series of
millimeter distances and then it's
really up to the programmer to convert
that array of distances into something
graphical and what I've done here is
I've converted it into a series of gray
scales where white is for objects close
to the Kinect and black is further away
all right
all right I've got after seeing Simon's
talk yesterday and today I decide
decided that risking live demos is a bit
dangerous so I there's two demos which
I'm which I really ought to do live but
the other six demos I'm gonna rely on
videos that I recorded earlier but they
all do work but I just wanna risk the
demo God's wrath by trying all eight
live so I'm going to show some videos
all right okay what am I doing here well
this is my piece here back in the office
and I'm gonna throw up two windows here
nice way I'm doing okay now what's
happening here well I'm getting the
depth information from the Kinect and
I'm converting it into two different
forms I'm converting into a grey scaling
and see that my seat there in my office
and I'm also converting into a histogram
you can't really see the numbers here
but they're millimeter distances along
the bottom so and the histogram records
how many depth points are at a certain
distance so what this chart is saying is
that most of the most of the depth
information is about two meters away
from the camera at the moment and this
of course will change as I move into the
scene all right I'm gonna get up off my
chair now there's my hand all right
I didn't bother getting up I just will
lifted up my hand and now my hand if you
remember white means very close to the
Kinect and that's sort of reflected over
here in the charts as well because
there's now a couple of data points that
ran about seventy centimeters you can
get about to about fifty centimeters to
within range of the Kinect that's about
as close as you can get
all right carry on I'm using a j-3 chart
here oh that's something else I should
mention let's just pause again back a
bit oh yeah
you might notice here there's a sort of
black stuff what's this well it's when
the connector doesn't get back depth
information that the black really means
that there's no in depth data for for
this part of the image and that's
something you have to deal with with the
Kinect it's all built on top of
reflected infrared so if some of the
infrared beams have bounced off to the
left and right then the Kinect doesn't
receive the information and it can't
infer the deaths so you often get images
like this which are very sort of blotchy
yeah because it's not been able to
return all the info get all the depth
information okay so it's a little bit
inaccurate but okay all right now what
am I gonna do I'm using J free chart
here so I'm gonna zoom in on one part of
the chart and then I put my hand up
again you'll see this bit a big peak
appeared around about five hundred
millimeters Oh juice luck but anyway
there's a big peak appeared here so J 3
shots quite nice I know I should have
used JavaFX but I have J free chart on
my machine so I use that so how's this
Kody well the good news is I'm only
gonna be showing you two slides of code
maybe two slides too many but I'm gonna
be relying on these sort of pictures to
explain how things work is actually very
straightforward really the programs are
mostly just a loop what you do before
you enter the loop
you have to set up something called a
context and generators now there's a
whole load of these generators image
generator which captures the RGB camera
infrared generate which returns infrared
information and most useful a depth
generator that's what I'm using to
create the grayscale on the chart so I
use my generators I'm gonna use a depth
January to set them up here and then I
go into a loop which basically just does
three things I wait for the context to
be updated which means that the
generator returns some information I
then change that information the map
into something else in my case I'm
converting it into a grayscale and the
chart remember what is returned is
basically a two-dimensional array of
numbers debts milimeter debts and I have
to convert that into something visual
the grayscale image so I convert it and
then I just paint it into a J panel then
I go around the loop and wait for the
context to be updated again this happens
every 40 milliseconds or so alright so
here's my first slide of code this is
the initialization stuff again maybe
this is too much detail but it what it's
showing is setting up the context then I
create the generate in this case a depth
generator I have to initialize it in
various ways in particular specify the
size of the image that I'm capturing and
the framerate then I start the context
off it will now start obtaining
information from the generators in this
case there's only one generates but
there could be more here as many as you
like we'll see more complicated examples
of it later on this is loop so the last
slide was the initialization this is the
loop that's going around there's
essentially just three steps here inside
the try block we wait for the context to
be updated which will be
when the depth generator returns
information then this is my function
which I'm not going to explain which
converts the array of depths into a
grayscale and the chart stuff and then I
update things I should say you know I'm
gonna be skipping over the details here
but if you want more details you can
have a look at my book or you can go to
my website and I'll give details about
that at the end all right when Lea loop
when the loop eventually ends for
whatever reason you have to close down
the context switched off and there's a
lot of programs just like this one all
right this shows some of the generators
that you get in open ni remember but
open ni is the library I'm using on top
of the connect and what I've just been
using is a depth generator and there's
some other basic ones like image
generator infrared generator later on
I'll be using some of these more complex
generators which do higher-level
information-gathering alright let's do
another example transforming the user
now what am I gonna do here well I'm
gonna have the Kinect take a picture off
of the singing and the scenes going to
be split into two parts the user and the
background the background part of the
image is going to be thrown away and
replaced by a static static image I
wasn't out in a forest when I you know
when I did this demo is in my office but
the background of the pitch has been
thrown away my office has gone bye-bye
and been replaced by a nice beautiful
forest secondly the user part of the
image is transformed in different ways
and I actually got a whole series of
these transformations I'm going to demo
a few but there's basically three sorts
of transformations that I'm going to do
this transformation is a sort of global
transformation where all of the user
is user images changed in the same way
in this case changed into hexagons we'll
see the video in a minute so that's a
sort of global transformation of the
user part of the image the second sort
of transformation I'll be doing is a
time-based one in this case of blurring
and this is a transformation which
varies over time in this case more
blurry less blurry more blurry less
blurry and a third transformation this
isn't me doing a disco dance I'm just
standing still but the transformation is
using my center of mass and the
transformation of the image depends on
the distance of the pixels the the user
image the distance of the pixels from
the center of mass and I'm sort of
twisting the image but it's twisting
more or less depending on the distance
from the center of mass
all right examples okay so I'm back to
my office again I've started up the
application we go now that's just so I'm
not airing the voice time in my office
standing around I'm doing a global
transformation this is a MA marbling
effect so notice that I move about you
know the the marbling carries on a Bing
on me but
all right so that's me moving about now
what I'm gonna do is select now I should
just mention something here this menu
has nothing to do with the Kinect it's
just so that I don't have to keep
walking over over to the keyboard so
this this menu allows me to change the
particular transformation without having
to leave the Kinect field of view all
right so let's see what what change I'm
gonna make there we go that's a blocky a
pet again it's a global change to the
whole of the user image in a block
effect now I turn into man of chrome so
metal man now though there's something
interesting you might notice here
where's my head gone you notice my
head's disappeared off the top here what
I'm using here is the depth information
and if you stand too close to the Kinect
it won't see the top of your head so
really I ought to be standing further
back so that just means I'm too close
yeah so it's cutting off the top of my
head all right man of chrome so all
those have been constant a global change
transformation now here's a time-based
one where I beam in and out fading in
fading out so it's a it's a time-based
transformation there that's enough for
that change crunching come on hurry up
so down at bottom there 12 right I know
I'm just go dancing here and I'm not
actually doing that I'm just like this
and it's doing the twirling for me yes
now this is what happens when you have
too many pizzas oh gosh yes now what's
happening or something here
it's a lensing effect so based around
the center of mass it's making the
pixels big
round the center of mass through my
stomach and less big round the side all
right how's it all work
well it's a bit more complicated I'm
using three generators here I have to
use the image generator to get the image
I have to use the depth generator mainly
in order to do some transformation of
depth coordinates to image corners don't
want to get into that and the
interesting one is the user generator
which returns something called a scene
map and the scene map it basically is a
series of integers 0 representing the
background of the scene and 1 for a user
and if there's more than one user you'll
have integer 2 for the second user
integer 3 for the third user and what I
can do is I can use the scene map to
very easily subtract the background
because everywhere where there's a zero
over on the image I can just erase the
image pixel so leaving just me with no
background what I'll do subsequently is
replace this this empty background with
that picture of the forest now what
about the user well now I do the filters
and the thing is I'm not actually doing
any coding here all the filters I've
used I actually got from the JH lab site
they're just standard very nice standard
filter Java 2d filter ops which I've
just used and I apply to the user image
turning me to metal making me twirl
around yeah and then what I do is I
recombine the modified user image with
the with the forest background at the
end all right what's the program look
like I'm not going to show you but it's
a loop again so I initialize this time 3
context depth image and
and user generators then I go into a
loop just like before the only
difference is of course it's a little
bit more complicated I have to do more
changes to the image and I have to
combine two drawings when I repaint have
to draw the background image and the
modified user image but it's really just
the loop going around all right now
let's move on I'm still on open it in I
what I've said so far is about how open
and I can give you access to the camera
image infrared depth and things like
user generator now I'm going to talk
about skeletons and very nice this open
and I can return information about the
joints of a user at currently it returns
information out 15 joints things like
the head and neck joint the information
it returns is the position in the in the
scene XYZ and also rotational
information as well so you can get that
for each joint all right you can use
this for all sorts of nice stuff all
right
what I gonna do what I'm gonna do is
basically modify my previous example but
instead of changing me into mr. twirly
what I'm gonna do is I'm gonna find my
head joint and neck joint and replace me
by son who's much uglier than I am er
every talk about the connect that I've
ever seen always mentions Minority
Report so I've got it back in here yes
so anyway and let's do the demo is sort
of fun
yeah all right back to my application so
I started up now there's an interesting
that all thing I have to do at the
beginning here once it starts up like
alright this is loading up all the
various images right you'll notice I um
my head is not my head's the same yeah
well when you when you're using the
skeleton information in open and I up
until recently you have to calibrate the
skeleton first which means that you have
to initialize it and you initialize it
by assuming a standard positioner so I
surrender positioner or Sipos you have
to do this so that the open ni can
detect your skeleton initialize it and
then after that it will follow your
joints around as you move so in a moment
I'm gonna assume the Sai position all
right there I go all right now now it's
tracking my joints and now I can change
my head to something cruet improves
there we go there's something evil being
detected some evil presence we should
arrest him immediately yes all right Tom
that's enough of that
all right all right I've got that change
please all right I want Shai choose now
probably more accurate for me as mr.
bean
do you know as it follows me around okay
I don't want to be mr. bean anymore
I'll be Barney's no no it should have
been Skynet not Microsoft he's blown up
the wrong place all right now this is a
bit like a blue screen but it's it's a
feature not a bug it's a feature it as
one words I have to explain in that word
is developers developers developers so
hours of fun so how is it work then what
the applications getting more complex
I'm still using the depth image and user
generators but the user generator I'm
using in a more complicated way I'm
using two capabilities pose capability
is I need that in order to detect my I
give up position thus I pose and the
scouting capability is used to to build
the joint information head neck and so
on what are these are yellow circles
well the way open ni works is that you
have to attach listeners to the various
generators and capabilities these
listeners are woken up when various
events occur such as when the
calibration finishes or when the
skeleton moves and then you can have
Java code in these listeners which are
then which is executed so there's an
awful lot of listener code everywhere in
the more complicated applications you
still have the loop going around but you
also have all this extra listener code
attached to the skeleton capability to
pose capability and so on okay so
listeners lots lots of them
maybe what you didn't notice that maybe
is that when I was moving around the
head was moving as well in fact it can
rotate it rotates only in the around the
z-axis and sort of like this and the way
I did that was I used two joints did a
bit of you know simple trig and
calculated the angle based on the
rotation between the head and the neck
joints I could have used the open ni
rotational information but it's a bit
tricky it's all sort of matrices and
stuff and so for something simple for
simple rotations it's easier just to
look at the joint positions and do the
math yourself working out the rotations
the angles alright let's move on to
something a bit more tricky what I was
using in that last example was actually
just two joints the head and the neck
joints well now I'm gonna use every
joint now what's this well of course
there's only me here but the Kinect is
actually capable of detecting up to four
people and this screenshot was taken
when there were two of us me and my the
other guy in my office stood in front of
the Kinect each user is going to be
converted into a 3d model in this
artificial world and the joints of these
models move following the joints of the
real user now I think this example I
need to really sort of demo it for real
so let's cross your fingers and try
demoing this one for real
okay now just as with with the user have
to assume the psy position but so it can
calibrate me I'm over here hello ah
there we go
boy we are sailing and I can turn around
a bit and things now this 3d world has
got a camera which I can rotate so I'll
just move over here there we go
so the camera it's a bit hard to
visualize but the cameras moved over to
the right hand side now
now where have I gone well as soon as I
move outside of the root of the connects
field of view it loses my joint
information because I'm not there
anymore so you have to decide what your
applications gonna do and joints
disappear and what I decided to do is
simply make my my 3d model disappear so
let's move back into the scene should be
seeing me do this oh there we go so the
camera is sort of over here now yeah so
it's so there we go so it was up to what
worked okay all right that's my first
live demo I got one more live demo that
I'm gonna do all right how's it work
well the code is actually pretty much
the same as for changing the head so I
have to use skeleton impose capability
pose capability you know to initialize
calibration and these yellow things
remember the listeners and the most
important listener here is a listener
which is fired when calibration
completes that's when the skeleton is
initialized and what I do here is I then
create my skeleton object in the three
the world and then after that as I move
the joint information is then used to
update the skeleton in the 3d world
alright but as far as programming goes
it's the same as last time it's a whole
load of listeners and the same
generators and capabilities all right
well that's open and I dealt with
so remember open and eye is the sort of
basic level library on top of the Kinect
gives you maps user generators skeletons
now I'm going to move to night which is
another library on top of opening eye
that gives you extra stuff hand points
now you might think well why do I need
hand points because you've already got
that with the skeletons well it it gives
you the information in it in a easier
fashion less coding required so hand
points and it also gives you gesture
information like I know that's a that's
a wave that's a push or a click yeah
let's do the examples I've already said
that so skip that how's it all work then
well it's based around detector classes
each detector class detects a different
gesture so for example wave detector is
for detecting waves guess what circular
detectors for detecting circular motion
and they're all built on or they all
extend a basic class point control which
is detecting basic hand position
information so you can use the basic
class to just get hand point so you can
use the subclasses to get different a
detect different gestures right all
right so what am I going to do here well
I've built myself a whole new GUI a
gesture GUI my gesture Juke GUI has
three GUI controls a button a slider and
dile now let's go back to the video come
on
I command you to start oh oh I know what
I know is all right so I'm starting the
program again hmm okay so uh you can see
how happy I am her here Oh God ten hours
trying to get this to work now I'm I'm
aren't using the night library now and
I'm doing gestures and it's the same
sort of thing you have to have an
initialisation gesture in order to get
things going just like you had to
calibrate the skeleton with the gestures
you have to do an initial gesture which
usually is a push so you're going to be
seeing me doing this to try and get the
thing to start working I'm just about
that's why I look so miserable so I do
the push to and start the gesture
recognition all right
all right so I'm gonna start pushing
that's me pushing to get the thing going
yes now I move my hand up to here now
what's happening well when it detects my
hand in the same location as a button
the button is selected which means it
turns yellow if I keep my hand in the
same position in space for about two
seconds the button will be pressed
we shall it'll turn green and also in
the code it returns a an event saying
button pressed you don't see that
because I'm only showing you the window
but anyway so there yeah now I'm moving
to the slider and it's the same sort of
idea when my hand is detected over the
slider it is selected it turns yellow
and the blue tab moves with my hand if I
stop moving my hand for about two
seconds the button it it's select it is
activated and a little yellow circle
appears in the code what's also happens
is that an event is xn tau which we
cause the position of the tab alright so
I'm moving my hand about you know all
right when I stop for two seconds that's
enough stop your hand yes no it'll go
area is selected there now I'm over to
the dial and again it's the same sort of
idea it's selected when it's yellow and
then as I move my hand around the dial
will change I've got this little hand
here but actually the important thing is
the black line if I stop moving my hand
for 2 seconds the dial is activated and
an event is sent containing the position
of the dial now I've coded all of these
myself but I'm not going to show you the
code but they all all the three gooeys
subclass of basic goo
the panel class which deals with this
selection and activation it's a basic
sort of state diagram implemented and
then I subclass it to have these
different gesture GUI controls the
details are on my website or in the book
all right so I move it around stop for a
bit
click so it was selected now I move back
to the slide or a bit all right stop
activated okay now over to the button
stop for a minute
activated all right how's it all work
well we're going to see different coding
now because I'm using the night library
and the night library uses different
generators it's a while since I talked
about the generators but I'm using
higher level generators here hands
generator and just to generate and you
can guess what they're doing the hands
generator returns hand point information
and the gesture generator returns
gesture information like the push and
they wave yeah you pipe these through a
session manager which which directs the
information to different controls and
you implement things using lots of
listeners so for example for the hand
point when a hand point is detected for
the first time the listener is fired
when a hand moves a different listener
is fired when a hand is lost so the
Kinect loses your hand another listener
is fired and you code up different
things using those listeners yeah all
right
all right that's nice finished so that
night gives me hand points and gestures
now let's move over to using the Kinect
for thin sort of slightly different
application domains I'm gonna look at
computer vision augmented reality
alright computer vision well what I'm
going to be doing here is using a really
wonderful computer vision library called
OpenCV in particular I'll be using a
Java binding of OpenCV called a Java CV
and it does everything where does the
Kinect fit into this well it's being
used as an input device you know I can
supply it with I can use the Kinect to
supply image information or infrared
information or in this particular
example depth information I have
students working on different projects
like this such as motion detection using
the Kinect face detection and
recognition in fact this screenshot is
an example of face face detection it's
found my face with a bullet with a
crosshairs over it for shooting me
so that's detection and down the bottom
here you may be you can't see it but it
says recognized Andrew what it does is
it sort of compares the picture against
the database of images and decides which
person this is that's the recognition
part of face recognition
all of this is using the Kinect supply
as the input device all right well I'm
not going to do that I'm going to use
hand recognition yeah so the idea here
is I'm going to put my hand in front of
the Kinect and it's going to detect my
hand and more than that it's even going
to be able to work out which finger is
which now I should say again it's a
little bit approximate so for example
here you can see there's two problems
it's not detected my little finger
that's not what the red circle me
and it's also decided this is a finger
as well down here
so you've got to real you've got to
accept this when you're doing computer
vision type stuff there's a sort of
trade-off between accuracy and speed and
if you want things to be happening in 50
milliseconds you've got to accept a
certain amount of error in the analysis
all right so let's just run the video
all right so I'm sat in front of the
Kinect and I move my hand in front and
it finds all the fingers at first now
I'm moving my hand about notice there's
a few little problems oh actually that's
okay I missed that bit house move on a
bit I have to stop when there's a
problem no that's okay as well you can't
really read the blue it's too blurry but
occasionally for example it will decide
that I've got two thumbs or it will put
the middle finger to the in the wrong
place sometimes it will detect extra
fingers yeah that is not too bad you
know this is this is real speed I'm not
speeding up the video or anything you
know it's the actual speed of the
analysis it's detecting a few extra
fingers there oh it's mitt lost my thumb
weight and now I've moved my hand away
now back again it's deciding my index
fingers my thumb moving around a bit
it's not doing too badly yeah but the
point about this is that there is some
error there all right now how's this
being done well the Kinect part of this
is actually quite minimal what I'm doing
is I'm using the depth map which
remember in the basic approach is just a
series of gray scales white for
something close and then dark gray black
further back so that's what I'm
supplying to OpenCV a Java CV and then
I'm using our whole series of built-in
functions that
cleaning up the image so I fresh hold it
that's a built-in OpenCV operation to
cut away the darker grays and the blacks
then I smooth the image a bit to get rid
of all the blotchiness remember Open
Kinect Kinect often we open ni often
returns an image which has black blotchy
bits around the edge of the image
smoothing cleans it up a bit then I find
the biggest contour the biggest shape
all of these operations are open CV not
not connect not open ni then I switch
over to finding the convex hull polygon
around the outside of my hand and then I
use find the defects in the convex hull
thee the idea here is that the sort of
pointy bits will be where my fingers are
so this is where one finger is that's
where another one is yeah and it
actually tends to get it wrong so what
we see is it's decided these are my
finger fingertips that's good but it's
also decided that this is a fingertip
and that's a fingertip and that's a
fingertip so the information it returns
is a little bit wrong so you have to go
through a bit of a cleanup phase and
there's all loaded techniques for
cleaning up the data to try and make it
a bit more accurate which I won't go
into yeah so you end up with the finger
tips and then there's another issue as
well which is of course my hand can be
rotating and so you have to sort of
ignore the rotation information so
that's my thumb and if I rotate my hand
that's still my thumb so you have to
sort of undo the rotation effect and
finally you label the fingers now I
should say
as it stands my codes a little bit on
the hackey side I'm gonna have a student
I did this for this for this talk I'm
gonna have a student working on this
next semester to improve this stage to
make the identify identification of the
things better what am I going to do with
that well the next step which my
students going to do is I'm gonna use it
for doing gestures yeah so if you can
identify thumb and forefinger index
finger then you can do things like pinch
and zoom and wrote and rotation yeah so
even if you only identify two things
it's useful for doing gesture type stuff
and all this is based on just holding my
hand in front of the Kinect okay now
let's move on to my onto the second
application I think I'm gonna I'm gonna
I think I'm now I use the video here
this is augmented reality now what is
that well it's you know it's Google
glasses
it's a scene the real world scene and
somehow it's augmented with computer
information the way it works is it the
library finds a marker in the scene and
then it uses that marker to to add a 3d
model to the image let's just run the
video it's clearer if I run the video so
there's my marker and you'll see that it
as soon as it detects the marker it'll
then draw a model now as I as I move the
marker about the library will calculate
the markers position and rotation that's
then used by my code to position and
rotate a 3d model which is drawn on the
front of the image alright so there's a
few libraries working here the Kinect is
being used to get the camera I'm using a
library nyr toolkit to do the augmented
reality which means finding the marker
extracting the position and rotation I'm
using Java 3d to draw them 3d model
using the position and rotation
information let's move about a bit all
right now you'll see again it's a bit
flaky let's see in a moment come on go
through first let's just go back a bit
do you see that it went wrong a bit
and very quick ah no yeah what happened
there well again these libraries you
know work fast and sometimes they make
mistakes what's happening is that NY our
toolkit for some reason has lost track
of the marker and it's generated the
wrong position and rotation information
it's then pass that on to Java 3d and
Java 3d is drawn the model at the wrong
position so it all depends on the
library being able to track the marker
but most of the time it works ok all
right so I went wrong there
move around and you notice as I as I
step towards me please Andrew Andrew
step towards me thank you as it as he
comes towards me that since the marker
is getting closer to the Kinect the
model gets bigger as well yeah all right
how's this all work well again I'm not
going to show you the code the Kinect is
actually doing very little here it's
just supplying the RGB camera
information
having said that implementing this which
is passing the camera image over to the
nyo toolkit library is actually quite
tricky yes or issues with threading and
things but anyway it works and why our
toolkit finds the marker in the scene it
then uses it calculates the position and
rotation of the marker then that it
passes that to Java 3d which has a model
loaded and then Java 3d modifies
positions rotates the model and draws it
in the scene so it matches up with a
marker another way of looking at this is
it from the users point of view
you have the picture which was generated
by the Kinect and Java 3d is sort of
drawing a model in front of it so it
looks like it's sort of augmenting the
reality by adding the model alright so
and so what do we looked at well what
I've talked about is two libraries so on
top of the connectors open ni that's a
basic library which gives you things
like access to the cameras things like
the RGB image the infrared image the
depth image it also gives you more
high-level things such as scene
information so the background is zeros
and the user is one it also can give you
things like skeleton information joints
where you are in the scene quite useful
then I also up to the higher level
library night which is built on top of
open and I and that gives you things
like hand points so it'll track your
hands as they move around and more more
high level it'll also detect particular
sorts of gestures waves and circles and
we looked at sort of many examples well
I've got one last example to do oh oh no
I haven't this cyclist what have I not
talked about well I've not talked about
lots of things actually the for example
I didn't talk about the accelerometer
which is in the Kinect or the
microphones or the motor actually the
tilt motors a bit of a joke if you if
you look at the tilt motoring sights
these two little plastic gears which are
pretty useless but anyway a tilt motor
and the microphones are quite useful the
the microphones on are on either side of
the Kinect so you can do things like
beamforming which is basically using the
microphones to detect the rough
decision of the user based on the noise
they're making you can also use the
microphones as input to Java libraries
such as speech recognition yeah
also I I didn't talk about games which
is my particular example I did a Kinect
version of breakout I think I've got
yeah I've got time to show that
so anyway this requires me to what am i
doing
get back over there Oh turn on music I
have this is using gestures I have to
know now what's happening is that my
hand is moving that little trolley at
the bottom and they over there
oh yeah the ball ball bounces back up
and so it's based on my detecting my
hand movement not the most exciting game
but anyway shows it working now if you
want more information about this you can
either pay money and please buy my book
which is in the books store very cheap
yes but if you don't want to spend any
money you can go to this URL and draft
chapters and all the code can be found
at this URL if you want to download it
and try it out okay now my last real
example okay one last example I'll see
if I can get this to work
all right now take a little while to
start okay that's using Java FX music
come on I'm over here a minute in a
minute a window will appear window come
on window this is why it's always a good
idea to video everything window I want
the wind up for you right now I have to
assume the pots side position right now
you look you have learned about the
Kinect sensor but now you will pay the
price for not buying my book yes young
developers come on
yes young me though cause you
so I I haven't actually done any
questions are there any questions yeah
well my office is very small so I'm not
quite sure now officially it's three and
a half meters that's the extent but I've
never really tested that certainly the
closest is about 55 centimetres for the
open ni anyway
yeah calibrate as you're moving well
that the next version of open ni is
meant to not require you to assume the
Sai position it's meant to be able to
calibrate the skeleton just when you
walk into the scene but the version of
open and I at the moment requires you to
go like this and it's a standard
position that you have to assume it the
size hard-wired in it's waiting for you
to do this Simon mentioned this the
other day yeah it is well well to be
precise Okin ni is open source but night
is not night is free but it's not open
source and I don't really know why they
include the license because you know
it's it's they it's freely available
it's always the same code that you have
to type in I I don't really know why you
have to do it but you do you have to
include the license anything anymore
yeah
oh yeah yeah all right it was it was as
close as I could get it so in my office
I'm sat here and I'm my hands about
about 60 centimetres away from the
Kinect yeah yeah yeah it's it's doing
the conversion of the infrared to the
depth information
yeah well the zeros and ones that's the
scene generator that uses the depth
information and it's done by open ni so
the Kinect is supplying the depth
information and the rest of its being
done by the open ni libraries yeah yeah
yeah what over library you're using yeah
yeah oh yeah yeah yeah yeah though
that's one of them sort of nasty bits
about using open NR you have to actually
install three different drivers in order
to get there's a there's a there's a
low-level driver for the Kinect then you
have to install open ni and then night
actually that's not true you have to
install open NI first then the driver
then night don't know why but that's how
it is yeah yeah yeah yeah yeah yes yes
yeah and Mac as well
well it's a bit flaky what happens is
that if if if you're lucky it will not
lose you but as you turn you your part
of your skeleton is or is hidden you
know so if I'm stead like if I'm stood
like this it can't see this arm and it
can't see this leg because it's being
obscured by this part of my body and it
can then lose those joints and it's sort
of a bit iffy we if you if you then go
like this it might see the limbs might
see the joints again oh it might have
lost them you have to move quickly so it
sort of depends
oh yeah you can you can do stuff like
that the people who have actually
developed that sort of techniques on top
to improve things I didn't bother doing
that you know they're just demos but you
can
there are various tricks you can use for
sort of dealing with the errors you know
the the approximate information now
there's a few there's a few different
Java libraries for the Kinect but I've
only use this one so I wouldn't like to
comment on the others but the civil yet
there's there's a interesting one
there's a apparently a java binding
which is going to be on top of the
window that the Microsoft SDK I think
it's called jaynette yes if you if you
do a google for java and connect you'll
find all of them oh no no they don't
they don't
that's as something isit of blazed past
when I was talking about it the the two
don't match up so you have to in your
code specify that the two should be
matched up you have to change the
viewpoint of the depth map so it matches
with the image map and they don't
actually match up the depth map turns
out to be smaller than the image map
even when you line them up so they're
not the same size the depth in map is
smaller yeah hard way
you know oh yeah there's there's a
really nice website called connect hacks
calm it has lots of nice screenshots of
things
typing to Google connect hacks your
you'll find lots of on YouTube actually
lots of fun ones great great fun
possibilities I am well my student is
but well it's not so great actually
because you're gonna have you have to
sit like this so it's not it it's
perhaps not the most practical approach
yeah so and the trouble is really well
the results we've got have been very
iffy yeah
the it really depends on OpenCV there's
various techniques you can use yeah but
if the image isn't really you have to
for example you have to shine an
enormous bright light onto your face
so that you can improve the contrast of
the picture it's not that great really
and arson from some you know product and
they the I'm not really want someone to
ask about the commercial expects a thing
I just do fun stuff but there are
certainly a there's certainly a lot of
stuff on github and SourceForge sort of
open source examples but I'm not sure
that you make any money out of it now
prime sense yeah so they're the people
who've actually released the driver for
the and they're the ones who in cut
they're the ones behind open ni
librarian night yeah yeah Asus is the
other device that can use open I didn't
talk about this open ni is meant to be
more general than just for the connect
it's aimed to be used for a range of
different devices which can detect
movement yeah but there's really only
two at the moment the Asus device and
the connect but it might will change
okay so don't forget to buy more e-book
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>