<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oracle Sharding Overview | Coder Coacher - Coaching Coders</title><meta content="Oracle Sharding Overview - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oracle Sharding Overview</b></h2><h5 class="post__date">2017-09-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e7Lbpdl2PE0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is Mukesh Petula
and the seen a principal product manager
of Oracle shouting welcome to step cast
what we're going to talk about today is
the product overview of the brand-new
Oracle 12.2 feature named the Oracle
shouting first off safe harbor statement
what we're going to talk about is the
general outline of the product direction
it should not get into any contact based
on what you heard today right the agenda
is simple but it is a content rich
material we do have a more almost 50
slides that we'll cover in the first 45
minutes and in the last 15 minutes
we'll do Q&amp;amp;A okay the first item we will
be that will introduce you to Oracle
shouting what it is about and what are
the advantages and and the key
capabilities we'll discuss at length on
the concepts and architecture and use
cases as well and we'll summarize
against all right let's begin or go
shopping what is Oracle charting right
it is basically a database architecture
pattern that renders linear scalability
and total fault isolation all right and
in essence it entails horizontal
partitioning of data across n number of
databases we call them charge and each
of these databases have their own
dedicated CPU flash memory and disk and
all the local resources there is no
shared storage there's no clustering
technology that's used yeah in essence
it's a loosely coupled system right and
each of these shards holds a subset of
data and then this subset again is
replicated for our on availability okay
and then in the first release the number
of shards in the Oracle shard database
will be thousand that's the limit that
we have set in the very first release if
you look at the picture on the right
hand side what I'm showing there is a
simple table a table one that is charted
across a number of shards and by doing
so basically I am storing a set of
partitions in each of the shots so
petitions ABCD in The Shard one EFG H in
char to so on and so forth and each of
these databases have their own server
right and then there is no shared
storage there's no clustering as I said
before although the data is distributed
across a number of charts to the
application user to the to the clients
the charting platform makes it appear a
gives it gives an illusion to the end
user as if it's a single logical
database right at the physical level
yesterday tis segregated across multiple
charts but logically it's one large
database one logical database and
because it's a loosely coupled system
they shared nothing architecture pattern
gives a couple of key benefits one is
linear scalability as your workload data
and their and the concurrent users grow
all you would do is add more charts to
the existing pool of databases right and
you'll achieve linear scalability
because there's no dependencies amongst
these shots so this char doesn't never
will never contact another shark so on
and so forth they're all independent
databases and they are loosely coupled
right so you get linear scalability of
data work on users that's number one and
it also offers another interesting high
availability candlesticks we call that
maximum font isolation if I lose one or
more shards only the data in that shard
is unavailable but the rest of the
shards are still available for
application service right that's one and
then when second characteristic is the
mixed database releases I could have
hundreds of shards but some shots could
be in 12.2 some charts could be in 13 1
etc right they know what there's a
requirement that all the shards have to
be in the same version now the first
question that comes to anybody's mind is
what kind of applications can I run
against the shouted environment thank
what we're talking about here is
please from any industry vertical
whether it's a billing system ticketing
system financial social media companies
who have a requirement of massive
scalability and high availability and
are willing to design custom or TP
applications we'll talk about what those
custom old deeper applications are in a
day right but what we're not targeting
there are applications as PeopleSoft s
ap those are not the applications of
targeting so we target in custom all TP
applications whose capitalistic s-- are
basically number one these applications
will primarily do single chart
operations basically the applications
pass a shard in key distribution key
based on the key without you to the
right chart which has the data live into
this particular key and the application
does its transactions on that shard and
then exit out right that's the primary
method of operation right think about
any telco or for the matter I children's
you do you send your account read your
email ID and the charting platform will
take them to the right shard which has
your data and you do your transactions
and you get up I think of that fatter
those are the single shared operations
right we do support two kinds of
patterns that's a single chart the
primary usage patterns single chart we
say 80% of the transactions should be
single shadow operations we also support
an ancillary usage pattern these are
multi shadow operations what if you have
requirements for batch processing such
as simple reports or analytics we
support that also but what we're saying
here in the first release is 20% or less
of what road built along to that
characteristic right
so in summary discussed multiple
applications should be able to specify
the key to get the optimal performance
and 80% of workload will be single
shadow operations and 20 or less will be
be multi shadow operations a couple of
use cases
for Oracle shadi versus traditional
approach when would I consider that
again when my requirement is I have
single charter operations I want to do
but I want to get extreme scalability
for my web scale transaction processing
application Tibbets
that's number one what if I want to have
a fault isolation where in failure of
one shard does not take my entire
application service down what if I have
a requirement for fault isolation then
charging is a great fit for that geo
data distribution is another interesting
one the two characteristics that come
under that data residency and data
proximity it approximately is nothing
but you want to bring data close to your
consumers that is so you'll reduce the
network latency between the data and the
application right and then data
residents is another interesting one
many customers have explicit requirement
query and they want to store the user
data in the country of citizenship it's
a regular requirement that they want to
order right and if you have such a
requirement or two charting is a great
fit and the last one is low cost
Hardware if you have a requirement where
you don't want to use a shared storage
whether it's sad Mel's etc here is an
opportunity where you can establish a
very large-scale database without any
stray storage right here are some
opportunities for Oracle charting versus
traditional approach now charting in
general vendors great benefits we talked
about scalability fall translation but
what is Oracle shouting bring to the
table right so here I have a list of key
capabilities that Oracle shopping gives
you first of all it's a complete
platform for the end-to-end lifecycle up
I shot a database right what do you mean
by that number one as part of shouted
child database platform we provide auto
deployment and that includes the
application configuration so if I have
hundred shards right the creation of
hundred databases and the replication
whether its data call of Golden Gate is
automatically done for you I only will
do is specify the debate and enhanced
aerial of how you want to lay out your
topology and we bill
the systems for you build the databases
and the replication for you we'll talk
about all these details in the next
slides but at a high level let me touch
upon what are the key capabilities the
next thing is the database and policies
they give you pulses with which you can
set your just geo distribution nature
you may want to say I've wanted some
shards on the cloud some shards in my
local data center or some shots in
Europe some shots in American data
center already hybrid as well might be
allow you to create the topology simple
radial specification the next bullet is
the charting schemes we support three at
a high level three different types of
sharing schemes one is consistent hash
based partitioning of data across the
shard
the other one is range or lists in your
list its categories and they usually
find sharding consistent hash what we
call it has a system manage sharing and
we also support composite methods what
if I want to do range consistent hash of
Lists consistent patch we support that
also so in the shorter database you have
n number of sharks let's say for example
hundred now if I want to propagate my
schema to 100 charts if I had to go do
it in every shot it should be a
laborious activity right so what we what
we have done to simplify the schema
maintenance is we've really allow
customers to go to a central shard
catalog from where you will propagate
your DDL so you create tables create
table spaces etc from your shard catalog
and our chart directors in conjunction
with charge catalog make sure that these
details are propagated to all the
hundreds of shots right so char
directors 10 shard catalog they together
make the shard management and shout
routing male ok and the next thing is
data dependent routing for single shot
in crush short queries a platform
provides DDR which is a two pin locking
wherein if you specify a key from the
application side right our our
directives are instrumental in routing
those connections to the right shard
based on the key that you have provided
that's a single shot if you don't
provide the key let's say for batch
processing etc
we also support treasure queries and
that happens through the shard catalog
so you connect to the short catalogue
neuron a bat massive bat sequel without
providing any keys etc and the shard
catalog assumes the role of a
coordinated database and executes the
queries on your behalf and I'll talk
about how it's done the internals off it
in the next few slides right so in
essence maybe if any routing with single
shard as well as cross cut queries the
last thing is very interesting the
elastic scaling an auto rebalancing I
start with 50 shots let's say now I want
to expand my capacity I want to add more
charge right we allow you to elastically
scale your shattered database I'd charge
and then be automatically we balance the
data across all the shot right so these
are a high level key capabilities are
for platform
okay now architecture wise what are the
key components let's talk about it right
on the left-hand side of you see I'm
showing two regions it could be two
sides and I have my clients applications
servers typically run Oracle integrated
clients such as the OCI ODP dotnet JDBC
UCP etc the typical Oracle integrated
finds that we normally use those are
supported as part of the application
server and then we have a set of char
directors there's a nothing but global
service managers if you use DDS the
global data services that was release as
part of 12-1 these are the shard
directors that we have enhanced in
support of Charlotte database so
typically you'll have some char
directors in one region some in the
other for the overall availability and
these shortages are instrumental in
routing as well as overall shard
management as well okay
shard catalog when the simple told our
two database it contains the overall
metadata of the shadow database it
contains the gold schema of the
application and also it performs the
crusher queries on your behalf right it
does those and of course it needs to be
protected for high availability and
disaster recovery so you do have a
standby for this catalog database now
this is the chart management and charge
routing here we talked about right now
let's go deep down and VCD dated here in
this dated here I have in the system I
was shot in the first case that we
talked about I have a sod space which
basically talks about all the shards in
the shadow database right I have one
shard space in system managed database
and in in the charge in this shard space
can be logically segregated as two
different shell groups in this example
shot group one could be all the database
in the primary and shall group two could
be database in the standby and each of
these databases are automatically
configured for data guard with FasTrak
failover right if a daily basis in this
case in red is down automatically
failure will kick in and this database
in Mashhad group too will become the
primary right and we do it at the
database level not at the group level
the failure that I just talked about
okay so now in terms of what are the
technologies that were built shut this
charting platform is not an acquisition
we are not written it from the scratch
it's a pure integration of time-tested
technologies that Oracle always had
partitioning it's been there for almost
couple of decades now like was an
application we leverage these
technologies for building the Charlotte
architecture right so partitioning is
used for distributing your data across
charge and application whether it's
dating or going it is used to replicate
data across the across these shot groups
thank and then GDS is leverage big time
in terms of routing as well as the
Charlotte area this maintenance and
management okay
so automatic deployment and application
complication the first bullet that we
talked about right so what do we do at a
high level the first prereq is of course
you would have a 12 not real database
that hosts the shard catalog it's a
simple Oracle database and if I have 10
charge you would have to install Oracle
software from each of the 10 shards and
of course you'll have to install the
remote scheduler agents on these Oracle
halls as well we'll talk about how the
remote scheduler is used later on and
then in addition to this software on the
shards you will have to have dedicated
environments for char directors where
you install the GSM cyclone and service
managers right so that's your pre that
you create a short catalog
install the software for both Oracle's
or go home as well as the shower
directors
then you will do it declivity of
specification for the configuration of
however you want you will say create
shard catalog and shard directives add
child spaces add shot groups and create
charts you just specify it basically
you're submitting the metadata and you
run a magic command called deploy deploy
is the command which basically does the
end-to-end automation in terms of
setting up these our database right the
first thing that it does is it uses the
DBA scheduler package and communicates
with the remote hosts remote host
scheduler agents and it kicks off the
dbca in creating the shard database in
using our man duplicate to create the
standbys and it also does the local
configuration real transport is also
configured observers have started and
fast-tracked available is also enabled
it does the end to an automation of
deploying the database as well as the
standby and we do the same for going it
in a bidirectional
replication as well ok so my intent here
is not show you the syntax and bore you
with that but but at high level
allow you to appreciate how how simple
it is to create a Saturday phase I do
have viewlets that we can share with you
later on through that we have workspace
first thing this great shot catalog so
you specify what is the database that on
which you want to create a shot Chad can
log you specify what is my child
database administration user is and
what's the password and what are the
regions that I want to create in this
case region 1 region 2 and my shard
space I could take the default or I
could have a name for my shot space
called customer shot space and I specify
the number of chunks and I'll talk about
what chunks are in the few slides down
the line but for now just comprehend
that there is a concept called chunks
and that you would have to specify as
part of the creature camera and then
once
childcare logins created you douche our
director edition say add GSM and you
will give a lesser port for the GSM and
you get a password for the GSM catalog
user basically shard editors talk to the
catalog database over this user there's
some cat user and then you give the
catalog information and also the region
that this shard director is created on
likewise you would create however many
shard Richter's you want okay once the
shard directors are created at this
point you do add shard group and you say
shall group one deploy is primary and it
belongs to region what ever we talked
about in the architectural diagram
there's certain shots and shot up one
certain shots in the shot go - that's
why you're defining here and another
shot group for standby databases in this
case it's they belong to region two once
you define the shard groups all you
would have to do is create shard and you
specify the shard group name and also
the destination the hosts on which you
want to create the database and then you
give the credentials for Oracle that
it's basically the OS user and the
password on these shards nodes right and
you'll establish create shards of how
many shots you want okay well basically
what you're doing now is compiling the
meta data right and then once the
metadata like metadata is compiled you
run the deploy command which does the
enter an automation of creation of
databases your data guard your faster
failover observers and all the good
stuff
right and then once the deploy is done
you can create a global service in this
case I'm calling it as ole DB readwrite
service which will run on all the shards
whose role is
primaries and this is the service that
you would use in the application so in
from the application connectivity
perspective let's say you're using UCP
your url will list out the shard
director lesser endpoints right and also
the service that you want to access
in addition to that you also pass the
key this charging key and which I'll
talk about how you pass the sharding key
as part of UCP in the next few slides
okay so in terms of high availability
the different flavors of topologies that
we support built at this one in the last
few slides basically the first topology
is used in data guard in a plastic
cinema mode right this is the automated
deployment and it's a default mode is
the data guard
we also support Golden Gate at a chunk
level by an addiction application right
if the customer wants to use Golden Gate
yes they can use Golden Gate in in
conjunction with charting and then if a
customer wants to use rack at the shard
level to beef up the node level
available can scalability they can do
Oracle RAC as well okay so we talked
about deployment now let's talk about
the Charlotte schema so one of the
design objective is drawing this goal
that we had is you want to make sure a
given customer a customer's orders
custom line items are always kept
together
right so when a customer comes in and
passes his key lets Americans in
participate all her data her orders her
line items are always kept only within
one shot so at most Mary would have to
go to one and only one shard to access
her records
that's our design objective like
likewise other customers also if the
past a customer ID or or shouting key
they would only go to one chart that's
the 80% of the vault room that we talked
about right but in a schema in addition
to the Charlotte tables you typically
have some tables such as products these
are read mostly or not often modified
right maybe it's a batch processor that
updates once in a while and those kind
of tables products geocodes puzzle coil
etc we we give you a mechanism with
which you can duplicate those tables on
all the shots
right so by doing so your application in
in addition to joining data of customers
orders and line items if they want to
join with products or postal code etc
you would always do a one shard query
you would never do a crush on three you
will not span your craze across multiple
shots when you provide a key right so
how do so what did we do to achieve this
behavior of giving you the opportunity
to put all your related data together in
one shot
what we have done is we have enhanced
sequel in support of that design
objective that I just mentioned the
first thing is the create tablespace set
so we have enhance the create tablespace
to now to automatically create table
spaces across charge so you know say
creating space set remember I talked
about chunks parameter which I showed as
12 in been a great shot catalog so the
create tablespace a set B secretes 12
table spaces across in this case 3 shots
it could be any number but in this
example I'm saying 3 shots so in essence
I have 12 chunks right well table spaces
I had to create across 3 shards
that means each child will have 4 table
spaces and those are automatically
created for you okay this is in the
system and it's showing that it does
once the table space are our automatic
created you would do the HR table for
the route table of your table family in
this case customer is your route table
right and your in this case your charted
in here the partitioning key is the
customer ID so you would specify a
partition by consistent hashing define
your charting scheme and your specifying
on which column you want to shard and
you're also specifying your tablespace
set on what table spaces you want to
create this table on and there is
another clause called partition sort of
what you're telling here is you create
12 partitions and map each partition to
the table spaces that I just created our
Marquis the system manages the mapping
of the partitions and the table spaces
that are spanning across multiple charts
right so I have 12 partitions in this
case and have 12 tablespaces each
partition is mapped to one of these
chain spaces so that's a root table what
about my child tables right similarly we
have a chart a table in this case orders
but if you see I use partition by
referenced what I'm doing here is
actually partitioning I'm inheriting the
same sharding scheme at the orders level
also so essentially I'm creating
customers p1 and/or sp1 that are
literally partition on the same customer
ID column if I have line items I would
do the same as well right so later on
I'll show you how we map customer
Stephen orders table line items p1 to a
logical unit called a chunk right and
that would be there you need to update a
moment chunk would be the unit of data
moment so once the Charlotte tables are
created I would go ahead and do the
create duplicated tables in this case
products which needs to be duplicated on
all the shards ok remember I just talked
about chunk which is nothing but a group
of table spaces of related partitions of
tables in a table family right so in
this example customers p1 or this p1
line it will see one even though this
these partitions could be in the same
table space or different table spaces
but they're all logically are mapped to
what's called as a chunk so if maybe
comes in and all her data is in this
chunk and this basically in this chunk
which has customers p1 orders people not
my mother's p1 ok and then this is the
unit of data moment for returning when
we add charge or remove charge we move
data right we move data and when we move
data basically removing these chunks so
one question that comes to mind is
what's in a shard right
a shard basically contains n number of
chunks in this case I'm showing chunks 1
to 6 right and also in addition to the
chunks you could have one
or duplicated tables I could have
products table geocode table or whatever
that I could have set of duplicate
tables in addition to that I could have
a number of chunks
okay we
talked about schema creation now how do
i propagate the schema right remember I
mentioned I you don't have to go to each
child and create the tables you would go
to the chart catalog and execute your
details there so first you connect to
the catalog we had a we had a specific
service you set your session to have
shard enabled right you enable shot
video and then you will go ahead and
create in space at creating spaces our
create tables the char tables or the
duplicated tables and the shard the
master chart director which is elected
by the system automatically we'll make
sure that these details are propagated
to all the shoutouts whether these are
Charlotte tables or duplicated in the
case of duplicated tables we leverage
read-only materialized used to
synchronize these tables are
periodically
shouting methods now I mentioned that we
do support tea types right
the first one is system managed shouting
which is done we are consistent hash
scheme we're a range of hash values are
are mapped to a chunk in case of user
defined in two types in your list where
in this case a range of short key values
are mapped it shuns and in case of list
it's a range of in the list of short key
values are mapped to a Chuck and of
course we do support the combinations of
range consistent hash let's consider -
I'll sit through these slides
in the interest of time and show you
some examples of how you and create each
of these ok so in the case of systematic
shouting as I mentioned scheme is done
via consistent hash the data is charted
to be shot at automatically the system
does it for you and you get an unified
and you get a even a distributed data
across the in charge and the important
benefit is the automated balanced state
distribution if you want if your
application needs a balanced data
distribution which is done automatically
and then this is what you choose system
manage having is a default for Oracle
Shari in terms of how would I create it
we have seen this example before you'd
say you'd created teams being set in
this case I have 240 chunks and I have
two charts so basically I have 120
chunks in one shard the other 120
another shot right once this tablespace
are created I do my create shadow table
and I say partition my consistent hash
and partitions Auto basically I would
create 240 partitions and map to each
other 240 table spaces that system
managed in the case of in in in this
case basically what we're doing here is
0 2 to the power of 32 is broken down
into n number of chunks in this case
let's say that chunks are defined as
1024
I'm basically creating 1,024 ranges
of data between zero to the power start
to do and a mapping each range to that's
just okay essentially that's what's
happening behind the scenes user-defined
shouting it's primarily used for
regulatory compliance and support of
hybrid clouds etc right it's it's useful
when you when you want to have different
starts at different locations different
size form different applications across
Z if you wanna have some control on how
your data should be placed on a place
then usually fine charting is the way to
go
but it won't be uniformly redistribution
it all depends on which shard is being
used heavily and
to manage it system automatically
creates stable spaces across shards and
perhaps it across right in the case of
usually fine you have to you as a d be
able hard to do this James Chris
creation as well as the mapping of
partitions so essentially when you
create a shot a table you say partition
my list over the state feel alright and
then you create partition P Northwest
article in Washington its map to
tablespace GBS one right we're just here
likewise you do two three and four
okay so explicitly you would have to
define your table spaces as well as
mapping you have full control on how do
you want to do the layout composite is
an interesting one where and if you have
a requirement where and I I won't have
some set of shards for gold flats
customers some set of shots for silver
class customers right and I want to have
that level of segregation but within the
subset then you want the data to be
partitioned by consistent ash if you
have that requirement comfort the
charting is a good way to go
here it is where I have two sets of
shards a goal class customers have two
high-powered servers and silver class I
suppose I have three low powered servers
right and first thing first you do
create tablespace fdbs worm in the shard
space for gold and there's another shelf
space for silver right create your
tablespace set and then you would go
ahead and do it creates on a table but
you list out the super key which in this
case partition set by list you do the
over the class feel first that's a
higher level of sharing and then within
each class you can do consistent hash
over the customer IDs correct so you can
do that composite shouting
effectively for having different set of
shots for different classes of users
alright now let's talk about led pin and
routing we talked about two types right
single char operations and crush our
operations what we have done is we have
enhance our authenticated clients in
support of single shadow operations what
do they mean by that are JDBC you see
POC I or it be not that now can
recognize shard keys as far as a
connection check out you can pass a key
and then
these basically these keys are used in
routing the connections to the right
child which has the data relevant to
that right likewise we also support
crush on queries this is for aggregation
of data simple reporting that's it
alright let's talk about session based
routing a short key the application
client passes the key first it is used
to route the request at a uses a session
level during a connection check out
right what this chart directors would do
is they look up at the routing map info
and understand
oh this key belongs this particular
shark let me read out redirect that
connection to the shard and from then
onwards the application will talk to the
shard directly right
so one optimization we have done as part
of the first connection to a given shard
this connection also retrieves the
ranges of shard keys that are in this
particular chart there is a lookup table
there that that's retrieved and kept in
the application connection pool this
happens that the first connection so or
a period of time when application spawns
connections to each of these the ranges
of keys are cached in the connection
pool so what do words advantage in doing
so right so the biggest advantage being
the connection pools take the
responsibility of shard directors all
right so they can take a key and go
directly to the shard by passing the
shard adductors completely right and
they call this as fast path and it's
very fast and scalable so in indra at
runtime your connection pools are going
directly to the shards completely
bypassing chart director here a couple
of API calls that I want to show just to
highlight some enhancements we have done
as part of UCP and other API that I just
mentioned basically I have a plate shard
in key builder you Billy a key and you
would pass it as part of a great
connection builder as part of a checkout
of connections
so if I don't pass my key what happens
then you would establish a connection to
the catalog database in we call it
corner 2 DB coordinated DB parses the
sequel and sends the sequel to the right
chart where the data exists queries get
executed at the shard level and the data
is brought back and sent back to the
client
same is the case for crush short queries
as well again this is the 20 person
works on that I talked about this is
done only for developer convenience and
it's not for high performance then the
final phase the last lifecycle
management the in lifecycle of a shared
database of course you want to add more
shards to scale your workload data and
users as you add more charts remember I
talked about chunk which is a unit of
data movement they used the mechanism of
chunk to move data to the newly added
charts to rebalance they pair cross all
and we behind-the-scenes we use the arm
an incremental backup and transportable
tablespace mechanism okay so that's the
online edition rebalancing of charge
chunk split and move if you want to
split a chunk into starting from folder
treating 100% online and initiated by
bb-8 this is done for user-defined
shouting we do provide automatic move of
free sharding when your anusha us we
automatically move chunks right and you
also have the flexibility of selectively
moving a specific chunk as well if you
want to limit hotspots right and move is
practically done online and remember I
said the use arm an incremental Tim's
way of backups and transportable
tablespace technology we take an L 0 and
then we when we take the last N 1 we
convert that chunk into read-only yeah
and then we take that back up and then
restore and recover it during that time
you have an opportunity either to
reconnect or access the data in the
read-only mode on that given chunk right
and of course whenever we do the split
move read-only or add charts in
bouchard's
we do have earnest notification lat
collection polls are told about the the
topology changes are marecus so neck net
in summary why do we use on khechari vs.
no sequel in stores that's because it's
best of both worlds it gives your best
of our DBMS capabilities and of course
best of no sequel arses a multi-modal
database we support relational JSON XML
Oracle tax etc we are talking about
standard sequel and programmatic
interfaces that you have been used to
all along PL sequel or CR JDBC and there
are the interfaces that you have been
using and they are they are supported in
your local charting platform you get
developer agility with JSON that we have
started as part of 1200 - and charting
supports JSON as well
we give you asset better consistency
than most no sequel databases strictly
consistent with Mashhad as you've seen
all the related data is always kept
within that shard it's ok it's it's
strongly consistent we our schema is in
the database it's not in the application
and they all the enterprise class
features but that security backups
compression ASM all these time-tested
technologies that you typically get with
amateur articles like Oracle are
supported by the sharding as well and of
course you can leverage all the DBA
skill set in Pelzer worldwide plus the
extreme scalability and fault isolation
okay all right
to conclude basically shouting is a
complete platform for charting and
Oracle relational database
it's ideal for all TP applications that
require the greater levels of
scalability and fault isolation then can
be achieved by scaling up or scaling out
a single database</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>