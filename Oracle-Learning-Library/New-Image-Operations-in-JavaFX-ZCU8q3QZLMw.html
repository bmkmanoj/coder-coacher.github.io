<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Image Operations in JavaFX | Coder Coacher - Coaching Coders</title><meta content="New Image Operations in JavaFX - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Image Operations in JavaFX</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZCU8q3QZLMw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Jim Graham I'm the 2d
graphics architect for Java and Java FX
and today I'm going to tell you about
some new api's we introduced in Java FX
2.2 which went into Java 7 and we think
it's numbers get confusing well sync up
1/8 and it's for accessing the ability
to access pixels and images and a few
other objects in Java epic scenes so
sorry except focus ok so why do we
create pixel access it seems like we
moved from in the Java world we've moved
from dealing with individual primitives
and pixels to dealing with scene graph
and letting us do a lot of the work
behind the scenes for you but that
doesn't always meet everyone's needs for
a variety of reasons sometimes you just
need to do something that we haven't
provided in terms of rendering or
effects and you know how to do the work
but you have no access to the pixels no
ability to give us some pixels or get
pixels from us to modify so this will
meet that need you might want to create
your own image loader or image writers
but right now but prior to this all we
offered was the ability to point us at a
URL or a file to load an image from you
may have some legacy code which involves
custom operations on buffered images or
some work in SWT involving images and
you could need to get those images into
a Java FX application so this will
provide a pathway from which you can
through which you can import such images
and finally we've had hidden unsupported
api's for doing snapshots of
nodes and scene graph trees but along
with this support we created a public
API for creating snapshots of trees and
parts of trees so let's take a quick
look at the API and then look into some
demos and examples basically the the way
you get access to the pixels is you get
a an object called a pixel reader or a
pixel writer and there are several
places you can get those on the basic
image which you can load from a file you
get it you can get a pixel reader which
means you can have read access to the
images you load you we don't have right
access to the images you load instead we
created a special class called writable
image that you create with a width and a
height and you can still get a pixel
reader for reading from it because it's
a subclass of image but you can also get
a pixel writer and write to it so that
that sort of is the destination of any
image operations that you might want to
have finally we introduced at the same
time a an API elsewhere in JavaFX called
the canvas api which has a graphics
context which lets you render into it
and that also lets you get a pixel
writer so you can supply pixels and kind
of a put image operation I'm not going
to go over canvas to today but that'll
be a talk tomorrow but that's another
place where you can get one of these
pixel writers so pixel reader is a
pretty simple API it has there's a few
more methods here because you'll see
that ellipses there because I combined a
couple of methods but basically you can
get an individual pixel either as an
integer for quick access convenience or
as a color if you need an object to work
with the color there are several methods
for getting pixels in bulk
actually there's three methods we're
getting pixels in bulk
for larger operations and if you want to
deal work with the pixels in a
particular format other than this
integer RGB version or other as a color
and finally you can get a pixel format
which describes the underlying pixel
format that we use internally in case
you want to optimize your bulk transfers
to be in the same format the pixel
writer is almost perfectly analogous to
that all the guts are just sets and they
take the corresponding argument there
are several set fixes methods as well
and also get pixel format which
hopefully will agree with the pixel
reader if you're talking to a writable
image and it basically that's you know
which format is optimal for transferring
bulk pixels the pixel format is not
extensible and not doesn't include every
format Under the Sun we focused on
formats we thought we thought would be
most useful for casual to intermediate
use of reading and modifying images so
there's four formats that are full-color
a byte format if you prefer to work with
byte arrays individual components in a
byte and an int format in case you would
rather work with pixels being one single
integer value then for reading we also
provide a ways for you to give us data
opaque data and three byte format or
even indexed data if you have some of
that that you need to provide to us the
pixel format includes it is
premultiplied method which you could
infer from the type that the types that
end in underscore prae are the
premultiplied formats but if you just
want to if you have a pixel format you
just want to know if it's pretty
multiplied you could ask that is
writable is true for the full color
versions meaning that we can use those
as a destination
two stick pixels but any of them can be
used as a source to supply us with
pixels and finally it has a get a RGB
method which can look into a buffer
figure out all the appropriate math to
to find the pixel in the buffer and
return that energy before Matt that's
not meant to actually do work that's
just meant to be the canonical reference
implementation of the way this pixel
format works but typically you would
look at the type of the pixel format and
write your own code to do that kind of
access you wouldn't be using that method
as your primary means to perform the
access but it's there in case you have
an unknown pixel format you need to get
the pixel data out of it the writable
pixel format is it extends pixel format
and inherent almost everything but also
provides the canonical reference set a
RGB method which again is not meant to
be used for work it's just there to
provide a reference implementation of
the way the math is supposed to work
we provide static factories to get these
as opposed to instantiating yourself
this is so that we can grow in the
future and basically have also some of
these Kim can return a singleton
instance some of them need to create a
custom instance on the fly so the
factory will either be a get
such-and-such instance factory if it's a
singleton or a create such-and-such
instance if it needs to provide a custom
one for each one the only ones that need
to provide custom ones are the byte
indexed instances where you give us the
color map and then we return the pixel
format it uses that particular color map
all the other ones are pretty much
Singleton's we can return the pixel
format that specifies that particular
byte
and they pretty much map the enum that I
showed earlier and the four that are
full color richer and writable pixel
formats the others that are not
full-color returning only a readable
pixel format so the various get pixels
met methods that are available in a
pixel reader they all take an X Y width
height rectangle region that specifies
the rectangle of the image you're
getting or in the case of a pixel writer
that you're setting there are there's
the canonic the third one is the the
format for dealing with a java new Ione
i/o buffer and all of the pixel readers
are tagged that they're genera fide so
that they have the appropriate mile
buffer defining their underlying pixel
data format so it'll either be a pixel
format of a byte buffer or a pixel
format of an imp upper and it can read
from that corresponding buffer or it can
read from the corresponding Java
primitive array type that corresponds to
that buffer so you can only use a pixel
format of a byte buffer for the method
that will get pixels into a Java
primitive byte array and you can only
use a pixel format of int buffer for the
method that gets the pixels into a Java
primitive int array and then of course
the type has to agree for the third
format that uses the genera fighty
specifier it's somewhat strongly typed
in that we associate the byte buffer
method with a Java primitive byte array
but again Java uses type erasure so you
could construe to compile a program that
calls the method with the wrong data
types and you'd then get a class
have class cast exception at runtime so
it's strongly typed in in so far as
writing your code if you use generics
the one you notice that the Java
primitive array formats
allow you to specify an offset into the
array where you want the data to start
and the scan stride which is expressed
in terms of the number of primitive
array elements to skip between scan
lines the version that takes an i/o
buffer does not provide an offset
because we assume that you're going to
position the Nile buffer at the location
you want us to start putting data into
there
the other interesting thing to note is
that this is a pixel reader but the get
pixels methods use a writable pixel
format and the reason is because we are
writing to your buffer so it has to be a
format that is writable even though
you're getting we're doing the writing
so we need you to press apply a writable
format the pixel writer on the other
hand you notice it takes non writable
formats because we're reading the pixels
from you that can be in any format
because internally restore them in full
color it's basically analogous to the
get pixels methods you specify an XY
width height rectangle that you want to
write the pixels into there are versions
that are typed by an i/o buffer but take
a Java primitive primitive array there's
the fully genera fide version that takes
the matching nail buffer to the pixel
format type and finally there's a way to
hand us a pixel reader you got from
someplace else and a source location to
use in reference to that pixel reader
and we will extract pixels from a pixel
reader directly into our image again
it's some weight strongly typed within
the constraints of Java generic generics
the Niall buffer version assumes that
you've positioned the buffer prior to
handing it to us so it does not take an
offset
and it provides easy transfer of pixels
from any pixel reader the pixel writer
talks to pixel readers but we didn't
really provide the other half of that
method because this was an easier way to
implement it so that the exact formula
for how we will access your buffer for
any pixel in the region that you
specified the X Y width height region
the corresponding Y is x thus can you
give us then we take the corresponding X
we multiply it by the pixel elements
that are inherent inherent in the format
and for some formats if there's multiple
pixel elements we then assume that the
components are in adjacent elements of
the array starting at that location
you'll notice that the Y is multiplied
by scan but not also by pixel elements
because we assume that you've already
multiplied your your scan stride by the
number of pixel elements that are
necessary this allows you to have a scan
stride that is not a multiple of the
pixel elements in case you want to for
instance have three byte data in lines
that each individual line is word
aligned in order to do that you need to
have your scan stripe not be a multiple
of your pics elements so this formula
does not imply that this gans right has
to be such a multiple you will get an
array index out of bounds exception if
you don't provide enough array space for
what you've specified with your
parameters it's undefined how much of
the data we will access before you get
that out of bounds exception sometimes
we do a balance check ahead of time and
you think there was data you we could
have accessed that we didn't but we
don't define that we access access all
possible data before we throw the
exception we throw it whenever we
discover it so unless your entire
rectangle is in bounds essentially the
operation is not defined how much day
we'll be transferred the number of pixel
elements should be relatively inherent
if you know how to deal with the pixel
format if you don't know how to deal
with the pixel format you probably
shouldn't be seeing these api's but you
know there's one element for any of the
int formats and for the byte indexed
format that's also a one pixel element
by BGR is three pixel elements per pixel
and the byte bjr a full-color byte
version oh shoot I should say for its
said for in an earlier version buddy did
some enemy so I'll have to edit these
slides after the fact but obviously 4
bytes for byte BG are a types this set
pixel is pretty much oh this is the
gettin set pixel of the neo buffer and
that the only difference here is that we
do absolute gets and puts relative to
your position so when when you get back
from this method your buffer position is
not modified but we took it into account
when we did the data accesses but it's
essentially otherwise analogous to the
Java primitive arrays with your buffer
position providing the offset into it
and again the position and limit you
have in the buffer so if you specify a
limit that doesn't allow us to read
enough data you will get the appropriate
exception you would get from an i/o
buffer just a quick reference for how to
unpack the data that I was showing
earlier that the set RGB the set a RGB
and get a RGB methods use an integer
pixel in this format which is 32 bits 4
bytes packed from most significant byte
to least significant byte as if you were
expressing and hex constant the first
two digits would be the Alpha the next
two digits would be the red the next two
digits would be the green and the last
two digits would be the blue placing the
blue and the least significant byte in
the Alpha and the most significant byte
and this is just reference code for how
you would do the shifts and ands and ORS
to unpack and pack the data note that
the the one that packs it up is going to
assume that all the components are
within the range 0 to 255 if they're not
then one component will end up throwing
some extra bits into another component
you'll get a random result we assume
you've done the appropriate data
clamping yourself for that particular
piece of pseudocode finally this this
writable image pixel reader pixel writer
API is paired with the snapshot API that
essentially uses writable image as you
can see here as the way to convey the
snapshot to you so there's there are two
variants on scene and two variants on
node each one of those will have an
immediate version that returns you the
image immediately when the method
returns and also a callback version
which you can which will be done
asynchronously the only other difference
between the scene and the node methods
is that the node doesn't have a lot of
context we get from the scene so you
supply that context in a parameter
called a snapshot parameters object and
it basically contains
the data that would have been in the
scene that allows us like the transform
that came from all of your parents from
the scene down to that node the fill
that you've gotten from the scene the
viewport the camera you would have
gotten from the scene the depth buffer
boolean which tells us whether there's a
depth buffer needed and a viewport which
would have been essentially the data we
got from the stage when you you know
some of this comes from the stage I
guess the viewport would be simply the
size of the stage that the scene would
have been in but the version that uses
scene we for all that from the scene
itself again they're both of the both of
the scene EndNote contain a version that
uses a callback and the callback will
basically get it doesn't your callback
doesn't need to return any data it's a
void callback we provide a snapshot
result argument which contains the image
that we rendered into and also the
source the node or the scene that you
called it on and the snapshot parameters
that you gave us if it was the node
version or null if it was the scene
version and the the source and snapshot
parameters are just there for your if
you like ran several of these and you
need to try to figure out which result
is being returned you can use the source
and snapshot parameters to try to figure
that out although typically you're only
going to have one outstanding at any
given time so why would you use the
callback version give them that you have
to construct a callback and it's a
little bit more of a pain and the answer
is two reasons one is when you do it in
immediate mode we aren't in the right
thread or at the right stage of
processing to immediately render
anything so we have to basically do
a handoff to the other thread we have to
we have to first synchronize a whole
bunch of state to a handoff to the other
thread have it render for us return that
data to us and we block while that's
happening that's a big disruption in the
flow of control of a JavaFX application
to start with and then the second reason
is that some of that synchronizing
operation we had to do in order to
transfer the control may have been
duplicated effort that would have
happened at the end of your pulse so a
pulse will allow you to respond to
events modify the scene graph at some
point we need to transfer over to
rendering there's a process for
synchronizing the data over to the
render thread and then the render thread
can do its work if you if you use the
non callback version we have to sync
right then and then again we'll have to
sync at the end of the pulse so you
duplicate that work in addition to the
fact that you're forcing a thread switch
and the block so there's two reasons why
callback would be better however it may
not matter to most of your usages of it
and so the DVD mode may be more
convenient it may not impact your
performance enough to worry about
especially if you're like writing
documentation you just need screenshots
of your application performance isn't an
issue there if you're doing thumbnail a
thumbnail of one state of your
application once it may not be an issue
if you're doing frequent snapshots for
any reason then you might want to start
using callbacks you know if you notice
that it's impacting your application
then the callback version may be a way
to have less impact so I'm going to get
into some examples here the first
example is going to be custom filters
and I have actually a couple of
different versions of that this one I
sort of wrote to be example code and
it's just a quick it loads two images
and performs an edge detect filter on
them so if I click on it that's the edge
detect
version of the of the image I did an
individual edge detect on the red green
and blue planes separately I could have
done a luminosity and then done a single
edge detect on the noumenon city but
that's just the way I wrote the example
to be simpler so two images and their
edge detected versions the code is
fairly simple loads two images calls a
filter method on them the filter method
returns actually a writable image but
you can just treat it like an image
throw them in an image view and they can
appear on the screen I install a mouse
handler so when I click I just go to the
next image in the array to cycle through
them a couple helper functions one just
accumulates the red green and blue
samples from a pixel reader oh I was
gonna say why am i passing in a a
rectangle I'm not I'm passing in the
next in the Y and I'm telling it the
width and height of the image so it can
do its own bounce checking here cuz I
don't want to access outside the balance
of the image I'm gonna treat those
pixels as if they're transparent or
black an array to accumulate in and a
multiplier that we can apply different
scale factors on different pixels this
combine function is just a way to take
the horizontal edge and the vertical
edge results and combine them into a
single result so the basic operation
will construct a riedel image of the
same dimensions as the source image get
a pixel reader from the source image and
a pixel writer of the destination image
creates some accumulation vectors for
each pixel in the region it zeros out
the accumulation vectors and then uses
this simple convolution kernel to
perform an edge detect you can see that
it it accumulates the current column it
does not accumulate the current column
at all it accumulates the current column
to the right into the left with
opposing opposite sign waiting waiting
factors that means that if there's a
change from the pixel row to your left
to the pixel rather to your right
this will accentuate the change if
they're the same data the results wound
up being zero so that's basically the
way an edge of detection filter works
and you do it once vertically and once
horizontally were again you just turn
the kernel on its side and then you take
those two results that are accumulated
into their accumulated horizontal or
accumulate vertical and I just combined
the results construct a pixel and write
it to the pixel writer of the
destination and return it you can see
doing the doing the simplified writing
each pixel individually and fetching
each pixel as its needed which does six
twelve twelve pixel pixel fetches per
output pixel and one pixel set for a per
pixel and it took a couple tenths of a
second so if that's just something you
have to do it startup to one image
that's not an issue you might want to
use the bulk transfer methods if you're
doing this in any kind of batch setting
or a lot of images the other examples I
want to show you there is Jasper's
actually gonna talk about in one of his
talks somebody had written a whole bunch
of image filters for buffered image and
in a few weeks they converted it over to
using pixel reader pixel writers methods
so this is essentially just an example
of a few of the filters that they
provide being run on in it this is a job
ax JavaFX application and he just wrote
an example that showed off a few of them
so you
that's just driving that package that is
available on the net I believe they had
like 50 different kinds of filters you
could use and then he took one of them
that provided a various kinds of lens
blurs and duplicated something you find
in Photoshop which is a tilt and shift
blur kind of effect and you can change
the parameters on the fly move this
around to change you know what's being
blurred and by how much and you can
change the bloom threshold which as far
as I can tell just sets the grass on
fire but apparently people who know
Photoshop and know what to do with that
to do exciting images the next demo is
creating synthetic chemist imagery and I
have a couple of examples of that the
first is affrighted zoom a I'm going to
go to the custom patterns so you can use
a writable image and in an image view I
have already shown that you can also use
it as part of an image pattern so if you
wanted to create a background on the fly
you could construct your own pixels and
put it in an image pattern and boom it
comes the background of your application
and an example of why you might do that
is that you don't know what color you
want the background to be until you get
running and you load their image and you
want to match their image or you ask
them their favorite color who knows what
you can't have a canned image for every
possible runtime consideration for some
uses so this allows you to construct an
image on at run time of use as a
background this one's a fairly simple
one I just create this kind of metal
corrugated metal pattern and then I
cycle through the colors that I'm
not seeing a color shift from here but
I'm seeing it on my screen is it
shifting colors over there
at this angle I don't see it but
basically I had this base file which
sets up a rectangle and asks the
subclass to make a pattern and then it
creates a simple property and animates
it and that changes the hue over time
and it just puts up the window for you
the the metal pattern is designed around
a simple six by six image using that
pattern to create the that sort of
corrugated metal look and then it of
course it's a six by six image but it
creates an image pattern that is scaled
up to 30 by 30 pixels just so you can
see it on the projector and another
example of how you might do this is if
if the background should contain
information you don't know it run until
run time like this one I basically
render the date using snapshot and then
animate you know once a second I
rerender it it could be the status of
the nuclear reactor whether it's gone
critical and it could change red to
green depending on how the coolant level
is and it could say critical when it's
critical or it could say nominal when
it's not that that's an example we a
limited set of preconditions might have
worked but if the status was something
that you know percent perhaps you want
to show the percentage that would
require 100 images or you can just do a
custom image and this particular example
uses snapshot on an ode to a text know
to grab to render the current date and
then there's one that basically shows
using it with a scene so it's basically
just the choice between using the the
method on scene versus the method on
node but both of them do sense we the
same thing an animated date changing the
color by sure what the color is visible
okay so those are the examples of custom
Oh zoomy I didn't do sue me i have to do
sue me
so zoom he was a piece of eye candy I
wrote several years ago for swing talk
and it's a relatively simple concept let
me just show you what it looks like
first I will use this version it
basically throws some color into a scene
and then runs an effect filter on it
which can sort of stir the color around
and the filters a fairly incredibly
simple filter all it's doing is for
every pixel it's choosing a ran not
necessarily a random but it's choosing
an algorithmic location to grab the
color information from the previous
frame so if the color information is for
instance closer to the center then it
will appear to be zooming out as you
apply it on successful exists at
successive frames some of these choose
sort of a this one's just a random
distance this one is choosing
interfering sine waves that caused that
swirling pattern but basically the to
apply that filter you need to
essentially perform a linear
interpolation on four pixels given the
location you're supposed to be sampling
from so I wrote successive versions of
that there's a there's a base
zoomy base hello
why can't I double-click this there we
go
this is basically sets up the windows
sets up the definition of the the maps
that it applies creates the frames per
second label and makes sure that that's
all going to show the correct frames per
second and every frame it adds the color
it plot applies the map and it swaps the
buffers that the source for one frame
becomes the destination for the next and
vice versa but then I have several
implementations of how to do the actual
work
using image ops so the simplest one to
displace a pixel you can get the colors
for the surrounding pixels that you need
to set to interpolate and then use the
color methods to interpolate this is
very simple to write because the color
object provides you the interpolation
and you're just grabbing the pixels you
need without having to do any indexing
yourself and then you set the color when
you're done and if you do it that
incredibly simple way you get three and
a half-ish three to three and a half
fish frames per second and the
interesting thing is that that's a half
a million pixel sized window you're
creating seven colors for each pixel to
perform the operation not include not
including the interpolation method so of
course it's gonna run really slowly the
next more most the next sophisticated
version you could do would be instead of
creating all those color objects you
simply get the integer pixel so it
involves seven million fewer actually
three and half million fewer objects per
frame being created but still three and
a half million function calls to
actually get the pixels and then I just
use a helper method in another file to
perform a linear interpolation on four
integers and this version
bumps us from three to three-and-a-half
frames per second to six to six and a
half rains per second so not quite
double the speed to get rid of dealing
with all those objects then if you want
to amortize your access to the image by
reading the data ahead of time so that
you don't have to be doing a method call
per pixel that you sample I wrote this
version which it overrides a method
called load data that that the the main
file calls so we can perform a get
pixels on the entire image into our
array and then we can just do array
accesses to get the data for the pixels
and notice I have to calculate my own
index so it means I have to understand
the pixel format enough to be able to
know how to calculate that index it's
fairly simple in this case I just used
the integer 1 of the integer formats
which is easy to access and when I'm
done I still call set a RGB per pixel
tourists stick the data into the
destination doing that we were at 6 and
six and a half six to six and a half
frames per second we're now at 17 is
frames per second that's not quite
that's two and a half to three times
faster to avoid four or method calls per
pixel
but you notice I still stick it back a
pixel at a time so what if I created two
arrays a source array in a desta ray I
load my data by grabbing the entire
image into the source buffer at the
beginning of the operation I store the
data into a destination array which I
then write back to the destination at
the end this eliminates the half a
million method calls to set the data
back and we were at 17 ish frames per
second this version takes us up to over
30 so
roughly double for not having to do that
method call per pixel to store the data
back now you notice I still reading it
every frame even though I've calculated
the previous frame I already have that
data in one of my buffers so I can skip
the read operation by using a swap of my
buffers so this is roughly the same it
doesn't override the load data it just
simply overrides swap buffers to switch
my internal integer arrays but to do
this now it had been filling one of the
images with the color spray some color
into the scene every every frame just so
that it has something to stir around I
have to now do that to my own array so I
overwrite the fill method to inject the
color into my own array but other than
that it's it's roughly the same I'm just
saving myself having to load the pixel
from the source image every time so we
were at 36 to 37 frames per second this
takes us up to almost 40 so that wasn't
as dramatic as as getting rid of the set
pixel call per pixel finally how much
time am I wasting by doing all these
linear interpolation work in my method
so I wrote a version that did a really
crappy I just copy the pixel so I figure
out what pixel you wanted that copy it
and it's gonna look ugly but this gets
rid of a huge number of my own
calculations per pixel how much impact
will that have compared to the overhead
of all the image i/o operations and in
this case I was at 30 almost 40 frames
per second and now I'm in the high 70s
so it's not quite double so basically
getting rid of my own calculations
doubled the performance that shows you
the ratio between how much time is being
spent with my own filtering operations
versus the pixel access operations
and I forget what the difference with
the optimum oh the optimal one is when I
did that first one I'm still calling
this displaced pixel from the main class
calls displaced pixel on each pixel
which means it's still doing all those
method calls what if I just overrode the
whole apply map operation and just
inlined it all so hot so I could go to
town and optimize this that was at
almost 80 frames per second I think then
this gets me to the numbers are quite
different than I'm to the projector but
yeah it bums me out before if I'm just
running this in without connected
projector I went from about 80 to about
105 frames per second so obviously my
chip is having to do a work here or
something anyway so that gives you an
example of the relative performance that
you can see from all the various methods
that we created for accessing pixels
some of them are more convenient don't
cost more that may not matter for your
operation if you want if you need at
maximum performance you're gonna do both
pixel operations and that's fairly
obvious application tuning but there's
an example that demonstrates the need
and then said oh one last demo I'll just
show demo of snapshot for thumbnails and
there's really just one demo for that
called thumbnails and it's what I did
here was I created a hypothetical
business browsing application that
basically just creates all these random
little nodes that have some combination
of these marketing buzzwords in them and
links them with little line so you feel
like you're actually analyzing your
business but really it's just a random
display and I create 16 of those
hypothetical
random project analysis things and then
I create a row of thumbnails so that you
can click and switch to different views
and then within the view I have a mouse
handler that lets you zoom into various
nodes so when you run it you end up with
hears ooh look I'm analyzing my business
here and then there's all these
different other analyses down here so I
click on that one I go to that if I
click on this node now I'll do it once
did you see so animated there and then
look if you watch down here I actually
update the thumbnail when it gets to the
destination so zoom zoom zoom new
thumbnail zoom zoom zoom new thumbnail
zoom zoom
thumbnail so it just basically running
the thumbnail with the variety of
transforms and you know you can you can
oops and I have no scrollbar down there
yep the screens not tall enough to show
this scrollbar so I can't scroll to the
other projects because there's a
scrollbar that's off the bottom of my
screen so the all this ends up basically
in the interesting part was this zoom
project basically just changed the
transform on the root node if you click
on a different project it just shoves a
new node into the into the view pane and
every time I do one of those timelines
to change the scale on the transform I
set an unfinished that when it's done it
calls me make thumb again and make thumb
basically sets up some snapshot
parameters to perform a scale to the
thumbnail size it sets a viewport to
match the image and then just calls node
that snapshot into the image
boom it updates on the screen I didn't
have to tell anything that it was a new
image I just called snapshot the image
got updated the image view showed the
new view so that is that example and to
wrap up has this compared to buffer an
image well a buffered image we
implemented with a number of partners
like Kodak and I think at the time macro
media was was talking with us and Adobe
and they had grand plans use this for
everything that one might possibly use
for Photoshop professional to do and
satellite imagery and medical imagery so
we basically wrote an open-ended set of
formats that you could customize as much
as you wanted to deal with any kind of
image data that you would ever need
under the Sun and it ended up with an
API that confused a lot of developers
you know how do i what what is a raster
why do I have to dig down into it and
what's that what the heck is a sample
model and why do I need it here we want
with the limited set of format so be
familiar to the vast majority of
developers and we focused on that for
the first release we may provide
something more sophisticated later but
remember we still have all of Java with
buffered image that can deal with those
markets that need incredibly detailed
custom images with a hundred different
kinds of planes of data and using the
pixel access you can import data from a
buffered image quite easily and so
therefore we essentially leverage now
all of the advanced imagery that was
created for Java now in Java effects
through an import mechanism but to keep
the API on the JavaFX side simple we're
focused on just the core formats that
developers tended to use we buffer an
image was created before and
created its buffers so it had its own
kind of rough equivalent to an nao
buffer that he used for data storage
here we just went with nao buffers and
rajala arrays for pixel transferred to
keep it simpler but freedom is also you
could dig down and get the actual array
that we were using to hold the data and
this caused two problems one was that we
would then not never be able to
basically store that data in a native
memory we had to store it in a java
accessible array because you could grab
the java accessible array also you could
go and start modifying pixels and we
wouldn't know when you were done to
update anything and basically if you
ever grabbed the array we would
permanently mark the images not
hardware-accelerated
not hardware-accelerated will ever again
so here we've gone with more of a batch
transfer operation that allows us to
maintain harbor acceleration despite the
fact that you're off modifying the data
so summary provided a way to get pixels
from your images credit a new writable
format that allows you to give data to
the images and also to a canvas which
will you know we'll talk about canvas
and another talk tomorrow provides a
bunch of methods from the convenient
give me a pixel at a time even in the
color object so I can play with it down
to both transfer into an array and then
I will figure out how to pull the pixels
apart myself time and pixel formats that
most developers are used to dealing with
it won't replace effects it's not
intended to be a way to get hardware
accelerated image manipulation this is
for I can write the Java code and you
can't do this with an effect so now I
have a way of doing it
and we we have internally some optional
packages which provide buffered image
and SWT import and export and this
mechanism is now how they do their work
so and it's basic it was like ten lines
of code to import a buffered image an
export a buffered image from JavaFX
using these facilities so provided
access to legacy code that is fairly
rich with image processing so open up to
questions now your hand went up first
but his is still up I don't know I'm
sorry the echoes in here it's hard to
and the in resolve of the
well earlier on I showed that the pixel
reader in the pixel writer will have a
get pixel format method on them that
tells you the pixel format that we're
expecting so if you stick to that pixel
format which means you'll have to do it
a switch on the enum typically right now
almost all of our images are byte BGR a
premultiplied internally but also I
think there was work that did not make
it into 2.2 that optimized all of our
pixel transform transfers in 2.2 we
shipped with basically calling get pixel
set pixel on your on the reader and
writer for every pixel and a bulk
transfer operation which is going to be
slow and shortly after 2.2 shipped there
was code that went in that basically did
direct conversions of all the formats
from it to so it might get better in the
next release probably it probably will
get better in the next release but but
the intention is that you're supposed to
use the get pixel format yeah yep yep
and then back there
so the question is whether we're going
to support ICC profiles and tiled images
for images that are too large to really
deal with in a single chunk and the
answer is no it goes back to that we
wanted to keep it simple for the first
release oh so there's nothing here that
deals with it yet we know this this will
need to grow we're not sure how we're
gonna do that yet we need it we wanted
to our goals were basically to not get
in the way of hardware acceleration and
to be accessible in terms of not
confusing developers beyond that moving
forward is work that has yet to be
defined so you could start up a
discussion on any of the mailing lists
to how you would like to see that added
and also this also comes back into the
buffered image import/export we have all
that with the existing Java stuff so it
was enough outside the core needs that
we don't mind creating a temporary
dependency on on the regular Java stuff
use that and import into here because
the applications that tend to need to do
that
are not going to be impacted by having
both the JavaFX stack and the AWT stack
at the same time so there's two answers
one is we didn't want to focus on that
yep that doesn't mean we're not going to
and you can do that using legacy code
and import it and then down here
he needs a while we had to call the
great American
get our Federation
recession
so the the question was that when they
were using buffered image with swing
they discovered that some of the
format's would not end up
hardware-accelerated and they had to
play with the color format they were
using in the color space that we're
using and how did how does this differ
and the answer is that we specific
designed this to not get in the way of
hardware acceleration so that's why we
do the bulk transfer methods when you
give us the data we will make sure it's
in a format that we can Harbor
accelerate it should have worked for
buffered image we should probably talk
after because unless you got the buffer
of pixels from a buffered image all
buffered images should have been
mirrored in vram and harbor accelerated
but if you ever got that buffer we'd
stop doing that and then we might have
needed to do a conversion of reclaim
Wow so what got what came back from
image IO there was also bugs in image IO
where it would leave an image in that
state of I've grabbed the data array now
it's not so real exactly the FX image
wall should always be accelerated
currently yeah
I think it ended
okay so I'm just gonna summarize that is
another vote for more advanced imaging
in it JavaFX okay and then did you have
a follow on
so
how would my software app handle it
JavaFX whatever format it's using
internally like if you if our FR jpeg
loader decided to load it into one of
those other formats the question let me
repeat is what if you had a source image
that was indexed or not one of the the
demo seemed to be focused focused on a
RGB formats what if you had loaded an
image that was not one of those formats
how would software dealt with it and the
answer is twofold one the result of
loading the image is not necessarily how
JavaFX internally represents it
sometimes we do have representations
that are not a RGB internally and if you
do get pixels on an image that is in
another format we can convert on-the-fly
and if you do set pixels and suddenly we
have to deal with a fault we will
internally upgrade or change it into a
format that can take the data you gave
it so if we look at a gift for instance
and we were representing it internally
in index format I'm not sure that we do
that but if we were and you handed us an
arbitrary color we would then just
convert it to a full-color image and be
able to store the color will handle that
internally for you
all right
and the question is between do we
support things like ycbcr at ycbcr there
is support internally for that but it's
basically I don't think we use it for
JPEGs I think we use it more for MPEG
but there's nothing to stop us
architectural II from supporting that so
what you're asking is more what do we do
right now which could change as the
needs are expressed we could you know
more optimally support those formats but
this API does not expose but whether
we're doing that and if we ever found it
necessarily necessary to support ycbcr
internally for performance or quality or
whatever and people were reading and
writing those kinds of images a lot we
could add that to the list of supported
formats but you will always be able to
get an air to be version of a pixel
because that they guys provide that it
might be it might require conversion but
that way you don't have to worry about
what we're doing internally but for
those people who want to worry about it
that's all work and api's that are TBD
and I think we're in the last couple of
minutes if any last question otherwise
we use break for our discussion and let
you get to the next talks okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>