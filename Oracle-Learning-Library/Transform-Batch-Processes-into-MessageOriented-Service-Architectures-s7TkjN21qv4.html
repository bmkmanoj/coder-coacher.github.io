<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transform Batch Processes into Message-Oriented Service Architectures | Coder Coacher - Coaching Coders</title><meta content="Transform Batch Processes into Message-Oriented Service Architectures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Transform Batch Processes into Message-Oriented Service Architectures</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s7TkjN21qv4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes so my name is Stefano vitam and I'm
working as an IT architect for different
customers basically consulting them for
transitions that they are applying to
their application landscapes in the past
like quite a few that were transitioning
from from a better landscape with lots
of bad jobs to a more real time
architecture based on messaging okay my
name is Tomas cruising I'm a consultant
from Germany as well and I found a Java
user group in Munster together with
Garrett perhaps you saw some Java fake
stuff from him and if you want to reach
out you can find me on Twitter
my name is Evelyn Sam okay the setting
good Jim
yes sir let's have a short look at the
Jenna we'll start with the basic setting
so introducing the different types that
you can use to build your application
landscape from online and batch and
messaging then we'll have an
architecture overview that shows you the
different steps of such a transition
followed up by the actual process of
transition and the implications to the
technology and to the people that are
involved there then we have a section on
technology choices and frameworks that's
not going to be exhausted but it's just
about what we found that actually worked
for our projects and finally we'll have
some examples tax and lessons learned
from the projects that we've done all
right thanks so if you consider batch
oriented systems and the contrary side
online processing then you
find they're quite different in the bed
rolled you have d carpet systems they
are running on nightly basis with high
latency so you have to wait until the
processing starts they are a synchronous
because they are running at nighttime
when there's low load on the systems and
they are triggered at the time so it's
always the same time they run and
there's no other condition treating with
them usually if you want to scale them
you use usually bigger machines and use
more resources on one machine to get the
job running faster on the other side we
have online systems are tightly coupled
and have low latency whenever something
has to be done it is directly done in a
synchronous manner and triggered by the
event which should be protests usually
you can scale that easily horizontally
by more machines so you add a load
balancer and more web servers for
example if you are talking about a web
based application and right between
these paradigms you find message
oriented systems decoupled they provide
most mostly low latency processing and
they can be a synchronous or synchronous
implementations they are triggered by
event as well and you can ask the online
systems mostly it scales and
horizontally by using more machines so
this provides some features which you
don't have in the batch world and some
features you don't have in your online
world because what we often find is that
those people who want to migrate from a
batch world that they actually try to
use more online paradigms and that they
kind of over do so they have currently
batch processes and lonicera care we
need this
in a more with with less latency and
with innate the responses faster so they
migrated to an online world based on
rest services or web services or
whatever currently is the technology of
choice for for doing online stuff but
then they find themselves in a
completely different world with totally
different constraints because they now
have a tightly coupled system and
message-oriented actually is kind of a
combination of both so you can combine
the non-functional properties of that
with the non-functional properties of
online to achieve this and that's also
why we made this box in the middle
rather big because you can't do
message-oriented much like batch if you
have large chunks and large messages
that you are processing or you can make
it almost online if you do very small
very small messages and have them
asynchronously processed but have a
client for example web client or
whatever actually wait for the answer so
that it appears to the end-user as it
defaults an online call
so speaking about the reasons why some
organizations want to migrate from
petrol to an online world or to a
message oriented architecture some
motivations are business driven for
example you want to speed up processes
and provide to your customers or clients
a faster turnaround time so that they
get their results faster for example if
you would order at Amazon something and
have to wait one day until a batch
process is finished until you get your
order confirmation that's not so
convenient and could lead to customers
using your support and asking where's my
order and what happened so you want to
send them out an email faster on the
other hand you don't want your customers
to wait until the email is successfully
delivered to the mail server so that's
something that can be easily implemented
as an christmas-tree other reasons are
leveraging the opportunities of mobile
and Internet transactions you you don't
really have the batch window at night
when you have a globally distributed
operation and you have to go to 24/7
operation it there's just no time to
take off business hours because
somewhere in the world there's always
business hours and you want to serve
your client other hour reasons could be
technology driven
so you want to decommission your current
mainframe system which may be too old or
needs to too much money for maintenance
or too much human resources technology
reasons here the same 24/7 operation
should be implemented or you might even
consider migrating to the cloud and as
you all know in the cloud you have many
small systems and not one big mainframe
usually which you can use for batch
processes
so
okay then let's have a look at the
architectural overview so what we are
looking here is just a typical system
with input data and output data and the
processing between those two this is
usually the situation we're encountering
one we see batch processes we have a
large amount of input large amount of
output and processing done in chunks
through usually a complex batch jobs
that do all kinds of things they process
the data they acquire additional data
that is that is required for the
processing and finally they generate the
output and see who with batch input heap
of data that output and the processing m
and those chunks the very first step to
migrate might be to just keep the input
and the output the same but change how
the processing is done you don't get any
real benefits from this but you can
misty-eyed using messaging internally
you can try it out you can see all the
different other performances now
different m and you can keep the
integration of the system the same
because input and output just don't
change so what do you do you have to do
to do this basically you need to split
the input so if you have like files or
whatever as input at any you know could
split it into you know into single rows
and to single data into single data
records and then let them enter the your
processing flow the processing is quite
different now because you don't have one
complex batch jobs but one complex batch
job but you have a set of individual
steps that are decoupled
and that you can connect by by pipes
usually those are in memory channels if
you are staying in one system or you
could use JMS message message queues if
you want to run it on separate hosts
what you get immediately from this is
that you can now scale horizontally
because all those splitted messages can
be processed in parallel once you are
done you have to aggregate the data so
that the output remains the same
actually you will just collect them
attached and aggregate them into a file
for output so what actually is important
and we often see with clients is that
they try to directly replace it
the batch job with which is often on one
program which is other complex directly
with another system which is using
messages but still does not leverage the
Messing messaging inside of the
application so this gets you the if you
do this if you split up your processing
into separate steps inside your
application you get a decoupling of the
steps itself and you can then easily
test these steps that's a protein we
will come back to that item okay so
that's the first step keeping input and
output the same but replacing the
processing next step might be to
actually change the output so now you
still keep the input by some files or at
least that big heap of data and in one
batch when you split the data as before
process it individually but the output
is now also individual messages so then
you can continue to pass on these
individual messages to the neck
system that also has to be message
enabled in this face usually you will
find this kind of this kind of
architecture when you depend on external
input so maybe you have a contractor or
a supplier that will deliver you once
per day a big file with with orders or
with status on your purchases or
whatever you won't be able to change
that in the first or a second step of
the migration because of external
parties that are involved there so you
would keep the input the same but then
we will split the the data do the
processing internally based on messages
and output single messages that can be
consumed by other next system in your
operation you can also do it the other
way around if you are producing data
that is consumed by an external party
then you use your input and based on
messages you do the processing by its
own individual messages and then you
aggregate the data to generate files or
whatever to pass that on to a partner or
to another a third party that relies on
that batch output
yeah that's the final step if you have
full control over the process then you
will have both input and output as
individual messages and the processes
processing also down on those individual
messages with loosely coupled steps that
are connected by pipes internally
so you may wonder why should we do this
um if we keep the output as a batch or
if we keep the input that's one big file
what you get is all the properties of a
message oriented system we can use
throttling or rate limiting to have a
fine grained control of how his system
resources I use you can scale inside of
your cooperation better by using smaller
machines and even using machines and
some peak hours which are not used
during these peak hours this is harder
to achieve with pitch drops so we last
we saw the the architecture picture that
we have input and output based on
messages and if you are inside your
organization have control over that you
can even between systems use messages as
transport for your datum this can be
extended inside the systems you can use
in-memory pipes to connect the various
steps of your processing who is a way of
the pipes and filters patterns well
that's not that much here there was
another talk at the Darwin it should
have been there yeah I patterns so you
we come back to book hints at the end of
the talk so the beta pipes and filters
patterns is to have separate steps which
are responsible for just one thing so
for example extracting a value from a
row and then passing that on through a
pipe to the next part of your system
this is depicted here the pipes are the
pipes and the messages are the small
cotton serum so you have the opportunity
to use that inside your application as
in memory and you can connect external
systems for example by JMS
to pass on the events yes the basic
survey zaleka what aporia see is that
you can use pipes and filters with inner
system then you have those filters
implemented as individual steps and the
the pipes just being in memory passing
on of the messages from one step to
another and you can take it to another
level where you consider the filters to
be systems and the pipes to be JMS cues
so basically it's the same pattern but
it's applied every as a different
architectural level and that's actually
what we want to show here so you use the
same pattern but you apply it
differently and this is especially
important if you're designing a new
system to be able to test each part of
the system and to be flexible by
configuration so for example there's a
new kind of authentication which should
be handled so you can add another step
and all the other steps are not aware of
what happens with the data before there
are after them and they don't really
have to care about that it's really
important to get this concept on various
levels so we're talking about the
transition from batch to
message-oriented systems there are some
implications from this transition which
we would like to point out now the
easiest part is phase one you just
deploy a message program for example
ActiveMQ or some other product that's
nothing you have to really take care of
at this phase but it has implications
for all the other phase because you need
to set up an Operations team in your
organization for that as well
you start usually with some small pilot
projects which are migrated to a message
oriented architecture after the message
broker is deployed and then
you gained some experience with it and
you learn how to operate that how to
monitor all that and what you need to
make sure that the environment fits your
needs that's phase two after you you do
learn that you start with a centralized
monitoring usually instead of just
calling one person if something does not
work you have proactive monitoring and
alerting systems you implement
procedures in your organization sorry
and implement the error handling and
escalation procedures in the
organization for that and usually this
is a time and you start to stop
implementing old-fashioned bad jobs and
everything that is now new will be
implemented as message oriented
architecture you may even start at this
phase to actively migrate all jobs to
message based jobs as well besides of
the pilot projects which should now be
running fine and then in the last phase
you notice that you need something
special for your Asian for example
customized tooling which is not
available off-the-shelf or special
monitoring solutions or reporting
solutions to provide insight Asian or
geo processors at this phase usually all
batch jobs when make sense are migrated
to the new architecture
you know usually what we see among
customers start migrating to message
oriented architectures is that you have
multiple starting points within the
organization they're usually pain points
that are encountered by different
projects in different locations in the
in the in the organization and they all
start to somehow experiment with with
other technologies and usually when we
come in and we already see a bunch of
message brokers being deep having been
deployed so one group started hi we can
just embed
activemq within our application nobody
sees that nobody knows that they using
messaging only within their system
usually they don't tell everybody big
things about that and then you have
other groups that are connecting ear
like ERP systems and all of the ERP
vendors also provide some integration
stack you basically get message broker
for free there and it's also somehow
installed or even if you're using an
Oracle database you already have Oracle
IQ deployed there some people figure out
hey we can use this from Java with JMS
we can also use it from pl/sql and say
you have a bunch of message brokers
within your organization's and so part
of that usually between phase one and
phase two is to actually select one of
those brokers for inter system
connectivity and finally for inter
system communication and that one will
be the one that is then supported
centrally by the operations team and
that will be monitored and managed this
is this really important because many
organizations don't get it this is in a
very important infrastructure and that
they have to set up operations team for
that so for databases it's usually
normal thing that you have a DBA or even
though you're an on team within
operations but for messaging
infrastructure usually maybe it's not
planned your head then I won't happen
yeah we want to show you an example for
customized tooling which we developed
for a client what what you sometimes
miss is for the message brokers
something to really manage what your
messages for example you're all aware of
JMS and all that but just to rephrase
and if you have a problem somewhere your
message cannot be processed all you have
an exception in your flow it will be put
to an error cue and now something has to
be done with this message and you can do
that manually with for example hammers
or some some basic tooling which is able
to use Jam as to operate on your program
but to to really be able to support your
processes you usually need customized
tooling which can leverage same as to
put this exception message back on track
for example something needs to be fixed
or there is a data problem where you
want to edit the message and then resend
it to your queue you know basically that
custom tooling is something that we
found really a good thing to have in the
last phase of the transition to finally
convince everybody within the
organization that this is a good thing
and that it's actually that it actually
matches their business so this is why we
proposed is just something that is
custom that is not based on the
technical tools that the message brokers
provide because then it can be even used
by there's some less technical people
because all those messages usually
represent a
a business concept so you have something
like an order or a contract or whatever
and that can be searched for by um I
think here by the data that the business
already knows then I can say okay what
happened to the messages which steps
went through that and as soon as the the
business people also gain some insight
into how the processing the processing
is done they can help fix problems
because they're usually caused by my
business the kinds of things like
missing missing reference data or wrong
mapping or something like that if they
understand how it works and can look up
the individual messages that really
helps a lot and so we'll just show you
one example are we did this for for one
of the customers usually it will look
different depending on the projects but
the concept should be similar so you
have an overview of the cues you have an
overview of the individual messages and
you see what has been locked for those
for those messages
if there's something I would like to
point out at this point if you have a
batch job which fails it's sometimes
harder to trace back what problem was
there and you have usually a complete
chunk which failed if you have a message
oriented application and there's a
problem with one data element this data
element can be identified easily and
then processed appropriately by
escalation procedures or management of
phase okay so what we have here yeah
especially the name of the queue those
are based on in this case business
partner names we say okay average an
overview of how many messages have been
processed how many have been are
remaining in the queue and how many
errors have occurred so this is just
fake data's
it just to get you an overview of what
it might look like basically the
interesting things are a number of
remaining messages because that's an
indication that some processing is slow
or that there are other problems why
these messages are not processed so this
is also something where I could add some
monitoring for the AI of this exceeds a
certain threshold then threshold and you
would cause an alert or notify someone
and then of course you have these
exception messages those are messages
that actually could not be processed due
to some kind of an error you can then
drill down into those into those
messages what you may or may not see is
that some of them are in displayed in
light gray those are the messages that
have already been processed so usually
they are not of much interested because
while they are there already a dumb but
depending on your message broker you can
easily configure it to retain those
messages for compliance or for debugging
or for whatever reasons you want you so
that you may even if they have been
processed in the past you'll still have
them available as a reference and come I
can look up the data so those that are
still available can never look at them
you can see the payload there so
basically what you see is in this case
it's an XML message and you have all
these headers available those are both
technical headers like the user who
include them as a message or time when
it wasn't cute and was DQ but this may
also be a business business information
like a contract number or an order
number or an article number whatever and
what we have here in that business
domain is usually a user that is
responsible for the business process a
contract number or just anything else a
custom header of the message our custom
value that's what you can use to search
for messages here you can then either
delete them if messages that ran into an
error error and you no longer need them
or you can move them to another queue so
if you have a fixed problem and you can
resend that message to a queue to
reschedule its processing what we also
added here is to search for log messages
that's basically the connection of each
of the nodes that were processing the
message that were executing some steps
on them and they all send their logging
data to a central to central system and
that stores it in the day
bass and adds the correlation ID of the
message so that we can track down
individual OTT messages to individual
messages it makes it much easier to
actually see what the problem what the
cause of the problem may may may have
been so here we have one correlation ID
correlation idea of the message that I
just selected there we see all the
messages block messages that have been
generated for that for that message so
if you click there discuss it some
stacktrace
yeah on the new else yet it's just what
4j output but with the addition of the
correlation ID and internally we also
use JMS to disability to collect those
logging information at one central point
so this is the expression especially
important if you're running your stuff
and the cloud because then you usually
have lots of nodes and we can't check
the log files of the individual of the
individual nodes they're usually
obviously with many customers right now
is that if you have a production problem
and you have a correlation ID or
something like that that they using SSH
to connect to a server and then they do
grep and then they may find something or
may not find something depending on
whether they have to write the correct
host and if you also have set up a
development system and test system and
maybe something else and it becomes
really hard to know which service to log
into to grab for the for the logging off
but and you need technical persons for
this as well so this kind of application
allows you that Politecnico persons can
really easy look look up data what is
happening was a customer with the order
where it's a problem
and you can even use this kind of
tooling and a higher level so this is
even this tool is more targeted at
technical users and actually we're using
behind this to arrest service which can
be used to integrate in higher level
tools as well so this allows you to
search for everything regarding one
customer or one process and that's
something you might want to implement
yourself it's not not really hard it's
some effort but on the long term it
saves you a lot of time and for some
managers it's always important to see
how is my system doing and this was a
good point the rest service that we have
offering a book with us that was just a
recent addition here and basically it
just exposes the HQ and the number of
remaining successful and exception
messages as well service and then they
can use monitoring tool like Nagios or
whatever they're using to watch out for
these counts and alert somebody on duty
if it exceeds a threshold okay
okay that was the demo so as you may
have noticed it's not that easy to move
to a different architecture there are
some implications for the operations
what we just saw but for all your roles
and your team you have additional
changes the way you design your
applications changes at least if you
want to do it the right way so that you
are able to test and extend your
application for future needs you should
consider using the pipes and filters
approach or some of the other patterns
in the yai book but it's a real paradigm
change you don't think in in an online
world on a petrol you think only about
messages and events and each step of
your application is separated and
dicovered from various applications just
focus on one responsibility it means
that for the development you work
differently you have test this single
step you make a unit test for it and
usually you can really easily do that
because the the step don't it doesn't
have many dependencies on on other
systems on other data or external
resources often it's just really small
you can mock all services and test it
really easily and what we did with our
customers is that we started with the
paradigm of message flows or job flows
in order to design all the application
works with all the data and how
everything is connected because one
message is really flowing through your
system from Brahm step to another step
and at some point it's decided to take a
different route and then it flows for
them so this is how we think about it of
course this sounds really easy but your
infrastructure will become more complex
you have the message broker which is a
new system
you may have more systems which interact
why the message broker and after all
it's more than just one big database or
one host so your infrastructure will
become more complex and you should be
able to manage that well but it gives
you on the other hand of course more
flexibility if you have scheduled
downtime for example you want to do
testing or have to replace on some some
hardware you can take the system offline
all the message design the broker and
after the system becomes available again
it will consume the messages and
continue with the operation and if you
want to scale up you can just add more
machines and all machines will consume
the messages from the message brokers a
message broker is responsible to handle
transactions and to prevent wkn messages
so that two machines are effectively
processing the same message as it's done
by the message program so you don't have
to keep taking all that that's really
good and after all you can even go to
the cloud and scale them yeah basically
the cloud scaling actually comes from
the parallel problem from the future to
be able to execute those messages those
individual messages in parallel right
yeah then we have first some technology
choices and selective frameworks just a
look at them we divided them into four
four parts one is development then the
message broadcast basic infrastructure
the runtime that you're using to run
your steps in and monitoring and
management ICU for development usually
you have choice between using visual
editor as it comes with many as we probe
approach products or you can use
text-based
develop
tools often we tend to prefer the tax
based ones because actually it's much
easier to manage them and the source
control repository or something like
that and if you have a text-based you
can still generate images and visual
representations of your flows there but
both are available examples for this are
eclipse with the spring tool suite on
one hand side basically uses XML and
provides an option to generate graphics
from that or the on the other hand the
WebSphere integration develop that is a
heavily visual based thing that you will
probably use a few years the other IBM
to a product but yeah you will have your
experience if you used it for the
message broker and basically you have
the option to either use a central
message broker or D central one so the
central message broker means that you
have high availability highly available
cluster of a few nodes that you will
have distribute your messages and versus
D central other means that brokers
deployed to each of your nodes and runs
their you can either use it embedded or
as a service externally and you can
decide whether you want to stand to us
on the API like JMS or on the protocol
like AMQP examples are Bob WebSphere MQ
probably the most widely used in the
enterprise enterprise environment
ActiveMQ as an open source solution that
works really well and if you're in the
cloud then you are normally tied to the
infrastructure that your cloud supplier
provides you with
so in case of I was an easy-to you will
probably use simple queuing service for
the runtime you can decide what you want
to use either you've a vendor for your
runtime using IBM stuff and it will
usually just apply it to WebSphere and
if you are running with more custom
things then you are free to decide which
footprint
you want your runtime to have so we made
a very good experience by just using
Java SE with Java service rep or
something like that to actually start up
the application and nothing more if you
need additional Java EE things when you
can deploy it to service container like
jetty or Tomcat or of course if you need
lots of integration they can also use a
fully fledged Java EE server like
GlassFish or one of the commercial
products basically the bottom line here
is that you should really think about
what is required for you your steps and
for your systems and go with the
smallest that is possible so really
consider using Java EE for running these
things especially if you are in the
cloud you can have some great
performance we have a very small runtime
then you can use small machines as notes
and I will have most of their power
available for actually doing business
work and not to manage the container
itself finally for monitoring and
management you have some of these
monitoring and management tools
integrated into the message broker so of
the products come with some kind of
monitoring or it's integrated within the
runtime maybe it provides J max or s
based things or you go with your own so
or a combination so basically what we
did most of the time is to use
monitoring some of the shelf usually
open-source products and build our own
things for management and to expose the
data that will then be monitored yep
the same holds true for the service for
the frameworks basically you have three
options the first is do-it-yourself you
use Java EE with the building blocks JMS
JTA and so on you can use that and build
your complete own stack of a message
based architecture
integration and transformation whatever
is necessary to serve your business
needs the other option is to use a
readily available framework there are
open source frameworks like spring
integration and apache camel both are
really good and light white so you can
go with them and will you complete
application using just Java Azim and
spring integration and then run it in a
library container like chatty or Tomcat
and then you can apply the pipes and
filters approach to connect your
building blocks those different steps of
your applications you have the third
option to use a vendor product sometimes
you hear the the the buzzword of the
enterprise service bus or is being and
are often more heavy right but supported
and with tooling and support supported a
product that depends on your needs if
your I wouldn't discourage you of
rolling yourself everything just go
ahead and use an open-source framework
I favor spring integration it's really
easy to learn and you can start really
fast but if your organization requires
some kind of really political motivated
or whatever motivated big when the
support
you cannot cost you that but consider
first spring integration and apache
camel there is compared to the point
available as well so try that first and
don't roll yourself okay finally we'll
provide you with one example of what
such a stack might look like and a few
lessons that we learned while doing
those projects so this example stack is
made of an Apache Tomcat as a servlet
containers this is absolutely a
mid-range runtime
it's not as lean as a Java SE but it
provides with endpoints for HTTP so if
you have to handle actually P requests
that are coming in or if to offer s
services then usually you will go with a
servlet container and for that
organization they already had all the
knowledge in the operation to him to
deploy Tomcats so that wasn't an issue
for them within that Tomcat we used the
spring framework for transaction
handling and for data access and finally
a spring integration for for wiring so
the those filters those pipes within the
JVM that connected the individual
filters were all configured via spring
integration were configured as spring
beans as well as for configuration and
while your spring integration is one of
those frameworks that actually
implements the AI patterns say that you
have a common language to talk about the
concepts as message broker they decided
to use Oracle IQ so this is just JMS
implementation there is a variable
inside the Oracle database that comes
for free with it and that is used
internally for application but when
different Oracle database nodes
it's quite nice because you can use the
same transaction for your database
access and for the JMS transaction
without having to worry about any things
like two-phase commit or something like
that yeah so that's what the stack looks
like from yeah brick architecture look
basically we have the business logic
layer at the top and then these
individual flows flow as just a
connection of air pipes and filters that
are providing one use casts of a want on
that way originally they were derived
from batch shops so where you at use and
then there are four times one big batch
job then now you have one flow that is
split up into different steps that are
loosely coupled and interconnected
foundation for that is being integration
as framework for for those flows Tomcat
server that's used to run them and then
Oracle IQ as message broker and whatever
other systems are to be connected HTTP
based services of other third party
third parties that provides data for for
the application as well as some database
lookup and stuff like that what it's
really nice here
is that there is only one reason to
change your business logic that is when
your business requirements are changing
if you need to connect to a different
message broker or if you need to change
authentication or
pression or something like that you do
that as one part of your pipes and
filters architecture and your business
logic is D confident of that
so there's known a need to change inside
of your code
besides configuration in the spring of
tribulation level this is really nice
and yeah that makes your long-term cost
of your architecture lower then if you
have a tightly coupled architecture okay
so finally the lessons learned basically
three items that we'd like to highlight
first thing is the mind shift mind shift
not only with the technical people but
also with the the business people that
are involved and not only for how
systems are connected but also our
systems are actually built because the
parents are leveraged at both levels at
connecting the system at and at
internally structuring the systems and
not just for the backend system but also
for the front-end system so if you have
a message oriented system then you will
not get an immediate reply as if you are
using an online system and but you are
getting a response much faster than if
you'll just submit your job until some
some bad process and next day check if
it's done next the effort that is
something that should be really
considered if you remember the three
phases that we had in phase one that's
usually done within a few months so
you'll get started I think that's that's
rolling this is really fast you have
early adopters in most of the
organizations that are keen to try out
new things and they're a better
the question was how many bad stops that
would include with a few months that's
that's a good question and depends of
course on the size of the batch jobs but
we usually start out with just one or
two of the of these jobs and keep
focused on the infrastructure then you
have phase two that usually takes a long
time and this is yeah at least six
months to a year and then for the final
phase that's a really hard one until you
have convinced everybody to actually
change this and to adopt the new
paradigms and this is a face yeah that
the bay can plan at least a year for for
that so the total at all the time for
such a project will be usually between
two and three years at least where you
start getting the benefits right away so
you don't have to be completely finished
to start having the benefits yeah but
speaking about convincing everyone that
there are some jobs that are really best
fit with a better implementation for
example if you have some payroll stuff
which needs to be done at the fixed
point in time you should really keep
that as a bad job as fish showed on one
of the first slides you can integrate a
messaging architecture easily with bad
shots as well and you should not apply
the golden hammer anti-pattern so now
messaging is wonderful if you love it
and you try to force everything into
that it's not how things work to keep in
mind that there are some problems which
should not be converted to you message
the architectures as well yeah good
example for where batches were really
good fit is if you have things that are
just based on time so if you have say at
the end of the month you are generating
accounting statements or something like
that
then that's just a batch job and I
wouldn't change that of course you could
you could generate a lot of individual
messages for for each account that you
have but it really doesn't make much
sense it's time there is not event based
so keep it keep it batch if that works
for you okay so we're done have a few
minutes left for your questions yep
you know
your question was regarding performance
and execution time and throughput one
thing has to be said of course that a
batch a well-designed batch architecture
is really a performance thing this is
real working well but it has one problem
and that problem is actually scalability
depending on the yeah depending on why
yeah depending on how you do it and this
is at least in our experience there was
something we had we had some some great
success in actually being able to split
all the data into single messages and
distribute them over a whole range of
notes so the throughput is only limited
by the number of notes you put in there
and these notes are usually much cheaper
than the mainframes that are used to
perform batch processes yes
yes yes
yes
no no yeah that's right and and of
course if batch actually works for you
then use it yes yeah
yes indeed so there's really there's
really no reason to say let's throw out
all our batch processes even if they
work and as I said spring batch is a
very good example and spring batches
actually built on spring integration so
that leverages all that infrastructure
and just says okay one message mustn't
be a single message but it can be batch
and we can put in like a hundred or a
thousand records in just one message and
process that so of course then also
patient yeah
yes
thank you yeah sorry I said IQ and I saw
we wrote MQ that Oracle MQ is actually a
mixture of WebSphere MQ and Oracle a
queue
to the the question was how does
enabling Oracle IQ affect the
performance of the Oracle database yeah
it turned out that it works really well
but maybe this is due to Oracle a queue
already being used internally because
it's used for application and we are
running in in a rack environment so we
have multiple nodes of the Oracle
database so every change you make to the
database is already replicated to other
nodes within that Oracle cluster so we
didn't see a massive performance problem
there of course you have to allocate for
this usually it depends on how many
messages you'll have there but our
experience was that the performance over
there was rather slow what were rather
low
a Christmas how we benchmarked
performance before and after basically
we just use the actual business use
cases and run this with a synthesized
data that's hard to say it's depending
on the customer so when in multiple
projects and some customers are
processing high numbers of messages in
their infrastructure and some lower but
sorry we're running out of time you can
grab us on the hall and ask more
questions but okay thank you for your
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>