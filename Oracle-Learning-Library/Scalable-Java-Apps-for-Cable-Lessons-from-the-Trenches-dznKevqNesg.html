<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable Java Apps for Cable: Lessons from the Trenches | Coder Coacher - Coaching Coders</title><meta content="Scalable Java Apps for Cable: Lessons from the Trenches - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalable Java Apps for Cable: Lessons from the Trenches</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dznKevqNesg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming out to our session it
is late and we're glad to have you here
I'm going to go over how we're going to
do this session I'm I'm Donald blah i
work for cox communications steve is
going to come up and talk about the
client architecture Luke's going to talk
about the client development techniques
and I'm going to talk about the server
side stuff after him and then we'll do
QA so I'm going to turn it over to Steve
can we get started thanks dad we are
glad you guys are here and we are we're
the team that actually helped build the
what's known as the Cox true to a client
that we also affectionately call trio
and trio and the true to a client is
really based on the open cable
application platform and what is what is
the true two-way well it is Oh cap and
it is Oh cap that's based on an older
Java profile known as the one dot for
java SE it's a cdc based profile in
previous lives we've also worked on a CL
dc-based profile that we also
affectionately termed on ramp 20 cap but
we're here to talk about Oh Cap'n trio
specifically and some of the
architectures and lessons that we
learned in actually deploying these
architectures to the field so one of the
one of the things that we do at cox as
we deploy television services to
subscribers nationwide in our markets we
have many markets nationwide we actually
worked with our marketing team and we
had actually called the product
something different before we call the
trio and then the marketing team came up
with the notion that well people won't
really understand what trio is so we're
going to call this the plus package and
what the plus package is is
gives our subscribers the ability to
watch mpeg-4 content and it's an HD
quality high-def graphics quality guide
if you are a cable subscriber or directv
or one of these other multiple system
operator subscribers most of the and
even to date I think this is true most
of the guides that are out there are SD
quality graphics 640 by 480 this is a
960 x 540 graphics guide so it's higher
quality looks much better on the big
screen this actual product is based on
okay up set top boxes now trio runs on
multiple styles of vocab set-top boxes
and it has recently been utilized by
CableLabs who created the open cable
application platform as one of the test
guides when they have a set-top box that
comes into their labs for certification
trio is the guide they run it against to
verify the functionality of the set-top
boxes so we're pretty we're pretty
pumped on stuff like that it's good
stuff anyway trio is actually based on
the docsis set-top gateway as well which
provides direct IP connectivity through
the cable infrastructure initially it's
a walled garden approach where trio set
tops are connected internal de Kock's
internal networks only but the trio set
tops and the DSD infrastructure has been
specified by Cox at least to have all
public IP address ability so these
devices can actually be accessed and
talk to Internet devices servers and all
sorts of stuff not just not just the Cox
back office and here's a description of
the actual connectivity in a Cox network
to the Internet so basically it's very
simple just goes through routers to the
CMT s to the set-top box and we have
regional data centers where these
routers are maintained
and that's how we get to the Internet
very very simple connectivity paradigm
there's also a Cox back office that's
connected in this paradigm but that's
not that's something that Don will talk
about it a little bit later in terms of
the server connectivity and the
challenges we have there so what is in
the actual client the client
architecture client software
architecture includes support for the O
cap specified monitor application sort
of like the controlling application that
controls what applications can do on the
set top plus it also includes
specified applications so we have
something called the ITV master
application which controls some of the
interactive television functionality
that's that we provide as services to
our subscribers we also support
third-party applications and the
enhanced binary interchange format or
even advanced advertising initiative so
these step tops can handle all that at
the application layer and how they do
that is they're supported by these
services libraries that map the cable
specific features such as video on
demand and switch digital video and
parental controls and digital video
recording which includes things like
conflict detection for when you're
recording multiple shows at once and say
you don't have the resources to have
multiple recordings going on at the same
time it actually tells you based on the
resource availability on the set top
itself that that you have a conflict and
you have to resolve it or else it won't
be able to record what your favorite
shows are libraries and applications are
all supported by these Oh cap stack and
the set-top box operating system
firmware so the O cap stack actually
runs on top of java virtual machine and
a linux operating system the set-top box
hardware is directed by those layers to
do the stuff that television
experience it requires so actual tuning
to the frequency modulated channels on
the RF plant for instance so some of the
things we had to deal with from a Java
perspective on the JVM in terms of
understanding what what television
viewing experience meant and how to
properly and quickly and have a good
viewing experience especially in light
of the higher end graphics that we were
supporting in trio are noted here i
noted dynamic binding and what what i
mean by this is this is sort of the
trade-off between the actual class
loading on the fly and class loading at
boot time so one of the things that we
do is to optimize our class loading so
that when the user says a I want to see
what's on this time on this channel and
they're navigating using their slow
remote control what we're able to do
actually is immediately have that class
available instead of having to load it
on demand we don't want the network
traffic to load the class on demand so
we load it all at boot time so all of
our classes are actually loaded at boot
time and that that improves the customer
experience garbage collection we want to
keep small chunks we don't want to wait
too long to do the garbage collection
the longer you wait the larger the
chunks the longer it takes to
consolidate and move the memory around
so due to the fast responsiveness that
we need on the clients we may we make
sure that our applications try to keep
things in small chunks it's basically
the same as a mobile JVM strategy it's
really bias to keep the amount of
garbage small threading model we can't
let the operating system itself linux OS
threading is fine but the JVM level that
there threading priorities don't
necessarily match
we want our threading to be so we we
make sure we are in control of the
thread priorities not just the OS thread
priorities but a java-based
prioritization and then object creation
sort of goes along with memory
management here memory allocation needs
to be smart again really goes back to
garbage collection and the capabilities
of the application where the application
needs to be smart enough to to not do a
lot of care arrays and string buffering
and adding two strings because that's
actually a pretty expensive operation
those operations not free so that really
helps us with the memory management and
in n.o cap okay I peck slits are pretty
much exactly like Java at X the applet
sorry excuse me do the oak app X let's
included in it start pause and destroy
and the excellent context is that's
passed in the in it is guaranteed global
in scope but you have to store it off
right away don't try and use it before
attacks switch or else it won't be valid
when you try and use it later on play
close attention to all of the created
objects and that way you can deal with
them in the destroy excellent method and
have reusability and reloading of your
actual X let's removal and reloading of
them so some of the deployment lessons
we've learned some of the issues we
dealt with that the client and server
layers are here remember one of the
things we have to deal with is the fact
that bandwidth is limited we we know
that everyone likes to say well the
Internet is always on but you know what
for us it's really not always on our
remotes are slow like I said earlier we
have to pay very close attention to
exception handling and not miss a
nullpointerexception anywhere else that
usually means bad things no one likes
their TV to have to be rebooted in the
middle of the day
so you know blue screens are bad right
so we don't have any blue screens or at
least we try not to actually we end up
with black screens but that's a
different story and when we do end up
with black screens we do extensive
logging for troubleshooting and
production environment so targeting with
trace builds is essential that's one of
the features that we provided in our
Oakapple implementation is the ability
to target directly in an individual set
top in a specific site in a specific
home if we want to do troubleshooting
operational monitoring and back-office
alarms is critical and one of the things
that we also do a cox's we deploy to
field trial first and we make sure that
our field trial support staff are very
well versed with the features of the
product so we put the wrong button there
there we go we believe we have the best
guide around especially based on our
performance and featured services based
on that that knowledge I'll have Luke
come up here and discuss the client
scalability optimizations Luke thanks
Steve my name is Luke Warren I am a
design engineer and for cox
communications in their electronic
program guide set-top box development
group is there anyone here that is
familiar with cable television
technology okay so you and people i know
you are so no one's really familiar with
Oh Kapoor the Middle where they run on
okay well scalability in cable
television is not just about designing
your back-end infrastructure to handle
load it's also critical how utilize the
available bandwidth we have especially
our upstream bandwidth from the set-top
box back up to the backend I want to
show you guys some of the techniques and
Java tricks that were used to improve
our messaging efficiency with our
backend what we refer to as our video
middleware
so I thought that Steve hadn't is in his
presentation a little bit about how we
can zip up our entire java application
all the class files and everything and
download just one giant zip file to the
set-top box and explode it so we can run
it but before we were able to do that
the set-top box used to have to download
each individual class file and each
individual image in the guide one class
at a time with a separate HTTP request
in a rich you I like an electronic
program guide there are hundreds of
images so by compositing all of the
images together and a single larger
image we eliminated hundreds of HTTP
requests on potentially hundreds of
thousands of set-top boxes to do this we
create an array of coordinates for each
sub image and then each sub image had a
constant that was an index into the
array the represented where its
coordinates were in the array and then
we had a vit a method that bleed the
image from system memory over to video
memory and we had some double buffering
in the JVM that allowed it to perform
very nicely so that was one neat trick
in the version of Java that we were
using a no and everyone's here to enjoy
the new neat new Java technologies or
come out we're still on Java 1 dot 4
that's part of the old cap speck in the
client only on the client and look like
so in Java 123 what we started and then
one dot 4 we are now there was no
configurable socket timeout value or at
least not in our JVM there's not so if
you would open to socket to an
unresponsive server the cycle was simply
hang their open in a wait state until
the default timeout came which on our
JVM was three minutes
so to work around that limitation we
created a wrapper class that scheduled a
timer to disconnect the socket after a
configurable time out that we will pass
in so pretty straightforward how it
works just when the time I ran we've run
that connection disconnect and we were
actually doing good stuff if we felt
like there was good work going on we
would put the timeout off just in case
we're downloading a big file something
else interesting about the cable
television environment is when we
perform an upgrade will will upgrade a
whole bunch of set-top boxes all at the
same time in a maintenance window at
night this causes a boot storm on the
back end so to maximize scalability of
our back office we implement an
algorithm that generates a random delay
for retrieving the largest chunk of data
our set-top boxes use which is the guide
data so depending on the size of you
know the scale of the number of the
population the set-top boxes we could
increase the delay that we want each set
top box to wait randomly before it
downloads of skype data and usually that
we would do something between you know 0
and 10 minutes so
some of the services we provide to
customers result in a high volume of
small messages so we have millions of
customers guaranteeing delivery of all
these messages during peak load using TP
is TCP is just not feasible so for a
product like TV caller ID where you get
a phone call on your bundled telephone
and cable sitting cable subscription if
we deliver that we need so the delivery
of them if the delivery of the TV caller
ID banner has not occurred when before
you've answered the phone then
essentially the message is outdated and
useless so what we do is for TV
colouring in particular we generate
small little packets of data these
little UDP datagrams that are
fire-and-forget and they're real easy to
digest by the client that allows us to
generate a tremendous number of messages
and use a small amount of bandwidth to
do it to support some of the features we
do we have like integrated content
search we're on the set top box you can
go and you can search for you can search
for things that are playing by keyword
like programs in our vodka log or
on-demand catalog linear broadcast
program and and DVR recordings the setup
box has to pass a lot of data up to the
video middleware especially for its DVR
recordings we represent each of those
DVR recordings as a as a Java object and
to transmit all that data up to the back
end we use a binary serialization
technique to transmit those objects to
the middleware and that saves us a lot
of upstream bandwidth as well along
those same lines is the previous slide
to ensure the DVR recording list is in
sync with the video middleware we
generate a list of recording IDs on the
on the DVR and then we have
then using sha-1 and that generates a
hundred and sixty bit message that can
tell us if the DVR and the servers are
in sync with a very small message so
that's all I have the tricks and
techniques to show you from the client
side now Donnell is going to get up and
explain a little bit about we've done
all go down the back end all right so
I'm going to start out with the
definition of scalability in the middle
of our presentation I know what we're
talking about is the ability to
accommodate growth or accomplish more
work by increasing resources and when
we're talking about scale we we
sometimes we're talking about data
sometimes we're talking about how many
transactions or messages or compute
cycles you can you can achieve sometimes
we're talking about the number of
connections you can support so in this
case I'm talking about the number of
connections and there are different
approaches to scale most of you have
probably heard of scaling up scaling out
and there's another dimension so is not
directly related to scaling but it
impacts scaling greatly and that's the
efficiency of your system so sort of
like the scaling coefficient it doesn't
change the ratio but it impacts how well
your application is scalable so things
I'm going to try to cover our issues
addressing scaling out and issues
addressing efficiency and I'm going to
try to make this more about Java than
anything else although I'm going to
start out with sort of a high-level
architectural overview here so I'm
calling this a hypothetical scalable
architecture where you you've got a
caching layer you've got some app
servers and you've got you've got a
database cluster for example and in
order to achieve scalability you want
your synchronous calls to be very quick
because if you make them quick you can
handle more connections right so part of
the
quickness of your synchronous requests
is determined by how far back into the
end of the system they have to travel
and one of the ways you can address the
synchronous requests is by decoupling
them using messaging making sure that
your longest running processes do not
impact your client facing services by
having them on their own servers and
treating different data differently
avoiding replication this is something
most of you probably are familiar with
as well if you can avoid replication you
can scale more horizontally without
having to deal with more complex issues
like Federation and you want to make
your request cash friendly these last
two are the ones i'm going to mainly
focus on and of course you want to be
able to collect metrics into end of your
steady state so you have something to
compare with when you have issues so
what's that aeo isn't that always the
case so having a good architectures
where you may start out with it's
essential but you also have to have good
code so if you have bad code good
architecture you're still not going to
scale and the things I'm going to try to
cover our how you can make your request
more cash freely and synchronization
contention and replication and how that
relates with understanding your
underlying frameworks so writing cash
friendly services so by web caching and
caching I'm referring to what is
commonly known as reverse proxies and
these are technologies like squid or
varnish or a patchy traffic server or a
content delivery network so typical web
caching they also involve the clients
whether their browsers or their a java
application running on a set-top box
that's talking HTTP the reason that
caching works is it skills very well it
shields your application servers which
are maybe more complex and harder to
scale and operations love these
typically because they're very easy to
maintain and they don't require a lot of
configuration or in-depth knowledge of
how they function
one thing you want to do regarding your
caching strategy is have one so so you
can't just go out and put caching
servers in front of your infrastructure
and expect to get the most benefit from
them you really need to plan it out what
helps is understanding the different
data characteristics their tolerance for
stale data and frequency of updates that
kind of thing and to that end you should
consider using an event based approach
to cass of cash eviction and
synchronization and most caching servers
technology support the concept of a
purge some of them support the concept
of an HTTP cache channel which allows
the caching servers to subscribe to a
feed for how frequently they are
refreshed and I threw edge site includes
in here because it's a really cool
technology it's not always applicable
but what an edge site include allows you
to do is specify parts of a response
that are not catchable so you don't have
to throw out the whole request as being
uncatchable because just one element of
it is so and the rule of thumb here is
the fastest query you can write is the
one that you don't need to run obviously
and some questions you may ask about
your API while you're doing the design
is can you widen the scope of the
parameters that you're passing in so
hypothetically if you had some XML data
and you're passing in a user ID to look
up some reference data in turn and find
out what information is related to that
user you might be able to get away with
coarsening that request in making the
request much more casual and the other
approach that I wanted to mention was
supporting HTTP cache headers so this is
very important for obtaining decent cash
ability and typically what you're
talking about is adding a cache control
header and in this case I've got the max
age and channel support and expires last
modified and etag are different things
you can add to your response so that
clients know how to treat your cash
cacheable responses and intermediaries
and web caching servers do as well so
expires basically you put a tag on it on
your content that says when this content
is going to expire last modified you're
telling the clients and caching servers
when your content was last touched and
etag your hashing the content so that
the client can ask questions about is
this the same content that I have today
and conditional get so in this case the
client can use information it has
whether it's the expires header or the
e-tag header to ask for data if it has
changed so in this case you would get a
30 for not modified now that's good from
a scalability perspective because it
means that there's data you don't have
to send back and if you're making your
request coarse grain and you're not
implementing a chatty protocol being
able to say I don't need to send data to
you what you have is good is a valuable
thing and as a Java developer how would
you do this well like anything else
there's a lot of different ways to do
this and in this case if you're running
a tomcat container something like jboss
web or apache tomcat there is a servlet
filter called the expires filter and
it's modeled after the apache mod
headers module and the thing that's nice
about this is it doesn't require code
changes the syntax for determining how
long you're going to catch something is
very intuitive and simple and the
drawbacks are it's static so you can't
do anything that intelligent with this
approach unless you're modifying it
dynamically so a similar a little bit
more dynamic approach is using an eh
cash module so anyone familiar with each
cash yeah so essentially to Java object
caching solution and it has a servlet
filter that's similar to the last one we
looked at it's called the simple caching
headers page caching filter which there
is this there's a shorter name but I
like this one the most so I
who did this one and what you do is you
set up your servlet filter and your
web.xml and you point it to a cache
configuration so in this case I've got
this filter looking for calls to index
dot XML and looking up that information
in my cash and the cool thing about this
is it's a little bit more dynamic and
you can get if not modified sense and
etag support from it and a third option
i'm going to show is if you're building
a rest interface and you're using jax-rs
you can do this programmatically so you
annotate your code as you usually would
and you create an object called a cache
control object and you populate that
cash control object with information
about your cache directives right so in
this case I'm not doing anything
interesting here but I'm just
demonstrating the code but what's useful
about this is that the my entity the me
object here is looking at its version
information from the database so you're
actually building your cache headers
based on what you know about the object
so getting a little bit closer to more
of a intelligent caching procedure here
and there's lots of other ways to do
this this is just three right now so
next topic I want to cover is understand
understanding your framework behavior
and this is tying into synchronization
lock contention and two-phase commit
here and so I like this quote from a
research paper regarding using Java
middleware as a way of replicating
replicating state across nodes and
basically it says you're going to have
disastrous results if you try to do this
and you can set up an experiment using a
tool like dummy net to simulate packet
loss and collision to verify that the
statement is accurate or not and the
reasons you may want to avoid a
two-phase commit is because it puts you
in a position where you're using a group
multicast technology that requires a lot
of expertise at the network layer and it
requires a lot of vigilance to keep up
with network changes
and packet collisions and little network
issues can lead to severe issues with
your cluster and finally if you're using
the same heap to serve your client
facing service traffic as you are to
replicate which is the case with the
Java situation because it's running in
one process maybe that's not such a good
idea so most people agree they take it
on face value that two-phase commit is
not the best thing to do so why are we
talking about it where am i bringing it
up because it pops up in your in your
applications in ways that you may not
expect it to so in this case I've seen
it pop up in persistent durable JMS
messaging solutions object relational
mapping cash replication the distributed
singleton that some folks like to use in
an application cluster and of course
transactions that are supposed to span
multiple resources you may not think
you're using it but you need to verify
that you're not using it so along the
same lines I'm putting a lock contention
so when you have a highly highly used
service it's under a lot of load you
will notice that lock contention becomes
more significant right so one of the
issues with this as well is that a lot
of developers don't pay attention to
handling interrupts rollback situations
there's not a really good way to test it
in a lot of circumstances if you use a
tool i mentioned dummy net you can
reproduce conditions that will cause
your application to fail and then you
can exercise those parts of your code
that usually aren't tested as developers
we're told not to spawn threads in your
container and that's probably good
advice so the question I have is if all
these things are true why do you have to
worry about threads if you're a server
side java developer at all and i'm going
to say that the best reason to worry
about threads and to understand you're
threading model is because the libraries
that you use use synchronization and
they have thread contention and they
have different kinds of behavior
so I've listed a couple of things here
but I want to do sort of an experiment
that I set up here so play along with me
if you don't mind so let's start out
with a simple table right we got we've
got two columns okay it's an oracle
style database that supports sequences
and we've set up a sequence which we're
incrementing by 48 so that we can be a
little bit more efficient 48s purely
arbitrary here just picked it okay and
we have our standard plain vanilla
persistence.xml and if you haven't seen
this before you can take it as an
article of faith that this is sort of
standard okay nothing too crazy going on
here just setting up your object
relational mapping in your persistence
file and here is a plain old java object
annotated to support the sequence
generator that we created so that's the
sequence we created we're using a
sequence generator type everything looks
normal to me and here's a driver code
that's going to set up our example here
we're going to loop through creating 48
objects persisting them and then at the
end of it we flush for posterity close
the transaction commits it close the
connection and move on okay so let's
let's look at this thing running
hypothetically what happens first we see
a call to the database select my
sequence get the next value from deuel
this looks like standard standard stuff
you would see everything looks good
anyone want to guess what the idea is
takers Steve well I would expect it to
be 48 for true or false does anyone know
takers the sequence was 48 initial 248
okay yeah maybe has he seen his he's in
your presentation no hold on I didn't
make this up I hope people have run into
this before or it wouldn't be useful
okay so what it is is to 2304 and and
what is this sequence high-low generator
under the covers that we're seeing this
is an implementation okay why is it
2,300 always it start there was it do
next okay okay it's at least it's
sequential but we don't have an answer
the first question of why it's there is
that like / 48 or a multiplication okay
yeah so this is what your your table
looks like now you just inserted these
items now go back to our archetypal
persistence.xml file here we're going to
add one line to the xml here right there
boom we're going to add this thing that
says new generator mappings please set
that to true why are we doing this
because we read it on the internet and a
blog or something it we're doing this
because it sounded like a cool thing to
do right why not right so now what do we
see well we rerun the test we didn't
clear out the database but we're not
doing anything different there's our
next valve from our sequence everything
looks the same up now it's printing out
96 that sounds more like what we'd
expect now we've got 48 sequence style
generator 49 50 and then that's what our
table looks like
so the next question is what does this
have to do with scaling that's a good
question so keep in mind the two
different sequence generators how we
tweaked it using the XML how it was
pretty much a nebulous we're just going
to try this out thing and then boom one
of the implementations here is
synchronized one of them is not which
one do you want to run on your servers
well that might not be a clear answer
right now but the point is that simply
by changing a configuration file that
seemed like add nothing to do with
scalability you've changed the behavior
of your application possibly drastically
in this case it would have been for the
better but if you went the other way
around and you saw that setting and said
let's remove this because I don't know
what it does you would now be trying to
take a lock on something that generates
IDs for everything that writes to this
table so here's another example that I
put in here so imagine that your class
loader is blocking on a thread and that
thread is in the JVM and you do not know
what that thread points to because this
is a bug that was present between 160 10
and dot 12 the point here is that this
kind of thing could drastically impact
your applications performance on the
server side and is fairly beyond your
control or so it seems right another
example again where you're using an
object cache life is good it's a it's a
great cash it's using lock striping for
efficiency you're adding lots of objects
to it and then BAM the eviction thread
grabs a lock on the collection and your
rights timeout and your transaction
attempts to roll back right so what's
the point of all this stuff well the
point is that as java developers we tend
to leverage a lot of libraries unless
an application server developer you're
using libraries in your application and
that's considered a good thing from a
software development software
engineering perspective right but it's
upon you as a developer to understand
how this library behaves so these are
some questions that you need to ask
about your library or I think you should
ask so if it involves something that's
got a timeout what happens when the
thing times out what's the default
behavior is their source available for
you to inspect if you need to and have
you looked at release notes or known
issues in the release you're using and
then you can ask what what libraries
does this library use and that's a
recursive tell call in this case and you
just keep doing that so that you know
your application now I'm going to switch
gears here and talk about scaling
horizontally and one of the things that
I've observed is that scaling
horizontally is a very popular approach
to doing things but what I'm going to
say here is that you really have to plan
out your scaling strategy it's not
something for free do you have the lead
times figured out it takes to add
additional servers to your system have
you figured out if your IPS have enough
range to accommodate your growth do the
ports on your load balancer firewall
whatever accommodate what you're trying
to do and finally I go I'd like to go
over a couple of scaling strategies so
we talked about web caching that plays a
huge part of scaling that you should
plan what you're going to do avoid
distributed transactions if you can and
look for them where you may not expect
them to be and handle synchronous
requests as quickly as possible and even
if that means that sometimes you have to
break your processes up into a
synchronous baking dynamic data I didn't
talk about but we've seen that be very
effective and understanding the behavior
at capacity so what is your application
do when your cues are full or the
connections lowes or you're dropping
some packets
and then you should organize your data
based on its characteristics and not
treated all the same every
infrastructure I've seen has some brutal
component usually it's a legacy
component and you don't want those to
become a bottleneck so you have to have
a strategy for isolating your services
to protect it from brittle brittle
components and disabling keeper lives so
people lives are great when they're
appropriate but they impact the number
of sockets that you can you can keep to
support high number of connections and
what we said in the beginning was you
should be collecting your metrics at
steady state and I put on here a couple
of libraries and frameworks that are
interesting and have some promise and a
couple of tools that I found useful in
scaling in an architecture so I
mentioned dummy net it's actually part
of the freebsd kernel and if you're
running Linux you can get a port of it
and install it manually and it lets you
set parameters like what your bandwidth
should be how many packets should
actually succeed and if you set up
virtual machines on your development
environment and use this tool to
simulate packet failures you can find
some interesting stuff so some of these
problems we used some high power load
testing tools and had trouble
identifying it because by the time it
gets to the point where this happens and
everything falls apart you know you're
picking up the pieces but with something
like this tool you can actually go out
and test it first and look for that
failure and you'll know what happens
when packets get dropped and some of
these other tools are statistics based
so you're collecting all these metrics
we talked about these other tools are
about how do you query all this data you
have and I'm a big closure fan and
kaskell log is a dialect for querying
Hadoop in a couple lines versus actually
writing a ton of Java code
and RN and canter are statistics
packages for processing so thank you
very much that's it I guess we have
questions now any questions comments
how much how long do we prepare for this
presentation more than it seems trust me
well I'm sorry go ahead how light so we
started actually with trio itself
started back in 08 where it was
conceptualized and then we we built the
client and server components starting
you know nine I think and that was so
we've so it's been a three-year project
yeah you find out these things really
quickly you know I mean if you're
dealing with volume a lot of companies
when they're approaching a scale issue
they're starting and growing and they
have growing pains but when you start
out offering a service where you know
you're going to have X number of clients
connected then you kind of have to come
to the table with a plan and you know
you have to come in with low tests and
stuff yes question
what size of package oh yeah that's
that's a good question and you so it
varies obviously / update with the large
graphics we have and we do an entire
replacement for all the components it's
it's quite hefty actually what I mean it
can't be more than a 100 megabyte no
it's it's not it's it's it's less than
50 but megabytes but it's it's on the
order of 40 to 50 meters about 1500
class files yeah it's quite a large
amount of code and data thing like that
and again because we're you know we're
compiling everything and making it
available on food that's that's an
important component sorry another
question yeah yes
of the set-top the CPU so the where we
have it depends on the the set-top box
but it's a it's a pretty powerful cpu I
think we have a was a two mega flop or
something like was our 2gig a flop
whatever that is I'm not sure what the
actual specs are but it's fairly
powerful with minimum like 512 megabytes
of DRAM and oh yeah so the the actual
power is not the power the cpu is
important but you know it's not it's not
one of the most important things the
ones that we're using lately I think are
on the order of two thousand mips so
we've got it's pretty powerful you you
know I I didn't put one up here but I
think we can if you if you want I can
certainly send it to you so so let me
know give tell you what we have to leave
doubt because they're asking us to yeah
we can continue talking outside if you
want yeah thanks again for showing up
this late i really appreciate it thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>