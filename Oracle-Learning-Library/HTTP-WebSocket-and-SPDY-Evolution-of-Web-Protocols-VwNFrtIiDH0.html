<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HTTP, WebSocket, and SPDY: Evolution of Web Protocols | Coder Coacher - Coaching Coders</title><meta content="HTTP, WebSocket, and SPDY: Evolution of Web Protocols - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>HTTP, WebSocket, and SPDY: Evolution of Web Protocols</b></h2><h5 class="post__date">2013-01-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VwNFrtIiDH0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody to our talk on the
evolution of web protocols my name's
Greg Wilkins I'm a senior architect at
Antalya web tied and the founding
developer of the jetty HTTP server and
servlet container which I've been doing
since 1995 and I'm also on all of the
standards bodies for developing service
api's and been helping out with the
WebSockets and now the HTTP to effort
speaking with me today is Simoni
Bourdais he's the lead developer on the
comedy push framework and also a vital
contributor to the jetty project Simoni
is written in our speedy connector so I
kind of represent the past of jetty and
Simoni is the future and it was kind of
a good metaphor for the talk we're doing
today what we're going to be doing is
looking a little bit as quickly as we
can in the limited time the history of
the HTTP protocol and how it's got us to
where we are today so we can look at how
it's evolved and how its continuing to
evolve because some big big changes are
actually happening under our feet at the
moment with the http/2 effort the speedy
effort and the WebSocket effort and so
the idea is to look at how the the
changing web load has made the protocol
adapt and to look at how it is currently
adapting again to give us better
semantics and to give us better
capabilities and scalability so if you
look at this to frame the way that the
web has evolved and has to continue
evolve
we've looked at a fairly typical
middle-of-the-road
website the w3c dot or website which
you'd hope would be up there with web
standards and whatnot and way back in
1996 when jetty was one year old the
sort of websites that you were serving
were a single HTML file in this case 600
bytes
okay that's a bit on the small side
there was bigger content than that but
they weren't rich they weren't complex
and they were just single files and
that's the environment in which our
protocol HTTP was designed
that was the other thinking are we just
serving a document it's got links to
take to other documents come 2002 the
web's got a little bit more complex and
we're looking at having some images in
there some styles and there to make the
web book a little prettier and we're
looking in this case there's five
resources that the website loads the
HTML is now grown to 31 K it's it's time
to grow but fast forward all the way to
2012 and the number of resources that an
average web page is going to suck in in
order to display and look good and
satisfy the marketers and whatnot is
about in this case about about 40 38 or
so the HTML still pretty much the same
size it's still the content but in order
to make it look pretty
there's 31 images that have been coming
in with it in order to make it our wait
also for style sheets in order to make
it dynamic and responsive there's a
couple of JavaScript files have been put
in there and this is a pretty
run-of-the-mill website in terms of it's
not really up there with the latest and
greatest of fashions in web 2.0 what
they call it it doesn't have a chat room
it doesn't tell you what other members
of the w3c online at the moment and want
to chat to you it doesn't tell you that
your Smurf is starving you have to buy a
wheelbarrow full of food or anything
like that it's so it's it's a fairly
stock standard website and if you
actually looked at what the average is
the average is for a website nowadays is
80 resources get loaded to make up a
typical page on a website so at 38 it's
well below the average but it's good
enough for us to use for some modeling
to see you know how HTTP can cope with
this increased load that we're putting
it under so if we chat it up it's kind
of exponential growth although the the
scales nah linear and that's both in the
size of the files and the number of the
files let's just say it's growing a lot
and so how is HTTP a protocol designed
in the early 1990s coped with this
growth in part it's been helped by the
fact but the networks have got faster so
back in 1990s we're talking about you
know 2400 baud modems and now
about you know you know 4 megabit
downloads on DSL is is kind of low if
you get faster as well Layton sees I'm
being very generous here Layton sees
back in the 1990s I've said well you
know in the hundred mills they were
probably more like 300 400 mil and
milliseconds
and now I'm saying they're about 50
milliseconds on average which is
probably good for all you guys that live
in the United States but for us southern
hemisphere Ian's 50 milliseconds is
something we aspire to you know 75 is
lovely 90s chemical billets let's say
it's 50 so we've got some growth and
capacity of the network has that been
enough well if HDPE had not changed
since 1996 it wouldn't have been enough
with we crunched the numbers and looked
at all the components that make up our
load time of a webpage which is
establishing a connection we've got to
do a you know ask it where the server
wants to take the connection we have to
warm that connection up because
initially we get a slow start flow
control window which is quite small and
it's only after we've sent a bit of data
does that window grow and grow and grow
until it meets the capacity of our
physical connection so just because
you've got a 20-megaton load capacity
doesn't mean you didn't get that until
you've warm that TCP connection up we
have to look at the latency it takes to
send a request and get the response and
then we have to look at the data rate as
it streams that data down to you so if
you crunch all those numbers through a
spreadsheet looking at the content we
had in 1986 with HTTP 1.0 and look at
the content we have now with today's
network and HD 1.0 we can see that the
load times would have grown from about a
second back in 1986 to two and a half
seconds for this fairly simple average
web page with 38 resources if it had the
80 resources an average page would have
it be about 5 seconds so if we're
striving for a moderate dynamic
interactive web going from a one-second
load time to a five-second load time
it's kind of trending you know grossly
in the wrong direction so we've had to
do something else to make the web keep
pace with the growth in content
yes so the content growth is definitely
outstripping the growth in the network
and we can't look to just having faster
networks to make HTTP work for us so how
is HTTP managed to keep pace we've put
some tricks into there or we've improved
that the specification it's the other
way we improve the specification and
then we had to resort to tricks so the
first thing we did was we came up with
icd-10 and its main innovation was to
standardize the keepalive connections
that crept into HD 1.0
deivis persistent connections so after
we've gone to the effort of cracking
open a connection and warm starting up
that flow control we don't then throw
that connection away we can then use it
for another request so we can send
multiple requests down the one
connection and we allow every browser to
have two connections to the server so
therefore we have two warmed-up
connections to send lots of requests
over and this has helped a fair bit if
we crunch the numbers
we've gone from only a second in 1986 to
HD you end up one on its own.you one and
a half seconds for this fairly average
or below average run the mill page so
it's better but it's not really good
enough we need to be faster than we were
in 1996 because we're going to be more
dynamic interactive blah blah blah so
we've had to do something else so the
next attempt was to in the specification
for 1.1 there was a concept of
pipelining rather than sending a request
over your your connection your precious
connection that you've warmed up and now
going very fast and then waiting for the
response to come back and all that time
you're waiting you can't send any other
data and when the response is coming
back you can't send a request you have
to wait and the responses come back
until you send the next request
pipelining enabled us to send lots of
requests down the one connection and
receive lots of responses back so we got
rid of that latency that round-trip time
between waiting for a response before we
could send the next request we've got 80
resources we've got to go fetch we don't
have to wait a round-trip time for each
of those 80 or two connections for T to
get the
connections so pipelining oops I've
missed a couple slides avoids these
round-trip times by allowing us to send
all the requests at once and we can see
that the numbers work out that that
pipelining gives give us that sort of
increase in or decrease in latency that
we're looking for the only problem is it
doesn't really work
there are semantic issues with pipelines
that in the most case they kind of work
but when you have non idempotent
requests like posts or gets that have
query parameters that actually cause
things to happen then if you have a you
know 80 odd outstanding requests and the
connection closes the browser has no
idea which of those requests were
actually made it to the server were
acted upon so it has no idea which are
the ones that should retry and you know
so it doesn't do a post twice and
doesn't bill your credit card twice and
things like that and while in specific
cases pipelining can be made to work and
in some cases like with mobile browsers
they just said I'll damn it we'll turn
it on anyway for the most part for the
browsers you're on your desk type top
pipelining is turned off because it's
just too problematic so why it would
work for the latency issues that we have
it doesn't it's just not there it's not
available for us as a solution to make
our web faster so the next thing we've
gone to is basically a hack we've just
started our specifically au stems be
damned they say two connections where I
go for six the browser Connect vendors
sort of worked out that well if I've got
two connections that I have to get my 80
resources over well it'd be a lot faster
if rather than sitting two at a time I
had more connections let's have six
connections and we'll get six resources
at a time we'll have six slow start
windows that we can use to get bandwidth
initially and things will go a lot
faster and indeed they do
uber I think it makes Boris the first
one to blink and break the spec and go
for 6-hour connections and that rapidly
everyone else followed and we get
basically a little bit faster than where
we were in 1996 although this was an ad
resource one rather than a 38 resource
one
probably be a little bit slower but that
is the fundamental trick that we've
actually resorted to to make the web
keep pace with the content growth to
make above and beyond the network
capacity growth but the problem with
this trick is that connections have a
cost if any of you have tried to use the
Wi-Fi network here at this conference
you'll probably understand what I mean
by connections have a cost if you wanna
the lucky few that get a connection for
the Wi-Fi it seems to work we're sort of
wrapped in a reasonable throughput but
the main thing is the infrastructure
just can't sustain the number of
connections that all our devices may
need connections each for every pump
host and if you've got multiple hosts
you're talking to six to each of them it
just runs out of connections and so
therefore it doesn't give you a
connection and you just hang there were
still your browser thinks it needs six
connections to render a page it might
get one or two off the infrastructure
and then get refused those other four or
normally doesn't get refused it just
goes into a nice long wait so what you
get is in this room of a hundred people
if you all got your devices out and try
to surf the web probably 80 of you would
get some connections to have a almost
but not quite working browser so you're
all 80 of you would have really sucky
performance maybe say twenty of you
would get your six connections and get
some sort of things so it's it's it's
not a good way to go connections have a
cost and what those just really
enumerate what those costs are so when
we look at the speedy in HTTP 2.0 we
realize what those are firstly on the
server side every connections going to
have to have buffers allocated and
internal resources allocated to it
they're not free so if you have six
times the number of users connected to
your server that's six times the memory
that you're gonna have two calloc eight
to them you're select sets are going to
be six times larger that you have to
select over if you're silly enough to
allocate threads bare that you might
have six times the number of threads
allocated it gives you other problems as
well because if you're in a cluster with
a load balancer it's taking in
connections
the last thing you want to have is all
the connections from one user ending up
on different nodes in your cluster
because then you're gonna have to have
either a completely stateless web
application which actually is a good
thing to aspire to but not many people
achieve it all you have to have session
state and other application state that's
very mobile across your cluster so it
can go from node to node to node to node
or you're gonna have to have a smart
load balancer that know is that this
connection is associate with that
connection so I'll put them all on the
same server and and you get stickiness
and look inside it and and it's yeah it
makes the lobby job of your load
balancer a lot more difficult it's also
makes the job of your application a
little bit more difficult because you
can get six requests landing on your
application server not necessarily in
the order they were sent they can take
different paths of the network and they
can overtake each other and they all
land on the same user object or session
object at the same time which is not bad
because we all write thread safe code
right no one's ever made any mistakes
there so yeah if you write your code
correct it works the if you've got one
user object running in six threads on
six different cores then your caches
aren't going to work very well and just
things are gonna go a little bit slower
they really should for a server point of
view it's much better to to take a bit
more latency for one user and run all
these jobs on one core and run unrelated
jobs for another user on another core
you don't want to spread one user across
to all your cause it just makes you run
slow for everybody and also it's that's
an arms race if six is good well 12 is
going to be better habit 18 let's go for
32 connections we'll go real fast
and um basically this arms race has
already started people set up browsers
only giving me six connections to the
hosts what a mess if I give the host two
different names oh I'll call it you know
ww1 and ww2 acne calm and so you're
getting domain sharding happening where
people are using different domain names
just really to grab more share of that
slows that window more share of the
server resources which works really well
until other people start doing it so
okay and all of a sudden we're all in
this arms race we're all going for more
connections and the service getting
slower and slower and and more buffers
wasted and concurrency issues it's not
the way to go to solve this problem
what's worse is our web pages there's no
sign of them getting less complex or
getting less large or left me more
faults the growth is continuing they're
doing fancier and fancier things and and
with this new semantics that we've got I
mean that w3c org is a very it doesn't
have any chat rooms it doesn't you know
have any games in there it doesn't do
anything to you doesn't tell you your
stock prices but that's the sort of
thing that a lot of our websites are
doing so they're getting the the growth
in the content is continuing an HTTP has
taken all the easy wins it's had a
couple of misses of wins that they
thought were easy which have turned out
to be wrong and now we're breaking the
rules we're breaking the rules just to
keep pace with serving boring old static
web pages little lone serving dynamic
new web apps with new semantics so the
natives are getting restless HTTP is no
longer good enough it's not providing us
the semantics we need nor is it
performing the giving us of the
efficiencies of is need so what has
already happened this is not happening
or going to happened what has already
happened is HTTPS been replaced to a
large extent and we're gonna look at the
two protocols that have been started by
frustrated groups and now in the
standardization process to give us the
new semantics that we want for dynamic
web applications and to give the new
efficiencies we want so we can continue
to grow and do other things so it's
WebSockets is giving us the new
semantics and speedy is giving us our
better efficiency and there may be some
coming together in the future over HTTP
2.0 which we'll get to in the end so the
first of these I'm going to look at
WebSockets and then throw to am Simoni
over here to tell you I'm speedy I think
there's been in quite a few WebSocket
talks happening so I'm not going to go
in great deal detail of WebSockets
getting a quick overview and then just
show you the huge benefit you can get by
switching over to WebSockets and then we
can look at the same sort of
in speedy so WebSockets came from the
browser vendors they wanted to provide
us this new semantics to allow low
latency bi-directional communication
from the server to the client so we knew
that our Smurfs were starving or we knew
that our stock price has changed or we
could chat to that person that didn't
have SMS or something that effect they
wanted to do this because the browser
vendors are very interested in keeping
people in side the understand it's
process they like open standards because
that's their defense against Java
effects Silverlight flash they want
people they want their browser to be
where we run all our applications and
they want to get rid of the hacks that
we're doing over HTTP like long polling
and forever frame and and using animated
gifts to send bi-directional
communication believe it or not and get
us back into an open standards world so
that's where the the initiative came
from and it's now been through some
standards process the IETF has
standardized the wire protocol and RFC
six four five five after a long
drawn-out knocking down battle the w3c
has in their html5 has standardized the
the API in the JavaScript although every
time I look it seems to have a few more
methods I'm not too sure about what
their process is about freezing html5
and now currently there's a JCP jasa
three five six has been started to
standardize the java api for the server
side of looking at WebSockets and I
think there's a session on that at four
o'clock today with Danny coward although
it's full up so if you're not going too
late so what is the semantics that we're
getting to give us these dynamic
websites
well what WebSockets does is it replaces
the HTTP semantics which is your restful
weo get put post with content home types
semantics with a Datagram semantics with
a little bit of connection oriented
stuff lay it over the top so with a
WebSocket you can know that there's a
non open and a non closed event so you
know someone's connected to me someone's
not connected to me there's on message
events to say a message has arrived do
something with it
and there's send event our methods so we
can send messages by the risk text Oris
datagrams
and that's basically it there is no
relationship between the outgoing
messages or the incoming messages
there's no request response paradigm
it's just bi-directional messages no
need to wait for any responses just send
as many messages you like and and
receive them and handle the messages
that you get and that's the brilliant
semantics for doing such things as stop
tickers and chat rooms and all these
wonderful things and the way we get
there is we start off in HTTP we open up
at port 80 or 443 for an SSL and we send
an HTTP request because on port port 80
belongs to HTTP and HTTPS site to have
an upgrade mechanism to allow us to
upgrade supposedly only to a new version
of HTTP but this has been subvert a
little bit to go upgrade to entirely
different wire protocol so if in the
magic headers that you got got down
there at the bottom you convince the
server that you're mature enough to
speak WebSockets and the server is
clever enough not to be hoodwinked into
accepting a WebSocket connection when
it's not it sends back this response
r101 switching protocols response which
is part of HTTP the moment the carriage
return line feed at the end of that
response goes out you are no longer
speaking HTTP you're now speaking a
completely different wire protocol
called WebSockets which has been
designed by only focused people who hate
overheads with them framing it's a
binary framing protocol which basically
for the average frame will only have two
bytes overhead to send arm data you can
send a big message which gets broken
down into little frames although the the
maximum frame size is the number of
electrons in the universe or some data
effects because you can have a variable
length encoding it's but essentially
it's a header that says the type of the
frame be a control frame for closing or
or whatever or data frame text or binary
here's the length and there's the
it's very very compact it's very low
latency very easy to send and using this
can have great effects here is um how
we've achieved chat using HTTP on jetty
and comedy protocol this is using the
HTTP transport for bi-directional
communication between the server and the
client and we are really quite pleased
with the numbers we get perverting HTTP
to our own purposes jenny has a
synchronous service in there so it can
scale really well for lots of
connections and comment dee has learned
how to use those to great effect so if
we look at those sort of numbers we can
get out of a this chat simulation this
is running on a couple of ec2 nodes and
we do actually have real clients and
users doing social media who get numbers
that very closely match this so these it
goes into into reality we can see that
we get we've modeled 5,000 10,000 20,000
and 50,000 users connected to the same
server and we've put about 50,000
messages a second through that server
and we've measured the latency that we
get for delivering those messages to all
those up to 50,000 users and if we chart
that out we can see that the latency
that we're getting is in the order of
you know hundreds of milliseconds here
most of them here we're getting it under
200 milliseconds with occasionally you
know up to 800 milliseconds but for the
bulk of them it's hundreds of
milliseconds which is a really good low
threshold it enables a whole bunch of
applications on the web you wouldn't
have done before you know chat rooms
definitely very easy games like playing
poker games like playing chess where
chess players are really obsessed by
latency because they play five minute
flash games for exchange traders so long
as the the latency is below human
perception they're very pleased with the
price is changing and can get their
their trades in on time it's enable a
lot of sin type stuff you wouldn't do a
first-person shooter with it but you can
certainly do a lot of really good
dynamic applications we're really
chuffed with those numbers but
some sites need more and if we take this
exact same piece of code still using
comment D still using the same jetty
server but replace the HTTP transport
with the WebSocket transport we get this
arm graph which I haven't put on to the
other one because it just you wouldn't
see it it would be flat long along the
bottom we're still doing 50,000 messages
a second but now we can take it beyond
50,000 users connected to the same node
we can take it to a hundred thousand two
hundred thousand users connected to the
same node and the latencies we're seeing
for delivering the messages are not
hundreds of milliseconds they're four
five six milliseconds we're now playing
first-person shooter games over the web
you know it's it's a order of magnitude
difference in the scalability that we
can achieve and enabling the types of
applications that we can put onto the
web so WebSockets has made this huge
improvement in what we can do and it can
be done if you use the right set of
frameworks like comet D below the scenes
if you talk to your browser doesn't have
WebSockets you do HTTP if you talk to a
browser that does have WebSockets you
talk work WebSockets and get the
additional performance but WebSockets is
pretty well supported if we look at the
browser's out there almost all the main
browsers have WebSockets in there in the
RFC form and most of them also have it
in their mobile versions as well which
is very important and if you look at the
stats from stat scatter global stats and
add up the the ratios of traffic seen on
the web with who supports WebSockets 59
let's call it 60 percent of browsers out
there support WebSockets so either you
can make it a WebSocket only application
today and be content that you targeting
60 percent of the users or you can use a
framework like comic D and have 40
percent of users using a slower more
resource intensive transport over HTTP
and have 60 percent of your users using
a very efficient transport and you know
that'll only get better and better and
better as you go so WebSockets is here
now it's changed what you can do the
orders of
you get in scaling if you're doing a
dynamic push application considering
WebSocket is going to make significant
differences to the number of servers
you're going to need to talk to your
user base it's in production big
benefits the jetty implementation is the
best so use jetty as the server for
doing this and if you want to talk to
everybody I just recommend using comedy
is a good layer on top of it for
publish/subscribe messaging you got to
get a little bit of puff piece in there
sometimes but WebSockets is not the
Silver Bullet that's going to solve all
our problems of web protocols it does
have limitations the messaging semantics
it gives us is great for the job that we
want to do for sending messages
backwards forwards but it's not the HTTP
semantics that we need for our average
websites and we've got a lot of web
sites out there so if we want to have
that same semantics we've got to do
something else
there's still a bit of a problem with
intermediaries getting a little bit
surprised that port 80 is suddenly not
talking HTTP and we occasionally get
reports of intermediaries getting
confused by the protocol was we try to
design it to not get tripped up by the
intermediaries but we have places that
WebSockets just doesn't work even though
the browser and Cerf can do it because
someone in between doesn't want speak it
or we have for our walls that block the
traffic or something you can tell it was
designed by browser vendors because
there's no connection limit so they
don't care about the server resources
they burn so people are going to write
applications that if one's good Moore's
many better so they'll have a hundred
WebSocket connections and blow the
server's up it's also very low level
would sit it tells you you know
connections and messaging but it's you
really do need good semantic layers on
top of it to help an average developer
user it's easy to make work it's really
hard to make fail nicely and time out
and you know restart and retry and stuff
like that which is why you should use
the comedy framework okay so with that
with those limitations it sort of opens
the door to speedy just show how we can
fix those for the HTTP sort of web apps
thanks Greg so speedy so speedy who is
our the speedy here before oh cool so
you're in the right place so what is it
speedy it's a protocol that has been
designed by Google and proposed by
Google in maintained by Google and in a
very open way we're really pleased of
how the protocol is being designed and
contributed to its it's been really
great but more than that it's a live
experiment
you don't realize but a speedy is
already there speed has been designed to
improve HTTP to basically address the
HTTP one-to-one limits and to be a
better faster web protocol so I said
it's a live experiment what does it mean
it means that it's already deployed if
you're using Chrome a browser the
supports speedy and you're talking to a
Google server to any Google server like
Google+ gmail calendar RSS feed and one
now you're already talking speedy you're
not talking HTTP to these servers
Twitter the same they switch the speedy
already what tied calm our outside runs
on speedy Chrome on the client-side and
Firefox already supports PD so it's
already there you're already using it
even if you don't realize it so how does
this work does it do a trick like the
WebSocket protocol does like it upgrades
to a new thing well it turns out that
since it's a live experiment this
protocol has not been designed by
committee first and then deployed and
real stuff what the guys at Google I've
done is how can we make speedy work
everywhere so they said okay there's a
problem because if I hijack the port
that is normally supposed to run HDB and
I speak something different the
intermediary's gets confused and they
say oh I expected to see some HTTP
header I'm looking
for the J section ID because I want to
be I'm a load balancer and want to be
sticky on one of the servers but I see
some random bytes what is that
closing the connection boom and you're
over so what they've come up by trial
and error is this let's try to run
speedy within another protocol a very
deployed one called TLS also known as
SSL so what happens is does SSL defines
a by a frame which can contain things
inside and you know the things that are
inside are encrypted so intermediary
says well you know what oh this is SSL
so I'm not even looking inside because
they can't so I just take what's inside
and pass it over but it's totally
transparent for application rights
browsers run SSL fine and your server
your server your web application doesn't
even know it's SSL it it says you'd be
over SSL it just takes the HTTP request
and be end up in its happy so speedy
uses TLS and as being extended TLS has
been a standard with something called
the next protocol negotiation it's a
it's a pretty technical bit but on you
know we can talk about it a little bit
later it's a new framing protocol over
TLS so basically what happens is that
instead of sending HTTP enclosed into an
SSL frame we send HTTP enclosed in a
speedy frame and closed in an SSL frame
okay
by doing this we retain the HTTP
semantics so your requests are still
doing gets post they still send the
content line the content type they're
really rich they have the richness of
the HTTP semantics but they run they are
being transported by something that is
faster okay so they just run faster but
semantical is the same it's like you
know been ups and you know transferring
packets using a bicycle or using a
Ferrari right
so for four applications
it's totally transparent okay you don't
realize this so speedy defines basically
new framing layer exactly like TLS
speedy says here's my frame okay there
are a bunch of metadata information on
the frame and then there's payload that
payload could be HTTP but could be also
other protocols like WebSocket for
example now one could argue why I want
to send WebSocket over speedy we'll come
to that in a in a few slides so how does
work here's our TCP connection it's a
bleep big blue pipe very big the client
wants to send a request to the server so
it creates what is called a speedy same
stream frame puts inside the HTTP
request and sends it over by sending a
sim stream to the server opens another
pipe which is called the speedy stream
the green one which is you know known as
a stream and speedy string the server
gets the request and replies with
another speedy frame called scene reply
which contains the response headers and
it may reply if the response contains
content with another speedy frame called
the data frame that contains the content
bytes of the response for example the
HTML content bytes the page itself okay
so once the request response cycle is
over the stream disappears okay but you
still have the connection open up and
running all right so what is the big big
big news of speedy how many streams can
I run concurrently on the same disappea
connection right because the key point
here is the following if the average
weight web page has 80 resources what I
can do is I can send 6 then I have to
wait
trip for those six to come back then it
can send another six wait for those to
come back then send another six and so
on right so I'm waiting basically the
the 888 request is waiting a number of
round trips before they even sent and
received okay that's the problem that we
have here is that we cannot send
multiple requests on the same connection
because HTTP lack a feature that is
called multiplexing
however multiplexing is built-in in
speedy you can open as many stream as
you can as you want in the same TCP
physical connection okay you see here
that this stream stream has been sound
at a time which is different by from
this one and then different from this
one
the same stream has been open at
different time still they can run
concurrently
when they're over we can send you know
as many as a a request that we want here
for the average web page we can open one
single TCP connection and sends 80
concurrent requests and open any
concurrent streams at the same time and
have you know one round-trip time to get
all the responses if I knew that I had
to send eight requests and we'll see in
the demo how how this is done so
multiplexing is the biggest feature of
speedy it allows to make a better use of
TCP connection especially it reduces the
slow start problem that we that we see
in in a CDN and it's present also in
WebSocket uses less resources on the
server because now you don't have to
open six connection to a single domain
you don't have to do domain sharding and
you know name your domains like ww1 ww2
GS dot acton.com images ahmed khan but
they're all pointing to the same server
so basically you're open you know 24
connections to the single server for one
user it's a waste of resources you
making just the number of file
descriptors that the server and the
kernel has to allocate for all those
connections and more than that the
server is free to send down to the
client responses out of order if you
request a very large image or if you
request the five icon and the five icon
has been requested after then the five
icon responds it's quicker and can be
sent down before the large image so what
is the key point here we want to reduce
the number of round-trip waits that we
have to make to actually present on the
client the resources the round-trip
waits are caused by the fact that we
have we don't have multiplexing so by
the lack of multiplexing by the fact
that we cannot open ad connection to the
same server we are limited on that we
can only open sex
let's see some interesting speedy
feature so speed has been designed for
HTTP and one thing there is really
important in in in speedy is that it
performs something called header
compression what is that a typical HTTP
request is you know around 1 kilobyte so
whenever you you send a request to a
server you send one kilobyte of data up
to the server what is that it's headers
HTTP headers like imagine the user agent
string imagine the capability of the
browser those are constant they don't
change over time you can make as many
requests as you want they're always the
same okay so why not compressing them so
what what turns out and this has been
done again as a live experiment we tried
that we tried several compression
dictionaries and we change those
dictionary because we found new ones
that were more efficient so what happens
is that on the first request on the very
first request speedy request we say
something around 25% but on the second
request or in the third then any
subsequent web
we can save up to 80% of the bytes that
we're sending over the wire let's take
an example here taken from web tied
accom website so if we make a request in
plain HTTP it's 506 a 68 bytes
okay it's not quite 1 kilobyte but um
you know I don't have cookies I don't
have additional stuff that is normally
there so if I do speedy in the first
request it's already only 389 bytes but
the second request made with speedy is
only 26 bytes we went from 100% to 68 to
only 4% four and a half percent
we had a improvement or 95% less data
being sent over the wire just imagine
what this means if you're paying your
website by the bandwidth another
interesting feature of speedy is is
called speedy push so what happens when
you when you request a primary image a
primary resource on on a in plain HTTP
for example an HTML page you make
requests get down the HTML then the
parser start to parse the HTML page and
finds oh there's an a JavaScript and
makes another quest oh there's another
JavaScript it makes another quest oh
look a CSS makes another request Oh
images makes another quest and so on so
it can only make requests for secondary
resources that belong to that page after
has got the primary resource on the
client because has to parse the HTML
first right in order to understand what
to ask nest next so with speedy push we
have taken this to a totally different
level and jelly is one of the first
speedy servers that actually implements
a strategy for doing speedy push which
is totally automated applications don't
even need to know what's going on under
the covers and we'll see a lot more
about speedy push in the demo
it's totally transparent we use couple
of tricks using HTTP headers
the referer header we check for it
modified scenes it's a bit technical but
you know it's all open source so you can
take a look so how does it work
here's the normal HTTP request we
request for index dot HTML one round
trip we start parsing it the browser
starts parsing it then it says oh I need
application door JavaScript and style
the CSS makes two requests to the server
that gets kind of served concurrently
that's fine but what happens if the CSS
file contains a background image only
when the CSS comes back to the client
then the client parses it and said oh
but there's an image reference here then
let me do another request to the server
so I made basically three round-trip
here to get the whole page now this
resources are somehow related among them
there's a relation among them right so
what we can do in jelly what we do in
Jerry we build a push cache so what
happens when we use a speedy tree and
speedy push that we request the HTML
page ok but then since we have this
cache we already know that the browser
will need the CSS the JavaScript and the
background image so we can just push
everything down to the browser in one
roundtrip
we'll take questions later all right
we'll see the demo but so don't worry
we'll be clearer so well speedy has a
bunch of additional features like
request prioritizations and more goodies
but um you know these two are really the
big ones
how faster speedy is with respect to
HTTP well it's early to say but there's
been a report by Google by Google so
real live data
hitting the Google servers and you know
gathered by the Google guys that says
that for kind of mobile application for
an average very common sites the
speed-up could be up to 25%
it's White's tricky to figure out how
faster speed is because right now most
of the website employ ACB hacks in order
to you know speed up the HTTP version of
it but those speedy those iecp hacks
actually work against the speedy for
example one clear thing is domain
sharding okay speedy has no way to know
the two different domains actually refer
to the same machine so it opens two
connections so by opening multiple
connection we go back to a CP land where
we say okay we have tcp slow-start on
both connections and you know it's a
problem we should be able to you know
stick with one connection only and we're
done with it so benchmarking real sites
against the speedy we have to take an
account that there are hacks the works
against it so it's a bit difficult can
your java enterprise web application
leverage speedy today yes they can
without a single change you take your
wire application you deployed in into
jetty done it just uses speedy how many
of you run a website over ssl ok so all
of you can just do one line change in
the jetty configuration file you ran
over steady and if you and if the
browser like Internet Explorer doesn't
support speedy you run over HTTP as
before and you know all the ones that
will be using speedy will be faster by
default by definition so how we have
implemented speed injury seven and eight
thirty seven and eight where HTTP server
were built two for DHCP protocol so we
didn't have multiplexing in mind when we
designed Jerry seven and eight but it
turned out that the internal
architecture of Jerry was flexible
enough to accommodate speedy it was a
bit of a trick internal trick but we
could accommodate that is kind of easily
Jerry nine which we just published the
first milestone release has been totally
rear connector to have multi
plexi and these new protocols as
first-class citizen in within so we'll
be supporting multiplexing protocols
speed is one WebSocket will soon have an
extension to that they will bring
multiplexing to WebSocket as well and
eventually other protocols such as HTTP
2.0 so jt9 will be ready for the next
for the future web and for the future
web protocols so stick with jerry so use
speeding now if you do run an HTTP web
site download jerry configure speedy let
us know how it goes
let us know if you have problems because
we will give your feedbacks to the
protocol as I said Google is taking the
this protocol evolution very open so we
really are craving for real light data
ok is if you have it give it send
resumes and say a as which the speedy
it's great I have zero problems and say
ok we're on the right path or the
opposite I switch the speedy and you
know what doesn't work or you know it's
my use case it's very complicated one
and you know can I do better and we'll
take a look ok support on the client
Firefox Chrome Opera the latest one and
the jelly client that there are Java
clients so if you want to talk Java
speedy with a server you can this is
particularly important for example for
servers - server communication ok Java
an application on one server that uses a
jetty client speedy client to talk
speedy to another server mobile Android
Chrome Firefox on mobile iOS is not
there yet but ah I guess it will arrive
so it is deployed where 45% of you know
of the population of Internet user
already has already ease speed enable so
you're reading using it it's a live
experiment as I said before
for them okay so the demo demo is based
on I'm taking a couple of tricks here so
what I've done is this I have slowed it
down artificially the local host
interface of my on my computer to
simulate a 200 millisecond latency
round-trip delay between requests and
responses okay just to make the demo
more visible so and what I'm going to
request here is a web page that has an
image a big image made of another 25
images like it you know pixelated thing
we're going to see how this behaves in a
shipping speeding speedy push okay so
take a look at what happens when I when
I run the speedy version of it sorry
they should be version of it okay so
click here have you seen how at the very
beginning it was kind of slow but then
you know pick it up and at the very end
it was kind of fast that's slow start in
action okay it started very slow yeah
and that what happened so have you seen
how it was very slow at the beginning
but then it picked up and you know at
the very end it was kind of fast so all
of this is non cashable so may we made
sure that it's not cached by the browser
okay so if we reload it that's what we
get
all right be faster now because the
connection has been made hot and now
it's a little bit faster okay let's go
back a second and then let's see the
speed version of it okay so same page
speedy version it's already faster than
HTTP okay even after the reload that's a
new connection because it's not talking
HTTP anymore it's talking speedy but
it's ready faster okay
let's reload you know it's faster it's
not noticeable faster okay now we're
going for the match
we want to see how speeding push works
okay so pay attention because I have a
question for you pay attention because I
have a question for you after this click
who noticed something different from the
speedy version you have seen ghosts okay
why because Jerry didn't have the push
cash for it didn't know that the first
request was associated to the other 25
requests okay so basically the first
request is as fast as the speed version
was okay so let's try to reload this I'm
gonna say click okay let's do it again
okay because you know it could have been
a trick or something like that so here
we go
as speedy push all right so enough about
the demo go back to the slide for the
conclusions and I leave back the word to
Greg you can have that on your web app
now no changes to your application you
just did a run speedy do it and you'll
you'll get the magic
thanks to Simoni so where is all this
going you know this is a live experiment
things are changing you know it's is
Google just using their market dominance
to make the web the way they like well
yeah but I mean I've engaged a Google
several times in the open source
capacity and normally they don't get it
but in this case they're doing a really
good job that to date they've been
really extremely open very receptive of
our feedback and have you know taken us
on board and we felt like part of the
team that was developing the spec and
and we see our ideas and our feedback in
the the versions were at version 3
version falls on the way out but most
importantly they have taken this to the
IETF and has proposed this as the basis
of HTTP 2.0 and as of yesterday the IETF
reached out at the HTTP working group to
start work on HTTP 2.0 and there was two
main proposals actually a third proposal
were put in front of them and there was
Google said let's use speedy as the
basis and Microsoft came up with an
alternative proposal which was to use
the WebSocket framing which is already
being deployed and putting our HTTP
semantics on top of it which actually is
not a bad idea because down the track in
you know three four years time it would
be really nice if we only had one
framing layer we had to deal with and
one set of upgrade tricks or next
protocol tricks and just around
different semantics on top of it and to
be fully frank having implemented both
WebSockets and speedy we don't really
care which framing we end up with as
long as we get both semantics on top of
it
but the key thing here is that Google
has got a more speedy project has got a
lot of momentum going it's out there
it's on 50% of our browsers websites
converting and the IETF works on rough
consensus and working code as a way to
getting good specifications and good
standards and the experience we're going
to get when all you guys go back to your
works at the end of this conference
install jetty and start running your
websites on speedy and give us the
feedback is going to be invaluable on
developing a better specification for
HTTP 2.0 so as a working live experiment
the ITF basically decided that speedy is
going to be the basis of HTTP 2.0 it's
unlikely to be adopted exactly as it is
there's going to be a lot of tears and
hair-pulling and other working group
carry on I expect between now and when
it's finally ratified it's unlikely to
be SSL only though I find it kind of
funny when Google argues for this
because they say all web traffic should
be encrypted because no one should be
able to read your web traffic traffic
except for us but there are good reasons
for that but I think it needs to be
unencrypted as well because you want to
have SSL offload at the edge of your
data center and go unencrypted too you
actually have application servers and
things like that the internet the
intermediary vendors have scary switched
on that something's happening here they
sort of ignored the WebSocket process
except for a few of them and they seem
to be a bit more engaged in htb 2.0
process because I realize that all that
value-add they do that if everything's
on 443 and encrypted then you can't add
any value so we're gonna be more engaged
here so I get bring us to the end of the
presentation we'll take a few questions
in the five minutes we have left
Simonian I've got some business cards
here so if you want to talk to us
afterwards grab the young business cards
we're also going to go for a drink at
the Grand Cafe on the corner of Geary
and Taylor at 5 p.m. and we'd love to
have a drink and answer questions and
what not there so come along to that
questions well I kind of like looking at
the way
Ajax win when Ajax was developing as a
technique all these people came up with
these books of it how to be an ajax
developer and they got themselves a book
and chapter one was xmlhttprequest how
many people who when you do Ajax eps now
directly code XML HTTP requests bad idea
too low layer not what you want to
expose your GUI developers to stand on
the shoulder of giants use
infrastructure layer above it making
WebSockets work you know a couple of
HelloWorld messages backwards and
forwards is really easy making it fail
well when people close their laptop lids
and want to retry they go into a train
tunnel whatever is difficult
let frameworks help you with that that's
what comedy is all about it's it's all
about telling you I think he's out of
contact I'll know he's back here here's
the messages that he didn't get or it's
their stuff so
like his PD mean for speed yes yes you
can
Firefox doesn't have a particularly
advanced tool to see the speedy
internals you have to enable some magic
common line option but in speedy is
readily inserting chrome is readily
available I'll show you
like for example here you go and you
open a page called Chrome sorry
chrome net internals and here you have
your speedy common line option oh sorry
oh don't say yeah their speedy thing
well now the connection has been closed
but if i refresh the page and i refresh
here i should be able yeah i have a
speedy request and i can see the speedy
session here and i see all the speedy
frames that have been sent in this so
you can actually see things I don't know
well this will be available in Firefox I
have no idea why you should ask the
Mozilla guys I don't know we have time
for one more question
no oh yes no I'll take okay so the
question was what if I have a reverse
proxy currently there is a mod speedy
developed at Apache for for Apache
however we have injury we have written
our worst proxy that can handle this can
can accept speedy on one side and can
talk or HTTP on one side on the client
side
I can talk speedy to other servers so
and you can do a bunch of transformation
in between them so you know could could
be that you need one more feature you
know jump over on the jury mailing list
and well you know file an issue and
we'll take a look and if you have you
know good use case we'll implement it
we're committed to speedy so it will be
the future will be HTTP 2.0 similar to
that so if you use it will certainly
take a look because it's you know the
future in a way and I think that's all
the time we have for if you got any
questions we'll loiter here and then we
go into the Grand Cafe on Geary and
Taylor</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>