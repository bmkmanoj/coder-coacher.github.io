<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Persistence 2.1: What's New | Coder Coacher - Coaching Coders</title><meta content="Java Persistence 2.1: What's New - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Persistence 2.1: What's New</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FHQzRyD3fuU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everyone thanks for coming this is a
session on jpa 21 what's new in our work
on the spec in progress how many of you
are using jpa today need to just get a
sense wow great how many you are
actively tracking what's happening in
the JPA expert group you can do that
i'll show you how at the end of this
session anyway generic disclaimer what
i'm talking about is work in progress
this is subject to change and i should
point out that it's also subject to
change pending on the feedback that we
get from not just the expert group but
members of the community like you all so
at the end of this talk will give you
pointers as to how you can download the
latest draft of the specification track
the work that we're doing and send us
feedback so what i want to cover in this
talk is the new work that we've done
within the last several months to a year
so this is an update over the
presentation that i gave last year at
java when we had just started this work
I'll basically summarized at the end of
this talk if we have time some of the
things that we did earlier in our in our
jsr cycle but this is what I'm going to
focus on here so first of all schema
generation this is on the newest piece
that we've addressed and it's been a
fair chunk of work so first of all what
exactly do we mean by this and what are
we trying to do in the JPA expert group
so schema generation refers to the
generation of database artifacts like
tables constraints generators indexes it
may or may not involve the actual
creation of the database schema proper
and the reason for that is in most
databases a database schema corresponds
to what's called an off ID that is
credentials the user is given along with
the space of tables that they can
actually they can actually create tables
in and and other database constructs so
depending on the
scenario that we're working with whether
the databases of shared database with of
the database is a database that's
dedicated to a particular application
the database schema itself may or may
not be generated so the term schema
generation is a little bit of a misnomer
I've been grappling for a better term
table generation is is used by some
people but that doesn't really cover it
either because it's certainly more than
tables that are generated so when we
talk about schema generation there are a
lot of scenarios and environments that
we need to consider from a jpa point of
view um first of all and probably most
obviously obvious is the prototyping
scenario where you're trying to develop
an application from scratch you have a
domain object model that you're
developing and you want to see that
materialized in a database so that you
can test it and work with it and
populate the database so the prototyping
scenario but as you move into production
as you work with your prototype and you
enhance your domain model then you
really want to be able to tune the
schema that's been generated so while
you may start with an automatic
generation facility the thing that you
wind up with is not necessarily from
that step what you want to put into
production you may want to hand it off
to a DBA we're going to hand it off to
your database expert and actually tune
the schema that's been produced and then
use this schema hand it off in your
deployment environment as the actual
schema that's going to be used in
production and also we have a number of
environments that we need to consider as
well so for example in a database
provisioning environment in a cloud
environment you may not get your hands
on this database it may be the container
or the the administrative system that's
been set up by the cloud provider that's
actually going to do the provisioning
and at the end of the day the schema
generation for you okay so that's what
we've got to consider in doing this work
from a jpa point of view so then there's
another issue in terms of generating a
schema what's it going to be generated
into is it going to be generated direct
lee into the database is it going to be
generated into ddl files into sequel
scripts and you would want to do this
for example if you want it to hand the
output of the schema generation to your
DBA or your database expert for tuning
or you may want to do both you may want
to generate into the database and then
be able to inspect the results of this
generation when we look at when we look
at actual deployment environment then
the question becomes you know from what
are you going to generate the schema
into the database directly from the o.r
mapping metadata from the anno tunes in
your persistence classes in the XML or
more likely if you're in a production
environment you're going to want to do
the generation from the output of sequel
ddl scripts and then there's the issue
of when is the schema generated are you
going to generate it prior to
application deployment or do you want to
generate it as part of the entity
manager factory creation so the latter
again is most useful in my opinion where
you're in a prototyping mode and you
want to generate from the object
relational mapping metadata so uh and
one more thing as to who does what the
creation of the database schema and
simple environments in in non-managed
environments you may actually be the
person creating it or it may be an
administrator in a more managed
environment um if the database schema is
being generated like in a provisioning
environment it may be the container or
it may be the persistence provider
because the container is delegating this
work and likewise the artifacts that
live in the schema okay so let's talk
about what JPA actually does to address
all these different combinations and
potential scenarios that we need to be
prepared for so we support two phases of
schema generation you get to choose
which one you want if you're the one
who's managing the process or the
container the container environment will
choose this for
you again depending on how things work
whether it's in the clot in the cloud
scenario or non cloud scenario so a
separate phase prior to application
deployment is probably what you're going
to see in a provisioning oriented
environment or you may want to do this
as i said earlier as part of your
entitymanager factory creation and as
you might guess we support two sources
for generating the schema from the our
mapping metadata and from sequel ddl
scripts again according to what your
needs are in the target environment and
we support two targets into the database
or into sequel sequel scripts or both so
the process is very flexible is highly
configurable there are lots of knobs and
levers to adjust in order to get exactly
what you need for the environment in
which you're working in the goals that
you're trying to address okay so let's
talk briefly about the api's so if
you're using the persistence class in an
essay environment for example or if
you're in a container environment where
the container is driving this whole
process there are two methods that are
available to you on the persistence
class and on the persistence provider
API which is the one that the container
calls there's a generate schema method
and the generate schema method is
designed for the case we are going to do
schema generation in advance of the
entity manager factory creation if you
want to do schema generation as part of
the entity manager factory creation then
there are properties that you pass to
the entity manager factory method or
they can the container passes to the
create container and the manager factory
method to drive the process so the
process is property driven they control
the properties control what you're
generating where you're generating how
it's being processed in addition we give
the ability for the application in the
case of the application has already then
finely tuned to package scripts
scripts are going to drive the schema
generation and these can be these can be
data load scripts for populating your
database in advance of deployment as
well as the scripts to create the scheme
as well as the ddl scripts so let's look
at some of the flexibility here so first
of all what what is it you're going to
generate are you gonna are you going to
do creation are you going to do dropping
of tables are you going to do dropping
and then creation drop an old schema
make a new schema or are you not going
to do anything they're not going to do
anything is not as likely a scenario
here and then what is your target are
you going to create in the database
directly are you going to create scripts
or are you going to create both of them
now if you're going to write scripts you
need to configure or the container needs
to be able to configure when talking to
the persistence provider where these
scripts are going to be created so we
have targets for the create scripts and
the deland the drop scripts and the the
and the values of these properties can
either be writers or they could be URL
string too noting a URL where are you
going to put the output of the schema
generation script okay and if you don't
have a live connection to your target
database if you think you know what your
target database is going to be but you
don't have a live connection to it
you're working detached then you can
pass in for portability sake the name of
the database product which in some cases
may be sufficient by itself to determine
the form of the native sequel that
you're going to produce in some
databases it won't be and it varies
again depending on on the driver that
you're using for example you may need
also to pass in major version and minor
version for your database we're doing
this for portability I know that
probably all of the vendors at
this point have their own values that
denote databases you know you can I
don't know any of the details offhand in
terms of the exact value but you can you
know denote that you're running against
Oracle 11 or db2 version and whatever um
but we JPA the JPA standard doesn't want
to get in the Nate in the game of hard
wiring in as values strings or
properties that denote all of the
potential target databases we can't
proceed what they might be so we leave
it up to we leave it up to you in terms
of when you're working with a database
and when you're working with a
persistence provider to either supply
these values or you would be hard wiring
in values that would be supplied to you
by your particular persistence provider
okay so similarly if you're using
scripts to do the schema generation you
spent can specify for you the the you
the container will specify the locations
from which these GDL scripts will be
read again either as readers or as URL
strings and similarly for the sequel
load scripts okay so let me talk a
little bit about the metadata JPA our
mapping metadata can be thought of
basically in two forms and it is it's
pretty clean the way we've divided this
in in the JPA spec and there's what we
think of as quote unquote logical
metadata that is denoting whether
something is an entity and inedible
class whether it's a basic attribute an
ID what cardinality of relationship and
then there's the physical metadata
there's the table or secondary table
column join columns you can think of
these as rough equivalents ID
interestingly enough i would put in both
categories because it does map to a
physical construct in terms of the
database primary key
so when we're doing schema generation
from the arm mapping metadata alone it's
important to note that there are several
layers of defaulting may be involved so
typically the logical annotations entail
physical defaults so the table name is
obtained from the entity need the entity
name itself which you can specify
explicitly is defaulted from the name of
the entity class you can use physical
annotations to override or to customize
the defaults that you would get from the
logical annotations so for example with
the unidirectional one-to-many mapping
that defaults to adjoin table mapping
according to the spec you can customize
this mapping using the joint table
annotation or you can actually override
the format of the mapping with join
column which instead of a joint table
mapping will produce a foreign key
mapping where the foreign key is living
on the many side the side that doesn't
have the the actual back pointer
explicit in your domain entity model
missing my voice so here's an example of
the defaulting at work we have a class
an entity class employee which defaults
to an entity table and the default
schema for the user of the application
if we're not specifying a scheme
explicitly the ID column gets mapped to
a caller the same name called ID which
is a primary key in the database the
name attribute gets a map to a name
column which defaults to a bar chart 255
now this doesn't seem useful in many
cases you may want to tune the size of
that bar chart column the mini 21 gets
mapped to a foreign key a depth ID as a
name of the foreign key column so
suppose we want it to
turn this into a one-to-many mapping a
unidirectional one too many mapping so
in the employee class we would no longer
have the mini 21 we just have in the
department class the one too many which
by default is the map to a joint table
called Department employee we have the
same mappings for the other columns as
we did in the previous slide okay so
this shows you how you might customize
that mapping in this case we're changing
the length of the the name depth name to
depth name and we're saying that it's
required so noble equals false by
default this column would be nullable
which is probably not what we want for
the name of the apartment and we're
choosing a length it's a bit more
appropriate for a department name and
we're overriding our one too many
mapping with a joint column annotation
where we're specifying the name of the
join column in the employee table which
is going to be depth ref and that's
going to be that's also going to be
required so an employee an employee
needs to be assigned to a department now
for the most part what we have as of JP
a 20 in terms of metadata is very close
to what we need for schema generation
from the object relational mapping
metadata alone but there are a few
pieces that are missing some important
pieces actually one of which is the
ability to specify indexes so by default
a primary key generates an index a
unique index for that primary key in
your typical database but often for the
sake of tuning it's very important to be
able to specify additional indexes so we
introduced the index annotation to be
able to do that this is the this is
annotation that operates over the
physical met physical metadata not at
the logical level but at the physical
table level so it can be specified as
part of the table oriented annotations
namely table secondary table join table
and so on an index can span multiple
columns within the table that it's used
and if you do specify multiple columns
the persistence provider is required to
observe the ordering of the columns as
you specify them again that's important
for a tuning scenario where you're doing
data access you know how you want to use
this index how you want to tune for it
so the ordering becomes important so
here's what it looks like the target is
empty because this is being used as part
of those physical annotations that I
just showed you and that's where it
makes the most sense because if index
can't span multiple tables it has to be
used within a single table itself so the
columns that you can specify for an
index are the columns of the table in
which this annotation is going to be
embedded so by default a name will be
chosen for you by the persistence
provider or I believe most databases
would default the name if he didn't
specify it the list of the columns and
the specification as to whether the
index is unique and the column lists
syntax follows basically the syntax that
we're using for order by that is for
each column you can specify the column
name and whether you want the index to
be ascending or descending on that
particular column and if you don't
specify ass integrity ascending
ascending is assumed so here we go
here's an example for our employee
entity we're specifying indexes using
the table annotation we're not changing
the name of the table it's being
generated so it will default to employee
but we're specifying that we want two
indexes one on me on the name of the
employee and another on the foreign key
that's going to map to the department so
we're searching employees of using this
foreign key or departments using the
foreign key this will expedite that
search notice that we did not specify an
index for the primary key that's already
there you can use the index you can use
the index annotation to specify an index
for the primary key if you need to and
why you might need to would be if you
were using a compound primary key you
had an embedded ID or you are using an
ie class and this was somehow sensitive
to the ordering of the columns within
that primary key then you might want to
do that ok another annotation that were
I should go back and say one more thing
about index so index is very useful
obviously and it's a good tuning
parameter but there's always a cost
associated with indexes namely the
requirement for the database to maintain
the index so if you create too many
indexes and you're doing a lot of
updates for example you may incur
non-trivial maintenance costs on on
those indexes so again it's always a
trade-off when choosing your indexes
judiciously and not you know not
succumbing to the tendency to index
everything with foreign keys um this is
something that should certainly be used
with caution because you want to be
careful not to interfere with the
persistence providers default far and
key treatment generally the persistence
providers runtime is optimized to
correspond to how they handle foreign
keys but there are some cases where you
might want to do this so foreign key
again operates over the physical
metadata and because of that you would
specify it as part of the call part of
the annotations that actually correspond
to what would typically be foreign keys
Lee joined column matki join column
primary key join column and the plural
versions of these were there are
multiple columns because you would have
a foreign key constraint that actually
span multiple columns which is why we
have the plural versions these
annotations here as well so far inque is
used to specify a foreign key constraint
or actually the removal of a foreign key
constraint if that was what you wanted
to do so again this annotation has an
empty list for the target the name of
the foreign key constraint is defaulted
the foreign key actually everything is
defaulted here the foreign key
definition itself is defaulted we don't
know what that's going to be and
disabled foreign key is here if you want
to disable the foreign key constraint
that your your provider has assigned to
the foreign key which typically is
restrict from a sequel point of view so
the definition itself is a database
specific sequel string here's an example
of what it might look like so here in
this case we have an employee with
employee has a manager and in the
foreign key definition we're saying the
foreign key manager ID again this is
referring to the actual column which is
going to be if a manager has a primary
key named IDs is is going to be manager
ID in the employee table and this
foreign key references the manager table
and if the target is is deleted we're
going to set the value of the employee
to of the manager and the employee
entity to know we're going to set the
value of that column to know okay so
this is a pretty generic sequel foreign
key definition but you may get into
cases where you're actually writing
database specific sequel here okay so
I'm going to shift gears now schema
generation was a big topic
the remaining topics are a little bit
more constrained scope here um
unsynchronized persistence context um
just for my curiosity how many people
here use hibernate cine buddy ever use
flush mode manual or whatever it's
called today Oh only two of you three of
you okay you really need it yeah okay so
we had considered this in I'll get into
this later okay so just a brief recap of
persistence context so in JP a 10 and JP
20 we have two different flavors of
persistence context we have container
manage persistence context which are
either transaction scopes that is their
lifetime scope to the JTA transaction in
progress or their extended and they can
span multiple JTA transactions an
extended container manage persistence
context are designed for conversations
that may be modeled by a stateful
session bean they were designed for
stateful session beans which are
intended to span conversational state so
container managed persistence context is
automatically enlisted in the JTA
transaction for you it's the
responsibility of the container and the
persistence provider to ensure that this
is always the case they're also
application managed persistence context
and they can be either JTA application
manage persistence context or resource
local application manage persistence
context and in the latter case you're
responsible for the for the the
transactions over the persistence
context but the JTA case is more
interesting and the one that we want to
consider here so a JTA application
manage persistence context because its
application managed isn't closed at the
end by the container at the end of a JTA
transaction by default it spans multiple
JTA transactions unless you do something
to it and however it's automatically
enlisted in the JTA transaction if it's
created within the transaction so if
there is knowledge that there is a
transaction in progress then the
persistence provider is going to enlist
that that application manage persistence
context in the JTA transaction however
if it's not or if there are multiple
transactions that you're running where
you created this persistence context in
one transaction but there other
transactions that are going on then it
becomes your responsibility to join this
persistence context to the JTA
transaction or it won't be washed to the
database on transaction commits it will
be aware of the JTA transactions in
progress so it's kind of a hybrid in
that sense from the standpoint of how is
this persistence context synchronized to
the database with regard to transactions
in any case if your persistence context
is joined to a transaction its flushed
to the database on transaction commits
okay so what's an unsynchronized
persistence context then I just
described a synchronized persistence
context it means it's not synchronized
with the current JTA transaction unless
you call join transaction and this
applies in both cases in the container
managed case and in the application
managed case if it isn't joined to the
transaction then you can't do things
that are intended to be transactional in
the jpa world you can't do database
rights you get an exception and the
database right is either wash or an
attempt to grab a pessimistic walk but
you can still call those methods that
you could call in jp-a 20 for your type
of persistence context so you can still
call persist you can still calmer to
remove and so on and those the effects
of those operations may not be reflected
in the database until your persistence
context is joined to the transaction
so you're basically working in a mode
that's sort of speak disconnected from
the database the intention is that
eventually will join a transaction in
progress because you do want your
activity to be committed in the database
so what's the motivation here um there
are several several factors at play one
of which is modeling conversations so
you may be running a conversation where
you don't want the intermediate phases
of that conversation as various
transactions come and go for other
reasons to be persistent in the database
until you have the end result of the
conversation will give you an example
shortly so you want to track persistent
changes in your persistence context but
you don't want to commit them until
you're done at the end of the
conversation not with each transaction
you may however while this is going on
want transactions over other resources
you don't want to be prevented from
having transactions over other resources
just because your persistence context
doesn't want to do its database rights
until the end of the end of the
conversation and you may want the
ability to propagate the persistence
context with the JTA transaction because
you don't want to have to pass a
reference to the entity manager to some
other method that you're going to call
that's not going to be it's not going to
be very application friendly if you have
to pass around persistence context okay
so those are some of the reasons behind
are doing this so here's an example in
the old style of where you want to model
a conversation shopping carts a pretty
typical example with an extended
persistence context so this is kind of
like a story here so we have our
shopping cart the the customer starts to
shop we're optimistically creating an
order for this customer under the
assumption that the reason they're
shopping is they actually may want to
buy something so our customer is
browsing in this case the the products
are books our customer is browsing books
by author or books by subject the
customer may be adding stuff to the car
they want to view the card but the
customer doesn't want the fact that
you're that he may be adding something
to the card you don't want to persist
that to the database because you don't
really know if the customer I should
have put a remove from cart method in
here you don't really know if the
customer wants to keep the stuff in the
cart or not so you don't want to log
that as a transaction but at the end of
the day when the customer says okay
confirm my order then you want to start
a transaction and you actually want to
add the order to the customer record and
persist the whole thing okay so in the
old world notice that to prevent the
intermediate phases from being
transactional we annotated our shopping
cart to cause um to cuz to be
non-transactional and then we used our
ejd transaction attribute requires new
to actually cause the persistence of the
changes here's how you can do this with
unsynchronized persistence context and
it has a couple of advantages but let's
look at the simplified case first so to
specify a persistence context is
unsynchronized you just add another
element to the annotation to specify
that you don't want the default
synchronization behavior you'll want it
to be unsynchronized and now we don't
really care about what's happening with
the intervening transactions because
this persistence context isn't going to
be flushed to the database until we call
join transaction on it so our code looks
pretty much the same until we get to
purchase our books and then in the
confirm order method we basically say
well join the persistence context to the
transaction and then at this point as
the transaction commits this is actually
committed in the database now so what's
the advantage over the old style well
one of the advantages over the old style
is as I said earlier that you can have
other transactional resources involved
while you're doing this so here's our
previous example
and I I didn't highlight it so I nearly
missed it we have a second persistence
context in here which we're calling data
mining entitymanager this entity manager
is presumably going to be used to track
everything the users doing while the
user is browsing for books right we're
going to log what the user is interested
in and we're going to use it later so
we've got a second persistence context
here and I elide at the code because
otherwise this gets kind of tedious but
um so when these methods were the user
is finding books and browsing them we're
going to use the second persistence
context to actually persist the data
that we're extracting from the
operations that the application is is
doing here so we're going to log all the
the books and the authors that the user
is looking for we're going to log all
the subjects and so on and then when the
user goes to view the car we're going to
say um looks like you did this this how
about these other books that you might
want to buy so all of this data is being
persisted in advance of our joining the
entity manager to the the transaction at
the end of at the end of the order
processing um another use case which I
mentioned earlier which this code
snippet doesn't capture is these methods
are obviously just code snippets but in
any of them you could be calling off you
could be calling out to other other
components other a jv's other managed
beans for example and that persistence
context on both of these persistence
context actually will be propagated
automatically for you because you are
running within a transaction so you get
the transactional propagation which you
wouldn't in the absence of transactions
so it presents you again with cleaner
opportunities for factoring your code
okay converters um this is a relatively
simple facility that we introduced um
it's on our wish list probably for jpa
22 we don't expect to get to it in this
release for reasons that I'll explain
later to provide a more general type
conversion facility but we're getting
our feet wet here writing something
that's useful in in basic cases so type
converters our facility for basic
attributes that allows you to convert
from a database representation to an
entity attribute representation and back
and it's designed for basic attributes
so you're converting one attribute at a
time you're not converting pairs of
attributes or complex complex attributes
like in Devil's so the converter classes
implement and actually be a converter
interface which does what you would
expect it to do it converts the methods
that you define in this converter map
between the database and the entity
representations there's a converter
annotation which you attach to this
class that denotes it is a managed class
persistence provider is looking for it
as part of your persistence unit to
process it so it's like the other
managed classes in your persistence unit
like entities or embeddable and you can
specify that this converter is either
applied on demand when you say you what
an attribute convert it or you can
denote it as an auto apply converter
which means that it will be applied to
any attribute of those that particular
target target type and target type hair
so auto apply converters are turned on
unless you turn them off or unless you
override them on a per attribute basis
you can specify the convert annotation
and this can also be used to disable an
auto apply converter on a per use basis
so the convert annotation looks like
this you specify the converter that you
want to use if you're converting
something that's like within an
embeddable we
to traverse a path within an embeddable
within a key of a map for example then
you might need to specify explicitly the
attribute name but by default if you're
just applying it directly to a basic
attribute you don't need to do that and
as I said you can use it to disable
conversion so here's a simple example um
here we're defining a weight converter
converts between pounds and kilograms
and I hope I've done the math right so
we have a method that's converting to
the database column the database is
presumably representing in kilograms but
we being here in the US are dealing with
pounds and then we have the reverse
converting to the entity attribute from
kilograms to pounds so using our
converter here if our part entity is
here in the US we want to we want to
assign its shipping weight in our domain
object model in terms of pounds rather
than kilograms so in this case we just
apply our convert annotation to the
attribute that we want to be converted
so this is what an auto apply converter
looks like so let's assume our weight
converter wants to be an auto apply
converter we just simply we just simply
expand the annotation here with the auto
apply value of true and we've got an
auto apply converter which will would be
applied to to all doubles which may not
be what we want so in this case we've
decided that we don't want to don't want
to convert our shipping weight maybe
this is a European part and we're going
to disable the conversion on again a
per-user basis using that using the
convert annotation so I want to talk
about some of the other features that
we've we've addressed just briefly um
multi-tenancy is a big one that we spent
a fair amount
time on as you all probably know by now
we've deferred support or formal support
for the cloud to java ee 8 initially we
thought we were going to address this in
java 7 and then for various reasons not
just time-based but also due to kind of
the state of the art and cloud support
we decided that it was best to defer it
to java ee 8 and try to bring in java ee
7 a little bit early and java ee 8 not
that long thereafter now jpa needs to
align with this roadmap is part of java
ee 7 so we'll be deferring multi-tenancy
support but i want to talk a bit about
what we've learned while we were working
on it because this will just be put in
cold storage as it were until we start
jpa 22 in which case you know our
intention is to bring back all of this
work and kind of get our leg up on the
multi-tenancy so there are two basic
approaches to multi-tenancy that we need
to consider um the first of these is the
pass or what I think of is the basics as
scenario which was what java 7 was was
addressing where you have a separate
application instance per tenant so a
tenant gets an application instance that
may be configurable but it's still
isolated from the application instance
of any other tenant um here's a variant
of size I labeled it pure size here
we're a single application instances
shared among multiple tenants and
obviously that has some issues in terms
of security in terms of performance how
do you how do you make the single
application instance work for multiple
tenants so that's downstream um from a
GPA point of view so to back up a bit to
what I said earlier in terms of schema
generation in a cloud environment
database storage maybe provision in
different ways you can't necessarily
count on getting an entire database for
your application you may get a database
for your application you may just get a
database schema for
of the use of your application so the
provisioning of the tenant database
storage determines the persistence
strategies that are usable for a
particular tenant so if you give your
tenant each teneyck it's a separate
database obviously that's the most
flexible and has a few as constraints
it's got the greatest isolation among
tenants your application can look pretty
much the way it does today if you have a
shared database where each tenant would
typically get a schema in that database
and the database itself is providing the
isolation between between tenants
through its authorization and security
mechanisms then you would expect that
the tenant gets a schema which report
ability you would like to think of as a
default schema for the tenant and
obviously you would want restrictions on
native queries so a native query could
not span into another into another
schema you'd be constrained on native
queries now in either these strategies
where the where the container is
managing multi-tenancy the application
doesn't necessarily need to be tenant
aware if it's been programmed in the
portable way and a portable way would be
you know assuming a default schema and
assuming that any need of queries that
you might write would only be addressed
against the default schema then you
would pretty much be portable so in this
container manage multi-tenancy case the
container would supply the provider with
a suitably configured data source and
and you and some information about the
tenant in any configuration that was
tenant specific and your provider would
be pretty much good to go um so with a
size case where an application instance
is shared then that's a much more
complex scenario so as a tenant comes on
board on a running application provider
we need to be notified
of the existence of the particular
tenant the provider would need to know
the tenant ID the provider would need to
know other contextual information if the
tenant got a separate schema in the
database the provider would need a
mapping to that particular data source
and so on in some cases with size
multi-tenancy you could use a striped
approach that is a shared table approach
where different rows in the table
correspond to different tenants this
might work for some applications for
applications that you wanted to be more
secure obviously there are issues but it
would be the responsibility of the
provider to maintain a tenant ID column
that denoted the particular data that
was tenant specific and this tenant ID
column would need to be used to augment
all of the relevant operations the
retrieval operations query operations
update and so on obviously in the case
of a shared table you would want to
restrict native queries because one
tenant should not under any
circumstances see the data of another
tenant so you would assume that the
persistence provider would disallow
native queries the only alternative
would be that the persistence provider
would actually need to transform native
queries which gets a bit dicey it would
not certainly not be something we would
require so the tenant ID needs to be
available to the persistence provider on
a per invocation basis from the
container again attend an ID may or may
not be visible in the application data
that would be that's a strategy decision
as to whether you would want it to be
visible whether the provider would want
it to be visible so there's another case
that I'm kind of mulling over which I
turn application managed multi-tenancy
where the application itself is tenant
aware and the application itself is
managing tenant identities and tenant
specific configuration information and
isolation of tenants and the application
intermediates and the access to tenants
specific data
so for this case I think a shared schema
approach the stripe approach is a
reasonable fit the tenant ID column
would need to be managed by the
application so this may work for some
carefully crafted applications um one of
the things I've been mulling over on the
basis of some of the conversations we've
had on the JPA expert group is whether
we should introduce even though we're
not doing multi-tenancy in the to that
one release whether we should introduce
metadata for the designation of a tenant
ID column to give providers the ability
to experiment with this and other
scenarios so that's something we still
need to consider in the expert group so
other features that we add it in jp-a to
one that I'm actually not going to talk
about here if if last year's javaone
talk is still available and I don't know
if it is you can you can grab the slides
from that and maybe even the audio from
that but other things that we've
addressed so far in the jpa spec or the
theory of providing CDI injection into
event into entity event listeners
additional unwrap methods to allow you
to get it in a portable way to provider
specific ap is a number of enhancements
to the query language both JP aql and to
the criteria API in similar ways support
for stored procedures which was
something that we had with many requests
for and n improvements for native
queries and result mappings and the
ability to dynamically define named
brewery so it is to create a query
object assign a name to it and then
associate it with the entity Vantage or
factory so that it can be pulled back
later by its particular name so I'm not
going to talk about those okay so to
give you some pointers on all of the
work that we're conducting in the JPA 21
expert group this is being done in being
done in such a way that it's visible to
you as developers so this is the name of
our project if you go to java.net
to the JP a spec project you can view
the mailing list the expert group
mailing list you can view all our
specification documents so far we've
produced eight working drafts they're
all available from the Downloads area
including the latest one which has all
the schema generation work in it there's
a user's mailing list which you can sign
up on it's basically a reflector of the
expert group mailing list so you can get
all the emails that are being sent to
the expert group and you can use the
users you can post the users mailing
list to channel feedback back to us so
we've encouraged all of the expert group
members to sign up for the users list so
we can engage in discussions there so
we're targeting the release of this work
with java ee 7 our target date is spring
of next year so it's a fairly fairly
ambitious schedule the RI work is being
done as part of the Eclipse Link project
the RIS eclipselink so there are many
downloads there that are available to
you and it's basically a date driven
release so anything that is not ready in
this release will be deferred to JP
a.j.p 22 so we're taking feedback is to
you know smaller can because of the
aggressiveness of the schedule smaller
candidates that we might actually be
able to achieve in this release although
the door is rapidly closing but it's not
too early to make suggestions as to what
we might consider in a follow-on release
so sign up give us feedback please look
at the specification direct draft see if
they meet your needs and join join the
project thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>