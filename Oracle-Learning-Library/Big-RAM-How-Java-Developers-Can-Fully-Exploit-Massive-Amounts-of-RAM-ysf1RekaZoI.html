<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Big RAM: How Java Developers Can Fully Exploit Massive Amounts of RAM | Coder Coacher - Coaching Coders</title><meta content="Big RAM: How Java Developers Can Fully Exploit Massive Amounts of RAM - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Big RAM: How Java Developers Can Fully Exploit Massive Amounts of RAM</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ysf1RekaZoI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everybody for coming and welcome
to this talk about big run I'm gonna
talk about techniques you can use for
for effectively and efficiently using
large amounts of memory in the JVM just
quickly a brief introduction to myself
I'm over from London in the UK I've been
a Java developer for about 12 years now
and at the moment I work for a company
called cause out it and we we make
software that allows people to
efficiently access large amounts of
their customer data in real time hence
my interest in large amounts of memory
first of all quick quick strawpoll who's
using more than a gigabyte of ram on the
heap in production most people ok 10
gigabytes quite a few people 100
gigabytes couple ok any more than that
400 ok thank you so to start off I'd
like to make a very bold prediction I
believe will be quite standard for
server class machines to have more than
a terabyte or multiple terabytes of RAM
and I'm gonna say four years time so
Java 1 2016 we all come back here and
I'll buy you a pint if this isn't the
case that's that's one point between all
of you by the way no no one each um no I
think there are a few factors that are
driving this the cost of the cost per
megabyte of RAM has only haft since
January 2009
so in January 2009 we could go out and
we could buy 512 gigabytes of RAM for
only about $5,000 today we can buy that
for about $2,500
however the difference is in 2009
the only practical to do anything with
this amount of RAM you'll have lots of
you know 2 gigabyte DIMMs you could
build some sort of Frankenstein machine
to to use all of this round but but you
basically couldn't do anything with it
in a single machine the difference is
not you can buy a server with 512
gigabytes of RAM for less than $30,000
and this price is coming down all the
time I haven't checked it in a couple of
months but it was about $30,000 a couple
of months ago um and one of the factors
driving this is the fact that the new
generation of processors allows much
more around to be addressed by the CPUs
so the current generate the new
generation of Intel processors which is
Romley was a maximum of seven hundred
and sixty eight gigs of ram in the two
socket server previous generation only
allowed 200 mediate and again next
generation probably a lot more terabytes
also the other the other factor is that
larger dim sizes are now quite
cost-effective so a few years ago I
don't know if even know if you could buy
16 big dims but if you could they were
very expensive now they're quite a
reasonable price and 32 gigabyte DIMMs
are still pretty expensive but you know
they're coming down all the time so
these two factors are really driving
forward increasing amounts of ramen in a
server machine so this presents us with
some challenges cuz I don't where I work
as I said we have many many terabytes
who did it I know what this needs to be
accessed in real time by our customers
Noren no matter how fast we make disk
access and things like that it's never
fast enough people always want their
queries to return faster people always
want their analyses to happen faster so
we would like to improve the performance
of our system by storing large amounts
of data in memory so what happened was
back in March our new CEO came to me and
said Neil I've bought you a server it's
good 512 gigs of RAM I want you to use
all
this to make our application faster and
I said sure no problem and I when I said
annexed 511 gigs why 511 hours I was
giving the operating system some some
space to do it stuff
and you know I saw back and I I looked
at the results and essentially this
presentation is is my journey from that
point onwards so first thing we're going
to talk about is garbage collection as
you might have expected so when I got
this server I sat down and I I created a
little benchmark which uses an LRU cache
it essentially shells did it in an LRU
cache up to a certain size and then hits
that cache over and over again with part
of 80 or 90 percent hit rate I'm gonna
present some results from this benchmark
in this presentation today however
before I did just a small word of
warning this is unlikely to be
representative of your production
environment your mileage may vary so
please do your own benchmarking and test
this yourself because my benchmark is
unlikely to be what you will see in
production we've noted by the test
hardware as I said 512 gigs of ram 32
cores and what was at the time the
latest build of the jdk 1.7 e all of the
tests I performed on the JVM were
performed with where all the tests went
about to show you were performed with
10% of the heap free so 90% live data in
the heap why some of you may be thinking
that's not really enough for the garbage
collector to operate and turns out
you're right so here's here's heap size
on the x axis versus garbage collection
time on the y axis for three different
collectors as you can see
with with g1 with 400 gigs of live
hundred gigs of heap size so that's
three hundred and sixty gigs of live
data we reach about 28 minutes for a
garbage collection pause and to be clear
that's that's a pause when the JVM is
doing nothing apart from garbage
collecting so while your application
threads are waiting there for 28 minutes
the reason why the line for the CMS
collector stops at about 128 120 gigs is
because I couldn't I couldn't seem to
allocate more than 120 when I was using
the the CMS collector there is a there
is a bug for this that I raised but it
doesn't seem to be that high priority I
think that the plan going forward with
hotspot is to use g1 for all large heap
situations I I don't think that fixing
the CMS to work with large amounts of
memory is a high high priority but
perhaps somebody can correct me on that
know the times I've just shown you our
worst case times right in a lot of the
time the garbage collector will be able
to perform concurrently to your
application the the reason why I give
the heap only 10% of free space is
because I know that's not enough for the
garbage collector to work concurrently
thus I was forcing a full GC so this is
really the worst case pause times that
you'll see however there are various
scenarios under which you can experience
these worst case falls times and I'll
talk a little bit about that later no
there are some fundamental limitations
to any garbage collected environment if
you allocate essentially if you allocate
objects faster than the garbage
collector can free them then you're
going to be in trouble and this is
specifically objects that are promoted
to the old generation you know with with
large heaps the assumption is that most
of the heap is in the old generation and
so it's specifically tenured objects
that seems obvious but it's worth
stating so what do we do by this well we
can leave plenty of free space in the
heap no I don't know about you but well
know what I have a three gigabyte heap I
have no problem with leaving two
gigabytes free for the garbage collector
to work in you know one gigabyte of life
did it I don't have any problem with
that
however for some reason leaving 200
gigabytes free and a 300 gigabyte heap
makes me uncomfortable I don't know why
I just I can't help but thinking about
all that wasted memory however the ratio
of free space to live debt is a sin in
both of those cases so you're really in
both of those cases the amount of space
you're giving the garbage collector to
work in is is identical so you really do
need to leave large amounts of space
free in the JVM no I just illustrate
this point a little bit further with any
with any garbage collector that attempts
to work concurrently to your application
there are going to be various trade-offs
so we trade off the proportion of heap
used for live data the object promotion
rate on the garbage collection rate let
me just go through these in turn so the
proportion of heap use for life data is
pretty obvious
that's the that's the amount of data
that that isn't eligible for garbage
collection and the reason why this is
important is that if you have a small
amount of free space in your heap which
isn't used by life data then you can
only then the garbage collector needs to
needs to have gone through and marked
everything for collection in the time it
takes to fill up that free space so it's
very important that you have enough free
space in your heap the other factor here
is the object promotion rate if you're
if you're promoting a large number of
objects to the old generation then the
garbage collector has got to keep up
without object promotion rates
again not effects things so if you if
you increase your object promotion rate
you've got a decrease the proportion of
heap that's used for life did it
likewise if you increase your object
promotion rate you've got to try and
increase your garbage collection rate as
well and that's the third thing so the
garbage collection rate is the rate of
which we can actually collect or at
least mark the data in the heap for
collection and again that's important if
if the garbage collector was able to
collect everything very fast then we
wouldn't have to worry about having lots
of space left in the heap now we can
improve the rather we can decrease the
object promotion read by tweaking the
size of the young generation we can
increase the garbage collection rate by
assigning more cores to the garbage
collector and there might be other ways
to speed up the garbage collector but
that's that's the simplest way I just
give it more course and of course the
proportion of the heap used for life did
it can be decreased by increasing the
heap size now just illustrate this a
little bit further here's the effect on
free space in one of my benchmarks so
this all of these tests were performed
with a 120 gigabyte heap on the CMS
collector and you can see that things
are ticking along quite happily 50 gigs
80 gigs of live data in the heap as soon
as we reach about or at least when we
reach about 110 gigs we get a concurrent
mode failure you've probably all seen
compare mode failures because
effectively the garbage collector isn't
be able to keep up with our object
allocation rate and all of a sudden we
go a fill GC a pause bar 600 and seconds
so even with plenty of free space in the
heap certainly in the hot spot
collectors there are situations in which
fill in which long pauses or fill
garbage collections can occur one
example that's talked about a lot is
fragmentation surgery if your heap gets
fragmented the garbage collector has no
choice but to pause and to defragment it
there may be other situations but I
won't go into this night just just one
final thing that you can do to improve
the performance of your application with
large cubes make sure you've got
explicit GC invokes concurrent enabled
so basically what this means is it uses
the CMS collector when somebody invokes
system GC and you may be saying well
nobody in my application and voc system
GC but unfortunately there are a few
cases that are built into the JVM like
our a my direct byte buffer allocation
which explicitly invoke system GC and
obviously you don't want this to bring
down your entire JVM so make sure you
have that flag enabled so I mean totally
I've talked so far about hotspot and
talk a little bit now I bought Azul
saying so this is a commercial JVM with
a so called pause less garbage collector
called c4 it's not technically pause
'less it technically it has small
pauses of the order of small numbers of
milliseconds but when you're coming from
28 minutes that's effectively pause with
I so again I run the same benchmarks
that I ran that I showed you in the
previous slide with zing and the results
were pretty good actually
I showed pauses of about 100
milliseconds so that's done from 15-20
minutes in my benchmarks and this was up
to about 230 gigs of live data in a 250
gig heap before Azul assuming these
poses are not necessarily garbage
collection related they may be pauses to
do with something else happening in the
application that's my smart disclaimer
may not be GC related but anyway you
know it doesn't it doesn't really matter
if you're going for minutes times 100mm
ii say i really care now the same
limitations about object allocation
apply with zinc it's not magic
if you allocate objects that are faster
it and the garbage collector can keep up
with and you haven't got enough free
space in your heap then you'll get a
pause in your application not again
before Azul Singh Sumi this is not a
garbage collection pulse of zinc
differentiates between garbage
collection pauses and application pauses
but effectively what will happen is that
your application will block allocating
memory so it'll it'll block on new so
you know there's no magic here you you
still need to keep plenty of free space
and watch your object allocation with so
I did some tests versus hot spot with
varying live datasets 250k keep the
results are here typically I was getting
pause times of tens of milliseconds with
zinc and with g1 and hot spot I was
getting hundreds of seconds up to
thousand seconds noisy managed to cope
with live data set sizes of up to about
two hundred and thirty gigs in a 250 gig
heap however this isn't necessarily a
good condition under which to run the
the less essentially the less free space
you have in your heap the more you're
gonna wear CPU because you can imagine
that if you're very little free space
the garbage collector is going to be
running continuously you're going to get
very frequent GC cycles and you're gonna
burn a lot of CPU doing unnecessary
garbage collection so again just just a
reminder there Xing might be able to
cope with you know large proportions of
live data and the he thought doesn't
mean it's a good idea next I just in
case you thought that Xing was was magic
from there in their marketing literature
then here's some graphs
so this is Inge keeping up with my
object promotion rate which is about 15
Meg's a second as you can see the you
know the amount of free space and the
heap which is the y-axis is going up and
down but it's it's relatively stable
this is Inge not managing to keep up
with my object allocation rate and as
you can see the amount of free space and
in the heap is going steadily downwards
because the the collector is just not
able to keep up with the amount of
objects that I'm tenuring and eventually
the heap size reaches zero in xing that
doesn't actually stop you from
allocating memory it kind of reserves a
big block of memory for its own uses but
that can't continue for ever and
eventually you're gonna run out and
experience appalls just just a quick
word about throughput I measured be the
throughput of my application that is the
amount of accesses I was able to do to
my cache in saying vs. do you want know
when the application was not paused the
g1 had higher throughput than Xing
however taking into account the
application pauses over a period of one
hour the the overall throughput of g1
was was about a third of zing so despite
the fact it was higher when it wasn't
positive spent too much excuse me um
it's been so much time paused the
trooper was much less in in general my
my benchmarks seem to be particularly
bad for g1 in general that performance
is much worse than expected
with with the amount of free space I was
leaving in the heap for some of my tests
I would expect you want to be able to
perform concurrently I'm not exactly
sure what the problem is because I
haven't spent any time trying to tune g1
yet unfortunately I missed the talk this
morning but yeah I mean this is this is
sort of a caveat it may be that we could
change e1 and get much better
performance from it but at the moment in
my benchmarks is Unchained so the next
session of the talk deals with off heaps
storage so to overcome some of these
limitations in JVMs or in
garbage-collected environments that
we've talked about many people are
starting to allocate memory off heap so
I'm just going to talk about this for a
little bit the most obvious way to
allocate memory off heap is to use byte
buffer dot allocate direct and this
essentially allocates memory in the JVMs
process but it's not part of the heap
and it's not subject to garbage
collection as such you specify microwave
memory size you can just allocate off
your memory happily for as long as you
want no this is only really useful for
long-lived allocations and here's why
Nord DirectBuy buffers you can't free
them explicitly so they're freed by the
garbage collection cycle so you can't
get into a situation where you know
garbage collection hasn't run or it
hasn't been running fast enough and
you've been allocating lots of direct
byte buffers and they haven't yet been
freed by the garbage collection cycle
and so you come to allocate a direct
byte buffer and here's the code that
executes so it realizes it hasn't got
enough at the right byte buffer memory
so at least this code is this code is
from hotspot I don't know whether it's
present in other JVMs so whatever you
try and reserve some memory for a direct
byte buffer it executes the system GC
which is obviously going to give you a
very long pause and then a thread dose
sleep for 100 milliseconds I don't know
why 100 it's clearly enough time for
anything to happen in it's it's
undocumented but but anyway so the point
is that at some point if you're
allocating lots of direct byte buffers
and freeing them over and over again at
some point you might get a nasty
surprise when you try an allocated
direct byte buffer so how do we get
around this well internally there
puffers use class called sun miss unsafe
on a methadone they're called allocate
memory which essentially allows lets us
allocate a big block of memory while
people and we can explicitly free this
memory using the free memory method
alright before I proceed just a word of
warning and you can cause all sorts of
things to happen all sorts of bad things
to happen by using some miss concerts
called unsafe for a reason I suppose but
you can crash the JVM quite easily by
using the methods in here so just be
careful um there's just on the final
point in this slide I'm not going to
cover this but you can in theory move
objects out of the heap into off heap
storage there's a there's a blogger
there's an interesting blog post by this
this seems like a terrible idea because
I have no idea what this is gonna do to
garbage collection and you know this is
this is beyond what I would probably
consider at the moment so here's here's
a sort of sketch implementation of a
wrapper object that allows us to store
stuff off heap now we have to get a
reference to unsafe using reflection
it's it's it's protected in some way in
the in the je viens jar file so it can
only be used by JVM internal stuff but
you can still get at it using reflection
and I heard somebody mentioned that the
Cassandra talked yesterday that they
were thinking about bringing this into
sort of more public access which sounds
good
continue yeah here's my here's my
constructor of my mafia object so field
of the object stores the address that we
have allocated memory out we we see a
realize our data this passed into the
constructor I'm assuming I have some
sort of serialization method that turns
out into a byte array I store the store
the length of my byte array of my
address and then I write my data to
to the offset to an offset of that
address Oh fairly simple then to get
that data back I have a getter method
excuse me I first of all get the length
of my data from that I've stored in the
memory location
I read that data from again from the
offsets and then I'm assuming I have
some sort of deserialized method to turn
that back into my virtual object
importantly we have a deallocate method
that frees up the memory that we've
allocated that address and obviously we
need to be very sure to call out or
we're going to experience a memory leak
I've seen people do things like
reference counting with this so you can
you can always extend this
implementation to provide reference
counting and that sort of thing of
course there's a performance impact
using often memory as you've seen
everything must be serialized and
deserialized when it's when it's stored
alphap and when is brought back into the
heap so there's clearly some overhead
here I'm not everything can be stored
off here I've given you an example of a
wrapper object and you're always going
to have these wrapper objects you need
them to store references between objects
you need them to to wrap the data that
you're storing off heap and if you're
storing your data in some in some data
structure that's probably on heap as
well so for example if you've got a
cache then you know you're probably
going to have to store the
implementation of the cache or the
implementation of your map or whatever
on heap so you tend not to be able to
store everything off heap with this
technique again my implementation is
just a very quick and dirty
implementation there are many things
that could be improved for example
direct byte buffers to page alignments
of their line they line the allocations
to memory pages for efficiency excuse me
No so I performed some tests of again
same as my earlier tests I had a big LRU
cache that I was constantly adding to
and affecting from I started a
serialized linked lists and here with
the roundup size between one and hundred
and forty on added live data set of
apart three hundred and twenty gigs run
this test for about six days creating
but three Meg's a second now the heap
usage in this case was around about 90
gigs so again as I said you can't store
everything off heap with this technique
you've got a you know in my case I had a
I had a big map of keys and the values
were stored off heap so I used to buy
25% of of the total storage on heap
however the maximum pause time was was
very good here I was using the CMS
collector my maximum pause time was less
than 600 milliseconds so this was with
about 25% of free space in the heap so
about 120 820 gigs total heap size and I
didn't experience any long pauses at all
everything was able to be collected
concurrently I think partly because I
had some free space in the heap but
partly also because I was allocating
less in the heap and again my process my
process size were meant stable over over
about six days so this seems that
decayed indicated that fragmentation
wasn't a problem here I'll talk a little
bit more about fragmentation in a minute
throughput as we might expect throughput
wasn't as good as with one heap storage
because again I have to I've got this
overhead of serializing and
deserializing objects so I got about
30,000 gadem put operations per second
compared to 160,000 for on heap storage
so you know not quite as good as on
Bonnie's storage
if you want to already made cash which
eases off leaps towards and like I can
recommend the Cassandra serializing cash
I mean essentially download the
Cassandra jar instantiate or the patchy
bla bla bla serializing cash and you're
off you have an up and all feed cash
this also uses some misc on safe there's
some additional overhead these reference
counting and they store the size of the
object off here on heap sorry and so
there's slightly more Ani memory used
than than my parent down implementation
but but not very much only about 10%
again the either these Google Google
guava they're concurrent LRU hash map to
store the the keys in so before I
proceed just a few words of I of
fragmentation know you've probably all
heard of memory fragmentation but
essentially this occurs when we allocate
blocks of memory and we free them again
on this concrete if we're allocating
lots of blocks of different sizes then
this can create holes in our memory
space which are sometimes never able to
be filled in so over time we allocate
lots and lots of different sizes and we
can end up with holes in in our memory
space which we may never be able to fill
in again no word of warning if you do
off the publication and in the manner of
just showing you you may experience
fragmentation of the Java process the
JVM process if you allocate dinner on
heap then garbage collector takes care
of fragmentation for you
so garbage collector detects the heap is
fragmented and it goes off and says
don't worry about this of the fragmented
for you Noren in hotspot this is an
offline process so this is a full pause
of of the JVM in in
for example it's not it that it's
performed concurrently but the point is
if you're if you're allocating all feed
memory then you risk fragmentation and
there's nothing to take care of it for
you
if your Java process is fragmented
you'll probably see the virtual size of
your process growing over time so keep
an eye on that and you may see delays in
performing wealthy publications no in
non garbage-collected environments
swapping out parts of a process is
pretty normal so you know your your
process gets fragmented the operating
system goes oh they're not really using
that part it anymore I'll I'll swap it
hard and you know fragmentation isn't so
much of an issue your your virtual size
of your process grows over time but
you've swapped out the parts that you're
not actually using in garbage-collected
environments swapping out parts of the
heap is a disaster if if you if you if
you have parts of your JVM that are
swapped out and you try to do for
garbage collection then you'll be you'll
be wishing for the days when you had 28
minute pause times so basically if
you're if you're allocating
coffee memory in the JVM and you're
experiencing fragmentation then you've
got to hope that it's only the off heat
stuff that's being swapped out that
nothing be give that nothing that that's
there's backing up your your Java heap
is going to be swapped out and this is
well this is difficult to guarantee the
the Apache Cassandra solution which
Jonathan Ellis talked about yesterday
it's um local which essentially locks
your process so it can't be swapped out
on Linux by the operating system so what
they did is when the when the JVM starts
up like allocate the complete heap then
they use em block all to lock that and
prevent it from being swapped on which
is which is pretty clever so so they're
all team stuff can be theoretically
fragmented but
their hip is guaranteed not to be
swapped I know slap allocation so um
what some people do to avoid
fragmentation memory fragmentation is to
use something called slab allocation and
essentially what slab allocation does is
it allocates a number of fixed size
chunks and these are typically allocated
upfront so typically for example you
might allocate a bunch of 128 byte
chunks a bunch of 512 bytes a bunch of
2k chunks a bunch of 4k chunks etc and
objects are stored in the smallest
available chunk that can hold them so
you find smallest available chunk you
you put your data in there and then when
when the date is freed you free the
entire chunk notice this avoids
fragmentation but as you can imagine it
it can be quite wasteful of memories so
you're not allocating just the amount of
memory needed for your objects you're
allocating potentially quite a bit more
arguably this is just another type of
fragmentation but technically
fragmentation doesn't occur you you
sacrifice well you essentially trade-off
like a fragmentation for memory usage
however this can be very this can be a
very very good thing to do if if you've
if many of your allocations are
converging around the same sort of size
the most extreme example is when you
have some sort of page cache in a
database you know all your pages are 4k
or air k or 16 care or whatever so you
just allocate slabs for those however if
you're allocating lots and lots of
different sizes of objects you might
find that this is this is pretty
wasteful of memory one product that uses
slab allocation is memcache D they used
to use just standard Mallett I don't
know which implementation but apparently
fragmentation was a big problem for them
in the early days of memcache D so if
you want proof the fragmentation
actually happens in real life
talk to the memcache D developers
because
they're clearly experienced it so they
replaced it with a slab alligator in the
Java world HBase for example has a
java-based Laval killer that allocates
memory of heap so this essentially
allocates a big direct byte buffer
splits this into chunks and allows you
to store data in those chunks there are
some limitations of this approach it
it's it's pretty it's pretty simple at
the moment it requires the chunk sizes
to be specified upfront which makes it
probably unsuitable for a
general-purpose cache you you need to
know the sort of sizes of objects you're
storing up front a better implementation
like the the memcache the implementation
will allocate chunks or allocates you
know essentially lists of chunks as you
request objects of a certain size I have
used this as a general-purpose cash for
my benchmarks there are some limitations
and there's some HP specific stuff but
it but it is possible I performed some
again the same tests on this and
performance was pretty reasonable it was
comparable to other wealthy techniques
however my tests were pretty artificial
because I knew in advance the the
average size of the objects I was going
to be allocated so I just told the slab
cache hey here's the average size I
would suggest that I mean there's I
think there's a need for a
general-purpose lob allocator that all
the kids off heap in Java so be good if
somebody could afford this and make a
general-purpose cash okay so one
argument goes if garbage collection time
is dependent on the number of references
collectors to traverse was supposed to
the total amount of data then storing
byte arrays on heap are supposed to
storing the both heap will have
equivalent performance and this is a
this seems like a fairly reasonable
argument intuition tells me that you
know the
the certainly the mark is if the of the
garbage collection cycle is going to be
dominated by the number of references
the collector has to traverse so to test
this I I store different sized byte
arrays on heap there were exactly the
same number of objects stored on heap
the only thing i varied was the size of
the byte arrays that I was storing and
then I'm vote system GC I look to the
results well I got some pretty
interesting results here the parallel
collector in other words the standard
multiple parallel collector seems
unaffected by the data set size so I
imagine is affected by the number of
references we have but as I increase the
size of my byte erase you know the
collection time remain constant
however G 1 did seem to be affected by
the data size so again not just a number
of references but the actual a mine of
data I was starting my byte erase G one
was affected by this
I would like to investigate this further
I it is possible that G 1 does something
when you invoke system GC that it
doesn't normally do in its in its
garbage collection cycle like I don't
know compaction or something like that
so I would like to investigate this
further before drawing any concrete
conclusions from this but these these
results are quite interesting so why
store data off people as opposed to on
heap in byte arrays well may reduce your
GC pause times again this is
inconclusive but as I'd like to perform
or test in this but it will certainly
increase the chance of you being able to
perform garbage collection concurrently
and again this comes back to the amount
of free space you have to leave on the
heap if you're not allocating large
amounts of memory on he plan you can
leave less free space on heap and you
will still have the same chances of
being able to collect concurrently so
that's that's the main reason for
wanting to allocate off heat it should
also minimize the frequency of garbage
player
and I've said pauses I should have said
pauses and spelt it wrongly I should
have said cycles hear it if you're
allocating less on heap then the garbage
collector will have to run less
frequently again there may be other
reasons like for example allocating off
heap should reduce heap compaction time
because you've got less data to move
around on heap however we have to trade
that off against the possibility of
offie fragmentation excuse me just so
terracotta big memory is a commercial
product that provides an awful cache and
the key difference between this and what
I've been showing you previously is it
in big memory everything's stored off
heap so I talked earlier about whenever
I was caching data in off heap object
having to store at least the keys in my
map and the wrapper objects on heap in
big memory it's a cache but everything
including the data structures that make
up the cache itself or stored off heap
again we have the same limitation
everything's got to be serialized so
again there's a performance hit there
are verses on heap stuff the way that
they implement this is that they've got
a custom off the memory allocator if
this is I believe it's entirely written
in Java so essentially they've written
their own memory allocator in Java and
they claim that memory fragmentation
does not happen this is an exact quote
I'm I'm not clear and how this is
implemented it may be a slab alligator
and terracotta are a little bit reticent
about giving me any details of how this
stuff is implemented so I'm not exactly
sure how they implement this however if
I was evaluating tarik
evaluating big memory for production
years I'd I'd want them to clarify this
statement on memory fragmentation does
not happen and get some answers as to as
to why that's the case
again I performed some tests on this so
sympathies before I started by 235 gigs
of data in big memory and impressively
the Java heap usage was negligible so I
mean I basically saw no Java heap usage
at all and I saw no old generation
garbage collections performs at all you
know in a four-day tests so everything I
think was happening in the young
generation no I haven't yet tested how
much overhead big memory needed for
storing data again if I was evaluating
this for production use this is
certainly a question not ask you know
and by this I mean the the raw data size
of this serialized objects of restoring
versus the amount of virtual memory that
big memory is using off heap and again
if you're using a slab alligator I don't
know what they're doing but they're
using a slab alligator that may be
significant again reread performance was
comparable to using offi project to to
other web techniques my cache loading
performance wasn't quite as good as
other solutions III need to do more work
on this the suggestion from Terra Cotta
was to use bulk loading mode for this
and I've tried this yet so there's no
conclusive results there but it
certainly if you're if you're evaluating
this for production uses it's worth
looking at all of these things what's
the performance of allocating objects in
particular ok so there are a couple of
open source I would say coffee storage
caches
I'm just going to cover these briefly
the the first one is a
it's something called big cash this
attempts to store most data off heap and
it claims that Theon he there is some
money pinned X but it has minimal memory
footprint at the moment there's no
automatic eviction so we'd actually need
to store LRU lists etc on heap if we
were performing in fiction which would I
suppose defeat the purpose somewhat but
I expected automatic eviction is coming
soon again I tested with two hundred and
thirty five gigs of data cache
population went absolutely fine
performed quite well however a few
things this used around 60 gigabytes of
heap space so for two hundred and thirty
five gigs of data I used to buy 60 gigs
on heap the the read performance was
pretty bad it was about one percent of
other solutions my test failed after a
few minutes with an array index other
points within big cache so my
conclusions on this are that it has
potential certainly it seems like seems
to have the right sort of ideas but it's
not quite there yet another open source
solution is patchy direct memory this is
an Apache Incubator product project at
the moment and this aims to be an open
source alternative to excuse me aims to
be an open source alternative to big
memory they have written their own
memory allocation routines in Java so
essentially they've written some sort of
memory allocator malloc or a slob
allocate or something I'm not quite sure
in Java
however the cache keys are stored on
heap in a in a Google global map so this
is quite similar to I think just to
allocating well feed memory and storing
the keys and the wrapping data
structures on heap
no the the final thing I'd like to talk
about is the final technique for storing
data off heap is is the operating
systems on the crash so operating
systems I think all of them have their
own caches for storing frequently
accessed data and you can actually use
the operating systems own cache to store
your data and this is a particularly
good technique to use if your data is
stored in the file system anyway and one
way to do this is to use memory map
files now memory map files are
interesting because what they allow you
to do is to map the operating systems
cache on to the virtual address space of
your application and what that means is
your application can access the
operating systems cache directly without
copying the data
so mostly when you access files if you
don't use memory mapping you load the
data into the heap space of your JVM and
you've you've kept another copy of that
you can use direct byte buffers but
again there's a copy of the data memory
mod files are efficient for two reasons
because they avoid firstly the overhead
of actually copying the data from the
operating systems cache and it also
avoids a double storing of data that is
when you're storing data both in your
JVM process and in the operating systems
cache so it's a very efficient way of of
storing data here's a quick code example
of memory mapping a region of the file
so we very easy we we use a channel dot
map method to to create a mapped by
buffer and then we called the load
method on the mapped by buffer now the
load method isn't guaranteed to load
everything into memory but it hopefully
gives you a fairly good chance of
learning that data into memory and it
puts it in the operating systems
in linux the page cache know the the
disadvantage of memory map files is that
it's not possible to control eviction
from the page cache at least I'm talking
I should have said this I'm not talking
specifically about Linux here so as far
as I know there's no way to control
eviction of data from Linux as page
cache hey it's also not possible in
Linux at least to control the size of
the cache as well so you're fairly
limited in the control you have over the
cache and that's the disadvantage of of
using memory mapping okay conclusions so
first of all I believe big round is
coming again we'll see in four years
time if I we all that point but I've
heard you know I've heard various talk
to bard well in Java 9 we're gonna have
the ability to deal with you know heaps
with with many gigabytes and then like I
don't think that's enough I think we
need to start talking about multi
terabyte heaps until until I hear that
until I hear people talking about multi
terabyte heaps in the JVM I'm gonna be
worried now at the moment I believe Java
has the opportunity to be the de-facto
platform for Big Data and this is
largely helped by projects like a Duke
and other other big data systems that
are developed in Java and you know as a
Java community this is a this is a very
good opportunity for us and we're on a
good path there however I think that we
need better answers regarding using
large amounts of memory and by that I
mean multi terabyte heaps or else you
know people will start using C++ again
or I don't know I don't know what people
will do but as a Java community I think
we need to have better answers to this
garbage collection is great you know
we're all aware of the benefits of
garbage collection and I don't want to
throw that away and start you know start
having to do starting up having to keep
track of my objects again I like garbage
collection the good news is the the
positive garbage collection is possible
there are implementations of it there
the buggy's is that they're not free
however well like I guess I'll make
another prediction I would predict that
in four years time there will be a free
positive garbage collector for the JVM I
don't know who's going to implement it
if there isn't one then the guys from
Azul didn't buy us all the point guard
so garbage collection has limitations
and certain scenarios are presented a
bunch of solutions or a bunch of
workarounds but none of these are
perfect I I do see what I've presented
here in terms of allocating all feed
memory and that sort of thing really as
worker runs we can we can't keep
serializing our objects on storing them
off heap I mean this is this is crazy
so these are just these are temporary
worker runs until we have a better
solution there may be a kiss for having
unmanaged objects in the JVM so in
certain circumstances you know that
there may be a kiss for for changing the
philosophy of Java and allowing people
to allocate objects outside of the
control of garbage collection on just a
final word please tests with your code
and your data don't rely on my
benchmarks thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>