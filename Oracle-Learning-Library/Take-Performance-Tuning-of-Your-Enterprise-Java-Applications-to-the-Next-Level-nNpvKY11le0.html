<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Take Performance Tuning of Your Enterprise Java Applications to the Next Level | Coder Coacher - Coaching Coders</title><meta content="Take Performance Tuning of Your Enterprise Java Applications to the Next Level - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Take Performance Tuning of Your Enterprise Java Applications to the Next Level</b></h2><h5 class="post__date">2013-01-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nNpvKY11le0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Marty itzkowitz i'm the
project lead for the oracle solaris
studio performance tools and my
colleague Scott oaks and I are going to
talk today on how to improve the
performance of your java application I'm
going to give the first half hour or 35
minutes of the top on profiling and Java
profiling and then Scott will talk about
a specific example of how the tools
improve performance on a large gap so
here's what I'm going to talk about
first about why performance matters why
you care about it and what to do about
it then I'll go through some low-hanging
fruit really simple example of two
performance problems then I'll talk in
some detail about Java profiling and
both the model is the user has in the
model that the tools have and talk about
profiling enterprise-class applications
and then Scott will talk about profiling
a fusion middleware so why do you care
about performance performance matters
because faster code means greater
productivity which means lower costs it
also means faster results which means
lower costs my favorite reason of caring
about performance is that it's
interesting and surprising and the
reason for that is that most programmers
have some idea some mental model of how
the program is going to behave and
that's what they code to if there is a
disconnect between that model and how
the code they wrote actually behaves
that often can lead to a performance
problem and you can do something about
performance problems it's with our tools
it's very easy to identify where the
problems are and how to fix them we do
simple runs fully automated runs when
you care about performance you care
about what criteria the obvious one is
that it just takes too long either a
transaction takes too long or a model
that you're running takes too long or
something it also could respond to
slowly if you're dealing with a
transaction system you may find there's
too much latency between the time the
user placed the order and the time they
got a confirmation
it may mean that your server can't
handle the required load it's also true
that it may consume too many resources
that is if you're running on a system
where you may have a cabal of
interacting processes to accomplish some
business objective if you are if one
process takes too many resources on the
machine all the others will be slowed
down to it by that you always have to
look when you find a performance problem
is it worth fixing what's the aggregate
cost of engineering the fix versus the
aggregate cost of the problem that is if
you have a program you've developed for
a special purpose you run it once you
fix it a little bit you run it again
still seem slow but you're not
interested in running it anymore there's
no point tuning it on the other hand you
may want to shave milliseconds or even
microseconds off the cost the time to
process a transaction so you can handle
a larger load with less hardware one
interesting thing that I've observed is
that most untuned codes have what i like
to call low-hanging fruit that is some
real silly way of writing something that
is very expensive and needless
needlessly so and even some tune codes
can have low hanging fruit depends on
how you look so once you decide yeah
once you decide there's a problem you
have to find it and you have to triage
it the way the key to being able to do
that is to have repeatable performance
runs that is I can run the same thing
over and over again and I know how much
time it takes and it's repeatable it
takes about the same amount of time
every time once you do that you can
monitor such a run to get the
performance data it's important when
you're tuning a code to use realistic
data and scale it's not much point
tuning a code to handle a list of 50
items when in production you'll handle a
list of 50 thousand items um you can
also just to decide whether or not
there's a problem
you can use virtually anything you can
look up at the clock on the wall you can
use bend time you can use a stopwatch
anything will tell you if this is taking
longer than you think it should it turns
out that most problems arise in terms of
scaling there is almost any code that
you run has some intrinsic scale factor
the number of items in the in the
database the number of lines of code etc
and it's important to understand how
does the performance of your application
change with that scale factor does it go
as order n or as log in or N squared or
end to a higher power X exponent of n
the higher depending on what that is it
tells you where you have to look on how
important it is to fix it if there is a
problem you need to get detailed data in
order to be able to understand it and
fix it so we're talking about the Oracle
Solaris studio performance analyzer it
has a very simple user model it provides
lots of different kinds of data so you
can look at many different kinds of
problems some of the data is statistical
in nature that is you're doing a
statistical profile of where the program
is spending its time statistics really
work some data tracing data for example
is exact so you know exactly what
happened that the trade-off is usually
the tracing data will slow down your
application and produce large data
volumes where a statistical data can be
tuned to get reasonable data volume and
relatively low intrusion into the
running of the program our tools have a
uniform interface to all the data is a
parameter that you give to one command
that says what kind of data you want or
set of parameters and once you've done
that when you look at the data you've
recorded the tools understand what data
is there and know what to show you the
performance analyzer which is the what
we're talking about here is the
application performance problem profile
or rather in Oracle Solaris to do it
works for
duction codes and production runs that
means you don't have to recompile or
rebuild your application it it works
well on runs that run from tens of
seconds to hours if we can deal with
many cpus many processes many threads we
have one example of building the JVM
where we profile the make command which
had over 17,000 processes in that run
and we just collected data on everything
we deal with real production codes which
means you can use fully optimized and
parallelized applications and we can
profile them when you're profiling Java
of course java hotspot is enabled and we
deal with that perfectly reasonably the
tools in general have minimal dilation
and distortion dilation is when you do a
measurement and it takes longer
distortion is where the relative cost of
different parts of your program will
change we have relatively low for both
order 5% somewhat higher perhaps as high
as ten percent for Java applications and
when I talk about Java profiling in
detail you'll see why the codes that we
have support codes compiled with the
Oracle Solaris studio compilers or the
new compilers for native code and with
various JVMs from the Oracle Java
development releases and the tools
support both Oracle Solaris and various
flavors of Linux the objective that we
had in designing these tools was to make
things as simple as possible and that
means show the data that we've recorded
in the users model of what they wrote
the code to do that means showing
javathread showing OpenMP MPI all of
that complexity however you can make
things as simple as possible but no
simpler there is some complexity that
you really have to take into account the
compiler for native code can do
extraordinarily complex transformations
on the code so that the generated
assembler kind of bears no relationship
to what you wrote in your sore
but it's important that the tools feed
the data back to annotate your code not
the code as transformed by the compiler
we also with the Oracle Solaris studio
native compilers we have a feature
called compiler commentary where the
compiler puts information about what it
discovered in processing and optimizing
your code into the executable we can
then read it and display it and you'll
see examples of that later for Java
programs it's important to show what the
JVM did that means show both interpreted
methods and hotspot compiled methods to
show the garbage collector activity or
the hotspot compiler activity basically
the tools run in three simple steps the
first step build your target I use make
a dot out as doing it the second step is
to collect the data and that's very easy
instead of running a dot out with its
arguments you run collect a dot out with
its arguments there are lots of flags
you can put between the collect and the
a dot out to specify different kinds of
data but that command will work and
collect what we call a default
experiment the third step is to look at
the data and you can do that either in a
GUI or in text from a command called
your print so once you're looking at
performance what do you really want to
know you want to know primarily what you
can change in your code to improve its
performance you want to know what
resources are being used by the code
memory bandwidth disk transfers etc
where in the program they're being used
and often you care how the program got
to the place where that's being used it
doesn't help you to know that you're
spending all your time let's say in a
matrix multiply routine if you don't
know that it's coming from the matrix
multiplies of 100 by 100 matrices and
not the two-by-two matrices you have to
understand the path to the hot spot the
objective we had in our in developing
these tools was to minimize the number
of steps you have to take to get to the
aha point the point at which you
basically slap your head and say what
idiot wrote that
and then you know how to fix it we have
several technologies we can do clock
profiling a part of call stacks that is
as a profiling clock ticks for every
thread we record the call stack at every
tip you can do the same thing for
Hardware counters which gets you more
detailed information about instructions
about floating point instructions about
memory operations and so forth or for
some things you can't race events you
can trace memory allocation and
de-allocation events you can trace
synchronization events the biggest wins
in performance tuning excuse me often
come not from understanding the detailed
code in a detailed sequence but from
large-scale algorithm changes that is
people who write compilers care very
much the generated code not have as much
as one extra instruction in it and they
will look for one percent on spec most
application developers will quit at the
ten percent point not the one percent
point and the big wins come from
changing the strategy you're using to do
the computation and I'll show you that
in a bit later so now I'm going to talk
about low-hanging fruit I have this
cereal silly program I wrote which was
called low fruit in C++ and is JLo fruit
in Java and it does two things one is it
initializes some tables and there are
two versions of it a good version and a
bad burners version the two are shown
you can note that the inclusive CPU time
a second column is substantially higher
in the bad initialization that the good
initialization and the other two that
I'll talk about later are is a good
insert and bad insert which are
bracketed so what you're seeing here is
the function list that is I recorded a
default experiment just like I had
described i said collect jlo fruit and
why came up is this data that shows the
functions in the program and how much
time was attributed to each first column
is exclusive time there is time
executing in that function the second
column is inclusive time which is time
in that function and everything that it
calls so here's an example there's a
factor of our know six or seven in
between the good and the bad if you look
at the bad initialization there's a call
to a static initialization routine
inside a loop and it's spending most of
its time there if you look at the good
version it's outside the loop and spends
much less time now you might think this
example is really dumb no one would
write that but in fact it's based on a
real problem in a code I wrote many
years ago the problem was not that there
was a simple call to initialize
explicitly in the code the problem was
picking which API to use from some
toolkit and understanding the
implications of that API in this case I
was for writing a program whose output
was a list of loops and next to each
loop was an icon saying whether it was
serial or parallel and if it was cereal
whether it was cereal because the
compiler thought it best or because it
couldn't be parallelized for one reason
etc work fine for 10 loops I tried it on
900 loops and it took forever clearly
something was broken there were two
api's I could pick to use one added an
icon to an entry in a table the other
relied on the user to build a vector of
icons and then add all the icons to the
table at once it turns out that if you
add the adding the icons causes a
recompute er of the height because the
icon might be taller than the text in
the line and you have to adjust it so
that all the lines are the same height
so if you add the icons one at a time it
will scan it to find out the height of
the whole list and if you do that for n
icons you're scanning an entry of
endless end times you have an N squared
algorithm if on the other hand you build
a vector of icons and add it to the
entire table at once you have an order
an algorithm actually it's an order of
2n because you have one passive and
up the array the second pass of n to do
one recomputation for all the icon
heights and here's an example for a list
of about a thousand loops API one took
30 minutes api to took 30 seconds and
that's the kind of benefits you can get
this is the kind of low-hanging fruit
that you'll often see in real codes so
now let me look at the second one
certain ones that simple task you often
you have to search some list of
something in this case i'm building a
list of entries and in the good insert i
will do a binary search to find the
right place to put it in the bad insert
i will do a linear search and again the
tools show you the difference in
performance in good insert and bad
insert you can see is enormous a factor
of three almost and if you look at the
source in the bad insert in the linear
search you can see all the time is being
spent in repeated search through this
table to find out what the right
position is if on the other hand you
look at the source for the binary search
which is more complicated more source
code and so forth there's virtually no
time spent in the search and that's
again simple example we're changing from
a linear algorithm to a binary search
algorithm gave you an enormous
improvement in performance so I've
talked about why you care about
performance and the low-hanging fruit
example and now I'm going to talk in
some detail about what we do to profile
Java applications you might think that
the JVM is just a C++ program so what's
the problem here should work on any
program well it turns out that jbm is
not quite as simple as it seems it has
lots of peculiarities for one thing it
dynamically creates the the bytecode
interpreter in its own data space it
allows calls to J and I and back to get
from Java to native and back again and
when the hot spot compiler comes in it
will generate machine code for a java
method and put it
to the target stated dataspace not
instruction space typically the JVM has
many threads there are half a dozen or a
dozen of what I would call system
threads that are always there they don't
do very much but when they're needed
they're needed like garbage collector
which may have many threads or the
hotspot compiler which may have several
threads there are also two call stacks
that are relevant when you talk about
profiling a Java program if you stop it
at any point and try and unwind the
stack you can do a Java stack which
tells you where in the context of the
Java program the program is or you can
do a native stack which tells you what's
actually going on in the machine
instructions and sometimes you really
have to do a combination of both and
what we do is well talk about what we do
later another complexity of Java
profiling is that memory management is
all done through a garbage collector and
that optimization of the code is done by
the hotspot compiler and it may actually
take a method and compile it more than
once but the same performance questions
you have about where's the program
spending its time where is it using
resources how to get there those are the
same questions but they're complicated
by the Java peculiarities so to cope
with those peculiarities our tools
support three ways of looking at profile
data on our Java program there's a user
mode which really corresponds to what
the user thought he or she wrote in the
user mode we don't count what's going on
in non user threads we merge the time
spent in interpreted and hotspot
compiled versions of the same program of
the same routine rather so you see the
time in that routine not separated into
two different times and when you asked
to see disassembly one of the features
in our interface we will show you byte
code annotated with performance data and
we also have an expert mode which is for
people who are writing or primarily
interested in Java but have a little
more sophistication so they're
interested in garbage collection issue
is in hot spot compilation issues for
that we do count the non user java
threads because GC and hotspot are non
user java we again will merge
interpreted and compiled versions of the
same user method and the disassembly
will show you either annotated by code
for Java code or annotated machine code
for the parts that are not java we also
have machine mode which will show all
threads it will show the interpreter not
the methods being interpreted although
when you're executing hotspot compiled
methods you will see those methods and
the disassembly will always show
annotated machine code so here's an
example of a program that I wrote is a
simple default example the application
is a java code that has calls to a C++
shared object so here's the call tree of
the program you can see that highlighted
line the selected one comes from a
shared object Lib C loop that s 0 that
tells you it's in native code not in
Java code but the routine above it that
calls it is from Java code so the
highlighted function actually call is
called from Java and calls back into
Java and that's what you're seeing there
if you look at the timeline for this
program it shows you basically the
sample call stacks as a function of the
real time you can zoom in on that and
you can actually see that those what
look like continuous bars or tick marks
that are separated by about 10
milliseconds in this case we're in user
mode so only the user javathread is
shown that's thread to in this example
and the java does a lot of funny things
so it's often hard to infer which which
thread number corresponds to what's
going on another complexity of Java is
that some implementations will exec
themselves so if you profile Java the
process that you really wanted profile
is not the one that you first launch but
the one that is exact after that and we
just deal with all of that seamlessly
you as I said we have other modes so i
can switch to expert mode and now
suddenly I see in addition to the user
thread this is zoomed in which is why
you can see the separation you can see
there are other threads thread three is
the garbage collector thread and thread
seven and eight of the hotspot compiler
in this case you can see that the non
green events there the non the green
events rather or when profile take
happen but the process was not running
on the cpu like it's waiting and in fact
that last thread most spends most of its
time waiting but there is one example in
thread three where there is a spike of
activity and you can see from the call
stack this is obviously part of the
garbage collector so I've selected that
event and I know it's part of the
garbage collector what I want to find
out is what's happening so I use the
button to move up to the corresponding
time on the Java user thread and I see
it's in this Java function called
routine memilih and it's that function
whose heavy execution is what's
triggering garbage collection if you
look at the source it's really pretty
easy to see why the source allocates a
large array of this my object and it
just doesn't in a loop but it keeps
overriding the pointer so it's basically
creating garbage as fast as its can it
can and that's why you see a spike in
the GC activity at that point so I've
talked about the general understanding
of Java codes and Java profiling and now
I'm going to talk a little bit about the
issues that arise when you're dealing
not with a simple Java code but with
enterprise-class applications
enterprise-class applications can be
very complex you may have many processes
all interacting and it may have each of
those may have many threads it may run
for a long time typically you really
want to track everything that's going on
so you can see what's happening and also
what's typical of these
agencies they may take 45 minutes to
reach equilibrium so you really you
don't care what's happening as the
process starts but you do want to get
what in urology would be called a clean
midstream sample for understanding
what's going on in your program another
issue in these enterprise-class
applications are performance issues
concerning multiple threads thank you to
do that performance issues on multi
spreads usually have to do with lock
contention and there's a difference
between locking doing locking a large a
large part of memory of global lock or a
local op and in fact the whole trick in
any lock programming is to get the scope
right if you get it too big you may be
blocking things that don't need to be
blocked if you get it to fine you may be
spending an awful lot of time acquiring
and releasing a lock that MIT that is
virtually never contended for so
choosing the scope of a lock is very
important performance issue another
issue is load imbalance what really
matters is useful work being done not
necessarily CPU usage in some
programming models when something is
waiting for something it will do a busy
wait it'll spin waiting for a lot to be
released that uses CPU time but there's
no useful work another issue about these
enterprise-class applications is there
often extraordinarily complex to launch
their launched by scripts and one of the
things you can do is you can look at the
various things that get launched by that
and just stick an environment variable
in front of each command that launches a
process you may care about if you do
that you then if you if you run the
script without setting that environment
variable it runs normally but you can
set that environment variable to let's
say the collect command that you want to
use for that process and then you'll
collect data at the same time you run it
without any other change another thing
that you can do is we have an argument
you can
to collect that will control when you
collect data and when you don't so if
you invoke it with let's say minus y
user that means don't collect data until
sig user is sent to this process and
every time you send sig user after that
it will toggle data collection on and
off another thing that you can do is you
can mark event so for example you you
may have set us a signal that will put a
marker in your data and you can send
that signal then you do one particular
operation then you send it again what
happened between those two signals which
is very easy to filter out will show you
what that particular operation did you
can also put API calls for markers in
your code those calls are ignored if no
data is being collection and they have
the effect that you want if data is
being collected once you've got this you
want a filter to deal down to drill down
on problems you may you may care based
you might want to filter only if a
particular function is in the stack or a
particular set of function you may
filter on a subset of threads or
processes you may only care what's
happening on one particular CPU you want
to care you may filter to only look
what's happening between marked events
and all those are important in
understanding these applications so I'm
going to go through a couple of simple
or not so simple examples one is a
profile of the Oracle database this was
collected during the t PCH power test it
was script launched using this flag to
turn data on and off and the queries are
launched by another script it sends the
signal to enable data collection it runs
the query it sends the signal to turn it
down again that means that every
experiment has start and stop markers
for every of every query in this case it
was run on 128 CPU machine which ran 906
processes many of those processes are
ephemeral and they actually don't even
get takes any eat they don't even
get even one profile tip but 256 of them
do significant work so here's an example
of the data from a 40 minute run
49,000 seconds of execution time here's
a picture of the data sorted by cpu
number and you can see that there are
some of the cpus are actually used
substantially less than the others and
you can also look at / process I've set
it up to look at the processes of that
Oracle run and there are many of them I
can filter to look at only the top five
of it and then when I look at the data I
see what's being done by only those five
processes so I'll talk one more example
spec J enterprise is a benchmark that
that emulates an automobile manufacturer
it's but it stresses servers the JVM
CPUs etc it runs on 128 machine of CPU
machine and is 282 threads basically
data collection was enabled in this run
for 2 300 seconds steps and the data
that you see only covers those that
those intervals so here you can see
where the the data was being collected
you have profile events otherwise you
don't and they're all in weblogic you
can see that most of the time in this
case is spent in the weblogic middleware
and that's the nature of the benchmark
and the last example I'll talk about is
the Oracle service oriented architecture
suite again based on fusion middleware
and weblogic handles many complex events
has a performance requirement of near
real-time performance and again this is
run on 64 cpu machine using 166 threads
and in one run we collected both clock
and cache miss profiles so you can see
there are two main paths here that are
highlighted and I infer these from the
names of course one is the user weblogic
thread and the other is the compile
broker so a fair amount of time was
spent deciding whether or not to compile
at the hotspot compiler and actually
doing it and again you can filter i can
say i only care about what happens when
that one function is in the stack so
i'll throw out all the other stuff
so I'll throw out all this stuff about
the Java the compile broker GC etc okay
I've described basically what the tools
do and how they work how they handle
java programming went through a low food
example and went through some examples
of fairly complex code and then I'll
turn it over to my colleague who will do
the rest of it and talk about profiling
a real application thanks Marty I work
in the in the performance scalability
and reliability group at Oracle and I'm
going to give a case study on on how we
use the collector and analyzer to
profile one of our fusion middleware
applications so i don't know probably
many of you are not familiar with fusion
middleware so i'll just talk a little
bit about about it and sort of the
important takeaway about it is that
we're talking about very complex very
large programs here so in particular
were if if you know the fusion
application suite were profiling the
peple or business process execution
language part of it which is a component
of component of so and that's what it
does the point about this particular
application and really a lot of the
fusion middleware applications is it's
it's very very large so all that stuff
that Marty talked about that you know we
can we can scale to processes we can
scale to we can scale across CPUs really
plays out here we're talking an
application that has around probably
40,000 classes and in this particular
example we didn't actually use it but we
regularly scale this application and
profile it with a 55 gigabyte or larger
heat so the scale that this tool that we
can handle with these tools is quite
quite large and quite quite large indeed
for this particular example we chose to
run on a solaris sparc t4 system so it
has 456 there are 456 software threads
there's 256 Hardware threads in the
thing for this particular example we
have only an 18 gigabyte Java he another
thing about this application show just
the scale the
the tools can handle there we have
javathread call stacks in this
application that are hundreds deep I
mean it's it's it's it's a big big
application so when we first bring up so
after we've done the collection win and
we and we've done the experiment when we
bring up the analyzer you know so this
is this is what we see so there's a
couple of it there's a couple of
interesting things to look at on this
slide or not I'm actually going to focus
on the left side you know forget to
forget the the red circle for a minute
which is where I'm going but typically
when you'd bring up a profiler you'd
look at the left side you'd say okay
where's my application spending a lot of
time and if you notice yeah so if you
notice under the user CPU columns it's
not really spending a lot of time
anywhere right the top the top one here
which is JVM system is you know spending
one percent of time and cpu there's
something else is spending point sixty
four percent of time and cpu this
application doesn't necessarily have the
low-hanging fruit that Marty talked
about right we've tuned it a lot we've
done hopefully a good job at tuning it
you know you're not going to say oh look
there's something that's taking forty
percent of time forty percent of my time
that's where I know where I have to to
go to go profile one of the things that
we can do with collector and analyzer
though is it gives a level of visibility
outside of a normal called call stack in
a normal call tree and so that's what
we're going to that's the examples that
we're going to focus on in this part and
so the first thing that we did notice
actually after the fact that you know
the call true profile was flattened
wasn't necessarily going to help us is
that we're spending it seems like we're
spending a lot of time in in user luck
so we're going to use the features of
that of the analyzer that show us
locking behavior which is not something
that you necessarily see in a typical
profiler and see how that can help us
find out where our where we can improve
our performance
so the first thing that we're going to
do then is you know so we're going to
select the user block metric we're going
to sort our left column by user loc
method we see that the top thing that
we're spending time there is we see that
we're spending forty percent of our time
when we highlight the jvm system lock
we're spending forty 46 for almost 40
Proust forty-six percent of our time in
JVM system so that's interesting but you
know what is what does that tell us so
this is one of the times where as Marty
explained you know you need to you need
to dig a little deeper your behavior is
not necessarily in your user you use a
code we need to actually switch view
modes to see what we can see that's a
little bit different so we're going to
switch now we switch to the View mode
you know the view mode that the menu
choice next to where it says view mode
we switched from user to machine now we
can see what actually is spending time
in our JVM luck and we see that you know
everything is calling lwp condition wait
it's a normal sort of unique style
condition variable that we might see and
now we can switch to the caller caller
collies tab and do sort of regular
profiling to see who's calling that lock
and look at a normal sort of call tree
that will tell us who's using that lock
and and what we're used to seeing there
and it will come as probably no surprise
that what's doing all of this locking is
actually the garbage collector because
of course all jobbridge all java
performance problems are because of the
garbage question right we all know that
so so this tells us that one of the
first things that we might want to look
at is perhaps tuning our tuning our
garbage collector reducing the amount of
time wearing garbage collector that's
what's that's what's going to be the
sort of first first order for how we can
solve our issues with with the user
locks but let's dive a little bit deeper
right because although we had forty
percent of our user lock code there we
still have a significant amount of user
lock time that we're spending in other
in other branches
so we're going to use the filtering
mechanism that Marnie talked about and
to look at the rest of that so in this
case we've gone into the call the caller
called the caller I'm sorry we've gone
into the call tree we've highlighted the
GC task we know how to deal with GC we
know how to tune GC that's not going to
that's not necessary going to help us
here so we're actually going to filter
out the GC we're going to we're going to
include only facts that don't contain
the calls into GC and now so now we're
going to see what only in now we're
going to see what's taking up the rest
of our user lifetime aside from GC that
is the user lifetime that we're actually
going to be able to do something about
that's actually in our in our
application code okay once we've done
that we're going to get a regular sort
of collar holly tree and at this point
i'm going to assume that most of you are
familiar with profiling tools you know
what a caller call each color collie
stack looks like you know how to say
okay this is what's calling my locks I
should go look at that coat I should
figure out how to make it call fewer
locks at that point it's it's it's sort
of a sort of similar to what you'd use
from other profiling tools the point is
we were able to get to this point in a
sort of unusual way because it wasn't
CPU time that we were looking at but
there's more than one way to skin a cat
and so one of the other ways that one of
the other things that the collector
annum analyzer provides is this
visibility into the timeline of the
thread stacks that Marty talked about so
again this is a level of visibility into
your application that you're not
necessarily going to see from a
traditional profile it turns out to be
very very very very useful with it with
the collector analyzer so if I bring up
the time if I still have my data
filtered and I bring up the timeline for
this particular operation you'll see i
see that the timeline is the part of the
window shown on the left here you'll see
that I have a bunch of stacks and most
of the stacks are busy most of the time
right there's the the colored bars are
where they're doing things and they're
busy and there's a little gap and
there's busy in their little gap and
that's what we normally see you can
guess that those little gaps since their
periodic and regular and affect all the
threads are GC
but what about that other the other gap
for those other big three those other
those three threads in the middle right
those three thirds in the middle have a
big big gap in the more they weren't
doing anything that's not GC we know
it's not GC for a number of reasons we
filtered out GC in the first place but
also we know it's not GC because it's
not affecting all the threads and it's
really much too long a time period to
BGC so here's a point where we've
discovered that the threads are blocked
they're not able to actually do anything
you know so they're so they're
particularly block and we can now use
this part of the time line tool to go in
and see what exactly is the code that's
holding the lock what exactly is
preventing those threads from actually
executing what sort of thing are they
blocked on and so we're going to go and
do that we're going to find out that the
particular the particular call they're
blocked on is this interceptor chain
manager and now we know where to go look
in our code we're going to go fix the
lock associated with the Interceptor a
chain manager we actually did that
getting rid of this bottleneck in our
synchronization getting rid of that hot
lock improved our throughput response
time by about five percent and we were
pretty happy about that because as I
mentioned this is a pretty well tuned
application we're pretty happy if we can
get five percent improvements at this
point in the in the life cycle of the of
the product
okay so what happens after we fix the
code so after we fixed the code we need
we want to sort of make some
verification that we've done something
well right we have you know we have the
ultimate verification because we ran the
code and it ran five percent faster and
we're very happy but you know it's
always nice to go sort of go in and
check to make sure that you've done the
right thing and check to make sure that
that what what you seem happen so here
we use another feature of the analyzer
tool where we can actually compare two
runs of the same code so the sort of
darker green column on the left of this
slide is where we loaded our initial
experiment and the lighter green code is
where we've changed the code now and
we've rerun the test and now we've
loaded that into the analyzer tool and
now are actually comparing the two
results and happily we can see that in
the in the dark green code we were
spending twelve percent of our time on
this interceptor chain manager lock now
we're not spending any any time waiting
waiting for that particular lock so in
fact we did fix the code correctly
that's why we got that's why we got
that's why we got the benefit another
really nice thing about this comparison
tool is that so this is all sort of
graphically based in and that's that's
nice if we're sitting in front of our
our computer but another nice thing
about this comparison tool and really
all the tools is that they have another
mode where they work which is through a
command line so this er print command
line gives you the same data that you
get out of your out of the profiler but
it gets it all I'll text all textual
base so here we've done the same thing
we've used er print and we've loaded the
we've loaded the two experiments and
we've dumped out the data and we can do
the comparison and we can sort it
visually here the nice thing about that
though is and actually one of my
colleagues asked me about this before
the meaning if you wanted to script your
performance your your if you wanted to
script your aunt your analysis of your
performance this way you can do it this
way right because we can script er print
because it's a script because
all textual base I can make this part of
my regular performance testing I can you
know collect i can use collector do do a
run during my regular performance tuning
may be using the minus y flag and that
Marty talked about turn it on only
during one particular part after I've
done my performance measurements then I
turn on profiling then I gather a
profile then i uz our print then I dump
this out then I write my script to at
least dump out what the hot methods are
or dump out any comparison tools or
whatever so this is fully integrated
belen to a nice sort of script scripting
based test system and we you know it's
it's something that in my group were
actually starting to do just just a
little bit of but the the power here is
actually fairly significant I think okay
so so that's it for the case study I'll
turn over to marty for some closing
words and then we'll take some questions
don't really have much closing words to
say that's kind of what the Oracle
Solaris to do looks like and there's a
download address for where you can get
it the tools are available free although
it's good to buy support contract for
them support is not free and I guess
well either of us will take questions
any questions yes
well for one thing we can deal with
getting java call stacks from
interpreted code for another we can show
annotated bytecode and annotated source
lines as well as showing some details
inside the JVM which I don't know that
they can do one thing they do very
nicely is they have these high-level
tasks that you can select select so
understanding long latency operations
for example and it will tell you the
combination of hardware counters to use
to profile to understand that and I
think that that's pretty nice I don't
know how they deal with large scale and
large complex up applications you know
if you've got 16 Oracle processes
running plus all the middleware
processes etc etc I don't know how well
they can deal with that situation yes
well there is a cost it will slow down
the application about 5% it may
interfere with disk i/o depending on how
much how sensitive you are to disk fan
with we try to we put a lot of work in
tuning our way we do I owe inside our
data collection in particular we open a
file and allocate a large chunk of it in
memory map but then close the file so
all the actual IO is done by writing to
memory mapped areas in the application I
don't I don't know the answer for that
but we believe that our overhead is
quite low as I said five percent perhaps
yeah excuse me yes hundreds yes you can
select any we have a set we have a set
of what we call well-known counters that
we try to make available on every
machine no matter what their real
counters is concerned for example we
have a cycles counter which translates
the one thing on sandy bridge and the
different thing on the hey LEM and a
different thing on t4 etc same thing is
true with some of the memory counters
but any counter that is supported by the
chip we can profile long you have to be
careful there are some counters on the
chip they're not useful for profiling
because they don't count events
generated by the code that's running at
the time you know for example cash
invalidates coming from something on
another cpu that you don't want a
profile on that because it will
interrupt your CPU which will don't tell
you anything at all about where that
came from yes memory as in heap
profiling understanding the heap there
are other tools and techniques inside
the JVM we don't actually do it on Java
code we do do it for native code oh yeah
it's the same tool it does native code
java code
native in Java you can profile scripts
and everything they call you can profile
OpenMP in C++ or Fortran you can profile
MPI in C C++ and Fortran etc we're
trying to cover all the typical user
models yes uh-huh
I'm not sure what what you mean when you
run a collect command it will create an
experiment in addition to running your
command you can look at that experiment
at any time anywhere so I don't think
does that answer your question uh-huh
huh
we do not support other people reading
our experiments you can dump them out
with our tool and there's our people who
have dumped out the raw data and built a
profiling gooey on top of that raw data
well it know that in reverse engineer we
document the command to dump out the raw
data but it's pretty tricky because we
use a fairly intricate way of storing
data for example a profile event has a
64-bit integer call stack ID the call
stacks are actually written with a
matching ID to a different file so you
have to get all that stuff tied together
correctly
right in fact we do that all the time we
run hundreds of tests on dozens of
machines every night and in those tests
we have we run codes that measure their
own behavior and we run them under our
profiler and we use scripts to compare
the file that the program wrote saying
how much time is spent and the ER print
output that says how much time we spend
and make sure that it's close enough
that is the error is that the difference
is between what the program says and
what we say are really statistical
fluctuations or understood in a
particular way any other questions yes
you mean the data object rather than
code no we don't support that right
well usually you can tell from the
method which object are you talking
about which instance of the object is
matters yeah but we don't support that
the profiling the statistical profiling
doesn't get the pointers into the data
space that say what the object is we
only get the instruction sequences any
other questions yeah yes no yeah it
basically all the functionality is
supported on Linux so the extent that
the Linux kernel is functional it the
wonderful example that Scott gave was
done on solaris because solaris does
profiling based on what they call
microstates so whether the program is in
user mode or system mode or waiting for
a lock or waiting for a data page or a
text page and so forth we get all the
profile tix they don't get delivered
until the process goes back on cpu but
we get an array of all those tix so we
can profile based on time spent waiting
for a law on Linux you can't do that
because the Linux kernel only knows
about CPU time any other questions yeah
yes well it's not quite automat you have
to give it a flag to say you want to
follow everything and I think and I
believe you can tell it to do that or
tell it not to follow anything at all by
default we follow everything which turns
out to be crucial for Java because it
Forks and execs in strange ways and
usually most of the prot only one of the
processes is interesting ok thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>