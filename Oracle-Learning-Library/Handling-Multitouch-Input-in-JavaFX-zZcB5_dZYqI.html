<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Handling Multitouch Input in JavaFX | Coder Coacher - Coaching Coders</title><meta content="Handling Multitouch Input in JavaFX - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Handling Multitouch Input in JavaFX</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zZcB5_dZYqI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the presentation of javafx
multi-touch API my name is portia
ferrata I've been working for sun and
oracle for over five years and recently
I've been leading the multi-touch API
effort so today I will start with a
brief even system recap to set up some
terminology for the rest of the talk
then I will provide some overview of the
new multi-touch API and the third
section I will go through all the new
events and describe their behavior and
some reasoning behind it then I will
provide some warnings and
recommendations for writing applications
that behave well on different platforms
with different input devices and in the
end i will show some concrete code
examples so let's start with the event
system this is a picture of a singer of
tree on every node you can register
event filters and even the enders when
an event is fired on a node this node
becomes the target node for this event
and it is then delivered in two phases
first is the capturing phase during
which the event is delivered to the
event filters from the root node through
all the parents to the target node and
second one is the bubbling phase during
which the event is delivered to the
event handlers from the target node back
to the parent node to the root node any
filter or entering the path can call
consumed on the event and this finishes
its way through the scene graph so the
usual usage is just a register event
handler on the note that wants to react
on the event and consume the even there
so for example if we have a button you
register mouse click handler on this
button and then during the capturing
face there are usually no event filters
and when user clicks the button the
target node is usually some of its
children for example the label there so
during the bubbling face the
labor ignores the event it bubbles up to
the to the button the button endless it
consumes it so it doesn't affect further
parents we have hierarchical even types
so the blue rectangles represent the
actual even types that are delivered as
events like mouse press or key type and
the gray rectangles represent the their
parents you can also register filters or
endless to them and they will get all of
the subtypes of the registered even type
so there are two ways of registering
event handler you can always use the
event handler method and give to it the
even type you want to register to and
the handler and for the commonly used
events we have also direct methods like
in this case set one mousepressed so
those two calls are more or less
equivalent so let's move to the
multi-touch so we did our best to
provide general uniform and easy to use
API while approaching the native
behavior as closely as possible so the
idea is that you will study our API
write your application in the spirit of
write once run everywhere and the
applications will run everywhere and
will behave slightly differently on
different platforms to provide you the
users what they are used to so this
should make users appear because the
applications are behave as they expect
and the developers shouldn't need to be
much concert concerned about those
platform differences because it's
usually stuff like different inertial
speed or users doing different things to
produce the gesture or something like
that for all of the new API we used
classic effects events so there are no
special delivery mechanisms or input
monster
put modes to choose from there are just
all the events are generated and the
application just chooses which of them
it wants to handle and registers proper
handlers the events are merged by
purpose so if you rotate mouse wheel it
generates scroll event if you drag
finger over touchscreen it generates the
same score event for most of the usual
usages this is a very convenient because
if you have scrollable content you need
just a single scroll event handler and
it's closed everywhere it's expected to
scroll for some unusual or advanced use
cases you might need to be aware of some
differences between the input devices
because the wheel cannot produce exactly
the same behaviour the screen I will
show it in more detail later so what we
did to the event this is a snippet of
the event hierarchy before we added the
new API there was input event and it's
children mouse event and scroll event
and there are others that are unrelated
right now so we inserted gesture event
between scroll event and input event and
moved some common stuff to it like
coordinates and keyboard modifiers then
we edit events for another gestures zoom
rotate and swipe and finally we added
touch event for checking the particular
touch point touches so the important
thing to keep in mind is that the events
are independent this means that there
are no high-level gestures depending on
a little touches or something all the
events are generated at the same time
and delivered independently so the three
should probably look like this all the
leaf events are actually on the same
level this gives you complete freedom in
to decide which of the events you want
to handle in which parts of the scene
graph but if you decide to implement
some more complicated
combination of those handlers you need
to be careful not to implement
conflicting answers for different events
produced by the same user action so
let's look more closely at the events
most events are generated by touchscreen
they can be used for some easy clicking
and dragging and things like that and
this makes the existing applications
work with touchscreen as they are
sometimes you might need to recognize if
the event were produced by touchscreen
so we added is synthesized flag for that
so the common gestures are recognized
automatically by the platform and
delivered as events we distinguish
continuous gesture events that keep
coming during the gesture I've started
and finished notifications and support
for inertia and one-time gesture events
that represent the whole gesture with a
single event we also distinguish
directly produced gestures that are
produced directly on the concrete
coordinates on the nodes on touchscreen
and indirect gestures that are produced
somewhere else usually on touchpad all
of the gestures provide the gesture
coordinate and keyboard modifiers so the
continuous gestures have three even
types for the started performed and
finished notifications when the gesture
is recognized it starts with the single
started event and at this time the
target node is picked for the whole
gesture so all the following events of
the gesture are delivered to the same
note the gesture coordinates for
directly produced gestures are in the
center of all the touches and for the
indirectly probe gestures it's usually
the mouse cursor location
so then the performed events are
delivered during the gesture and they
have two types of values the Delta
values contain different since the
previous event and the total Delta
leaves different since gesture start at
the end of the gesture there is the
single finish notification and if there
is an inertia to the gesture it is
delivered as another set of the perform
events marked with this inertia flag so
for the usual use cases you only need a
single performed even tender and it the
gesture works for you including inertia
so those are our continuous gestures
currently the first row shows the three
even types the second row shows the
values provided by the gestures and the
third row shows their usual usage to
achieve the natural behavior so to the
existing scroll event me
I don't understand the question exactly
what cygnus inner inertia yeah yeah
inertia is you yeah when you finish the
gesture and release the fingers the
content continues moving and this is
inertia okay so to the existing scroll
event we added the stuff needed to
support the continuous gesture which is
the started and finished even types and
total Delta volumes and we also added
the touch count value which is the
number of touches used to produce the
gesture the rotate event provides the
angle values that can be added to a
nodes rotate property to provide the
natural rotation and the zoom event
provides the zoom factor methods values
that can be multiplied with note scale
properties to achieve natural Ming so we
had it also the swipe gesture it is a
gesture where you press one on more
fingers and move all of them in the same
direction if this gesture is recognized
it is delivered as a single event which
even type representing the vibe
direction it's usually used for things
like paging for example you swipe to the
left to switch to the next top or
something like that the gestures
delivered to the note in the center of
the entire gesture and it also provides
the touch count value
yes oh now we don't recognize diagonal
stripes right now yes I guess it's I
think it's like that I think there is a
there is some space that if you do it
really exactly diagonal it doesn't
recognize any swipe so touch you ain't
burnin
let me first define multi-touch gesture
it starts with first touch of the screen
continues while at least one fingers
touching the screen and and when all the
fingers are released at every moment of
such gesture there is a set of touch
points representing the particular
fingers each of the touch point touch
points as its ID its coordinates and it
stayed if it has just been pressed if
it's moving or stationary or has just
been released the touch events are there
to track those particular fingers touch
points to allow for some advanced or
custom multi-touch logic and they are
only direct so we don't produce touch
events from touch pads because there are
virtually no use cases where you would
enter them the same way except of
gestures maybe but they have different
events so this is a situation somewhere
in the middle of the multi-touch gesture
there are three fingers currently
pressed the first one is moving the
second one is still and the third one
has just been pressed so the touch
points have their IDs they are
sequential numbers in scope of a single
gesture assigned to the touch points as
they appear so when you press the first
touch point is it is ID 1 the second one
is ID 2 and so on so now i have 3 touch
points and i may want to drag a
different notes by each of the touch
point so for that i need each of the
touch point to be delivered to different
target node so to make this possibly
create a separate touch event for every
of the touch points the events have even
types corresponding to the state of
their touch point so this allows for the
nice antlers like set on touch moved on
touch pressed and so on
so what if I now want to write for
example some battle game and I want a
logic where I select a soldier by
pressing it by one finger and then I
press the second finger somewhere else
to specify the direction to shoot at to
be able to encapsulate the shooting
logic into the soldier I need to be
aware of the second touch point there as
well and it is delivered somewhere else
so to make it possible every of the
touch events contains also references to
all of the other touch points so now if
I want to implement some a global logic
on all of the touch point the easiest
thing to do is to end the touch event
and use all of the touch points content
in it but then I need to somehow know
how the events belong together not to
compute again with the same touch points
so the events have even set ID which is
a number common for all the events that
were generated at the same time and are
carrying the same set of touch points so
I can just remember the last used even
set ID and then ignore the other events
in the same number there the touch
events are delivered to the note that
was first pressed by the touch point so
if you press the screen the note is
picked at this coordinate and during the
rest of the gesture the events for this
touchpoint are delivered to the press
note if you are computing something with
the other touchpoint referenced from the
event you can check where the other
touch points are delivered there is the
gate target method which returns the
target node for the touch point and
there is also belongs to method which
checks whether the touch point is
delivered to the given node during the
capturing and bubbling phases in another
words whether the given note is the
target node or any
parents so this default behavior of the
of the events is consistent with mouse
events and gesture events and most of
the time it's like convenient but
sometimes it's not really handy for some
use cases so we added also the grabbing
API so on every touch point you can call
grab and since the next event all of the
events for this touch point will be
delivered to the node who called it or
who sender called it so this is exactly
what every node does for the touch
pressed event to achieve the default
behavior where all of the following
events are delivered to it you can also
call on grab and since the next event
all of the events will be delivered or
ways to the note on the touch points
current location so for example if you
have a list and you drag a finger over
it and you want it always to select the
item under the finger so the list
implements the touch pressed handler
cause on grip on the touch point and
then each of the items can select itself
in the untouched move tender because
it's direct always to the node under the
finger you can also grab the touch point
for a specific node and this is useful
for example if you have a drawing
application you touch the screen it
creates some shape there and you can
drag the shape around so the the board
implements the touch pressed enter
creates the shape and grabs the touch
point for the new shape to allow for for
it to get the further events you can
also find out which node has grabbed the
touch point by the get correct method so
to summarize the touch event there is a
touch point which has state ID
coordinates and target node and the
method belongs to and grabbing API and
there is touch event which has
even types corresponding to the touch
point state there is the main touch
point the references to all of the touch
points number of the touch points even
set ID and keyboard modifiers so let's
look more closely on some problems you
may encounter on different platforms and
different input devices so different
events happen together this is mostly
natural thing but I should point out
some more interesting combinations so
mouse dragging doesn't exclude scroll if
you drag a finger over touch screen it
generates both most dragons and
scrolling events this is the only way I
how to make the existing applications
work because this is how you drag on
touch screen and this is how you scroll
and touch screen unfortunately this is
also the only exception from the rule
that the existing applications work as
they are because notes that handle both
of those events may have problems the
only common specimen we have encountered
is scroll bar at endless mouse dragging
so it reacts to dragging the thump and
it also endless scrolling so it reacted
to mouse wheel if you use such a scroll
bar on touch screen it gets all both of
the events at the same time and
depending on its implementation it
sometimes move twice as fast so this
needs to be solved and the recommended
solution is to ignore the mouse dragging
events that are synthesized from touch
screen also swipe doesn't exclude scroll
so if you drag finger over that screen
it generates score events during the
drag when you finish dragging it
recognizes swipe and delivers swipe and
even then this crowing inertia may
continue so you usually don't want to
handle those two events at the same note
at least not in the same direction the
touch events happen together with all of
the other events so
again if you decide to implement some
more complicated combinations of event
Enders be careful and think of possible
conflicts the gestures may differ for
different users so first of all all the
gestures doesn't don't even exist for
different users if you have pc with
keyboard and mouse you have no way to
generate a rotating or zooming so if you
restrict some logic in your application
to those gestures you are making a kind
of platform dependent the gestures also
don't have exactly the same features in
every platform so you should always be
prepared for the inertia to be generated
or not be generated depending on the
conventions on the given platform also
building some logic on the touch count
value may be dangerous because on
touchscreen you can produce the gestures
by any number of fingers but on touch
pass this is very limited because one
finger moves mouse cursor it cannot
produce any gestures and for example on
Meg there is a lot of system settings
that makes wipe with a certain touch
counts be consumed by the system before
it even gets to the applications as I
already mentioned the scroll event
behaves behaves differently for
different input devices so for mice
mouse wheel nothing has changed since
the previous releases there is a single
scroll event generated for single will
not there is a no continuous gesture in
particular there is no total Delta
because there was no just to start to
compare to on touch screen we can
generate a full continuous gesture and
touch pads are platform dependent on
windows touch pets usually behave
exactly as Mouse and on Mac we can
produce continues gesture as well
so please note that for the usual
scrolling you just implement the scroll
event handler you use the Delta values
and it works naturally for all of the
input devices if you decide to do some
more complicated use of those events you
might need to be aware of those
differences so i will show you some code
examples i intended to show the
applications i'm going to talk about on
a tablet but unfortunately I haven't
been provided with a proper camera so I
will try to explain you what the
applications are expected to do so i
want to write an image viewer
application it shows one image at a time
and it works on tablet where i can drag
the image to the side to switch to a
next or previous image i can zoom or
rotate it naturally and the same
application also works on pc where i can
drag it by mouse to the side to switch
the images i can zoom it bye-bye mouse
wheel and rotated 90 degrees by clicking
the image so there is a kind of unusual
usage of the events because i am using
the scroll wheel to zoom so you will you
will see what you need to do to achieve
this so for the touchscreen rotating
it's easy i implement on rotate handler
and i use the angle for the rotate
property of the image to rotate it when
i finish the rotation I want to fix the
image rotation to the 90 degrees
position so I implement the rotation
finished handler which does that there
is a problem here that the rotation
gesture may have inertia and it will
interfere with the second handler there
so I need to use
the rotate event only if it's not
inertia for Mouse rotating I use mouse
click handler and depending on the
button pressed i rotated 90 degrees but
this makes it rotate also if I top the
touchscreen and I don't want to do that
so I use the event only if it's not
synthesized from touchscreen touchscreen
zooming is again easy just on zoom and
ER and use the zoom factor to scale the
image Mouse resuming use this scroll
event because this is what mouse wheel
produces and uses it only if it's not
direct otherwise it would zoom also if I
drag on the touch screen for switching
the images i use mouse dragging events I
don't use it works on both pc and both
and the tablet so I don't use swype
because it's not continuous gesture it
doesn't support the smooth movement of
the image with the finger and I could
use scroll or touch but I don't need to
because the mouse events work everywhere
another use case I have some circles on
the scene and I want to drag the circus
independently by different fingers so I
just remember the position where I
pressed the circle and idea of the touch
point that is dragging it and keep
moving the circles with the touch point
it may seem that I should not need to
remember the idea of the touch point
because the circle anyway gets only
touch point that pressed it but this
solves the situation where I press a
single circuit two fingers so it has to
decide which of the fingers
well follow so it remembers the idea of
the first one so the touch pro angler if
the circus not jacked yet it remembers
all the values touchmove Tendler if it's
the touch points we are following it
updates the position and that release
then the resets the remember ID so more
interesting thing is when I have a
rectangle and I want to press one finger
on the rectangle and then touch
somewhere else and elect a rectangular
jump there to the new location and then
I can release the first finger and press
it again somewhere else to continue
jumping like that so I want to handle
event that we're the first touch point
is pressed on the rectangle and the
second one is pressed somewhere else so
I implement the touch stationary handler
because the note that is delivered to
the rectangle is expected to be
stationary I handle the event only if
the touch point the touch count is too
yeah I'm not interested in events with
different number of fingers then I
obtain the main touch point and the
other touch point is the second touch
point in the list of all touch points if
they have the same ID that means that
the main touch point I'm handling is the
second touch point pressed so I'm not
interested in this end as well so now I
know that the first touch point was
pressed on the rectangle and I do
continue only the second finger was just
pressed so it has depressed state and
was not pressed on the same rectangle
which is the belongs to this call so now
all of the requirements I've been met so
I execute the jump and now the rectangle
is on the location of the other touch
point so I grab the other touch point to
be able to continue the jumping so the
events for the other touch point now
start to be delivered to the rectangle
and i also want to unwrap the original
finger otherwise i would be able to
touch the rectangle by one finger and
then keep touching different places by
the second finger and the rectangle
would still keep jumping because the
original finger would still be delivered
to it so that's it I wanted to show you
some more cool demos on my tablet but
unfortunately I cannot so if there are
any questions
yes we are considering adding more
gestures and we are also considering to
allow allow some custom gesture
recognizer so that you can generate some
custom justjust NSS you won we are kind
of waiting for the feedback from users
on this API we have just introduced
please
we done we don't decide between the
gestures V right recognize and deliver
all of them so so for the penning we
have it generates scroll UN and the
swype even right so if you want to
scroll something you use the scroll
event if you want just the one time
event so that you I don't know switch
tabs or something it's easier to use the
swype you end so you don't use both of
them at the same time no now they are
they are the same thing actually
so you are planning with one finger and
it produces this crow and then you add
the second finger and scroll event with
touch count one is finished and the new
scroll event with touch con tu is
started and also zoom you end and rotate
event
please
I have tablet with windows seven so I
used standard JavaFX there any other
questions so this probably thank you for
attention thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>