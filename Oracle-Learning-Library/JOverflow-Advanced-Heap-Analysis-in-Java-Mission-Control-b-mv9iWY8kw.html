<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>JOverflow: Advanced Heap Analysis in Java Mission Control | Coder Coacher - Coaching Coders</title><meta content="JOverflow: Advanced Heap Analysis in Java Mission Control - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>JOverflow: Advanced Heap Analysis in Java Mission Control</b></h2><h5 class="post__date">2013-01-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/b-mv9iWY8kw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much everyone for coming
and let me let me start this
presentation so my name is Misha Dmitri
I work at Oracle labs and my
co-presenter today is Johann ring da who
he works at the Mission Control Group
also at Oracle in Stockholm Sweden and
the topic of our presentation today is
going to be a tool for advanced heap or
memory analysis which we call J overflow
and which is going to be a part plugging
in the bigger mission control system so
here is a legal disclaimer that I am
obliged to display basically we are
telling that we don't make any promises
to deliver this product we don't give
you any specific time frame whatever but
of course it's in its now interest as
engineers to deliver it sooner rather
than later but again no promises so far
so the outline of this presentation is
as follows we are going to talk a little
bit about why we started this work and
what tools what other tools exist and
what they do then we are going to
discuss the main thing about this tool
so the problems with the Java memory
structures that it looks for concrete
problems we're going to discuss this and
go in depth we are going to demonstrate
this tool and finally we are going to
present some observations about some
real-life Java clips that we inspected
with this tool and finally if time
permits talk a little bit about what we
plan or think would be interesting to do
next so the motivation for this work
well I guess if you are here you
probably already know that most modern
Java kind of business Java applications
are pretty memory hungry so they
typically require multi gigabytes and
unfortunately often these multi-gigabyte
heaps are used to support maybe just a
few hundred most few thousand users so
some people well some may say that
receive these days so whitewater well
this is yes this is the case but memory
bandwidth first of all is not that ship
so how many bytes you move between your
processor and your memory does matter
and it's not kind of cheap to move them
around
next thing is that high memory usage
very often translates into high GC time
and long pauses and that may really be a
big problem and that's what people are
fighting a lot
in fact if if you probably ask an
average performance person what their
main concern is these days it will very
likely be GC tuning and finally as the
increasing amount of computing goes on
cloud and some companies kind of find
themselves managing kind of thousands or
whatever a large number of machines
running these clouds applications well
they discover that you know that memory
starts to matter so maybe reducing
memory usage by just a few percent will
really help them to reduce the number of
machines and you know reduce the cost
and so it may really be kind of
considerable amount of money now if you
look into the heap of a typical business
application in Java you will see quite a
number of objects and classes maybe
millions are in fact tens of millions of
objects it may be in fact tenths or
thousands of classes so I've myself have
seen some applications which have as
many as 50 to 80 thousand classes and
furthermore like half of these classes
have no instances in fact so what what
they do is a good question but so let's
be what the life looks now and these
objects often kind of form interesting
kind of complex graphs and perhaps the
most kind of difficult thing about them
is that they come from tones of various
libraries frameworks and so on so it
usually not perhaps even the the small
fraction of objects that people have in
their business applications are written
kind of in-house and the rest are
various kind of libraries that are not
necessarily well understood so the
picture is
not probably far from being very easy to
understand existing tools for memory
analysis mainly concentrate in with kind
of two approaches so approach number one
is to track object allocations so
basically this tells us which allocation
sites create most of objects and they
may tell us about kind of GC and
turnaround stuff like that unfortunately
once we know where the object is
allocated once we have registered this
event it's usually very kind of
difficult with this approach or
impossible to figure out what happens to
the object after that so which kind of
where it gets passed around and
eventually what kind of long-lived data
structures it gets attached to so
approach number two is about is about
answering to that question so it's about
analyzing interconnections between
objects and answering the questions who
retains these objects in memory so this
is generally about long medium at least
live data and we are going to
concentrate on this so how to works with
heap dump with Gela Kip dumps existing
tools for heap dump analysis provide
usually the following functionality so
the most kind of obvious the thing that
all of them do is obvious obviously an
object histogram so basically how many
objects of what classes you have in
memory the next thing is viewing object
data fields and navigating the object
graph this is also kind of pretty
obvious and very confusing thing perhaps
the most powerful functionality that
again most of the tools this day most of
the majors those have ears object query
language where basically you can run SQL
sequel like queries on your hip so for
example this query tells you how many
strings in your hip are longer than 100
characters so these functionalities
generally kind of pretty useful and
powerful but it is legacy buggy so
if you know what you're looking for or
that at least have a kind of sensible
hypothesis of where the problem may be
they will help you a lot to kind of
confirm or disprove your hypothesis but
if you don't know where the problem is
if you are just generally concerned that
your memory used she's height and they
are not always that helpful so basically
the tools don't tell you the thing that
a human engineer often can tell by just
looking at your hip dump and code they
don't tell you whether these whether
your data structures are essentially
good or bad or how to change the code to
to fix your kind of memory consumption
what do we call bad data structures one
definition that can be used is that it's
a data structure for which memory usage
can be reduced without loss of useful
information and that often involves the
you know is famous trade-off between CPU
performance and memory usage but in not
so infrequently it turns out that if you
fix a bad data structure then both your
memory usage goes down and your CPU
performance improves as well for example
because of at least because of reduced
GC time and so fixing a problematic
whole kind of bad data structure may
mean either internal adjustment so you
tune something him that data structure
or perhaps a replacement with a better
structure at least something that serves
better in the circumstances and you know
there are kind of different approaches
when one looks for these bad bad data
structures of course the thing that
usually yields the best results is kind
of high level analysis so inspect the
architecture of your application and
perhaps you know if it's really bad then
maybe you need a deep redesign of this
thing so in this case humans are
superior because this machine analysis
of that high-level things is kind of far
from ideal these days however some kinds
of analysis are relatively simple and
that again
that is something that humans can do by
just reading the code or looking at the
hilum but it also turns out that this
kind of analysis is often in fact more
suitable for machine rather than humans
so when there is a vast number of data
structures of course kind of various
kind of problems are kind of it will
take much less time to do this
automatically rather than look at one
after another and so here are examples
some high-level things applicable to any
language problems that are suitable for
machine analysis data structures that
are allocated but not used so again
these are these problems most of them
are preachers but nevertheless people
kind of keep making mistakes which leads
to lead to these problems because it's
often difficult to anticipate how your
code is going to work or because
requirements have can change your code
can be changed by many people all the
time and none of them has a complete
picture of what happens so problems as
well as data structures that are
allocated or but not used data
structures that reserve a lot more
memory than they actually need various
kind of suboptimal data formats for
example objects used instead of
primitive types or kind of complex
collections used where in fact kind of
small and linear arrays would work as
well or allocating a large number of
kind of small collections where the
workload is small compared to the fixed
costs of this stuff that is pretty cool
and data duplication is another source
of overhead and so there are kind of
other problems of that spirit just
somewhat more complex and here are some
examples of how these problems show up
in Java applications in fact they mostly
show up in some windows in code where
you probably wouldn't kind of expect
this to happen
so consider this class fool which has a
field and map field m and in the
constructor of this field basically you
allocate the class allocates this hash
map and then puts the values from these
arrays into that hash map so this is
generally okay
for the fact that if somehow it is known
observe that keys and values are shorter
a most of the time so if in addition to
that you have many instances of Plus
fool then what's going to happen it
turns out that you know the default side
of hash map is default capacity of hash
map is 16 so it's pretty high and so if
you put just two or three elements in it
the remaining modern 10 slots are going
to stay empty but they consume memory
nevertheless furthermore the question is
again if kind of by design or whatever
the number of key is in this hash map
values is going to be small and do you
need a hash map at all so perhaps for
just two or three elements you know just
a couple of arrays and sequential lookup
in them will work no slower and maybe
even faster than hash lookup and your
memory usage will certainly improve a
lot all right so this is one example
here is another one so here some code
reads words accentuate the text from
some input stream perhaps a big file
that contains some real-life data like
name zip codes country codes and so on
and so forth so and it just kind of this
read word colds they obviously they
construct a new string from characters
read from this file because every time a
bunch of characters is read a new string
is created so what's wrong with this
well in real life the data tends to be
quite duplicated so same name same zip
codes you know there is very limited
number of country codes oops sorry
here at the right slide so as a result
your map will likely contain a lot of
strings that are just copies of each
other
and it turns out that this problem is
kind of everywhere and sometimes these
duplicate strings come from really
surprising sources like for example
people sleeping blindly the standard
HTTP headers where you know
you have a very limited number less than
10 these HTTP headers and nevertheless
people kind of read and say a header
kind of separates doing every time so
string duplication is really quite a big
concern sometimes here is one more
example so perhaps I think of course no
one in their right mind is probably
going to be crazy enough to store
two-dimensional points in this kind of
data structure but some you know some
sense or kind of less crazy cases do
occur in real life from time to time and
well of course standard Java you know
this generally collections are very nice
they provide kind of a nice API for
adding elements for lookup removal and
so on but just look at kind of what a
waste of memory this is going to be so
you're going to have ArrayList object
which points at an array of pointers
each of these is going to point at a
separate object this is going to point
at an array once again and finally each
of them is going to point at as what
object where the better
perhaps takes they or most likely takes
more space than the actual workload so
obviously the kind of the simplest way
to and the first thing the first step to
avoid this problem is to rewrite it like
that so replace your ArrayList with
something more compact which is an array
of arrays and that will save no dot that
will save a lot of memory but now if you
are kind of if you are still concerned
you can observe that arrays themselves
are also not entirely freeze each array
has its own header which is at least 12
bytes and on 64-bit vmz maybe up to 16
and so these arrays also have this kind
of 12 bytes and just two integers would
be what eight bytes so still in each of
these arrays the overhead is actually
greater than the useful workload so
again observing that we kind of the
right solution would be again if you
really conserved about memory usage and
maybe less about cash flow
things like that so the solution would
be to rewrite it to change this to two
longer rays in which case your overhead
is really now very small compared to the
amount of useful data that this data
structure contains so okay this has been
some examples to illustrate why we
decided to embark on this and so the
main idea of our tool is really to
automatically identify data structures
matching a various known bed patterns
and they will tell talk about this bed
patterns in a moment basically what the
tool does is it kind of finds all these
objects calculates the overhead
aggregates objects with same problems
shows reference chains so basically it
kind of presents you with the results of
these findings and one more thing that
we are also working on now is that it
turns out that for some bad patterns we
can also automatically analyze the code
so if you are presented with a long list
of bad kind of bad object clusters and
so on it's not always kind of easy to
find out okay where is the low-hanging
fruit what I can i what can I change
quick to get some quick progress so this
automated code analysis is what helps
you in this scenario and we will
demonstrate that later
so some definitions before I proceed to
the most kind of technical part of these
discussions so what we call known
collection is something that is or
extends a JDK class implementing the
list on that so the most of these
examples are things like Java util
hashmap java.util.arrays list plus how
to can recognize anything that extends
these classes however because it needs
to know the internal structure of the
collection it cannot unfortunately
handle completely custom-made
collections at this time so if you have
your own my great special map then that
does not extend hash map or something
then unfortunately the tool wouldn't
know it's a collection it would just
read it as an object plus a standalone
object array attached to it so
standalone object array is something
that
not be recognized as a part of a known
collection and finally there are
standalone primitive arrays which is
basically any primitive array
except for care arrays referenced by
strings so strings that treat especially
all other care arrays are considered
standard or whether primitive arrays are
considered standalone arrays all right
so here is the perhaps the most
essential part of this talk
what are these bad patterns that how to
recognizes we first of all empty
collections it actually turns out that
it's worth recognizing not just simply
empty collections but also but rather
divide that further into empty unused
and empty used so we decide whether a
collection has been in use or not based
on the modification mode count field
that is luckily provided to us already
by the JDK they use it for tracking
concurrent modifications whereas we we
happily use it for determining whether
this collection has been but it has been
never used or whether it has been used
whether it contains some elements and
then was cleared and the reason why we
have this separation is that it's
usually much easier to fix collections
that have never been used because in
that case you you can be pretty sure
that you can do lazy allocation or
whatever the next pattern that we
recognize is sparse collections and
again in this case it turns out that for
Java it makes sense to further divide
that into what we call small sparse and
large sparse and small sparse
collections are those that have less
than half slots in their array
containing real pointers nominals and
their capacity is or actually at node
capacity their size is less than the
default which is for example 16 for the
hashmap
and again the reason for this separation
is that typically these small sparse
hashmaps come from just you know people
carelessly or blindly writing like new
hash map a new array list or whatever
and the way to
fix them is often just provide a better
more suitable initial capacity whereas
for large sparse collections the story
is often more complex and you need to
take more you need to investigate more
on you may not always be as be able to
fix this at least easily the next thing
that we recognize is box collection so
those that contain box numbers and
recall this example number three where
we observed how much memory they can
waste at least in extreme cases so as it
was observed each number each of these
number objects causes overhead both due
to being an object itself and also
because there is a pointer to it
somewhere there are two bad patterns for
collections that recognized our small
collections so those that perhaps just
arbitrarily we we gave this limit of
four elements so typically we have such
collections they overhead the cost of
the object and array whatever the
implementation of this collection is
higher or considerably higher than the
cost of the workload and finally what we
call the vertical bar so this is
basically something that resembles a
two-dimensional list a two-dimensional
array where the outer dimension is much
larger than the inner dimension and
again recall this example number three
in this situation the the overhead of
this we can a fixed cost for each of the
sub list o sub arrays can be quite
considerable for standalone object
arrays we recognize pretty much the same
things except that in addition to empty
sparse boxed and vertical bar object
arrays can also be just simply zero size
so there is nothing except a header in
such a trick but it still consumes at
least 12 bytes in memory so these are
the guys that definitely should be
avoided by all means and again for
premature fairies we have this same two
patterns as before zero size and empty
and finally we have one more which is
specific to this erase what we call long
zero tails
it turns out that especially various
kinds of buffers like higher buffers
database whatever they sometimes tend to
produce the visa rays which contain a
long sequence of zeros in the end and so
we call this we consider this to become
a problem when the length of this is
equal operator to then half the size of
this array so again typically if it
happens when some buffers allocated with
too high capacity bad patterns for
Strings and primitive rays this is a
special case and here we recognize
duplications so two strings are
duplicate if for them the equals call
returns true but the identity of these
two objects is different and you can
device the same thing for primitive
arrays and as I have already said this
is quite often a noticeable portion of
the overhead one complication that this
particular kind of problem presents is
that prior to JDK 7 update 6 to string
objects can point actually to the same
career in furthermore they can point to
various possible sub ranges within that
within that cabaret so this can you know
substring can produce various kind of
interesting things for you so in this
case calculation of the overhead becomes
a little bit too much fun but well we
manage it
finally bad patterns for data fields so
it turns out that someday the fields can
be simply unused so not it's not
necessary that they are completely dead
at the code level but it may happen that
for whatever reason someday the field is
never written in this particular
application or it is almost never
written so ignatius unused and underused
data fields okay so now let's see how we
define the how we calculate the overhead
for each of these patterns so basically
the the main idea is that the overhead
is how much memory would be saved if you
apply the best kind of in the ideal case
if you would
by the best kind of possible
transformation to this data structure so
for example for zero size or empty erase
the overhead is obviously the whole
object size essentially so how much
memory would save if you simply get rid
of that of four empty collections it is
again its whole kind of implementation
size of this collection for sparse
collection stories against pretty
obviously it's a number of empty slots
multiplied by the pointer size and it's
not always achievable but that's how we
define this overhead for arrays of
collection with box numbers the
situation is somewhat more tricky
essentially we define the overhead as
kind of how much memory would be saved
if we replace say an ArrayList of
integer with just an array of its
primitive type and savings in this
situation involve both the difference
between the sides of that integer
java.lang integer object and the size of
the respective primitive type and
furthermore getting rid of pointers to
these objects but one caveat is that you
may have for example a single instance
of integer and all of the elements of
your array list or whatever pointing to
that single instance so the calculation
of overhead in the disk is basically you
have to be you have to be careful that
patterns for the vertical sorry
calculating the overhead for vertical
bar problematic arrays here essentially
it's how much memory we would save by
flipping dimensions as I showed them
that example number three so basically
if you replace that vertical bar with
kind of horizontal one and it involves
the pointer sides and the fixed cost of
sub arrays or sub lists for small
collections those that contain between
one and four elements the overhead is
defined of how much memory we would save
if we replace that with just an array or
two and so the collection fixed costs is
the main component of this so
for duplicate strings it's obviously how
much we would save if we completely get
rid of duplicates if all our strings are
unique and finally for a news data
fields is how much we would save if you
delete the data field completely how to
fix these bad patterns so these are just
very general recommendations and it is
not always easy to follow them it's not
always straightforward but at least in
some situations you are able to apply
these relatively simple measures and so
here are here here are these measures so
if you have a zero size array a last
large number of zero size arrays is
typically they typically come from some
method that you know that is given some
input and just blindly allocates an
array of any size without further
checking so if the input is empty this
method will allocate an array of size 0
and just blindly return it so a very
simple way to address this problem is
okay just check if your input size is 0
and return a singleton zero size array
instead of instead of allocating and you
want so that that's that's an easy thing
for empty collections over arrays one
recipe is to just allocate them lazily
so don't create a collection until you
are until the moment that someone is
going to put some elements into it
unfortunately this is sometimes easier
said than done because it may make your
code it may require substantial changes
to your code it may result in kind of
large number of these null checks or
whatever which will make your code may
make your code at least more kind of
brittle ugly it may present problems
with concurrency so perhaps better
solution would be to actually change the
implementation of these collections so
that they do not allocate at least their
internal details until the first element
is about to be written there
and in fact we have been for example
changes to concurrent hashmap recently
to address that so it's now semi lazily
allocated and now anti concurrent
hashmap consume a lot less space than
before so hopefully some work in the
same directions in the same direction
will be done for other JDK collections
assuming it does not affect CPU
performance negatively because for JDK
classes that I used so widely various
balancing various requirements is not as
simple tasks for small collections again
the recipe is well replace it with an
array as I said but again sometimes it's
easier said than done
it's especially problematic if this
collection is going to be is going to
escape the class where it is managed so
someone some other class is not going to
get to accept an array but it really
needs that collection for whatever
reason so and that is kind of not always
easy with small sparse collections as I
already mentioned the recipe is just
again this is sort of simple select
smaller initial size so for example
default size 16 is really a kind of
sometimes too high so just be careful or
rather at least when you optimize your
code pay attention to this for sparse
large collections the story is more
complex because basically we have to do
something with the resizing policy and
we can't do much about that except that
for example for hash maps we can
increase the load factor because by
default hash map is never never has more
than 3/4 of its slots occupied by non
null pointers so if you increase that
you will decrease the kind of degree of
sparseness but that may come at a price
of reduced CPU performance so here you
have to be careful for boxed collections
the recipe is to use specialized data
structures containing primitive rays and
there are a bunch of libraries which
provide their stuff so just just this is
simple enough
and finally for duplicate streams use in
turning or caching but here the word of
caution is that you know the standard
thing that people apply in this case
which is java.lang string in turn it
appears to be definitely not the fastest
or most economical solution this call is
designed to return a canonical string
for each string instance rather than for
just reduction of the number of
duplicate strings so this is not very
scalable this is not very fast because
of the internal synchronization and our
experience is that various kind of
weekly synchronized and kind of
custom-made caches designed specifically
to save memory and to kind of avoid
using too much memory of themselves they
tend to perform better than string in
turn and okay this is the end of the
first part of this talk and now Johann
is going to show you how the whole thing
actually works okay so I'm going to show
how we have integrated this analysis
into Java Mission Control in Java
Mission Control in the JVM browser on
the Left all found JVMs are shown and
for all locally running DBMS we can
perform an analysis and right now the
Mission Control itself is the only job
am i running here so I will perform
another analysis own on this ok so now
it scans the entire heap and marks each
object with 0 or several types of
overhead and calculates how large this
already and this overview here shows all
the the current set of objects different
properties for for the set and those
controls can also be used to reduce the
set of objects that are currently being
included in your calculations shown in
this
Able's so the top-left table is showing
a summary for each set of objects marked
with a particular type of Allred and
also a summary for all objects and
initially all our bigs are selected so
these tables not show all objects on the
heap and actually some problems may be
found just by simply inspecting the
object histogram so on the bottom left
we have the object normal histogram with
the classes and see how much memory each
consume and how many objects and this
can be used and to reduce for example I
can say that I only want some some
classes and I do not want some others
and so forth this bottom right control
is used to reduce the set by looking at
the referrer chain to each object so the
each row here is a group of objects that
has in there referral chain and answers
to refer that matches the pattern I'm
writing here so for example I can the
right come j-rock get and it groups the
object by the closest referrer that is
matching this pattern okay so in the
upper right I have a normal referral
table so each refer that is still in my
active set here it has the refer in this
table and if I select one refer only
objects that are referred to by this
river is still in the set and the table
is updated with the parent referrer of
the surfer that I just selected so I can
drill down this table to get a smaller
set of objects so for example doing this
now I've only 158 objects left and I see
how much memory they consume and so
forth the selected object can also be
inspected in object inspection here so I
have these objects here and I can show
their content
okay this small screen here but I get
the data for the object but most
importantly the tool marks these other
types of overhead and this Mission
Control was very idle when I took this
if so there is not much of interest to
look at but if I do perform a new
analysis on the tool right now while
this analysis is showing there is some
more interesting things to look at so
now I sort this by overhead size and I
see that 5% of the heap is wasted by
duplicate erase and I find that it is
actually a single very large in turay
that is using a lot of memory and
looking at the right top table I see the
referrer who are referring to this too
you can see that it is actually two two
instances that are duplicates of each
other and yeah this is not our code so I
am I cannot tell if this is actually a
valid valid problem or some for some
other reasons they are similar or
identical okay so if I select the next
problem category this is actually a
large number of 47,000 array lists that
are wasting a lot of memory since they
are small and that is you can see up in
the right table that they are referred
to by a class in the J overflow package
so this is the has to do with the
clustering of these objects internally
in this implementation and it's actually
valid that the we are wasting memory
here I could say that I've left it
intentionally but we actually it is
actually an area where we could do some
improvements or optimization they say
that all these really sell refer to by
this chain and a lot of them has just a
few of the agenda
there are also some duplicates a lot of
duplicate strength but those are just
some has to do with eclipse on so I will
open this tool as you see hopefully it's
reasonably optimized so I will open a
dump because you can also open old old
heap dumps and I will open a dump from
from earlier where some problems that
were detected in this very tool the Jo
flow while developing it we had we had
some some things that we actually did
remove so I can close this old analysis
and this is quite large dump so it takes
some time to analyze okay so in this we
see that actually 26% of the heap is
wasted by the problem has to do with
boxed collections so it's it's not that
those collections take they take more
than 25 26 percent of it but if we
change the representation we would say
26% that he which we actually did also
so this you saw this is not no longer
part of the tool so when I look here we
see that it is actually
this single very large hashmap that is
boxed and contains numbers which we
replaced with a custom implemented
implementation we also have found a lot
of duplicated strings that are referred
to from from Jo flow package here and
it's the the name of the refer that is
shown in the upper right table that were
not interned so they were calculated
every time and there were some
seventy-six thousand sixty seventy seven
thousand duplicates there yeah the same
same as as in more recent analysis I
will also open and show an analysis from
an old version of Mission Control which
is without Jo a flow but an earlier
development version that has some
problems that has been fixed okay I'll
open this one
okay so here first it's also marked some
duplicated erase and this time it's a
byte arrays and we see that most of the
memory most of the waste is coming from
this refer so when I select this refers
I see that it's actually some some few
byte arrays that are similar in this
case we could we could think that they
actually has never been used because if
we look in the inspection object
inspection we see that it's only once in
all of those arrays so probably there
have been created and never used so this
may not be a problem actually there also
a lot of duplicated strings in this dump
and if we click the most heavy the
string with the most overhead here this
string we see that all of those
duplicated strings two thousand and one
hundred comes from L or referred to from
the path class Whistler which is Eclipse
implementation of a path so Sigma so
this seems to have been optimized in
more recent versions of Eclipse because
we don't this is this is a seesaw not
part of the dumps anymore and we can
actually see if we select this referral
we see that and I deselect the user
string I see that this very refer the
path the segment's member refers to
twenty nine thousand nine hundred
duplicate strings and only thirty seven
about thirty thousand strings so it
refers very few unique strings so in
this case string interning would be a
good option we also have this problem
here where a lot of collections are
marked as small and you can see that
most of them are closed setting class in
the
the rocket or in this is the mission
control package for settings so if I
select this we see that this the object
the linked hashmaps refer to from the M
properties are marked with several types
of overhead so 12,000 of them are small
and 14,000 are sparse and so on the
order thank you no sequential lookup of
three elements is not a problem
if we look at the other members of the
setting class we see that it's the same
here are seven thousand almost half of
the array lists here are actually both
empty and unused and even worse than the
cache member where almost all the
hashmaps referred to from this member or
both empty and unused okay that was and
I'm now going to show one more piece of
this demo so this is something that has
not yet been incorporated into the GUI
tool so we are still experimenting with
this stuff this is combined analysis of
a heap dump and code so something that I
mentioned in the very beginning of this
talk so here we have a toy example I'm
not going to bother you with the details
of something real life which is usually
kind of complex enough and would require
too much time to just explain the
various second or the details but okay
so here we have a simple demonstration
program that creates a bunch of classes
each of them with kind of descriptive
name like dupe string holder dead filled
holder and so on so there are multiple
instances of this class and all that
this class does it it just contains this
string so a class called dealt filled
yet filled holder just contains a few
that is never written this class
contains business classes this class is
somewhat more interesting so it contains
a hash
map which as you can see from this code
will never have more than three elements
and that hash map is managed in 30 or
populated entirely within that
constructor and never escapes this class
so this is a special situation that kind
of does occur in practice and that is
kind of easy enough to recognize to
recognize for the automated analysis but
much less so for a human who may need to
sift through a large amount of code just
to determine what's happening so finally
there is this class called empty list
holder which takes an input array and
just blindly copies the elements of this
array into a list that it always
allocates and I have already pre
compiled precompiled keep dump of this
code and i'm now going to run our
command-line tool so this is the tool
that we actually created before they
give a version of gee overflow and we
still use it at least for experiments
because it's just easier to kind of
extend it and make experiments so it's
going to take the heap dump of this
small application we are the classes of
this application is going to write the
results of analysis into that text file
so here how here is how it runs and
basically we are done so now we are
going to look at the very end of this of
this analysis file where the results of
code analysis are contained so look at
this the to analyze the to first analyze
the heap dump and then it looked at the
ID classes and try to see which of the
problems actually can be which other
problems can yield easily recognizable
patterns in the code and furthermore
what things can the tool advise you
about and it found that for example this
small sparse hash map folders
it tells me that these hashmaps
referenced by this field they are small
they have average size of one and
maximum size of one as well so it
suggests that one option is to simply
replace that collection with a single
beta field and then it's up to you to
decide you may inspect the code further
and if the situation is really such that
this code this map always contains just
one element for whatever reason then
really or can welcome to do this simple
change and save a lot of memory then by
the way you see that these problems are
ordered in the these percentages tell
you essentially the amount of overhead
so how much memory would say by fixing
this issue so this is our way to rank
the importance of these problems so the
next problem that it recognized is in
this loop duplicate string holder class
it basically found that this class
contains duplicate strings and it found
all the places where these strings are
attached to the data field of this class
and it basically suggests me to inject
the intern call this at the specified at
the specified line as you see this line
temp is well not that one this one yeah
so this line 10 is where the kind of
three if we add in ensuring here we
would kind of avoid attaching this
duplicate strings to all of the fields
of this guy the next thing that this
tool found is again about small spouse
map holder is just analyzed it for a
different kind of problem so again found
that this hash map says oh but this time
it doesn't if we are kind of not
confident this these hash maps are
always going to have size 1 then it just
recognizes okay at least set the initial
size initial capacity for these hash
maps which is more adequate which will
be 2 instead of the default size 16 so
you can follow that advice as well in
class empty list holder it found that
these lists most of them are in fact
empty so recall that here we just you
know blindly allocate a new array list
no matter what what is the size of this
input string array so basically suggests
says that ok you can actually check if
your string array is empty and if it is
just return a singleton instance of
empty list and don't bother because kind
of this this list is not going to be
populated anywhere else except this
that's right and finally for this class
with dead field it tells us okay this
field is never written actually it's
dead so you delete it
okay and so finally it's interesting
that it's found even a couple of small
issues in the JDK itself where it has
duplicate strings but these in this case
the overhead is really very small so
probably is not worth not worth
bothering alright so that concludes our
demo and give us additional probably
addition of your hand can you please
make this kind of we have a nice Swedish
Swedish language API so I'm not quite
its UI sorry alright so fine let me
finally give some concluding remarks
well actually it's not that much
concluding so here are some details of
how the tool works but probably I'm
going to go over it very quickly
it's probably just enough you know for
most of you to know that this analysis
turned out to be this kind of analysis
turned out to be not very kind of simple
so we have to go over the heap in total
of three pesos actually so we first load
the heap dump and we keep some
information in memory and some
information outside it in a mapped
format because if we try to load in the
entire heap dump and all the index and
whatever in the in memory it will
probably kind of require too much really
so the next one the next pathways can't
keep them sequentially and that is done
partially to collect some general
statistic and partially to find which
instances of strings and primitive raisa
duplicate and finally on the third path
we scan the heap dump
depth first starting from GC roots so
this gives us these reference
Cheng's two objects that we ultimately
present to the user reference chains to
bed collections duplicate strings and so
on some things that we observed about
the two is that it's hip usage is still
kind of somewhat larger than the heap
dump site so you should be prepared for
for that we probably know some ways to
reduce this further but that can be done
kind of very quickly and easily and as
I've mentioned already we keep some
information memory and some some
information outside and by the way the
fact that we a map that hip damn far
means that you have to give you can't
run the tool with the Java heap set to
the very maximum you have to give it
some RAM outside the hip so that the
nmap works fast enough now one thing
that probably kind of should interest
most of you what we really found about
business Java business applications
after analyzing on the order of perhaps
50 or so different hip tubs so not kind
of surprisingly top offending
collections are hash map and array lists
but that's most likely because these are
just the most frequently used
collections in the first place and
within them empty small and sparse our
prevailing problems in terms of the
combined overhead it also turns out that
if the application uses some sufficient
number of concurrent Maps then they may
quickly become a problem it's just
because a single concurrent map has a
very much larger overhead than say a
simple hash map so watch these guys we
also found that boxed collections are
seldomly a problem they may become a
huge problem if the application for some
reason has like our own tool once had a
dominating data structure that map's
some numbers to objects so if you have
really big really big box hash map or
something like that that usually becomes
a problem but otherwise if you have kind
of some number of relatively small ones
they usually be yeah they have some
overhead but not not that much it also
turns out that problematic collections
often spread thin layers around the coat
so basically it this you can of course
fix some low-hanging fruit
so basically fix a couple of places and
improve your a couple of places in the
code and improve your memory usage by
maybe like two or three percent but
really to improve it by like seven
percent or whatever you may need to fix
a lot of lines of code so this calls for
and this is not always practical
especially given that many of these
problems actually tend to come from
various libraries where you may not have
access to source code and which are just
difficult to understand in the first
place so this is something to note that
maybe indeed some sort of automation
either better data structures or what
made the changes to the code or
something like that would work better
than manual fixes duplicate strings I
haven't seen any heap dump where there
would be zero duplicate strings in fact
I have never seen a heap dump where
there would be less than I would say
like 5% or so overhead due to duplicate
strings and much more often it's on the
order of like 8 to 10% and in this in
extreme cases it may take up to 25 and
even 30% so really duplicate strings is
a real issue what we observed is that
usually there is no single string kind
of value except for I've seen one heap
dump where some stupid X or a SQL query
that was repeatedly generated and saved
somewhere and each of these rings was
like thousand characters longer so this
guy ultimately contributed like 13
percent of of the whole heap size so
this was but this was crazy I had never
seen anything like that afterwards so in
most hip dams really a single string
name may not contribute too much or
anything at all but it turns out that
actually a few maybe 10 or 20 string
clusters so locations in the code they
may contribute a lot so the bottom line
is that it is often enough to fix inject
this into an or whatever in a relatively
small number of lines in the code to
essentially get rid of
most of the duplicate strings and
improve your GC time reduce your GC
pauses as well so this is usually a
simple enough and very efficient
optimization and finally one thing that
we observed also that sometimes these
problematic primitive arrays become an
issue and usually that happens if an
application uses some GB or i/o whatever
a library that is either configured
wrongly or maybe it's kind of written
not very well in the first place so
sometimes you can see it 10 to 15
percent overhead due to 0 empty or
whatever long 0 tail by 2 calories and
unfortunately it's not always easy to
fix that and the final remark is that
we've seen reference chains and business
applications that are extremely long
like 1,000 elements so between the GC
root and the final kind of collection
which is problematic you may have like
1,000 hops going from my journal web
thread WebLogic JSP god-knows-what tons
of libraries and tons of layers leading
to the final thing but it also looks
like fortunately in many cases just the
nearest refer or the one two or three
mirrors to first give you enough
information to figure out what to do in
this case so that's the thing all right
so that's the most essential things that
we observe and finally this is the
concluding slide so what we think would
be interesting things to do next so one
of those thing is improvements to the
GUI
so the GUI never suffers from more
better ergonomics more powerful search
and filtering and stuff like that more
powerful code analysis so we have just
probably scratched the surface observed
just a few of the situations which can
be relatively easily analyzed by kind of
code analysis where you can give
recommendations to the user so in
reality of course real engineers
discover many more things so why not
kind of program we program our tool to
discover them as well
one thing that is really a kind of
tempting but an export is automated
changes to the code so you remember that
many problems tend to come from various
libraries or other places in the code
that are difficult to change so
if instead the VMO some tool was able to
to do that for you perhaps that would be
much closer to the ideal world in which
we want to live and the same thing can
be said about better data structures so
perhaps given the effects that we
observed about some kind of JDK classes
and how much memory they use and whether
they so perhaps some kind of rewriting
or whatever amending the some of the JDK
classes would also benefit all the users
by improving memory usage and reducing
reducing GC time so but these are all
issues kind of open for research and
that's the end of our talk thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>