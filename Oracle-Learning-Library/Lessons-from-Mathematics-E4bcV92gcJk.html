<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lessons from Mathematics | Coder Coacher - Coaching Coders</title><meta content="Lessons from Mathematics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lessons from Mathematics</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E4bcV92gcJk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Joe Darcy for many years I
have worked in the JDK engineering group
perhaps so long you might want to start
using floating point numbers to measure
the length of time when i joined the
platform I work mostly on numerics and
over time I branch dr work in other
areas such as language and a little some
release management and most recently the
bug database but i found that many of
the tools i'd learn from my numerical
work were applicable in other context so
this talk is about describing those
tools in those lessons and I found them
applicable to parts of the JDK now this
is a very important message from
Oracle's lawyers you hear it up I guess
you'll just have to catch that slide
elsewhere the conference you'll have
many opportunities to do that so for an
outline we're going to talk a little bit
about the stochastic so part of
Statistics a brief overview of over
2,000 years of geometry a section on
norms which is the way to measure size
and some conclusions and of course it's
good to start with some wisdom from XKCD
here is a very robust random number
generator and you can see it's even i
triple e compliance there's an RFC which
mandates that this is the proper
behavior so when statisticians talk
about problems they'd like to use coins
so here here are ten coins silver
dollars and of course mathematicians
also like to assume that these are fair
points and that tossing coins different
times they there what they call iid
events independently and identically
distributed so if you'd want to compute
the probability of getting ten heads in
a row is you know top here you'd
multiply the probability of getting one
head so on a fair coin we say that's one
half so the one half to the tenth power
so that would be one out of 1024 let's
say we have a different sequence of ten
coin tosses say ten tails instead what
would the computation for that be like
will be pretty similar we take the
probability of getting a tail which is
again one half because it's a fair to
point can we multiply that together so
we get the
same answer namely one out of 1024 let's
say we take a third sequence of
positives in head tail tail headed head
tail tail head head well the probability
of getting this be well again we go to
the equation and we multiply all those
probabilities together and once again
all the probabilities are one half so
for a third time the answer is the
probability is 1 out of 10 24 so the
probability of each of the 1024 possible
sequences of corns is the same it
happens one 1024 the time therefore
unlikely events are happening all the
time right because each sequence of ten
coins has the same probability and if
you toss a coin 10 times you will get
some sequence of Ken 10 coin tosses so
it is extremely unlikely that nothing
unlikely happens now on the other hand
is the coin fair is that really
something you should assume so I think
we'd agree it'd be unreasonable that if
you saw a coin come up heads once you'd
assume like well every single time I've
seen this coin tossed it's come up heads
so I'm going to assume it has a one
hundred percent chance of coming up
heads next time would that be a
reasonable thing to do how many people
think that be reasonable I have some
coins here later if you think that's a
reasonable thing to do ok so here pardon
yeah so that would not be a reasonable
thing to do I in team seems a gross
exaggeration to claim from one data
point that you've known everything about
this this random process let's take a
different situation let's say you have a
coin it has come up heads 30 times in a
row is it reasonable to assume that
after you've seen it come up 30 times in
a row that there is still a one half
probability that will come up heads next
time would you want to take that bet
perhaps not so between going from one
coin toss coming up heads and 30 coming
up heads perhaps there'd be some way you
could up
your expectation about what the coin
would do and that's called Bayesian
reasoning in statistics the main hammers
are the main tools they use are things
like the law of large numbers and the
central limit theorem and what those
theorem say is that over time the
average of what you would observe tends
to be the actual behavior of what you're
observing so that if you observe the
coin long enough the probability comes
up heads versus tails is the actual
probability that will come up heads
versus tails so how do you rationally
update your expectations of what those
probability are given what you've
observed the way you do it is you make
assumptions so there are somethings who
you assume a distribution and then you
update your estimates of that based on
the observed data so what might that
look like so a draft here is the uniform
distribution of values between 0 &amp;amp; 1 so
you say well I don't know what the
probability of the coin coming up heads
versus tail is I'm going to assume the
probability if coming up heads could be
anything between 0 &amp;amp; 1 now the average
of the set of distributions here is
one-half so the average is a fair coin
but that's not actually what you're
assuming you're assuming a set of
distributions and you're going to update
your expectations of the set of
distributions over time so what would
that look like
build out the slide here yes okay so
here's the graph showing how you update
your egg the average expected value of
the coin toss is you get a successive
series of heads so as you can see up
here you start out with one half after
when you have no information about what
the coin actually does the average you
assume from what they call the prior
distribution it averages out to one-half
if you see the first coin toss it goes
down to a third and the formula to
update it properly is that the average
is 1 over n plus 2 so as you get more
and more coins coming up heads you think
there's a lesson lost less and less
probability it's going to come up tails
and this is a rational way to do that
based on the expectations of what you've
seen and at the end of that process
you'd have a set of skewed distributions
such as shown here on the white meaning
the weight of the expected value has
tended toward what you've actually
observed which is a series of heads so
after you've seen 30 heads come up your
your expectation is the coin probably is
unbiased at that point that's more
likely than not now this is a model
problem with coin flipping but it comes
up in a lot of other situations to this
is how you'd formally described say
defects on a manufacturing line you want
your manufacturing line to be reliable
of course but even if you've seen a
hundred or a thousand or maybe 10,000
items without a defect you don't assume
there aren't any defects on the assembly
line because you know they're made some
defects in manufacturing process so if
you want to estimate what your defect
rate is you use this sort of technique
about updating your expectations based
on you've seen so if you've seen a
hundred thousand items come up without
defects that's a much stronger statement
than if you've only seen ten thousand
but these sort of ideas are useful in
other situations too for instance we
want a lot of tests on the JDK before we
share
they usually passed they mostly passed
and we update our expectations about the
quality of the jdk release based on the
test results we have of course that
doesn't mean there aren't any bugs or
any surprising situation but the more
tests we won the more faith we have the
the higher confidence we have in the
results we've also used this kind of
reasoning informally when evolving the
java programming language for the
project coin features in jdk 7 we were
concerned about some of the
compatibility impacts so we instrument
the compiler ran it against millions of
lines of code in various open source
collection of projects to judge what the
compatibility impact was for instance
there was a theoretical compatibility
impact for the multi catch statement
with more precisely throw compiler could
recognize more dead code and therefore
not allow certain program to compile but
when we looked over millions and
millions of lines of actual code we
didn't find any instances of that
occurring because of that we thought
this would have a very low actual impact
on users of Java so we felt comfortable
going ahead with that expectation
without change now it's possible of
course that some people are using that
programming idiom but it's a pretty easy
to go around there are even little
things like the bug tale of strings and
switch I work on strings and switch the
implementation I also wrote a very
extensive set of tests the code in Java
Sea had pretty much one percent code
coverage so I was very confident of the
quality of strings and switch however I
was surprised shortly after we shipped
jdk 7 there were numerous bug reports
that case of parenthesized the string
would crash the compiler on all the code
to run past strings and switch this sort
of eating didn't come up and more
recently we've also fixed an explicit
cast to string so even if you have a lot
of testing you can still be still be
surprised so I some of the lessons I
would take away from things like this is
be careful of your assumptions and of
course to pay attention to check how
things of all compared you how you
expect to evolve going on to a very
condensed history of geometry going back
even farther back than high school
sophomore year where this is often
taught in us we go back to Euclid
elements around 300 BC and classical
Euclidean geometry is based on five
postulates or five assumptions straight
line segments can connect any two points
given us line segments you can extend it
to be infinitely long so far pretty
reasonable given a line and a point you
can define a circle again seems pretty
reasonable the fourth poslat all right
angles are equivalent congruent to each
other and finally the parallel postulate
there's a couple ways to phrase it the
one here is in a plane if you have a
line and a point that's not on the line
there is exactly one line you can draw
through the point that doesn't intersect
the first line now this fifth postulate
was very complicated and even a little
suspicious at the time and we'll get get
into that a bit later but if you assume
these five postulates and if you get a
little more formal have some other
mathematical structures to find you get
all of a Euclidean geometry so you get
familiar results like the Pythagorean
theorem that the square of the
hypotenuse of the triangle is equal to
sum of the squares of the other two
sides that result is actually a special
case of this more general result called
the cosine rule that relates the sides
in areas of non right triangles and
since the cosine of 90 degrees is 0 this
term drops out for right triangles
another familiar result from plane
geometry is that the sum of the angles
of a triangle is 180 degrees and
everything else however um what about
that fifth postulate people even even
back around you lose time didn't like it
they thought it's too complicated they
tried very hard
you get rid of it to prove it in terms
of the other for their efforts along
those lines throughout the sixteenth and
seventeenth centuries there are several
failed attempts at proof by
contradiction let's assume that the
fifth postulate isn't true and see if we
can get a contradiction and therefore we
know that well it has to be true because
we've gotten its contradictions so
around the eighteen hundreds various
mathematicians started thinking well
what would happen if we negate the
parallel postulate now the parallel
postulate has a couple parts to it one
of it there's a point off the line and
you can draw exactly one point that
won't intersect well there's a couple
ways to the gate that statement you
could say all right well let's say it
will intersect every time you have a
point off the line and you draw another
line through you will get an
intersection guarantee that's one way to
make that postulate false and if you
make that assumption what you get is
called elliptic geometry and let's say
instead of having a single line you say
all right there could be infinitely many
lines that don't intersect that n goes
through that point if you make that
assumption is that you get something
called hyperbolic geometry and let's
take a look at what those systems look
like so there we go so elliptic geometry
takes place on the surface of a sphere
or a screw like body and the way you
define a line on a sphere is a great
soccer rap so something that's as large
as possible this is the sort of route
that plane is typically fly since it
reduces fuel consumption if you ignore
wins so in that case if you have a line
say the equator on the earth and you
have some point that's not on the
equator you draw another great circle by
necessity somewhere along the
circumference that others circumference
it will cross the equator because it has
to go all the way around because you're
making a circle as big as possible
if you have the hyperbolic case instead
of taking place on the surface of the
sphere it takes place on a saddle shaped
surface like this and because of the
curvature of the saddle shape you
actually can draw many many lines
infinitely many of them through that
point they don't intersect the first one
and if you make these different
assumptions you get equally valid
geometries result they're equally valid
mathematically but the results look a
bit different for instance these are the
cosine rules you get in elliptic and
hyperbolic geometry now there is some
shared structure here you can see that
for the hyperbolic case the cosine rule
instead of having normal sines and
cosines has hyperbolic sines and cosines
you might might recall those from a
calculus class so they have a different
expansion Taylor expansion and sine a
cosine but they have some similar
properties so they have a similar name
now if you take the limits of the
spherical in hyperbolic cosine rules for
flat surfaces if you say if you make
this fear very very large then in the
limit you'll get back the familiar
euclidean cosine rule so even though
these are all derived based on different
assumptions there is a commonality in
all three systems and if you want to see
that on the surface of the earth one of
those other familiar results from
Euclidean geometry is that the sum of
the interior angles of a triangle is 180
degrees so if we take this picture
conveniently courtesy of wikipedia we
can say we have an arc on the equator we
go straight north so when you do that
those are two 90-degree angles so that's
180 degrees right there and however long
you make that arc when they come
together at the North Pole there's going
to be some angle there so you're going
to get more than 180 degrees now these
other geometries again they're equally
valid equally consistent mathematically
but they do have different properties
but they are very useful one reason
they're useful is that we live on the
surface of what's roughly a sphere and
we don't live on a plane if we lived on
a plane we might have to live
a somewhat melodramatic a word like this
it was called flatland and read the
flatline book it's a short short volume
so it's kind of entertaining read but
more importantly the mathematics behind
elliptical geometry form some of the
basis needed for general relativity
should be developed so there's a very
high leverage result from this work that
was done over many many centuries so
some conclusions I take here is that
patience is virtue this was a hard open
problem for thousands of years people
had suspicions about the parallel
postulate they are uncomfortable with it
but it took many many centuries for
people to find a productive way to
channel that discomfort and also that
there's more than one way to be
consistent and that because of that when
you're stating a truth it's important to
know what the assumptions are and also
that it can be very powerful when you
can change the rules the results in
having these non-euclidean geometries
had a big effect in philosophy and also
mathematics and are related somewhat to
things like girdles incompleteness
theorem which was a very important
mathematical and logical result in the
20th century and the essence of girdles
result was that in a system a
mathematical system that sufficiently
interesting we're sufficiently
interesting means you can describe the
usual rules of arithmetic there are true
statements in the system that you can't
prove true so that's a little
disappointing feels incomplete so you
say I know how to get around this
problem we have postulates before this
statement we believe it is true but we
can't prove let's add that as new
postulant done the problem is given the
self-referential nature of girdles
result when you do that you get a
different set of statements that are now
true but not probably true in the new
system we just added a poslat and you
know you can't just keep out he
postulates you get the same st. Paul
back again and this is considered by
some to address Hilbert second problem
of proving that the axioms of
rithmetic are inconsistent Hobart was a
mathematician who lived in the early
20th century and he laid out a series of
23 or so problems and that was the basis
for much of the mathematical work that
was done in the 20th century and many of
those problems are still open so let's
change gears a bit and think about what
you can do or what kind of systems can
develop when you make the rules so let's
start out with a very simple system
arithmetic on positive integers where we
define in addition and that the graph of
that function would look something like
this is these step functions here and
this is kind of how we start out
learning arithmetic in grade school
right which is that positive numbers in
addition but we don't stay with positive
numbers in addition very long once we
have addition then we want to have the
inverse operation of subtraction and if
we limit subtraction to the cases where
one number is bigger than the other then
it's all fine and dandy because we just
got back positive integers but of course
if you subtract four from two then
you're kind of stock you only have a
partial function you have two inputs in
the system grab an operation like to
define but there's no defined result so
what do you do in this case what you do
in this case is often you make do math
and new math you make there of course is
just negative integers so you extend the
system to include negative integers and
then we get a full graph like this shown
on the right hand side but we've seen
this many times throughout the math
we've learned over the years in school
we start out with multiplication of
integers again that gives us just
integers but then we add a division what
happens lots of complicated things
happen now we have to have rational
numbers fraction but even there we still
can't divide by 0 if you're familiar
with I Triple E floating point I Triple
E floating point defines special values
when you divide by 0 they have things
called namm not a number as well as
infinity and those are to provide
closure to the floating point system so
that for each two operands and each
floating point operation there is some
divine find result even if the final
result is one of these special values
that's kind of odd to work with
now we're not done when we have rational
numbers rational numbers are very good
they form a mathematical structure
called a field that has all the
properties were used to commutativity
associativity and so forth but they're
really not sufficient and one of the
rate ways you can see they're not
sufficient is if you start taking roots
if you do something simple even like the
square root of two there's a very famous
result won't agree to that is not a
rational number there is no fraction a
over B that's equal to square root 2 so
when you take a square roots or you take
rusev equations you get irrational
numbers but more technically you get
something called algebraic numbers
algebraic numbers are ones that are
roots of polynomials of integer
coefficients if you want to formally
described all real numbers instead
that's a lot more work and I won't be
doing that in this talk which is which
is good and the fundamental reason
rational numbers aren't enough is
because they don't include their own
limits so let's take a case here we have
the expansion of Pi 33.1 3.14 and so
forth and we know pi is irrational it's
actually very irrational called
transcendental and what this sequence is
every number in this sequence is a
perfectly valid rational number right so
a 3.14 is 314 over a thousand 3.14 one
is 3141 over 10,000 and so forth so
every number in this sequence is
rational we know that there's an upper
bound and usually that's they converge
we can show they converge and usually
when you have those kind of properties
you want to know that what you converge
to is part of the system but
unfortunately for the purposes of
simplicity that's not enough because we
know this will not converge to rational
number despite having all these
otherwise very good properties and it's
because and only by going to real
numbers do we allow something like this
to converge within the system and not
have to go outside
so here's a quick died about diagram
about that and the box is all real
numbers one way we can divide these up
is into the rationals and the ear
rationals we can also overlay algebraic
numbers on that algebraic numbers
include all the rational numbers that's
pretty easy to see because if you take a
over B you can write a polynomial that
has that as a root that polynomial is
just be X minus a equals zero so if you
see if you plug in a 0 would be 4x
you'll get zero as a result so that's
easy to see there and then numbers like
Roger two are included algebraic numbers
again because their solutions assistance
like this and that leaves the
transcendental numbers that don't have
this property and most real numbers are
actually transcendental because there
are a countably infinite number of them
but there's only a untimely incident
rather but there's only a countably
infinite number of polynomials so this
might seem fairly esoteric talks about
partial functions and adding values and
sup but there is actually some relevance
to this to doing things in job api's we
actually see what amount to partial
functions all the time so you have an
API for javadoc it's as it returns XYZ
based on the inputs unless these other
conditions occur and then you get
exceptions so you can think of the
exceptional cases be as being a partial
function for the overall method
sometimes it returns a normal result and
sometimes it returns an exceptional
result which you can think of as a
partial function you only get a normal
result back for a subset of the possible
inputs and in the Java platform the j ck
tests are very thorough on a negative
testing so we do care about the behavior
of the so called the undefined behavior
of the platform but we think it's fair
game to shrink that as time goes on so
generally we don't think it's high
ability issue if we increase the set of
inputs for which you get a normal result
as opposed to
throwing an exception so that's
perfectly fine also throwing a you're
returning no could also be viewed as
having a partial function now in the jsr
269 annotation processing API I worked
on we also had to deal with partial
functions over time because the new
language be evolving so I'll describe
that over the next few slides so we have
a language model so we have to model the
language you know the language is going
to change we anticipated we want to
change it back into ATP 6 so if you're
writing code that operates on a language
model what should your code do when it
encounters an object that represents a
structure created after your code was
written so let's say you have something
like a jdk six error annotation
processor and it comes across an object
representing a module definition from
JDK 8 what should happen in that case
well it's you to throw an exception
should it see some approximation of the
new structure in terms of the old
structure there's no one right answer
and there's actually name for this
problem in evolving interface
hierarchies called the expression
problem that was a point a coin term by
fill water the basic way we chose to
approach this problem is by using the
visitor pattern which is how you operate
on people three with the visitor pattern
we poll the audience ok so basically
know how that works you define a method
for each type you have in the interface
hierarchy you have the visit method that
calls when appropriate and so forth so
this lets you do double dispatch and
have the effect of adding operations
without having to edit the types of
question so what happens though when you
have the data case java sea sick
structures you have the eight ones what
should the visitors do for your java 6
error code when they see a Java 8 or
object the semantically right answer
depends basically what this problem
escribes is once the language changed
what was a total function that you
Oh in jdk six is not only a partial
function so we have library code that
needs to fill in that gap for you or
help you fill in that gap and the right
way to fill that cab varies if your code
is doing something just counting methods
if it comes across a module definition
maybe there's nothing else to do it
should just ignore it but what if the
code is doing something like checking
naming conventions if a structure comes
about the camp after your code it might
have naming conventions you don't know
about you can't check it perhaps you
need to throw an exception in that case
and when we were considering this
problem in the jsr there's a paper
published the expression problem
revisited for solutions are using
generics I was very excited when I heard
about this paper I went to read it and I
was quite disappointed because each of
the solutions provided was worse than
the problem at described it alerted the
code with generic type parameters and
was not workable at all so we ended up
doing something else we end up adding
another point of indirection that
allowed the author of the code to decide
how to complete their function how to
make their partial function of complete
function even on unknown inputs when
they wrote the code the read the way we
did that was we added a visit unknown
method to the visitor interface so this
does not correspond to any of the type
hierarchies that are being visited it's
a new method explicitly for the point of
future evolution of the code when we
release new version of the language we
added a method to the visitor interface
this is very atypical for JDK code
evolution we generally do not add
methods to interfaces but in this case
we thought it was appropriate but
besides adding a method to interface we
have a set of version abstract visitor
classes that have the right default for
the release in question so when a jdk
six era visitor is a the abstract
visitor in jdk 7 or 8 that has new
methods added to it do the right things
for
generate structures and that default is
to throw an exception because they're
not known there's a separate exception
for visit unknown a jdk 7 visitor jdk 8
visitor will do something different for
those structures because it's assumed
that if you're writing code with a jdk 7
visitor you should know about everything
in jdk 7 and if you're writing against
the dedicate visitor you'll know about
everything in jdk 8 and the benefit of
this is the client code gets to control
how their partial function in the face
of a new platform is turned into a full
function but there is a price for this
there's extra work about for the client
to do this properly not all annotation
processors will be written this
carefully novel visitors will be there's
also sizing and Fleck city added to the
API and it is very atypical for a jdk
platform evolution it also does make it
hard to test I did have to have a few
chats with our testing teams to explain
gonna know it's okay you will you should
not see calls to the visit unknown
method in jdk six because the situation
that will trigger this doesn't exist yet
it won't exist until jdk 7 or 8 and that
was many years in the future so one
conclusion I draw a parallel between the
work that's done to say fold up
something like real numbers in analysis
versus this much smaller problem of
building up this visitor structure is
that sometimes you need a rich enough
structure to support the operations you
need to do and that might be more
complicated than you want but you have
to go through the work to put that in
place otherwise you can't get your job
done another case would be that for
things like job generics or cold methods
in project lambda we're making
additional structures to the language
not to make it turn complete because it
was turning bleed in the beginning but
to allow a richer set up operations
flouse to evolve things better
finally we're going to talk about norms
which is way of measuring size so
formally a norm is a function with three
properties it Maps some element to real
number and it only maps zero elements to
zero it obey something called the
triangle inequality which is the norm of
X plus y is smaller than the norm of X
plus norm of Y so that's the usual
relationship between the sides of a
triangle right the that that holds and
also if you multiplied by a constant you
can just pull that out now they're
actually many functions that have these
properties so here are a few of them are
they different names so the first one is
called the one norm and in the one norm
all you do is you take the absolute
value of the numbers and you add them up
so for the two norm you might recognize
this as a Euclidean distance or one way
you usually measure sighs you take the
square root of the sum of the squares
you can generalize that by what's called
a key norm instead of taking a the
second power you take the peak power so
you add up the absolute value of the
cubes and then you take the cube root or
you add up the absolute values of
raising it to the fourth power and take
the fourth root and so on and in the
limit you get the Infinity norm which is
just taking the maximum element now the
reason this is called the Infinity norm
is if you make P very very big that
tends to emphasize the contribution of
the largest element how do you think of
vectors at different sizes you take one
you raise it to a high power that one
gets really really really big dominates
all the others and then when you take
the routing everything gets really
really really small and you're basically
left with what you started with and in
the limit you just end up with infinity
so let we need a quick center you check
about whether these functions each have
those three properties so one property
is about zero take the one norm the one
norm is only going to be 0 when all the
inputs are 0 that's fairly clear right
because routing absolute battles same
thing about adding up the square roots
and so forth let's just look quickly the
second the third property about if you
can pull a constant out if you pull out
a multiply all the values of x by see
the way the arithmetic is fine you could
just pull see outside instead and you
likewise you can verify that on the two
norm you get the square of C with each
element and you could move this square
root of C out so it all checks out so
each of these functions satisfies each
of those three properties so why would
you want to have more than one norm all
the norms are equivalent in some sense
over vectors tell groups numbers if one
of them is getting small they're all
getting small and you can bound the
distance between them with two different
constants a and B here and often you'll
care about a sequence of vectors in some
kind of iterative process and if the
sequence is converging under one norm
its converging under all them and here
are the actual numbers here we can see
here that the one norm is between the
two norm and the square root of N and
the too long for instance and we'll see
some graph speak later oh why do these
differences matter why do you have more
than one more sometimes one norm is
easier to work with than another for
instance the two norm is differentiable
and that means you can apply all the
tools in your calculus 2 lakhs to the
problem some of the rooms are cheaper to
compute than others if you want the max
that's just a scan of the array if you
want to do something like compute the
square root of the sum of the squares
accurately that might be a lot more work
mathematically so sometimes what you'll
do is you'll really be interested in one
norm but then you'll use another norm to
get a bound on so for instance if you
want the tune or doing small than
epsilon instead you can get the affinity
norm to be smaller than epsilon over n
you'll know that the two norm is molded
what you want so you can use one to get
the other through this equivalence now
however why all the norms are equivalent
when they're getting small there are
regions where the different norms will
give you a different answer for the
relative sizes of vectors and we'll see
that in next few slots and we can see
that by looking at the so-called unit
circle in different norms
so in the two norm the unit circle is an
actual circle that is this is the circle
of vectors of size less than 1 under the
too long so it's a square root of x
squared plus y squared which is a circle
here is the unit circle under a
different normal which norm is this
that's picture to the yellow yellow
square one that's right it's the one
norm why is it the one that's right it's
the mat distance or the taxicab metric
and the boundary of the yellow square
all equal to one so for instance we can
see that at the cardinal points 0 1 1 0
and so forth we get one so at that point
the two number one or more equal those
four points but also along the line here
it's equal to one so in particular all
these vectors here have size one and
these are just along the line the
diagonal here so for instance the point
one half one half is equal to one under
the one normal and that's why lines here
so what's this blue box ABS infinity
norm like that's the other norm here and
that's taking the mac so we can see it's
a larger area that that's involved and
again at the four corners here it's
equal but in other places it's not so
let's see oh yes where would the P norms
appear in this diagram that's that's
right so the P norms were between the
two norm in the Infinity norm so you get
boxes that were over time more and more
square like that with the sharpened
rounded corners to fill that out so on
the next slide we can see the relative
distance being different under the
different norms so let's take two points
a point a at 34 and we have another
point B at 51 so if we take the one norm
of a what's that equal to 4 comes up on
the slide 7 right we just add three and
four what would be the one norm of point
B and at six of course we just add them
up so under the one norm point a is
larger 7 verse 6 now let's look at the 2
norm so what's the to norm of point a
fight 3 4 5 triangle so the enormous
five and as you might guess the norm to
norm of B is not equal to five it's a
little more than five because that's the
way the math works out so under the 2
norm point B is larger and under the one
norm point a is larger and these are
both valid ways of measuring the size of
a and B but they give us a different
answer in this case and you can see the
region where that occurs here this is
the read this is the boundary for the
too long being a five and we can see it
goes through point a but doesn't include
point B it doesn't move the center here
and if we include the diagonal which is
of the one norm this is
or 11 or being equal to six that
includes be but that a outside of that
so is as larger so this sort of
phenomena comes up in other cases too
even though this is a more formal way of
describing it so which is faster quick
sort or merge sort right it's an
excellent question you need you need
more information to answer that are you
worried about the average case which
we've described it the one norm or are
you worried about the worst case which
we described with Infinity normal and
you know this comes up in other cases
well like how should benchmark subscores
be confined or how do you value doing
very well on one part of the component
one part of a benchmark versatile whole
thing how they defined together so these
are some of the issues that come up I
think these sort of ideas are also
relevant to things like managing release
in a software project for instance at
the start the project if you're on the
release team maybe you have something
closer to the one or unexpected case of
the risk of making a change you know not
likely something bad will happen and
it's okay you just let him but is the
release states gets nearer and nearer
you get more and more conservative and
you start putting more value on guarding
against the worst possible case that can
happen that's more like an infinity norm
you're guarding into the worst case so I
view that as you're increasing the
p-value using for the dorm the closer
you get to the release date these sort
of ideas have also informed the project
Coyne project which went into the ATK
seven I wrote a blog entry a few years
ago about how to measure the size of the
language change the deliverables for a
language change in Java include the
specification the reference plantation
and testing and coins had to be small
and you had to be small in both of these
at the same time so what is that
correspond to
corresponds to the Infinity norm we're
looking at the worst case if it was too
large in any of those three areas we'd
say was too large to be considered as
part of the platform part of the reason
for that was that you know different
people working on it and that we
couldn't switch say the testing people
to work on the spec and vice versa there
are different ways to measure the size
of the coins that went into the platform
depending on how you wait implementation
effort versus design complexity so you
worry about what the norms are there so
I brings us to the end and some metal
essence one lesson I take from math
here's the question assumptions because
if you make different assumptions you
can get different results and sometimes
you can change your assumptions and get
results that are better in some
circumstances also there are often more
than one consistent way to do things
like we saw at the norms for quick
search the first mergesort sometimes
it's valid so you care about the average
case sometimes it's valid to care more
about the worst case so it's important
to know what's important in different
circumstances so you can have the right
criteria to make decisions also I think
tools like this can be applied in many
different areas not just throughout the
JDK but other technical areas and
finally I'd like to end with a quote by
Don Knuth encouraging people to be an
edge not an oak that is in a graph
theoretical sense you don't just want to
have expertise in your area you also
want to connect to the expertise in
other areas and bring people together
and with that I'd be happy to take a few
questions and thank you all for coming
tonight I know there many other things
to do in San Francisco on a weekday
evening the size coming to talk about
very esoteric subject so thank you for
coming by
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>