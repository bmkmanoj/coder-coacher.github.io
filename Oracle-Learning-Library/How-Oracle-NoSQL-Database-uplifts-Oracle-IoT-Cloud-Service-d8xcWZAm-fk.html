<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How Oracle NoSQL Database uplifts Oracle IoT Cloud Service | Coder Coacher - Coaching Coders</title><meta content="How Oracle NoSQL Database uplifts Oracle IoT Cloud Service - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How Oracle NoSQL Database uplifts Oracle IoT Cloud Service</b></h2><h5 class="post__date">2017-05-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/d8xcWZAm-fk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone this is chanel Cooney
City I am a senior director in Oracle
IOT cloud service team I'm going to talk
about how we leverage or koenigsegg work
what are the great features we used in
IOT and how no sequel contributed to our
success let's briefly talk about IOT
before we go into the details what I
think most of you know about IOT are
your T stands for Internet of Things the
it's originally coined in 2000 but it
started becoming more famous more
popular in the last two three years for
various reasons right for better clarity
much better Big Data platforms that we
have to leverage the data that we get
the cost has reduced significantly for
the devices sensors and overall business
value so what is your T it is the
network of physical objects but
essentially all the smart thing the
smart devices
fitbit's webcams camera sensors
everything is connected right the power
of the hyper connectivity we can do many
things the monitoring prior to actions
right improve experience right we talked
about some of the specific use cases
later on right and it's in a massive
scale competitor ahead of people right I
think just last week
Gartner came up with 100 billion things
will be connected by 2025 right some
even the conservative numbers has put
220 billion by 2020 right so we see a
lot of potential there and it's a very
disruptive technologies right what they
say is mobile cloud big data IOT is the
next in that trend of the new disruptive
technologies we are same so why does it
matter why do we need to pay attention
to that right right with IOT we get huge
amount of
that an information right and that we
can make important decisions right right
we can promote and make important
descriptive decisions based on the whole
data right with aut the huge amount of
volume with them as we talked about the
number of sensors right they're not in
millions anymore right it's like
billions of data devices the amount of
data they descending is humungous easily
terabytes of data per weeks and months
of that days not just the volume of
devices but the velocity in Wiki
descending right unlike previously where
they used to send days and hours
now it's seconds and milliseconds these
devices keep sending us data in seconds
and milliseconds different varieties of
data each sensitive one has their own
type of data so they be sending the
message in different formats and much
more very secure right much more
accurate much more appropriate data in
all that one
so with this right huge potential with
more volume data with more accuracy we
can do much much better right we can
solve the last mile connectivity problem
right which we which can enable us to do
better automation but the biggest
problem we have today is the disconnect
between the OT and IT right on the
operational things we have all these
devices and machines and IT we have all
the enterprise applications what are you
T does is bridge the gap between ot and
IT that enables better automation right
we can do things more proactively
there's a mass of an investment in IOT
from all the large companies right and
so this is a pictorial view that was
given by the UT Bowie forum if you look
at the bottom of the stack we have all
the physical devices the things the
so-called things nay ot right and they
use different connectivity protocols 80
blue to
ZigBee mqtt Wi-Fi right so either they
can talk to the server IOT server
directly or in most cases they go
through via some kind of a gateways it's
computing once we have the data right
then you need to abstract that right we
need aggregate the data we need to
analysis we do prediction forecasting
anomalies on the data right and we are
building custom applications that can
leverage and which are very
domain-specific and on the top of the
stack we have all this collaboration and
processes and all that one right so
we'll talk about that how a UT or
clarity cloud service has a plane each
of this layer so let's shift gear and
talk about our clarity clock service for
a minute before we talk about the new
sequel value-add if you look at the
world it is a huge paradigm shift that's
happening across every shift of life
right on the maintenance side we used to
go these to fix things when they get
brave and then we started getting better
we started scheduling things but to be
more efficient we need to prevent
predict and prescribe right you need to
probe all predict when the issue is
going to happen right we need to based
on the prediction analysis and to top it
off we need to prescribe okay we know
it's going to happen how do we fix it
can we prevent it and what are all the
possible solutions and all that one
that's a big thing and for that you need
much accurate data right live real-time
data as well as a create data in the
ownership in the consumption models is a
shift from ownership to as a service
right instead of being you owning it you
just want to leave something as a
service for the consumption period the
same things happening in the analytics
model right in the old days we is taking
the data and then we used to run
analysis data mining SAS on a post of
in cases what we should do is there's a
shifting interest rate instead of doing
that static analysis we are doing more
real completed analytics based on the
whole lambda architecture streaming
analytics and all that one and from a
service model
mr. of a central service it's all
self-service whenever you want you can
log on to the tank and leverage the
service how does a you disturb the or
political is going to help you in this
one right so we have a very robust
platform to start with right the
platform can do the device
virtualization Gately is it has basic
modeling aspects application data model
right extreme analytics and all that
well and good but to leverage it and
build particular applications it's time
consuming right and it takes probably
six to nine months of implementation
time to cut short that to reduce the TCO
right one other thing that we are doing
is building smart applications on top of
the IOT platform right we call this
packaged domain-specific applications
right these applications are very lob
specific but and they're all these
algorithms and anomalies that work very
low be specific and all that one and as
part of that we certify the gateways and
devices so overall it cuts that option
time right with that we take the creates
faster ten to you
so here are some of the IOT application
application that we are building in the
first phase right we have asset
monitoring production monitoring fleet
monitoring and connected workers right
let us look up a couple of them just to
give an idea of how it will be
specifically to win specific they are
asset monitoring as the name implies the
goal is to monitor the assets that the
health of their says the utilization of
the SL and more importantly their
location of the etcetera we want to
start one thing they said whenever the
asset leaves it decide location you want
to be notified so the basic premise of
asset monitoring can be leveraged with
this vehicle app next is the production
monitoring right the goal of this app is
to monitor your factory right you want
to look at all your machines in the
factory whether they are up to the mass
what is the production capabilities do
they meet the production output
they're supposed to meet and if the
machine goes down how does it impact the
overall production output and whether we
can proactively to place the machine in
a very short time and see whether we can
meet the person output right so the
whole production equipment monitoring
and prognostics can believe it by the
SAP and the other app that we'll be
working out frequenting and connected
workers so if you look at the overall
IOT cloud service we have died ot
platform at the bottom of the stack with
the whole premise of connect analyze and
integrate connect with the devices
analyze the data that data integrates
with enterprise applications and other
cloud services on top effect we're
building the so called mini apps as
they're set monitoring the Freak
monitoring approach model we just talked
about that but and on top of it at the
high level we have this cross
application collaborations right so if
you go back to the
if you google to the supplications right
so this is where we are building out
mini apps and all that one right the IOT
platform gives you till number five
right so we started doing number six
which is building applications and later
on we'll be working on much more cross
application collaboration called voice
of factory application so this in a
nutshell is or Colaiuta clog service
right we have the platform we have the
mini apps we have this high-level cross
up cross vertical solutions voice of
Factory is a label centered maintenance
right in addition to that we provide
various integration with Oracle cloud
services mobile cloud service bi storage
so ah cloud service etc so I think
hopefully that gives a good overview of
IOT or critical service and that with
that we can segue into how we use Oracle
no sequel so if you look at in general
the advantage of an Oracle database is
the main thing is their complete
horizontally scalable right you can
easily add a shard no sequel can
automatically scale out and balance
across charts it's a huge feature with
IOT as we talked about that the amount
of data the velocity of the data is sort
of unpredictable right so we can start
with the baseline and keep adding charts
whenever the capacity reaches a
threshold so we should be able to have a
platform that can hold up a scale right
without zero without any downtime with
zero downtime right so this is the one
of the biggest feature that we got from
low signal compared to the database
curse and all that what right ability to
scale with zero downtime and the data
model
musical provide is very simple it's a
simple key value that will allow us to
support a variety of data as we saw here
with our UT the data that we get is not
the same from different devices and
sensors so we should be able to adapt
and ability to support multiple
varieties of data right so we cannot
have a structured schema for that so we
went for this key values approach where
we should be able to support any type of
data extreme performance what we talked
about the volume and the variety of the
data that asset e of the data that comes
from my UT devices right you need to
scale to that extreme level of velocity
so for that I think we our rights have
to have minimal throughput right with
minimum latency with maximum throughput
right I think I'll talk about that in
the next slide the KPIs that we have and
how we were able to meet that KPS with
no sequel built in a chain scalability
we need to have a system that is 24/7
365 days right
we cannot have zero downtime because of
the number of devices that will be
connected and all that one right the
moment we lose connectivity and all that
one you can potentially lose data and
that might impact the forecast and the
anomaly detection some of the other
advantages are a little bit more
intangible like online maintenance
management upgrade right and
the Oracle no sequel advantages or other
it's again because you can have shout
across different data centers that we
give us a much more dr capabilities even
if a data center has some issues and all
the fun so some of the
we talked about the previous slide are
very common to all new security
technologies but how does Oracle know
sequels stand out what are the specific
features in order Oracle new sequel that
we really got attracted to right couple
of things is we as we talked about that
in the previous slide and before we need
to have something with very high
velocity of data support right so we
need something which is very fast right
throughput with low latency so our KPI
was 10 kms per second right with the
traditional databases with all the asset
transactions and the commits and all
that one right
this is pretty hard to achieve what
right of course it completely admissible
you need to have achieved but you need
to have a very high hardware powerful
hardware right but with a low hardware
with a commodity hardware with no sequel
we should be able to achieve this number
with a small set of shards we talked
about the data modeling right the other
advantage is most of the IOT data time
series data right so this gives an
advantage the way no sequel is modeled
and all that one the queries by making
that as part of the shard that gives us
the advantage the short key right we
should be able to easily query the data
based on the times time time zones and
the time points the other advantage we
have in musical is seamless integration
elasticsearch right the the latest
version of no sequel supports syncing of
data with elasticsearch you can write it
to no sequel it will do an automatic
sync to elastic search and make sure
that the data is in sync this gives us
very flexible architecture where we can
query either ignition or sequel or with
elastic search the other
feature that we use heavily in IOT cloud
services the TTL feature TTL stands for
time to leave right as this we talked
about couple of times by now is we get
huge amount of volume of data right so
we need to be able to roll up the data
we cannot keep on accumulating the data
forever so we use this feature to sort
of run the data another great feature of
no sequel is configurable durability and
consistency right one advantage of this
no sequel is you should be able to set
the durability levels on a per table
level right so some of them we want to
make sure that the data is synchronized
across all the shards not just the
primary but we want to make sure that
it's a replicated Ignace secondary right
things like which is important critical
data like alerts data that is not
critical like regular all data you want
that to be written faster but you don't
want to wait until it all the shots got
replicated too so you should be able to
set on a table level on what kind of
durability you want what kind of
consistency you want this feature really
helps us in differentiating the critical
data versus non critical data how did we
use use it well in IOT right
so for us we went with the replication
factor of three for our dr so we have
three shots one primary and one master
in two slaves and we say lot of data in
no sequel right not just that rot data
that we get from devices but more
importantly analyze data but the data
that we get based on the analytics we do
for predictions anomalies what annotated
messages incident everything for us goes
into no sequel all the basic information
we still we persist in
work on top of it we do historical and
role of data for predictor Alice's right
again all that information also goes
into the new support and as I briefly
mentioned earlier right we use the
elasticsearch backing of musical feature
so the data that we write into musical
some of the data not all the data is
also replicated into elasticsearch that
gives us the cave with no sequel as a
source of fruit so that gives us the
flexibility to carry either a national
sequel or elastic search for faster and
simple queries we go as national sequel
for advanced and complicated queries we
use elastic search so but we don't have
to worry about making sure that data is
in sync between this thing we we put the
onus on no sequel for the data sync
issue and the other feature that we use
is the ticket feature we talked about
that earlier right so we use both the
table level as well as a row level
features of key TL we have defaults at
the table level but critical data can
also the row level TT can also be
employed for the critical data one other
great feature that will improve the
performance of priorities the bulk puts
at literally upper right through putting
put by a 3x up to this feature is bulk
put instead of writing one row at a time
we batch and right to no sequel that
really has because of the data we get is
very humongous and we sort of bat it and
write it to no sequel that gives us that
gave us a huge performance improvement
and the last thing I talk about won't
talk about is the usage of secondary
indexes as we know secondary index is
costly no sequel so instead of using
secondary indexes across the board we
use a hybrid approach cases where we
want the data is small the tables are
small but we want fast queries we use
secondary indexes
but cases where the data is huge
the second instead of creating secondary
indexes we replicated the data so that's
all I have
hopefully this is a good case theory of
how we used Oracle no sequel and how we
leverage no support thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>