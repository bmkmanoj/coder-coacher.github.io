<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>In-Database Hadoop: When MapReduce Meets the RDBMS | Coder Coacher - Coaching Coders</title><meta content="In-Database Hadoop: When MapReduce Meets the RDBMS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>In-Database Hadoop: When MapReduce Meets the RDBMS</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v3N_sF4TZv0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is my me I'm going to talk about
in database hadoo i'll try to be i'll
try to stay away from product you know
i'm going to talk about technology and
also the implementation in order so this
is the agenda very briefly on overview
of a big day tomorrow deuce etc and then
in database MapReduce as a concept you
know what is this on ok on them I'm
going to deep dive into the Oracles
implementation of in database I do you
know I need to pick one implementation
to talk about the concrete case that's
the case but I can assure you it won't
be product peach anyway and then give it
the more and tell you you know what
we're thinking for the future this is
our legal disclaimer you know it says
you know if I talk about anything which
is like part of the future product you
cannot go to court and say oh I bought
this because I heard this guy talk about
it in a Java answer cannot do that
Garrett is not here yet Garrett's wat he
is the architect and he is the one who
had a a PhD in turn from Yale you know
his name is xuan and he worked under the
direction of garage to build a prototype
and they presented the prototype at
sigma 2012 and you can see the link
layer and so you can read the whole
paper online oh it's all the details of
the implementation and now we are in the
process of going from prototype to
product so we're going to productize
this and in the next 12 months
as this is the usual language we are
using now it probably will be available
as a product but I'm not going to talk
about for that this afternoon anyway so
big data the big picture so when people
talk about Big Data they usually refer
to the left side must be the left side
column you know non-relational star
where you have you know Muslim
structured data and the storage varies
from file system like like HDFS or NFS
to key value stores like no sequel
databases to document databases like
MongoDB to graph DB and Google's big
table etc so it's a variety of storage
and but it does not care it does not
include a DBMS so usually when you hurt
big data they are talking about anything
but our DPMS in terms of storage and
then they usually tell you you know big
day tied for it three or four V's you
know the volume another velocity which
is no fast acquisition of data and
variety variety means you know data
comes from you know sensors meters call
details web traffic in a weblog social
media feeds those are the most common
data that you find in you know in in the
Big Data literature you know they want
to stress those data but you can also
have special data graph data text images
audio video metadata so variety is key
key word here and then a volume you not
go from terabyte soup it up by two
exabyte you know I have not seen any
exabyte but if we talk about it then the
value or the density it means the
content of each individual data is very
low it has almost no value know what is
the value of a treat if you lose a treat
who can
so it's a low density and then the
analysis model it starts with you know
programming model you know the MapReduce
programming model and then it translates
into Hadoop implementation and then now
people are now adding a little bit of
declarative on top of Hadoop you know so
you have hive and different approaches
to simplify not to make it easy for non
Java gurus to be able to do my producer
as well so that's the story of big data
now we are claiming that not only Oracle
you know many companies mostly are DBMS
vendors but also research companies new
startups like about a DB and some others
they are claiming that are DBMS is also
part of the Big Data story okay so when
you look at the random image which is
the other color you can see that the
storage is a de minimis the velocity can
be low to medium you know you don't use
it database to do higher high
acquisition of sensor data you can do it
but you know live and then in terms of
variety you we can see the database is
stirring you know call details special
data graph audio images images metadata
sales record you know those are business
data cells record the financial data
medical data intelligence data and the
volume can go from terabytes to
petabytes okay and the density or the
value is high you know a record in our
DBMS can be worth you know millions of
dollars ago it's a transactional data
okay so it carries high value and then
the analysis model can be declarative
you know mostly databases have been
providing you sequel functions you know
analytics functions aggregate functions
those are the qualities it means you
don't have to write code but those are
not enough to do very complex
mapreduce operations so that's why all
those vendors and our DBMS vendors you
know you will hear like aster data some
other you know many players you know and
newcomers are claiming that we can add
programming programmatic MapReduce to
the RDP ms as well okay so that will be
the my focus for this talk okay so what
is the concept of in database MapReduce
the concept is to apply MapReduce
techniques to data stored in relational
database you know so it could be a
sequel MapReduce as I call with
declarative MapReduce but it could also
be programmatic MapReduce that's why i
put this yellow elephant there that's
hadoop in the database and that will be
the topic of my talk so why are we doing
that we have seen customers a lot of
customers you know moving business data
in a data start in relational database
that can be qualified as business data
they're moving those data on to Hadoop
clusters in order to apply MapReduce
techniques you know to those data and we
all here know that data shipping is a
bad idea so you don't want to do that
what you want to do is functional
shipping so which is you know bring
Hadoop to the a DBMS data you know it
has a lot of consequences when you're
shipping data to Hadoop cluster not only
in terms of space you need three times
the storage in any Hadoop cluster you
need three instances of your data for
high-availability reasons and there is
no straightforward way to export data
from relational databases to Hadoop
clusters there is a scoop on some other
tools but they have issues and for
example Oracle does not provide any tool
there is no official tool to export data
from Oracle to the outside we want your
data
Oracle or not the other way around okay
and so the the we also want to leverage
our DBMS declarative analytics and bi
tools you know there is a type of a and
then as I said before in the previous
slide extend our DBMS with programmatic
analytics and the other thing you might
consider also is put my producer
analytics closer to 0 RT p which is
where you know the database operations
happening is already p it means like you
will be doing you know check out at the
register and maybe look into a pre
processed data for fraud things like
that so it's not exactly return but it
could be near real time so you can
refresh those data very frequently and
then during the oltp you can just click
and find out what's going on with credit
this credit card this customer etc etc
and so in database MapReduce so there
are a lot of initiative as I said here
we are not the only one thinking about
that so you have hive pig latin which is
my language but anyway and then tin zinc
from google then sequel MapReduce from
teradata / aster data then you have had
two DB from company adapt and this comes
from Yale you know those are research
people from here now they created a
sternal on there you know product icing
and selling this engine you have MongoDB
who have added Hadoop or programmatic
interface to that database and they
harvest a booth in the exhibition hall
in moscone south I should stop by and
ask them what are they doing in the
hydrofoil and then okay so but but the
limitation we have seen you know look at
all those implementations I'm not saying
that all of them has those limitation
I'm saying that you will see those kind
of limitations so the first limitation
is
you probably we need to rewrite the how
to program in a different language I
mean they do not support Hadoop as is
your Hadoop mappers and reducers as is
I'm not saying that all of them are in
this situation but I'm saying this is
one limitation you will find the second
limitation you will find is they have a
dependency on a Hadoop infrastructure
which means in addition to the are dbms
maybe they sell you one box but in the
box you have a DBMS infrastructure and
you also have Hadoop infrastructure and
they load data dynamically at runtime
from the stories from the are dbms
stories to the Hadoop infrastructure so
it's not technically the same
infrastructure it's to infrastructure
combined unsold as one so those are the
limitations now the question is can we
do better so now I'm going to talk to
you about the Oracles implementation of
in database Hadoop so the product name
will not be in database I do you know
the product name is different from that
because the product has seeker MapReduce
and in database headed ok so but I'm not
going to talk anymore about the product
so what our goals and what are the
features so the first goal and as
everybody is to process data in place
you know no need to ship data to our dbm
in our dbms to a separate infrastructure
we have implemented the notion of Hadoop
container in the database so I know you
guys are Java guy so you're familiar
with day to day three container etc so
think about this in the three world you
have a container you drop your beans
your jelly beans into the container and
it just okay you need to do some
configuration and then it works and you
can move your bins around to any gate we
container so here we're doing the same
thing you know we're trying to have
inside the database
really inside the a dbms a Hadoop
container so you can move your Hadoop
mappers and reducers from anywhere ok or
you can ask yet because yes
now okay in the initial slide I
explained that Hadoop we use different
storages and it's not necessarily file
system the storage can be anything now
we are reading from tables I'll talk
about it in
now we're using traditional database
table oh ok ok so the question is at the
end of the day database storage is done
on top of how to a disk drive or some
sort of stories sure but it's not a file
system yeah what I'm trying to say is
that we are using natively the rdbms
table so this is garrett swati just
entered the room i already introduced
you as the father of the project so you
want to raise him to your question I'm
just saying that and the way we have a
we use a DBMS storage you know and if
you have data inside the database where
you can restore it you probably get a
story about exactly so the idea is that
we take the data that's available table
and we make it available as Java a dupe
writable as java objects you can that
your mappers
you know how do you connect you know
sequel with Java intervention and take a
sequel row there's no HDFS so i think
when we cut to the implementation
details we can talk more about the this
thing so the next thing i want to talk
about is the source compatibility we
accept Hadoop mappers and reducers as is
and that's what I just explained we have
a kind of Hadoop container in the RT BMS
so you can bring your Hadoop mappers and
reducers from any Hadoop cluster and
with some configuration and I'll show
you the configuration in the demo you
can run those in in the database so we
have a Java interface which means a
Hadoop developers who knows how to
invoke Hadoop jobs the traditional way
the traditional hardeep way will
continue to do that with a little little
Delta of how to invoke tava in the
database there is a little bit urgent
okay but that that's it and then we also
have a secretive face which means we can
have map and reduce steps in sequel
statement I'll show you and I'll demo
that as well okay so how does it work
I'll just show the whole picture so this
is kind of our Hadoop container and what
you see here is each box is a parallel
query slave so we use the parallel query
engine inside the database the parallel
query is a mechanism by which you can
fire up a query and it will fire up
multiple queries slaves to process the
query okay so in this case what we're
using is we are using parallel query so
each box is like a query as query slave
and then inside each box we are running
a pipeline parallel table function and
on the top the table functions are the
your Java mappers at the mappers code
and at the bottom it's the reducers
it's your Hadoop mappers and reducers
that we are running as parallel pipeline
table function and then the shuffling is
done by the partition by cluster by
close you know which uses some hash
function to route and data with the same
keys to the a one particular query slave
you know exactly as a Hadoop framework
does when you get data from different
mappers on you shuffle to the register
okay good point the question is can we
have do we have a restart mechanism no
we don't but traditionally a DBMS does
not run on TC box it run on a robust and
you know reliable box so this might be a
minus in our case but we do not have a
restartable mechanism as we speak you
want to add something to that no data no
started is a failure to node but in fact
the Oracle implementation of parallel
query if you have a node failure what
happens is that query the database will
stay up needs to be restarted right and
we started so the from the beginning you
restart from the beginning of that
particular statement
that's that's a theoretical possibility
that you can have a sequel engine that
can do be restartable if you have a
failure in a process the Oracle engine
basically just you know doesn't doesn't
fail so you don't need to why would you
want to fail as I'm saying you know
people i've been using it to do business
phone it is in my face you know we
should be honest here in my fail but you
need to restart so we don't have this
restartable mechanism our container does
not have that ok so those are the pieces
of the architecture as you can see there
you start with a sequel query you know
it's like select something but even if
you don't start with the query on you
invoke the job activists at the end of
the day it is a sequel operation you
know because how do you start by a query
well by sending him some sort of sequel
ok so we have the query engine you can
see the mappers here and the reducers on
the top you know you have the table
reading on table writer so those are
implementation of record reader and Rico
rider those interfaces so which makes
transparent to the high dude mappers and
reducers the notion of table and
although they are reading from table
they just reading from some some data
source then we have a Java VM in the
database people know about the Java VM
inside the database no it's been around
since we're released eight I okay I have
one slide to explain what how it works
and then you have the data story so we
can get data from a table from XML table
and also update you that's a that's
about online so we do the type marking
task execution in the JVM and the
partitioning and a scheduling is done by
the pkng ok so now now we are going to
zoom into into a specific part of the
the implementation so for this
implementation on this finger with demo
here use the new you know the upcoming
or 12c database we're using the the vm
in the database the vm in the database
you have the choice when you create the
database to pick which dedicate you one
if you want jdk six compatibility or if
you want DD k 7 compatibility right now
we are using Hadoop 0 22 so this is the
master mode do you want to comment on
why we're using a diff little 22 I think
it's the most stable by the time we
started the project yeah and and
basically it's the AP remember it's only
the ado pap I so it's you know it's
basically you know what you need to
write your application it's not the the
underlined and we basically replace the
entire engine with our own engine but it
does have the you know all you know when
we created this this job environment we
basically cloned 0 22 that's not to say
that we can't make a new one with other
versions of Java and be able to reuse
the code but that's that's the one we
release right now what was the question
so so the guys so so the Oracle database
is basically is a shared everything
database so the distributed cache can
just be stories just stored in you know
Oracle table it's accessible read-only
to all the slaves I'd like to repeat the
question for the others well what's the
question
Oracle also has a file system called
dbfs which is if you want to store your
things in a file system this dbfs file
system is available to throughout the
oracle database and actually even
externally so if you want to keep it
things in a file on you and you can
configure dbfs then we can access that
uniformly throughout the system you can
also feel like NFS Oracle can use NFS
and you can store your things there and
you can get access to that the nice
reason to put it in table or dbfs is
because Oracle has a distributed cache
for that if you have your file and used
in an NFS server well then everybody's
going to beat up on that poor NFS server
if you want to distribute it cash you
have to store it in an Oracle storage
format which is either dbfs which is our
file system or inside of a table so if
you have a CSV file I mean if you are I
mean so this is oriented towards towards
customers who likes equal and you like
tables in fact what you're trying to do
is you're trying to get portability of
your of your Hadoop application so they
can run inside of your native Hadoop
infrastructure but then also be able to
run them inside your Oracle Hadoop
infrastructure and so you're so usually
that the viewer if you're in an Oracle
Hadoop infrastructure you're probably a
sequel kind of guy and so serving things
inside of tables is actually makes you
feel all nice and warm and fuzzy rather
than making you feel black so the
assumption is that tables are what you
really like and so storing things in
tables isn't isn't the problem for you
but we also inside of Oracle have if you
like file systems we have a file system
that I'll be enough do you know how we
implement the Oracle file system I'm
clean the table yes I think the cuboid
so in terms of configuration you we will
show you how to do the job
configurational it's exactly as they a
good job config plus some data type you
need to specify what are the types of
the key and the value input
kion values types I mean database type
what are the sequel types to wish you
map those things so for each job we
create a config object that we saw in
table call when the date of it so we
have ways to retrieve this row for each
job you know so we can pass this
information from sequel to Hadoop back
and chop so that's how we ended
consideration in terms of marketing as I
said before we reimplemented the table
reader recovery toriko writer and we
have our table reading on table writer
which implement recall reading people
writer okay so now we use those to do
the mapping between Hadoop types and
simple types I'm not gonna get into the
details of the mapping but we the
framework that we have does takes care
of the mapping you know between a d-type
and sickle we might need to do some
optimization later in future releases
but somehow it works well this is the
architecture of Java database what you
seeing in the top are the database
processes you know and Oracle has
different architecture but the most
commonly used is what we call dedicated
server which means each user each client
two of the database as a unix linux or
unix process which is attached to it or
a windows right if you are using windows
so in each of those process and thread
we have what we call a session database
session database session bronze in a
process or windows thread so it has a
memory it has its own memory we call the
kitchen program global area okay that's
where we instantiate the Java VM so each
user has sort of his own private Java VM
which run in his session okay you might
say oh is it better to overhead
out of overhead in terms of memory then
look at the bottom the bottom picture
the bottom part of the slide is what is
called the Oracle databases shared
global area which is a huge shared
memory that all the sessions have access
to all the processes can access this
shared global area this is where we put
the system classes you know which make
the Java VM and also application classes
so all those classes are shared across
our session so if on your machine you
have eight thousand users and we do have
customers who are using Java in the
database and they scale up to 8,000
concurrent users are doing all Java
which means they have 8000 session and
they are all sharing the same system
classes the DVM on the same application
classes that's how we amortize memory
across all the session more session you
have less memory per session we consume
so this Java VM skills very well
provided at the machine framework and I
mean the infrastructure has the
resources so that the limit the system
is ready ok that the DVR scales very
well that's why we we taste this Hadoop
implementation on it and this is the
pipeline table function so pipeline
table function this is not unique to
Oracle I think I feel lot of our DBMS
has this notion of pipeline table
function so what it does is it can take
a stream of data as input and produce a
stream of data as output so think about
mapper for example it's going to receive
it's going to receive the its input from
the actual table but the parallel queer
with dynamically partition so the
partitioning in our case is dynamic we
don't have files no secret
five for each no we dynamically speak
depending on the degree of pollen is
when you specify so each PQ slab will
receive its stream of data from the
input table will produce an output that
will be consumed by the reducer so those
are pipeline table function they can
receive a stream of data produce an
output of data and we know inside Oracle
I mean inside the Oracle sequel how to
treat the stream as a virtual table and
I'll show you an example where you can
run a sequel query on this virtual table
which is the output of the stream of the
parallel pipeline table function and so
I think now pieces are getting together
in your mind you can see the peak in
slaves running the mappers and reducers
and they can receive input either from
the table or either from a string you
know another 591 function and they can
produce an output that can be put into a
table so we can recoup the output of the
reducers and store them in the table so
we read from the table to feed the
mappers the mappers process in the
reducer the reducers process and fit
enough food that we can just start in
table so we can keep the output of the
reducer this way okay this is so this
facility has been around they're
available in pl/sql for all you pl/sql
programmers this is this is pl/sql one
right so it's been around there for a
long time except for we decided hey why
not make it available to Java programs
and if you're going to make a parallel
infrastructure available to Java
programs why not make it match the
Hadoop API rather than some other crazy
API we might make up that way you have
an immediate base of people actually
understand how to write to this thing
and a certain amount of portability of
programs between different
infrastructures all right so so now we
talked about it the implementation now
how do you use it and
the configuration of things like that
but what are the different interfaces so
just summer i'll then i will show you
freedom which is more meaningful not
talking we have two interfaces as i said
in the left as you see here select star
from table and what this is doing is a i
don't know if this is working but the
other wants it so let me just say the
select star from table is selecting the
output of the reducer so the reducer
pipeline table function the name is h
reduced on the sky JP on the skull word
count this is the reducer of the word
count job okay so what you see here that
we are making explicit to you the name
of the pipeline table function so you
can use it within sequel queries so you
can use the Hadoop jobs the steps you
know the reducer step or the damaja step
explicitly in sequel query so that's
what we're doing here so the secret
after that square a is selecting from
the output of the reducer and you can
see the reducer has a configuration key
which is where we start the
configurations and the reducer is
getting its input from select star from
table H map on the sky JP underscored
count which is the mapper so the reducer
is getting its input from the motto okay
the map / now is getting its input from
the input table select star from
interval at the bottom of the box on the
left so in this query you are invoking
the Hadoop markers and reduces and
you're selecting from the output of the
reducer you can do whatever you want
with that so what I'm trying to say here
is that in any language provided
somebody has written the Hadoop mappers
and reducers from there from any other
languages you can use those and do some
query on invoke those operations so it
is universally available because it's a
simple interface please click on info
a thumper and you could also really
you're designing your sequel query you
oftentimes will have your own custom
mappers and reducers but there's a bunch
of boring junk you want to do to like
you know renaming some columns adding 3
to something whatever well that you can
just do in sequel and so it basically
give mixes the sort of usefulness of say
pig and hive where what you want is you
want to scripting language to invoke
your mappers and reducers but you know
what's the best scripting language for
you know invoking data transformations
sequel and but now you have sequel now
augmented with MapReduce to do the stuff
that sequel doesn't know how to do
because sequel is not a sequel is not
touring complete and that's actually
considered to be a plus because it
actually is draka bowl by an optimizer
but but now you can extend your sequel
with maps and reduces to make it
understand the new data types new kinds
of algorithms that sequel doesn't
understand and now you get to use sequel
to script together your complicated
stuff so speaking of optimization that
means everything everything that comes
out of the produce is not indexed that's
correct it's basically distance it's a
stream following the usual sort of
MapReduce paradigm now now of course you
can take the output of any of listening
or and store it into a table and index
out there but yeah and that's where i
talk about near real-time GPS that the
output of the reducer can be used from
some oltp application where you can just
you know use an index on query this
credit card information and see what's
so that's the secret interface the Java
interface is on the other
and you can see this is the classical w1
I do say now let me show you those
things will be talking about 4 1 i'm not
stop by showing you the configuration so
this is the virtual box and if people
were waiting in my pencil lab session
yesterday are you will play with exactly
the same thing so this is a let me show
you the five this is the job the
configuration of session is a shin job
when I do specialization tell session is
really the very common use case in happy
why you want to find out you know I mean
I'm in session and guy on how much time
he's spending on my website blah blah
blah so it's a very common history so
this is a let me go right oh this is the
front the static method this is the
wrong method okay very important to note
is that ok you can see here this is the
complication we're leaving this duck
session addition as you were doing
having the proper and this is the Hadoop
map classes it is the standard hazard
map as you know you can get it from
Apache demo or you can get it from
anybody you can get it from any happy
process so it's only challenge and then
this is the class of the reducer as well
here okay um although this is the input
key club
one class the I could keep a cup of tea
by the class all those stand there happy
ok here we are adding database things
you know we re database thing we are
telling what this is the sequel type
which correspond to the into the output
key and this is it's a bar chart to in
this case varchar2 tank and this is the
number you know sequel type number which
is the sequel time corresponding to the
output value of key and also we are
specifying the degree of parallelism how
many mapper and how many reduces this is
how you control how many queries slaves
will be fired by the paper engine okay
and this is the job in it and job wrong
so this is a configuration now we will
load the configuration in the Oracle
database this is how you access that by
in the date when you load the Java class
inside of it okay i have already loaded
the class and and unjustly indian and
not all these things have to be
specified likes for example degree of
parallelism oracle as a feature called
Auto V you know so if you don't specify
it and you've enabled auto DOP it'll try
and basically take over your entire
machine or use it were different
percentage of the machine you decide to
give it to it so in this demo what I'm
doing is I'm i crack the table i create
the key value table okay and then I'm
populating the table
it's a cheeky rose okay and then we are
calling we are calling the configuration
and method in the configuration to
generate the configuration so remember
we need to generate the configuration
key on study table so that they know
between the Icicle we can retrieve the
configuration and use it to process the
matter of the reducer functions so what
you see here is that in order to invoke
Java in the database you need to create
a rapper this is the rapper and this
rapper will invoke this method which is
the wrong method in the function i show
you here so this is the method is in
public static void wrong this is the
method we are invoking and in that
method we will be invoking job any job
run so when you call this interface okay
from any language once again you are
invoking the you know you will a trigger
p qu with you will execute your other
job okay so that's what we're doing here
that's what the demo do so when i say
called sexualisation here i am invoking
this java method from the session
ization class that have been produced
lee loaded into the data okay so let me
run this so I'll just execute session
means
so it will process every sequel
statement i will show you we drop the
table insert rows in the table process
and you can see here the output of the
reducer that we start anything that's it
okay next I want to show you also the
secretive so that was the direct you
know how to direct interface now let me
show you the signal interface and we
distinguish this by calling it cone so
cough means we create the invoke a
function to insert the conflict object
in the table and then notice that here
we only have job in it and Delta there
is no job wrong so we are just inserting
the config object in the table now when
you explicitly a book the mappers and
the table functions which implement them
up and reduces that's why you're gonna
in triggered execution so we are not
executing here we are just creating the
configuration object and story so this
is what this this configuration file dot
so same thing where you can unload this
configuration file into the database and
we
invoke it we will create the wrapper so
if I show you ravagin session musician
Oh God sequel so same thing here we're
dropping the table create a table
inserting rows in the table exactly the
same rows and then we are invoking this
sexualization conf that job three
electric job kicks and then we call the
session ization into country so when we
do that the configuration key we've
carried the entry of the row and then we
can drill we can read the config object
from there okay so this is the sequel
query sequel statement which is invoking
the mapper and reducer so what this
statement is doing is is inserting into
output table the output of the reducer
select star from table edge reduce
sexualisation with the configuration key
is inserting into the output table the
output of the richest we are treating
this as a wittle table and we can start
culture and then the reducer is getting
its inputs an extra from table from the
Marvel okay with the configuration I can
execute it about in Ingrid diction keep
the same rhythm so we should see exactly
the same thing happen okay so that's
good now what is powerful about the
sequel interface is is not only the
ability for any developer in your
company you tell me hey this is the name
of the Hadoop steps you know the mapper
this is the name of the table function
the matter this is the name of the
reducer table function go and use it
whichever way you want okay well it is
even powerful because look here I am
combining multiple steps of two jobs two
hundred jobs so if you read from the top
to the bottom I'm selecting from the
output of the reducer of job number two
which is getting its input from the
mapper of job number two which is
getting its input from the reduce of job
number one which is getting its input
from the mapper of job number one which
is getting its input from the an actual
table so you can combine whichever you
want and the good thing about this
approach is there is no intermediate
materialization of data you know us in
the classical Hadoop what you want to do
two jobs you need to do the first one it
will write to the file system and then
you will read from the first one hey
guys so we'll be right in assuming that
those are provided to the outside part
of the query until the reduces complete
there's no micro none feature
you know that Billy
sir no no you don't have to do it
parallel by plaintiff unfortunately
don't have to wait for the completion
well do I mean except where there is a
blocking step in the computation so like
let's say it was map map map map well
then everything can just flow through
the trouble with reduces as everybody
knows is that you've got a is that
they're so there's a there's some
dependencies there and so in fact you
know things will will you know build up
and if the amount of data is big you
know then we use we spill Tori called 10
to temp areas and stuff like that but
they aren't part of the third part of
the users model it just happens because
you happen to run out of memory so you
know so there's there's no magic
unfortunately i just did a demo and this
is a summary of what we have done so far
i want to rapidly database book Adam
closer to ATP integration of sequel and
we are looking at we have input family
will be available when the product is
available I'm surprising pretty and this
is easy for DBS not deviate I don't need
to know anything about how to cluster
you don't tell him deployed this job as
you usually deploy a sequel charge and
enterprise the floppers they don't all
need to learn java I do you can have a
kike by the prophets you know you guys
will write them hoppers and reduces and
everybody else can consume what you have
written you know for the sequel
interface and then looking forward a
guard I let you talk so what are we
looking forward now we see well we can
accept my burden with users from our
engine you know our of the price are
where you can do statistical advanced
statistical analysis they can produce
Hadoop code and we can consume those go
ahead well I'd also just like the I mean
we don't talk more about what we do
today which is the you know input
formats and and hive cattle the hive
catalog you know so we can basically go
in and access that kind of stuff so
basically wherever your data is
the model you know that we're aiming for
is real support you in whatever language
you want to use if you want to use
sequel if you want to use Java Hadoop if
you want to use are you want to you know
use xquery you know basically whatever
language you like we want to support you
accessing whatever data sources you want
to access whether it's HBase or no
sequel DBA or any of the other data
sources that might be allow or even
Oracle tables we want to be able to
access those and know whatever
infrastructure you want so you want to
run an exadata you want to run on the
Big Data appliance you want to run on
your own you know home built ikea do it
yourself for your system you can do it
that way list yeah so that's what we're
looking at you know supporting more
classes mahadev interfaces on output
forward and maybe do some optimization
of them might be best that's what I have
we have I think we have five more
minutes to take traditional question if
you have yeah we have eight minutes
we're doing that internally as we speak
we're not public yet but that was the
questions of a kid of the question you
mentioned curiosity vallelunga does
Mongo actually in the new space does the
same thing
they don't do it inside long ago that we
would have two formats that go straight
to the basics that any intention of
offering up that capability so that if
you had a Hadoop cluster you could
rewrite straight from Oracle using what
you've already built essentially so so
we can we already have these things
called the big data connectors one
direction ah well be reading is
important but why would they do want to
go back remember remate sonic told you
we were nice and army shiner or inside
of oracle why would your data going to
go ahead Nicole you know Ruby we are
looking at that and especially in the
Big Data appliance market where
basically it sort of smells enough like
Oracle uh-huh there's likely to be more
without with Pharma we should be about
to do that yeah when we have out with
fellow yep any other question how does
this impact is this what does run on the
database this runs in the database
knowed it it in fact runs in the same
processes and that's one of the things
it's not a to process solution where
you've got your jvm and you've got your
Oracle process they do it's the JVM is
in process so you don't have the sort of
the context switching overhead but it
complexity of what yeah so so if you
write some sort of image processing
library that goes over pixel by pixel
and you want to run that on your Exadata
system you know goes what are we happy
yes yeah but as you know so the idea is
that we aren't sort of saying that oh I
do pet you know you know all your Hadoop
systems throw them all away because
because this is it's basically this is
more of hey I want to run it on the
infrastructure I want to run down my
data is already here if you've got let's
say five percent of your load is to do
you know why should I be forced to buy a
hoop system for five percent but if
ninety-five percent of my loaders to do
I'd be sort of silly to run it all
inside of Exodus
you said yeah sorry I think I run yeah
we've got bigger iron so it's you know
then you want to use the infrastructure
that's optimized for the you know that's
the cheapest one for the particular
environment you happen to be running in
so your examples had this rock star from
the input table yeah is that how you
know he's doing oh no you can secular
gonna do whatever you want yeah but
those are virtual table so you really
want to store everything and then do
whatever you want before afterwards I
think it tables the rosin oh well
millions of rows is fine as millions of
columns you know you never have millions
of calls you can have thousands of
columns but let's say there's you know
people do have to have tables like
especially SI p tables and stuff like
that that really have a zillion columns
and you want to pick out the ones you
want and and just because basically if
you if you give too much data to the
java app that the job app doesn't look
at you know wasted wasting a lot of
cycles moving data run so you want to
just select just the pieces that you
want to did your app once
my contact alert system they got a lead
on my search queries and always real
fast
and some graphs are not move it which we
can say that we can share your jobs or
you can ask for address and we'll meet
you back something or something like
that so if i run those as MapReduce I in
your database so they would kind of be
affecting the regular conventional put
up that but but this is this is one of
the wonderful things about rack and
Exadata is that we can use the same
infrastructure for both oltp and beyond
and this is irrespective of whether you
use Hadoop or just use sequel for be odd
because this is a big market and you
know we don't want to force you to buy
two different systems you know the will
sell you two different systems if you
want to but if you want to use the same
system for both we have these things
called rack services and what you do is
you submit your bi queries and they
basically can make sure that they don't
interfere with your oltp sls well
different instances within the same rack
yeah yeah different instances oh yeah
that'sthat's can be done and that can be
done for any bi versus oltp because bi
in oltp they sort you know if you want
to do bi against your operational tables
that's quite common because and then
another common thing is you actually put
them on two different systems and use
etl to move operational data to your bi
so you'll be so sometimes people have a
separate operational database and a
separate data warehouse and use etl to
move them that's great then the nupe
would be running probably just in the
data warehouse but if you have dual use
databases then you're probably aware of
how to do that properly which is to set
up your SaaS and bring your oltp jobs in
a separate you know as a separate rack
service with a different priority level
so that you don't get exactly that
problem where we're some fool oh sorry
some data analyst comes in
and Bailey takes down the entire set of
resources running their crazy Romanians
we can take when you one last question
mark it on it pricing so kind of discuss
packaging we can I discuss anything you
don't remember this is it your bum did
it for this conference we are pre
announcing so we can obtain so I go back
to my director tomorrow or friday hey
that's the what we're doing right now
because you just described but most of
these earlier people have school until
she pees it but idea for sure well we
have big data connectors moving data
back and forth between Exadata and big
data appliance or your own Hadoop system
there's some places where it makes sense
where you're going to do enough work to
make the transfer basa can't transfer
didn't matter there's other places where
it's a lot of data you're not doing that
much computes on it moving it to
someplace else doesn't make sense about
not have the other infrastructure
basically we want to give you that
option do things in place do things
remotely you know just like we talked
about with operational you know separate
systems and using etl sometimes that's a
good idea sometimes the bad idea it's up
to you because you know your workload
and you know how you can you know you
know whether it's a lot of computes and
it's worth it to move it sometimes
thought yeah now as far as 12c goes you
know this you know income is not being
released right now we do have a beta
program for 12 see if you signed up for
the beta program and what you use this
stuff will be all over you like Oh too
late we not good no longer taking
application oh I I cannot tell you to
wait but i can tell you well if you
willing to wait maybe well no we can't
tell anybody oh don't you know don't we
can tell
do anything wait you know it's just
we're just telling you that we're doing
we doing something we're doing something
and it's just you know FYI um at this
point okay thank you are and they take
the restaurant</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>