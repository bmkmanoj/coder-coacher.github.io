<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Real-World Performance - 9 - Deduplication | Coder Coacher - Coaching Coders</title><meta content="Real-World Performance - 9 - Deduplication - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Real-World Performance - 9 - Deduplication</b></h2><h5 class="post__date">2014-04-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/amKwh7JzLYQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi okay for our second example of set
based techniques to show how we can
speed up operation on large data sets
I'm going to show to you the process
deduplicating data in many cases we want
to make sure that data is unique and
these techniques can also be applied to
other parts of things like checking out
relational relations and making sure
we've got foreign keys as foreign keys
and all those sort of things but what
we're going to do through is is go
through again and run races and bake-off
deduplicating a lot of that data the 100
and 1 million rows we just loaded we're
going to try and deduplicate it as
quickly and validate that there are no
duplicates in the data and pull all the
duplicates out programmatically ok so
the classic example of this is people
are loading batch amounts of data into
the database what happens if there's an
operator error or a programmatic area
and they load the same data twice well
it's pretty obvious it's going to
generate a duplicate so with that in
mind we need to start thinking about how
would i duplicate it from those sort of
errors so again I'm going to run the
race and then we're gonna look at the
code and we'll talk about it in relation
to that so we're gonna run the same
techniques row by row programming array
programming homegrown parallelism and
set based techniques so let's just start
that race off again and again we're
starting to see as as this topic is
about set-based processing we would
expect the set-based processing it's a
thing to start winning by quite a margin
and as you can see it's moving across
the screen at a good speed and you can
see the row by row and the array
processing is struggling remembering
they're only using a single core and you
can see this on the database
screens here you can only see a
single-core in action on both of those
two screens yet the homegrown
parallelism and the set-based processing
you can see they're using multiple of
their 32 processors you can see the
homegrown processing is more CPU bound
and then is seeing a little bit of
contention inside the database whereas
the set-based techniques is more CPU and
just straight i/o bound you can notice
that it is also finished in that point
of time if we actually look at the CPU
utilization for the times that it was
running you you can see that basically
the set-based processing actually
actually didn't give as much utilization
on the hardware but it actually did
finish first so in expressing that it
was more efficient and the manual
techniques actually got very good
hardware utilization but just look at
where it was in terms of the performance
so you can see that the homegrown
parallelism was not that efficient let's
just go through the results 41 seconds
did you deduplicate a hundred and one
million rows that was done with
set-based programming techniques going
backwards nearly five minutes using
manual parallelism techniques racer two
was taking one hour and 12 minutes by
using array tape knitting techniques and
racer one took nearly four hours and
nearly 5 hours 4 hours and 40 minutes to
deduplicate the data again you've got
this huge comparison of hours to seconds
just by the way we programmed the
problem so again let's go back and just
study the code very briefly and look at
the pointers of what was done to make
things go so the row by row program you
can see basically we're doing a cursor
loop and we're selecting all the rows
and then what we're doing is we're
looking for duplicates by joining back
to ourselves and in fact for this case
we speeded things up by building an
index on the table on the primary keys
so we can actually find out and look up
very quickly
it's effectively a nested loops done
inside the PL sequel logic effectively
going one row at a time
validating one row against all the
others by doing a self join and then
basically we got some conditional logic
in the code such that basically we find
the duplicate we put it in one table and
if we get it and if we do get a
duplicate it goes in one table and then
and no duplicate it goes straight into
the table to be to go on to the next
level of transformation this is pretty
common you know so we're throwing it
into two buckets a good bucket and a bad
bucket and this is very functional code
very easy to write it's quite long but
it's very simple code and it's the sort
of thing we see daily you know a self
join to check the duplicates well let's
take that a little bit further
programmer had read a little bit more
about PL sequel programming or other
techniques or array programming and
basically was doing the same program but
realize that they could speed this the
cost of access of getting the data out
of the database by using the bulk fetch
operator and so they're now working
through an array doing that nested loop
join and then likewise we're building up
an array to do the insert in either to
the duplicates table or the transform
tables and so what we are effectively
doing is array processing on fetch to
get the data we're doing programming
within PL sequel and lookups basically
use it using some looping techniques and
then finally we're using bolt techniques
to put the data away and that is the
yielding a performance gain or from
nearly five hours four hours 40 down to
one out of just by improving the
efficiency of access of put and get of
data didn't actually really increase the
performance very much of the actual
lookup to see if there was a duplicate
because we're still doing that again one
row at a time but boy did we increase
the efficiency of just buffering the
data out of the data into the program
and putting it back away we really
prove that efficiency and then taking
the similar technique to the next level
to use manual parallels and technique
remember we used that are a hash
technique combined with better buffering
we could basically have 32 threads all
looking at different data sets
checking for uniqueness in parallel so
we've got the advantage of better
buffering we've got parallelism and
we've managed to divide the data again
using the hot or occur hash function and
that's basically how we've achieved
effectively manual parallelism broken
things up by data range and very often
we see as I say people don't use a hash
function they may use a logical function
such as state code geography day of the
week country all sorts of things but the
great advantage of a hash function is
it's usually you can get the same amount
of data into each program and if each
program is looking after the same amount
of data it's a fair chance that each
parallel process will finish at the same
time and so you're utilizing the
hardware for as long as the program
takes to run again you'll notice this
code is getting quite convoluted and
there's quite a lot of it so let's go
have a look at the set-based techniques
again the set-based programming has got
a lot less code okay but what we're here
doing with the set-based programming
code things are getting a little bit
more interesting in this but we're
starting to combine the techniques that
we use for data loading which is where
we have the insert append okay so we're
loading things in but what we're
actually starting to use is a feature of
the inserts table which is the multi
table insert so we're allowed to take a
result set and apply some conditional
logic on the insert and apply it to
either one of the two tables either the
duplicate table or the data table we
want to keep the scan events so you can
see we're applying conditional logic two
sets of data and a sequel statement
through the insert statement
and then the actual deduplication
process you can see that we are in fact
using a windowing function to basically
partition by all the primary keys and to
give a sense of order of it so we can
actually programmatically eliminate a
duplicate should we found it and then
what has done we have done and in the
windowing function is we've created a
label effectively a virtual column which
is used this is called error end okay
and this virtual column to provide the
conditional logic in the insert
statement so we're using some new ideas
here we're creating a window function to
start labeling rows and once we're
starting to label the rows whether
they're in arid rows or whether there
are good rows we're pipelining that in
parallel through sequel to an insert
statement which is again working in
parallel and is bypassing a lot of the
buffer cache activity undo information
and redo information so you're getting a
full set based technique of basically
duplicating data its exploiting core
database parallelism and set based
techniques and you can also notice again
it's a lot less code than the other
cases so often we find everyone thinks
all set based programming is quite
difficult which it is because it's
different but in fact when you talk to
get on set based solutions you tend to
find yourself writing less code than
when you were writing paragraphs of
conditional one row at a time type
program with a lot of conditional code
the problem is we need to start thinking
about doing things in sets and it's not
the easiest but the problem is that most
resistance we get to it is that it's
different
and when people from find things are
different they resist it but those that
exploit it and learn first they have a
huge career advantage for both
themselves and for the companies that
they're working for because let's look
at the performance results again we're
taking a deduplication process that was
functionally correct they're run in four
hours 40 minutes down to 41 seconds
which one would you like to have
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>