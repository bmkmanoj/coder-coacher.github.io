<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Part 2 Predicate Pushdown | Coder Coacher - Coaching Coders</title><meta content="Part 2 Predicate Pushdown - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Part 2 Predicate Pushdown</b></h2><h5 class="post__date">2017-01-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OYyOmB8A9oo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thanks all wrong
hello Iran um today we going to continue
our Big Data sequel tip time series and
today we are going to talk about some
performance features which we have a big
data sequel and let's have a look how
could we run your big your queries
against Big Data more efficiently and
faster so before all I would like to to
do a quick reminder that actually picked
a testicle when we are talking about
picked a testicle we usually are talking
about some special engine which we have
on the Hadoop tire which works over
either HDFS or no sequel database data
and this engine has few optimizations
I'm not going to talk today about
database side because database side has
plenty of tasty things and it's it's
different topic today I'm going to talk
about processing file which you install
on every Hadoop not and which precede
HDFS or non sequel databases so on the
database site you have to create
external table which will be linked on
the data which you start on HDFS and
then you could start scanning this data
like a quick reminder I would like to to
go through the this flow so what happens
when you open the cool plus or sequel
developer or any other two for writing
sequel and run some query on the first
step you jump to the name not and or
hive master
and get some metadata which tells you
where exactly you store these HDFS data
you get file formats you get file
locations you get structure in case of
its oracle hive and then database plan
scans and schedule stands on the sell
side it's going to be second step you
run smart scan over the Hadoop data and
then on the third step you should pack
only hopefully small piece of the data
and join it with Oracle database data
and do final transformation not
everything could be offloaded on the
sell side on the head website for
example pill sequel could not and if
your choir has some particular we will
ship data towards a database ID
hopefully we'll do all filtering or all
other creations which could be done on
the sell side and then move data on
towards the database side and do final
transformation there and let me zoom
Hadoop part so let me zoom a Hadoop
piece of data sickle what's going on on
the on the other side when you start
scanning it is step number two in my
previous diagram so you already start
scanning the data and what's happens on
the first step you read data from the
data not and you convert this data in
some special optimized format which is
understandable and which is convenient
for Oracle database smart scan then pass
it to the Oracle smart scan smart scan
as input smart you have to feed Oracle
block as input to the smart scan
so external table-service read any kind
of data no sequel HDFS Park in HDFS text
in HDFS our files on HDFS whatever you
want whatever you able to describe in
surgeon in hives realization
digitalization and then it converts it
in oracle format and feed smart scan
smart scan it's very similar with this
component component it's very similar
with exit data smart scan then you do
parsing and sorry you do processing and
rest of the data if you move towards
database set so let's have a look
what could you optimize here what could
you optimize within this picture within
this part of the Big D testicle within
cell side I was a big data sticker
so first optimizations which you could
have it's partition tronic so how how
you store your data in HDFS I used to
say that HDFS it's a dupe distributed
file system and key words is here
filesystem which means that it's
scheming read and so but also it means
that it's kind of file system and as
many of you has laptops and you you
could imagine how you store data in your
on your laptops there is some
directories the recent sub directories
within these sub directories you store
some files so same with HDFS HFS stores
data in directories in some directories
within the file format and we could
store for sure you could stop different
type of files so ok but it's quite its
its smallest manageable within your
laptop
if you have director for photography
director for the documents directory for
something else but even in laptop
sometimes it became a challenge and
he'll talk about big production
enterprise system you have to categorize
it you have to define metadata somewhere
you have to explain what do you store
how do you store in which form of distr
for these purposes many I would say
majority of do cluster use hive visit
hive you have some meta store where you
define that hey this directory is I call
database database it's like set of the
sub directories which I call table
within table I stole some files and I
define file formats i define record
twitter i define serialization sister
page and I use hive meta store like
catalogue of my of my HDFS and very
common technique to have sub directories
which represent different partitions for
example on month or monthly basis you
store some data like transaction like
web web clicks like whatever we want and
and you partition it by date as I'm
sorry whoever I'm you to commute to
yourself please thank you so and then or
you have this metadata what happens when
you open when you run the query which
are needs to read some data you get any
file then create the splits splits it's
like piece of the processing by default
it's block you could think in simple
cases in simplest cases you could think
about split like def
block that then you create a record
trade electric razor records records
then creates a columns and you feed it
to the cycle engine it's most common
path must come in the way how to proceed
data within sequel engine but what if I
do have three subdirectories and I only
need to read data for one certain year
per month we can do partition pruning
partition pruning works with hive it
works as well with Big Data sequel and
when you specify a some sort in year or
month or some other partition key you
automatically prune out unnecessary data
and read-only only small piece of data
which you requested within your very
predicate it's very simple but very
efficient feature so you've talked about
if you if we will talk about best
practices and ok good practices within
Big Data signal it could be a really
good practice to partition your data
like within all the tree houses so ok
you partition the data and now you
partition the data and now you want all
you wanted to anyway you wanted to scan
even faster what can you do we have
optimizations which we call storage
index it if you are familiar with
Exadata
it's roughly the same with couple dupe
related enhancement when you reach some
grano which is in Hadoop HDFS block you
and this kernel doesn't hook are written
in rows you scan it again and calculate
some metadata over these HDFS block
like minimum and maximum for example you
scan use dental blocks you calculate
minimum and maximum over loose to blocks
like shot on my slide and then you run
the second curve which requires all date
where movie ID is equal 1109 and based
on the storage index you may understand
that you could skip second block because
minimum is larger it's high then
requested predicate and you just skip
whole block very simple idea but very it
could bring really significant
performance benefits so a couple of
things which I would like to note about
storage index is that it you could build
it up to 32 columns per table so you
have one external table and you could
build up to 32 columns it's completely
automatic process you don't have to run
any create index statement you just end
the data and if Oracle understands that
it works that you have quite efficient
where predicate you will build storage
index automatically it maintained
automatically good news that we don't
have a lot of updates within Hadoop
files actually we we don't have updates
you could append file but you could not
update the data and so
so far external storage indexes is not
supported for non secure databases and
for other sources with a store which
defined with storage handler and so what
else
let me share next let me show the way to
monitor it it's really easy you could
use statistics from the my stat and for
example if you run the query which will
filter out day to by some key then run
it again and then you could check the
statistics like a child shown on my
screen you will find statistics which we
call cell X T Colonel I hope I'd saved
by storage index and here this start
show you how efficient was your storage
index in ideal world in best-case
scenario it will be equal cell external
bytes requested for predicate of load
which means that you have n bytes on the
HDFS you run the scan and you skip
everything you skip all the data you
don't scan all the data just because of
storage indexes and this query will be
extremely fast so like one second or so
okay another one feature predicate
pushdown it's related only for or c or
parka file but as far as we know many
customers use parking or RC files like
like default or format for storing data
so we made couple optimizations around
this and let me explain it so again it's
our favorite flow how we read how is key
in any data format we read any file
great splits from splits we create
records then we create columns and then
we proceeded with cycle engine either
big data signal either high if either
Impala drill press to what everyone all
those guys works in very similar way
but how I can optimize this rate so I
could materialize my are my structure so
by default and like I just show it's
hard works with any data type and every
time when it rich data it do
transformation it's what we call
scheming rate which means that you read
any data format and you do
transformation on the fly but what if I
could materialize result of this
transformation and write with some
optimal format and write some metadata
and write some statistics it's what
called
schema all right and like example of
scheming right it Sparky file parka file
it's motorized view of your data which
has some statistics and metadata so our
parking file has some meta block which
could tell you which could tell you
which maximum and minimum numbers of
certain columns to have within small
some small chain
so file divided on blocks which is 256
Meg's by default each flock divided by
chunks which is one megabyte by default
hood and each chunk each chunk has
metadata like minimum and maximum like
storage indexes which will tell you do
you need to reach this data or not this
channel cannot so sounds very familiar
restored index and ID general idea is
the same but only couple differences
first one its storage index it's build
it on the fly so and you don't need to
do to create something you just run the
query and it works parka file it's
creating parka file its kind of details
so for great party file you need to do
some transformation and storage index
works on the block level which is more
coarse parka file or parka file
predicate pushdown worked on the chunk
level which is more brenell so for
example by default in one block you have
256 chunks so predicate push down it
small crenell let me summarize all those
performance features
let me show you our couple slides which
will I hope which will help you to build
this puzzle and after this I will be
glad to answer on any questions so you
read how works are picked a testicle in
general you have iostream
you have Big Data sequel on the head
website you filter on the head website
on the storage tire tear and then ship
rest of the data towards database set so
if we will zoom it we will find two
components one component read any kind
of data any data format then convert it
to the Oracle block format apply smarts
can do other stuff other optimizations
and move toward the rest of the data
towards the database set when you run
the query
I really like the schema thanks for
Marty Gubar for this schema and I'm glad
to reuse it and show it to you again
when you run the query as a first step
you do you get the partitions you get
the partition list and do first
optimizations which we call partition
pranic we so far we don't start any scan
we just on the query describing on the
career planning phase on the database
side on the day-to-day he said you
decided hey I need only last year I
don't need to read other five years
which has stolen my database and based
on this I get block location list only
for certain partition then I distribute
I apply my processing I plan my
processing I plan how I will distribute
those scans errors air cells do column
pruning and run scan
fetch face its scan itself as the first
optimizations I apply storage index it
is exist then if I still need to read
the block I applied predicate push down
so and then if I don't if my block
doesn't return any data I create storage
mixes then like next step I do data type
conversion I do I transform any data
format towards the Oracle format and
then I do final filtering filtering and
final smart scan after this I ship data
towards the Oracle database so for
example you have 100 turbine table sorry
push wrong button
you have 100 terabyte table you specify
some certain partition and you prune out
9 out of 10 partitions you need to read
only one partition and you need to read
only 10 terabyte of date turning
terabytes of data not so bad then you
start the scan and you understand that
you could skip 9 out of 10 terabytes
because of starch indices but you still
need to read some some of the data some
you still need to read 1 terabyte of the
data good news that you store data in
Parkville or file format and you could
read even part of the HDFS block because
of predicate push down and then you read
only part of the of these blocks and in
the end is can only 100 gigabyte of data
so there is no miracle and if you wanna
to finish your query faster you need to
either have a modest
it's like approach of the route
or you could read less data big the
testicle allows you to have
optimizations within that you read less
amount of data so you could find more
information on the big data secure blog
post either on the documentation now I
think we do have time for answering on
any of your for answering on your
question regarding the performance
feature solves a big data secret okay
you should be able to unmute yourself so
I'm not going to tell you every one
because it just causes a back row noise
so if you could unmute yourself and ask
a question or type in you have anything
to ask don't be shy it's always clear
and nobody wanted to ask any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>