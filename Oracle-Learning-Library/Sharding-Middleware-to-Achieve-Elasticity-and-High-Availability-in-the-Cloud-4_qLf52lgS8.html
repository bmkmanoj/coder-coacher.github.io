<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sharding Middleware to Achieve Elasticity and High Availability in the Cloud | Coder Coacher - Coaching Coders</title><meta content="Sharding Middleware to Achieve Elasticity and High Availability in the Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sharding Middleware to Achieve Elasticity and High Availability in the Cloud</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4_qLf52lgS8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is brian oliver and standard
disclaimer which says everything you see
here doesn't exist does exist it doesn't
exist we all have to have these so today
we're going to talk about shouting
middleware and as it so we often hear
about shouting databases now we're going
to talk a little about applying some of
those concepts and hello how they apply
to middleware and what we're trying to
achieve with those things I said I'm
brought over I've been in working the
coherence team sort of a solutions
architect at large go around to other
teams to learn them about shouting and
scaling this is cameron purdy you
introduce itself I'm hoping these slides
are good because I haven't seen it so I
wrote these slides and I've been working
with Cameron from many many years so I
deliberately didn't give them to it it's
it's really good to watch Cameron work
and see him on his feet so if anyone saw
the Java one keynote on Sunday years it
was pretty good on his feet so we're
gonna do that again long as nobody calls
me fat this time yeah yeah they did
Callie fat in front of like to where's
your fuel bed two thousand feet so a lot
of these concepts are talking about
actually I think I've Cameron invented
them but I was certain influence from
working with camera for the last eight
years on on these things and we're
applying these principles over and over
again and there a part of you know
products like coherence which is our
work and Cameron's a founding creator of
coherence and we wanted to try and try
and uplift that and talk a little bit
how we apply these not just to
applications but how it's time to
investigate applying these two just
middleware in general and how that fits
with Oracle so and of course we're on a
you know we're on a mad drivers everyone
probably knows to like move to cloud so
there's a whole range of sort of
paradigm see that has stained stitched
together we want to sort of step back a
little bit and talk about how you get to
cloud
the challenges to get to crab so let's
get going so far so good so far so good
we'll get you at the whiteboard a minute
so we'll do it this auto we're going to
talk a little bit cloud what it means to
be cloud-ready what cloud providers give
us and then start this drilling and
we'll talk about code and things that
you should do or shouldn't do when
you're writing code if you want to move
towards cloud and we're not we're not
just talking about you we're actually
talking about what we do as well how we
think about middleware how it's starting
to think about middleware and the
principles we apply then we're going to
talk about sharding and the i guess the
sort of real real examples will come
when I'm we're basically going to work
shop a few architectures and let me
interactive so you can feel free to fill
out and so try and white board as much
as we can so not death by slides so
let's just set seen a little bit I just
went to Wikipedia and when okay defined
cloud for me and it comes up with a
whole bunch of stuff so provisioning
agility starting and stopping stuff
quickly pretty easy it must be able to
sort of reduce operation and capital
costs it's interesting it doesn't say
anything about development costs no
where is the dollars reduce development
costs it's all about the other side so
we're going to talk about that like the
cost in terms of development device
location independence we're sort of know
that that virtualization so you know we
know about cloud we want to virtualize a
lot of stuff and once you start
virtualizing we talk about multi-tenancy
and multi-tenancy is very difficult you
know that the apt at the sort of serve
it here and virtualization you can sort
of do it but and the camera will maybe
provide some insight into this you know
how do you virtualize and have multiple
tenants inside a JVM so that i run into
each other and starve each other of
threads and resources can we talk about
that the notion of reliability how does
reliability effect
when you move to cloud most the times
when you talk about reliability what
they say is oh you can have two sites
you have multiple sites okay I'm not
quite sure if that covers what we need
to do as developers and so on we can go
down the list so typical cloud provider
and we can pick any one Rackspace amazon
this is basically the sort of stuff you
get now what is sort of interesting is
that well it's gone there are different
types of sort of infrastructure
providers this is sort of application
other cloud providers and sort of Oracle
would sit in here is like virtual
machine as a service social integration
app logging security you know the first
the first set of sides are really
talking about you know infrastructure as
a service so that is that PowerPoint
you're running yes it's a PowerPoint
what what address is it loaded at and
RAM don't know do okay is it how many
threads does it use don't know anyone
want to guess many threads as PowerPoint
use a lot probably you know how many
chorus do you have done not you don't
know how many cores you have but it's
working yeah I think I have pool don't
want it to actually have two of this
it's an old you know I remember years
ago before we had threads we actually
had to like know exactly what was
running where because we actually had to
put it there like if you had me two CPUs
in an SMP box you actually had to like
cut reboot the pre-boot did the chip and
actually linked on on your your code to
actually get get executed so what you're
saying here is we don't care so much I
think we're in the stone age or the
cloud right now I think we're in some
ways we're kind of in the stage with
with middleware and that hey how many
servers does your app run on you know
and oh I see someone in the audience who
who's probably heard me talk about this
before like for the most part you
shouldn't care for the like how many
CPUs do you have how many threads do you
have how much how many kilobytes kill it
what's
like like hey I mean if it's a Java app
how many gigabytes are you faking like
you shouldn't care right but when it
comes to new areas of Technology like we
haven't gotten there yet right we
haven't we haven't pushed those concerns
down into the operating system right so
if we say well how are you going to
shard an application so that it scales
across multiple machines it's still a
conscious thought but if we said you
know if we said how are you going to
shard thread shard your app server java
ee app so it'll use more than one core
like so if you ask someone that they
look at your like what are you off idiot
i can you just do this and you know
requests come in they get load balanced
and you know goes to different threads I
think there's a thread pool or something
how many threads i don't know i think
it's dynamic it Tunes itself doesn't it
turn itself I think you helped write
some of that so when you look at when
you actually look at what you're offered
it's basically a virtual datacenter you
haven't we haven't progressed well
here's a box but it's not a box it's a
box it's not a box it's somewhere you
don't already is so we've got the don't
know where it is part but it's still the
same stuff and so there are high level
so some of some of the cloud providers
are starting to provide other levels but
it's still like you have to understand
it you have to like allocate know where
these stuff is and really take advantage
of it you almost have to change the way
you write your code and one of the
things I think we want to accomplish
with a concept like partitioning or
sharding in Java EE is that if you're
talking about it in five years we failed
if you still have to think about it
consciously like how do i how do I
partition up work how do i how do I
split this up so it will scale like part
of the goal has to be how do we push
things down like just like you would
into an operating system right I don't
think about dynamic linking if I'm
writing executables for windows even
though I know behind the scenes it does
all that right it's creating lots of
junky little code snippets formed behind
the scenes just so when I call foo it
calls
into the actual deal like why would I
want to know that yeah and if I'm
running something on it on a scale out
elastic infrastructure why would I want
to know how it's being divided or how
many servers it happens to be running on
on a particular time so you can see like
working with this guy this is what I
listen to all the time and but one of
the one of the passion of visions here
is like this point here like we want
developers using the stuff that we're
working on now including like the
commercial stuff but also with the
community stuff part of Java that you
don't have to think about this stuff
less and less I've got a question in the
third row gia pasion is if you're using
a cloud you should you have an ops team
or you don't even need an ops team did
you get at the mo you absolutely have to
have an ops tape if you move to one of
the sort of infrastructure to the
service you have to have it oh you
remember what happens the developers
become the ops team the river original
Java to EE j2ee is it called back in 99
I guess the you know this idea was that
you know you would put these components
in place and then the server that hosted
them would provide a container for them
that would allow you know multi-threaded
multi-core multi SMP environments to
spread the load over it and you didn't
have to when you wrote a servlet
actually I'm gonna lie a little bit cuz
i remember you did actually have to
think about it at first if you remember
that there was an interface you could
tag onto a servlet to say make me a new
one for every thread or something some
hack like that but other than that other
than that you didn't have to think about
it at all like you would you'd write a
servlet and you know you don't care how
many instances there are how many
threads there are and with a program
that's what a programming model gives
you it says that someone spent the time
to think through up front you know what
questions you have to answer so in the
servlet case what question do you have
to answer somebody hears written the
servlet there's only one question you
have to answer service thank you to
Santos g6
right you can also implement an it if
you want I think there's a method called
knit and things like that so you could
do lifecycle stuff on top of it but
basically question comes in at service I
have a request and response do something
you know take something from there put
something in there you don't know what
poor that's right you don't you don't
need to know a port it's running on what
IP address but actually there's a lot of
that information available you could
drill down and get that if you needed it
and I think that's what the same thing
we have to think about how are we going
to build middleware elastic environments
for the cloud and think about what we're
trying to accomplish how many we're here
yesterday when I spoke was it yesterday
I spoke here how many anybody Monday
couple I mean so it's that concept of s
plus h a how do i get scalability and
high availability baked into everything
i do so what we're trying to do with our
middleware you know java ee design with
the products that we build on top of
that this thing about how do we push it
into the programming contracts as a like
service right request response service
you answer this question and you'll get
scalability of of your servlet the same
thing here do you find it somewhat
ironic if it's more like that you know
we went through these phases of having
containers which are providing all these
facilities to contain as being semi evil
and very evil to coming back now to you
know virtualization is in a container
like and we're actually talking about
having more and more containment to
provide this so in Java EE the reason
the concept of container was denigrated
hated for some period of time wasn't the
container concept itself was the fact
that they get anything to run in the
container you had to write a
dissertation of XML cut and paste your
code seven times in two interfaces and
abstract classes and then you know run a
lock on alacea a pre compilation process
it took five minutes and then restart a
server that took eight minutes only to
find out you miss type something so so
this is enormous gap between sort of
what we have been what we're being
provided like we look at what the cloud
infrastructures like and what you know
that
list of abilities and things that we
have to achieve is it's a phenomenally
large gap and you know what's what I
found is particularly scary in you know
just having a customer haven't told you
but I had a customer call last week and
they're like desperate we got a new CTO
a huge company revenue CTO and we're
told we have to move all of our stuff
about our data center to two separate
cloud instances with a che in two weeks
right and it's like okay and they're
like they know their CTO is bad like
insane but there is this total gap and
right so how has developers do we start
and like you know Oracle's like actively
researching thinking about what we can
do to move forward so you know you like
I don't know how to answer that like
we're like we're working on it but it
won't be ready two weeks I guess that's
a different form of a two-week notice
that's so yeah this is a fundamental
disconnect okay and if you're an
architect you know we've got a lot to do
so so we have to deliver these things
and what we could we could sort of let's
take out a few of them and let's talk a
little bit about them we really have an
hour so okay so let's think about our
code if we want provisioning agility
what what do we need to do what do we
have to do how would you write your
application if you want you know
provisioning agility so what do you mean
by provisioning agility I need to be
able to start this thing and stop it
there may not be a whole application
maybe I want to serve as a service or a
component in an application that's
faulty I need to take that out and
upgrade it perhaps roll it back I think
one of the first things I always think
about is how to build something in such
a way that it operates in the same
predictable manner whether I'm running
it on a single server or running it on a
hundred servers and you know one of the
key principles for this and and this
doesn't necessarily apply to an
application as a whole but it applies to
what
ever whatever level that you consider to
be a service or a component or maybe it
is the entire application but for that
unit it has to work in a predictable
manner regardless of how elastic it is
whether it's hard coded to run on one
server hard coded run on ten or it
varies or whatever and I think rule
number one is that responsibility
gravitates to the server in other words
if I have one server all responsibility
gravitates to that one server so if I
start up one server it's responsible for
everything it's responsible for every
every aspect of the surface if there's
something that happens every once a
minute to met you know to check some
backlog of something or to refresh a
cache or something that server is
responsible for it on the other hand if
I get to 100 servers I may still out of
that hundred servers only have one
server responsible for a particular task
there may be a task that can only be
done by by one server so i have to have
a way with in that hundred server
architecture to make sure that only one
server will do it and if that server
dies that someone else will definitely
pick it up and still there will be
exactly 11 server doing that so in terms
of let's move on in terms of the foss
and location independence we had this
interesting conversation i'm going to
write an application that has to be
deployed to a cloud and has to move
around what are the sort of things am I
not or shouldn't I be allowed to do well
that's and I think you're going to start
seeing some of the show up certainly in
our own products as well as in Java EE
what we're doing CDI all over the place
you know I would like to get to the
point where you never say new and you
certainly never have a static method or
use a static instance of something a
singleton instance you know because
basically you as a as a developer if you
want to build something that can move
from one machine to another or you you
can't make any assumptions about the
environment you're in and what CDI gives
us what injection gives us is ability to
say look I know I need something
and I know I could write a line of code
that would give it to me and it would
work you know on my notebook maybe on a
single server but i abdicate my
responsibility for knowing what it is
I'm looking for all I'll defer that to
the environment that I'm running within
so declarative we say this is what I
need instead of saying okays cash
factory dot get static factory dot get
something exactly because I mean you
think about this way if you have a
component that's dependent on a static
inside a JVM it's very hard to move that
component and you just screwed yourself
if you want to go to osgi for example
totally so as soon as you start using
statics you're you're in you're going
down a path or you're already in a part
but think about think about some of the
things that are implicit statics as well
when I say new file new java.io.file
that's a static you don't see it because
you're doing some some object you
thinking well you know I know there's a
/t mp4 it's actually what not going to
work on windows but you know hey I'm
going I'm going to say new file but it's
a static because there's a file system
implied by saying new file but if i say
inject a file into me i need a file i
need to read and write from something
right now in the injection i can say i
need you to inject for me a temporary
file okay temporary file that's no
problem because I could I could have a
local drive on my server or maybe there
are no drives on my servers because I
redeploy to a new data center and they
have a policy that everything goes on
the sand so I don't have a local file so
i can create a memory memory mapped
memory-based file system or i can use a
stand portion of the sand that's
allocated as temporary storage but you
know in other words instead of assuming
that you know something about your
environment because where it gets really
pernicious is when that new file is
something that has to stick around for a
while you say new file and this is why
java ee has a prohibition actually from
using going directly after the file
system so that for things like this for
all the types of resources that you can
be consuming they need to be injected
they should never be never be grabbed if
you
well you immediately create a dependency
between and I simply cannot be even
worse embedded in the co2 the file
system to the structure of all system so
you imagine if your application has to
be injected with some file means that
your application doesn't necessarily
have to know it is you can configure
what that is and you can have through
CDI you can say oh look it has to be
injected if it's not there the
application can't run or the component
doesn't run so you imagine what it
happens changes how your code because in
the code you write new file if file
exists below you have all this error
checking around your stuff just binding
yourself to the resources as much as you
yeah implicitly oh it's the old way of
doing it but imagine you take that new
file and you have to be injected with
your files or injected with where you
can get a group of vials or resources
world changed a little bit your code
changes the other thing it does is it
caused a configuration reach around
because because the decisions have to be
made from that that's probably not the
most politically correct thing to say
from that place is grabbing the grabbing
that file right so if you need to change
how that works it's down in the code
right so it's like oh I'm going to make
this more flexible so how do you make it
more flexible well I'm going to go check
system dot properties or you know and
suddenly you get into a giant hairball
and the hairball is like way down there
but then it has like reach out to find
you know what its dynamic behavior
should be where with something like an
injection style approach you just
abdicate all that responsibility to say
this thing will need something okay
where it goes well I don't care where it
comes from so now I can mock it more
easily I can you know so which makes
much more unit testable it also makes it
friendly whether it's running on one
server or a hundred whether it's
virtualized or not whether it's two
other two other related issues mi led to
create threads and secondly my eyes open
sockets again their resources you should
not as a developer but what does do
something I synchronously well how would
how would what you know what are the
what are the platform services for doing
something asynchronously if you need
some platform service just declare it
say inject me with
something that I can call back to do
something a synchronous with so so then
one using Apple like this nominee apple
notebooks around what do they do in the
last how many months ago I saw a year
ago they introduced what into
infrastructure and the operating system
Grand Central yeah GCD grand central
dispatch take away all of that threading
nonsense that you had to ride like we
have access executor service yeah but
obviously like we need something a you
know and then that helps with
multi-tenant right so if I have multiple
tenants you know you don't want it you
want to be inside a JVM and then each
ten it's just like creating their own
threads creating their own files you
want the infrastructure to be able to
manage that don't you study would we
used too often do with JMS and we'd
create some complex patterns of you know
sending messages to maybe a
message-driven beam or something like
that but you know it's interesting that
that what JMS gave you in that case was
actually two things it gave you that
asynchronous nature but it also made I I
talked about software's is kind of like
energy you've got kinetic energy and
potential energy right so if you have a
boulder sitting at the top of the hill
its potential energy right when it
starts rolling its kinetic energy right
and one of the problems that you have
with highly available systems is that
boulders rolling and then the server
loses the power and the boulder is not
only not rolling it ceases to exist
right so a JMS gave you was the boulders
persistence right so you know another
server would be responsible until that
JMS message was committed in terms of
the consummation of it until that
transaction is committed that Boulder is
still sitting at the top of the hill
somewhere right so even if it's rolling
until it's been until you say yep i'm
done with that that that potential
energy remains and this is a pattern
this is a pattern that's extremely
useful for building highly available
systems is that you have to turn what
you want to do into data into
information it says i want to do this
that's the potential energy and then the
converse of the converse the other half
of it
is that you need something like a
message-driven beam or like you know
we're talking about I need one server
out of 100 to do be doing something
specific you need someone that's
actually looking for that work to be
done executing on it like as if you is
if you will an executor service so a few
other things about cloud reliability and
this is obviously / 0 understands this
like going multi-site you know just
because you deployed multi-site as I
mean you're suddenly highly available
it's like okay so are multiple data
centers so you can dr it's like i'll
look it's very easy you know in cloud
infrastructure to say i'll look I'm
going to run up several instances my app
but availability zones for example on
ec2 yeah so question is like here do you
even know well I mean I mean currently
it's basically you deploy your app
multiple times it's like okay I've got a
you know a center here in or Center in
New York and a center in London and
center in Dublin then you just deploy
your apps to those those places and then
you configure the load balancing itself
so it's just as if you had built data
centers and you're doing all that work
to some extent you have some control of
it so the only one I have experience
with on this is ec2 and basically if you
don't say anything you have no idea that
could all be on the same piece of
hardware which is you know fairly likely
to be a single point of failure but but
you can explicitly say you know I want
these on different servers I want these
in different what they call availability
zones for example and you can even force
them to be in different different
regions and basically every time you
tell amazon something that you want its
you just your ching ching yes they do
give you free traffic between the data
centers between the availability zones
yeah not between the right it's not
between your regions interesting so you
know as we
so you although you can like do these
deployments it doesn't really help you
like you have to build builder design
components that they can move and they
can work in a cooperative way and we'll
talk a little bit more over the
shoutings now obviously elasticity i can
start instance and shut them down but
that's fine to do that but how do i
actually move stuff you know we were
talking about if you have statics and
your files are in a local file system
now i have to make sure there's file
system replication if I've bound to that
so you know there's a lot of effort
going into it so I had this slide and I
sort of think there's a scale right so
resource coupling in it that's sort of
what you know Karen's talking about is
this notion of like coupling yourself
your program your application to the
resources are you there you have on that
single machine the more tightly you're
coupled to your machine or your
deployment the less likely you will be
able to deploy it and scale it and
choose some of those cloud things on the
butt on that side on the if you're
really tightly coupled the applications
tend to be high high performance because
they're not going to traverse networks
that you know so there's a spectrum and
on the right if you have something
entirely loosely coupled and we've seen
this paradigm you know the actor pattern
is a classic example you know very sort
of academic design lots and lots of you
know messages are passing between them
and there's nothing new about this or
object oriented programming is all about
this but deployed like that yep
completely loose coupled you can move
things around but incredibly chatty
right like on basically this probably
the second slowest piece of
infrastructure apart from disk so
separating every object potentially with
network isn't going to be a great
solution so you know some applications
work very well like that so somewhere
between those two is some sort of
balance and we've to strike that balance
so so we talked a little about this so
being cloud-ready means being a becoming
sort of loosely coupled but
we're going to strike this balance so
let's talk a bit about shouting so one
of the things you try to do is shouting
or petitioning is to strike a balance
because we want to be able to sort of
partition up data and from we always
talk about data we think of services as
data sort of the way it is across
multiple servers and order preserving
manner and this is actually taking the
MongoDB order preserving actually quite
important when it comes to requests
coming into some chard some petition now
you hear about this in a lot in
databases is principally done to
horizontally scale so a very simple
example is I'm just going to take parts
of a data set I'm going to put you know
a to see surnames on a particular set of
servers and then d2f on another set of
servers and and we've been doing this
for a long time and we would manually
allocate and associate tightly couple
lows shards those petitions down to
individual servers and essentially
that's sort of what MongoDB does and
there's nothing wrong with that I
recognize writers really no I mean the
New York Stock Exchange uses that
architecture for their trading engines
they have a pair of servers free shot
for each shard and you know it's it's
it's predefined you know there's not a
lot of new ticker symbols every day on
the equity equity exchanges the data
sites the amount of data things that you
can try it stays very static even even a
few thousand added or removed today is
pretty static for them yeah so shouting
ah what so there's a there's actually on
the site Korra calm there's actually a
discussion of what's the history of the
word sharding the first time i heard it
was back in a conversation with Gregor
hopi who at the time was at Google and
he said that the reason they used the
word sharding was just because they
liked it but they really meant
partitioning so and and basically it
says shouting ease the petitioning so we
should just say petitioning of data
services but I use shouting because it
sounded cooler sorry you see this
throughout most database if you look up
sort of shouting technology or shouting
patterns you'll it's just going to be
databases so but it's applying like
those principles into middleware yeah as
long it becomes yeah basically you can't
do a lot of operations across the entire
database because now there's lots of
little parts it becomes harder to do
things across the entire data set but
within a data set it's better and you
can have finer granularity so this is
good and bad with shouting so but in
principle we won't apply it to any sort
of state we are applied to search so I
think it's really important it's not
necessarily that simple shouting it's
hard so the question is like how do you
sharp like do we shard based on region
you shut okay a beast coast of East
Coast data center west coast somewhere
mikis okay and then have all the
infrastructure design like that you
design your whole app every part of the
tier the data center everything this way
so what happens if you make a mistake in
your choice of sharding strategy you
just sort of host a little bit
and you know this is offer some f5 slide
so they're talking about you know
different different techniques not
obviously f5's interest in switching
technology between these shots but
that's what's called global load
balancing which is another word for DNS
so MongoDB does sort of shouting as well
it's like lots of little MongoDB some
configuration and you can go to one
Mongo and will point you and route you
to the appropriate shard and off you go
if you don't have to do joins which key
value store no SQL and that don't lean
essa saraly need to do that then it's
never a problem there are some
challenges with this this architecture
though too it basically has to keep it
index of where everything is in all the
shots which itself is data which itself
could be large which you should actually
shot as well so it yeah it becomes a
little challenging so I'm going to
that's um let me get Cameron to do some
talking I want to ask him some questions
so what goes we're going to talk about
what goes in a shard what's associated
with charge how many charge should there
be if you're building a system how many
should you have there's one what is the
number that each that should you go to
add and remove them how what sort of
technique to use to associated with
physical infrastructure you dynamically
what's the granularity of them where
they still got some read a lot that's a
lot of lessons yeah we're gonna do it
through an example and strategies for
his annoyance so you can you can take
the stand so I want Owen Cameron to this
is his test I care until walkthrough
like breaking down something like nike
nike plus so like they are around like
20 million runners for all sports people
may like Cameron he uses his band and
they have you know 20 million people I'm
making up a number it's around that who
you know login and want to upload their
you know sports results jumping results
running results into a system that has
to stay running all
time how do you go about solving that
that's the only thing I get that's what
you get so you could have let you know
we could say happy well what would be a
traditional way we could take a whole
bunch of stateless servers we could
stick all the data in a database and we
could just smash the database with
requests actually now we know actually
hoped it the first time yeah so we know
that doesn't work yes on 8 8 8 so
September august eight the 2008 they had
the they're going for a million
concurrent runners this is the end of
the Beijing Olympics and it they had a
massive overload on their on their
systems and that's when they started to
re-architect my understanding is this
when they started to riorca tect for the
new system of fluff that nike+ uses but
you know one of the interesting you know
one of the first interesting things you
start to learn when you look at a system
is is what things are what things are
throw away and what things aren't that's
the first thing you try to understand
what things have to be consistent what
things don't matter and the reason I
always try to start sorting into these
buckets is it the only thing you pay for
in scaling is consistency everything
else scales infinitely if you don't need
consistency you know like Paulo wilts
kels also Google scales it's great
because it doesn't have to be consistent
if i do the query twice on google i get
to a different answers because it's not
running off a shared database you know
there's not like some database in the
back you know Wizard of Oz back in the
back like okay you're looking for that
yep it's on HTTP but like there's no
there's no one look up right there's the
information is completely out of sync
across you know thousands of servers
well not completely they try to get in
sync but they don't care if it is or
isn't this is anyone if it rises support
requests with Google if you do the group
the same search twice and get different
results is it on really care do you
understand why they have the I feel
lucky button but it's like this is that
I really don't care just to show me
something and then there's other
information that they also don't care if
it's consistent like advertising stuff
right they want to make a quick guess
really quick what should I throw up here
for an advertisement on this query what
should I show them on gmail if they're
looking at their mail what advertisement
should I show
they don't have to have like the
absolute transactionally consistent both
like not only that the things that you
would think are incredibly important to
have like high quality as a service like
charging for ads like it's a nickel and
add who cares if you accidentally lose
one right so like a nickel who cares if
you lose a nipple you so you have to
like challenge the assumptions of what
the quality is a service are that you
want because what you're trying to do in
architecture is you're trying to push
back each each area of functionality to
its actual set of required quality as a
service so for example if I don't have
to be transactionally consistent on you
know the number of dollar is being
charged for ads you know I can I can
asynchronously store that information
asynchronous is fine because if I lose
the server there's a couple nickels
right even if the customer says don't
charge me more than this budget this is
my budget amount well if I have more to
charge them than that I just take a loss
I eat the the overflow right so i can i
can see the cost of architecting a
perfect architecture is wide right oh
that's right and asynchronous
architecture is architecture that scales
way better than synchronous architecture
so if I can make it asynchronous in
terms of data updates and if I can relax
consistency in terms of data access I
can scale infinitely your infinite with
a shotting lets you do a bunch of things
I synchronously like what well instead
of having all of your customers queued
up coming into one system you can break
them up break them up into buckets at
least a call them buckets right we
actually call the shard buckets inside
of carrots so you can break them up into
buckets and they can all work
independently you know there's probably
some ordering within that bucket yeah
its fundamental principle okay so I've
got all of these customers what what am
I shards going to be well the
interesting thing with the nike one is
that they can't fit all the data in
memory right so that's like problem
number one like they want to run real
fast like everything's in memory but you
know it's more more data than fits in
memory you know it's not a question of
cost
I mean you just there's just much more
data than there is RAM so you know how
to deal with this well you know you go
running and you come back and you sink
your band right the thing is you know
you come back and you sink your band
like you're there but while you're out
running you aren't connected you weren't
well maybe maybe where I guess they have
apps on the phones and stuff now too but
like when you're not connected that that
system doesn't have to do anything in
relation to you so if your user is a
shot like we talked to I granular if you
use as a shard that shud doesn't
actually have to exist in the system in
the law faced and though I've system in
a live set and then you know but what it
does is it introduces an another concept
that that becomes very handy in this
type of system which is an event-driven
system like when the user logs in that's
a pretty good hint that that user is
going to start doing things that the
system to answer those requests will
need information about that user right
so like a login is an event and it's an
event that you know a system can use to
prefetch asynchronously for example
start to prefetch all the information
that's likely to be needed on behalf of
that user so it's as if you have all the
information in memory because login
takes you know 10 milliseconds but from
the users point of view it takes you
know 100 to get there and back and then
you know they're looking at the screen
it actually takes us time we're kind of
slow as people to like actually read the
screen and click on whatever it is we're
wanting to do so I have two questions in
what what would be in the user shod and
where would that shall be located so i
mean amazon and i have lots of service
well the first thing is when you after
you push back on on consistency
requirements after you push back on
synchronicity requirements then you
start to think about how you're going to
collect things together I so in this
case in in a nike+ case all of the you
know I don't I don't actually know their
system in and out I mean I did so
laborious trusts but you know on a
system like that or a system like an
e-commerce system all the information
related to eat to a user that's the
information that
need very very fast response times for
so basically what you want to do is you
want to start to co-locate things bundle
them around the user so whatever the
user information anything related to it
what do we call it the affinity we call
it affinity and coherence this idea that
yes i have a user and then I have orders
I have some way for example maybe it's a
shopping cart or maybe two orders or
something like that I want that
information to gravitate to the user so
that when i go to do something answer
some question about the user and i send
the request to where that information
would be in a distributed environment
that all the other information it needs
is actually going to be either there or
close by there or assuming it's not
there it'll know how to get it okay so
so you can imagine why you not to do
Mikey did have lots and lots of service
and you'll use a Charlize yeah Cameron
moon shot and the request comes in from
outside because you're plugging your
band you upload your run probably the
first thing he wants to see is as is
uploading previous runs good they don't
use nike+ you want to see your previous
what runs because I've just done a run
i'm going to compare them i'm going to
see the white light it's very cool side
so where would i not store my previous
runs like wouldn't store it over here
right you know I would have them have
these have my first run this is mostly
create affinity between between the
state that you're that you're processing
and you'd also it's not just the state
you also create affinity for the
requests themselves so the requests hung
in if you will on the part of the system
that manages that information there
could be data base its architecture its
so yeah I wish you had John John's slide
do you remember the one he had the four
elements of scalability right so you can
do some things I know like we've done
some work in this area but yeah your
database is here and probably a load
balancer you can actually direct them to
the server weathered a little bit but
even if even if you don't right but you
can actually plug in load balances first
of all you can plug in load balancing
algorithms the other thing that most
people start with at least is a
two-phase so what we called on the first
one we call it spraying so what I'll
call stateless load balancing meaning
the request comes in no matter the
balance or doesn't even look at the
requesters oh I gotta go there go there
go there cuz I go there oh they're gonna
write whatever random round robin and so
that comes down to the first tier of
servers typically those are web servers
apache with a plug-in like mod proxy or
something like that and then that set of
servers actually now takes the time to
look at the HTTP header and figure out
what server should go to so the and by
doing this basically you completely
unload the f5 which is getting hit by
every single incoming request so it has
nothing to do except send it on okay so
from the web server it goes down to a
specific instance of the next year not
whatever that whatever that is app
server
here right and and at this point like
let's say we don't even go on with
Europe though is actually a completely
different use case that's where they
were talking about with the pluggable
routing that's called global load
balancing so global load balancing is is
a great story because it took me a while
to figure out how it worked because no
one published any documents on it
there's a reason why the dirty secret of
global load balancing is it's called DNS
in DNS is free and they wanted so at the
time around the year 2000 you had a
couple different companies including
big-ip and f5 with a couple cisco bought
one of them and anyway they're these
companies making tons of money on load
balancing hardware and basically
embedded just a missing embedded stuff
for doing load balancing and they wanted
to be able to load balance between data
centers and so you would come in and
you'd come into a data center and it
would give you an IP address as
basically as a redirect if you will
using DNS and they wanted they wanted to
sell this but they didn't want to call
DNS because it's free so instead we
called a global load balancing and so
here this is this is where you would
actually plug in some logic and so that
when it comes in it would no no based on
who's coming in or or whatever to send
them somewhere else and you can often
see this you know if you do nslookup
switch and turn your caching off you can
see this you know company like yahoo or
google you know it's not it's not like
one address and it's not static I'd yes
so that's called routing correct well
you want to answer that so well you had
some slides in your showing you know
some of the other day so let's just
think about Middle where it's not not
worry so much for the database so
there's a few ways so it's with Mike so
we won't worry about the database
because we don't know what it's a Davis
actually if you and back we went back to
the Mongo one Mongol actually has a set
of config service is basically index so
a request comes in it goes to that
Fonzie index for your data and then rats
you some rats you back but that's one
strategy okay here it's actually does
something so no it's another one would
be something like mem mem cached
memcached right so memcached you give it
a static list of IP addresses it hashes
whatever it is you're looking for modulo
is it by the number of items in that
list and that's the address to go to
always if that servers down it's not
there if it sounds easy it's not
surprising since they wrote it in a
weekend right that will get you to that
piece of data yeah that's right so the
question here was again the question is
doesn't it rely on a uniform
distribution to be load balanced right
so one of the challenges would you know
and you look at shouting in a database
database world is this challenge of how
do you balance the number of users that
are in each shard so there's there's
that problem but let's say there were
all balanced let's say you did have
balance charts how do you allocate those
in a way that they're distributed evenly
across the service but that's still not
enough because I could have a
non-uniform requests for users in a
shard yeah green even if I have a
completely perfect shard allocation the
volume of requests could actually change
the diner dealers in financial services
so we have a couple cases where
someone's built an application it you
know spreads everything out uniformly
and it's like everything's perfect
except one of their customers is is my
grandmother who comes in you know twice
a year and she's like okay my balance
looks okay good and another customer is
like Charles Schwab you know with
600,000 customers of their own or
something like that like so so all
coming in like so you have like one
server doing nothing and another server
running red hot at a hundred percent so
again it's a
of example where distribution doesn't
always mean the same thing in terms of
uniformity is uniformity of users per
machine or is a uniformity of load or
universe uniformity of information
capacity what's the what's the sort of
target so 11 way I like to think about
it I'm sorry you said you were going to
show a movie oh yeah do them each other
10 1 way 1 way I think about this is
like okay let's say the nike case i have
had infinite amount of hardware and data
center and increment of money i could
basically give every single person who's
a customer 20-odd million of them a
their own computer right they have
dedicated cpu dedicated memory I and we
back it up and we'd do it and so we just
take out amazon we're just like we're
going to take all your computers are
going to we have awesome quality of
service or we could try it all on one
computer somewhere in between is a
balance it and that involves you know
money and air conditioning and all sorts
of things so the concept of chard is I
may have like 20 million shards but how
do i allocate them or how what is the
process of allocating them to the
hardware it's another problem and you
can round robin them you could the
notion of being able to move them so if
you have we go back to the beginning
like we were talking about what what are
you allowed to do your shard shouldn't
be statics your shard shouldn't be
creating files your shards right I need
to alleviate stuff that my shards do the
resources I have them attached to so I
can move them around so I do have a
system and I add another server I can
balance the shards out well is your
audio on that madly no no idea we're
going to make it up as we go sure so
we've been doing research in this area
for a long time and this is some
research it's a movie that came as some
we're looking at shard recall petition
allocation strategies so this has done
my son labs
and took home some absence yeah article
/ sun labs so this is actually
demonstrating a little bit of coherence
and we have one server here and what
we're going to do it'll there's a couple
of services running inside this is a
basically JVM and we're actually going
to start up some more jvms in a second
and so our cluster so you imagine center
cloud I'm you know add so I've added
another server and it's actually the
left is shown the allocation or the
server coming up and at this point I
might just or was it so at this point
these blue things that just came in
those blue things are what we call like
primary charts so there they hold data
or data that represents services and the
green ones of backups because we want to
make sure that shards have a backup so
if I lose a server I can bring it back
up and recover the shot so and at the
top here we say you know theoretically
or logically there are unknown ones that
haven't been allocated to servers yet
and these are the green ones that are
backups so you can see in this demo like
they all come in and as as new servers
arrive this is actually real time
they're actually servers coming out real
time and we start to move the shards
around and this we wanted to visualize
how this happened in real time in the
system so there's no outage like with
Amazon like they add new stuff all the
time there's no outage they can move
things about and they're showing you
here as they mouse over which partitions
are where the where that for each back
up where the primary is refused primary
with the backup is so each server and
Cameron alluded to this like each server
has a responsibility and they have to
look after you know in this case a
certain set of active shards and backup
shots and as you know as the mouse moves
over as the point moved over a drew a
line between where the primary was or
whether back up was because you want to
make sure all the primaries and backups
are on different servers okay now we're
just doing this in a really small
example but if you imagine playing
shouting to
nike with 20 million customers and they
don't have 20 million shards there's
some number they've worked out some
number and it's actually pretty easy for
mathematics of it is fairly
straightforward to work out what a
suitable number would be it's nice
because they can add servers on the fly
this is right remember we talked about
elasticity this is what we want right be
able to add server on the fly or remove
those on the fly so so the interesting
question is what if you only have one
server so go back all the way go back
all the way to the beginning here so we
start out we we start up the one server
go ahead let it play and you see that as
the first server comes up basically
everything goes there so what is that
everything right it its responsibilities
like if I'm the only one here and
something has to get done you know I
can't give it to someone else to do
that's the that's the model so each
shard isn't a piece of data it's not a
service it's a it's a responsibility it
could be data it could be services it
says that you know if ownership of a
shard is here that means I'm responsible
for whatever is associated with that if
it could be a specific piece of user
information it could be a background
process that has to run so this is how
you make sure you know whether you have
one server 100 servers if there's a if
you're all singleton process that has to
be executing you assign it to a short
you assign it you assign it some
identity that's that's uniquely owned
somewhere within that environment so
that no matter how many servers you have
only one of them is doing that work so
let's do it the opposite way I'm
building an application i'll write it
runs inside a JVM and then i decided to
scale it you go through this process of
basically sharding your architecture
afterwards the degenerate in sort of a
degenerate case a single server case is
no no no real different from a to server
case it's just that the single server is
holding all the shots you start thinking
about shards early or immediately and
when you're deploying with the singles
you're just running on your laptop you
still have all of those shards they're
just running in the one jbm I want to
have two JVMs we just spread them out
that's absolute key to go to clap like
right so we're not intentionally yes
right so we have these these are you
don't have to move first of all you
don't have to move it all at once it can
be completely asynchronous yeah you know
so this is this is showing it moving
very fast because they were running on
40 gigabit InfiniBand that's cheating
yeah so we can move a lot very quickly
but you know we could also we could also
plug in something here that would for
example even with no servers being added
or removed so for example one of the
things we were hoping to do we had a
customer years ago the online gambling
called betfair and actually used to work
there that's where I met you isn't it
yes um that's what we need this we need
shot so but we didn't steal him away
from betfair we stole him away from this
next business he worked in that was a
financial company yeah so anyway at
better are the interesting the
interesting problem they had is that
half of the load on the site which turns
out to be like slinging billions of
pages an hour I mean the number is just
obscene and most of its robots right
because it's built like billions of
pages and our something i Mena the
numbers are just off the scale but half
of the load on the site was against one
atomic piece of data one object one
object think so so think about sharding
right the goal of sharding things like a
deck of cards right if I have five
people at the table and I'm dealing out
the deck of cards 12345 12345 12345 it's
like really easy then the decks gone
that's like sharding right but what if
everyone is interested in one card right
what if
everyone comes and talks to the person
who has the ace of spades hey everyone
right so I have 10 players now 1 2 3 4 5
6 7 8 9 10 12 things right everyone
still come into the ace of spades does
it even matter but I could have a
thousand servers and it's not going to
be any better so one of the so what we
when we started designing how we could
we could augment the the default
partitioning approach that we used in
our coherence product one of the goals
we had was to be able to push away
partition so you could imagine a case
where you have let's say we did it as a
heat map and in the betfair case they
would have had one white hot red hot
white hot pick a pic a colony hot one
very very very hot chard and a whole
bunch of very very dark blue freezing
cold charts right so you would want to
put that you would want to leave that
white-hot chard wherever it is don't
move it because it's got like everything
coming don't move that but slowly peel
off all those blue ones from those
servers send them somewhere else make it
so that one server owns the primary copy
of the white-hot chard one server owns
the backup copy and stick everything
else on on the other cher bitch right so
you dedicate two servers for one object
and for a long time they thought was we
should move the hot move the hot one to
another server but Candice server that
wasn't doing much let's move the hot one
that's actually the worst thing to do
like you you've got all this hot data
moving it's going to cost you right we
have to move it across the wire or some
infrastructure so leave it there and
move all the others because they are
like a below SLA and we've been this is
what we've been doing is experiment
because there is no like every use case
is slightly different so what are the
permutations and we're like so oh there
you go so we actually can do this it's
like we can move partitioned the shards
manually but we also want to be able to
this this short leash starts to show
heat as well don't actually in this demo
yeah it's in this demo so you can start
to see they become hot and those are the
hot ones the red robots yeah
so we start to identify it now in the
past we just we were just looking at
while making sure we have the right
amount of data but we realize you know
if you you can actually design a system
have perfect shard you know allocation
and still lose service like a denial of
service attack will take a server out
and so in the worst cases degenerate
case we have one super hot object it
which is atomic you cannot cut the ace
of spades up everyone has to queue up on
that there is no other solution so you
start to worry less about it but so my
question with other question was how
what's the number of how many time we've
we're over time Alton we should have
finished we are we over time remember
how many charge should we have that's
you can answer well actually yeah it's
exactly what yeah we in coherence good
well at some point you have to go like
you have to actually get into a data set
up you're going to deploy it but when
you write your code yes model the model
we're moving to is that everything did
sharted ever having to Luke every single
because it turns out that the only thing
that actually works in a distributed
environment is a definite partitioning
approach either for availability or for
scalability or for you know things like
reliable you know knowing the right
answer to a question so how many shards
you know the model I don't know maybe I
shouldn't explain all of our detailed
strategy for the long term but the model
the model that we're using is that
there's there's actually different sets
of shards yes so in the first set of
shards is what we call basically the
configuration the bootstrap which is a
single shard now why would you say why
would you do a single shark well because
you build the key for high quality
software send everything through the
same code path right you don't want like
10 different ways to do something so by
having a single shard approach you can
you can still have the high availability
by
having multiple copies and having you
know everyone knows to go to that shard
but now you don't care which server
starts up first you know when you use
weblogic ever my budget yeah what's the
first thing if you're managing a cluster
what do you have to do you have to start
the the admin server right and then it
starts the managed service right well in
the future shouldn't be that at all
right you just start up service doesn't
matter which one starts first it owns
that initial bootstrap configuration
right so that's the shard of one so as
the other server startup they know to
get configuration from that it's a
partitioned configuration if you will
but it's a partition of wine so everyone
goes the same place right but then what
is inside that bootstrap is the
information about the next layer Nick's
of char depends you know is its services
information so if you won one way to
also like so from we're working from the
bottom up but also yeah I'm working from
the top down pick parts of java ee peg
part of like pig part of the upper stack
in terms of like so uh pick JMS pick hv
sessions pic web sockets all of those
you can apply shouting to it and when
you do you have this you get this
amazing ability to an elasticity built
in that that's the direction that way
and in fact what you'll see for the
reference implementations for java ee
that oracle is responsible for in java
ee will be introducing these concepts so
you know the reference implementations
will all be shards of one so to speak
but it's so that we can start to bake
into you know the api's of our products
and in standards themselves these
concepts is basically fundamental
building blocks so for every service
that we introduced in Java EE it'll be
built from the ground up to be running
on one to a thousand servers and by the
way not one to a thousand servers that
you have to know about but one to a
thousand servers that hopefully within a
few years you don't even know how many
it is other than the fact that Amazon
gives you a big bill at the end of the
month so you download your you know
glossary future the app server feature
or container feature and you run out
your notebook your desktop and it's got
one chart and these
okay now I want to run in a clustered
environment as many many it's okay you
just turn up the number of chart not to
go so everything is this transparent you
should got it and deployed the cloud you
can develop with that technology and
that metaphor and we've been working on
this for some time like live objects so
last year we did a demonstration I thing
called live objects you could take a
regular Java object annotate it throw it
in a grid we demonstrate with coherence
and you can it was exactly prove what we
did previously these live objects at
their event driven you can talk to them
you can invoke requests on them you can
kill the server and they're just appear
somewhere else so it's just like this
object that's alive compared to like a
regular Java object which would be dead
stateful listen handles events can move
around so that's the boulder the boulder
both in rolling and standings call it
the same and the number we gave that we
go that presentation like three times
and the number one request was can we
get it into Java Java EE and so we're
okay so we've like now we've got other
research projects when we started
thinking well let's bring along the Java
metaphor so we started talking about
elastic bean so you're going to define a
javabean and hey I what this seemed to
be elastic and I wanted to be grouped
with these other ones and just look
after it container look after it for me
and then you know when you move to clout
like all of those abilities that you
wanted I will just do that for you we'll
just have that sorry for running over a
little bit but thanks for staying</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>