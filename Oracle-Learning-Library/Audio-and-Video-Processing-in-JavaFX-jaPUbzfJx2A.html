<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Audio and Video Processing in JavaFX | Coder Coacher - Coaching Coders</title><meta content="Audio and Video Processing in JavaFX - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Audio and Video Processing in JavaFX</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jaPUbzfJx2A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody welcome to audio and video
processing in JavaFX my name is Brian
burkhalter with me here is david DeHaven
we're on the media engineering
development team so again welcome thanks
for taking time out of your schedule to
drop in today first a little bit
disclaimer but the title is it would be
better if the word processing wasn't in
there
so it's what we don't really do any
processing per se but if this is an
audio and video and JavaFX we got a
little exuberant okay this is our
message from Oracle legal staff don't
believe anything we're going to tell you
now the Angwin case this is the
disclaimer so here's our agenda today on
that going to go over some background
material on FX media may just see out of
curiosity how many people have actually
used FX media no 1 2 3 and obvious 4 or
something okay
that's good so most of you are guests
are interested but haven't actually
touched it yet so that's good then we're
going to go over what we have added in
our most recent public release 2.2 what
we're working on for the next one which
jumps to the you know 2 to the 3rd
instead of just going to 3.0 we don't
know why either but that would be FX 8
then we have a very brief part on FX
media of coding best practices because
we promise to do that in the abstract
then there's some demonstrations and
we'll leave some time at the end for a
QA or general discussion diatribes
requests whatever all right so first
thing is media architecture to get them
give us some background on what the
structure of the package is we have a
multi-level stack here that you would
see primarily through JavaFX seen dead
media that's in public Javadoc for
JavaFX that's the only part that
application developers should use these
api classes interact with yet another
job and layer below that's Khamsin media
which is used internally by our web node
stuff
to play media content those things
however should not be used directly that
if you do so that's at your own risk
then below that we have a j'ni
connection that goes connects the
internal java stuff to our native layer
which is what does all the real work
these intervening layers the purpose of
them is to adapt the design of the FX
media interfaces to the behavior of the
native layers below or actually the
opposite of that at the bottom we have
most things sitting on top of this GST
platform which is based on the GStreamer
open source package very mature set of
code that was developed initially on
linux and it's now I'm running on all
kinds of things including panda board of
it's very active community I've just
went to their conference at end of
August and they're just about to go 1.0
so it's very stable and solid stack and
that's it's on this G lib thing which is
a cross-platform version of many common
operating system
programming libraries like threading and
i/o and all kinds of things like that
then we have some of our own plugins
that plug into GStreamer because it's a
pluggable framework and these are based
on different native packages depending
on which OS we're on and windows we have
DirectShow linux we use something called
live AV just kind of like the Swiss Army
knife of codecs and so on and on Mac we
use audio converter and video decoder
packages or interfaces frameworks I
guess also one particular case on Mac
only we have an OSX platform that we use
with a cutie kitty mutation underneath
and that's to handle some cases they're
not really simple to be handled by the
the gstreamer stack so at this point I'm
going to pass it over to David it's
going to take some of these next slides
oops
wrong way wrong way there you go
so media API is fairly straightforward
basically there's two segments we have
audio clip for quick fire-and-forget
little agency sound playback I have a
demonstration for that later and and the
other side of that is the media API
which is based on media the JavaFX scene
dot media that media is the core of that
and everything else builds on that in
the only actual scene graph portion of
that is the media view so when you're
creating your applications you'll be
working with media views and creating
media and media players and plugging
things together so we try to make this
as simple as possible but still you know
be powerful enough to be useful without
having to write down write thousands of
lines of codes and then
like that was it oh yeah we have we have
event mechanisms so for example we have
audio equalizer or a audio spectrum so
you can do visualization in your
application of audio that's playing
through the pipeline and also there's
error feedback on both media view and a
media player I believe media also has
error yes and metadata is available
directly from the media object and also
well actually now on track yet but we'll
get there and we also have programmable
markers so if you want to coordinate
aspects of your UI or application with
the events that are happening in the
media you can add a marker and set it to
fire it is a specific time and it'll go
off within you know certain amount of
time and it's a next
and so we have you know basically what
is covered at the bottom here or on the
left you have media which basically
describes the source and it's your
source of information for the media
itself so if you reference an mp3 file
that you'll have metadata describing the
the file and any metadata that's
embedded in the mp3 will be available
through that and the player has the
standard controls you just your player
state volume rate autoplay all these fun
things and then the media view sits on
top of that
and this one's yours okay
and this is the same thing yet again
emphasizing that you when you're
creating a play situation in it your
scene you're moving outwards in this
diagram you create the media then you
create from that a player you attach the
player to view the view is a node in the
scene and that's all part of your stage
so next go back down to the bottom level
this is a mode mostly applies to the
GStreamer sort of pipeline this is
what's actually going on at the native
layer is you have a a cyclic graph of
objects you have a source then that's
generally goes to a demultiplexer which
splits into audio and video uncompressed
packets each of those goes to the
respective decoder and then for audio to
the sound device and for video we pass
that off to the internal prism rendering
engine so that takes care of sending it
via whatever
ideally accelerated pipeline there is or
if none of ailable to a software
pipeline for rendering so see
so part of this whole thing is that
there's a state machine that's defined
at the FX layer and again we adapt the
state machines of the lower layers to
this so that you have a uniform behavior
for example a GStreamer there's only
four states and that you only you can't
enter one without crossing whatever the
intervening ones are in our case we have
a state machine that starts with an
unknown state and then when the pre-roll
is finished and stuff ready to happen
this goes into ready state an already
state is is really paused but the first
time paused is entered so from that you
can send things to playing and from
playing you can pause it you can stop it
stop it is the same as pausing and
rewinding to the beginning it may go
into a stalled state from playing and
that means that there's not enough
bandwidth to do the progressive
downloads so we stop playback while the
buffer fills up and then when there's
sufficient data we resume playing again
ok here's yet another diagram of how
good things look like this is a JavaFX
scene graph overall and you see that
this there's the stage as the overall
thing that's the application has that
has the scene in it and the scene has
some content in a part of which is a
group and the group contains all these
nodes well at very bottom there you see
media view is the third item under node
so that's where media fits in all this
hierarchy so the visual representation
just ends up being a node in the scene
graph then where's the little example
I'm going to pass this back to David for
a demo as I've talked through this slide
here this would be your start method you
pass in a stage create your scene name
it and then the part that's interesting
for us is create the media object from
the source which is a URI a string at
this point you create the player from
that media object here we set it to play
automatically that means that it starts
its transitions to the playing state as
soon as it's ready with
you having to take any action then we
create the view node from the player and
add that into the content group that's
part of the scene and this is enough
code to start playing a video that
simple so I'm going to pass it over here
much bigger
all right so I thought we'd just do kind
of an interactive version of what he
just said so we're going to start fresh
from NetBeans and we'll just create a
new project this is NetBeans 72 new FX
application I'll call it demo demo media
I'm using JDK 8 but nothing I'm doing
here is specific to it this all work
with the stuff that's currently publicly
available ended wheel where to go
and that means is non-cooperative
it's a demo guys
let's try it again I created the folder
let's try opening it
you know I kind of want this assisted
with Mouse
all right
let's try it again
there it goes ah much better
so when you start a new FX project you
get kind of a pre-baked thing it's not
very interesting creates a button and
print something to stand it out
hopefully runs
it
so let's get rid of that
and media constructor takes one argument
which is a URL you can use file or HTTP
or jar assuming you have you know the
right security privileges to access the
jar
oops
and create the media player we just
passed the commedia 5 or the media
object we just created Sutton I teams up
about that
and then finally we create the media
view itself and there are builders for
media player and media view if you want
to use the Builder method to do all this
oops am i doing here
and for chef manager imports and then
outlooks left is to add the view to the
scene graph oops
oh yeah and because we have no controls
we'll set out it played it true so it
automatically starts playing as soon as
we launch that was fun
and there it is
so you'll notice it doesn't scale
properly because we haven't
we haven't had anything to do that so we
can fix that by doing crude dot was it
with
with property add listener new
validation consider the beans nicely
creates a nice little
inter-class for us and we just say the
view set fit this is with width to route
get width and it's going to complains
they're not final
don't do what I do
and we just do the same for height
control keys unfortunate place
but I
if that cat
you can also use the observable past sin
but because I'm being lazy we're not
doing and now if we run
we'll say that the movie scales properly
as we change the window size
so that's it I mean there's you know
basically three lines of code to set up
the media and then the rest is just
standard Java effects scene grab stuff
and you're you're up and running
cool
switch
and if it's gonna crash
alright so we just he just went over
that live three lines to get the media
going pretty much moving on from that
we're going to review here what we did
in our most recently release 2.2 what
does they what we added this was the
first general availability release for
Linux actually on media team we've had
the Linux stuff running all along so we
were just waiting for the rest of the
VEX to catch up we've put in here
audio-visual decompression using the Lib
AV external packages which are readily
available those have to be installed
separately and only reason we can't
bundle them is that the usual stuff
about codec problems for IP and so on
but it's a reliable and good quality set
of external packages we've added HTTP
Live Streaming which is a form of
adaptive bitrate streaming we'll talk
about that a little bit more after this
the audio portion of that can be mp3 the
video portion is AAC audio and h.264
video in an mpeg-2 transport stream so
this allows for effectively multi bed
bandwidth streams and they can be live
or canned and we also added the Mac OS X
cue ticket pipeline that handles the as
you might remember an architectural
diagram that was a separate platform at
the native layer separate from GStreamer
this handles the HTTP Live Streaming
part on Mac and it also handles the
h.264 decoding fall back because our
gstreamer pipeline for h.264 decoding on
mac uses in an api that only works with
GPUs and it only handles GP certain GPUs
and then it can't handle more than one
stream at a time so we had to do
something about that situation and we
did with this QT kipped
pipeline so from that we move on to talk
about HTTP Live Streaming so is anybody
not familiar with HTTP live for
yeah so HLS is basically it it was a
scheme designed to make it easier to
string media using just a standard web
browser or web server rather so on the
server side you you have an incoming
source that gets transcoded to multiple
bitrate streams and then they're
segmented into typically ten-second
segments and then each the stream is
accessed through an index file it's a
dot in 3 you file and on the client side
the client is responsible for monitoring
how the band the network latency and
bandwidth and it's responsible for
choosing what stream it it connects to
so there's no it's unlike RTS B where
there's actual negotiation and then a
protocol between the two you're you're
separated by the client actually just
talking to a simple web server so
there's there's less you know setup
details on that and aside from creating
the stream in the first place and it
works with either live streams by use of
a what they call sliding window so
segments can be created and deleted on
the fly as the stream is as a stream
plays on so if you want say 15 seconds
of a live stream to be broadcast at any
one time it'll it will rotate through
however many segments it needs to do
that or it can work as a in that video
on-demand mode and that in that case it
just it leaves the segment's on the web
server so they're available as as the
clients need them
yes this happens if he washes RealPlayer
yeah yeah apple spoon pushing this for a
while there they they came up with this
back and they've I believe they started
broadcasting keynotes and in hls a few
years ago maybe more than that but it
works really well it's a lot easier to
implement and than RTSP because it
doesn't require you know routing logic
to get through firewalls and other such
things
I think it's an IETF draft that they
seem to renew every September so it
never goes beyond draft that they can
never get it off the off lines yep and
it's useful for audio and AV streaming
audio supported as either raw mp3 string
or encapsulated in a DTS Packard's next
night that's it
okay this is an example of a top-level
playlist that you may get when you're
you will get when you're trying to use
an HLS stream this is what the client
gets from the server yeah so you can see
the each each stream has basically the
the program its associated with so you
can actually have multiple programs from
the same index and the bandwidth
specified for each stream and in this
case they go up in bandwidth from 500
kilobits to one point one megabit
roughly more or less and in this case
they also scaled the video that's not
always the case but sometimes this is
actually in index to other playlists
which will be even further segmented
probably by bandwidth again yeah so this
is you may have this second if this is
in a file system you may have this at
root and then this has four sub folders
or sub directories each of which has
another m3u file in there yeah and
that's here's an example for 720p and
then this these indexes get updated in
real time by the segment or unless it's
video on demand then there'd be on stab
so those are all ten seconds mpeg-2
transport stream pieces here okay moving
on this is our report card I guess I
thought I this is some stuff and the
left column that we claimed we were
going to do a year ago a year ago being
October 5th 2011 instead of the third
almost a year ago so this is stuff we
claimed we were going to do and then in
the right-hand column that's the current
status of it so the stuff in 2.1 and 2.2
delivered as promised
that is to say HLS that we just
discovered HTTP Live Streaming mpeg-4
support and AAC h.264 encoding support
for our next release we are in progress
implementing Auto audio visual capture
or recording as you will and we're maybe
putting in something to support plug-in
elements from the user plug-in elements
but that's to be determined another one
that's to be determined is we had Vorbis
audio codec that this seems to have
become irrelevant now that opus is
pretty much a standard so we're thinking
about the opus codec instead then the
other thing audio synthesizer that's not
on this roadmap at all right now so in
other words we did some stuff but not
all of it so we are making progress okay
so moving on to FX eight we have some
plan features and some perspective
features I'm going to what the biggest
one here is recording that's the one
that's got the most votes in our JIRA
bug database which you guys should
freely contribute to because we do pay
attention to where the reports come from
and the stuff that's come from outside
it's gets higher visibility to us than
stuff we find then the other things here
I think I'm going to let David run down
the rest of these things so some of the
features like accessibility requires a
multitrack support so mp4 files support
say multiple audio tracks in a movie
multiple video tracks sometimes so we
need that so we'll have language
selection available through that GPU
video decoding we kind of have already
we're always trying to push the
boundaries on that we're trying to move
that further up in the rendering
pipeline to get it closer to the
get the compressed data closer to the
GPU so that there's less of a less data
flow through the stack as Brian
mentioned the codec sensibilities is
we're pondering a plug-in architecture
maybe or we're not quite sure how that
would work but we would like to allow
people to implement their own codec
packs or you know be able to work with
formats that they want to work with
instead of what we can deliver and
stream sources is something we have had
interested in interest in this is
basically providing say a byte stream to
a media player instead of having to
create a media from a file or HTTP or
jar URL and then media caching we need
to basically yeah as media is pulled
from a source it it's a little messy
right now so we're hoping to clean that
up and make it more efficient and reduce
the memory footprint okay we're going to
review a couple of these things in
diagrams and one more text slide and
then go to some demos so here's our
accessibility so accessibility basically
our primary concern is closed captioning
for initially for mpeg-4 because it's
got there's a standard defined for that
we were also investigating implementing
empty time text and will be providing a
consistent mechanism for getting that
data from the media as its played and
also providing some means of rendering
it over the media
that has most that's to be determined as
of yet so just a short example of time
text it's basically it's just XML data
and it it works very similar to the
markers and what will happen is you'll
get events in this case
at the very beginning you'll get an
event with the time text markup language
being sent to the any listener and then
as the media plays it'll it'll continue
sending events as a as they're
encountered in the media okay and the
thing that we're starting to work on
most actively in all these things is the
recording infrastructure this is a
reasonably accurate diagram of what we
have already designed and partly
implemented we have starting off a
device manager an ability to detect
which devices are connected to the
system
audio and video and depending on the
attributes of those what are what
attributes of each of those might be for
example audio you may have different
dimensions you may have different frame
rates for each dimension you may have
the ability to spit out stuff that's
compressed or not or just raw pixels you
may be able to spit out a still image
likewise on microphone you may have the
ability to pull out a AC encoded stuff
or things at different sampling rates it
just depends we're a little bit new at
figuring all this stuff out ourselves so
we're learning some things so the idea
here is the somebody could discover
which devices are available then they
select their device devices from the
list of available things and set those
on a recorder object it's sort of the
analog of a player but in the other
direction and they can select to preview
the video part
and to write the whole thing out
multiplex to an output file and we're
also looking at the moment I have having
a callback to get raw or compressed
audio and video packets as well we're
not looking at at the moment putting out
a multiplex byte stream however it's all
kind of a function of what we can get at
on the platform
so back to GPU decoding currently for
example on the Mac platform we use the
video Dakota acceleration framework on
Windows we use direct show and we're
transitioning to media was a media
foundation foundation
yeah media foundation these are all
being done at the very low level under
gstreamer
and it it works it it's beneficial but
it still requires pulling pixels back
and pushing them up the up to the
rendering pipeline so one thing we'd
like to do is is be able to render
directly to a texture and hand that off
to prism and delete display directly on
the GPU so it'll reduce CPU overhead and
memory bandwidth requirements and then
again media cache so everything that we
pull is goes basically through some Java
later we have to do that for security
reasons and unfortunately right now as
you create a media player it grabs its
own stream from the source and has its
own caching we're going to put in a
cache manager that will juggle blocks of
data as as they're pulled from the
source so it's a little more unified
between the different media players not
not a typical use case but we see it
often enough that it's worth putting
into we're putting time into
okay where are unique slide and best
practices just to live up to the
abstract we just put together a few tips
so one of them was mentioned earlier in
the presentation I think was that the
idea if you have an observable list or
map it's best to synchronize that on
that when you receive a notification and
do whatever and then get out of that
block but the main thing is we came up
with here were partly due to the state
diagram because what you have to realize
that underneath this stuff at the bottom
layer we have these native engines and
they're all asynchronous so you just
because you create a media player
doesn't mean it's usable you really have
to wait until you get a state
notification of ready to be able to use
the thing if you have autoplay that
doesn't matter it will just skip
directly to playing for you so there I
guess the main thing this first bullet
is to pay attention to async versus sync
operation there's also the idea of
cleaning things up which like being in
Java it shouldn't really be necessary
but it turns out there are some
references that can hang around not
entirely the due to the media stack
itself but if you're really done with
something that's best is to do a little
cleanup to improve performance and get
rid of any trailing memory hopefully
we'll be able to deal with something you
know deal with this internally so none
of this would be necessary later on but
suggestions are to stop players
disconnect a listeners disconnecting
objects from the scene in particular
because there's hard reference held back
up through the scene so that just allows
us from doing our collections internally
so you may have resources allocated and
then lastly there's a job FX scene media
package Javadoc package summary it's a
really good thing to read the whole
thing if you're going to work with the
FX media that includes a section on
errors and this is just culled right out
of that and since the idea is you want
to watch for synchronous errors when
you're creating objects anything that
has its error property set you can
assume is garbage and you should get rid
dereference it get rid of it and any for
async errors you should register your
listeners and handle them accordingly
that's all in the
package Javadoc so moving on I'm going
to do a couple of demos here start out
with a player just to this thing here
this is not available for download as
yet we the people that handle getting
this you know people between us and
making it available we're just too busy
so this thing is supposed to show pretty
much of all the bells and whistles as it
were of what we have for playback yeah
so this is just it shows a you know time
volume balance on the left we have a
state diagram intergate indicator at the
bottom there's a progressive download
percentage you have media info up here
if there are any metadata for example
mp3 id3 tags those would be displayed
here we have the controls enable you to
seek for example there's volume which is
relevant because you can't hear it and
you can set how many think times this
thing loops we have a start and stop
time that's not implemented so you can
actually set the period of time over
which the things played we have typical
play/pause stuff and you loop you can
loop it I might as well do that
we have audio spectrum we could discrete
Fourier transform negative phase samples
out every periodically so you can use
that for spectrum display or for
creating some sort of visualization we
have a graphic equalizer so you can set
different audio bands emphasize or
de-emphasize I don't know what's in the
playlist here we have this these markers
you can set markers at certain times and
as that time period is traversed then
you get into values you to the land of
the gatekeepers so I'm going to just
cause this guy here I guess there was
some sound I'm going to do then the next
part here which is a proof that we are
actually doing something so this is for
you people in the front I'm sorry but
there you are so now you know if you're
sitting by somebody you shouldn't be
then maybe you know if this might get
posted layer you might want to move so
there's a little record yeah this will
be on our YouTube channel tomorrow gets
being uploaded right now so that would
be awesome so here's record ok turn the
preview off let's see what happened
open then this should have put the stuff
in FX recorder if wallah so you know
what it may look like in future time you
may get turned in you know if this is
fifty years ago it would look like this
but there's proof positive that we're
actually doing recording this is uh on
Windows right now and we're going to be
doing the same thing else on the other
platforms so at this point I think David
has one or two demos or something and
then hopefully we have time for some
questions just while he's firing that up
how many people think they're going to
have some questions so we can leave some
moments one two or three Mitton not a
lot then okay
I'll need the audio - can't do my audio
clips that audio
okay so if you're on the open jfx
mailing list you might be familiar with
Jose Martinez he's been working on a
video game written in Java FX and he
graciously provided us with a build of
his game to demonstrate mostly audio
clip but he does actually have some
background music running as well so give
me here here the background music
knowing
and I'm absolutely terrible at this game
so
you
better in rest on crispy critters God
walk faster there you go is AI oh no I
ran out of gas I thought there's one
more weapon that it so there you go we
have audio clip and media burning in the
background I guess we can go straight to
okay if anyone has any questions now
would be the time yes in the back
okay the question is about callback
events in particular markers marker is
just a point in time and a name so you
can set these things on the media object
so the the call would be just like set
marker or something with the string and
a time and then you have a listener for
this marker listener and when that point
is traversed that point in time is
traversed during playback the thing will
minute emit an event that has that name
so if you know what name is associated
which time then you can trigger your own
stuff the ideas of you know basic
synchronization against with other stuff
like animation yeah is that answer that
yeah there's a couple of things there we
have a thing that we didn't talk about
too much call this the start/stop time
and a cycle you can define a start/stop
time that are between zero and the
duration of the media and those are
actually defined as sub media between
those two limits and we call a one
traversal of that sub part a cycle and
so each time the thing hits it into the
cycle you're going to get an event that
in case that happened and there's also a
separate event should it hit the end of
media which is the the things actually
done playing the number of cycles that
you asked for and these are all separate
events on the media player object itself
so you can add individual listeners for
different events okay
of course are running the is the API
self-contained of the needs of freestyle
software if I create an application that
makes use of video or audio distribution
and finds they need something else
inside the genican traffic speed so as
of seven new six I believe FX is part of
it and we're everything that we have
right now is part of the FX deliverable
so we're in the the Java Runtime the
only caveat is that because of the the
IP associated with some of the
compression algorithms there are in
certain cases things that you do need to
install separately written it and these
are becoming fewer so on linux you have
to install lib AV to do any kind of
compressed audio or video decoding this
is you know on a bun - this is a
standard package so it's a
straightforward thing through synaptic
or whatever and then on Windows XP you'd
have to install something like main
concept but as of Windows 7 and onward
the those decoders are built into
Windows so there's nothing to install so
pretty much it comes down to in the time
frame of FX 8 Java 8 it would only be on
Linux next
beginning connector led them to my like
swing application and just play actually
now in fact I'll I've written a lot of
test code that just creates an audio
clip plays it so it doesn't as far as
I'm aware doesn't require you don't need
an entire scene graph to be able to just
play audio clip
you don't need actually a scene graph to
do anything except play the render the
video frames if you have no video frames
then you don't need a media view that's
the only thing that's a node the rest of
the stuff could just be you straight up
yeah yes is the media package available
at FX embedded well not yet but we
actually have run the thing on
BeagleBoard so you know it down that
Avenue there don't look to be any
impediments in terms of the arm beagle
panda sort of thing it's mostly a matter
of what they decide to do yes
and the question is how firm our codec
extensibility plans it's up in the air
right now there's issues that we have to
resolve internally before we can do that
but it's something we would desperately
like to do yeah we would love to do it
you know part of the thing is if you
allow people to plug it in then where
does your support issue end you know you
there it opens up a can of worms there
so you know we'd love to be able to have
something as wide open as the GStreamer
plugins but it creates a big testing
problem etc and so we're you know we'd
like to do it but we don't know yet
anything else
going once okay well thanks everybody
for showing up again and hope that you
find the stuff we've developed to be
useful</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>