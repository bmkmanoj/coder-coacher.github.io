<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Rise of NoSQL and Polyglot Persistence | Coder Coacher - Coaching Coders</title><meta content="The Rise of NoSQL and Polyglot Persistence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Rise of NoSQL and Polyglot Persistence</b></h2><h5 class="post__date">2013-01-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/txtiFCOqjhs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to California and let's get
started this is a little bit about me
I'm a software architect that just me I
am a frequent speaker in a lot of
conferences I guess this is probably my
fourth time speaking at JavaOne I
started a few user groups over the years
one of it was the no call jug and it's
per user group in the Bay Area my
twitter handle is Polymathic coded this
presentation is licensed and under the
Creative Commons so do whatever you want
with it
you know share it all you want as long
as you don't make any money and you're
nice enough to point out the source so
let's get started all right all right so
the first thing I want to talk about is
this section called the Golden Age of
relational databases they want to
discuss the relational model first so
what is this foolish talk a little bit
about relational data stores that have
been the predominant choice of storing
data in the past probably a couple of
decades I mean we've got oracle mice
equals equals service and all of these
things that have all of these
technologies and products or solutions
we'll call him whatever you want to call
him they have a wide adoption and
familiarity
we've got developers and even advanced
business users using and using these
things there is an abundance amount of
tool out there to the point that this
became the de facto standard for storing
data in the past even like 50 years so
these databases store data within what
this relational model which is simply
stores data in two dimensional tables or
relations in rows that we call tuples or
columns and has a well-defined schema
that is enforced with for the relations
within the relations between the
relations which for the tables between
themselves or referential integrity
and a set of an enforcing a set of
constraints anybody who has been using
the relational model dealt with this
idea of normalization what normalization
is is that smaller tables are always
good and well-defined relationship
between these tables with a well-defined
relationship between them why because we
wanted to minimize redundancy and we
want to try to avoid what we call
modifications anomalies right so we do
that through modification propagation or
cascading and and and such this
relational model is supported by a
sequel structured query language a
somewhat standardized sequel a query
language there are different dialects
and variations out there it's very
flexible and it allows us to do a lot of
good and operations across multiple
relations or tables such as drawings you
can do aggregations such as using
goodbyes and and and such etc the
relational bottom model is transactional
and we have this concept of acid and
acid as an acronym that stands for
island automaticity
it means like all or nothing when we
execute an operation and I our DBMS it
happens so it does not happen
Machida committed or rollback it's
consistent I mean consistency that the
database is taking or your data is
taking from one valid state to another
valid state and there's this concept of
isolation in the sense that concurrency
results in a valid in a valid state
always it doesn't really matter how many
people are manipulating your data your
database the re BMS guaranteed that the
data is not corrupted and this
durability that means once it's
committed it's forever it's not
something that just stays in there in
memory or like goes away so this
relational model that we've been using
over the years is designed with a few
assumptions the first one that the end
user would directly interact with the
database right so we've got an and it
makes perfect
that these are DBMS should manage these
things concurrency for us and make sure
that there's referential integrity and
all of the these things because what we
have is somebody sitting behind a dump
terminal avoiding sequel statements all
day right and you don't want the user to
deal with all that can with all of that
stuff to make the system usable what
this implies is that the access patterns
to access that data is and known nobody
knows how the data is going to be
queried so then we have sequel as a very
flexible query language there is very
close to English and we have this data
structure that is not biased to any type
of query I mean it's and so the second
assumption is that the database would
run on a single machine because it was
the only way to guarantee these asset
transactions so as we use this
relational model we had some a few of
old maps along the way the first of it
when we started actually building
complex applications on top of
relational databases and what happened
is this business logic started moving
away from within the LD b ms itself we
used to have a lot of triggers and
stored procedures all that logic was
moving away to the application layer and
the applications themselves evolved
beyond the procedural paradigm to
something more opie so what we have is
we ended up with this thing called the
object object-relational impedance
mismatch right so we spend a lot of time
and a lot of energy and you created this
object relational mapping frameworks to
help us out with that so no big deal but
we became data holders right our data
sets were growing out of control our
performance decreases as this data and
as the more the more data we came and we
we accumulate you know exponentially so
we ended up buying bigger and bigger
machines
you know Oracle RAC and all of that kind
of stuff and that actually put off the
part of the part of the problem for it
for a little while for us I kind of
solved it temporarily so we got to a
point when we actually went performance
started becoming an issue so we hired
somebody and he walks into the account
of the company indexers half of the
database you know created creates
secondary indexes which made our queries
little faster right he also created
materialized views for complex joints
and we have it was a nightmare to
maintain you know and they get stale and
then it's not we couldn't really have
something we couldn't really have
real-time data in there because they
took so long to execute so he told us
that we actually need to D normalized so
we denormalized our schema and it was
anything but a smooth smooth transition
because what we ended up where there's a
lot of redundancy that we did not want
to deal with to begin with right so um
so he introduced caching we have this
other layer of caching right there that
sticks between us in the database right
but what we we had these cache that has
data that is too stale and we have more
redundancy on top of that on top of the
denormalized the normalization that we
that he had so we are another guy all
right and he tells us that we pretty
much hit the limit of the one machine
that we need to actually get to the
point that we actually need a cluster we
couldn't buy bigger machines and they
were way too expensive even if we could
afford them so he told us that we need
pretty much just to scale out
horizontally came up with this gave us
two options you know we're gonna have a
master slave cluster assuming that you
read more than you're right so I have a
lot of slaves out there to do all the
readings and we have one master to write
that data and replicate it later all
right and you know what we're risking
right here is the risk of incorrect
reads between the time that the master
would actually replicate that data
across all the all the reading the
reading slaves so how's that
consistent so we lose another important
thing that the relational model is used
for to begin with and he told that it
told us all right since this is not
really going to work for you it can be a
little problematic you're gonna have to
chart your data and what charting is
pretty much taking your data set and in
distributed across and machines in a
cluster this was great because they
improved the reads as much as the writes
we have machine note n2 n1 n2 and 3 that
that persists or manages data of
employees whose first name starts from a
to n and such the problem with that is
we couldn't join across the different
partitions of this of this of this shard
so we lose whether a reference to a
referential integrity as well and this
actually is was very hard and there was
a big investment because they required
us to modify our client applications
themselves to be shoved where I mean
your actual application would need to
know what partition to query and to
query to make this work so we again
introduced a single point of failure how
is that consistent again right so what's
the point right
we vertically scale our relational
database because we had no other choice
we hit the limit of the one machine we
are no longer no longer consistent there
is no acidity anymore and we lose that
query flexibility because we couldn't
join across the different charts or the
different cluster so are we doing
something wrong right with constraining
ourselves to this relational model so
the next thing I want to talk about is
introduces the cap theorem right you
step back a little bit and then the cap
theorem is something that was introduced
by Eric Brewer on distributed system has
nothing to do with the databases of data
stores that pretty much tells you you
have to pick two things or if it's out
of the three consistency availability or
partition tolerance
this is similar to fast cheap good
service if you want cheap good service
you won't be fast if you want fast fast
good service it won't be cheap and if
you want fast cheap service it won't be
good right so if you want your data to
be consistent and available you're not
going to be able to have petition
tolerance that means you cannot be
distributed you couldn't scale that
scale out horizontally so to put this
into perspective and tie this back to
the relational model is that what we
have is that relational data stores
happen to favor consistency and
availability right the sense that
consistency that your data is consistent
that when you go and you're right or
like you update a particular record with
value one the very next read the very
next person that reads it reads the
value one a consistent value right and
availability is that the database is
always up it's always an answer every
time somebody fires the query right for
historical reasons there are certain
type of applications that whose
consistency and availability is
extremely important and these we're
talking about everything that deals with
money you're like banking data you're
all familiar with the example I deposit
$100 in my friend's bank account and all
that kind of stuff in the transaction
for that money to move be moved to one
account to another one so probably not
gonna go over that so according to cap
right partition tolerance is impossible
meaning that horizontal skia scaling is
impossible of relational databases
because they favor consistency and
availability all right
so we're in a pickle we've got too much
data in a CIA model vertical scaling
which is on the one machine is too
expensive it's not sustainable it's not
something that we couldn't sustain even
if we had the money to buy the biggest
most powerful machine out there so we
are forced to explore other alternative
and other options in light of this cap
theorem right so before we move on one
of these options is
ap availability and partition tolerance
we want a system that is available all
the time 24/7
everybody somebody fives the query he
gets an answer and we want something
that can be distributed because this is
important for application X so you use a
use case X so since we reach the limit
of the one who in machine we have no
choice but to scale horizontally of
course which means that which means that
we need this this is you know we just
have no other choice and availability
right nobody is willing to give this up
most of the time everybody wants this
system to be available 24 hours it's
pretty important so um and this this
pretty much becomes very much better
with distribution once you have a lot of
servers right there because in clusters
the individual the individual know it
might be an reliable by itself but the
cluster as a whole is is reliable if you
have 100 machines and one of them dies I
mean you're still up you're still
available but if you have one machine
and that machine dies I mean you're not
so what looks so according to cap we
simply simply cannot have seen that's
what that's what it will simply cannot
have consistency if to manage that much
data and this is what I talked about a
little little earlier about what
consistency is it's that I make an
update all the next reads would give me
an exact value the newest value the most
recent value that was committed or that
was written so anyways let's look at an
AP system an example of an AP system
before we move on so um the most popular
example is the DNS name and server right
not all the notes have the most updated
record when you register a domain name
you have to wait for a few days to
guarantee that every DNS knows about you
if you go and you register WW Polymathic
Polymathic CARICOM you're gonna have to
wait for a few days for every DNS on
earth to have that record out there you
know which is completely fine so we're
talking about
we're not really giving up consistency
but we're introducing this new concept
of eventual consistency she's not so bad
means that we just settled we didn't
give it up 100%
but we just settled for a little lesser
degree of consistency right
so if Mohammed in Morocco updated his
relationship status to from sync from to
single on some edge node right his
cousin or lives in Spain we see it
immediately because they happen to be on
the same age note Morocco and Spain
their neighbors all right but he's
secret admirers Sarah who lives in the
United States it's not gonna see that
until like an hour later no but it's
what I mean Japan will get the update
the next day but they will all get it
eventually I which is completely fine
who cares this is not financial data I
mean most of our applications actually
fall within this this this type of use
case so we're talking about eventual
consistency as opposed to immediate
consistency right some people actually
would make the case and argue that even
banking and all these financial
institutions can be eventually
consistence because of the nature of a
lot of transactions when you go to a
bank and then you deposit a cheque they
make the assumption that that check is
gonna go through and they hold those
funds so make a partial a portion of
those funds available for you for like a
few days and then they do the math later
on and rectify that particular balance
but anyways the compromise the
compromises that we settled for a weaker
consistency model which bring out a
bring bring us would bring us up to this
new customer for this new acronym called
base instead of acid right which is
basically available soft state which is
this transient state of your data that
is half updated have kind of half
outdated right and eventual consistency
and eventually that soft state is going
to - eventually everybody is going to
get the latest update and we're talking
about acid on
individual node and based on the cluster
based on the cluster right so the
slippery slope of the faithless right
first day that we lost faith on this
whole thing based on this cap theorem
and why not question the rest of you
know all of these things that the people
in the relational model told us is
extremely important the first of them is
the schema right having a schema which
is whether it's logical or physical and
what we mean by a logical schema is a
will defined very well it's a biological
schema is this vision relationship
between all of these tables all right
that is enforced every time why not a
flexible schema even know schema why not
you know how much are we actually
getting out of that right the physical
schema how the data is actually
physically persisted in in our DBMS a
lot of them are in B trees most of in
most of these systems right there why
not use some other and some other kind
of underlying data structure and
implement some kind of data management
layer that completely relies only on
another data structure also integrated
constraints who cares about those yeah
whether we're talking about referential
integrity in between the two tables so
without we're talking about the type for
example of the actual attribute on a
particular table whether it's number or
string whatever it is write a query
language anything would do do we really
have to have sequel security we don't
really care how many people I don't know
I guarantee you fifty percent not
guarantee you everything anything but a
lot of us end up just using root
password for their database so I hadn't
they have the security layer completely
in the external to the Lu BMS system
itself their own within the native
application right so name it we're ready
to question it no sequel that's what so
let's talk about it a little bit and see
so what before we proceed no sequel is a
wide range of specialized data store
stores with with the goal of addressing
the challenges of the relational model
as simple as that
um Eric Evans let's consider it to be
the person who coined this term said
that the whole point of seeking
alternatives the whole point of seeking
alternatives alternatives to the
relational model of course is that you
need to solve a problem that relational
databases are of a bad fit for that's it
so let's make this easier it does not
mean NT sequel or anti relational
database any data store that it's not
relational fits within the no sequel
family of technologies some people have
this not only sequel instead of no
sequel kind of you know mommy day that's
what they promote the term means so
let's kind of put them side-by-side and
kind of compare the sequel or like
relational technologies on one side
versus no sequel on these other
technologies just keep in mind that this
does not have to be this is not not not
strict in the sense that you'll find no
sequel new net and also cool
technologies that cannot run on a
cluster that and they can only want like
on a single machine but what these ones
are like common traits of most the no
sequel offerings that are out there so
we found one side that no sequel is
designed to run on a single machine no
sequel a sequel with its designed to run
on a single machine and no sequel on a
cluster the sequel or relational model
is usually ca the no sequel can be a PC
a and CP and out of even variations
degrees of consistencies and all of
those in some relation some no sequel
offerings and sequel scales vertically
no sequel scales horizontally sequel
relies on sequel and no sequel relies on
a series of custom API calls
sequel is acid as opposed to based on no
seek
we have full indexes on sequel and
mostly indexes on keys in most of the
new sequel technologies right rigid
schema on the sequel versus schema less
on most of no sequel technologies and we
have flexible queries versus predefined
queries and we'll talk about that a
little more in the upcoming slides right
so um what we have right here is that
the sequel or like the relational model
is concerned about what data what the
data consists of and the no sequel the
non-relational is concerned of how the
data is queried this is a very important
distinction when you go and you design a
relational model for your data the first
thing you do is you sit down you define
the schema and you denormalize as much
as possible with no bias toward any kind
of query patterns you have immunity make
sure that you have no duplications that
you have all this scheme and all these
like referential integrity and you've
done with it then you go and you write
your application on top of that and in
your fire those those those queries
against your data store on the other
side no sequel is a completely different
different different story you start off
with how the data is queried itself you
start off by defining this exact access
patterns that your application uses
first off and then you derive what is
the best way or how your data is
supposed to be structured within your
data store and this is kind of a cute
thing but anyways next section is the
zoo for this to make sense
all right beyond all these concepts
concept conceptual jargon and theory we
would like to kind of survey the most
common different kinds or
classifications or categories of the
different no sequel offerings that are
out there in the market the first thing
I would like to talk about is these cave
key value data stores so what's key
value data stores are basically a big
hash map
or a big Associated associative array
that is a very simple you know data
structure very fast it has a very fast
reads and writes and it has no secondary
indexes right we have featured projects
like Amazon DynamoDB Redis react and and
and such and what you want to use it you
want to use it when your data is not
highly related and all you need is basic
crud for example it's just a big map
right persistable map out there all
right there are a lot of challenges for
the ASCII value data store is that I had
thought you you start having problems if
you have complex queries or if you have
you're trying to persist object graph
that are that are really deep and there
was high degree of relationship between
between them check out the Amazon dynamo
paper it's a very good read it's not
that long featured projects again of
dynamodb react and and Redis the next
family if no sequel technologies is
column based data stores right this is
pretty much your data stored in a table
all right and with the data that
belonged to the same color a column is
physically stored together in a
hardening and in the hard drive in the
hard disk right as opposed to in a
relational database were storing will
end up persistent tuples so like rows
right next to each other this is kind of
different in the sense that your data is
in a table and the data that belongs to
the same column is stored and stored
together right this is the this is great
because you end up saving a lot in
storage for if you're using oracle or my
sequel or anything remember that you
actually have to go and define a very
care of xxx meaning that you have your
allocated 30 bytes for a particular
attribute to be stored as and it doesn't
really matter whether it's known or not
that space is reserved right there for
that particular attribute and that's all
wasted so if you have data that is kind
of sparse you have a bunch of null
there's all this space that is being
wasted if your data is stored in
column-based database you're not gonna
do that because we're stalling columns
together and the nulls just don't don't
don't have to be accounted for at all
and there is no scheme as well so
nothing is enforced these guys are great
again for sports sparse tables and
they're very fast they are very fast
column operations including including
aggregations very fast if you're trying
to do operations like across columns and
find the average whatever it is age you
that data would be like in a fraction of
a second versus the query on a
relational database that would actually
have to go and scan the entire table and
kind of deal with that column separately
you want to use this when you have big
data because it has an excellent
leverage of MapReduce technologies such
as such as Hadoop and such and if you
need compression or you need versioning
this is what you want to do a lot of
these things of built in the champions
of Apache and HBase Cassandra and they
all like leverage Hadoop right there you
might wanna if you're very interested
you might want to check out the Google
Google's big BigTable paper so a lot of
challenges associated with it with these
kinds of data store which is and the
main one is you really have to be very
careful in actually designing your key
space beforehand and it's not trivial at
all because that could really affect
your performance the way you design your
key space you really want to be mindful
of how that data is going to end up in
your cluster whether it's going to be
and you really want it to be distributed
in a way that is even to get to get
optimal performance so if you have login
log information that you're trying to
store or anything of this sort you want
to keep forever I mean this is or like
audit so anything Apache or Cassandra
would be it would be your your your
database to use the next kind is a
document data story alright and these
guys are great
nested structured of house structured
structures of hashes and their values
right so if you have this hierarchy deep
hierarchy of objects you know and like
in many applications out there you might
want to use one of these because at the
end of the day it's pretty much a
document and what you have is which is
you just take the entire the entire
hierarchy and you persist it as JSON and
then you throw it in this database and
you could refer a document can refer to
another document and there is like no
limits in depth you can have a really
deep hierarchy and does not really
affect performance that much it's very
flexible schema which is really good for
a lot of use cases when you actually
when you first start building your
application you don't really know how
your domain on a domain and model is
going to evolve this is very
accommodating and it has well indexed
data has secondary indexes as well which
makes query in query in a breeze works
were really well with op0 impedance
measurement mismatch you have the
hierarchy translates to a document and
vice versa so when using these guys you
want to denormalize as much as possible
no I'm all your data will end up in if
you have messages recovered and threats
of messages and then things on them you
just wanted you normalize them just like
store it in one blob
you want to use this when you don't know
much about the schema of your data you
have no idea how that thing that thing
is going to change a very good example
for this one is a product database we're
talking about products across the
products across different departments
maybe for genes you have you know the
size the size of the genes and all that
kind of stuff but then you move to a
different department and it's like
hardware and you have a completely
different criteria that you have to
worry about as a product so this is very
accommodating in the sense that you can
have your size for the genes and the
whoever and the type of screwdriver on
the on the on the hardware on the
hardware item or like your screwdriver
and it's the same entity as far as your
application as far as that a.document
story is concerned and use this when the
schema is very likely to change the I've
also there's no silver bullet a lot of
challenges associated with this their
query language if you might call it or
like their api's have very complex and
really it has a really hard time with
complex join like queries so it's very
good if everything is denormalized you
Gillian Fitch fetch me this particular
entity where this aggregate where you
know this value equals that value in
wherever it is it will fish it really
quickly but as soon as you have joins
across multiple aggregates it becomes a
problem also if you have self
referencing documents or circular
dependencies becomes a little a little
hard to manage champions of this type is
MongoDB and CouchDB so the last no
sequel kind of data stores that we would
like to talk about is a little different
before we won't compare in these
technologies side by side we said that a
lot of these technologies are built to
run on a cluster not in the case of a
graph database this is actually acid and
it runs on the one machine right but a
graph is perfectly high which is deep
you know the the data structure that
everybody is familiar with familiar with
is perfectly perfect for high
interconnected data and it allows you to
specify explicit relationship between
the data and it would allow you to do
this for graph traversal queries right
there like a friend of a friend and all
of these and query the relationship
between data itself not only the data
it's very flexible and it works really
well with our P there's no impedance
mismatch what
so ever I'm so you want to use this when
your data looks like a graph pretty much
and it requires the graph questions like
graph traversals and and such and and if
you are smart enough that you try this
in another data store try to file a
query in a relational database that
would answer you know third degree
relationship in a reasonable amount of
time try to model that then try to query
that with good performance it would be
very hard and a big challenge this does
not scale well horizontally so the
featured project right now the best one
is knio knio knio for j so back to the
relational to the dinosaurs to the
relational data stores do not want to
leave these guys out of the zoo there
they're there so there is room for them
no sequel it's not about forgetting
about them and not ever using them
because they are a bad technology no
sequel just acknowledges the fact that
there are problems that are better
solved within in a different model other
than the relational model you still want
to use these guys if your data is highly
relational relational because relations
are really hard to not trivial to model
in a lot of those data stores some type
of relationships are and if you have a
need to break data into small pieces and
assembly in different ways like a slice
and dice type of deal so reporting if
you do not know how your data is going
to be queried in the future you want to
structure it in a way that is not biased
toward any kind of query pattern and
when consistence is king when immediate
consistency is important to you this is
what you get what you need to use nobody
else beats it again reporting we said
that the problem with that as we
mentioned is that it does not scale
horizontally I'm sure that you guys are
familiar with all these poor all of
these these products the list of the
product so the question that this
presentation is trying to answer
how do we choose how do we choose so the
questions that you want to ask first is
actually look at your data and see
whether your data whether it's whether
it has a natural structure of some kind
whether your data itself looks like a
graph or looks like a document or looks
like anything out there you want to also
ask yourself the question how your data
is connected to each other you want to
ask yourself the question how's your
data is distributed the distribution of
data is important as well and you want
to ask yourself how much data am I am I
dealing with if you if you're not
dealing with your data can fit into one
machine and we never go beyond that
don't even you know bother introducing
something like that in your in your
enterprise that is associated with a
learning curve that steep you're fine
and also you want to ask yourself about
the access patterns
what is the v2 right ratios because a
lot of these technologies are really
good in beats and really bad in right so
they're really bad in writes really bad
in reads and it could be tweaked to be
to be both ways so this is really
important how are we reading more than
we're writing so we can optimize the
reads and not worry about the rights and
such and another one's whether these
actions access patterns are uniform or
like rent or random to begin with and
you want to also look at cap and decide
which one which two letters are
important and important to you
and and again all these no sequel
technologies would actually allow you to
kind of raise the degree of consistency
depending on certain design certain
access patterns now with the details of
that is kind of out of the scope of this
presentation right there are other
considerations as well which is the
maturity of a solution a lot of these
solutions are just open source projects
that were created less than a year ago
are you really willing to bet the future
of your enterprise and your data on
these things you really want to be
careful on that one the stability of the
project how big is the community around
it is there ongoing development and and
all that maintainability is this
something that you can mint maintain in
your code the maintainability of the
open source project itself durability is
important
whatever your data is actually
persistent persistent or this is just
like some kind of caching another
caching solution out there which a lot
of people would argue that caching
technologies that have been around for a
long time these big shared maps are not
really I'm not including in the no
secret family of technology so cost as
well how much does it cost it's an easy
one one of these things are free tools
this is the winner for Augie BMS I mean
we have all these great tools out there
there is a lack of tools in the no
sequel space mean because they new and
because they're so different than each
other so familiarity as well how many
people you have in your and enterprise
and how familiar they are with either or
the technology you decided to go with
because the learning curve can be can be
very steep sometimes so for fairness
sakes I have to say that the relational
data store did not fail us they actually
perform really well for what these
things have designed designed for they
do a great job for what we designed them
for we actually failed ourselves by
using them as solutions for problems
that weren't designed to to solve to
begin with
but to be fair take any one of these no
sequel technologies and do the same
thing try to fit all these problems
within that model and you will fail the
same way we failed with a relational
database so you can't expect a flathead
screwdriver to you know work really well
and as well as the matching Philips on
actually on actual Philips screw and you
can't expect the cross head screwdriver
to work on a flat one message how it is
anyways so now that we went over the
relational model talked about it really
well then we surveyed all these
relational data all of these no sequel
technologies I want to bring the idea
that have been popularized in the maybe
last couple of years of polyglot
persistence polyglot persistence
acknowledges the fact that enterprise
applications are complex and they
combine complex problems right and the
assumption that we should use one data
store that we have been making all these
years oh you don't even think about it
use Oracle or use whatever it is to
store all the data is just absurd it
doesn't it does not work and you just
can't fit try to fit it all in one thing
and expect no problems no problems to
happen so what polyglot persistent
brings to the table is that is is that
the importance of leverage in multiple
data storage within your application
based on the way the data is used this
is certainly associated with a learning
curve and it's a long-term investment
and it would be more productive in the
long run so you want to take use two or
three or one of along with a relational
database if you need one to
and then and kind of leverage like their
strength to just solve to solve to solve
the different problems that you come
across an example and they have to say
that I'm not making any recommendations
here you know it's just examples then
they just sat down and wrote in like 30
seconds like MongoDB I would put a
product catalog in there in MongoDB
because my schema is more likely to
change and I'm dealing with aggregates I
would use Redis for a shopping cart
because this is temporary data and they
want my leads to be to be super quick
and I'm not writing as much as I'm
reading I want to use dynamodb for
social profile info only use neo4j to
model my sort of my my my social graph
HBase for example for index and my
public feed messages my sequel for
payment and account information
something like Cassandra for audit an
activity log the way that you would use
these technologies together would be
you'd have your profile information in a
relational database you'd have like a
graph to model the relationships between
your users together as a graph of IDs
only user IDs and then you'll be the
responsibility of your application to go
and actually read from the relational
database the entire profile information
and it would actually go and take that
ID or like the two IDs the two profile
IDs and query the graph database to kind
of infer that relationship between two
of those guys within the the graph
database itself so things things like
that a lot of that work of that load
would move way to the application move
to the application layer to be
responsibility to manage or like juggle
all these data stores of course using
all of these things at once it's foolish
you do not want to do this you know day
one wherever it is you want to kind of
pick and choose and be be be be wise
about it now sequel in the cloud no
sequel has become a commodity which
means that all these open source
solutions there are all these offerings
that would
give you fully managed data stores you
don't need to maintain these clusters
you don't need to do anything they
elastically scale the stuff for you
storage is extremely cheap and some of
the features offering out there's Amazon
AWS and he Roku has a bunch of add-ons
for every open source so every no sequel
datastore out there
Cloud Foundry offers a bunch of these
guys in services in a pass so as
promised these questions that were
designed in the abstract to draw you in
there we're gonna try to answer first
one is what does the rise of all these
no sequel databases mean to your
enterprise I'm guessing a lot what is no
sequel to begin with anything that is
non-relational does it mean no sequel it
does not mean anything that it's not
relational does not it's anything that
is not our DBMS could this be just not a
fad I really do not think think think so
I mean we came a long way I don't think
these things are going away and this
should be taken seriously the big one is
it an idea is it a good idea to be the
future of your enterprise on these
exotic technologies and simply abandon
the mature proof in our DBMS it's up to
you
that's no glory but you can how you
again want to be very careful about it
and maybe start with little small
projects here and there to get some
confidence then do bigger things later
how scalable is scalable it's however
much you need it to be
it depends all depends on you it depends
on how much data you're dealing with how
many how it depend of your user base and
and such assuming that I'm sold how do I
choose the one that fits my need the
best tell you if you hire me I don't
know your needs you nobody knows you
needs more than you we're different than
than each other different products
different everything I mean you some
yeah I I don't know is there a middle
ground somewhere
the
is a polyglot persistence which is
leveraging these data stores to solve
the different problems within your
enterprise
what is this polygon a polyglot
persistence I hear about it's the middle
ground so any other questions
sure so classically what we would use
something like mid you need it 2% let me
use like radius or something like that
you use like radius probably or
something like that just Oh like that
kind of information or you know I mean
it probably would be fine fine with any
distributed caching out there as well so
because I mean it's you need fast reads
and you don't write as much any other
questions
okay
so most in most of the open-source
projects actually I could almost say
like all of them you don't really need
that cluster you if you download the
binary it's from their site you install
it in your machine and they kind of
create a one-note cluster you can play
with it all you want as a developer and
put it out there and on the web it's
very it's not like in Iran configure it
on a cluster it's not that challenging
at all so they've been doing a very good
job on that
but I would recommend reading books like
seven databases in seven days or things
like that or any good no sequel book
that would expose you expose you to all
of them that way you don't blindly fall
in love with very first no sequel
technologies that you happen to to play
with
so maybe we can small weekend projects
and to kind of get a sense of what's
really out there yeah
yeah yeah see the problem with these
ones is it's not as easy or as smooth to
take your data from my sequel and then
kind of update it - you know Oracle cuz
you only gotta deal with is this like a
little variation in the other ones what
you end up doing is you write a lot of
code you commit a lot of code and then
you change your domain sometimes and to
fit like one of these guys and moving
away to something that is radically
different
like maybe a document store would be it
would be very easy because it's very
there isn't there isn't much impedance
impedance mismatch if you have a
well-defined model all you do is like
kind of create like another layer of
yeah just here is this thing persistent
as a document so it's not that bad but
I'm not I'm not I'm not sure I'm not
sure how to answer the question I mean
it depends but mostly is the learning
curve how much you paying your
developers to learn and when they learn
you're not guaranteed always the first
time you're always gonna do it wrong
that's like guaranteed so um yeah it's
how much money you can afford to pay
these guys sit there and they play with
no sequel there I guess that's the yeah
what what don't what
so don't get me wrong security is not
optional security is just not done it's
not done by the L DBMS itself security
will be the responsibility of your
application and all of these things that
we give away what happens is we do it on
the application level and then we do it
on the on the LD BMS level you have all
these like oh this one must be a string
must be a number it's bigger than 20 or
must be a positive number and all that
kind of stuff and you have that same
logic duplicated on the application
layer how you convince an enterprise I
don't think you can just walk into your
boss and tell him a throw away that
Oracle RAC that we just spend millions
of dollars on from dead tomorrow then
start using HBase think if you kind of
put on like pick like small projects
here and there you know like you take
your auditing and you throw it like when
HBase and kind of have like a and not
something that does not that does not
serve the core competency or like the
core there is not at the core of your
business and you kind of do it over
there so they can see it running and
kind of learn about its benefits then
you have a case but I mean let's say
your management that's again a
management decision but the cost I mean
it's something I mean it's and matchable
I mean that's Sam that's that's a hard
question
you know it's sending it to business
people it's yeah so yeah yeah I mean a
lot of these open projects actually
would have a section open source
projects that have like eight sections
of how to sell this particular solution
to your manager they literally have like
a two paragraphs so like emails pre-made
emails that would highlight you know the
benefits of this over like the
relational model or whatever it is but
from the manager I wouldn't allow that
maybe like small little projects here
and there until when
the distinct works for us and we know
that people know what they're doing and
then you know send it to them that way I
guess
but money money is important this was
she said yeah yeah with with with their
degree and graph databases all of acid
missed report like fallout 4 full asset
transactions as well but then most of
them you're actually gonna have to do
that yourself on the application layer
so um and also like consistency you can
literally go and then raise the degree
or consistency and say I want this to be
immediately consistent and I'm willing
to wait if you haven't cluster and no
cluster of like 100 nodes I am willing
to wait for all of them to be updated
immediately and not just you know wait
for two days or one day and let the data
store the way that they're only at their
own time
I'm gonna try to kind of say say it
again we have somebody in the back in
the audience that kind of answers your
question saying that if you actually all
hit problems with relational databases
for queries that long work for that you
know that execute and like I don't know
30 seconds whoever it is and kind of do
the same thing in a MongoDB or some
other solution and kind of demonstrate
and that's a proof of concept that it
runs much much much faster your managers
and business person would be you know
interested in in pursuing that further
further and taking it seriously yeah
there are a lot of their offerings and
solutions out there for like dynamo that
would like back up your entire dynamo
database for like 10 bucks a month and
things like that and they're not I mean
they're they're different from each
other but there are solutions like out
there and you know how open-source
projects are a lot of people are like
passionate about it I really want this
thing to work
and um yeah I mean it's it's hard to
answer questions when they are about 10
different products that do not look like
each other and that's what no secrets
know to look exactly like each other I
mean at all the question was about data
recovery and backups right in other
questions
well well a lot of them what you mean
Marian like replication of data across T
yeah a lot of them I mean a lot of them
do when you have on data on a cluster
you want fault tolerance right there I
mean they don't just take your data and
then spread it and then shard it and
then you know evenly and have like a
single point of failure you know
DynamoDB for example at this gossip
protocol that with them you know
replicate your data you know for your
data will end up in like five different
nodes and and it will be distributed
that's why it's eventually consistent
when you're in Japan and you hit this
node it just hasn't gotten the
they'd yet in the next hour it will it
will it will get it but yeah I mean most
of him I don't know one of him that does
not that that doesn't do that with the
exception of graph database in year four
J it's very much like relational but
with a focus on inter related data all
most of him a lot of him actually
written in Java you know 100% I do not
know one of him that does not have a job
is DK that you could use I don't know a
single one of him they all do a lot of
remember reading in Java actually
because they answer a lot of problems
that that are problems in the enterprise
and the language of the enterprise's
happens to be happens to be Java so it's
important that
yeah not really there is this under and
this video is just like a convenient
query language MongoDB has this horrible
way of querying things but there are
other efforts like of on languages that
are simple like hold n equal because a
lot of developers are familiar with it
and it's just much more convenient and
actually doing a series of method calls
I don't think it's it's a bad thing at
all you might check but these things are
very early I mean even this an equal
thing I think maybe the MongoDB people
and a few people in the open-source
community who are like interested in
pursuing this pursuing this further so
but sequel is good everybody knows how
to do it so the problem is like taking
that and doing an implementation across
all of these technologies of the query
language any other questions I guess
thank you all this presentation is
available on youtube so if you go from a
different form another conference if you
go on cut and and Google for like
YouTube for the rise of no sequel
polyglot persistent you'll be able to
watch it from a different conference out
there if you are interested interested a
long wait there with the slides also if
you follow me on twitter we get these
exact slides but they're not much
different than the ones from the other
conference that i did i think a month
ago or a couple of months months ago so
just going to youtube and watch the
video over there if you if you'd like
and thank you guys very much for being
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>