<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From Java Code to Java Heap: Understanding the Memory Usage of Your Application | Coder Coacher - Coaching Coders</title><meta content="From Java Code to Java Heap: Understanding the Memory Usage of Your Application - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From Java Code to Java Heap: Understanding the Memory Usage of Your Application</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FLcXf9pO27w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I'm going to talk to you about
this morning is called from Java code to
Java heap and it's a presentation that
hopefully will apply to anybody that
either writes Java code or managers or
administers applications written in Java
and what it will explain is effectively
how much memory is used when you write
Java code and run it to allocate objects
and how to go about profiling and
analyzing a deployed application to find
out where the memory is being used and
how to optimize it first for a small
memory footprint now if you've been to
any of the other talks from the IBM team
this week you'll have seen this slide
it's our standard legal disclaimer that
effectively says do not trust this man
anything he says may be a lie so don't
hold it against IBM so this is me I've
been working for IBM in IBM's Java
technology center in the UK for for 12
years now it becomes 13 years in three
months I think three three weeks in that
time I spent most of my time working
with clients to to optimize and
successfully deploy applications I also
do our work in what we call consume
ability and serviceability but it's kind
of how to use the product how to
understand it how to debug diagnose
problems and so on
and I now do a little bit of work on our
Java in the cloud platform
so my background is around looking at
applications trying to optimize them for
performance and to understand what they
do at the bottom of my contact details
both my email address my profile on
LinkedIn and my SlideShare account which
is where all of the slides are going so
every presentation I do I publish on
SlideShare now so they'll be there
available for download afterwards so the
goals of this talk is to introduce you
to the anatomy of a Java object the
amount of memory that's used when you do
use the new operator to create a Java
object from that we'll go and look up
from simple objects through to the Java
collections so the Java util collections
like array lists hash
apps hash tables and so on we'll talk
through the different ones talk about
the memory usage of them the access
speeds to help you decide which is the
right collection to use
we'll then look at some problems with
the connection collections as they are
today and then we'll show you how to do
some some analysis and to work out how
to better apply memory usage inside your
application so the agenda is to first
talk about memory management for a java
application so it's not a wrestling
garbage collection it's just how
memories lay down and how it's used when
you start a java application we will
then talk through the anatomy of a Java
object so when you create an object what
memory is allocated and we'll talk about
those Java collections and then I'll
show you how we analyze applications
internally inside IBM and some
improvements that we did as a result of
doing some analysis so memory management
when you start your Java process so you
call Java on the command line or you run
an application server or a java product
the first thing that happens is the
operating system creates a process for
you and it does exactly the same thing
whether you're running c or java or
cobol or fortran because actually most
JVMs are just to see process most JVMs
are actually written in C and then a mix
of assembler and a few other things
thrown in so what you get is a standard
operating system process and the maximum
amount of memory that process can use
depends on the architecture if you're
using 32-bit that Java process can't use
more than four gigabytes worth of memory
no matter how much memory you've got in
your box how much physical Ram you buy
it won't use more than four gigabytes
now if you move to 64-bit that explodes
it goes from four gigabytes to I think
it's 16 exabytes which is just just
massive but if you're running 32-bit
you've got a limitation at four
gigabytes now of that some of it is used
for the operating system itself and the
C language runtime which allows the JVM
to run so some of that four gigabytes is
reserved from the operating system so if
we rep
that and this isn't a specific operating
system it's just you know an idea of
what the memory is useful so say of that
four gigabytes you've actually got three
gigabytes for the Java process the Java
bit of it to run now in the same way
that the operating system needs to use
some of that memory in order to run a si
process some of that memory is used by
the JVM itself so the Java Runtime so we
now have a little bit less than three
gigabytes maybe 2.8 in which to run our
application now of that 2.8 gigabytes
some of that's used for the Java heap so
when you create your java application
you use the minus x MX option to set the
maximum heap size that gets allocated
and what we've got is a little bit left
over and that little bit left over is
actually used by the JVM to do stuff on
behalf of Java code so that section in
there is used for things like sockets if
you create a socket or you create a
thread some os-level resources are
allocated and that goes into that little
bit now we usually refer to that as the
native heap and we add in the allocation
for the JVM itself as the native heap so
it becomes one big chunk that's referred
to as the native heap so of this 4
gigabytes someone's going to be used by
the operating system and that's usually
a fixed size and then you've got maybe
three gigabytes left which is used for
both the native heap and the Java heap
and either of them can have an out of
memory error so if you run out of Java
heap you'll get an out of memory error
if you run out of native heap you'll get
an out of memory error and the way you
saw is the native heap is just by making
or you make the native heap bigger by
making the Java heap smaller that
dividing line in the middle where are we
that the dividing line in the middle is
specified by the minus X MX setting for
the maximum heap size so as a quick
guide of some of what's on the native
heap here's an example of a thread
object so you've created a new thread
using new java.lang thread and what's
happened is we've allocated a thread
object on the Java heap and every thread
object has a runnable and it's part of a
thread group so you've got some Java
objects on the left
but on the right hand side on the native
heap we actually have to create an OS
level thread and that OS level thread is
going to have a stack and that stacks
used for running functions and methods
and you're also going to have a Java
stack with your Java operands the things
you've passed from your one method to
another stored on it and that's that's
in the native heap as well so when you
allocate stuff on the Java heap using
the new operator sometimes you get
things on both sides now the anatomy of
a Java object they start talking about
Java code which is where we really work
so when you create an integer right you
take an int value and you put it into a
new integer object that integer objects
bigger than the int value that it
contains so the question is how much
bigger do you think the integer object
is the knee in value that we've stuck
inside it who thinks they're the same
size anyone okay who thinks the object
is one and a half times the size of the
int value so only a 50% bigger anyone
keep one who thinks it's twice the size
okay that's probably about a quarter he
and who thinks it's three times the size
okay so either people know the answer
here or some people don't like putting
their hands up because the answer is
actually four times the size so there's
a significant growth when you take that
int and you wrap it into an integer
object so what do we actually use that
extra memory for that that four times
multiply so the first bit of it is when
you created your integer object we have
to know that this object is an integer
and we do that by having a pointer to
its class type so we store an identifier
that says this is an integer the next
thing we do is we have a series of flags
the flags tell us whether it's an array
which stores the hash code it stores a
few other things so we have another bit
of data that's that's flags for the
object and tells the Spanish state now
you can synchronize on any object and in
order to be able to synchronize so you
do synchronize in your code in order to
do that we need to be able to
have an indicator that it's synchronized
what's not and who owns it so we have
more data for that and then we add our
integer value which in this case is 10
so here we've got the 4 to 1 multiplier
of we put an int in but we had an extra
3 other bits of data that we had to
store that makes it a Java object now if
you're using a raise it's actually
slightly more we again have to have our
are type so this tells us it's an array
of int we then have our flags again we
have our ability to synchronize on it
then we have the size that says how big
this array is and then we have our
integer object value of 10 so for arrays
there's more header data for your object
so there's a 5 to 1 for a raise and 4 to
1 for object and additionally if you're
using compressed references compressed
oops we've sometimes put padding around
these objects but that's not probably
all that important so that was 32 bit
now if we move to 64 bit what happens so
the difference between 32 and 64 bit
really is the amount of data that you
use to reference one bit of memory to
another bit so this means that when we
have that class pointer that describes
the type of the object whereas for 32
bit they were each 32 bits wide for 64
bit they're each 64 bits wide so the
ratio of your int to the size of the
objects gets significantly bigger at 64
bit and this means if you take an
application that runs on 32 bit and you
move it to 64 bit it's going to use
significantly more memory and of the box
with no changes so we've got this
picture of four Java objects and again
for Java arrays we had four bits of
information that told you about the type
of array there were 32 bits and for 64
bits they now become 64 so you get a
significant growth in memory now that
gives you a ratio of 9 to 1 which is
really painful
so here's the table of the sizes of each
of the data types so an inch there was
32 bits and you can see what it is for
each of them but the important bit is
the row at the bottom objects and this
is the reference from one type of object
to another
here it's 32 for 32 bits and 64 for 64
bits but the Asterix is depending on
whether or not you're using compress
troops or compress references so does
anyone here know what compressed
references are okay and does anybody use
them okay so where we said that there's
there's that big growth where we did
have 32 bits for everything and then it
became 64 bits
what compressed references does is it
tries to represent that data on 64-bit
using own than 32-bit values so he
removes the big growth of where we've
got lots of 32 bits to 64 bits right it
just gets rid of it and it does it using
pointer arithmetic and offsets which
isn't that interesting in all honesty
the fact that it removes that memory
increase is so if you're running 64-bit
basically used compressed references or
compressed hoops there's there's very
little reason not to name so the effect
of this is without compress troops when
you move to 64-bit your memory usage on
average goes up by about 70 percent so
if you were using a one gig process it
now becomes 1.7 gigabytes in size so you
use significantly more RAM when you move
to 64-bit now the introducing of
compressed references were compressed
hoops solves that problem for Java
objects it doesn't solve the problem for
our native heap allocations there's the
JVM itself the C runtime the threads
that we create when you create a thread
object on the native heap that doesn't
get improved by compressed references so
the memory usage for that still grows so
when you move to 64-bit if you turn on
compressed references your Java heap
size usage will not increase by 70% but
you're off
other heat memory usage will still
increase by 60 70 % or so so you still
need more RAM when you move to 64-bit
but if you use compressed references it
greatly reduces the amount okay so we
set for a simple object for an INT the
multiplier was 4 to 1 in 32 bits now the
thing is we don't create objects as
simple as that and the idea of java
programming an object orientated
programming is to do encapsulation and
to do delegation and so basically have
lots of references between objects so if
we take java.lang string as an example
the implementation of a string object is
2 objects you first have your java.lang
string object and that contains a hash
value it contains a count it contains an
offset for where you are inside the
string if you're if you're manipulating
it and it has the value itself and the
value is just a pointer to another
object which is a character array so
when you create a string you're actually
creating two objects so that means that
we in this case have a size ratio of
3.75 to 1 and it would be worse if if we
were holding only a single character
okay so basically when you create an
object usually creating several of them
and each of them introduces its own
overhead so every single object has at
least three words worth of extra
overhead ok so if we now move onto
collections so something that's a bit
more complicated but that everyone uses
inside their applications so as you go
from the top of that list to the bottom
you have decreasing amounts of memory
that is used by the collection in order
to store your data but you also get
decreasing value right so hash maps are
pretty flexible they do fast lookup they
grow etc etc and they're easy to use and
they perform well but they use a lot
more memory than an ArrayList does so if
you use array lists and you can use it
your application you'll use less memory
by doing safe and most people don't
actually think about the memory usage of
the collection that they're they're
using as they create it and that's what
we're going to we're going to look at a
little bit here so the first one in the
list was hash set and hash set is just a
collection that allows you to store
objects with hash codes now the
implementation of a hash set is a hash
set object that points to a hash map so
it's actually just a wrapper around a
hash map and what that wrapper does is
actually give you less function it
restricts what you can do so a hash set
is bigger than a hash map but with less
function so there's kind of an argument
as to why you would want to use one in
the first place but the size of it so
when you create a hash set even if you
don't put any data into it you just do
new hash set you'll get a hash set which
is capable of holding 16 objects so
that's 16 objects which you're not
storing so you've got overhead to do
nothing at this point in time so when
you create it and it's empty it's a
hundred and forty-four bytes in size and
that's just the wrapper around the hash
map so if we look at hash map when you
create a new hash map what you've got is
an implementation of the map interface
with hashing that means that when you
try and access an object with a key the
the access speed is fast it the access
speed shouldn't change as the collection
gets bigger so it's a good collection to
use again it's default size is 16 so you
create it and it's capable of putting 16
entries into it even if you aren't
putting any in if even if you're only
ever going to put two entries in it it
still creates it 16 for you it's empty
sizes 128 bytes and every time you put
an entry into it what's created for you
when you do a put is a new hash map
entry so you're not just putting your
objects into it there's management
around those objects as well and this is
what a hash map entry looks like so the
hash map entry has four objects in there
one is your key and one is your valley
so that's what you usually put into a
hash map when you do a put but has also
got a hash key and it's got a next
pointer so every time you put something
into a hash map it's not just your date
that's being put in there there's some
wrapper around that so it uses more
memory so the effect of this is if you
create a hash map to put 10,000 entries
into it but is completely empty at this
point in time it uses 360 Ches worth of
data so that's a third of a megabyte to
do nothing at this point in time okay
next if we look at hash table so hash
table is virtually the same as hash map
in terms of its memory usage so the
layout is the same the difference is
that it's default size when you create
it is 11 entries so whereas hash map and
hash set are 16 hash table is 11
apart from that basically it's the same
except hash tables are synchronized by
default so people often use hash tables
as a safe collection because it's kind
of like a hash map but because it's
synchronized you don't have to worry
about concurrency issues the downside of
course is is synchronized even if you
don't want it to be which can cause you
some performance problems but again you
put a new entry into a hash table and
what you get is a new hash table entry
object with a hash value your key you're
next and your value so again if you had
a 10,000 entry hash table it's going to
be using about a third of a megabyte and
this is just for the collection it
doesn't include any data that you're
storing inside it next as we go down
down is linked lists so a linked list is
an implementation of you know the list
interface and linked lists are a little
bit better in that their default size is
1 when you put an object into the linked
list
it grows individually as you put more
data into it and it's its memory
overhead is quite a bit lower as a
result it's empty sizes only 48 bytes
when you put a new entry into it you get
a new linked list entry object in the
same way you had a hashmap entry object
to a hash table entry object and that's
what it looks like so because you don't
have a key value pair because there's no
key it uses a little bit less memory
per entry than the hashmap on the hash
table and this means that if you had a
ten thousand entry link list it's only
using 240 K so link lists are smaller
than hash tables and hash maps and hash
sets next is a ray list so an array list
is just a fictive Li a growable array so
the implementation of an ArrayList is an
array of object so what you've got is an
ArrayList object and the array that it
manages the default size for ArrayList
is 10 so so far we've had hash set and
hash map with a default size of 16 hash
table with default size of 11 and
ArrayList with a default size of 10 I
would love to know where they got these
from what was the decision I think it
was kind of you know maybe a bingo
machine and they pressed a button but
yet so ArrayList start off with 10-foot
for whatever reason at the end the empty
size is much smaller because it's just a
single object and an array and one of
the advantages is when you put something
into an ArrayList you don't have to
create an entry object which has memory
of its own size as well you just put it
directly into the array so this means if
you had a 10,000 entry array it only
uses 40k is worth the data whereas hash
table hash map and so on used 360 K and
linked lists used 240 K so it's smaller
and there are other things which kind of
behavior like collections even though
they're not part of the Java util
collection package and one of these is
string buffers so when you create a
string buffer it's kind of like an
ArrayList it's a management around an
array of characters so when you create a
string buffer it's default sizes 16
entries 16 characters is what you can
put into it and therefore if you create
one with nothing in it it's empty sizes
72 bytes and it's just 24 bytes worth of
overhead over the data inside it so it's
it's a really quite cheap so there's the
summary of the collections
basically as you go down the list you
use less memory in order to store data
but as I said earlier on you lose
function so you get more function in our
hash map than you do in a string buffer
or in an ArrayList so why would you use
the hash collections if they're you
significantly larger and what it comes
down to is their speed of access so the
idea of the hash collections is as the
collection grows the amount of time it
takes you to find an object in that hash
collection should not change so the
access time the update time the modify
time remains constant regardless of the
size of the collection for the non hash
type collections you may in the case of
a linked list have to iterate down the
linked list in order to find your object
so as the collection gets bigger your
access time grows for array lists it can
be the same it all depends whether
you're storing an index into the array
list or not so it actually may well be
that using the hash collections is the
right thing for you to do but kind of if
you care about memory you want to know
it's the right collection for you to be
using in this case and you want to have
decided whether you're using a hash
table because you want synchronization
rather than a hash map what we find
by analyzing applications is that
developers just seem to have a habits
they don't sit down and think which
collection should I use they just create
the same collection they always do and
when you study code you can always work
out who wrote it based on the collection
types that are being used so the next
problem is empty space I said I've kind
of been highlighting that each of the
collections start off with a default
default different default size so we had
16 we had 11 we had 10 then we had 16
again so if we look at what happens when
we create a new string buffer and put my
string into it is we've created our
string buffer object and then we have
this this points to the the character
array which we said by default is 16
bytes in size so 16 characters inside so
when we put my string into it it only
uses the bottom parts of it that's
required to store the string the rest of
its wasted
so what we've got is nine characters
being stored in 16 slots so we've got
seven that are being wasted
that's just memory that's being
allocated that you're not using for
anything so that costing costs you an
extra hundred and twelve bytes and it
actually gets worse because string
buffers are designed so that you can
build strings and add more data to it so
if we look at the same thing what we've
done is we've started off of a string
buffer with my string in it and this is
using nine of our 16 characters so if I
start adding more characters into it I'm
building a bigger string so if I put in
my string of and then I want to add text
so I want to have my string of text what
happens is as I put the extra individual
characters in once I've got as far as
the X and my string buffer is full but I
still want to add an extra character
into it the string buffer grows for you
so that you can put the extra data in
and it does this by doubling in size so
in order to put that extra single
character into the string buffer I had
to allocate another 16 entries so now
I've got 15 spare entries whereas
previously I had what was it 6 so
basically most collections grow and
expand by doubling that the algorithm
actually changes but depending on the
collection but most of them double in
size now here it's not really that a
problem we're saying we've got 15 spare
characters and let's face it that's not
a lot of memory but if you consider the
case where what you're doing is sending
large soap messages around and you've
gotten seven and a half Meg soap object
and you make it 8.1 Megan sighs when I
using a 16 Meg string in order to store
just over eight makes worth of data so
you're wasting basically eight Meg and
this is the problem with the way a
expands by doubling is for large
collections suddenly you can go to
having a huge amount of wasted memory so
the additional 16 character entries for
the single character was in this case a
waste so if we update the summary
we've got a range of different default
sizes we've got a range of different
overheads depending on function and
we've got again a random selection of
the ways that the collections expand in
general they work on a owner times two
linked lists are always the right size
because we allocate and remove them as
you allocate or add or remove objects
but yeah in general they double in size
and this causes that problem of for
large collections if you just want to
add a small number of extra entries it
will double in size and use twice as
much memory for the collection itself so
through the whole part of this
presentation I've been talking about
very small amounts of memory right we
said that a 10,000 entry hash map uses
360 K for the hash map and that you can
get down to 40 K by using an ArrayList
so that difference is only 320 K for a
very large collection and let's face it
for most se or EE applications you've
got huge amounts of Java heap available
to you I guess as a result of the
keynotes on Sunday there's a lot more
talk around embedded so memory will
become important again but it does sound
like being able to save a third of a
megabyte here and there for our big
caches isn't really that important but
it turns out that if you actually
analyze an application there are large
numbers of collections in use in any
given application possibly a
surprisingly large number so running a
simple test against the IBM WebSphere
application server it happened to be
version 7 and plants by WebSphere which
is a standard demo that's shipped with
it so we ran a load test using just a
simple load runner with five rotating
users so five people connected at the
same time but different user IDs being
used for it and so on and we ran that
for a couple of hours and then we took
we took a heap dump and decided to have
a look at the collections in memory and
this is what we saw so first of all
there were two hundred and sixty two
thousand two hundred and thirty eight
instances of hashed
so the first thing that tells me is
we've definitely got at least one guy
who likes hash tables but of because
there's over quarter of a million hash
tables the memory usage of just the hash
table and the hash table entries not the
data that it's storing is actually 26
and a half megabytes so that's 26 Meg
just for the hash tables themselves next
we cache map we actually have 19 and a
half thousand of those and they use
twelve point six megabytes worth of
memory just for the week hash maps again
not the data that's stored inside them
the collection itself then we've got
some hash maps we've got over 10,000 of
those holding on to 2.3 megabytes then
we've got a ray list again almost 10,000
array lists because they're much much
smaller the fact that we've got 10,000
of them means they're actually not
holding on to a lot of memory they don't
use much memory themselves we've got
hash sets not many of them but still one
and a half thousand using a megabyte
we've got a number of vectors we've got
linked lists we've got tree maps so
we've got a range of different
collections and a large number of
instances of each and some of them are
holding on to significant amounts of
memory so in total we've got over
300,000 different collections live on
the Java heap and they're using 42 point
nine megabytes worth of memory just for
the bits of the collections themselves
not the data that's inside it so that
was when we were running an application
with five parallel users and the memory
usage of the application was just over
200 mega so it's not a big application
compared to some of the things that you
see being deployed at your running j2ee
or the like and of that 206 megabytes
the fact the 42.9 is being used for
collections means that 16 percent of the
Java heap is being used just for
collection objects so it may be we've
got some scope to save some memory in
there but the question is one you know
how do you find out what collections
you've got and what number they are and
how much
they're using and - how do I find out
whether I've got empty space in my
collections whether they are empty
whether they've been miss sized whether
someone's using the wrong collection
type you know how do you start digging
into to find out whether you've got
problems well the answer is there's a
toolkit memory analyzer
does anybody know memory analyzer will
use it okay that's good news so memory
analyzer is an eclipse open source
project it's been around for four or
five years now
IBM does have its own version where we
take the open source and we've extended
it with extra function to know about IBM
products so you can get an IBM version
of this as well but one of the things
that memory analyzer does is it lets you
take an image of your application so a
system dump or a heap dump and it loads
it in and it knows about every single
Java object that's that's on the Java
heap the values that are inside them
what they're being used for the
relationship between them they do memory
analysis and so on so it's fully capable
of finding all of your collections and
one of the things it will let you do is
check the fill ratio so what percentage
of the collection size is actually being
used to store useful data so when you
open your dump and you select collection
fill ratio it asks you whether you want
to limit your search to certain types of
collections and that's just a regular
expression so I can use Java util
hash-table because I know um seems to
have some hash table guys and once I've
run that it gives me a simple breakdown
of some bands of ratio and how many
collections there are inside it so that
top band is objects with a fill ratio of
less than or equal to zero so basically
empty hash tables and we have 127,000
empty hash tables storing no data so the
chances here are that we've created
something that just doesn't get used or
doesn't get used in our existing use
case so it's great that has told us that
I've got some kind of problem that I'm
losing you know ten megabytes worth of
memory
to 127,000 s empty hash tables but I
kind of need to know who created them
and what they're being used for so can I
relate it back to the code well what it
will do is it will let you list all of
these objects and you can choose to list
them to find out what code or what other
objects are referencing it or what it
references so inbound or outbound
references so if we look at the incoming
references to find out what's holding
onto this hash table we find in this
case it's being used for session data so
the plants by Web Store application is
asked for these hash tables in order to
store some session data for its
application so it's it's kind of a
problem in plants by WebSphere for the
way that we we've been running that
sample demo so now I know that I look to
options I could either say well let's
not create the hash table until someone
wants to put something into it why have
a hash table if it's got no put to put
into it or I could say well I can
minimize some of the memory usage by
making its default size smaller rather
than having eleven entries for a hash
table by default if there's a good
chance I'm not going to use it I could
make it a lot smaller by default so when
we went through the full analysis of all
of those collections in the in our
sample what we found was there was a
great tendency for them to be empty
almost 50% of the hash tables were empty
when we looked at we cache map basically
they were all empty so they're not being
used hash map was again a high for empty
across the board it was kind of like 50%
of the collections that were allocated
and live in memory weren't being used to
store any data so they were just
effectively wasting memory for us so we
then went to see how we could optimize
this now one of the easiest places to
optimize is in the application itself
right you know what data you're putting
into it you know whether your collection
is likely to store for entries or 40,000
entries and you can create them to be
the right size to begin with but we were
trying to do stuff in the JDK lab
as well so one of the things we looked
up there was we cache map and we cache
map is actually quite a large object
when it's empty and one of the reasons
for this is in order to do the
processing of the weak hash map to deal
with the weak references in there it has
a weak reference key and that weak
reference queue is there to process the
objects that you've put into the
collection so we don't actually need
that weak reference key until there's an
object it's being stored by the weak
hash map so what we did was we took our
own advice and we said well let's
allocate that weak reference queue
lazily all right there's only created
when we've put something into our
collection to begin with so by doing
that we actually really remove something
like five hundred and sixty bytes per
week hash map that's empty and the
effect of this is we save ten point nine
megabytes out of those empty we cache
maps just by allocating the the weak
reference queue lazily so there's a
number of techniques that you can apply
to try and minimize your memory usage
right the first of them is this this
concept of doing lazy allocation of
collections right rather than just
creating a collection in the
initialization of your object you can
wait and only create it when you want to
put the first item into the collection
there's no point having a collection
with nothing in it so the moment you
want to put the first item into it
that's when you should really be
creating the collection to create lazily
secondly you do actually have the option
that if you're only restoring a single
object of not putting in a collection at
all you could just store the object the
use case flatty's is somewhat smaller
the third is sizing collections
correctly so the fact that the default
is 16 or 11 or 10 may not be right for
your application so if you're only ever
going to store four objects inside a
collection you can create it of size 4
by all of the constructors let you pass
a number into the into the constructor
that says this is the size of collection
that I want another one is to try and
avoid the expansion of collections so
avoid this doubling particularly for
large collections so you don't want to
be storing 17 Meg's worth of string in
32 Meg's worth of string buffer and
finally and this isn't what I've covered
but I'll explain now is we said when we
get to the limit of collections usage we
put one more item in it that collection
doubles in size now when you then remove
that extra item so you're back down into
usage that you hat you could have done
previously that collection doesn't
shrink my collections only expand so
they will always stay at the largest
size that they've ever been so this
means that if you did have a one-off
allocation that caused you to have a 32
Meg string buffer and you only ever use
that same string buffer it's going to
stay at that size it won't shrink again
later so if you know you have this kind
of problem one of the things you can do
is create a new collection that's
smaller copy the data even throw away
the old one so if you really do want to
minimize your memory footprint the fact
that shrinkage doesn't happen is a
concern and it's something that you can
implement in your own code if you want
to so with that the summary is the
significant overhead to your data right
when you put a bit of data that you care
about into a Java object gets bigger you
put it into a collection it gets bigger
again most applications are filled up
with the same type of collection because
it's just what the programmer is used to
using you can be more intelligent about
that you can select the right collection
by doing so you drop the footprint a
little then making sure that you create
them to be the same size you make sure
that they don't expand too rapidly
you worry about the fact that they may
expand and not shrink and you can
replace them at smaller sizes if you
care about these things and you do it
then you will drop the footprint of your
application you will be more memory
efficient and usually smaller
applications run faster and in order to
do this the tooling that you can use is
the Eclipse memory analyzer tool which
will do collection analysis for you and
allow you to identify empty
or collections where you only have a
small number of entries for the size of
them okay and was that with through to
any questions so there is an article for
this is on developerworks and the slides
are on slide chat yes so the question is
when's the best time to start doing
memory analysis and the answer is the
same time it is to do performance
analysis a lot of people leave their
performance work too late in the cycle
and suddenly you realize that in order
to make my application run the faster I
have to make architectural changes and I
should have done that a long time ago
and it's the same for optimizing for
memory where the optimizing performance
or memory the earlier you start in the
cycle the less risk there is to your
product yes
so the question is are there any
third-party implementations of
collection classes which are good the
answer is yes there's a few packages
from Apache which are very good but in
general I mean I would like for us to go
back and revisit the base collections as
they are inside Java SE they've been the
same way for well 15 years and the fact
that though default sizes don't seem to
make sense
the expansion algorithm being 2 x is
great for small collections but the
moment you're getting up to 16 32 Meg's
collections a doubling really doesn't
make any sense
there's no option to plug in the
expansion algorithm that you want or to
have it adaptive based on size and I
think those would be improvement and I
think being able to shrink as well as
expand would be great as well yes
Oh
so the question is when using compressed
references it's possible to get a native
out of memory error even though for
64-bit you have vast amounts of address
ability and the answer that is yes it is
kind of one off corner case and it's to
do with how the operating system tries
to get all of the memory in under the
four gig line I think certainly on the
IBM side we've made enough changes now
so there automatically moves the heap to
the right place to avoid it we certainly
had a couple of issues early on it was
corner cases around where the boundary
of the heap was yes yes okay so shallow
heap is the size of that specific object
and retained heap is the sum of that
object and anything else it references
so it's it's the sum of the tree under
it is retained and shallow is that
individual object okay yes
so the question is if you're running
32-bit on a 64-bit operating system are
you still restricted to using four gigs
worth of memory how's that laid out when
you have multiple JVM s so the answer is
you're still restricted to four
gigabytes right you can only use four
gigabytes for a 32-bit process
regardless whether it's on a 64-bit
machine regards and how much RAM you've
got now the layout of that in physical
memory isn't just as a slab so the
operating system basically breaks up
your physical memory into pages whether
they're 4 K 16 K 64 K 6 megabytes you
can add get 16 gigabyte pages but it
breaks it into pages and some of those
will be used for some applications some
of them will be used for others there's
a level of abstraction that means you
can't really map chunks of physical
memory to the application that uses them
but each application has visibility of
its own 4 gigabytes so yeah everyone
sees their own 4 gigabytes and they're
kind of interspersed in each other on
physical memory
so yes if you've got 8 gigabytes and
you've got 2 JVM is using 4 gigabytes
each they will use 8 gigabytes total
memory yes yes
so the question is you know it's easy to
size the collection to have the right
capacity if you know what's going to
happen inside your application but if
you're I guess a framework with someone
else using your code you don't know what
you're going to use so it that's where
the collections are today right they
will dynamically grow and shrink for you
your only option there is to kind of
profile the different use cases
sometimes you do find that if our codes
used in this code path we use four
entries here we use 16 and you can try
and start off with the right size based
on who's calling you that is an option
but it's painful but that's why I think
there should be improvements to the
collections themselves so that it can
actually be more intelligent and not
just double in size which really doesn't
make a lot of sense but yeah it's just a
problem so the question is is there any
active work to change the expansion
algorithms there's there's been some
discussion but I don't think anyone is
interested enough at the moment I mean
it is true that you could just subclass
the collection yourself override ensure
capacity which is the expansion
algorithm and you do it yourself but at
the moment whilst we would like to do in
essayed there's no there's no direct
work being done
so the question is is there any work
being done in garbage collectors to try
and help with this yes no I mean there's
there's some research projects which are
going on in a couple of universities
which are looking at how to do
allocation profiling to work out how to
allocate better in terms of garbage
collection the simple answer really is
no because this is all Java level right
you have a java object but that's
telling it how much memory it needs to
use so the garbage collectors - lower
level to be able to influence that some
work that has been done in garbage
collections around rail it's so the idea
is whilst your java application thinks
that you have an array of size 16
megabytes what the garbage collect has
done for you is it's broken it down into
small segments so it's not one large
object it's a number of small objects
but that doesn't help with memory
efficiency it helps with avoiding the
need to do compactions and the like
because of fragmentation yes
sorry
a treemap so yes I didn't cover tree
maps and tree maps are actually quite
efficient because they grow similar to a
linked list but they don't have the same
linear growth of access time so tree
maps kind of sit between linked lists
and the hash collections both in terms
of lookup speed and memory footprint yes
so the question is how to compress
references work so the basic concept is
for a 32-bit application the reason why
the address ability is 4 gigabytes is
that a 32-bit pointer gives you numbers
between 0 &amp;amp; 4 gigabytes so it's the size
of your pointer now what they do for
compressed references is rely on the
fact that every object starts on an 8
byte boundary so we don't need to be
able to access everywhere in memory for
a Java object only the start of 8 byte
boundaries so that means we can actually
ignore the bottom the bottom 7 bits
right and therefore with a 32-bit
pointer knowing that we only care about
slots on which objects can start that
gives us 32 gigs worth of address
ability so compress references work for
the 64 bit heaps up to 32 gig only
internal compress references you just
add for IBM - X compressed refs on the
command line for all rakul I think it's
minus xx compressed eeap's yes
so the question is if I'm moving from
32-bit to 64-bit what's the advice well
the first bit of advice if you can do it
is run 32-bit on 64 because at that
point your memory doesn't grow and you
don't have the other effect moving
64-bit which is your application
actually runs slightly slower at 64-bit
applications are slower than 32-bit so
if you can stay on 32-bit on a 64-bit
platform do it if you can't then what
you need to do is make sure you have
more RAM available so if you had a
32-bit machine that's got 12 gig of ram
in it and you move to a 64-bit machine
with 12 get your ram in it you're going
to have problems so you need to make
sure you've got at least 50% more RAM
probably to be safe double it when you
move to 64-bit yes so the question is is
any reason not to use compressed
references and I think the fair aunts
that is no I mean the only reason why
compressed references were not enabled
by default for for Oracle and hotspot
IBM did is stability right its new as I
said IBM out of the box straight away
turned it on by default particularly
under the WebSphere application server
for heaps up to 32 gig in size Oracle
made it optional and I believe compress
troops became fully supported about a
year ago 18 months ago will say but I
don't quote me on that
so is
so the question is does JDK 5 support
compressed references and I think the
answer is no certainly for IBM our first
release was was Java 6 that had support
for it and I believe it's the same for
the hotspot yes
so the question is memory analyzer m80
only knows about the Java memory usage
is there anything we'll tell you about
the the off java hit the native memory
usage and the answer is yes but
currently you have to be running the IBM
jdk so when you get a heap dump on
hotspot it is just an image of the Java
heap what the IBM approaches to this is
to take a full OS level system dump and
so on Linux is or ax as a core file and
windows it's a mini dump and you can
generate those non-destructively so it
doesn't crash your application it just
gives you the full image so for that we
have access to all of the native stuff
and we've actually put a couple of
things in our version of memory analyzer
which will access the native stuff but
we literally six weeks ago released a
new tool called
ie D de Interactive diagnostic dump
Explorer and that does exactly that it
gives you access to both the native and
the Java site and the relationship
between the T but again it's it's it's
IBM JVM is only I'm afraid
since so the question is but that gives
you both it gives you native and Java is
the way to have only native the answers
now the dots gives you both so yes you
pay the extra cost of writing the Java
heap information as well but core files
typically compress down to 20% of their
original size so they do compress well
in terms of size on disk yes the
question is is there a difference
between running in a virtual machine or
in a normal oh s and the answer is no in
terms of all of this your virtualization
layer hides that from you the
virtualization layer says that you still
can only see 4 gigabytes worth of memory
it just may not be giving you 4
gigabytes under the covers yes
Oh
and
No
I
so so the the question or the statement
is that you know effectively collection
selection is a trade-off between memory
and performance and yes you know the
more performant collections are the ones
that cost you more memory and you're
right some of what I covered is a
simplification you know you say you use
a hash collection and it's a constant
access time and that depends on on your
your load factor in your entropy so yeah
it's a simplification but it's it's
approximately right any other questions
okay then thank you for attending as I
said the slides are on SlideShare</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>