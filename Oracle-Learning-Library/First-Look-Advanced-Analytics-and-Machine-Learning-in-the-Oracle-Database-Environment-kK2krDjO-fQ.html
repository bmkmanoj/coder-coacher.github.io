<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>First Look   Advanced Analytics and Machine Learning in the Oracle Database Environment | Coder Coacher - Coaching Coders</title><meta content="First Look   Advanced Analytics and Machine Learning in the Oracle Database Environment - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>First Look   Advanced Analytics and Machine Learning in the Oracle Database Environment</b></h2><h5 class="post__date">2017-07-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kK2krDjO-fQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to this first look
advanced analytics and machine learning
in the Oracle database environment my
name is Mark Warnock a director in the
Oracle advanced analytics and machine
learning group and we're going to be
taking a tour of the Oracle advanced
analytics option to the database
let's start off by looking at some data
analytic pain points many of these we've
heard from customers at enterprises
around the globe many of our customers
encounter all of these so we'll start
off by looking at it takes too long to
get my data or to get the right data in
this case it's the interaction with
perhaps IT or database administrators to
request a dump of data that is needed
for doing analysis this causes delay in
requesting and getting responses to that
data but also then having to pass that
data through ad hoc means perhaps CSV
files the second pain point is that I
can't analyze all of my data or it has
to be sampled and with this we're
talking about not being able to produce
the most accurate of results because
some of the data points may be missing
especially if they're rare the third
item putting analytics and predictive
models and results into production is ad
hoc and complex this is a challenge for
a lot of companies because each time
they do a data science project they end
up having to re-implement some key
functionality to enable invoking
analytic engines extracting data or
putting results back into production
systems so it's ad hoc it requires a lot
of testing and it adds to the overall
complexity of the solution the fourth
item is recoding predictive models
whether they be are or from other tools
and doing this in languages such as
sequel C or Java that can perhaps be
more readily included in applications or
dashboards but doing this recoding takes
time and it's error-prone so you pay a
data scientist to come up with a
solution but then you end up
reimplemented that solution in some
other environment the next one is our
company is concerned about data security
backup and recovery this we hear almost
universally that having to info
okay separate servers extract data and
you don't necessarily know where the
flat files are going or who has access
or having to invent new ways of doing
backup to file systems or encountering
recovery issues this is certainly a
major concern among enterprises and the
last item we need to build thousands of
models fast to meet business objectives
this one is becoming more prevalent as
enterprises are seeing the value in
building many models perhaps not
thousands maybe tens maybe hundreds or
others are actually building millions
where they're trying to have models per
customer for example and this allows
them to have very precise analytics on
individual entities I did a blog series
at blogspot oracle comm /r and you can
see it at the URL posted here with that
in mind let's take a look at some of the
Oracle advanced analytics
differentiators first one is this
ability to work directly with data in
the Oracle database and to do we want to
eliminate the need to request extracts
from IT or DBAs providing immediate
access to the data whether it be in
Oracle database or I do in addition we
want to process data where they reside
if we can invoke queries on the systems
that are housing that data that are
optimized to do that we can have
significant gains in performance but in
addition the eliminating this need to
move data out of one environment into
another in order to process it can have
dramatic gains and we'll see some
examples of that later
the second scalability and performance
is that we have parallel and distributed
machine learning algorithms that scale
to work on big data in the Oracle
database and with that you can leverage
powerful engineered systems such as
Oracle Exadata or the big data appliance
to build models on up to say billions of
rows of data or be able to
build millions of models in parallel
ease of deployment is certainly key when
you get finished producing a script say
an R or in sequel you want to be able to
immediately put that into production
with no need to recode in addition you
want to have production quality
infrastructure that eliminates the need
to have custom plumbing or additional
complexity in your environment in order
to enable analytic applications and
lastly is process support you'd like to
take advantage of data security backup
and recovery that you already have in
place for say your Oracle database take
advantage of the existing processes and
then to store access manage and track
your analytic objects the models of
scripts workflows data in the Oracle
database itself rather than having to
maintain those say in a file system or
in some other environment from a
management or business decision-maker
perspective there are three concerns
that we can elevate even above the
differentiators we just spoke about the
first is faster time to productivity we
want it to be simpler for users to make
progress so we have automated modeling
tools available we want it to be
familiar in terms of the skill sets that
users have whether the R or sequel and
they can take advantage of all the
benefits that those ecosystems have to
offer the second one is faster time to
results so by eliminating data movement
by giving direct access to data and not
having to request extracts of data we
can produce results faster in addition
this ability to leverage parallel
algorithms that take advantage of say
Oracle engineered systems also helps
produce results much faster and to allow
making decisions based on them much
easier and the last one is faster
time-to-market which goes hand-in-hand
which with the previous item you want to
also
your results into production faster not
having to re-implement the plumbing or
the components necessary to get your
system up and running in a production
application is key you want to be able
to create change and update your models
quickly and then immediately deploy them
in this next slide we're looking at
Oracle's advanced analytics capabilities
in general to give you the broader
picture of what is available with
respect to advanced analytics at Oracle
so we talked about having multiple
interfaces across platforms so
supporting sequel our graphical user
interface dashboards and applications so
we support users at many different
levels certainly our programmers that
can take advantage of say the our client
interface and certain third-party IDs
business and data analysts where they're
able to leverage the graphical user
interface Oracle data miner with not
necessarily comfortable with coding but
we can give them a visual way of doing
analytics with a lot of automation built
in business analysts and managers need
access to the results of data so in the
form of dashboard tools such as OBIEE or
data visualization and finally the
domain end users that can take advantage
of advanced analytics result in
applications so these are tailored to
the needs of specific types of users but
they are leveraging the analytics that
are in the underlying platforms speaking
of platforms we see on the right here
the Oracle database Enterprise Edition
has the Oracle advanced analytics option
to the database and that can run on
typical platforms that are supported by
Oracle but also of course on exadata
taking advantage of the RAM and CPU
capabilities there in addition on the
Hadoop front we have Oracle our advanced
analytics for Hadoop which also enables
parallel distributed algorithms and a
number of other features that we won't
be going into
in this particular presentation all of
these are available on the Oracle cloud
and we'll say more about that later so
let's dive into the Oracle advanced
analytics components as mentioned we
have the Oracle database Enterprise
Edition with the Oracle advanced
analytics option being available in that
environment diving into the advanced
analytics option we see that there are
two key components the Oracle data
mining and the Oracle are Enterprise
components and Oracle our enterprise
relies also on an AR engine we have the
Oracle our distribution which is
Oracle's own redistribution of open
source R which is supported by Oracle or
you can also use open source R to
interact with the advanced analytics
option you have a few ways one is the
Oracle data miner user interface that's
available through sequel developer I
will be touching on that more later in
this presentation and also standard our
open source ID es such as our studio
that allow you to interact directly with
the Oracle our enterprise component
across the various interfaces whether
sequel AR or the Oracle data miner
user interface we have a range of in
database machine learning algorithms
classification regression clustering
predictive queries feature extraction
etc and all of these are available for
execution at the database server side
taking advantage of the optimizations
within the Oracle database we should
note that many of these algorithms are
also supporting up what we're calling
partitioned models I'll discuss that
more a little bit later as well as the
ability to incorporate text to enable
text mining so for our first look let's
take a look at Oracle data miner user
interface this is intended to be an easy
to use tool for what we'll call citizen
data scientists or those that are
capable of doing data science type work
however might not be as comfortable
with programming or the very specific
details of individual algorithms whereas
if they have a tool that can provide
automated assistance for many of the
data science steps they can actually get
quite far Oracle data miner is a sequel
developer extension so it's built right
into the sequel developer tool it allows
users to define analytical methodologies
and these can be shared with other users
by exporting what we refer to as
workflows it also has a workflow API for
programmatic control and execution of
workflows if that's desired and when you
get finished building your analytical
workflow it actually will generate
sequel code that allows you to
immediately deploy it in the Oracle
database as we've labeled here you see
the canvas where we can create the
analytical workflows by dragging and
dropping nodes from the pallet of tools
and connecting them there's also the
sequel worksheet if you wish to work
directly in sequel model viewers that
allow you to inspect the various types
of models whether clustering models
decision trees regression models and the
like so the sequel developer interface
for Oracle advanced analytics supports
the data preparation transformations and
in database mining functions when you
craft a workflow and you execute it the
nodes are actually corresponding to
sequel or PL sequel statements that are
then executed in the database as
illustrated here Oracle data miner also
provides a variety of visualizations for
the data science results here we're
looking at a lift chart here we're
looking at the coefficients associated
with the model and a decision tree
viewer as well that shows the
relationships between the nodes in the
decision tree and various statistics
associated with them for clustering
models
we can view the tree hierarchy as our
k-means and oak cluster models are
hierarchical clustering models but we
can also see details about each cluster
in the tree another cluster
visualization is to compare two clusters
to see how does one differ from another
oracle data miner also has some built-in
visualizations to allow you to quickly
inspect whether through bar charts or
box and whisker charts or other
visualizations continuing our first look
we're going to move to the oracle data
mining sequel api here's an example that
is using automated in database data
preparation as well as scoring as we'll
see in a moment so at the top we see
that we're using the PL sequel function
from the DBMS data mining package drop
model and then we're going to create the
settings tables notice here as claims
set and saying that we want to use a
support vector machine algorithm and we
want to use auto data preparation by
turning prep Auto to on from there we
can build our claims model by passing in
the claim settings themselves and using
the DBMS data mining package function
create model at the bottom you see what
you would do to invoke the model to do
scoring so we can use the prediction
probability sequel function passing in
the claims model that was produced in
the previous statement to get a ranking
of the top five fraud cases and on the
right-hand side you see what the output
of this query would look like the model
exists in the database and we can use
standard sequel with the Oracle data
mining enhanced functions to allow you
to score models and to produce results
in this next sequence of slides we're
going to look at some features specific
to Oracle data mining
the first one is automatic data
preparation or we
prefer to is ADP it allows users to get
results faster and more easily because
it supports automatic variable or
predictor or column depending on how you
want to call them transformations
it'll take into account the specific
algorithm and the data characteristics
that are necessary to prepare that data
for that algorithm different algorithms
have different data preparation
requirements for example on bidding we
look at naive Bayes and decision tree
and they use the supervised binning that
is provided through automatic data
preparation to generate bins prior to
model building similarly for a
normalization we have support vector
machine and generalized linear models
and they need the normalization
transformations before model building
can proceed usually a DP is the simplest
approach for users to get a model
quickly and they can assess the quality
of that model and decide if they need
more control over the data preparation
for feeding that model itself and if
they do they simply turn off data
preparation and then build the model one
of the newer features of Oracle data
mining is this notion of partition
models where users can get better
accuracy through building multiple more
targeted models but they're managed and
used as one so in this case the user is
specifying that they would like to build
a partition model and indicate which
columns should be used for partitioning
the data and then the Oracle data mining
will essentially build an ensemble model
where each model consists of multiple
sub models built for each partition of
the data simplified scoring is also
enabled where the user simply provides
the top level model and then the
underlying system will choose the
appropriate sub-model based on the
values found in the row to be scored
visually this looks is depicted here
where we have Oracle database the table
we specify the partition columns and
behind the scenes the data is
partitioned applied through the
algorithm and we get this top level
model that is available that we can then
use for scoring
Oracle data mining also supports text
analytics or text mining it uses Oracle
text the native capability of Oracle
database to automatically process or
tokenize unstructured text data found in
columns in a data set for model building
this tokenization occurs automatically
and those results are fed into the
machine learning algorithms this
capability for text analytics is also
supported in Oracle data miner and
Oracle or Enterprise Oracle text use a
standard sequel to index a search and
analyze text and documents that are
stored in the database in files and on
the web and it also supports multiple
languages and uses advanced relevance
ranking technology to improve search
quality a new feature provided by Oracle
data mining for text analytics is the
algorithm explicit semantic analysis or
ESA and it's designed to improve text
categorization essentially what it's
doing is to provide well-defined
topics or terms that describe the
document as opposed to abstract concepts
that are just related to individual
words found in those documents so it is
more meaningful to the user and this is
in contrast to the latent dearest Laius
OCA ssin or known as LD a where with es
a we're using a knowledge base such as
Wikipedia as the basis for building the
model and that allows us to assign these
human readable labels to concepts
because Wikipedia is defined with titles
that are these relatable concepts LD a
on the other hand has topics that are
difficult to interpret because they're
defined by their keywords as opposed to
the abstract descriptions in addition
with algorithms like LD a the topic said
itself will change when the training
data changes which is not necessarily
what you would like if you're trying to
do analysis over time the data that can
be used to support ESA analysis are
individual text doc
but you can also have data with mixed
column types including text categorical
and numerical data the last feature
we'll talk about for Oracle data mining
is the extensible our algorithm models
and what this does is to allow users to
leverage our code to build models but in
the Oracle data mining model framework
and this is in particular for Oracle
database 12 to users define scripts that
say how to build a score and view models
in R and these are scripts are stored in
the our script repository
it supports classification regression
clustering feature extraction attribute
importance and Association mining
functions users can invoke these models
from R or sequel as well as Oracle data
miner because they fit in the overall
Oracle data mining framework for mining
models in R the predict method will
actually execute the score function
that's specified at the time of model
building it's represented visually here
where you see we have the oracle data
mining modeling framework three
functions that need to be specified and
then the user defines those functions
store them in the our script repository
so they can be leveraged in the overall
Oracle data mining framework and that
brings us to our next topic of our
statisticians data analysts and data
scientists are increasingly using R for
their their needs
r is a statistics language that's
similar to base SAS and SPSS statistics
one of the reasons why it's so popular
is because it's very powerful you can
accomplish a great deal with very little
specification it's extensible you can
custom write functions or packages that
can be shared within your organization
or with the art community at large
highly graphical as you see many of the
visualizations possible on the right has
extensive statistics it's easy to
install and use and there's a
very rich ecosystem where there are over
10,000 open source packages that users
can draw upon to supplement the base our
functionality and this is used by
millions of users worldwide and one of
the things that have contributed to its
popularity is it's free it's a simple
download from cran our project org so
let's look at traditional R and data
source interaction and highlight a few
of the issues that arise we have R and
the r user needs to access data from
some source perhaps it's a CSV file in
which case they can use the built-in our
capabilities to read and write data to
the file system or perhaps they have to
go to IT or the database administrator
and request an extract in the form of
flat files perhaps again CSV or other
representation and then they have to
load that into the R engine when their
computations are completed those results
might have to go back to say a database
to be leveraged by applications still
other users might directly access
databases by using packages such as our
ODBC JDBC or our Oracle now the issue
here is one of access latency how much
time does it take to load the data from
the data source into R it may also
involve a paradigm shift especially if
you're doing this programmatically where
you have to go from R to the data access
language perhaps sequel and then back to
R in addition there's the memory
limitation R is a memory based system so
the data that you want to analyze must
be able to fit in RAM so not only must
the data be able to fit in RAM but you
also have to have enough memory leftover
to actually perform operations because
of ARs call-by-value semantics even if
your machine has multiple cpus our code
is generally single threaded and so
you're not able to take advantage of
more powerful machines unless you do
special coding yourself leveraging other
our packages that are available for that
purpose also ad hoc production can be an
issue
when you need to deploy solutions do you
have something like an R script cron job
where the cron job is going to start up
the R engine retrieve the data execute
the script put the results back in the
repository of choice and lastly backup
recovery and security as we mentioned
earlier that you do have to figure out
how am I going to backup this system
what type of recovery is necessary in
the event of failure and also how is
security going to be maintained if the
data is being pulled out of its native
source and made available say through
flat files that are exchanged through
email through memory sticks or similar
means to address these concerns and in
many cases limitations we've introduced
oracle our enterprise where we treat the
database as a high performance compute
environment where our users can leverage
in database parallel and distributed
machine learning algorithms and
processing and they can manage their our
scripts and our objects in the database
itself and integrate the R results into
applications and dashboards by a sequel
Oracle our enterprise is built around
three key functionalities the first is
the transparency layer transparency
layer leverages proxy objects what we
call our e frames so that the data
remains in the database but we've
overloaded our functions that translate
the requested functionality from R into
sequel and then users are able to
leverage standard our syntax to
manipulate database data they can take
advantage of indexing partitioning query
optimization all that are readily
available through Oracle database the
second area is parallel distributed
algorithms where we provide scalability
and performance by leveraging algorithms
that are built into the kernel of the
database where we provide a convenient
our interface over the Oracle data
mining algorithm
and then we have additional are based
algorithms that are executing at the
database server and they leverage the
next capability we'll talk about which
is embedded our execution embedded our
execution allows users to execute their
our code at the database server side
they can store there are scripts in the
our script repository of the database
and then they can invoke those scripts
in a data parallel or task parallel
fashion and even non parallel execution
if that's what they require open-source
cran packages can also be leveraged in
the writing of there are scripts for
execution at the database server side so
we've talked a lot about scalability and
performance here's an illustration of
the function o re LM which is a custom
LM implementation that mimics exactly
the results that are would provide in
its LM function but does it in a
scalable performant fashion first we're
going to look at the processing time
using LM if we have a data set that is
roughly 184 million records with 31
numeric variables and we want to build a
linear model on that well we first have
to load the data from disk into memory
at 220 gigabytes in this particular
environment it took over an hour to just
to load the data then the model needed
to be built that took about 43 minutes
so this is our baseline 1x at an hour
and 46 minutes now let's compare that
with Oracle our enterprises or e LM
function even using a single thread as
for LM we see that we don't have to load
the data the data is already in the
database so without having done anything
we're already two and a half times
faster than R however because we have
parallel distributed implementations
users can indicate that they would like
to use multiple threads in which case
here we're seeing at 32 threads the
performance jumps to about 68 times our
baseline execution we can continue to
increase
number of threads to get even greater
performance benefits to illustrate
several of the Oracle or enterprise
features I'm going to use this Internet
of Things use case for a sensor data
analysis where the objective is to
predict aggregate demand for energy from
customers over the next few days to
approach this we're going to build one
model per customer to model that
particular customers behavior and to
identify deviations and individual
behavior and the overall aggregate
demand let's say that we have two
hundred thousand households for this
particular utility and each house has a
smart meter well we have one reading per
meter per hour which produces almost two
billion readings if we had multiple
years worth of data at three that puts
us over five billion readings yet each
customer has a relatively small number
of readings meaning that are handled
that exceedingly well even if each model
took only ten seconds to build for two
hundred thousand models that would
result in a serial execution of about 23
days which is way past the objective for
being able to predict demand in the next
few days however if we increase the
degree of parallelism to 128 we can
bring that down to four point three
hours so now a previously intractable
problem gives plenty of lead time for
the enterprise to make decisions on
energy allocation and utilization so in
this scenario we have our data coming in
perhaps to an exadata and being stored
in Oracle database perhaps also
partitioned and we ask our data
scientist to write an R script that
builds a model to predict consumption
for an individual customer this already
simplifies the problem because they
don't have to worry about how they're
going to deal with that for every
customer this our script is then wrapped
in a function and stored in the our
script repository from there we instruct
the database to spawn multiple our
engines 2x
tutor our script passing one customers
data to each our engine invoking the
function upon executing the function we
see that a model is produced by each one
of these our engines and instead of
storing them in flat files we store them
directly in the database in an our data
store so here rather than having 200,000
flat files that we have to manage
separately these are incorporated into a
closed system within the Oracle database
where all of the mechanisms are
controlled by the database not through
external code reducing the amount of
testing time and issues for backup and
recovery you might think that putting
such a solution in place would require
hundreds of lines of code if not more
however notice here that in just
fourteen lines leveraging the O regroup
apply function were able to take the
customer usage data which is our proxy
object for a table in the database
partition that data on the customer ID
and then invoke the function you see
highlighted there to take one customer's
data build an LM model to predict
consumption and then store that model in
the our datastore and at the bottom note
that we can also indicate what degree of
parallelism we'd like we can specify
true for the default degree of
parallelism or we could have specified
even 128 there to say that is the degree
of parallelism we would like the system
to use if it's available to round out
our discussion of the advanced analytics
option let's take a look at one possible
deployment architecture with the
database we have the advanced analytics
option from there we can reach out to
Hadoop environments leveraging Big Data
sequel or we can go directly from client
systems whether we're using say an
Oracle our enterprise client or sequel
developer to invoke through the sequel
API or Oracle data miner to interact
with the advanced analytics algorithms
in the
other users especially from our our
leveraging third-party our studio server
components where they can have browser
access to Oracle our enterprise through
such a tool
others are leveraging Oracle business
intelligence Enterprise Edition or
Oracle data visualization to invoke our
scripts or sequel scripts that leverage
the advanced analytics capabilities and
all of these capabilities can be made
available on the cloud in this next
section I'd like to touch on the data
lab and building applications using
Oracle advanced analytics components
many of you may have seen this
information management reference
architecture from Oracle in this case
I'd like to focus on the data lab but to
simply put it in context on the
execution side we have input events that
might be coming into a streaming engine
we have the data pool and the data
factory that are also producing results
that might feed into enterprise data and
we have the structured data that's
coming into the enterprise for capturing
perhaps in a data warehouse or doing
reporting with dashboarding tools but
feeding a lot of this can be the data
lab itself where we're looking to
identify actionable insights and
methodologies that can then be
incorporated into the streaming engine
to allow dynamic production of
actionable events or to incorporate into
the data pool or data factory where we
can have results that are made available
to the enterprise for more accurate
decision-making so in the data lab and
if we talk about the machine learning
paradigm we can say there are several
steps that data scientists will go
through they'll have to acquire the data
where that can be largely dependent on
the environment that the data scientist
finds themself in ideally the data
resides in the Oracle database or Hadoop
where it can be readily accessed but
once you have a
fired the data then it's a matter of
preparing it what's referred to as data
wrangling bringing the data together in
a form that allows you to actually
analyze it data cleaning can take a
variety of forms some of which might
involve outlier detection and missing
value treatment still other data
preparation techniques involve variable
selection which columns of data should I
actually include and also the creation
of new variables or transformations on
those variables at this point is helpful
to explore and visualize the data to get
summary statistics or correlations among
variables in the data and then to do
graphing and plots that help us to
understand visually what's happening in
the data data scientists will then build
models and explore multiple algorithms
and settings to see which ones are most
appropriate to deliver the results
expected those models then need to be
evaluated for accuracy or the degree of
interpretability that makes them useful
to the organization and finally we need
to be able to deploy models so whether
we're generating batch scores or naming
enabling some type of real-time scoring
or integrating with applications and
dashboards however this process is
seldom that straightforward we can go
from data preparation back to realizing
we didn't get the right data or all the
data that we need after building models
we may also say that we don't have
enough data or we have to go back and
explore and visualize the data or do
additional preparation of the data after
evaluating the results that we got or
perhaps once we deploy the models we
realize that either rebuilding those
models or Reap repairing that data are
necessary to achieve the desired results
while we've touched on data sources over
the preceding slides at various points
let's just be precise about where data
needs to be to be used for Oracle
advanced analytics essentially any table
or view that's accessible within
Oracle database in the user schema can
be used for data mining in general the
data should be stored locally in Oracle
database tables for optimal performance
however we can leverage a tool such as
Big Data sequel or DB links external
tables to reach out to data in other
data sources so what support is given to
building applications in advanced
analytics let's start with Oracle data
miner at one level we create analytical
workflows that produce a model or a set
of scores or some result that is
necessary for an application from there
we can use the built in interface in
Oracle data miner to schedule individual
recurrent workflow execution in this
case the workflow perhaps creates or
populates tables with results that can
be readily used by applications and
dashboards or we can export a workflow
as a PL sequel script that can then be
loaded into a database and invoked as
part of an application this can be done
either through DBMS scheduler packaged
directly for script execution or perhaps
the application will simply invoke the
pls sequel procedure that references
this script in building applications
using the Oracle data mining sequel API
the user can write a PL sequel script
directly and then optionally create a
stored procedure or function for that
script but then use the DBMS scheduler
package as noted for the oracle data
miner to actually schedule the execution
as illustrated here on the right where
we're passing in a PL sequel block and
indicating when we would like to start
any repeat interval and end date etc
alternatively users could invoke the
sequel script directly from the
application perhaps using ODBC JDBC or
other interfaces and lastly even
singleton scoring is possible using a
sequel query as indicated
the right where we're passing the actual
row that we wish to score directly in
the from clause by indicating that the
values are coming from dual when using
Oracle our enterprise to build
applications the user will typically
write an R script and then they can
invoke that R script from other
languages like Java using our Java or
our serve but that's not the ideal
solution as we pointed out earlier what
we'd like to have is store that our
function in the our script repository in
Oracle database then we can invoke that
R script by name from a sequel query
using the oracle or enterprise embedded
our execution this supports the sequel
enabled applications and dashboarding
tools such as OBIEE and oracle data
visualization those functions can then
be invoked through DBMS scheduler
package they can be invoked from R but
embedded in other languages as above or
those same scripts can even be invoked
through Oracle data miner to be part of
an analytical workflow as we've
mentioned earlier that data science is
available on the Oracle cloud leveraging
the Oracle advanced analytics option
through the Oracle Exadata cloud service
as well as Oracle database as a service
as part of the high performance and
extreme performance options on this
slide we have a series of links and
resources to help you get started with
the Oracle advanced analytics option
various overview materials listed as
well as YouTube recorded oay
presentations and demonstrations there
are other tools that are available such
as tutorials online tutorials slide
series and other resources such as our
blogs and websites so thank you for
joining this presentation on Oracle
advanced analytics where we've given an
overview of the components of the Oracle
data miner graphical user interface the
oracle data mining sequel api
as well as the Oracle our enterprise our
interface to the advanced analytics
capabilities in Oracle database
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>