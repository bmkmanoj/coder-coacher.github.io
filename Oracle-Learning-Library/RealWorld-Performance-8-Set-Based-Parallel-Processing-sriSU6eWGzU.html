<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Real-World Performance - 8 - Set Based Parallel Processing | Coder Coacher - Coaching Coders</title><meta content="Real-World Performance - 8 - Set Based Parallel Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Real-World Performance - 8 - Set Based Parallel Processing</b></h2><h5 class="post__date">2014-04-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sriSU6eWGzU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in a previous demo I showed you the
basics of just step 1 of learning to
learn about set based programming and
set burst based programming techniques
and this was using an array just to
insert some data versus using a set of
independent Java threads all loading one
row at a time and these were
experiencing contention within the index
or they were all contending because they
were all committing at the same time but
let's take this idea of set based
programming a little bit further and
start thinking about what we do with
large data sets and we're going to break
it down into four blocks we're going to
think about loading data because we all
have the requirements to ingest or load
or insert a large amount of data into
the database at speed
how will we best do that then we're
going to start think about validating
data we get data from all disparate data
sources how do we validate that it's
good data its unique keys are unique
foreign keys or foreign keys and orphan
rows and things like that so we have a
means of validating to see that the data
is correct from an integrity point of
view the next level is we're likely to
want to transform and modify data
because it probably isn't perfect or it
came from lots of disparate systems that
may have anomalies in the data like the
tax code the dates formatting and the
time zones appropriate different
calendar systems and things like that
and then we will look to aggregate data
potentially for pushing on to smaller
data Mart systems or just to provide
easier querying on the data warehouse
for modelers who are quite happy working
with aggregates so we can see that we've
got this sort of pipeline of data
disciplines that we have to learn how to
do with large data sets
and what we're going to do is show
through and we're going to pop up some
code snippets and some ways to show how
to do it
but we're gonna do follow the same
techniques as we showed in the previous
demo is that we're going to start racing
these different programming techniques
against each other they've all got the
same amount of hardware they've all got
the same data but some just take longer
than others and it's all in the
programming techniques that are adopted
some of the techniques will only ever
use a single CPU some of them wheedle to
use multiple some will use multiple CPUs
more efficiently than others and these
are the things we need to learn as we
move forward in this game all those
disciplines we described let's look at
the first one which is loading and to
load the data we're going to use four
different techniques and we're gonna see
those techniques over and over again
we're going to use a mixture of row by
row processing we're going to use array
techniques as we showed earlier and then
we we don't call what we call homegrown
parallelism and I'm sure you've all seen
that in somebody who's got a job that's
taken too long and they've chopped the
job in half by doing two different sets
of data or something arbitrary like even
numbers and odd numbers or months of the
year and that segment of data up and
they've basically taken their serial
programs and running multiple copies on
it on different sets of data and then
the last one is going to be true
set-based processing we were able to
exploit core parallelism within the
database to achieve both parallelism and
get the work done and so you can bake
off and see which is probably going to
be the best technique for you so in
terms of data loading let's go on home
let's start a race of data loading using
the four different techniques I'm going
to start the thing and you'll notice
very quickly the set-based techniques
are pulling way ahead of the
the techniques and as you can see the
red bar shooting across the side and
we're going to leave loading I think
it's about a hundred and 1 million rows
in this case and each of the techniques
is using an individual node in a half
rack of exadata so they've all got the
same amount of hardware they were got
one node but let's look at the
difference in the performance no the
winner was recent number four which was
the set based technique and he took 22
seconds the homegrown parallelism
technique they took a minute and a half
the array interface technique which is
the first method we showed earlier took
14 in nearly 15 minutes and if you've
done it as a row by row program it would
have taken 3 hours 43 minutes and 5
seconds again we're demonstrating the
difference in coding techniques can make
literally orders of magnitude difference
to the performance and to get things
down from hours to seconds is always
interesting because when things come
down from hours to seconds that means
you're a thousand times faster and that
is a great objective to have if you're
doing real-world performance if you make
your target a thousand times faster you
actually start to change the game you
make the business different imagine jobs
you are running in the middle of the
night that are taking hours to run if
you get those jobs from hours to seconds
you're more likely to run them in real
time in phase with the business you're
able to make decisions in real time and
basically you bring performance
engineering is bringing an edge to your
business instead of having performance
as something that is a problem it's
something you can be proactive on and
make the game a better game so that was
loading well let's talk a little bit
about the code techniques that we used
to load all this data well it's pretty
simple in this time and we've coded
things up in PL sequel but don't let the
language distract you the fact we did it
in PL sequel just made our lives easier
but you can make all the same
performance enhancements if you're
coding mechanism is via
even COBOL if you're writing still
writing COBOL programs the language
doesn't matter it's the algorithms and
the techniques you adopt is the
important part of the game
so let's have a look at the row by row
code and as you can see it's a very
simple PL sequel program basically a
cursor loop that's going into a fetch
state and it's taking for every row that
you fetch it's inserting into a table
pretty simple concept if I had a dollar
for every time I saw programs like this
in the real world I would be having a
very high-performance car in my garage
so anyway that's the way it goes these
are very simple now if we take it up to
the array process it's not you will be
surprised to see much more on that but
we're going to see the similar sort of
code construct but the programmer in
this case has used the PL sequel bulk
operators to effect an array insert and
that that worked well but remember you
can only use a single core it was a lot
faster than when if you did it row by
row they were talking 3 and 3/4 hours
down to quarter of an hour so that's a
big gain and you're just using the same
one CPU but if you're looking to get
this data or 101 rows of 1 million rows
of data loaded in seconds we need to
start taking up some parallel techniques
now the homegrown parallelism runs are
pretty much the same program but what
we've actually made the homegrown
programmer has actually decided to talk
to start cutting up the data and handing
off the work so we in fact what we're
doing is we're scanning the source data
multiple times and we're using the Ora
hash function to figure out how to
divide the data between all the parallel
inserting slaves and we're by this
mechanism we're getting some manual
parallelism and manual of diversion of
data between query slaves and so we run
32 of those jobs all hashing by 32 on
our hash such that each slave loads a 36
of the data but again you can see we
fundamentally are running the same
algorithm as we ran in the serial
program but we starting to take
advantage of chopping the data up
parallelism and the array interface the
bulk operators but if we I should come
down and actually look and you'll notice
that with all these elaborate techniques
the code seems to be getting more
complicated and more confusing so let's
look at the set-based programming code
well that was a bit of a surprise
because we've got a lot less code there
than all the other ones put together
basically a simple insert with an append
hint the result of the query from the
extant external table scan events table
this is the least amount of code yes
this is also the fastest now the reasons
why it's fast is is basically able to
take advantage of core oral oracle
parallelism to do the insert it's also
taking advantage of the appendant and an
appendant is a pure set based technique
because it allows us to do bulk data
loading but it can also be done in a
fast path such we're avoiding generation
of undo and redo so we're cutting down
the amount of CPU we're burning we're
cutting out the amount of i/o we're
doing and it becomes an extremely fast
way of loading a large amount of data so
much in fact that basically a program
that was running in three and
three-quarter hours is now running in 22
seconds and this is just taking Vantage
of core Oracle database set based
techniques and technology that run in
parallel and has been designed to handle
large amounts of data
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>