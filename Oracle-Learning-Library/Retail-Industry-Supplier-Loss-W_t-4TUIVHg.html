<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Retail Industry - Supplier Loss | Coder Coacher - Coaching Coders</title><meta content="Retail Industry - Supplier Loss - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Retail Industry - Supplier Loss</b></h2><h5 class="post__date">2017-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W_t-4TUIVHg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to this retail
customer related demonstration showing
the Instant Streaming insight into its
financial losses attributed to specific
products from various suppliers the fast
of these situations can be identified
and isolated losses can be minimized and
the company's procurement department can
be quickly engaged to renegotiate more
profitable product price points this
entire application solution built using
an intuitive user experience talling
from the industry-leading Oracle stream
analytics platform powered by a spark
streaming and Kafka messaging runtime
let's kick things off by logging into
the product the stream Explorer talling
and take a look at our configuration
system settings for the SPARC Kafka and
the visual analytics components
you
now for this application we have some
foundational artifacts that will be
utilized the streaming source of the
products transactional events the
kathcart topic target for the determined
and created event results that can be
immediately processed by the procurement
management applications or processes an
underlying connection and finally a
reference to a specific database table
which provides much more information
context for the specific supplier
now we will go ahead and build the
solution application providing a name
useful description tanks for searching
artifacts later in the catalog and we
select the initial based streaming
source pressing the Save button
initiates the creation of a spark
streaming application that will receive
the event messages and display on the
canvas in the live output stream section
you
here we can see on the console that the
smart application has been dynamically
created and is executing in a draft
state indicating more capabilities will
be added before it is ready for
production
so let's include more capabilities by
creating a new query stage in this
application pipeline we provide a name
and description then save to have the
stage added to our application as the
live output stream section on the canvas
receives the process events from the
previous stage we now want to add more
context to the stream by joining or
correlating with a persisted database
table using its previously defined
reference from our catalog specifying
the source correlation properties now
instantly the stream and the related
table information is joined together
providing much more valuable information
for later decision-making by the
application
we now add a new query stage in the
application pipeline in which we will
make use of the integrated function
expression builder features available in
the user experience to calculate the
transaction costs the revenue and the
associated profit and loss amounts all
valuable new information that can be
leveraged later in this application
you
here we specified the required
mathematical calculations and news of
rounding capabilities as each new live
apple stream column is created it is
renamed to indicate its purpose and the
common view manipulated for better
readability
notice how the talling provides help by
introducing named completion for the
available columns
you
we now want to specifically identify the
losses discovered in the streaming data
so we add a new application stage and
implement filtering to only show events
that have negative amounts
you
we once again used the expression
builder to now provide an absolute value
for that profit loss amount and then we
remove unwanted columns
you
finally for our visual analytics
features used later in the application
development we need a transaction time
so we add that system time and convert
the value as needed
you
now we have all the core data elements
required by this application we will
send the required columns from the
resultant live output stream event shape
shown to its target for distribution and
management by a listening procurement
system we ensure all required columns
are correctly mat so that the live
output shapes stream shows the
information required note here that
until the application is finally
published no output events will be sent
to the relevant Kafka target topic okay
so now that is all completed we will add
some more visual representations in the
application pipeline on our losses only
stage
by selecting the visualization tab we
can define various charts that provide a
richer visual experience and help to
identify situations from a real-time
operational perspective
for now we will select a bar chart and
apply the required attributes
you
as a new streaming data flows for the
application pipeline the graphical image
builds for immediate situation awareness
of the offending products and suppliers
you
once verified the application can now be
formally published to the spark
streaming runtime
here again on the console we can see the
applications now executing in a public
state and will continue to provide
permanently the streaming analytics with
the results past to the target Kafka
topic for downstream processing
now we will implement both real-time
streaming and historical visual
analytics by creating a cube
note that a cube is a method of storing
data in a multi-dimensional form WHMIS
data measures categorized by various
dimensions
you
once the cube is created data is flowed
by the streaming application into the
cube for further analysis
by selecting the cube artifact it will
provide features which we can now use to
create the visualization slices that
will be subsequently laid onto an
interactive dashboard
we create a big number value indicating
the total loss count until now
you
by using the group by selective features
we can create a table view and then
subsequently a bar chart representation
of accumulated losses by each supplier
you
accumulated losses by channel can also
be represented in a pie chart view
all of these charts saved for later use
in our new dashboard
you
finally we can create a more exotic bar
chart view using metrics with a deeper
breakdown showing losses by Channel and
then by country
you
now we have all the visual slices needed
for our dashboard we return to the
catalog and create a new dashboard
artifact with a name and associated URL
you
using the Save button the dashboard
artifact is created and by selecting we
are provided with the dashboard canvas
on which we can add the required slices
define an automatic refreshing period
for the visual components
and finally save the dashboard and copy
its URL for use by other browser users
and applications
you
so there we have it a complete
end-to-end retail supplier analysis
application built not in weeks or days
body minutes
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>