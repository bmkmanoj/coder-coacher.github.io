<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building JavaFX Interfaces with the Real World | Coder Coacher - Coaching Coders</title><meta content="Building JavaFX Interfaces with the Real World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building JavaFX Interfaces with the Real World</b></h2><h5 class="post__date">2013-02-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lxrDKtvMHg0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right well good afternoon and welcome to
the session on building javafx
interfaces for the real world well
almost real world my name is Simon
Ritter this is Angela cocido my
colleague we r technology evangelists at
oracle and we have spent a lot of time
working on today's presentation every
year that I come to job one I promised
myself that this year's going to be
different and this year I'm going to get
everything ready by the time i leave for
java one so the demo that we're going to
show you the end finally got it working
at six o'clock this morning so next year
I'm going to get everything working by
the end of the presentation but my vm
makes young everything before the
presentation right so I work for some
for 14 years before we were acquired by
oracle and one of the things you notice
when you start working for oracle is
that lawyers get involved and so every
one of our presentations now has to have
a safe harbor statement at the beginning
or at the end and as you can see it says
the following is intended to outline our
general product direction well i can
pretty safely say that this is not our
product generate a product direction
bear with me and you'll see what i mean
so what we're going to talk about is how
you can interact with machines and so
we'll talk about the concepts of the
man-machine interface how things have
changed over time how things have
progressed in terms of how we actually
interact with these things called
computers then we're going to go into
some of the technical details of how you
can use Java to work with different ways
of interacting with computers so we can
talk about two big things we're going to
talk about Java and the kinect sensor so
I've got that taped to the chair there
and I'm going to talk about Java and
reading your mind kind of so that's
that's like I say it's not our general
product direction
despite what some people might say and
then the last bit we're going to talk
about javafx and how you can interact
user or how you can get user interfaces
to interact with gestures detected by
the Kinect and we've got a really nice
demo based on a casino we'll see how
that works at the end so let's start
with the ideas of the man-machine
interface now it's interesting because
this is really where it all started
certain in my opinion is that we started
having the ability to use some kind of
mechanical device for interacting with
machines and producing output so a
typewriter it was a good example of how
we really started this this really goes
way back to the sort of 1870s when the
the qwerty layout qwe rty layout was
first designed and thus misconceptions
about white was designed the way that it
was the reason that it is laid out in
the the form that is is so that you
don't have commonly paired letters in
such an arrangement that the the Hammers
of the typewriter would be click next to
each other so what you don't want it's
S&amp;amp;H or th having the the levers or
hammers next to each other because if
you type very quickly when you track
them together they can get jabbed so the
qwerty layout is actually designed to
spread the letters out in terms of their
usage some people think that it's
actually designed to slow down typists
but that's not the case it's actually
designed so that you can type very very
quickly without getting the keys to
actually bump into each other
I don't know actually ok we'd correct
you on that one it's the original layout
before who is it O'Connor and the
company that took over the original
layout had a period or the full stop
where the R is so the original layout
was qwe dot T why you I opie and then
the company that bought the rights to it
changed it so the r was on the top row
so that you could type typewriter only
using the top row of the the letters on
the typewriter so anyway let's move on
from that because that's a real
aggression into into history so if we if
we go on progress was clearly made so
now we have these lovely plastic
keyboards that have a lot more keys on
them for a start so you have the
traditional qwerty layout but now you
have a numeric keypad you have function
keys you have an escape key what you
want to escape from i'm not sure but
well yes they'll go there and and so you
know we have the same style of thing
obviously we don't care about the layout
being organized because from a point of
view of bumping hammers into each other
it doesn't make any difference but we've
using the same layout that we've had
since the 1870s and then some very
clever people Xerox PARC came along and
they started designing this thing this
is really the very first mouse that was
created and it's the idea that you can
have something that you can put on a
table two-dimensional surface and you
can move the mouse around using your
hand so it's a very intuitive kind of
way of working with something because
you know we like to move our hands
around and if you think about it you
know as I'm talking I'm pointing at
things and waving my hands around so
it's a very intuitive way of
communicating with a machine is to use
something that you can position we're
having a flat screen as well you've got
a two-dimensional surface that you want
to work with so it's very easy to have a
mouse that allows you to move a pointer
around on the screen and then you've got
the red button which you can press to
indicate you know that's where you want
to do something so that was the sort of
the first one I rather like that because
it was made of wood so you know computer
science and would work I think they go
very well together
so clearly we've progressed from there
and we now have you know very
ergonomically shaped mice we have nice
that don't need a cable we have mice
that have lots of buttons and several
scroll wheels and all sorts of things
like that but essentially the same kind
of concept a two dimensional way of
interacting with the machine on a flat
surface what we've seen certainly more
recently is the multi-touch type of
interactions become a lot more popular
so this is an example of the Microsoft
Surface the first surface rather than
the one they've just announced and so
what this does is it's the idea that you
can have a table where you can interact
with it and you can have multiple points
of interaction so you don't have a mouse
it's still a flat surface but rather
than having a mouse you can actually
touch the table and the table will no
way you've touched the surface and
similarly you can have more than one
point where you can touch it so you can
do the traditional things of spinning
things around and stretching and things
like that great fun those because they
can also have things that recognize
shapes and they can have things that you
can put like a glass there and you can
add information around it and things
like that I actually built one of these
for java one a few years ago don't know
if anybody came to that session that was
a great example of computer science and
woodwork working closely together
clearly as well things like the iPad
have become very very popular you know
they have a keyboard but it's a touch
keyboard rather than a physical keyboard
and you tend to interact with it through
much more sort of multi-touch kind of
gestures and things like that so this
has certainly been a big swing in terms
of direction for devices like this the
other big thing that's really driven
interactions with computers is the whole
idea of gaming because gaming is a very
different kind of interaction and you
don't necessarily want to have a two
dimensional mouse type of interaction
yes you can do that but if you're trying
to drive a car you don't want to drive a
car with a mouse so you need something
that's more realistic in terms of how
you interact with things like a car or a
first-person shooter game or something
like that and so we've seen you know the
development of very sophisticated
interfaces here we have
the Atari joystick I had one of these
when I was a child and you know it's
great because now you've got a different
way of interacting it's still two
dimensional because you're still
actually moving x and y but rather than
moving the device you're actually moving
your hand and you've still got the
button so you can do some interaction in
that way then people like a Nintendo
came along and they decided you actually
needed more buttons because that will
give you more ability to do different
things so they have the idea of a sort
of some kind of joystick idea and then
various buttons that you could use with
your fingers and that's really sort of
progressed now we have things like the
the dance pad so that you can't only use
your hands you can also use your feet so
you can have one of these games where
you play music they show you where
you've got supposed to bounce around and
you can make a complete fool of yourself
bouncing up and down in front of a
computer clearly you know it's different
ways of interacting with the system and
then you know very 1970s Buck Rogers
style of interaction again Nintendo
always been at the forefront of design
for interactions and so they came up
with the idea of a data glove where you
could put some controllers on a glove
and then you kind of interact with it
not really a true worse but it does have
some Bend sensors in it but it's you
know a next generation of design and
then of course we've got the very
familiar gamepad kind of idea which is
this thing that somebody of a more
mature age you watch children playing
with these things and they they have
this phenomenal dexterity of their
fingers and ability to sort of move all
these different things at the same time
and interact with the game but really
not intuitive I mean a gamepad is is
very non-intuitive in terms of how
people interact with it so now it's
become about gestures the recent changes
are all about you know how do we become
more natural in terms of our interaction
with systems again looking at gaming
you've got the the Nintendo Wii this was
a real game-changer pun intended in
terms of how people interacted with
games rather than having this complex
gamepad suddenly you have the ability to
be much more natural in terms of your
interactions if you wanted to bowl you
know why would you want to press a
combination of buttons in order to
indicate how your bowling know what you
want to be able to do is move your arm
backwards and then move it forwards in
the type of action that you would do
with your actually bowling and that's
what the nintendo wii allowed each
didn't allow you to do so using
accelerometer using various bits and
pieces they could actually monitor the
movement of the controller and then feed
that into the game and give you a much
more realistic way of interacting with
the actual application and that made it
much easier for people like my mother
people like you know children to play
with these things and learn how to use
them clearly Sony came along and they
decided they needed something similar
they had a slightly different approach
yes they had accelerometers built into
it but the problem with it just having
accelerometers you can't get precise
information about position you can get
the relative information in terms of how
fast you're accelerating but it's very
very complicated to take the data from
an accelerometer combine it with
gyroscopic data and have some idea where
the device actually is so what they did
was stick a light on the end of the
controller so that they could track that
with a camera then you can get
positioned data in the X and Y plane
plus the acceleration data and maker
again a more natural kind of interaction
and then Along Came Microsoft and with
helping some other people developed the
Kinect which was again a big shift
because it meant you no longer needed
any form of controller to actually hold
so it became very natural in terms of
the interaction because you could just
move your arms around move your body
around and there was no actual
controller involved that you actually
had to hold so we have my progress so
let's talk a bit about how we can go
about tracking gestures using the Kinect
and Java so the first thing is to have a
closer look at the connect itself and
what you've got here is quite a complex
piece of hardware and for the price it's
phenomenal to think of what it can
actually do in terms of giving you data
about where somebody is in front of the
sensor so you've got a number of
different things here you've got a
tilting motorized base
okay well that's not fantastically
exciting but it is there so you can move
it there's a 3-axis accelerometer now
that's okay that's a useful thing
because it actually can tell you the
tilt of the the sensor but typically
you're not going to be moving this thing
a lot so it doesn't give you a Norma's
amount in terms of extra information
it's also got some downward facing
microphone so it can actually pick up
sounds and because you've got four
microphones placed along it you can
actually get some directional
information about the sound we won't be
going into that in the Java part of this
but those are available in terms of the
hardware then really you've got these
three different parts to the front of it
which is the thing that really does the
hard work or east in terms of gathering
information you've got a standard sort
of emoji be webcam type of camera so it
can take an ordinary type of picture of
visible light and see exactly what's in
front of it so that gives you certain
amount of information in terms of okay
that's what we could see as a human
being but then to give you much more
information or give the system much more
information you then got an infrared
light source and that basically projects
an array of point data or point dots
dots not point point dot dots onto the
field of view of the the sensor and what
that then does is use the reflected
infrared light from those points gathers
that up analyzes the intensity of the
reflected light in order to give you
some idea about how far away the actual
points are and so this is the really
clever part because of course you're
using infrared lights you can't see
anything as a human and so you can get
three dimensional information from this
combine it with the vision of the RGB
camera and really get some very
sensitive data in terms of like the
sensitivity in terms of the size and
scale and soul of what is in front of
the camera and in front of the sensors
and then you can combine that with other
information to really get details about
what the user
the system is doing so we look at the
typical sort of information what you've
got is an RGB image okay so that's just
what you're getting from the webcam
somebody's standing in front of the
sensor then what that does is it uses
the information from the the depth
sensor combined with the information
from the RGB image as well and generates
a depth image and what it can then do is
it can actually identify a user from
that so it's more than just a simple set
of data points that represent depth it
also has some knowledge of the shape of
human body so it expects a human body to
have you know two legs two arms ahead
and be a reasonable sort of shape I've
not tried standing on my head in front
of the Kinect sensor to see if it still
recognizes me anybody tried that No okay
I'm not alone it I would be interested
to see whether whether it confused and I
suppose you could just turn the sensor
upside down it's taped to the chair but
we'll try that later okay yeah so it has
certain knowledge about the physiology
of human being and it uses that to
determine where it thinks a human being
is so you get depth sensor image and
then what that does is it says okay we
know the general shape of human body and
we've got some depth information and
then it starts to infer where the
different joints of the body are so it
gives different colors to different
parts of the body in terms the arms the
head the legs and so on and ultimately
what you get from that is a set of
points which represent the joints of
your body so you get the hand position
your hands elbows shoulders head knees
ankles and so on so you've got a
complete set of skeletal data which
shows where a human being is standing in
front of the sensor and that's three
dimensional data so you've got the x and
y and the z axis coordinates for each of
those joints as we'll see a little bit
later on now it's really quite
impressive how well it manages track
things because if you think about it it
when you move your body around you can
obscure different parts of your body so
you know you move your arm in front of
your chest how do you know that that's
where your hand is so there's got to be
quite a lot of complex processing going
on in terms of matching up the position
of the the points that it's detecting
with the the joints that it's actually
reporting now in terms of how we can go
about using that with Java well there is
this wonderful library the open ni
library which is all about natural
interaction so it's been designed for
all different types of sensors there's
not many other sensors that like the
Kinect there is one other that I know of
but it's really designed for working
with these types of three-dimensional
sensor and what they've done is create a
C++ library that allows you to access
all of the information that's coming
from the Kinect and so you've got all
these different parts to it in terms of
that the nodes of the the library so
you've got a device itself so you can
actually if you wanted to have more than
one you've got the idea of the depth
generator so you can see the depth data
that's been generated by the sensor
you've got the user generator when it
recognizes a user in front of the device
then you can get information about that
user because the Kinect can actually
track up to four people simultaneously
in front of the sensor so you can get
information about the different users
but it's seeing image generator is the
RGB camera image so if you want to
project that as poly replication then
you can the infrared generator again you
can get the information from that and
include that as a an image into your
application and then you've got things
like the scene analyzer and gesture
recognizer and we come back and talk
about this a bit later on in terms of
how that works and what you can do with
it the other nice thing is that somebody
has created a java wrapper around this
so there's a lot of Jay and I code which
actually allows you to access all of
these C++ functions as Java methods and
Java classes the only downside is that
it hasn't been fantastically well
documented in terms of the Java API it's
and so it's it's a little bit tricky at
times to figure out what you actually
have to do yeah you can kind of relate
it back to the C++ documentation but it
does sometimes take a bit of working out
what
actually going on so here's some some
Java code in terms of what you can
actually do with this so essentially
what you have to do first of all is get
a context for your Kinect sensor and
that's really the kind of key part that
you need and everything works off the
context so you need context in this case
what I've done is use an XML file to
define certain things about the sense
that i'm using one of the key things you
need in there is actually a license key
for one of the libraries that use
underneath its kind of bizarre more
quite sure what I they've done this
because they have put a license key on
this but they give it away freely and
it's always the same license key so I've
never quite understood why they wanted
to put a license key on that but it's
there it's freely available you can use
it you just have to make sure that it's
set up correctly then from that you can
get your depth generator so you can get
the information about the depth points
that are being reported back from that
you can also get a user generator so
that when the system recognizes a user
you can get events about the users that
are being recognized you can get
information about the width from the
height of the images that are being
processed so it's 640 by 480 and by its
standard and then from that you can also
use the debt to generator and the user
generator to give you a skeleton
skeleton in terms of the users that it's
seeing that's the set of data that you
get in terms of the positions of the
individual points and you also got a
skeleton capability which is the
capabilities that skeleton I guess not
too sure about that one then from that
obviously you want to be able to do
something useful and the easiest thing
to do is to get information about the
user that's standing in from the system
so here what we're doing is simply
saying okay I want to get the position
of the users joints reported back as a
skeleton we convert that there's a
conversion thing the real world position
data into a projective way of doing
things because of course you've got a
projective
sensor that you're using and what what
the system can do is convert that into
more usable points from a programming
point of view and then you can just
write yourself a little method where you
can say okay give me a particular joint
position for a particular user and
report that back as a three-dimensional
point in space and you can say okay I
want the skeleton joint left shoulder
and that will give you where the left
shoulder of that user is and so it's
there's not too much code that you need
to write in order to get a fairly
sophisticated amount of information from
the sensor so having got some
information in terms of like the
skeleton and seeing what the user is
doing in standing in front of the system
then you need to start thinking well
okay how can we make that more
sophisticated and start recognizing
gestures and so if we think about the
simple description of how we're going to
process gestures what you really want to
do is to say okay for the sake of this
description we're going to use our hands
only obviously you could have gestures
that involve more more parts of the body
but simply you know we do a lot of
expression using our hands so how do we
track gestures that we make with our
hands well the easy thing to do is of
course get the position of the joints
which represent our hands they've got
two three-dimensional points that we can
get from our skeleton okay that's pretty
easy pretty straightforward and what we
need to do is track the position of the
hands over time okay so we're going to
say okay gesture is a swipe gesture so
we're moving from left to right over a
period of time that's an easy to
understand gesture so we won't track the
position of our hand and see whether
it's moving from left to right then what
we need to do is say okay match the
position movement of their hands arms
head to predefined ones so that we can
say okay that's a swipe gesture that's a
circle gesture push gesture wave gesture
things like that and then once we've
recognized those things we generate
events which will actually say okay we
just gotta push event or we got a swipe
vent or whatever so that's the easy
simple for line description of how we
deal with
gestures of course it's a little bit
more complicated than that so tracking
the position of the points is fairly
easy because you've got a sequence of
points that have been generated by the
kinect sensor libraries allow you to say
ok continually give me the position of
my hand and then you can record that
enlist so you can have a cyclic buffer
continually recording the position of
your hands and so you can you can
develop a track of where your hands are
moving properly it's ok you've got a
selection of points now what you do next
so you're faced with complex mathematics
faced with the idea of ok do you do
linear interpolation so you say ok if
we're doing a swipe gesture we start
here we go to there if we do linear
interpolation between that we can
predict the points that we should have
passed through and how close are the
points that we've recorded to that
particular movement and if they're close
enough we say that's a swipe gesture you
can do more complex things you can do
quadratic interpolation you do Bezier
curve interpolation if you want to look
for like a circular movement or things
like that and then there's this
wonderful thing called hidden Markov
models which is about learning or
getting the system sort of learn what a
particular gesture is and so you build
up a set of data and you can use a
hidden Markov model to compare what the
user has done against a predefined set
of data and give you a confidence level
of whether they've just done the same
thing so you could say ok I move my arm
cross did it look like the time that I
did it before they look like the last
ten times that type of thing so this is
obviously you know complex stuff in
terms of lots of mathematics I started
this a couple of weeks ago and I thought
I don't have time for this so the other
thing we've got a problem with is saying
ok how do we recognize when a gesture
starts and when a gesture stops because
of course you know you move your hands
around all the time are we actually
doing a gesture or making a gesture or
is it simply we just want to move our
hand to a different place within the
field of view of the sensor so there's
got to be some idea of how do we detect
the start and the stop of a gesture
fortunately there is a solution to this
problem I mean ice people are open and I
have produced a nice little libraries
which actually will recognize gestures
they've done all the hard work for you
and so you can simply say okay using a
gesture recognizer within the libraries
you can look for certain events that are
happening in terms of particular types
of gestures so you can look for a push
gesture which is a z-axis movement you
can look for a swipe gesture and that
can be left right up down there's a wave
gesture there's also a circle gesture so
all of these things are pre-loaded into
the libraries and very nicely available
to you so what you need to do is set up
a whole bunch of listeners as a whole
bunch of events that are generated and
one of the things i found straight away
is that the type array sure that happens
in java generics can cause a bit of a
knot problem with age it causes things
to be a little bit more complicated than
you want them to be because everything
in the NI openni library its uses this I
observable sort of type and then they
have a particular type parameter that's
used with that to indicate what type of
observer but it is so you get hand event
args you get ID event args and so on but
the problem is of course if you want to
have a class that listens for events of
a different type so you want the hand
events arguments and also the ID event
arguments if you try and say my listener
implements I observe bore of type hand
event args and I observable of type ID
event args type of radio certainly says
well we don't care about the different
type parameters so we'll just say you're
implementing I observe what I observe
but but that can't be done because of
course they're the same type so you end
up with a mice in my sort of opinion you
end up with a lot more classes and you
actually really want because you've got
to do separate classes to deal with the
different types of events but that's
just a minor aside so let's have a demo
yes so the demo remember which demo
we're doing now right so what I'm going
to run now is ok so this is now as you
can see showing you the image of the
what's coming from the kinect sensor now
one of the problems with this is I'm
gonna have to turn my back to you
because of course the sensors over there
and I need to stand in front of it so if
i go here you can see it's gone blue
which means it's recognized me as a user
now in order for this to do what I
wanted to do I have to adopt the poems
which indicates that I am in the
calibration position hopefully ok
it's great okay that wasn't quite what I
was expecting to happen and in all the
times I've tested it that didn't happen
so okay let's try that again because i
think it's
I normally have this much problem
getting too okay good right it might be
because the bright lights but I don't
know so now what we see is the skeleton
and i just added some information so you
can see the position my hands and you
can see the position my head and what
I've also done is I've included at the
bottom there that's a plan view so it
shows the z axis so if I'm in my hands
you'll see that that I should certainly
bring my hands towards the center
was it up and then the top left hand
corner you'll see that there's a number
there which is actually very quite a lot
and that's the distance between my hands
so for a simple as an example what I've
done is to to indicate that i want to
make a gesture is to bring my hands
together so that it it will indicate on
stop gesture so now I've started a
gesture and so you see it will further
club in the corner what everyone have a
spot that then tracks the position of my
hand so if i move my hand like that i'm
going to circle
asswipe
just about see that
yeah so
you can see that tracks the position of
my hands okay yeah that's all right so
the idea there is to show that you can
track the position of the body and you
can track the position of the hands and
then you can record the position of the
hands as well so that's that's all
useful information but then my problem
was okay so what do you do with that
next so what we then do is we say okay
let's let's do some use the openni
libraries to detect gestures so I'm just
going to run this program first I'll
explain this later and then run this and
we do a time okay so now what it's doing
is it's now waiting for me to create
gesture to actually start things so if i
do that okay you'll see that now it's
tracking the position of my hands
fortunately I didn't have to start the
new they're funny poems but as I'm going
my hand around and you'll see that's is
pretty little arrow as I move the air
around those under the cross around
maddox basically saying it's detected a
gesture which is swipe gesture left
right and then I can do up and down and
same thing I mean if I do a gesture
which is to push gesture then I can so I
make a little circle up here and it says
okay got to push Jess you there a swipe
gesture and we can actually
see there's a few points where he gets a
little bit confused but it's not that it
actually works pretty well in terms of
detecting the gestures that are involved
in that so that shows the the difference
between the my sort of first attempt of
like okay can we track the position of
the hands and then the idea of okay
using the open and I libraries and
getting much better sort of results from
that so that's sort of like the idea of
so that was the idea that the demo there
so the next thing was to say okay how
about we go beyond the idea of simple
gesture recognition why don't we start
thinking about well thinking about
things so how can we monitor what we're
thinking and it feed this into some kind
of application so there is a company
here in California in fact called
NeuroSky or neuro sky and I think then
you're a ski but what they do is they
they've created a sensor which can pick
up information about your brain in
effect the really clever technology here
is that it's a dry sensor because
typically if you want to monitor your
brain activity you need to have a whole
bunch of sensors on your head and you
know all this jelly that's conductive so
that you can get the information from
the actual electrical information from
your brain so what they've done is come
up with a dry sensor which means you
don't have to smear jelly all over your
head in order to read your brain so they
come up with this and what you
effectively get is is this headset which
looks a bit like a you know one of these
hands-free telephone headsets that you
use with a phone but is actually
something which picks up the
electroencephalograph e information or
EEG information from your brain now it
doesn't pick up as much as if you went
to the hospital and had your brain
analyzed but it does give you a
reasonable amount of information from a
programming point of view it's actually
really good because they have a dongle
which plugs into a USB port and what
that appears to be from the point of
view the operating system is simply a
serial device so you all you need to do
is talk bytes across the serial
interface
you know fairly high speed it 115 point2
k and then they've got a proprietary
packet format which is published so it's
very easy to look at it and go okay this
is the you know how you get the
information from that and so you can
send you get status packets to say when
the headsets being connected when it's
been disconnected they report to
specific values about your brain
activity which is your attention value
and your meditation value so it's
basically how much you're focusing on
something or how much you're relaxing on
something so two orthogonal values and
then they also report back specific EEG
values it's like Vita beta alpha and
gamma waves that they can actually give
you values for it's not altogether clear
how much value those are from a sort of
interaction point of view and then
there's a bunch of raw values which I'm
never quite figured out what those r all
values relate to so from a point of view
of Java and the mind way which is what
they call it it's really quite easy so
we have the RX TX or Java calm library
that you can get hold of so you can talk
to a serial device usually set it up as
115 point2 k 891 parity and so on and
then you simply read and write packets
to the the port and so you can parse the
data packets I wrote a library for this
demo which basically did the interaction
with the the dongle read the dates
packets and then generated events based
on what packet was coming in so you
could say okay you've got a meditation
meditation packet came for you or an
attention packet came for you and then
you can do something with that with your
application and then you register the
listeners for whichever events you're
interested in which is good because the
the raw packets come at a fairly high
rate so you don't really want to be
flooded by those you can just ignore
those if you want so from point of view
of the code so what I did was I said
okay you've got a little test here which
implements the mind reader status
listener and the mind reader brainless
now I'm sure when I was at University if
somebody said I was writing a thing
called a mind reader probably would have
been quite skeptical but so i have a
class called mind reader
and then you just say okay mind reader
opens TTYL us be 0 and then reads sets
up a connection for that and then says
okay request to disconnect reason I do
that is because if you've if you've been
using the system before you might have a
connection left over so you request a
disconnect to make sure that has
disconnected first and then you add a
listener for the status packets as your
class as well so what then happens is
when you get a status changed event
happen you say if we've started so we're
waiting for the response from the
disconnect packet then we say okay once
we've got a response we also connect to
whichever headset is available and then
when we get the connection completed we
say okay mind meld complete so we are
now connected and then depending on
which value comes back we can have a
listener which says okay your attention
is this your meditation is that so let's
have a quick demo of that so I get to
put on my very very geeky headgear so
there's two things you have to connect
one is you have to put something on your
earlobe and then you have to have this
one that goes on your forehead I have
this thing about like coming to like
computer science conferences and wearing
bizarre things on my head okay so what
I'll do now and must remember Mary right
so I must remember to turn it on because
that always throws throws a spanner in
the works if it doesn't really do that
right so what's going to do now okay
tempting mind-meld that connection
established there we go good now so what
I've done is I've basically set it up so
you've got on the left hand side you've
got the attention value on the right
hand side you've got the meditation
value and then the the lines at the
bottom are the different EEG values in
terms of the alpha wave beta waves and
so on I really don't know what the graph
at the bottom shows other than that I
appear to be alive and then I I will
attempt to to meditate first I don't
know my attention is now fixed there we
go so so they gate so that was that's
the idea now one of the things I got
from this because i was thinking of
introducing this into one of my
applications is that it really doesn't
work because you don't have enough
control over the the levels to make it
Lucy I'm now in complete meditations I'm
talking to you right I'm going to stop
that okay so so as you can see that
gives you some idea of this information
that you can get from now right let me
go back to slide ok so now i'm going to
hand over to Angela who's going to talk
a bit about the the demo side of what
we're going to do for the rest of the ok
so as you can see Simon is kind of the
crazy guy from the team and I'm trying
to be a little bit of the designer so we
can build decent demos so what I'm going
to show you what I'm going to walk
through is ok we want to play we want to
put together a demo that is actually a
casino so let's do nice and nice
graphics nice interface and if you think
about it we're looking into having
unusual kind of interactions with a
computer like you can see with reading
your mind so
really want to have different and unique
user interfaces we don't want the
typical window with menus stuff like
that we want to make it really dynamic
really unusual and pretty interfaces so
javafx really fit really well you can
think about a lot of things animations
you can animate a lot of things you can
play with the transparency of the
components and one of the best things
that I've found is you can actually get
rid of your window itself so you can
actually have borderless application so
it actually blends really nice with your
background for the purpose of this demo
we need a full screen so that's why it's
actually square but you don't need to so
I remember one of the very very first ab
demos that I did which I've effects with
a swimming shark so it's actually pretty
nice because he was just the shark
doesn't have any background on the shark
with swimming on my screen so I can kind
of get bigger as you get close to you so
you have the reflection it's actually
pretty amazing to have a shark passing
by your screen right so it's a lot of
things that you can think of you can
have different shapes it will think
about to that application you have a
little dude on your corner of your
window and popping up some bubbles with
the messages that are getting for you so
things like that really nice application
that blend really nice with your
background and how do you do it with job
effects so it's actually extremely easy
so normally you have effects we have
your your scene so you have to first set
the feeling to be no so if there is
anything just don't show anything right
so just make sure you said that and
therefore they stage itself that is kind
of the window that you normally have by
default you can actually set it to
transparent so you say okay I don't want
to show anything that's how you actually
have data screen without any water or
anything right so something nice then
again nice features for example you
don't want to have a conventional menu
so the idea for the menu here with
everything is hidden and if you get
closer to the bottom the menu will just
show up just just you know slide in or
also from the side so just your manage
would just get into your screen as you
move close
today to the border so things like that
are very very easy to program with jela
fix the demo has a lot of animations
going on so we're going to see that the
dice are actually passing by those lines
so you don't want to miss what is
underneath when you have the menu
popping in so a few things that we did
that I did for example was have
transparency so the man is going to be
transparent enough to let me see what is
behind but it's actually providing me
with some information so
semi-transparent bar very easy to do
animation extremely easy to do we're
going to see how animations are where
one of the trickiest thing to build for
this demo for example when you're
playing poker there is a lot of
animations going on you're just hitting
the players with the cars there is a
sequence that you want to finish a
sequence that you want to follow all
those sort of things are really really
easy to program using javafx wanna be
able to customize everything so you
either can use the CSS for example the
bar is actually using CSS to get a nice
style for that panel so it's making it a
nice gradient linear gradient is doing
it nice the little border I don't know
if you you actually notice at the top of
the water is kind of a white line that
is giving you the idea the impression of
being a little bit pop out so you can do
things like that reflection so you see
that each icon has a reflection I don't
yeah you can actually see it a little
bit especially with the coins that they
really a right-hand side some
functionality very easy to do push
effect when you select one of the
bottoms things like that so you can
really go into great level of details of
how to customize your screen for for the
menus as I say just an animation we do
have javafx provide you with our two
ways to create animations one is through
a timeline and basically in the time
like you have to specify what is going
to happen over time but because there
are some animations are very common we
also provide you with what we call
transition so for example if you're
doing translate transition translate is
something that you do translate soon
in play with the opacity of the
components so those animation that are
very common we actually provide you
already with the transition so you don't
have to create timeline and specify
exactly what is going to happen very
simple you create a transition in this
case translate transition and you're
gonna set from where do you want to move
to where in this case if I'm using the
many of the button I'm just playing with
the y coordinate and from down right
good thing is you can actually use the
same animation for showing and hiding
things so one thing with the animation
you can actually play with your weight
so when you say play rates etre 21 you
just play normally forward you can set
it to two three or fours they go faster
right if you play with minus one you're
going backwards so it's the same
animation that i'm going i'm using for
showing and full height and i'm just
going to opposite the rivers so actually
very useful a great way to reuse
animations once they are created then
again the components you can play with
opacity transparency is really cool
effects really easy to do and as you can
see Simon and I are you know we like
programming I mean we like creating
demos maybe not very useful but we love
creating demos so one thing is it's the
symbol there is a great tool to easily
create user interfaces I never seen
anything as easier I mean I don't think
I can get any easier so here 11 sample
11 points that I want to hit for example
I have the image over there you can even
set the effect on sim builders so forget
about coding or going through the
objects anything like that through the
theme builder you can select the
component going to affect and in this
case for example you're creating a
reflection I with the settler sliders
you can actually set what is the opacity
of the top at the bottom what is the
fraction how much of the component you
want to see reflected and stuff like
that but sim builder is going to do is
going to generate all the F XML file
there's just a file and dart file gets
loaded into your applications you know
you normally have F XML load
it's just one line you call the f xml
loader load this file that was generated
and there you have everything is
actually generated for you inject it for
you so you're just injecting the
components into your Java code so that's
all it is fantastic i really enjoyed
again so a lot of animations a lot of
things like that also for a styling in
this case we do not have a lot of
components that we need to start pretty
much your graphics and animations but
for the components that we do have we
actually use in CSS for this style so we
have for example a master color very
useful because what i normally what we
normally do is the styles that we define
are going to be based on that default
color so meaning all the gradients and
all the what i'm going to do is take the
black and either go lighter or darker so
you can have a different kind of shades
but all based on block if i change black
for example to read without changing
anything else in the code you will get
the second bar that you see the second
image over here so very powerful style
your applications and then you have dark
corner this is how I define the bar menu
is just a linear gradient using some of
the functions that you have on CSS for
java effects like derive so what we're
doing is I'm getting the black and I'm
going lighter so things like that so you
can actually really start change the
style of your applications right so
Angela did all the hard work in terms
building the the actual application that
we're going to show you so she did all
the javafx side of things what I then
had to do is take the the work that I've
been doing with the open and I libraries
the gesture recognition and the connect
and say okay how do we link that
together so that we can take gestures
and interact them interact integrate
them with the javafx application so
effectively what I did was built like a
small library on top of the open and I
stuff to give the kind
information I needed for the code that
we were working with and use the the
library code handle the gestures and
everything wet from that the way that I
did it was to say okay if we're dealing
with non gestural hand movement so we're
just moving our hand around in front of
the kinect then we'll treat that as a
mouse movement so we still need to have
the ability to to move a mouse around a
pointer around so that's quite easy in
job because there is the robot class and
that allows you to say okay I want to
move the mouse pointer to this
particular point and you know you can do
you can simulate mouse clicks it's all
about testing user interfaces but it's
actually very useful if you've got
something which is not a mouse and you
want to simulate the idea of a mouse so
you simply use the robot class to say
move the pointer to a position X Y and
that works quite nicely and then it's
like ok so how do we process the
gestures that we're getting from the
open and I libraries and turn that into
something that's useful for the
application we've got so the way that I
worked it was I said okay if we do a
push gesture we'll treat that as a mouse
click so if I need to click on a
particular part of the application I can
move my hand around and then by doing a
mouse click or push gesture I get the
equivalent mouse click and then for the
different parts of the game will use the
in this case I'm just using swipe
gestures to indicate which particular
part of the game I want to affect so in
terms of the events that you get from
the swipe gesture you get the direction
as we saw with the arrows so you get
left right up down and you can treat
those differently depending on what you
actually want to do with them so that
was all good this was about sunday we
were at this point so everything was
good so we had had the library the
kinect stuff where they open had the
javafx stuff that was all good so all we
had to do was bring the two together
piece of cake ah so it's never easy as
it
so we define an interface we do this a
long time ago when we first started
talking about this we said right we
define the interface that we're going to
use for the application so Angela knew
which messages I'd be sending to her to
control the game and I knew which
methods I'd be calling based on the
gestures in order to control the game so
that was easy then obviously Angela
wrote the JavaFX code I wrote the open
end I gesture code and then we just need
to integrate the two together CZ yes
actually no this is where we ran into
slight problem because what I found was
that on I'm running on a Mac I'm
actually running windows 7 on my Mac
what I found was that when i had the
open and I library running on its own no
problem when I had javafx running on on
its own no problem when I put the two
together in order to have the open ni
libraries not interfere with Java
effects because the the application
thread in Java effects I needed a
separate thread to run open and I so
fine Java has new thread runnable great
so I create a new thread runnable with
my code in it and I start the
application up moment the opening I
libraries going to try and get some
information boom jvm crashes every
single time some native code somewhere
we were way down in the open ni
libraries I have to say not the JVM was
doing some sort of memory access in
combination with the JVM and the the
javafx libraries and the native code
under there something was a weird
combination that just crashed the system
every time so I'm then faced with the
situation on sunday with ok so how do I
make this work if I can't do it using
you know a thread and doing it that way
so I came up with a solution which is
there is always more than one way to
skin a cat and so basically I wrote two
applications one was what i called my
gesture sender and that is a just a loop
that sits there looking at the openni
libraries looking for gestures and then
it sends it through tcp/ip over the
route back to a particular port and I
listened on the JavaFX code on a
particular port and so that way when I
send a message I pick it up on the other
side
and the to address spaces remain nicely
separated and therefore I don't get a
jvm crash so let's move on to the final
demo I had to put here that's me yes yes
actually i said before i go i do have to
thank andrew a lot this is andrew
davidson who has written a wonderful
book about kinect hacking and a lot of
and his his book was very very useful
because it he resolved a number of
things in my mind in terms of the way
that the code worked and like I say the
Java API is not well documented and so
if you want to buy his book you will
find there's an enormous amount of
information there which really does help
in terms of using those API so I really
do have to say thank you very much to
Andrew for that yesterday I went back
and watched a little bit of minority
report because it's always fun to watch
that sort of classic bit of film of Tom
Cruise they're not Simon Tom Cruise
doing the wonderful thing of that you
know the sort of wiping things around
and stuff like that so let's move on to
the final demo see if we can make this
work
right so so the first thing I have to do
is I have to run the the gesture
generator so this is the the other
program this is the one that actually
just basically sorts out the connection
so it says the moments attempting to
connect to the listener and so I just
did it that way around so that I didn't
have to start them both up at the same
time I can start one and then the other
so it's just sitting there trying to
connect to a port on the other
application so if i run the casino
application now hopefully look good x so
yeah term about you so the first thing I
have to do right so I've got the mouse I
now control them houses you can see
many comes up I man
and so we've got a fruit machine and
what i can do is i can use a gesture to
indicate that i want to see you pull the
lever of the absorption if i do a swipe
down we get a crew team that runs and so
obviously with the jabra tax papers
actions and okay we want and then notes
return to the previous screen go back to
that
black captain
so bring up blackjack now what i can do
is i can say okay so i want to let this
player
that this plan
and then what I'm going to do is even
start the game so these are all the
animations annually
is that correct
ok we make just like program nope oh ok
don't quite know what I ok all right
I'll run that one more time yeah we did
you know you've been plagued by a
nullpointerexception so let me just
that would be okay let's just run that
again
that's good
ok so now they
what does it always happen yeah they've
not been good to me okay so I'm afraid
we're out of time I can't write it one
more time if you want but I think you
get the idea of what I was going to do
the gesture basically it was going to
use two gestures one was to do a twist
which was just an arm down and one to do
a stick which was an arm up and then the
game would play I don't quite know that
seem to be anything in there that's
stopped working but anyway so thank you
very much I think we've got two minutes
of questions if anybody's got any
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>