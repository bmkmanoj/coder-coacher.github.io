<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java Workload Optimizations in the Cloud from salesforce.com and Intel | Coder Coacher - Coaching Coders</title><meta content="Java Workload Optimizations in the Cloud from salesforce.com and Intel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java Workload Optimizations in the Cloud from salesforce.com and Intel</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DYEcgFpG_3E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay Neil's twisted my arm to kick
things off here so nice to see we've got
some people here on a late Monday night
especially when we're competing with my
chicago bears playing on Monday Night
Football so I'm a little disappointed
that I have to be here not able to watch
the game so hopefully I'm not too
distracted by wanting to step out so I'm
Charlie hunt I'm the performance
architect at salesforce.com and if you'd
asked me what the job responsibilities
are of an architect is pretty simple
it's to keep people from doing something
stupid it's just that simple onstage
with me here is I've got an eel kumar
from Intel and anneals of performance
engineer Java performance engineer a
little bit different perspective than
what perhaps most people have with Java
performance because especially for
instance myself and what we do at
Salesforce is we look at things from the
Java level or application level looking
downward kind of what I would describe
is from the top down Intel and an eel on
the other hand look at it from a
different perspective they look at it
from the hardware looking upward I know
what I call a bottom-up approach so it
turns out the two different approaches
sometimes complement each other and
other times they identify different
sorts of issues so what you're going to
see here from us anyway is will give you
a sense of kind of how this works and
then we'll also give you a sense of the
sort of issues we found by working
together so just by a sort of
coincidental sort of interaction we
started working together on performance
issues so that's kind of how things got
started and as I mentioned we will give
you a sense of the things that of what
we have found thus far and working
together and hopefully there's something
in this conversation that will have with
you tonight which you're free to ask
questions as we go along we actually
kind of hope that this will end up being
pretty in a
active so feel free to ask questions as
we go along so here's the agenda so a
little bit of background which I've kind
of covered and of course the
introductions and then we'll give you an
overview of a little bit more detail as
far as the depth of what it is that
Intel offers us at Salesforce suggest in
general maybe to other companies who are
engaging with Intel in a similar way and
then also i'll give you a sense of what
that Salesforce architecture looks like
and then obviously certainly some
interaction here with some questions
from you guys so as I mentioned this
kind of interaction of us working with
Intel started with this clinic
coincidental sort of interaction we at
Salesforce have a very large number of
applications and deployments and we're
always striving to deliver the best user
experience intel has a lot of specialty
and a lot of skills in the way of tools
and optimization methodologies and also
the other thing that they help us with
because our deployment runtimes our
Intel architectures they can help us
with the expertise that they have on the
Intel Architecture and ultimately the
goal for I think both of us thing is
true for both of us is is to deliver
that best experience with that hardware
so a little bit more about myself as I
mentioned I'm the architect performance
engineering at salesforce com a year ago
at this conference there was a book
called Java performance that was
released I'm the lead author of that
book if you happen to have a copy or are
you planning to buy a copy there is a
book signing tomorrow morning at eleven
o'clock at the bookstore so I'd be happy
to sign one if you have one with you
tonight I'd be happy to sign one here
for you before you leave and lastly at
Salesforce performance engineering if
performance is your passion and you got
a heartbeat will hire you that we're
hiring
we tend to grow the performance team
rather dramatically over the next
several months so if this is something
that's very interesting to you we're
hiring so with that I'll turn it over to
an eel thank you so this one just to
give you some idea that what we do at
Intel from the Java site so you might
have here a lot about the hardware but
we walk across the complete software
stack this is for performance team so
starting from hardware to OS to Java
Virtual Machine and in Java Virtual
Machine any component to the java
application to the extent that let's say
in your profiling we'll talk about later
for example class libraries you find
that compression or security or
something is taking a lot of CPU cycles
we have the in IPP libraries intel
perimeter performance libraries and we
are even ready to work through you or
some other components so you can benefit
with the java for those liabilities and
if that is a big component you could see
some time big performance gains this is
just what are the different thing we
work so the intel java team perspective
we start from the when hardware is not
even released the hardware which is
going to come in one and a half year we
are walking now to make sure different
variety of applications are running
smoothly so that's the first part then
we start walking with the JVM vendors
that the new features are coming in the
hardware the JVM benefits from the day
one so we start working pre-release and
later we need to make sure that the
tools you will use to debug and other
work that those are working fine and
then as we come towards the release
point we start walking on the
application stack all so different let's
say Oracle a pebble weblogic or the IBM
websphere different application stack
different event workload are working
really good and the last we make sure by
the time end-user start using it and
there are many times you will notice
oviya be upgraded to the new hardware
and we haven't seen the performance gain
we were supposed to expect and that also
we try to work and lastly we try to
understand from application behavior for
example from the
else for how are they using the system
can anything we can learn from it and in
two years time frame do it in the
hardware or three year time frame do it
in the hardware so there is a hardware
acceleration for it so those are the
wide range of work we try to work
through now this is just some idea on
the application stack that if you are
really into the performance sensitive
areas particularly the response time
then many components shown here going to
impact you for example starting from the
turbo and give you performance power
management to give you power saving at
the same time not impacting your
response time sometime it could
prefetcher and there are other bios
settings hyper-threading most of the
time gives you the performance gain some
time thirty forty percent but if you
have a content lock case then you might
may want to be aware of it next is
number of course so when you get a new
hardware it has usually more number of
course and if you application is not a
scaling to the number of core you may
not see the pokemon gain from it the
next ones are there are many processor
excuse so in the olden generation you
it's possible you have higher frequency
skew let us say three guards and the new
one with more number of core came at 2.6
suddenly you might see slightly almost
no improvement in the response time but
you can end a lot more throughput so you
need to be aware how you are defining
your performance criteria is it through
port or is it response time and/or in
both and the other cpus and then lastly
is the memory capacity always impact the
performance and if your application
memory needs start being becoming bigger
then you want over the memory strapping
you need to make sure there is enough
memory for application so there are many
aspect of the hardware which you have to
worry in the production environment as a
application writer for functionality you
may not but by the time it goes into the
production there is a lot more involved
in that space and adjust to get some
idea that are you most anyone who does
interact with the hardware here just to
know that what is on my system pretty
much man most of the end of programming
just so this one charlie was already
talking about that from the application
and top view versus bottom view and one
of the things happens that you might see
at the jvm level many profilers what JVM
can do but problem is there only certain
number of trust on it one of the view
the hardware counters from bottom up
gives you the confidence level what you
are seeing in the profiling is matching
with the hardware behavior if not that
there is some where is they are not
correlating and then e2 investigation so
that's the extra the confidence we have
working from bottom up to we can try to
correlate things why they are not
matching so I think from that if you
have any hardware questions I will be
glad to take and talk otherwise we can
talk to a Charlie work through
application detail so one thing of
interest is I think the hardware with
respect to servers is increasing the
same we have a Java deployment other
things are increasing and couple of year
ago it used to be the big monolithic
application server need to scale to the
large performance now that side has gone
into the cloud or many deployment where
it need to scale up while using cluster
approach so with the virtualization that
technique is changing for the
performance rather than single
monolithic distributed and are you able
to scale up or what kind of response are
able to give so all these expending
areas I think there are a lot of
opportunities to optimize them different
way with a different set of challenge
and so that I think that new area is
pretty interesting something
yeah disappears in terms of tourism
hardware
when you say virtual machines running in
a virtualized environment actually no
i'll explain this a little bit in over
the next several slides so before i talk
to this particular slide it's worth
talking a little bit about the general
architecture at salesforce.com so
Salesforce uses a multi-tenant
architecture so what multi-tenant
basically means is imagine a huge
apartment building and each one of these
units is some customer that we're
leasing out some space to rather than
given each one of those units a
virtualized environment to run in they
all share the same environment so we
need something to identify each one of
those unique units in that apartment
building the way that we do that from a
data standpoint is their assigned an org
ID so from the managing the data so when
you get down to the point of saying okay
I need to go to the database to find a
given customers piece of data the way
that we isolate that is by an org ID
everything else when you're talking
about an application server every
customer on that given deployment piece
of hardware shares that same application
server so that's kind of a key piece to
understand I think an easy way to look
at that would be to take each customer
and give them a virtualized environment
with some kind of guaranteed s la's so
in this multi-tenant architecture you
also have to have this sort of SLA s
that say an throttling mechanism that
say you only get a certain amount of
these resources that are available on
that given platform it's just like
saying in that a huge apartment building
for that given unit that person can't
hog all of the hot water for instance
you got a limit on how much hot water
you'll assuming you got one hot water
resource that shared amongst everybody
in that apartment in that apartment
complex so you'd have to have some
mechanism that controls or some
throttling mechanism that says you only
get this much hot water on a given day
so that's a you know so
multi-tenant architecture with
Salesforce is something you can find out
on the internet so if you do a search
for Salesforce and multi-tenant
architecture you'll find a lot of
information about how this stuff is as
assembled together so the main
deployment mechanism for Salesforce from
a standpoint of okay we got all of this
infrastructure to support these
customers that are coming to us with
their applications are running the cloud
so that deployment abstraction is
something that's called a pod so within
one of these pods at a very high level
you have a set of application servers
and a couple of other processes that run
with in that pod and then they
communicate to a database cluster on the
backend that's kind of a very high-level
simplistic view of this as more and more
customers come online you essentially
take one of these pods and you just
start stamping out additional instances
of these pods as essentially the
deployment and scaling model so within
one of these pods is probably useful for
me to talk about how the data flows from
a customer's perspective so you heard me
mention that customers are identified by
this thing called an org ID so a given
customer makes a request to an
application server that's geographically
close to their location and that's where
they log in from and once those
credentials are established they figure
out okay where does my data actually
reside on what given pod so once they've
got that information this information
then goes down to a given pod it talks
to a load balancer and then some request
is made
it gets directed to or load balance to
an app server assuming that the data
requires something that's going to come
from a database which is probably the
case in 90 some percent of the time goes
to some rack node some database node
where we pick up that information and we
send that result back to a given
customer so that gives you kind of a
high-level sense of what that deployment
architecture looks and how data flows
through that that architecture so that
can lays a little bit of the groundwork
of the types of issues we might see from
a performance standpoint some of the
challenges we might see so in some of
our initial observations that we've
gained by working with an eel and the
folks at Intel is of course there's many
potential opportunities much easier for
us to spot them to go off and make those
changes and do the implementation
changes so the next three that are
listed here are where we see the biggest
bang for the buck one of those is in TLB
misses so a TLB miss is basically you're
looking for this object reference you're
looking for this object reference which
is a virtual address and need to do this
translation from a virtual address to a
physical address and memory well there's
a cash that's set aside called a
translation lookaside buffer where this
translation takes place and it's got a
fixed size and the size is dictated by a
certain number of entries in that cache
and the address range of a page size so
the solution to reduce the number of TL
B's is to increase the page size so most
operating systems or most underlying
hardware has the ability for large pages
so the solution here was to reduce TLB
misses is to increase the size of the
page because on a given entry in this
cash you have a wider range of addresses
that could be covered for a given entry
and in that cache
so I'll talk a little bit more about
what we found with working with a larger
page size which kind of kind of an
interesting thing that happened there
just in general the sort of CPU cache
miss sort of thing so you could imagine
that in a multi-tenant architecture
knowing that you got getting data
request from a large number of different
customers you would expect that that's
going to be starting to hit things all
over the place as far as memory goes so
you would expect that CPU cache misses
to be somewhat of an issue so I'll talk
a little bit about some of the sorts of
things that we think we can do to help
address that that in the third
observation is memory locality so many
of and most of most most modern hardware
uses a NUMA architecture non-uniform
memory architecture but that basically
means is when you go to access a piece
of memory it may not take the same
amount of time to access that piece of
memory depending on where it's located
relative to the hardware thread or the
CPU and which it's running so depending
on where it's located it may be what's
called a remote access to find that
piece of data in memory or maybe a local
one so there's some things that we can
leverage from an operating system
standpoint or at the JVM level to help
us with memory locality okay so back to
the TLB misses thing so this was kind of
a can of worms or perhaps Pandora's box
so Red Hat has a feature called
transparent huge pages well it really
doesn't work very well we've got some
work to do there to make transparent
huge pages work well well you can
configure huge pages explicitly through
the configuration the operating system
through this thing called an hour number
of huge pages well it works kind of what
we observed is we saw an issue with red
hat five dot five
on jdk six you 21 it took us quite a
little while to figure out what the
issue was and the observation and the
symptoms that we were seeing is we were
getting these very unusual null pointer
exceptions we'd see a situation where
you'd say object not equal null and then
we'd say object dot some either an
invocation of a method or some access to
a field and it would throw the exception
on the line of object dot method name or
field name we just checked if it was not
equal null ahead of that so we refer to
it as the impossible null pointer
exceptions through several weeks maybe I
should even say several months of
analysis we figured out that the culprit
was large pages the problem went away
when we migrated to six update 33 we
also noticed that a symptom was that the
problem went away when we disabled large
pages we also noticed that on redhat 690
the issue didn't occur it actually took
us several months to reproduce the issue
the way that we reproduced is we tended
to see this sort of issue at JVM or
application server launch time so we
just in a sandbox we just started
relaunching the app server over and over
and over again until we would see it and
then once we got to this point where we
could start to reproduce it we could
start to do some forensics on it it
turned out to be what the issue was is
the issue was that there was missing a
protected page at the bottom of the heap
and it turns out that that fix was made
somewhere between 6 update 21 and 6
update 33 so the workaround for us was
the disabled large page support and in
fact actually if you disable large page
support it doesn't matter whether you
use use large pages or not because it
essentially will get ignored
so in the area of CPU cache misses we're
just starting to brainstorm and analyze
and this is usually kind of the sort of
non-trivial sort of brainstorming that
you do and what I kind of listed out
here is I try to order this in the way
war I think you'd probably get the
biggest bang for the buck and reducing
cache misses at least from our
perspective in our application and one
of those is with object type integer and
long and the various different you know
mappings of primitive types to their
corresponding object types they take up
quite a bit more space than the
primitive type themselves so any place
where we might happen to be using a
object type integer or object type long
replacing that with the primitive type
so that allows us to get more
information on a CPU cache line so we
get better cash utilization another
place that we can get big bigger bang
for the buck is in the area of looking
at alternative algorithms you know are
there alternative implementations where
we're seeing a lot of cache misses or
maybe we can take advantage of better
locality is there an order in which
we're accessing fields that might help
is there an alternative approach or
alternative data structures that we can
use that maybe don't take as much space
or something that might help us improve
locality and the last though ace here is
this is something that's probably not
widely known amongst Java developers is
the order that you declare fields in a
class has an impact on how they're laid
out in memory so the way that these are
laid out by the JVM by default is it'll
take your static primitive types and lay
those out first then after that it will
lay out in memory the static object
reference types then it will take your
object class instance non primitive
types followed by the non static object
reference types and then what I would
suggest people to do is to declare
anything that's an array after that
because arrays are going to be laid out
in memory and if you think about the way
that things are laid out of memory
and a CPU cache line if you have this
array there and then you need to go
search and find that reference after
that array you've got this big span of
memory there so you tend to get better
CPU cache behavior by laying out your
your fields in this order unless you
have a situation where you know that a
pair or maybe a couple of fields tend to
get reference most frequently together
then you can tend to group those
together in your class declaration so
these are the sorts of brainstorming
ideas that we've been talking about to
help us improve in that area you can the
you can use the retune profiler from
until there are one which you can for
two weeks or more than the talwars and
indifferent walks then you can provide
what you do there there are several
devil profiling if you select here for
memory profiling it will do the
profiling on it and it will suggest
based on if the cache misses are high
rate it will suggest you which area it
is high the tricky part is a bit
correlating in the source code it will
show it will show you assembly and even
the method name problem is with the Java
inlining you need to be little bit aware
that are those type of objects are
really in this matter
so you can find them and tool will show
you the little bit more black magic in
making sure that where is it coming from
because the jawline lining if you
disable in lighting summer learning
still done but it could have bigger
performance impact but that might be
little easier if do it mrs. are still
happening JVM has an optimized it then
you might be lucky doing it like
low-hanging fruits in that way so in
essence what you need is you need a
profiler that can profile with hardware
counters and there's a couple of
profilers so intel or a neo mentioned
intel vtune is one of those Oracle has
one called the actual official product
name as Oracle Solaris studio
performance analyzer it's unfortunate
the name has Solaris in it because it
also runs on linux my impression of
working with them is I think vtune and
performance analyzer work very well in
the linux platform if you're on the
Windows platform vtune is going to be
your choice because the performance
analyzer doesn't run on the Windows
platform and then if you're on solaris
you would use performance analyzer
regardless of whether that's x86 or or
spark AMD also has a similar tool that's
specific for the AMD architecture and if
I remember right there tool is called
code analyst so those are the profile is
essentially what you need is a profiler
will give you Hardware counters okay so
memory locality so really this is a
matter of understanding what are the
memory access patterns and this is
really difficult to predict in a
multi-tenant architecture because it's
somewhat unpredictable what those access
patterns are going to be what you can do
is as you start to look at user flows
you know so if you're talking about
agile development these are the sort of
stories and you look at that code flow
you can start to predict what a given
user stories memory access pattern is
going to look like or if you're in the
traditional rough model were you looking
at use cases you can kind of see what
those memory acts
patterns would look like so you tend to
focus at that area so you heard me
mention earlier the non-uniform memory
architectures Numa so you can leverage
the capabilities at the OS and JVM so on
Linux you can use this command called
Numa control so you can tie a specific
JVM to a cpu and memory node what that
basically says is that JVM or that given
process is not going to access memory
away from that binding of CPU and memory
node so you leverage those that
assignment of local memory to that given
CPU and then in hot spot there's a
command line option called use Numa so
if you have a situation this has always
been the recommendation that I've given
for using use Numa if you have a JVM
that you're running and that JVM spans
multiple memory nodes and it's a NUMA
architecture then use use Numa otherwise
don't worry about using use Numa at all
what ends up happening is if you use
Numa control and if you use use Numa
command line option at the same time the
JVM will still go off and do some things
with remote memory so the thing to
remember is if you're using anything to
isolate a given JVM whether that's with
Numa control or if you're on a different
operating system and isolating it to
processor sets don't use that command
line option use Numa easiest thing to
remember we don't have any conclusions
as to what the performance benefits are
going to be here yet we haven't done the
testing or we're not completed with that
testing just yet but this gives you an
idea the sorts of issues that we're
seeing that intel has helped us to be
able to identify and i would generally
say that like most people are kind of in
this category of you don't have the
expertise to take a look at the
application from the hardware on up and
that's what intel has been able to help
us with and to be able to leverage that
expertise example
this case at the nothing in the software
will show you but hardware has the
counter that how much memory traffic is
going cross so again on the beat you in
profiler with a particular type of
profile for memory will show you how
much cross traffic is happening and if
there is a lot of cross traffic then you
definitely will benefit by using you if
you can there are certain conditions in
your application where it's not possible
for example you have two CPUs and yes
you know most of the time it will use
only one cpu but in the extreme load
case you want that application to you be
able to both of them then that's a
tactical or strategic decision that when
I want full backup lord I can't just
restricted to one because once you
restrict it you can't let it go on both
so it's a bit difficult but if you know
that one CP is enough you're running
multiple applications and at the most i
need only one then you better off by
restricting it to that signal unless you
have other reasons to leave it free flow
recent
it's actually it's a combination of both
the combination both i would actually
just kind of summarize it as we're
trying to get the best performance out
of our hardware and really that kind of
lays to our summary here that it's time
consuming to do this sort of thing but
it's worth the time investment because
the benefits are in the performance and
when I say performance I wouldn't just
say you know throughput but also
reducing jitter and where you get better
predictability what I think one thing
that's probably be even more important
is the cost savings that we have for our
data centers so as we can get more and
more performance out of our hardware if
that prevents us from you know as we
maybe we don't have to scale to the next
pod because we can take better advantage
of the existing hardware that we have it
saves us money plus the software
licensing costs the more mostly you babe
I'm sure some might be using this but
mostly they are poor poor basis and
doing licenses so if you can get more
performance out of single core you you
can get more money for it without buying
new license</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>