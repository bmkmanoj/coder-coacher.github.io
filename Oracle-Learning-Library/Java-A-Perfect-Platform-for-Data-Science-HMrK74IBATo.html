<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java: A Perfect Platform for Data Science | Coder Coacher - Coaching Coders</title><meta content="Java: A Perfect Platform for Data Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Java: A Perfect Platform for Data Science</b></h2><h5 class="post__date">2013-01-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HMrK74IBATo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everyone thank you very
much for coming to our talk Java a
perfect platform for data science just
curious how many of you know what data
science is a few of you how many of you
do Hadoop programming a few of you if
you do Java programming just that's good
that's good
all right so thank you for coming so who
are we
first of all let me introduce Alex this
Alex alex is a senior software engineer
at Verisign he's also a proud author of
a new book called Hadoop and practice
which is which is released already I
think right and he's going to be doing
book signing right after this
you can email him at grab dot alex a
gmail corrupt or Alex crap that sounds
pretty geeky Alex so anyway you can even
like grab Alex at gmail.com or Twitter
add crap underscore Alex or crap Alex
calm is this blog right there all right
thanks Karthik I'm going to do some
Catholic Samsung over here he's a
principal I Verisign he's a member of
the adjunct faculty at Johns Hopkins
University ability to the variety of
courses including disability computing
and a new one we just kicked off on
Hadoop which is pretty exciting his
email on Twitter information is also
there I'm not going to repeat it because
I can't under one of mutually and last
name for second times all right so what
is the overall goal of this presentation
we actually have two goals first to
clearly articulate this new field this
emerging field of data science that's
happening in the marketplace the bus
that's around there and then to
illustrate the many technologies that is
built on top of JVM that is built on
tough Java that allows you to practice
data science when I talk about data
science people immediately say oh yeah
we do that in Python you know it's a
great platform but my goal here is to
convince you that Java is actually a
great platform for us to do data science
so here's the overview first we're going
to talk about the data revolution then
we're going to talk about data science
in theory then we're going to talk about
data science in practice using many of
the Java technologies that's available
in the
marketplace that's where we're going to
be spending most of our time and then
summary hopefully if time permits we'll
do some Q&amp;amp;A at the end okay so let's
talk about this data revolution
unless you've been living under a rock
for the last few years you heard the
term data you've heard the term big data
some of you may even heard the word data
science and in fact many magazines have
already started talking about this
this is science magazine talking about
data
this is Time magazine talking about your
data for sale talking about privacy and
then you have the Economist magazine
talking about the data deluge if the
Economist magazine is talking about data
it must be something that's important I
guess so you know if you really think
about it the data volumes are really
growing it's a it's a fact and the
reason I've represented the data volumes
as an iceberg is because if you actually
look at the iceberg a little bit closely
you realize that although iceberg is
pretty big pretty massive the reality is
that most of its volume is right at the
bottom it's pretty much the same story
with data too right there's a lot of
data in the marketplace there's a lot of
data that's coming in but that's just
the beginning the next 10 years is going
to be a lot of stuff that's going to be
around manipulating data or data
analytics data intelligence and things
like that so it's important to
understand this so there's data
everywhere enterprises are generating a
lot of data New York Stock Exchange
generates about a terabyte of data
Yahoo processes 24 terabytes of data
Facebook generates 22 terabytes of data
per day is a lot of statistics and
changing on a daily basis the question
is where is all this data really coming
from well if you if you look at your
enterprise you may have a lot of
existing OLTP databases you may have
several products within your company
right they're serving different kinds of
customers so in a bla you know in a
medium-sized enterprise you have all
these different databases you have user
generated data you also have logs system
generator data and and all this data
comes in different shapes different
sizes different forms different schemas
structures you know just to kind of give
you an idea as to where we are coming
from we work for Verisign and
to kind of give you an idea as to what
DNS resolution is but the point is what
we are really trying to do with this
stuff so if you're a user sitting in a
browser with a browser you want to go to
www.hsn.com you type in www.edmodo.com
is that dot at the end which represents
root so it's www.cnn.com dot so we need
you know it basically talks to the
recursive resolver and says do you have
the IP address for WWDC n and calm the
recursive resolver says I don't know
what it is but I can get the answer so
it goes to what is called the root
nameservers
the root name is services I don't know
what www.cnn.com is IP address says it
says I know whom I know the answer so it
actually redirects you to what is called
the calm name server and the stuff
that's in blue is what we run we run the
a and J routes and we also run the D
name servers for calm the dot-com says
WWE or cnn.com I don't know the answer
to that but I know who may know the
answer to that so it redirects you to
cnn.com name server cnn.com name sources
say I know the answer gives the IP
address back that IP address comes back
to your browser and then the browser
makes a request which is the HTTP
request to get the answer back lots of
stuff happening there the point I want
to make over here is that the stuff in
the blue with the step number four and
five is what we do we have the domain
name registrations for dotnet CCTV dot
name
TLDs and then we also have these
resolution servers we get about 65
million queries a day within a second we
get about 1.2 to 1.3 million queries
right so there's a lot of data that's
coming in and we want to take this data
and make sense of this you want to pull
in the data that's from the DNS
resolution you want to bring in the data
from you know from the registration
system from websites from links all
these things to try to make sense of why
people buy domains and what are they
doing with it just to kind of give you
an idea as to what's happening in the
marketplace there isn't actually a
change in computing industry in terms of
scale if I compare my PC from 1984 to
2012 I had a PC at that time 1 megahertz
was just 3 gigahertz right now know some
of you have gray hair you probably
remember that right 256 K RAM versus 4
gigabytes
fifteen thousand time improvement now
thirty bytes per second was just one
megabyte per second data transfer rate
when it comes to hard disk drive
actually the first computer I had didn't
even have a hard drive but I had to put
some value over there so it's not
divisible by zero so it's when megabyte
versus one terabyte of hard drive that I
have in my desktop at this point that's
about a million time improvement so the
point I'm trying to make is in our own
personal life there's a lot more data
that we have that we need to process
right all these pictures and videos and
all these things that you're taking so
there's a lot of data that's just
personal life think about what happens
in an enterprise and enterprise you all
these different systems and you want to
make sense of all this data you know the
reality is that there's actually a shift
in the marketplace and I'm hoping to
show you this particular shift so if you
look at the the timeline of the
computing industry I'm going to break it
down into three different stages so
there was the first stage which is the
50s and the 60s where hardware was the
king right
you had companies like IBM they touted
their hardware hardware they were the
king right they and in software was it
was important but it was only important
to sell hardware to the extent that when
they actually came up with the IBM PC
they said we want to write the operating
system for this they gave it to Bill
Gates and he wrote it but they didn't
realize what had changed they want what
a change he's going to do and suddenly
software became important in the 1980s
and 90s it was all about writing
software and visual basic visual C++ and
selling that software everybody made
money that way the 1990s change give a
shift was about the World Wide Web it
was about ubiquitous access let's put
everything on the internet and now data
becomes the gang if you look at
companies like Google Facebook Linkedin
literally what exactly are they selling
to you really nothing they are giving
you free services but in turn they are
actually collecting data about you and
actually cashing it in the ad
marketplace so there's a lot of data
science kind of activity that they're
doing within the companies so what I
want to say is that there are two kinds
of organizations organizations that use
data effectively data-driven
organizations that are like Google
Facebook LinkedIn Twitter bitly which
basically use data to make their money
and that's all they do free services but
then
other organizations that actually Bogmen
their business by you doing data science
and data related activities analytics
like activities amazon.com is a classic
example I'm not talking about Amazon Web
Services I'm talking about amazon.com
they sell books they still movies but
then they also keep track of every time
you clicked on a link every time you
browsed a an item they do know they know
every item that somebody bought they
know that if this person bought this
item they also bought that item so
they're using all that stuff to upsell
and cross-sell so there's an example of
data science that's happening over there
same thing with Netflix the cross
selling that's happening they're showing
you all the different movies that you
might be interested in so wake up guys
this is the data economy we are in the
midst of information science
not long ago data what's expensive
storing was expensive there is no limit
to how much you value you know valuable
data that you can collect and we are no
no longer data limited at this point we
are actually insight limited just to
kind of give you an idea this is
actually a field this is actually
changing the entire computing industry
you know it's changing the biotech
industry the linguistics industry mining
finance journalism education the slides
are going to be available and what I
would like you to do is take a look at
some of these pretty you know
presentations and videos and I would
like to concentrate on one specific
video it's video on linguistics there's
a professor at MIT that recorded every
minute every second of his child's life
from the day the child was born until
the child was 5 years old because he was
a linguistics professor he was a
linguistics researcher and he wanted to
see how language develops notice even in
the field of you know regular normal
science you know a bio science he's
actually using computers data processing
to do this work so data analytics so
he's able to figure out how his child
started saying the word Baba - the word
water you know from the age of 2 to the
age of 5 so he's actually seeing how
languages is being developed so it's
pretty cool thing to think about it
that's video that's being that has to be
processed through some kind of a masses
and things like think about the size of
video that might be that so now let's
talk about data science in theory itself
right so what is data so what I did was
let's look at the dictionary so I looked
at the Merriam dictionary and one
definition is factional factual
information now we understand that
factual information used as a base of
reasoning that's transaction
every time you do a transaction that's
factual information but there's also a
second definition information output by
a sensing device or organ useful and
irrelevant or redundant information you
can think of this as access log files
for your HTTP in our case the DNS
traffic that we're getting 65 billion
times a day and that's information
that's data that we want to make sense
out of and then a third definition is
information in numerical form that can
be digitally transmitted or processed
think of that as data analytics right so
we need to do all these all these three
things so let's talk about traditional
data analytics versus big data analytics
you know you probably are saying wait a
minute we've been doing dinner at
extraor for decades now what's so
different over here well in traditional
data analytics you have clean data
versus you're dealing with messy data
and noisy data here there you're dealing
with terabytes at the data here you're
dealing with petabytes or the data here
you often know in advance the question
that you want to ask where as in big
data analytics and data science you
don't even know what question you want
to ask which means you know you can't
even design your BI system in your data
base data where systems around the
questions that you want to ask you know
there's no there's no designer I mean
you just store the data and then you
process it later on and alex is going to
talk about how you can use MapReduce and
all those things to process this
architecture doesn't lend for high
computation you've got single storage
single machine whereas here you need a
distributed computing and distributed
storage architecture typically answers
for factual in nature here the answers
are probabilistic in nature you might
you know get into an accident or you
might end up buying this particular
product structure data versus
unstructured data semi structured off
and I say unstructured data I mean you
know things like you know JPEGs movie
files and I say semi semi structured I
mean XML and HTML files you deal
typically with one or two domains over
here versus you deal with dozens of
different domains in big data analytics
for example at Verisign when we are
looking at the DNS traffic data for four
domains we are looking at the DNS
traffic data we are looking at the
domain registration data we are looking
at the different links that are
happening through bitly the through
shortened links we're looking at the
actual content we are crawling the
website downloading the content and
we're trying to put all these different
things together notice somebody needs to
understand
to understand access logs they need
understand websites they understand how
you're shortening words all those
different things to do data data
analytics so what is data science data
Sciences using data elements and clever
ways to solve iterative data problems
which when combined achieves business
goals there's another definition for
data science that I would like to talk
about this is from Drew Conway he
basically said there are three things
that make up data science one is hacking
skills which is basically programming
skills Java programming would be an
example of that doing expertise you need
to understand the domain for which you
are trying to do data science where it's
the DNS traffic domain registration or
whatever business you are in
then there is math and statistics
knowledge that's also important when you
do data science now when you combine the
domain expertise and math and statistics
it's traditional research researchers
have been doing this for hundreds of
years when you combine your programming
skills with domain expertise you get
into what is called a danger zone which
means don't ask your developers that no
Java and Hadoop to do data science what
you really need and then and then on the
other side if you take hacking skills
and combine with machine math and
statistics knowledge then you get into
this field of machine learning if you
want to do data science you kind of have
to do all three of them right you need
to have a combination so your team has
to be made up of all these thing people
so when it comes to data science there
are different tools that make up the
data science and the cool thing is that
Java has lots of products lots of tools
that's built on top of the JVM on top of
the java virtual machine that allows you
to do this for example if you're dealing
with big data processing you can do
hadoop mapreduce if you want to store a
lot of data you can use the HDFS of
course the dhoop MapReduce is built on
top of HDFS if you want to store data in
real in in big data for real-time access
you can use something called hp's and
Cassandra which output which I'll talk
about if you want to do data warehouse
type analytics on Big Data you have
tools like Pig and and hive that allow
you to do that if you want to do machine
learning kind of work then you have
mahout project the Weka project if you
want to do our statistics or like things
now there is actually integration
between R and Hadoop it's available and
if you want to do you know visual is a
we have we have JavaFX too but the
reality is that the difference between
what the tools that we have before for
doing the other science versus what we
have right now is the fact that scale
becomes really really important all
right so Catholic talked about some of
the systems that work well for data
science and what we're gonna do now is
we're gonna you know look going to
remember more detail we're gonna start
with looking at how they can scale
so remember Karthik's lied when you had
a big iceberg and it's kind of grown as
a date of omens are growing but this is
kind of my version of that I mean I can
imagine when this person started you
know their delivery business they fought
had one box and two boxes and the kind
of you know cycle there was probably
plenty big for connect transporting
these two boxes around right but as her
business grew as a number box that they
had to deliver grew you know clearly
that M that cycle they just isn't gonna
cut it right and and that's the case
today with a modern hardware about ten
years ago you know we could have
conceived that with the hardware had
then we could post ik all the data that
we cared about at that time onto that
one box but that one box just isn't
cutting up for us today so what can we
do about this so and this isn't a nice
new em Grace Hopper who was one of the
first and programmers in the Navy over
fifty years ago one of the first
programs to work with large-scale
systems even way back then she already
in connect saw the writing in the Ennis
and was saying look I mean I go
shouldn't be to go our single machines
the goal should be to harness the power
of multiple machines together and that's
really where systems like Hadoop really
come into their own right so em Hadoop
is based on papers at Google published
em over ten years ago around how it was
handling storage and how it was having
however supporting its M computational
tiers and what ended up happening was
Doug Carden enough how many guys of
heard of cotton but and he invented
listen
and at the time he was working on knots
which is an open source and web search
and and crawl project and he was having
scalability problems he actually read
these papers I good was published and
said it you know that would be a good
fit for solving his own scaling problem
so he actually implemented them that
partly became known as Hadoop him and
Hadoop moved into Yahoo and at some
point Yahoo decided to open source
Hadoop and OOP as a technologist I've
written almost entirely in Java and it
solves a lot of the problems that you
have when you're trying to design and
write distributed systems I mean that
there's a lot of challenges like that um
you need to solve if you're gonna do
that and the key thing here is I mean
not only has had to have made inroads
into the high technology companies that
you'd expect Facebook and Yahoo to be
using a but you know it's made inroads
and tighten into financial health
sectors and at this point pretty much
all of the fortune the top fortune 500
companies using Hadoop and some capacity
so that will give you a sense of cannae
inroads that's made so you know dr. ting
who created Hadoop he likes to call
Hadoop the kernel of Big Data the kernel
of distributed computing and when you
look at the Hadoop ecosystem I mean you
can probably realize why he's saying
that so first of all you've got a number
of other tools
I said of Hadoop which make it easier to
work with Hadoop and then beyond that
you've got tools either use Hadoop for
storage or the use of the connect move
data and from databases to Hadoop and
vice versa
and or the move layer log files into
Hadoop and Hadoop has really matured to
the point now where there's a number of
commercial organizations that provide
not only commercial distributions of
Hadoop
balls of support which is important for
enterprises and so what is Hadoop look
like in a single node well he's a single
node a typical mid tier racks over 24
cores for they gets a ram 12 terabytes
of storage nothing too surprising here
so this is Hadoop storage to your HDFS
and it provides a logical interface to
the Hadoop file system and then
internally organizes the physical
storage that you have on your server and
then a computational interior sits on
top of a MapReduce for the most part em
it's a distributed processing framework
and for the most part it's gonna be
reading and writing data by HD
on the same note and in the best case
scenario
so one Nord not particularly interesting
right but I mean if you look at the
numbers in the bottom right there and
see what happens
and when you add you know more notes
here's a train or cluster here's a
hundred node cluster right and then
here's a thousand north cluster and
there's no clusters actually do exist
and organizations such as Facebook and
Yahoo that's a lot of cores a lot of
memory and a lot of storage for you to
solve your problems with right and the
key thing here is that they're all
working together in parallel to solve
your problems so let's take a little bit
of a deeper dive into him the Hadoop
distributed file system so the first
thing I want to call out here is a fact
the fact that is written in Java
I mean who here ten years ago would have
thought any file system would have been
written in Java I mean III certainly
wouldn't have thought that and I think
kudos to the Java team really for
building you know and the garbage
collection and all optimizations and the
bytecode of my face optimizations and
allow us to actually have a file system
in job I think that's pretty cool so
what does what does Hadoop look like
what does HDFS do in this drawing a file
as maybe a one gigabyte in size so the
first thing it's gonna do is actually
gonna break that into blogs and for a
file system these blocks unusually large
the at least 64 Meg's in size and F not
large if you're dealing with larger
clusters and then let's imagine we've
got a fallen overhead rip cluster the
first block is gonna be written onto
three nodes the second block is going to
be written onto another three nodes and
so on and so forth and and there's a
couple of good reasons why there's more
than one copy of each block here right
first of all distributed computing a lot
of some of nodes nodes are gonna fail
right so you don't have only one copy of
data on your cluster but the second
reason is actually that of performance
when MapReduce is actually optimizing
and deciding where to push work mff is
an overloaded nodes in the system it can
actually push works in North out em know
so overloaded so having multiple copies
of data lets it do that so in terms of
group I mean the modern you know Dave in
rotating hard drive I mean the best-case
readwrite sequential read/write rates
are probably looking about
15 megabytes a second I mean that's
that's not bad but it's kind of like you
know the caveman trying to lay some fire
right it's not it's not particularly
impressive but you know obviously when
you've got a thousand of them and
parallel as 150 gigabytes are sick and I
just want to pause here for a second I
mean hundred fifty and get back her
second
I mean think how much data you could
read and write at that rate right that's
that's a lot of data
alright so let's switch gears here and
talk about MapReduce which is a
computational tier and Hadoop
so again it's written in Java and has
underpinnings and functional programming
you know and Google when they publish a
MapReduce paper they were the first
people to really am
talk about my producers a concept it's
existed in languages like Lisp for quite
a while now I like to call a
shared-nothing programming model I mean
with distributed computing one of the
challenges is that you got to be
constantly trying to defeat dolls Laura
are you trying to make your your work as
paralyzed as possible and and shared
nothing architecture that's a lot easy
to do then in architecture we've got to
do distributed lock and all I kind of
stuff and again the key thing here is
that MapReduce solves all the problems
you get with distributed computing right
I mean things go wrong all the time I
mean on a daily and weekly basis drives
go wrong you know and racks can go down
all sorts of cares can happen your data
centers so MapReduce handle that all for
you seamlessly which is actually really
important um so what can the things can
use MapReduce for well when Google
published a paper I mean at the time
they were using MapReduce to create
their web search index and but you can
do things like searching sorting graph
traversal created Lucene indexes and
machine learning so let's look at the
map produce functions and what they do
so here's a map function it's pretty
basic
takes endless tips and a key value pair
and emits a key value pair that
information gets ready to the reduce
function which again takes a key a list
of all of ice like key and once again
puts a key value pair so let's let's
take a step further and look at and the
situation where you maybe have one file
either multiple blocks here
and and the parallel nature of my
produce here is a fact that you can have
multiple map functions working on the
same file and parallel and the map
function you can do things like you know
filter projection kind of stand with
things you're going to output your
values and then you you know what's
really happening in the middle here is
what I like to call the shuffle phase in
Hadoop where there's a guarantee with my
producer for every unique key that's
emitted by the map function that each
value is going to supply to the reducer
and that list there and then and the
reducers very common to do things like
aggravation summarization before you
emit a fineliner parent so if we were to
compare relational databases which but
probably a lot more familiar with to
Hadoop and transmit you know however we
use transportation as an analogy for
that so a relational databases can like
a sports car right equals they were just
60 real fast and but can probably handle
and you carry about one or two
passengers was doing that versus group
is kinda like a freight train
but it can carry a large number of
passengers and but it's going to take
you know a while to get up to speed the
other way that the environmentally
differ is M religion database is much
more real time system Hadoop is more of
a bad space system relational images
they really excel at structured data
buses with Hadoop Emily can support
unstructured data as well as structured
relational databases for the most part
unless they were designed from the
beginning to be parallel that they can
to scale only so far versus with Hadoop
it's a truly linear parallelism that you
get as and when you add nordstrand
so a question that's often asked is you
know how does it fit into my
organization right so what is to happen
in a lot of M production applications is
that they tend to be siloed they don't
tend to talk to each other very well so
a great way of using Hadoop is a
mechanism to you know gala date and I
heard you join your data together and
then what you can do is you can actually
you know take learnings from that feed
it back into your TP database like you
can see on the left hand side here or
you can move some of the summarized data
into your OLAP database where you can
use existing BI tools as you do today
okay Alex makes up a really good point
about Hadoop how you can you know if you
notice the back drop has Hadoop in it
and you have all these which means
basically for your enterprise your
hadoop system which is bringing in all
the data and then you from all these
real-time systems and you process it and
then you do you write these MapReduce
programs and and alex is going to talk
some of the more examples in the future
as we go through the point I want to
make here is that you know yeah you can
do analytics like that which is really
good but we also want to do real-time
analytics real-time processing so how do
you do that well typically traditionally
the way we have done this is we would
load all our data into some kind of a
relational database data warehouse I
don't know if you were here yesterday
for the Apache Cassandra talk how do if
you were here for that talk right so
basically Jonathan Ellis talked about
how he has the analytics cluster so one
of the things that you could actually do
is use another technology but before you
do that I gotta do some database hashing
I know it's an Oracle conference but
what can I do
so database architectures have really
not kept pace the last 40 years it's
pretty much the same you have a big
storage a big you know machine that does
all the processing and you know we as
application servers know how to scale
applications and through the different
application servers but when it comes to
databases it's about you know growing
the database by out throwing more money
at it right so you scale it vertically
by you know more CPU more memory more
hard drive more storage all that stuff
throw more money at it but there comes a
point where teachers cannot do it and
this is all about big data analytics and
you just can't do it in a single machine
so you know what you really need is a
horizontally scalable database so where
you can add databases as you as you grow
just like the way we do it for
application servers it's time to take
over from the DBS I guess sorry if there
are any DBS over here the reality is
that there is actually a phenomenon
that's out there right now called no
sequel or no SQL which doesn't mean no
sequel it really means you know not only
sequel so let's go beyond sequel and
there are many reasons why it's called
that but you know what I want to do is I
talk about two very specific
technologies that's built on top of Java
MongoDB is also no sequel which potala
in C++ but to toughen our technology
that's built on the Java that you can
use today for storing big data and then
retrieving big data so one is Apache
HBase which is distributed sparse
column-oriented database based on Google
BigTable they were just like the HDFS
evader want the GFS paper it leverages
the HDFS distributed file system and the
other one is Apache Cassandra which is a
distributed sparse database eventually
consistent column-oriented database it's
actually a lovechild between dynamo
based database versus BigTable database
actually brings in the functionality of
both of them it was actually open
sourced by Facebook in 2008 two really
great databases we'll talk about the
details of that so let's talk about
HBase so we Alex talked about Hadoop and
you know it has its own internal
architecture named nodes and things like
that but I wanted to talk about HBase
HBase is built on top of hi tube it uses
the distributed file system which is the
Hadoop distributed file system and in
HBase there's something called an H
master typically when you have to scale
a database you know what do we do we
short it so each master is the master
short keeper and then what you do is
there are all these different machines
these different machines have what is
called region servers that they run
which is where all the different
partitions are kept right and now you
can have more than one H master because
you want you want availability but if
you have more than one each master which
one is going to be the master well in
this tributed computing there is
something called zookeeper which is an
open source project which basically
keeps track of and it's usually an
odd-numbered kind of you know the
supporting that takes place and
basically it keeps track of which is the
current master so in an age master goes
down the other one will be elected as
the current master so the idea here is
that you have a you know fully
distributed architecture for storing
real-time data on built on top of HDFS
and when a client wants to read or write
records it simply goes to zookeeper
figures out where the H master is then
goes to H master figures out where the
shards are and eventually talks to the
appropriate region servers which is
where the different shards are to do
reads and writes now all this stuff is
done for you automatically under the
hood
in the same way you've Cassandra you
know I like Cassandra from the
perspective that it's an architecture
where every node in the architecture is
equivalent all nodes are created equal
there's no master slave architecture
here so here the concept is that you
know you have nodes in a ring everybody
maintains a shard and when a client
wants to write a record it can talk to
any one of those things because all
nodes are created equal
it's just I want to write a record the
Cassandra diamond then says okay I'm not
the primary owner of that particular
record that particular key so it
basically proxy since you know attaches
to the Cassandra diamond that is the
owner write to the record over there
which ends up writing to the next two
nodes in a round robin fashion and you
got it done in the same way if you can
add multi data center deployment now and
you could do writes in both locations
and when you do when you write a copy of
it is also written to the other side and
replication takes place and if you if
you read the Cassandra architecture they
talk and great detail about this and
it's a pretty cool architecture but for
real-time data it's it's great for us
whether if it's even if it's within the
same data center okay so performance and
scale already very easy to improve
performance and scalability
you know you need more partitions just
bring in out the region server
automatically the sharding takes place
for you you don't have to worry about
that
when it comes to performance and
scalability in Cassandra you can add
another node as soon as you add another
node basically everything gets
automatically registered in the
background so you're able to scale your
Cassandra or architecture okay so now
let's talk about data representation in
general so we talked about scale both
MapReduce processing batch processing
and real-time processing let's talk
about data representation when you want
to represent data if it's do you know
you look you represent data in HDFS file
system which are really files and so
it's a flexible schema here even in
HBase it's a flexible schema which I'll
talk about if you have databases it's a
very rigid schema if you have to do
something you have to change the schema
and you know authoring is Devi table it
may take hours sometimes there's no
concept of indexes over here files can
be stored as CSV files PI files
Avro JSON whatever you want to do
different formats whatever format you
want to store your files and
and you know it's a pretty cool idea and
then if you look at HBase and Cassandra
they actually have this concept of a key
space witch and they have this concept
of column families and column families
have key value pairs and then you can
have in Cassandra super column family so
you know they have a you know it kind of
a very flexible architecture we can have
you know column names and on demand on
fly as you go through so you can have
families families or ones that don't get
added on demand but but the columns can
be added on demand as you go through and
there here's the data model that allows
you to do that so you have a key with a
bunch of numeric attributes that go into
the database
and then if you if you look at Cassandra
and Cassandra they actually have a
combination of Cassandra with solar
which is which is part of data stocks
that allows you to do queries sequel
like queries using something called CQL
pacific which is the cassandra query
language so you can actually do data
analytics how about it crashing
do you have your presentation on
what I'm going to do here just a second
because we have a yes Boise if you want
to swap it over okay just so in
distributed computing hardware fails no
problem in this architecture it you have
redundancy built into it and we wanted
to show that T over here that's exactly
the reason so that's what happens in
Hadoop in HBase and all those all right
so no shoot this all right good night
alright so we've so far looked at
scaling and how to do data
representation so we're gonna now look
at analytics crashing might as well hold
on one second let see we can continue
here alright so um let's talk about
analytics honest and am in a scope of
social networking so no social network
you may think you know well that's only
know only Facebook and Twitter need to
deal with social networking but what's
really happening today is that in a more
and more website such as toys are ours a
begin
use social networking as a mechanism to
you know can have better engagement with
our customers as well as to really help
customers communicate with each other so
what we're gonna do is we're gonna
assume we have a small social network of
five people and you know what kinda
interesting things can we find out about
a little network over here well and
let's look at Ali so Ali has a couple of
friends Joe and D and then D and Joe
also have a cup of friends Bob and cure
and so one way that we could keep Ali
more engaged in our website would be to
recommend Bob and here to Ali because
there are friends of friends right so on
a small social network like this you can
be can I simply love to come up with a
solution to do this but the question
becomes or if you have a how do you come
up with a solution is going to scale to
millions of users in a certain Network
right and that's where an algorithm
called friends of friends and
combination with MapReduce come into
play so the first thing we're going to
do we're going to assume that our
network is an undirected graph just to
simplify things a little bit so that
means that you know if car sex my friend
and I'm also Catholics friend right
Catholic thanks and thanks and you know
what we're going to do is and share
nothing frameworks like Map Reduce
really the best way of modeling or
undirected I graph is using an adjacency
list so what ends up happening is that M
each user will be represented as as a
node here and that node will happen
adjacency list containing all their
first-degree relationships so in a
MapReduce implementation winds up
happening is in a map phase we already
can figure out some of the second-degree
relationships and you can basically do
that by doing a Cartesian product of
everyone in that adjacency list and then
what we're going to do in the reduce
side is that we're going to filter any
first-degree relationships and at that
point you have the complete set of all
sake of all a second-degree
relationships and then we have an
optional last step here which is really
an optimization to can I give high-level
quality of results to to each user so
what we're doing here is oh you know
ostensibly the more friends I have in
common the more likely either you can
know that secondary person or you know
hopefully have some other
and common that will make you in a
prefer to add them to network versus
someone else so if we look at the map
function here if remember earlier the
inputs are map functionally value tuple
so the key in this case is I user and
the value is whitespace limited list of
your adjacency list which is that all
the users you are connected to so the
first thing we're doing then here is
that we're extracting all the friends
but organizing and we're going all the
we're going through all the friends am i
omitting then but meeting the fact that
there's a first degree relationship and
then we're doing at the bottom the
Cartesian product and omitting the fact
that this is a potential second degree
relationship because we don't know at
this point but it could be and then the
reduced side remember that all all the
keys and all advise their keys are gonna
be sent into one reduce invocation and
so what's happening here is that the tip
the key is really a tuple of m2 users
which is a potential secondary
relationship and the values is a list of
values one indicates first degree and a
to indicate indicated second degree so
what we're going to do is we're going to
go through we're going to look through
all the values there if we see how one
rustically relationship we don't care
about that we're going to discard that
for all other ones we're going to count
the number of friends they have in
common and then the very end we're going
to omit the fact that this is this
really is a second-degree relationship
and this is number of friends you have
in common so I'm gonna shameless plug
for my book now am I talk a little bit
more in detail about how this actually
works but in all seriousness I mean you
can actually the complete source code
for this is um get hub if you want it go
take a look at it so another thing has
happened in Hadoop is you know much like
with the JVM now you have a bunch of
different languages that kind of sent
off the JVM
the same is happening in Hadoop as well
so you know Hadoop is like cutting says
it's it's it's it's a corner and we end
up happening is that you have languages
like pig and height which is probably
the two most popular abstract language
that sit on top of MapReduce
they can either they have their own da
cells and what ends up happening is that
they translate the DSL that you write in
to my produce and mines
process for you so it's a lot it's a lot
simpler than working with the Java not
produced at a lower level so what we're
going to do is we're going to take a
look at an example of how we can find
bad IPS in your web block so here you
some Apache web servers they're
generating a bunch of access logs and
you know an entry and a third log
typically behalf information when the
client IP the path has being requested
and the HTTP status code so the first
thing we're gonna do is we're going to
get that log data into your Hadoop
cluster and there's a great tool out
there called flume which lets you do
just that I said it's a log and
collection and distribution framework
and then once you do that you can then
start using hive to issue some queries
against your data so hive and qlm a
sunray similar to sequin line too and so
the first thing we've got to do it just
like you go do in regular version
database is can I represent your data so
this is the syntax of your hive and can
like M construct here see we got create
table looks pretty familiar right um I
didn't find the name of the table give
you a bunch of field names the types for
each field name and the only deviation
here really from sequel 92 is the fact
that you're telling because hive doesn't
know how your data structured so you
gotta tell hive you know how your fields
and records are too limited so here
saying well my fields limited by the tab
character and by the way my data exists
in this directory in HDFS so let's write
a real simple query here to connect find
you know the most the the most popular
IP addresses are making requests a
result and for for status codes and and
at this point this is pure sequin like
to write by selecting from a table for a
status code is equal to some value but
grouping by something in a more ordering
by something so you could probably
copy/paste this interracial database I'm
gonna work just fine so you know hive
and pig the really wonderful tools for
for non-programmers and there's a lot of
people in our organization that aren't
necessarily programmers but we want to
give them access to data and hadoop
so higher is really a great way of doing
that who in this room has heard of our
as a foreign language wow that's a war
that's worth not expecting
that's cool I mean so uh I mean as a lot
of you already know mr. language is used
by mathematicians by statisticians and
the reason that people and that
community tends to use are is because of
fact there's got a really large amount
of packages that have pre-built you know
M linear modeling classification
clustering statistical test it's a bunch
of boxes you can just download and start
using out of the box so that's why the
our community is as big as it is as a
thriving as it is and the really great
thing is that you know those programmers
are familiar with the bracken are they
can actually use our in conjunction with
Hadoop as well and and there's a couple
of ways this can happen and we're going
to talk about one use case where you're
something called Hadoop streaming so
Hadoop streaming you're gonna write your
our script near client-side you can use
hard Hadoop streaming that's going to
copy that our script unto all each and
every one of your nodes and Hadoop and
the way here to extreme is works is it's
really a way by which any connect
process any application that can read
data from standard input and write it at
standard output can actually be involved
in a MapReduce job so what's happening
here is that MapReduce is getting your
input and tuples is feeding them to your
are script you're gonna be reading
things from standard input things art
organized by the table character you can
do a processing you can write it a
standard output and then Hadoop will
take over from there so this is a really
nice way of you know an existing
programming language button with Hadoop
together alright and now let's switch
gears a little bit and talk about
machine learning so machine learning is
really the ability for computers to
learn without being explicitly
programmed to do so and and the first
example of this is probably back in the
50s Arthur Samuel who's working for IBM
at the time Hiro a program they were
play checkers it would learn the winning
and losing
strategies and after every game and then
apply these learnings and playing the
game again so this is one of the first
examples of artificial intelligence at
play and what's interesting was that one
IBM publicize the fact they had someone
had done this the IBM stock you know
went up 15% so this is a big news back
then but when we're talking about
machine learning it's kind of three
broad categories
tends to fall into the first one is
supervised learning and you know email
spam detection is a great example for
this but supervised learning you have
what's called label data label data in
the form of spam email would be a bunch
of emails where for each email you have
indicator whether the webinar is spam or
harm and and what ends up happening is
this supervised machines you know they
will look at them they'll look at
keywords a frequent that occurring more
frequently less frequently and spam and
harm they'll build a model and then
they'll apply the model on to unseen
emails to do em spam detection and this
is pretty much how most bomb detection
and solutions work today but other
applications for supervised learning
include handwriting recognition speech
recognition face recognition is a whole
bunch of disciplines that use em
surprise learning em unsupervised is a
lot is a lot more challenging with
unsupervised learning you're not telling
the Machine anything about your data at
all you're just giving a machine a bunch
of data and you know one of the more
common algorithms called clustering you
know we're basically look at that data
try and infer some relationships and
come back to either with a bunch of
clusters that you as a human they need
to look at and understand okay so the
machines told me this this cluster of
data you know how are they related so
this there's a lot of human f involved
news as well but the machine definitely
helps come up with these correlations is
this kind of data points there and then
finally have collaborative filtering
right this is something that we see
every day when we go to sites like
Netflix and Amazon they'll recommend
movies based on what you've bought what
other people have bought you know how it
thinks and you know these products you
can't kind of coexist and I looked at
the bottom like I pulled off my armors
and someone must have hacked into my
account and you know was clicking on
romance novels then I I don't know how
they got there but I'd look into that I
guess but but the big challenge with
machine learning has always been that of
scale and I mean in the scientific
community community there's a bunch of
tools that do machine learning but none
in scale are particularly well so apart
to my how is really the project again
it's a built from a ground up to be a
scalable predictive analytics machine
learning set of libraries you know most
of the algorithms and
had distributed so they can actually
work on a standard machine as well as
within the context of a MapReduce
cluster and there's a bunch of
algorithms there I think there's almost
50 algorithms there and across the three
kind of disciplines I touched upon there
and firstly we don't have much time to
go into them in much detail but there's
a great big how an action which kind of
goes into that in detail and I also
taught I spend a chapter talking about
it as well okay so so I would like to
talk about visualization at this point I
mean yeah we talked about data we talked
about data analytics but if you give a
report to somebody and say here you know
here's a one-page report or ten page
report it just just doesn't look good I
mean it's you know people are able to
read it you got to tell a story and so
there are a lot of tools available that
allow you to do that at the point I'm
trying to make is that when it comes to
release I haven't really done much work
and visualization myself you know
usually when I think of visualization
I think of HTML and JavaScript and Ajax
but to do visually visualization
something like this you know it requires
some some other kinds of tools and what
I suggest is that you know I think Java
effects I'm not selling Java effects I
don't say Java effects or EJB people
start walking out but I do I do have to
say that I did attend a couple of
sessions on Java effects it seems like
it has a it has a lot of potential it
has a lot of you know drawing
capabilities and things like that and
I'm sure people are going to be working
on visualization tools that's built on
top of a Java effects and you yourself
can try that how are you going to take
all the data and map it and plot it and
things like that and Joe effects would
be one of those platforms built on top
which our there allow you to do that so
in summary what I have to say here what
we have to say here is that data is the
new oil of the 21st century data is
growing exponentially look out for data
science as the next field of big data
analytics in fact what I have found is
if you actually put the word data
scientist in your resume you know you
can actually get 10 or 20 K pay raise so
you might want to do that but on a
serious note this is
this is where the next generation of
computing is going on this is where the
next-gen you know next-gen of
information technology engineers are
going to be coming into play and you
know I mean I talked about data science
to people they say oh yeah you know we
do that in Python we write these Perl
scripts you know I have some students of
mine that work consequence University
who do you know data science data
analytics using Perl for gene gene
systems and thing I'm like what are you
doing you know there's a great
technology if that's available over here
that you can do in fact they're doing
that those guys are actually turning
into this so we have Apache Hadoop we
have a distributed file system where you
can store terabytes petabytes of the
data Hadoop MapReduce for batch
processing pagan high for data warehouse
data analytics and that generates
MapReduce code HBase and cassandra for
real-time data access mahout JSF J
effects all these different technologies
that's available out there that's all we
have to say thank you very much and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>