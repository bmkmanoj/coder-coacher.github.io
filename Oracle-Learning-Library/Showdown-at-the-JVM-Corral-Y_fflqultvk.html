<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Showdown at the JVM Corral | Coder Coacher - Coaching Coders</title><meta content="Showdown at the JVM Corral - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Learning-Library/">Oracle Learning Library</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Showdown at the JVM Corral</b></h2><h5 class="post__date">2013-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Y_fflqultvk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody I think it's too late to
say welcome to Jovelyn but for the few
of you have joined this section very
specific we have the first one ball onto
javaone this is the showdown at jbm
corral maybe the showdown at the jvm
crow i'm not sure above in the Powell
year we have members from IBM and from
Oracle some introductions starting from
your right that's me alright I'm John
village I'm the Java CTO premium and all
the MS come from us all the good ones of
me so I'm Ryan shampoo Coney on the
manager on time architect in the JTC and
this will be my last day at IBM so I
work for it I'm responsible depending
John's fiction into reality and all
right I'm Dave Keenan I deal with the
performance at Oracle we got into Java
or in other words for ninety five
percent of the market share regarding
job and I'm David Michael bits that I'm
JDM architect the biggest kvm in the
world the biggest to jvms in the world j
rocket and hot spot just rub it in LOL
to working at oracle whatever we say
from the Oracle side here today is a lie
until proven otherwise their lives are
bigger he just said so yeah we had to
minimize your text to make it fit on the
screen
with that said we we want this to be a
you know community driven or I would
know straighten question questions and
answers session so we appreciate if you
could ask us questions and we'll do our
best to answer them and if that might
actually work or we didn't test it with
invested my wishes it sure will do that
for the recording we probably want to
make sure that the questions are
recorded it going it's weekend we have
to repeat them we can repeat them it's
fine and twist them to something that
make sense to us huge okay so with a lot
of dynamic language coming up on the GBM
you'll deal with a lot of these api
especially in America
take the eyes
what I you guys doing on both sides to
deal with making you work better right
so being the biggest JVM in the whole
world one of the things we are investing
in is soon as you say on the
multi-language side we are realizing
that people will want to write you know
express their problems in some cases not
using Java language but that they still
want to make use of the powerful JDK and
the you know especially the very
powerful jbm wrong time we have and so
we realize that we need to let me be
able to leverage other languages or
leverage to run time here for other
languages as well and this is something
that for a big shoulder pj/k serves yeah
and we're continuously investing it is
and it's wrong more mature for these
dynamic languages in hot spot today
working very closely together with JRuby
folks with judgment pokes we have a team
working on the NASA on open source
projects running JavaScript it's on top
of the JVM yeah let's go ball yeah we
continue to leverage the same type of
technologies with my avid new support
for lambdas and new things that we're
doing in the v JK so apparently we're
supposed to repeat the question so what
are we doing optimized for dynamic sorry
but like the languages that's all I'll
follow the rules unlike you cheaters
over there so specifically I mean things
like 292 the the key to get the
performance out of the new by code is a
fairly aggressive in lining right so you
get the fully transparent I'll all the
codes visible at every call psych to
make sure that it just collapses into a
call and that's really the focus to try
to make sure you can discover all of
those in all of the cases and get
basically you know native call
performance that's you know so you asked
specifically and the way that strip
being driven is you know benchmarks you
you know you compare you know it's not
fast try again fix and just push up push
on that so that's it's pretty it's
pretty basic in terms of other
things you know further down further
down the line now we're looking at you
know different memory formats within the
heap and things like that so that they
can be more dense more packed more those
those kinds of things but the initial
initial focus is clearly going to be co
Jen and in lining for the for the
exploit of that by code and hopefully
all the use cases people start to use it
because not all of the languages that
were mentioned have picked up to 92 yet
or use it well and so when when they do
you know those use cases will become
maybe surprising you know we just
haven't seen enough of the abuses yet so
and that's really the interesting case
trying to you know get what I would say
is a very unique use cases that are Java
has traditionally not seen and and
flatten those and go fast yeah that
answers your question and well and to be
clear IBM is members of the 292 and 35
expert group so you know part of our
responsibility is also making sure that
we keep Oracle honest during these
during the drive through the
specification just to make sure that you
know the interests of the community as a
whole or kind of you know putting a
calculus Tanner gto I have to translate
the word honest look what happens
actually in these things is there's a
implementation-specific like sort of
once a optimizations but implementation
details it leaked into the spec you want
to make sure that it's equally
implementable and more efficient by
having the spec clear not the
implementation drive how it works ok so
I don't know if you're familiar with the
process but often the design is oh well
this is easy to do on my VM so I'll just
make that the design and we certainly
want to make sure that it's
reimplemented on lots of different
architectures because we actually ship a
lot more architectures than other
vendors and so will stop with the slams
in a minute but I had to throw one more
out in the one you can add
check it I'm still inst yeah and it
totally makes sense in a part of
actually the work that has gone into the
evoke dynamic give functionality lately
over the last year or so now it's
actually kind of moving a large part of
the implementation from the JVM often to
the jdk so that other vendors can make
use of that same code and to simplify
the footprint of the implementation in
the vm and we you know if we ship a lot
of architectures at least we think you
know we we have a lot of architectures
we need to test and ship but clearly
other vendors are building their JD case
on top of odds but as well and shipping
them out your platform so you know key
is definitely to make sure that we
simplify the implementations as much as
possible for the openjdk community in
general I'll add one thing the 292 is
now being is going to be used as part of
lambdas and so what will happen is you
know in fact it may be JavaScript and
these others don't use it will be moved
Java's going to use it and the libraries
are going to start using it so that you
will see an extreme focus on aggressive
optimization because you cannot regress
between seven and eight then over the
performance will have to be better and
that will drive pretty much maniacal
focus on making sure that that optimizes
away because it's a lot of overhead for
a call just be to be frank for something
that you really don't need to waste your
time on running so so who else wants
that's a great question ok
credit
mark 14
large 1200

I think the first question is what is it
we're doing to support very large heaps
as in 10 to 100 gigabytes and beyond and
the second one is around multi-tenancy
and I'm not going to say in hot spots
specifically because that would make
this panel boring i guess but
multi-tenancy multi-tenancy in the JVM
or in our jbims right um that's good so
let me do the heap one first yep okay so
you guys can either start or I can talk
about our new GC for large I ran all
right but thanks so in terms of our
cheap support you know especially within
IBM we have a lot of products than you
know having to use hardships if you
think about some of the Cognos work
cubic services and so on we're talking
you know easily in excess of 50
gigabytes well into a hundred sometimes
in excess of 500 gigabytes which is
incredibly huge and a lot of the
traditional technologies that people are
used to using things like generational
and so on you know they work to a
certain point but once you get that
global collect you're dead right i mean
your performance is done you might as
well just recycle the system so i
believe if i'm not mistaken IBM was the
first to ship a specifically large heap
garbage collector for production use not
as a feature and that it's hard on then
it is no it's just the facts it's just
it's just don't don't be mad don't don't
be angry but that so this was balanced
GC and it was you know largely targeted
at eliminating the large pauses in
global collection so that was a bit of a
trade-off in order to you it was
effectively you can also look at as a
generational collector where would
selectively take the appropriate parts
of the heap in order to get best bang
for buck best ROI and you know be able
to smooth your paws times out to keep
them both low while always collecting
the entire heap effectively doing away
with global collections that's working
wonders for us we've seen that even even
a smaller heaps and all your questions
large fees but even as smaller heaps
this has actually demonstrated really
good performance and we did design it as
for throughput so for a lot of solutions
you'll typically hear things like yes
well works great but you're going to
incur a 30-percent performance hit
that's not the case with balanced if you
see anything yo greater than a couple of
percentage points difference between
that traditional generational it's not
it's it's you know it it it's certainly
something that that would be considered
bog and we would end up fixing it so
absolutely and that's on all of our java
7 vm sin there's a included websphere
eight if you you're at IBM I don't wanna
mention products but it's it's widely
available and pretty much all of our
latest production run times and and just
for the reference it's called balanced
we gave a keynote last year put it on on
the screen as a new things so it's
pretty cool and what are you guys doing
you know so we have G 1 which is first
supported in 7 update for unlike balance
to you when I'm sorry bounce is the
balance g1 what's a call to see sorry
which was shipped for Java site we
didn't start our word in garbage two
shots of wicked marketing on Oracle's
part so it's essentially the same
regionalized heap I mean a traditional
garbage collector or a generational
garbage collector single generation
garbage collector what have you the size
of your live data set and the size of
the data that you have dragged your
garbage collection performance in how
long it takes and as as Ryan has
described a regionalized heat allows you
to deal with the region at a time or
those those smaller sets of garbage at a
time and we're able to be able to handle
much larger teams we've tested up to 3
terabyte would you one with success
didn't crash awesome yes neither did
parallel GC CMS or serial leaving that
500 gigabytes in the dust but that being
said yeah it's the regionalize garbage
collection our defaults are different
for g1 initially our default of g1 are
low pause for large heaps so greater
than
greater than eight gig we've tested well
highway you know up to the many many
hundreds of terrible hundreds of gigs as
well with with reasonable low pause
times meaning 250 milliseconds is a
general target now as far as tuning you
know as Ryan said they're targeting
towards towards throughput we're talking
to our lower pause and now you can tune
either way I'm assuming it's the same
thing with Alice GC you can tune for low
pause as well whereas you can tune for a
throughput for g one as well yeah i
actually have nothing i don't not a
whole lot more dad i think i think we
may have been the first one starting to
work on balance DC were sorry the region
base so you're going to take our name
for it yeah all right I chose to call it
garbage with um ok the next question was
on multi-tenancy because first okay so
multi-tenancy I think in many cases when
people are talking about multi-tenancy
is really a density play please correct
me if I'm wrong but that's kind of the
key value add or the key area we're
looking at so how do you achieve good
density and still maintain many of the
other aspects and very specifically I
think it's fairly easy to maintain or to
get good density if you sacrifice
isolation and security especially you
know security in isolation closely fine
so you can start in that end what you
get is good shareshare ability you know
your density but you'll again you will
have to work on the isolation depending
on your requirements that may be a good
way to go typically this would be
something like implementing the isolates
Jays are at Oracle we have the MDM
project the multi vm product that has
done this and successful driven out of
oracle labs and on the if you start in
the other end of this end of the
spectrum you'll have you know you'll
start by sharing nothing you'll have
perfect isolation but clearly no longer
density features we have there are
things like let the class data sharing
functionality in hot spot where we can
store a snapshot of the class meta data
to disk essentially
and then have some subsequent deep je
viens pick up that same data and
transparently share it so that's kind of
the the background correct me if I'm
wrong regarding the question but it's
wrong density I guess now there's a
couple of our technology specifically
last data share that could be extended
beyond what we are scope of that
technology now our main concern
surrounding that not they have
surroundings expanding that technology
or is indeed isolationist give security
I was our biggest concern the same thing
with ahead of time compilation so we are
you know they are the points in which of
interest for from multi-tenancy right
now from a multi-process point of view
and sharing amongst processes and we
certainly have all those on the table
yeah so we already do those things so
the the two kinds of multi-tenancy
approaches so you run many copies of the
JVM and so we're attacking that with
more sharing across JVM so things like
our share classes facility which is
similar to the bytecode memory mapping
stuff is is in there but we also put
code in there so that you can share
computation essentially if someone
compiled it somewhere else you can get
it and so that's to try to drive maximum
sharing across JVM so you can have more
because they're all sharing the same
read-only copy of some artifacts and
we're putting other artifacts in there
tues of caches and lots of other things
so that's that's the fully isolated
multi-tenancy where you're running
multiple copies but in your then you're
in the mega bytes then we have work on a
separate facility so for within the JVM
isolation so this is a single copy of
the code multi copies of the statics you
get state isolation so that's fairly
straightforward so on a use case where
you might run a single application but
for thousands of tenants you don't have
to load the code a thousand times into
class loaders and it saves lots of and
that's that's targeted for 10k tens of K
and the problem is if you put all of
that stuff into a single JVM there could
be
us so we have resource management across
these isolation units so that you can
control the number of sockets files io
speed percentage CPU and those will be
delegated to the OS if it supports it in
your thread group or you managed within
the JVM some so you actually have
flexibility of depending on the platform
and this way you get the ability to
drive very high density this tens of
caper tenant in the same jvm with a
level of isolation however since it's in
the same address space you are trading
that isolation for extreme density so we
can see 10 50k tenants and thousands of
them and that's one of the reasons we're
starting to see these larger heap so we
talk about balanced if you're going to
put 10,000 tenants or units of isolation
computation in a single JVM we use our
region based GC to manage the separation
of their app memory allocation rate so
that we can actually keep track of the
memory per tenant and throttle and allow
you to manage those things and keep
track if one is behaving poorly and then
you could evict that tenant if you
decide to some policies based on the
middleware this is what i would consider
the approaches is is an infrastructure
approach because the above the middle
where the various different middleware
that's going to be built will then
decide how to exploit this for their own
particular programming models so you
could decide to have a multi-tenant
container and try to drive free free
multi-tenancy within an ee kind of world
or you could have different things like
scripting languages that may already
have been built in a way to that you
know it's a sort of a safer way to
deploy a multi-tenant version of the
scripting language and those kinds of
things you're not being very
prescriptive on how you use it as just a
facility to give more sharing ability
with more isolation I think we'll see
more of that and it'll be a decision at
hopefully at application deploy time
when you say I'd like this quality of
service this level of isolation and then
the system that's standing up like a nap
pattern as we described earlier this
week will decide whether to use a multi
10-inch a vm or give you a fully
isolated JVM to deliver
exactly the quality of service and
isolation you've asked for and so in the
end it's really cool technology out of
the covers and that's how you should
think of it really cool stuff but
hopefully you're not setting those knobs
unless you're sort of a person in that
layer who builds those things as an end
user you'll just see more density you're
paying less per instance in the cloud
and that's kind of important because
when you're starting to pay by the SIP
in the cloud or by the unit you want to
count those and use less foot right so
so and just add John sort of touched on
this a little bit but one of the big
things you want to do is you don't want
a JSR one situation where you end up
having to rewrite your code to make use
of any of this right you want your app
just to work your app today works the
same way in a multi-tenant environment
John head do that this one is a deploy
question based on your patterns are you
deploying high density or unifying you
know completely isolated has to be the
same application not rewritten and you
know you water the good one of the ways
you really achieve that is having
runtime support baked in that handles
this for you and carves it up and
creates the isolation for you and so
that's going to be key right it's just
it's it's the same path this is not jsr
one rewrite everything forget it it's
got to be the same up so there have been
two sort of along these lines help me
with the numbers Graham 12 284 is to
resource managers are Spanish employment
121 there's so there's been things like
that that that have already gone on
obviously right now especially with the
changes to the whole JCP process and
everything you know experiments are
being performed right now when groups
feel that they have something that's a
good answer that's when the jsr at the
time for the proposal is so you know
we're not making any secrets about our
plans there was a talk earlier yesterday
if i'm not mistaken from our
multi-tenancy you'll be able to look
that one back up
no but the direction that we're headed
in certainly no secret when we have
something that that were I think ready
to move forward with in the sense of yes
you got the kinks ironed out then yeah
isolates plus plus and resource
management plus plus i think is on the
agenda we're just timing right
any other questions they're afraid to
ask they're so in depth and
thought-provoking they're hurt
general before what is
that inspires the general
this could be doing that
or something that is you're looking
forward to putting into the
don't they both like
exposing more than all games
right
things are implemented
each of the answers the old legs levels
so also are their weights to standardize
some of them before all and or in terms
of what
is exposed maybe not the exact levels of
types your general sense of these are
the methods that the BM too exposed to
the user other people from that of the
things you are thinking
right i think in general we you know
basically what we then we try to keep
the specific you know so we need to
stick to the specification and you know
it's there for reason we need to fulfill
the requirements and there are the
things that are expected out of a JVM
that I getting out yes even though it's
not in the spec we both implement it
everybody that does the jvm has to
implement that we obviously we need or
we want to maintain some kind of
flexibility because the implementation
is the value of like there that's where
we can innovate and and you know make a
difference so to speak but I think over
time as we see things that we both end
up implementing in kind of the same way
and it's you know there's something in
there that people are starting to depend
on clearly we want to look at how we can
standardize that in some way and
typically and you know we need to do
some of these kinds of investigations
first we need to come up with you know
what what parts of the implementation
actually is similar and then you know
that's when we kick off the jsr and try
to standardize it in some way or you
know it doesn't have to be a JSR
specifically but you know things we can
agree on so there's lots of candidates
for this kind of thing you're gonna sort
of like you talked about we talked about
isolates being involved to take into
account the fact that we can do them
better and new functionality you know
classloader enhancements potentially to
be your tenant reentrant those kinds of
things and so eventually those things
will be standard api's for middleware
providers who want to refuse to provide
more isolation and control resource in
the isolation context they're using so
that's that's fairly straightforward the
question I think you're asking is for
some of the specials weird stuff that
you can get from different platforms how
do you sort of make that available int
it a little bit on things like jmx kind
of events up through for hypervisor
activity so we see for example L par or
which is called the guest OS depending
on which hypervisor resize we're told
and it would be really great if
middleware could respond to that in
addition to the JVM so the middleware
thread pools are sized to the number of
cores you had at the beginning now every
single hypervisor to date is either
getting the
is adding the capability of either a
dynamic CPU reassignment to up and down
by one-tenth of a cpu in a hey access
case or hot plug cpu and things like the
M where all of those things are going to
be communicated and those are fairly
abstract things the number of CPUs and
things like that and then you should be
able to respond and that's going to give
you the sort of virtuous circle to
optimize the whole stack dynamically and
those I think would it would benefit if
we if we cooperated earlier because
eventually eventually hope you'll have
to land will end up changing the name of
one of them but but but those are fairly
straight obvious ones for other ones
like you're not performing well those
are a harder one like you're asking like
you know now your code is generating
cash miss as well I don't know what
you're going to do about that your Java
code you know nothing and but it's great
for performance tools that might want to
write so there's thurs maybe the
middleware can't help or the middleware
can decide that the app is particularly
hmm expensive or something and decide
that because of that the next time it
gets placed in a cloud placement it you
know it's it's actual resource claims
versus it's true measured resource
demands are inconsistent so could you
put me on a place I don't trouble the
other kids so to speak consuming and so
those could be kind of other ways to do
it but you know in the end I think that
for certain things that become sort of
common it's it's obvious we're going to
have to they don't have Stan likes to
jsr s and spec those things so you could
write once and just get the right answer
not everything's available so you need
will need to worry about capability
style queries to like can you get this
information or not so so you know the
JVM like in the case that John talked
especially with the layer of liars right
you know you got your hardware
hypervisor and then guest OS and so on I
think the JVM is actually really
uniquely positioned right to understand
what's happening from both the app level
as well as from you know communicating
underneath and sort of you know
following this up is one thing so
although as a an app developer job app
developer you may not
care about you no cache misses or what
have you or be able to really control
you you care what you can't control it
the JVM is actually really uniquely
positioned to be able to measure these
types of things with a bit of
cooperation on the app to say what's
happening per transaction and then
possibly be able to go down the road say
well look here's the better
configuration to increase your
throughput for example writing right our
CPU burn what have you so it's certainly
something that we're aware of we're
looking at and we're concerned about but
again you know if this will be one of
those things where when we have an
answer given all the different no
hypervisor vendors right the different
hardware characteristics that the
information that they provide and so on
being able to walk that all up into
something that makes some sense as a
general API that's when will you will be
ready to move forward yeah I just just
add to a one point from a performance
perspective I think one thing that we
all agree upon is that we want to
eliminate as many options as we can to
the point where we're just out of the
box you know let it go and attack John
made a very good point beginning of this
is that that's one of the first things
you should be doing run without flags
see what your baseline is and then to
and from there and also that being said
you add the question kind of blended the
case of you know what you know us you
know working together what can we gain
from that from a performance perspective
as well beam each other each other up
and you know winning benchmarks and and
we have one being fast to the other you
know what that leads to innovation you
know in the end you guys are the ones
that benefit from that because
everybody's faster no we are sorry so I
said everybody so we're we're fast right
everybody on the IBM sovereigns much so
five or seven pounds are
but you really happening the cases is
not a DM it's an application but you
know where the modeling is fun currency
or memory or something but to this
solvent an application-level arbitrarily
they have to first find out what the
problem is that find out what's causing
that problem is addictive so it's not so
much about the computer so common
feedback and observability of your
application but amongst j-jerry
implementations yeah that's a good
feedback it is I think from a jmax Larry
as John was saying that's certainly
something that what we'll try to do but
there is a lot of flexibility that we
need yeah that it certainly is good
feedback from an application-level
perspective I think that's something
that we probably can't drive for right
well sure we all of ems I'm sure have
you know stats on the lifetime cycles of
objects you know lots of that kind of
information to tell you the
characteristics of your application and
whether they meet any reasonable
criteria or you're like you're you know
allocation heavy or whatever and could
certainly give you better information
about that there's certainly lots of
tools have been built on top to give you
traces and analyses on not just what
you're using in the JDK part but the
fact that you're up in the Middle where
and where you started from what you did
that caused this happened down in the
JVM and then you can sort of feed that
back but you know how you get up the
harder that is to deliver really clear
information so it's from our perspective
it's enabling the information so that
people could take it and give better
sort of observable bad behavior which i
assume is what you're looking for so you
can fix it right right no and I think
that's exactly it like we if we can you
know with JV MTR in a few other things
like that we have ways of getting data
you know we can definitely add to that
but I think the key is to you know we
can implement all kinds of functionality
in the JVM but ultimately it's it's
really better that we try to expose the
same kind of data and have tools vendors
you know help us visualize that help
developers draw conclusions from what
they see and you know ultimately help
them right you know rewrite the code or
update the code that's needed and in
some cases it's really hard to help
developers you know as John mentioned
early like cache misses what what do you
do with that you know is how do we
expose that in a way so that it makes
sense to people ultimately but yeah good
feedback for sure right right right
right so yeah we would say that
information right we would see that
information going back to something like
a load balancer and or a deployment
system because then free for example you
may decide if your if your cash isn't
working well you might have to choose
the newer machine with double cash which
because you will see performance
difference they based on loops based on
the architecture of the machine so you
might actually say I need something that
has double the cash because i'm just
getting cash and then someone else can
use it and that's what we're talking
about it's like standing up the app in
the right places and that'll be true
even more when there's other
accelerators crypto accelerators GPUs if
you can use them if you if you notice
that the app could have used one that
information could be used for later
because if you're deploying lots of
copies you go I prefer this is my
environment because I get the right
hardware as opposed to cookie cutter
hard work it's not all hardware anymore
is equal and you'll have cloud that is
going to have older stuff and newer
stuff and the flexibility to deploy it
in the exact location for the best
performance is something that you could
determine you know dynamically at
runtime and then later to give me a
better place to run it's perfectly good
at you know good data so yeah so so
being able to you know it
misinterpreting the data is actually
often even worse right like if you get
that much data and then you misinterpret
it or pick the wrong thing so you know
help dynamic you know dynamic feedback
to help make those decisions right and
be able to say look this is what the
system seeing and what it recommends is
going to be a big part
there's a lot of people you know people
there are actually a few people that
really really understand how to take all
the volumes of data that you get even
from just the hardware let alone
everything else and interpret that and
say this is what I should do on my java
system right my java application you're
going to need something that's going to
be able to help you on that right they
can gather the date and say look this is
what's happening why here's here's here
in the next steps for you is a request
no
not any more than you two okay there's
another JVM than these two unless that
matters ok sorry just kidding i just got
Wilkes only one big one should one of
the issues that we run into each divvy
em if they get a flag option that they
don't recognize they don't start okay
that's one thing the second thing in my
size are this estates and direct
translation from this option in budget
you have to imagine a together and can
you guys agree on something we have an
option for the first one it's so ignore
unknown vm options so sorry it here's
another actually is an option for them
and if you can find them both they'll
probably ignore each other to write so
great that's right you can the latter
one is harder obviously because in the
you know we we use or MX means one thing
in one VM and may have end up meaning
something a little different in other
via you know clearly we have the command
line options for jbm aren't well
specified that for sure and I think
we've you know we see that as well we
have a number of different options being
open source as we are there also
available to everybody so clearly people
start depending on them and what we're
trying to do from our side is to just
clean that up make sure that we document
what is supported a knot and make sure
that it's you know clear what options we
expect people to use I think we over
time you know hopefully again we're
trying to minimize the amount of
parameters you should specify to start
with it again coming back to that like
the goal is don't along with parameters
and then start tuning if you absolutely
have to right and if there are you but
we are we are you know I think we're
actually getting there I was going to
actually ridicule Michael and say well
you know for IBM you don't need to use
5000 options to get the right garbage
collector right but so you know if they
actually took our model of sort of
minimalist options that'd be great but
you know maybe woke in our mobile okay
I'll yeah
shot no and that's that's fair no no
it's an important question and I think
you know if you look at so I'm going to
I'm going to actually decide here with
them and say what give you if you take a
look at something like g1 or balanced
right depending you can see we're both
collectors are actually headed right
which is to say what there are certain
trade-offs that need to be made but we
want to get out of the box you know GC
is typically the number one thing to
tune right and it's well ok for some
people right so you got your favorite
fruit pain point that's fine but you
know what we should be able to do is be
able to take a happy medium right
because in all the collection that all
the collection strategies you have
something that there's always a worse
case that's unbelievably terrible and so
you both strategies are really trying to
get to a place where there is no worse
case like your average case is your
worst case so we are taking steps right
in that direction i think i think we're
headed towards that less options
required for tuning but you know we're
not all agree or not I'm not there I'm
going to interpret his question by
selling the answer is your question was
if the option is needed could we use the
same name if it's a reasonable
approximation of the same option that's
what your question was can we
occasionally beyond the common name sir
says pick our names they used already
much where we're open source you can
look at them see what they do and oh
well I'm gonna bring that up here I
don't know now's not it is now what I
think is so i'll add two things the
first one is again like we are trying to
minimize the obvious if you want to
squeeze out the last few percent of your
your job application improve the
performance they will realistically
always be a way for you to find a better
option some combination of options that
give you a few more percent but our goal
is clearly to make it so that you don't
you know in the general case like a 99
or you know 95% whatever case you
shouldn't have to the other problem you
have is a jvm is figuring out what is it
the application is trying to do is post
i'm an issue is it a bad job you're
running you know so the clearly
somewhere we don't have the information
necessarily to make the decisions on our
own so in some cases you would like to
have at least one dial to say you know
am I am I kind of a real-time each
application or am I about
job running in the background somewhere
so that's just an example of the
challenge I'm trying to make this work
well Viking right you got it Yeah Yeah
right so we do see that and because a
lot of our customers have both we get a
lot of requests for why did you make you
know the options different and we will
try to make sure they're their common I
can't say that we currently have a
process other than hey would you like to
get together on some common options and
the process at the moment by right yeah
I'm certainly get it if it does
something that's you know essentially
the same and gratuitously different
options this just doesn't help anybody
so yeah absolutely we should common
those up yeah I did say yeah okay yeah
once you get to implementation
differences that's that's where yeah
yeah so a challenging but the thing is
for size sizes and things like that you
can get there for like Iraq for some
sorry well is that we keep sizing well
now just now that you finally got rid of
permgen mm-hmm have another from jen and
IBM jdk since Java 142 we never had
wanting j-rock that's nice they're
actually they're up so can we start
supplying in the hot spot for a while
because it's the worst l sorry did the J
rocket team and the j9 team without this
trash hot spot so we are actually we can
actually start to converge on things
like that like the obvious ones it's
true we can actually now at this point
something like MX really does mean the
same thing yeah don't ask impressed rest
it's kind of and and our generational
collection strategies really aren't that
far apart NM x specifically something
we'd like to get rid of so maybe we'll
go down to 0 comments I'm sure everybody
would look like every that yeah no but
yeah yeah now we need to get it's a
value point like we should be talking to
each other more about whatever you know
when we introduce options try to make
sure that we kinda name them the same if
they try to do the same thing but I
share focus fear also that if we name
them the same and the implementation is
different then it will be then clearly
that's the problem itself
there there's lots of places where you
can have the deeper buried options that
are very specific to the vm and you'll
always be able to get those and then the
question is do they ever get promoted to
something that's common enough that mean
the same thing and I understand that
they're not always possible and if you
try to fool yourself into thinking
they're synonyms that's get in trouble
at some point but you know the other the
other side of it is you just have to
speak two languages to VMS that's not
good either so yeah it's a balance
cool
especially given what sizes right inside
having not really did the total
displacement excitement because those
are faster than us right now but there
are many other pieces that are not so
you can't create a is then processed and
say okay once we figure out that
something is kind of like elected you
have a process for lunch it right and
that is what I think okay I was also
thinking defaults as well I think
default out of the box is a lot of
things that go on that are significantly
different than people ya can't get
bitten by absolutely that's a fair point
so we should sing Kumbaya at the end of
this that's right okay I think as an
opportunity moves forward to work more
closely together wit openjdk and other
avenues well we'll get to that point
right um who I think that's a hard
question we all like four minutes well
so we have a hard question in the back
oh wait there's a go ahead yeah you're
going
okay so i've heard recursion but it's a
tail call optimization so is there
anything going on in the JVM for tale
called optimizations I basically the
question yeah do so I know there was a
discussion of the JVM language Hamid
last two if i'm not mistaken on Rose
brought it up that's right now at jb MLS
this year right so you are you asking a
cat is their way to guarantee there will
be tail call optimization because that's
the only useful case i can imagine it's
the guarantee because otherwise you
cannot rely on recursion to deliver
right because the VMS will tail call
optimized in the jaded code but there's
it's not guaranteed so if you run
completely interpreted and there's no
forcing functions ensure that tail call
is is eliminates the have the stack then
you're you you're going to stack
overflow so you're are you asking the
specific is there a way to get the VMS
to force the the tail call to go or or
is that what you're suggesting or are
you just wondering if we tail call
optimized
right right so in wien ours are
certainly will optimize but that's
possible the problem is you can't rely
on it because it's printed for example
that's not one of the optimizations that
happen so if you in order for it to be
generally useful which is what the
languages guys need is that it has to be
able to way to guarantee that the last
call pops the stack and goes and is
optimized and that would require vm
changes at the bike code level to ensure
that the bike code where the bike would
pattern is guaranteed to deliver a tail
call and that's that's so you could
actually do recursive recursion with as
iterators so there's so there's these so
there's the question is you know are you
just interested because tail call school
and it would make go faster yes we do
that but if you're relying on it to
actually work to remove for stack
overflow then there currently is no
mechanism in the JVM to do that so I
think that's the the two sides of the
coin sure but it is it is being
discussed like this is this is not
something that's not on our like it's on
our radar absolutely it's being
discussed the the budget will do it
right once you hit a certain
optimization level it will do it right
it's just a question of did that piece
of code hit that optimization level or
not therefore the code that you wrote is
not guaranteed to not stack overflow
that's right because you could be down a
hundred fifteen hundred deep before it
decides to do it and then all of a
sudden you just wasted most your stack
get in there right
we're not the discussion stage really
isn't there yet right so nowadays what
the jrs have to remember with jrs you're
supposed to show up with the
implementation in hand and then you go
from there so I'm not always true but
yeah well you know I know but it helps
it certainly does but you know we're
still talking about right you know how
it is this something you should just
auto-detect you know what would the
conditions be then how do you guarantee
it give you something like annotations
what have you right i meanthis you know
or is it language level change right
these types of things right okay i'll
call it I think we're out of time yeah
yeah never doing thank you very much
Michael Drummond guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>