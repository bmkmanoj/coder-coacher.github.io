<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning K Nearest Neighbour Classifier ScikitLearn on Digit MNIST Dataset  Part 4 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning K Nearest Neighbour Classifier ScikitLearn on Digit MNIST Dataset  Part 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning K Nearest Neighbour Classifier ScikitLearn on Digit MNIST Dataset  Part 4</b></h2><h5 class="post__date">2016-08-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3sGPyixO8UE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends welcome to the video
lecture on machine learning so in a last
lecture we have seen about we imported
this iris dataset and we used the
nearest neighbor functionality from the
scikit-learn library so in this tutorial
we are going to see about digit
recognition so for that we are going to
use this m-miss dataset but instead of
using this nearest neighbor we are going
to use the key nearest neighbor
classifier now let's see what is the
difference between this nearest neighbor
and a key nearest neighbor classifier so
I have already opened the documentation
of this s scale and neighbors so inside
that there are a lot of different types
of nearest neighbor classifier or
different constructor has been given so
in the last lecture we use this nearest
neighbor so what that nearest neighbor
is given as the top a nearest neighbor
point with respect to the query point
but manually we have computed out of
that top five nearest neighbor which
where the majority would will go and
manually we have decided that we're in
different category the taste data lies
but in case of this K nearest neighbor
classifier it will calculate this
nearest neighbor all top nearest
neighbor point plus it will
automatically it will decide also where
that particular test data will reside in
which caste class form in which category
that test data will reside so whatever
the task just in the last lecture we did
manually that task will be done with the
help of same functionality in this K
nearest neighbor classifier so in
today's lecture we are going to use that
K nearest neighbor classifier rather
than nearest neighbor and we are going
to use this M nice data set so this data
set is again the part of this scale and
library so I have done already necessary
inputs so let me just run it I've
imported a scale and data set and one
more is the pandas library for the
visualization of this data and MATLAB in
plot matplotlib library
for the inline display of this image and
I have created one more function to
display this image because this all data
set is an image data set so we will
frequently need to display the image to
just verify that what image we are you
know dealing with other white is just
the set of numbers for us so I have
created one display major function where
we provide the input as a number then um
the number 3 code will supply and that
numbers record image it will display so
we are going to use that function a lot
in this lecture now let's just load this
digit data set so I have loaded this
digit data set and this we already did
in earlier lectures also so I have
already pasted this code so this loading
of digit data set and putting into data
frame object so there are total
somewhere around seventeen hundred and
nineteen seventeen hundred and
ninety-seven data sets are available so
there are total 1797 images are there
but I have used your first 1700 images
also remaining 97 I have kept for the
tasting person so out of that remaining
97 image one of the image we will take
for our testing purpose so what it has
done actually individual email digit
data has a total 64 integer value for
each record m64 is nothing but in 8
cross 8 grid it is showing as a image so
we'll just display some of the image and
we'll just verify that how it's going on
so we have just displayed a few records
so from the first 5 image slow ok let's
display the very first image
okay so first image is nothing but the
zero basically and it has can put this
one-dimensional 64 digit 64 different
numbers into 8 cross 8 grid and it has
displayed for us so this is all about
the data
one more thing about the data we'll see
how that target functionality will looks
like so we'll apply this keys function
and see what are the keys are available
so in this image if they have formatted
to 64 bit number into 8 crosshead creep
this way we got this image but I all
supplied this data because we want to
display in a data frame format so that
is a 1 record and each record contain 64
bit number target name individual labels
name or the description of a whole data
set and a target so we are going to use
this the image data and target basically
so data is nothing but our input data
and a target is nothing but our output
data now let's go for how we can create
this training data set and how we'll
supply this training data set to our K
nearest neighbor classification then
we'll supply that one of the testing
data second so let's create the training
data set first so we'll create the train
underscore acts so training data set
will create like digit
data and I have told you that we'll just
use the first 1700 the remaining will
use for testing purpose so train X and
trained by will created so we can use
the same one but here we will use the
target so this way we have created a
training and a testing sample now let's
just import the be necessary
functionality for the K nearest neighbor
classification so for the K nearest
neighbor will import this as scaled on
dot near neighbors and we'll import
keye nearest neighbor classifier okay we
have successfully imported this
k-nearest able classifier now we will
create the constructor for this KN n K
nearest neighbor classifier so K nearest
neighbor classifier and we'll supply
some number that number is nothing but
what we have seen in the last lecture
that how many nearest point you want to
take into consider while predicting the
output level so let's take 10 and we
will supply this training and tastings
data to the server for fit function
we'll supply this training rain Y so in
a last lecture we have supplied just our
training samples examples only no output
level we have supply because that
functionality was used for just for
finding the nearest neighbor but here K
nearest neighbor classifier is used for
the finding nearest neighbor plus it
will find the majority vote will go to
which category also so for that category
we need to supply this output label also
okay KN and actually that's a typo okay
so we have successfully created this K
nearest neighbor classifier constructor
now let's create our testing sample so
for K nearest neighbor we just supply
this tan as a nearest neighbor so it has
chained this value 10 remaining all
values are quite a default value you can
change those value this is available in
the documentation so individual
algorithms you can change it you can
change the number of jobs even you can
change the lip size that lip size is
used for different algorithm like KD
tree or even a Bowl tree kind of
algorithm you can use different matrix
like a Euclidean matrix you want to use
for the computation of do
distance between the two points or any
other l2 or l1 distance there are a lot
of a distance measure will be available
inside this a scale and so that we'll go
to see in future lecture so this is the
taste data a train data we have created
and we have supplied to K nearest
neighbor classifier now let's create our
our testing data so we have used for
1700 example as a training set now let's
use some arbitrary in number about the
1700 for using our tasting samples so
let's just use some somewhere around or
1726 number example and we will display
it and we will see then we will supply
this sample to the K nearest neighbor
classifier function to predict that
which digit it this image belongs to
okay so we need numpy library for kids
so i am importing this numpy as n p and
n p dot array so we will supply
digital data and 1726 number example we
will see so we will put it into touched
now whatever the test dimension is
available that is a kind of column data
set but we need to supply to the SQL and
liability while predicting the output
level in a one form like a number of
instance multi cross number of features
are available so that kind of matrix
only we can supply so let's just reset
this matrix so here we are going to use
just a one sample as a testing sample so
we will use the one row in any number of
column and winds up assign it to the
test one okay so we have created the
testing sample now let's just disarray
this number of imogen so we'll just
supply the number to the function which
we have already created so we created
this test when testing samples +17 we
are going to display this 1726 number
image okay so we displayed this image
plus we have created this test one
successfully so this three we are going
to view as an input to this our K
nearest neighbor classifier function and
let's see what that algorithm will
predict for us so we have already
created this key and an object so we
will supply on K&amp;amp;N function the predict
functionality so in our last lecture we
use the key neighbors functionality so
key neighbors will just you or will just
return the top I or top 10 based on
whatever we have supplied in a
constructor the top matches out of that
but this predict function will predict
at what particular data set
particular image in this case will
belongs to which class or belongs to
which category so we'll supply this test
one okay and let's display it and let's
see what it hasn't okay it has imprinted
in the array three so now array 3 is
nothing but whatever our target data
sets were there it has a target names
also so let's just bring that three so
we'll come to know about it teach it
okay so our a three is nothing but the
tree and it has already predicted that
in given images so that is quite
successful
I have already supplied just the one
image as an input for the testing you
can supply hell lot of image and you can
predict the accuracy also so that's it
for this lecture friends I think we have
already are we almost cover the nearest
neighbor and K nearest neighbor
classification stuff now in a future
lecture we are going to see about K
nearest neighbor regression so in this
lecture we are have seen about how based
on K nearest neighbor classification how
you can predict the output category but
if this output categories are supposed
the continuous or regression for the
regression kind of problem how you can
predicted output continuous value so for
that we are going to use this K nearest
neighbor regression so I hope you are
enjoying this video please to Like
comment and subscribe it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>