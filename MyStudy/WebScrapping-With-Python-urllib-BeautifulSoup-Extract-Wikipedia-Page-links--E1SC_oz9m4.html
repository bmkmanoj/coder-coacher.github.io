<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>WebScrapping With Python urllib BeautifulSoup Extract Wikipedia Page links | Coder Coacher - Coaching Coders</title><meta content="WebScrapping With Python urllib BeautifulSoup Extract Wikipedia Page links - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>WebScrapping With Python urllib BeautifulSoup Extract Wikipedia Page links</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-E1SC_oz9m4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends this is the another video
tutorial on our web scrapping with the
help of Python beautiful soap library so
in this tutorial we are going to see
about how we can extract the way tvoaqui
PDF page and inside the Wikipedia page
what about the inbound links are
available which is nothing but
redirecting to another Wikipedia page we
are trying to extract in this tutorial
so let's see what our just inbound links
so for that we are going to use very
home page of this Wikipedia only so on a
Wikipedia or whatever these inbound
links are there like this person's name
or blaming harm or anything which is
having uh some kind of common or pattern
which is nothing but the individual
Wikipedia page so let's try to see the
content of this webpage so just right
click and view page source so inside the
page source if you have look at there a
huge amount of HTML and JavaScript
contain Sarkar but the main contents
will start from here this is the body
contain a chili will contain will start
from here then if you observe the lot of
links which looks like this which starts
from sorry today I just click it by
mistake so I will just search so we will
come to normal
okay okay so if you observe this there
are a lot of inbound links which is an
odd again redirecting to another
Wikipedia page only so something like
slash speak is less and there is some
content that content is nothing but the
individual page of death Wikipedia or
any kind of other portal so there are a
hell lot of actually Wikipedia links are
there inside the single Wikipedia page
now there is a common pattern if you are
observed it here or the Alba key links
will start from slash wiki / and there
is a one article name which contains any
kind of alphanumeric character in a
capital letter or a small letter plus
some numeric values also possible so
let's see how we can extract this
specific inbound kind of link with the
help of this beautiful soap library and
how we can extract this particular kind
of link we'd help up some regular
expression so are we are going to use
this very first page so let's just copy
before starting anything let's just
import a URL Lib library
as I'm using the Python 2x family
version i am using URL Lib - if we are
using Python ta 3x family version the
community has already renamed URL Lib to
do you are relieved so you can import
just the URL live and I will import a
URL open function okay and another
beautiful soap library so from vs for
import beautiful source okay so we
imported the two main library now let us
define our URL which you are we want to
X thread it so this is nothing but the
very first page of this Wikipedia
okay now let's fetch this web page with
alpha this URL open function and we will
supply the URL which we want to fetch
and whatever the content will be
available after the fetching this
webpage we will give the reference HTML
moon
so now whole HTML source code what we
have seen it here that is available
inside this HTML string okay so we got
this all content inside this HTML now
this type of things we have already seen
in a lot of earlier lectures also now
let's extract or let's beautify
all elements or let's parse all those
elements with the help of this beautiful
so function so inside the beautiful soap
we will supply this HTML and we will use
l xml parser so what it will do it will
parse this HTML string and it will put
into very hierarchical manner so we can
extract individual element based on ID
or a class or some kind of common
pattern or based on some kind of went a
Gore so so it will assign it to the BS
okay so we got everything inside this BS
now our next actual job will
start so we our tasks is to extract this
kind of links so this last week islas is
the common part and after that the
Wikipedia article name we want to
extract
so let's first the extract the
develoment where our whole content is
starting the very first I have inform
you that the whole Wikipedia content is
starting from this page this body
content tag after did actually kibriya
content so let's extract all the element
inside that and this element of body tag
first and out of those element we will
apply the another filter to find this
all those linked so be s dot fine we'll
apply this on to development it is a div
element having a ID is equal to body
contained
okay so we got all element inside this
body Content ID now let's apply the
filter on the top of all this element
what we have extracted so we love apply
this find all matter and we will fetch
all those links a so what it will do
actually inside this body Content ID tag
it will find all a all a means the all a
tag but we don't want all a tax
we just want which are the inbounding
and we inbound link has a one
characteristic we have seen all links
which is having an inbound link has a H
is property href which is nothing but
the link value which starts from /p g /
so we will use this regular expression
to extract only those kind of link so
for a regular expression we are going to
use re library that's a regular is one
so we need to import it so import
okay now in re we can supply this with
alpha re compile function that what kind
of pattern you want to excel so we want
to extract all a element which is having
href means href value is given by this
regular expression and regular
expression we can compile with the help
of this regular expression library re
now how to supply this regular
expression so we have seen that / v ki
slash is the common part so there is a
one group of content is last week is
less and there is another group of
content which contains all kinds of
alpha numeric value plus some specific
symbols so let's divide the whole over
but a regular expression into two part
so that is a one group and that is
another group so first one is the one
group and this one is the another room
and everything is repeating one or more
than one time so we are using this plus
sign for extracting this regular
expression if you want to match some
character with one times or more than
one times you need to use this plus sign
now in a first group we will just supply
this wiki class okay so the very first
group of slash week is last character it
will detect an in a second part it will
try to decay all kind of alphanumeric
characters so how can we give
alphanumeric character so we will give
like a to Z in a capital letter so it
will detect all a to Z capital letter
character which is having a zero or more
than zero occurrence or one or more than
one occurrence will supply even a to Z
in a small letter also because Wikipedia
page name contains a all capital and a
small letter also it can contain any
alphanumeric also so we will supply 0 to
9 also so all numeric value also been
including so there are other other
things also possible like a underscore
also is available we can supply even a :
also which we need to detail and we have
a simple cows also possible so this is
another part of or nothing but a
Wikipedia page name how we can explain
it so in this way we will find all
element having a tag a but its href
value means the link value will be given
by this regular expression and we will
store it inside the links reference now
there are a lot of links are available
so let's just iterate it individual link
so we'll iterate it individual means
then we will try to print it now
individual link contains either title
also or it can contain this href value
so you can see this a a tag which we are
going to get in return
so it has a acharya value in its title
so let us try to print both the values
so link
we can extract individual title with
their because it is in a kind of
dictionary for it
and we can print href also
okay so we got so almost we are ready to
run this program so what we did we have
extracted the web page from the
Wikipedia the very home page of the wiki
pedia we applied the beautiful soap to
beautify this HTML content so it will
pass this HTML content and it will put
it into very much hierarchical structure
so we can extract individual element
after that we extracted this develoment
which is having ID body tag and inside
this div element we apply another filter
we tell pup this find all function which
is having a tag a and is href value will
be given by this regular expression that
this regular expression is nothing but
the link of individual Wikipedia page in
inbound link of Indy pickup area page so
let us run it and let's see how what's
coming okay there is some problem
okay first let's try to run this thing
there may be some typos I think
okay this is looks like a correct
so problem is here might be find
all
Acharya
okay it looks like everything is correct
only let's try to run it again okay we
have some invalid sin text they are
saying okay there is a comma has been I
have missed the comma okay so it's
running so this symbol indicates that
it's busy it's trying to fetch the
pentane from this homepage okay we got
the continued mediately okay so we have
got this individual links like a
Wikipedia it's a title and it's a slash
wiki slash Wikipedia is nothing but link
the free content is the title and this
is the link so in this way we have got a
hell lot of links from the Wikipedia
which is nothing but again redirecting
to under Wikipedia page another the same
website Wikipedia only okay so we have
got this almost all links majority of
links we have got it I think so I hope
you enjoy listening this video if you
don't understand anything please do like
and comment it and subscribe it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>