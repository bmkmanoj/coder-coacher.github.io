<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning K Nearest Neighbour Regressor ScikitLearn on Boston house price Dataset  Part 5 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning K Nearest Neighbour Regressor ScikitLearn on Boston house price Dataset  Part 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning K Nearest Neighbour Regressor ScikitLearn on Boston house price Dataset  Part 5</b></h2><h5 class="post__date">2016-08-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xGoRCVryUDk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends welcome to the video
lecture series on machine learning
so in these tutorials here we'll we are
learning about the nearest neighbor
search algorithm so in the last couple
of videos we have seen about how to
calculate this nearest neighbor search
will work how you can extract those top
5 or top K nearest neighbor but these
are all used for the discrete value
output prediction so that is business
namely used for the classification
purpose that whatever the top nearest
neighbor you have expected out of that
majority what will go to particular
category and that particular category
will win and it will be decided for the
new test data prediction but suppose we
want to predict the continuous value so
how we can go ahead so in this tutorial
we are going to see about this K nearest
neighbor for the regression kind of
problem and those regression kind of
problem will be useful for the
continuous value prediction so for that
we are going to use this Boston house
price data set along the lecture I will
tell you what is the actual difference
of about between this K nearest neighbor
for the classification and K nearest
neighbor for the regression and what is
the actual difference and a trick behind
that will be used for the calculation of
continuous value while predicting the
outcome outcomes and outcome is nothing
but in this case the continuous value so
let's just use the Boston house price
data prediction so I have done necessary
inputs so a scale and data set import
then I reported this Washington data set
so they have given us the data
individual features then there somewhere
around 13 features are available in
description of individual features plus
as a whole data set description and a
target target is nothing but the house
price so I have frame it inside this
find us library so I have imported
almost all data inside the PI to PI
finders library io
geranium this column's name and one more
column I have added that is nothing but
the price and prices have been given by
this target index and let's just display
first few so ok tariffs first fire
course it has a high displayed so for
the first house for these features in
the house price is 24 for the second it
will it is 21 point 6 so in this lecture
our target is will supply the new one
test data set which is having all
different value for this 13 feature and
our task is to predict this house price
but this is not a discrete value it's a
continuous value so in a nearest
neighbor classification what we were
doing actually we were finding the top
okay nearest neighbor I mean the top
matches with respect to the query point
and a majority of what will go to
particular category so in this
regression kind of problem for
predicting the continuous value will use
the same one and scikit-learn
functionality scikit-learn library also
provide one more function like a
k-nearest regression rather than
k-nearest classification contrast to so
it will just use the same functionality
as a k-nearest classification it will in
the moment we provide the testing sample
it will calculate the top 5 nearest
neighbor but out of this top find
nearest neighbor what it will do it will
fetch its price price is nothing but the
output value and it will take the
average of those prices let's say we
have supplied some query point and this
fire result is the top 5 nearest
neighbor with respect to that query
point so how will predict the price for
that particular query point so for that
whatever this fire results has came as
an output for the top five nearest
neighbor it will fake the price of all
those fire records and it will take an
average of all those 5 which is nothing
but the house by house price predicted
house
for 20 points now let's see how we can
so that is the only difference between K
nearest neighbor for use for a
classification in K nearest neighbor use
further regression so in a last lecture
we use the digit data set because that
is used for the classification purpose
and that is output is a discrete value
between zero to ten but in this lecture
we are going to use marginal house price
prediction data set which is out whose
output is the continuous value because
it's a house price it can be any number
between some range or the third real
valued number so let's apply some sample
data set to turn our k-nearest
regression problem so let's make some
necessary imports for us so we'll use a
scale n dot two neighbors and in this
lecture we are going to use a K nearest
regression so in a last lecture we use K
nearest classification fire in this
lecture we are going to use K nearest
regression okay now there are total
somewhere around five hundred and twenty
round samples are available so we are
just going to use first 500 sample so
let's create our training sample first
so we use train underscore X
then we'll supply this Boston to the
data
and first 500 record so all all row till
finding and for all column we supply
this output label for each and every
variable also so Boston for that we are
going to use the target the target is
nothing but the house price for all this
500 records okay
so we have created our training sample
xn training sample Y which Y is nothing
but the house price and training sample
X is nothing but the 13 different
records 13 different feature for all
those 500 records now let's create the
constructor for the skinny razor
aggression
so k-nearest eraser same way in an
earlier lecture we need to supply how
many top K nearest neighbor you want to
get it consider so let's will consider
some first 20 neighbors out of those all
500 neighbors then we will assign it to
K so k nearest the regression and we
will supply our training set so in a fit
function will supply tech train x10
trained by ok ok so it has returned as
the K nearest neighbor object K nearest
regressor object actually so same way K
nearest classifier it has already
assumed lot of different parameters only
nearest neighbor we have supplied to the
20 you can tweak with all different kind
of parameter like a leaf size or a
matrix or matrix parameter weeds number
of jobs and whatever the algorithm used
for calculation this calculation of
distance between two different instance
or two different records okay so we have
created now let's create some test data
set now for first 500 record we have
taken it granted for the training set we
will use some arbitrary like a 504
number record for testing purpose so for
that we are going to use numpy librarian
we will create the testing sample so
we'll assign it to the taste reference
Marie
Boston today down and we will supply the
findeth forth number sample as it is
data second for us for because for 504th
number we haven't used for the training
sample now let's just reset it the way
scikit-learn want that taste data sample
to input for the input purpose here we
are supplying just
so one row in any number of column
because we are supplying just one input
taste it aside for the house price
prediction okay so we have created the
testing and data set now let's just
display the house price for this testing
data set also so we have already created
this Bosch boss is nothing but the data
frame object so for data frame will only
display this 500 all records on
versafine return okay so we have created
this tasting sample and let's just see
that finding it fourth number record how
it looks like actually so for 504 number
record is this 1 and output price is 22
we already know about it and training
samples is from 0 to find it all the
course now let's apply this test example
to our K and our reference K n R is
nothing but the K nearest regressor
constructor or K nearest regressor
object and let this algorithm to predict
what is the house price for this our
testing data set we have just one sample
in this case so we not ready k Nadi's
dot predict and will supply test 1 okay
so for find ridden forth number record
the output price is 22 now let's see
what our algorithm K nearest regressor
will classify we how we are going to
take first 20 records and whatever this
first 20 record will come those 20
records output output is nothing but the
price for all those 20 top 20 records
with respect to the query point it will
just average it out
okay so it has given us the 27 point 777
but our price is 22 so there is a good
amount of I think errors are available
you can now tweak with lot of different
parameters like you can change the
distance or nearest neighbor number like
a 20 you can take it even 100 also you
can change the leaf size you can change
the algorithm and you can try to achieve
the most nearest result so this is just
to give you overview at how you can use
this scikit-learn library then what is
the fundamental concept of different
machine learning library this all
machine learning results are a kind of
predictive modeling so it may be near to
your actual output it may not be also so
you can tweak with different algorithms
also in algorithm also there are a lot
of different hyper parameters are also
available so this is to give you very
small glimpses that how you can use it
so I hope you are enjoy listening this
video thank you guys for watching in a
future lecture we are going to see more
machine learning algorithm please do
like comment and subscribe it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>