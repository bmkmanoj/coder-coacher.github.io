<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Prepare Data Tutorial 12 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Prepare Data Tutorial 12 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Prepare Data Tutorial 12</b></h2><h5 class="post__date">2016-07-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kSslGdST2Ms" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends this is the 12th video
tutorial inside the video lecture series
on machine learning so in this lecture
we will see about how you can prepare
the data to fit the data inside the
different machine learning algorithm so
before starting how to prepare for the
data let's see some of the prerequisite
and recap what we have done till now for
the data preparation or the different
programming prerequisite required for
the machine learning programming so in
the last lecture we have seen about the
numpy the Python numerical manipulation
related a library we have seen about the
data set import that we have imported
couple of data set like handwriting
character recognition diabetes or Boston
house prayer price prediction data set
with help of scikit-learn library and
after that I I told you about the
prerequisite of the pile pandas that is
a data science or data analysis related
library inside this Python and mat plot
lib that is for the visualization
purpose so for both of this library I
have prepared a different video series
you can go through that video series
that is the much more basic require to
get started and to get those function
used to inside this machine learning so
the tour the link for this video series
I have given inside the description you
can see about it so these are all the
pre de cuisine now let's see we have all
imported this data what we can do
further so let's prepare for the data we
have just imported this data but now the
next these are the three tasks we need
to perform based on our requirement what
kind of problem we want to solve first
we need to select the data and then we
need to some kind of pre process data
then we need to transform the data so
these are the three essential
stuff that require before you fit the
data inside this machine learning
algorithm so why these tabs are required
required so let's see one by one let's
first select the data so while selecting
data you need to select the subset of
data
why just subset of data because the more
deep no doubt more data is always better
to get better accuracy of the machine
learning algorithm but if you select a
subset of data for the prototype
prototype purpose it is always invited
to take subsets of data which is
required based on your problem you want
to solve so it requires the least amount
of time also less amount of memory also
so better select subset of data
no doubt more data is always better so
there is a trade-off between these two
things while you're just starting to
coding inside the machine learning now
there is a data source so sometimes the
data is available on the internet
sometimes it is available on to SQL or
sometimes it is available inside this
file system of our Windows machine so it
may be available inside the different
form so it may happen that apart from
this data you may require some more data
which has not been record in any which
is not available inside the data source
but the problem you want to solve you
require those kind of data also and you
need to record those also there is a one
more part is that you need to exclude
some of the data so we will see about
that why this exclusion also require
that will be nothing but your selecting
the essential data which you actually
require for the further processing let's
see once we have the selected data the
subset of selected data
what kind of pre-processing we require
for the this data so first is the
formatting so it may happen that us the
data available inside the different
the different source will be in a very
much different format so source format
is always different in a require form it
might be different might be same so
whatever the data you have gathered you
need to transform into require format
which you can feed inside the different
machine learning algorithms let's say
your data is available on total Internet
of any web page but that web page you
can transform into directly the machine
learning algorithm so you have to do
some kind of feature engineering stuff
so that feature engineering stuff will
convert your data of whatever the
webpage available inside it to the
feature vector sometimes the data will
be available inside an SQL database or a
MongoDB kind of no SQL database then you
need to convert with the help of feature
engineering to the required format which
you can feed it into different ml
algorithm so this way the formatting is
must now there is a data our cleaning
stuff is required why data cleaning
stuff is that while analyzing all those
different records you may found that
there are some of the records might be
missing or even if the some of the
records like all records are available
some of the features are missing so
that's why you need to clean those data
you need to remove those data it may
happen sometime that there are all
features are available all records are
also available but some of the features
or particular record will be need some
form and some of the records of other
some of the features of other record
will be in some other form like some of
them might be nesting some of them might
be an integer so you need to Club it you
need to put it into very much same
format so you can feed it into the for
the processing now there is a data
sampling stuff so data sampling is
nothing but you need to take a very
small subset of data rather than taking
about each and every data which is a
but why that is important because while
prototyping or while initially applying
the different MLL algorithm if you take
a small subset of data you require a
very less amount of time to get it
processed less amount of power
consumption of your machine also less
amount of memory requirement also so you
can easily prototype it once your
confidence about that you participating
fine you can go for the very large data
set and all full data set which is
available inside the total data base set
now that last part is the transform data
so transform determines what if you have
selected data you have cleaned up all
data you have put data into required
format so now there is a scaling part
easel scaling first of all in a
transformation stage so it may happen
that some of the features are available
inside the centimeter sometimes so some
and some of the features are available
inside the meter forming so it is it
doesn't make sense so you have to
convert it into the proper format either
both should be in a meter or by either
both should be in a centimeter so this
way the scaling will perform so suppose
some features are available in a range
of 0 to 20 some features are available
between the range of minus by to 10
during the analysis of different phase
you will come to know about it but you
can't feel it like you have to normalize
the data so either you can define any
danger there is a very standard range
machine learning people will use they
convert all data between range of 0 to 1
so they just normalize it so all data we
can feed it in sometimes it happens that
you are getting a bad result because you
haven't done these steps because this
app is most essential because in that
case what is happening data the
centimeter numbers may be very high and
meta number is very low so this F 1
feature gives more important than I have
to feature but II actually I mean data
is
what normalized that's why those kind of
abnormal visas come so that is a scaling
part now let's see what is the
decomposition part now you are analyzed
data your scale the data you got that
there are some of the features are
available individual feature now you
have suppose the date/time object as
feature to the individual record in your
data set but that date/time object
contains total date plus month plus year
or individual time hour minute and
second also but combinely all those
quantity you just cannot supply as a
feature to the system so based on your
problem requirement you may consider to
break it down inside the date and the
time like hours as a time and a date
means the the date of that particular
month so in this way you can do the
feature decomposition and you can supply
to feature instead of one feature to
your machine learning algorithm next
step is the aggregation so how you can
aggregate the data sometimes you what
happened it based on your requirement
suppose you have been given a one
feature or a couple of feature that
feature directly doesn't make sense or
individually that may feature makes
sense but you can produce another
feature based on those feature also so
let's take an example of house price
prediction what we have seen in earlier
example so suppose in house price
prediction we have been given the house
rate and height as a feature of
individual house while deciding the
price of the individual house but you
can think that you can multiply this
weight and height and you can generate
area as a third feature also so you have
already have two feature f1 and f2 like
in a form of width and height but with
the help of some combination of two
features with NIT you can generate the
third feature like area of that house
also so eventually you got this three
feature and that three feature will be
much more helpful
for the deciding factor of your house
price so this way you can aggregate the
different feature and you can move ahead
so that is what all pre-processing steps
and how require before transformation
and then how you can transform the data
in our last lecture we will see about
how you can actually do those kind of
step inside the different for the
different machine learning algorithm I
hope you enjoy listening with you thank
you guys for watching please do like
comment and subscribe</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>