<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning DataScience interview questions - What are ways of reducing dimensionality of data? | Coder Coacher - Coaching Coders</title><meta content="Machine Learning DataScience interview questions - What are ways of reducing dimensionality of data? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning DataScience interview questions - What are ways of reducing dimensionality of data?</b></h2><h5 class="post__date">2017-03-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nI-m0jEN1KY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends welcome to the video
lecture series on a machine learning
interview question and answer so in this
tutorial we are going to see about one
more important question that what are
the ways of reducing the dimensionality
of the data so first of all why we
require the to reduce the dimension of
the data because if you consider this
video data or even our image your size
data it is a very very huge dimension so
very difficult to fit into our main
memory so it has sometimes from more
than a 10,000 dimension per image
feature so it requires to reduce the
dimension now what are the ways of
reducing this kind of dimension so I
have listed couple of one there may be
some more also but these are all some of
the minimum every developer applies
owner
data to reduce the dimension like
removing feature from a low variance
removing Col linear feature you can
combine those feature one can use a
standard technique like a PC a principal
component analysis or independent
component analysis so let us try to
understand little bit one-by-one fits
one small example so removing feature
with a low variance let's say some of
the feature have a or data across data
or value across all data like 1 comma 1
3 2 1 3 2 1 1 it is 13 arrows written by
mutation so if you see this data across
of one feature that has a very low
variance because it has not pure across
its feet it is just going around 1 &amp;amp; 2 &amp;amp;
3 1 2 2 1 3 this way so that is not
contributing to predicting our final
outcomes because its value is almost
constant throughout all the example data
we have available so we
if it is not contributing to decide or
what our predicted objective function is
it is a meaningless feature so we should
remove it because not enough
contribution so this way it will
automatically discard all those features
which is having almost zero variance or
whose variance is below some threshold
so this way we can discuss some of the
features because it is not contributing
enough to calculate or predict our
objective function let's see another one
removing collinear features so let's
take one example like we have a one
feature of x1 like a birth here x2 is
like our death here and x3 is equal to
let's say age let's say we have these
three features are available now you
keep consider these three features first
one is the birth here when you bond
death here when you when is your death
here and one is a but is you can again
write like that x2 minus x1 so death
here - born here
so ultimately this third feature which
is interdependent on a very first
feature so in this way you can remove
one of the feature and it is indirectly
accumulating all remaining features so
let's say our objective function will be
something like a outcome I will write as
outcome predict predict prediction
function like a X 1 W 1 plus X 2 W 2
plus X 3 WT
but in this case explains again our X 3
is again X 2 minus X 1 we should replace
this let's say X 2 X 3 with X 2 minus X
1 and if you simplify this equation your
whole outcome predicted value is
dependent on two variable only two
features only so you can remove that age
factor because age is nothing but the
dependent on another two feature so
whenever one feature is dependent on
another feature you can easily discard
it
that all depends on your domain
knowledge or expertise in particular
domain so this way one can discuss some
of the feature like removing curl in
your feature if one feature is dependent
on another feature let's say the third
way combining feature let's say we want
to predict the house price house time
house size and we have a data available
like or features available like a length
of the house so I will write the feature
here length length of the house in terms
of meter or in terms of food or we have
a width of the house so we can written
we can write our objective function like
W 1 is like a parameter and length plus
w2 into Chris okay now if you consider
in this case we are trying to predict
the house price and we are just using
the two feature length and width but
ultimately the we all know based on our
intuition that
housewives depend on the total area so
it is meaningless to the mind to feature
because we can merge this to feature and
we can write like only 1 w 1 x area so
we can combine two feature
okay sorry actually I'll write like a
drive here
I light it here only instead of that we
can write like this is the equivalent
format area and area is nothing but the
length into pit so you can combine those
two feature length and width because
length and width as independently was
the same way the area will work will be
much better to predict the housewives
because houses are directly linear
correlation with the the area of your
particular house because individual
length and width one contribute that way
but area will contribute the better way
to predict your house price because we
are not interested particular length is
bigger or which is bigger so in that
case you can emerge it you can combine
those features and you can create
another feature like area and you can
use it so this way you can merge in a
combined decision so we have seen
multiple ways like removing features
with a local covariance
removing collinear features you can
combine and emerge to features but these
are all based on your intuition based or
your domain knowledge you can use it for
the first one you can use with the help
of a statistical measuring technique the
remaining two all depend on your domain
knowledge because one feature is depend
on another feature so you can discard
some of the features if two feature you
can combine to create another feature is
multiple more than five feature you can
combine to create another two feature
that will be much more beneficial to
predict your final outcomes so this way
the last is the principal component
analysis independent component that is
all statistical measuring based it will
just try to find out that which
particular features is contributing lot
which has a very high variance so it
will try to expect only those principal
component which is a very significant
contribution across the data which has a
very high variance and it will try to
expect only those significant components
remaining all components it will
discard so this way we can reduce the
dimensionality in the case of principal
component analysis and a independent
component analysis so that's it for this
lecture friends so I hope you enjoy
listening video if you haven't
understood anything please write into
comments if you know about any many more
dimensions it is resolution reducing the
dimension dimension reduction technique
please write it into comment and if you
haven't subscribed my channel please
please to subscribe and like this video
at last thank you guys for watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>