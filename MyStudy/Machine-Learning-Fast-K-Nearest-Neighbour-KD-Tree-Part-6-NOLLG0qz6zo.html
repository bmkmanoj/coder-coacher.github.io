<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning  Fast K Nearest Neighbour KD Tree Part 6 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning  Fast K Nearest Neighbour KD Tree Part 6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning  Fast K Nearest Neighbour KD Tree Part 6</b></h2><h5 class="post__date">2016-08-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NOLLG0qz6zo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends welcome to the video
lecture series on a machine learning so
in this series we have started about the
K nearest neighbor and we have seen
about the implementation of the K
nearest neighbor in a new base or with
the help of built-in function available
inside the scikit-learn library now in
this tutorial we are going to see about
the fast KN n so whatever the new base
implementation we did actually
and whatever the built-in function we
have used inside the
scikit-learn library will see about what
is the complexity of those algorithm and
we'll try to find that is there any
other algorithms available which will
make it even more faster so let's try to
understand the complexity of K nearest
neighbor algorithm when you implement
very new way so let's say we have a
total and data points are available in a
training data set so this data set is
the data from where we are searching
throughout the whole data points space
given our query point and each data
point is the dimensional space so
individual data is represented by the be
different number so to find the nearest
neighbor we have to calculate the
distance of query point which each and
every individual point available in
inside the whole data space so there
will be a total n into B times
multiplication or kind of operation you
need to perform so when you go for n
become a very very huge something like a
more than a million or billion it will
be a very difficult in a run time to
calculate the distance from query point
to each and every point so our task in
this tutorial will be can we make it
little faster or is there any algorithm
we can use to make it faster so for that
actually we are going to use the KD tree
KD tree is nothing but it's a
k-dimensional tree then we will see
about
how it will make it faster with the
compromising of the space complexity so
it's a k-dimensional three
it's another extended version of the
binary search three in higher dimension
so you might have heard or learned about
the binary three binary search three so
binary search three works in a single
dimension in a one dimensional space you
sort this data and based on sorting you
just find the query point nearest point
within a login complexity but there is a
drawback is also available extra space
complexity will be added so in our
earlier case what we have seen we do not
require a extra space for creating this
kind of training representation of the
data but if you go with this K
dimensional tree approach for finding
the nearest neighbor
you have to create a training data set
out of training data set the model of
the data set that is nothing but the KD
3 construction so extra space for
creating this disk ad tree will be added
but there is a very good big advantage
is available which is nothing but the
time complexity will be drastically
reduced at the runtime so if query point
has been given we can immediately find
or in a log in complexity level we can
find what are the nearest neighbor for
this KD tree so let's see how we can
reduce this time complexity
provided space complexity will be
increase so let's take one example that
how we can construct this KD tree or K
dimensional training let's say we house
this simple six data set is available so
instead of just one point which we use
inside the binary tree this KD 3 is used
for higher dimensional data
representation so we have taken a very
simple example of a two dimensional data
in reality it can be a even a thousand
or even a 10,000 it can be a
million individual features for
individual had I call also possible in
case so specially amazed your video
related feature so let's see how we can
construct this KD tree now if you have
seen in a case of binary tree generally
we sort this data while constructing
this binary search tree so we sort this
data and we'll take the middle value as
our root node and whatever the values
which is less than that particular root
node we will put all those data point
inside the left subtree of that root
node and whatever the data point which
is greater than that root node which
we'll put inside the right subtree of
the root node so instead of one value in
this case we have a two value so if you
compare with the binary search tree how
we can decide based on whether X values
we have two features available so
whether based on X values or a based on
Y value we will find what can be the
possible root node for this KD tree
construction so what we will do for
there is a one good thumb rule is
available you try to find a variance of
individual features so how much data has
been eliminated along particular
direction that is nothing but the
variance the more variance will be there
we can take that particular dimension
into consideration so let's if we find
the variance of this particular feature
X and y I have observed that the
variance for particular dimension is
high so we will take in this case X as
our base root node so our first task
will be the sorting of this data point
along the x dimension so we have a data
like two five nine four eight and seven
so if we sort it it will be a two four
seven two four five
seven eight and nine so if we just sort
it and we'll take a middle value so
there are six data points so we'll take
a middle value as a fourth value which
is nothing but seven comma two as our
root node of the KDT construction now
for the very root node we have taken an
X dimension as our consideration for
creating this KD tree now what will
happen that whatever the value with
respect to X dimension which is less
than seven we will put inside the lap
subtree so if you consider this data
point two comma three four comma 7 and a
5 comma four all is having an X
dimension which is less than seven and
in this case a remaining points 8 comma
one and nine comma six
so all these two value have a X
dimension greater than seven so we'll
put it inside the right subtree of this
KD tree so we have decided about the
root node and whatever the value will
lie what are the data points will line
the left subtree and what are the data
point will line the right subtree so our
next task will be to create that this
lab subtree and right subtree so till
the root node we have sorted this data
based on x dimension now let's consider
the Y dimension for this left subtree
and write something so if you take this
Y dimension of this lab supreme point
there is a 3 4 and a 7 will be in a
sorted order so 4 comma 7 will be a
middle value so we will take it as a 4
comma 7 and in this case there are only
two values are available 1 and a 6 so oh
sorry actually it's not a 4 comma 7 the
middle value will be 4 so 3 4 and 7 so
pi comma 4 will be a our left subtree
root node and in this case it will
we can take a nine comma six so if you
consider here 4 and a 6 both are the
root node for our max iterative process
so already we have removed this pi comma
4 and 9 comma 6 from here because in
this case we have cut down the 3 with
respect to the Y dimension so 1 and a 6
so we have taken 9 comma 6 now further
further there are only 2 comma 3 and a 4
comma 7 has 11 now very first root node
we have cut with respect to X dimension
this we have cut with respect to Y
dimension let's cut off again with
respect to X dimension so if you do it
this way this 2 comma 3 so 2 comma 3
will be less than this data point will
be less than the X dimension of its
parent node and this dimension will be
greater than the parent ok the same way
this point will decide so this is how we
create this K dimensional data point so
in this case we have taken just a
two-dimensional yes if you have even
more higher dimension like a 10
dimension so for each and every
dimension we'll find the variance and if
the variance is the maximum value based
on that feature value we will cut this
whole data space into two part because
in that case we'll is equally distribute
the data into two sets of a data space
so that bell disk whatever the kd3
construction process will be there
whatever the kd3 will be created that
will be in balance so it won't be
resides just on the right half of the
sub tree or a life half of a subtree it
will be in a balance so both of the
places will get almost equal amount of
data either in a lab sub tree or a right
sub tree that's why there is
is to select this maximum variance
feature while cutting down the tree now
this is just the two-dimensional case so
we have taken a half width gap cut with
respect to X then then with respect to Y
and then again with respect with suppose
we have a very high dimensional space so
you can go for maximum variance again
you find whatever the subtree lab based
on that what are the again variance is
there whatever the subtree or left
subtree or right subtree data said has
been left you can put it with respect to
a variance available very ins of the
remaining data and you can keep creating
or keep cutting the space into half half
portion with respect to this variance of
data so this is how the kd3 construction
will work now let's observe the
complexity of this kd3 with respect to
our new based approach so if you observe
that in case of new based approach we do
not require an extra space actually
whatever the data points are available
we'll just start searching for the
distance between query point with each
and every individual data point okay
space complexity we have seen that no no
space complexity is the choir but in
case of this KD tree we need to create
this KD tree our k dimensional tree
based on the data available so we need
to allocate the extra space for all of
those point again plus the construction
of KD tree again requires some more time
so this is you can consider as a like
the training phase of our machine
learning algorithm so this is the one of
the trade off for this that you require
the extra space complex but once you
create this KD tree our construction has
been made for these three such
complexity will be drastically reduced
now suppose you feel given a query point
send for that query fine you keep
comparing first with the X dimension
then you again go to the second level
node and you compare that query point
with the Y dimension
in in a very leaf node or next level nor
European compared with the X dimension
or or any other dimension the way td3
has been constructed so Bettina just log
and step you can search for the
complexity now if you see suppose we
have a kind of thousand data set so in
case of new based approach we are
searching for a nearest neighbor with
respect to all of this point and we are
comparing the distance and then we sort
it
but in this KT 3 construction apart from
this extra space complexity required in
case of this search complex city we just
require login so log thousand will be
somewhere around that 10 so in-between
adjust and step rather than thousand
step we are arriving at the nearest
point with respect to our query point so
this is one of the big advantage for
this KD 3 and it's another extended
version of binary search tree so this is
all about KDP construction and we have
seen about how this base
space complexity and search complexity
in case of this K dimensional tree works
and how we'll find nearest neighbors
search with respect to query point now
there is a one big or somewhat you can
consider the problem in case of KD 3
this KD tree is not the exact nearest
neighbor search algorithm but it's the
kind of approximate nearest neighbor
search algorithm so you may land up in
to exact point which is nearest to your
query point or you can you may land up
inside the what are the approximate
nearest neighbor with respect to your
query point but in most of the practical
case it works very fine so this is also
one of the very useful algorithm inside
machine learning so this is all about
the theoretical aspect of the KD tree in
the next lecture we are going to see
about what are the built-in function
available inside the scikit-learn
libraries neighbors
nearest-neighbor module how does kd3
construction and how searching will will
be done inside this scikit-learn library
will use those built-in function for our
another data set so I hope you enjoy
listening this video please do like
comment and subscribe</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>