<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning K Nearest Neighbour  Part 1 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning K Nearest Neighbour  Part 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning K Nearest Neighbour  Part 1</b></h2><h5 class="post__date">2016-08-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SU8Alpp58KQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello friends welcome to the video
lecture series on a machine learning so
from this lecture onwards we are going
to learn about the individual tutorials
on different machine learning algorithms
so we will start with K nearest neighbor
classifier so let's see what we are
going to learn in this lecture so in
this tutorial we are going to start with
a very basic example to give you a
glimpse of what this nearest neighbor
classifier how it is useful in the real
world application what is the exact
problem definition will define a very
formal wave the problem of nearest
neighbor classifier then we will go for
the new implementation in a Python of
this nearest neighbor classifier then
the same implementation will go with the
scikit-learn implementation and then we
will see is there any optimized
algorithm which will reduce the
complexity while searching for the
nearest neighbor in this
multi-dimensional hyper plane ok so
let's start with a one very simple
example and let's try to him put or
let's try to correlate those example
with the help of nearest neighbor
classifier ok so let's just consider
this digit data set so this is a one of
the very famous
m-miss digit data set this is also
called one of the hello world
application of the neural network or a
machine learning also so in this data
set we have been given total ten classes
of category each class contains a
different different image of different
digits so the class one contains almost
seven different instances of image 0
digit the class 2 same contains seven
instances of digit one now in the case
of nearest neighbor what is our task is
suppose we have given one test image
which looks like six because for human
it's very easy to understand
so in case of nearest neighbor
our task is to find what are the images
which is the most near to this image in
this multi-dimensional space so suppose
our nearest neighbor algorithm has found
this three nearest neighbor which is
very near to this one so that is what
called the nearest neighbor with respect
to this taste image out of this whole
multi-dimensional space of all different
instances available so all of the
machine learning algorithm you can
correlate with this neural network
classification or display nearest
neighbor classification or nearest
neighbor matches let's take one more
example of Google search or even Amazon
product recommendation system so in a
case of Google search or product
recommendation or a product searching
you giving the query and what this
services will return this will return
the top 10 matches which is nothing but
the nearest neighbor with respect to
your query out of all products are
available or out of all links are
available inside the Google index data
set so that is how the nearest neighbor
works so this is very informal way or in
titute way of understanding what this
nearest neighbor algorithm is then let's
try to define a very formal definition
of nearest neighborhood so let's just
concentrate this two-dimensional space
so this space has some data set which is
having each having a two dimension let's
say this red class of point belongs to
this cat category and this green class
of point belongs to dog category so this
set of instances are belongs to the cat
and this set up in instances belong to
the dog now
these individual points are just the two
points but in reality it can be a 100 or
even a 10,000 or even a million point to
represent the single image also just for
the visualization purpose we have
projected into um only two-dimensional
space now let's see how this nearest
neighbor classifier problem will fit
into this category so these are one of
some of the two feature associated with
this nearest neighbor so in our case of
nearest neighbor this is kind of very
lazy learning kind of model so this is
all our features or even a pixel you can
consider as a feature in case of this
image so in case of this nearest
neighbor this feature itself are the
training model so there is no training
required for the case of this nearest
neighbor classifier problem so whole
training time has been reduced to zero
only testing time will remain as we have
seen in an earlier lecture that for all
machine learning application everything
has been divided into two step either in
an offline mode you do training and in
online mode you do kind of testing but
this is one of the unsupervised way of
learning problem so there is no training
required you just need to search out of
all possible value and matches fit the
test data set available most of the time
we'll use this Euclidean distance to
measure the similarity with the test
data set with the help of all other
training data set available so this is
all about the training so training is
not required so all those data's are
available itself is your train model now
you have all the train model suppose you
have been given a new image which looks
like a cat and you job is to find which
are the most nearest point out of this
multi-dimensional space out of all
images are available individual points
is nothing but one image actually that
point is the representation of this
image so what this nearest neighbor
classifier will do it will try to
calculate this new test data set
distance between this new test data set
with each and every other point
available inside the training data set
own and whatever the nearest distance
are there that will be classified as a
nearest neighbor with respect to
distaste data set so in this case it
looks like a cat
so whatever the point which are nearer
to the cat class it will classify as a
nearest neighbor whereas those point
which belongs to the dog class which is
much more farther away from the cat
category now let's put some vicinity of
circle so in our case we have defined a
problem with respect to the K nearest
neighbor this K can be anything it can
be a 1 or 2 or any odd number so in case
of this K is equal to 3 that means the
with respect to this this data set we
have a three points which is be quite
near a tutor is datacite so these three
points will be classified as a nearest
neighbor with respect to this test data
said this circle is not a prude of
marking it looks like that this two
might be the most nearest but anyhow
just for understanding purpose I have
kept a circle so in US vicinity of this
circle this three point lies that is
called the nearest point with respect to
a case theta said and out of that
nearest point which point belongs to
pitch class the majority vote when will
green so in this case all three points
lying in to red class array class is
nothing but the cat class so we will
classify this
image or this new taste image has been
given into the gate class so that is how
the nearest neighbor searching will
works so in a next lecture we'll see
about will generate to some random data
inside the Python and we'll try to apply
this Euclidean distance way of measure
on the top of this data to find some or
implement the very name approach of
nearest neighbor classification then
we'll go with some standard data set
like a Boston data set or iris data set
that we have seen in earlier lecture and
with the help of scikit-learn library
we'll try to implement this nearest
neighbor classifier so I hope you
enjoyed listening this video please do
like comment and subscribe it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>