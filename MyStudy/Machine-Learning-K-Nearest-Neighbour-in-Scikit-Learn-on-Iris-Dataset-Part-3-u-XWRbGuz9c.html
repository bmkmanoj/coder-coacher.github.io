<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning K Nearest Neighbour in Scikit Learn on Iris Dataset  Part 3 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning K Nearest Neighbour in Scikit Learn on Iris Dataset  Part 3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning K Nearest Neighbour in Scikit Learn on Iris Dataset  Part 3</b></h2><h5 class="post__date">2016-08-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u-XWRbGuz9c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi friends welcome to the video lecture
series on a machine learning so in the
last couple of videos we have seen about
K nearest neighbor classifier this is
one of the most simplest classifier
inside the machine learning algorithms
so we have seen about K nearest neighbor
and its new implementation with the help
of Python and not pandas like him
so in this lecture we are going to see
about how we can implement this K
nearest neighbor or whatever the
ready-made functionality available
inside this scikit-learn library will
apply on some of the standard data set
iris dataset so iris dataset in our
earlier lecture we have already seen how
to import it so I have already created
the code to import that's iris dataset
so this is some of the necessary imports
for whatever the standard data set
available inside the scikit-learn
library and I have imported the pandas
library I'll rename it to PDF so let's
just run it okay I'll successfully
imported now I am importing this load
iris with help of Lodi this function the
iris dataset and I have assigned the
different class to it okay this all I
have already discussed in an earlier
lecture that how to import this data set
in a Python library which is available
inside this scikit-learn now let's just
bring some couple of data so this
scikit-learn data set has a total for
feature one is sample length sepal with
pattern length and paddle bit so based
on this for feature it decides in which
class this flower belongs to hi so there
are total three classes are possible
here 0 1 and 2 we have seen already in
an earlier lecture so this all about our
our training and data segment now our
task is to apply the k nearest neighbor
algorithm on the top of that which is
already ready made available inside the
psychic reliable so let's make necessary
imports for SK learn library so from
as skilled I will import neighbors
nearest neighbor okay so what we did
actually there are a lot of class of
families of neighbors algorithms are
available inside the scikit-learn out of
them we have imported this nearest
neighbor let's see in a documentation of
scikit-learn so i'll just open the
scikit-learn documentation with this has
scale and so in a nearest neighbor there
are a lot of algorithms are available
this is the one class of families of
algorithm like a nearest neighbor k
nearest neighbor classifier radius near
neighbors classifier so all arthur's the
variance of each other some of the
algorithms are very fast with respect to
some data and some algorithms are very
fast with respect to some other kind of
data so we just use the nearest neighbor
the very simplest nearest neighbor
searching algorithm and we learn apply
on the top of this is data set so let's
just create a one constructor for this
nearest neighbor in the nearest neighbor
you it is already there are all default
value are associated like how much
nearest neighbor
how many nearest neighbor you want what
kind of algorithms you want to run on
the top update while calculating
distance between individual distance
what kind of distance metric you want to
use so either you can externally supply
all those thing inside this constructor
also any of you don't supply any kind of
parameter it will take all those default
algorithm so i'm just supplying one file
which is nothing but when i supply the
test dataset give me the fast by most
nearest instance point okay and i'll
assign it to the and that is the nearest
neighbor and in a nearest neighbor i'll
treat this is the
I recently done so we have all created
this nearest neighbor constructor so it
has created one object of nearest
neighbor and we have supplied this iris
dataset so iris dot data is nothing but
the actual data available of sepal
length sepal width title length and
petal with data we have supplied it so
this way we have already created the
training part is kind of over because it
doesn't require at all training but it
has written us the nearest neighbor
objects which use all those default
parameter like algorithm it use the auto
but you can externally supply all those
algorithm also let's see what are the
possible algorithms it can accept so if
you supply the constructor of this
nearest neighbor there are a number of
parameters like an neighbors how many
neighbors you want we already supplied
five default water so five the radius
the radius among which you want to
calculate all your distance or all your
point distance there are algorithms like
Auto so it will automatically decide
which is the most appropriate algorithm
based on your training data are
available you can use the Bowl tree as
external you can supply it or you can
use KD tree or even brute-force search
algorithm also or you can supply so in
this way you can change those parameter
but I have kept all those default you
can tweak all those parameter and you
can experiment with it so this is all
about creating the nearest neighbor
construct a nearest neighbor object we
have created now let's create some taste
data and with respect to test data let's
find what are the five most nearest
neighbor we tell Bob just single line of
code
so for creating place data we require
this numpy library so let's use this
number
by as the empty hand will rename it to
and ping now let's create some test data
so we'll use NPRA and we will supply the
one instance of training or testing
sample one one tasting sample
so each testing sample also contains
there are four features so first we'll
supply sepal length then sepal width
then petal length and petal bit so let's
just supply some arbitrary number like
five point three then three to two point
five it's just arbitrary you can use as
we have imported all those data inside
the data frame object so you can apply
describe matter also so it will describe
the individual feature related
information like min Max or even a
standard deviation or average value of
individual features also so based on
that you can predict that individual
feature lies in a which mean let's just
display that one also I'll just do
describe okay
so that way you can analyze data I
already discussed those thing into
earlier lecture so individual feature as
a minimum maximum standard deviation and
how many total data's are available for
that particular feature so let's stick
with our testing sample so is this the
testing sample is a one dimensional
sample which is having of one instance
of record and for different features we
have supplied it now we need to reset p2
let's do two dimensional say because
either you can supply one sample also
you can supply more than one sample as a
testing sample also and for each of the
sample it will return us the Pioneer s
neighbor for us so let's just reshape it
into two-dimensional matrix shape to the
total one row in any number of column so
column we don't know how much in this
case it is of any hope for only okay so
we have created the testing samples we
have already supplied our training data
set inside the nearest neighbor
constructor so let's just now apply this
key nearest neighbor indirectly the top
five nearest neighbor which we want so
let's just supply N in not it has one
matter K neighbors so we know we want to
find it K neighbors on the top of and we
will supply testing sample said that for
this sample I need the fine nearest
neighbor with respect to whatever the
data we have supplied data vo supplied
in this case we tell pop fit
functionality so let's just sprint it
okay so it has given us the two set of
array the first set of array is nothing
but the distance of our query point or a
taste dataset with respect to whatever
the five most nearest so it has it is
already always in increasing order so
1.8 to 2.0
to two point zero to two point zero five
two point eleven and the second array it
has given us which is nothing but these
numbers of instance record available in
your training data set which is the most
nearer so let's just display it with the
help of data frame object this part so
we will supply with the help of IX
functionality so we display these
numbers of record okay so with respect
to whatever the tasting sample we have
provided which is having a 5.3 as a
sapling 3 as a separate 2 as a pattern
length and 2.5 as a title weight it has
given us five most nearest neighbors
which is lying at a ninety eight sixty
four 4323 and fifty seventh numbers in a
record index number in a training data
set so this is the four five final
instance record which is the most
nearest and it is given as the class
also because we have already assigned
this class column with the help of
whatever the target data set available
for individual so you can see that this
is the fine inverse it has immediately
given us with respect to this K inverse
functionality so out of this pi there
are threes are three instance recorder
belongs to one class in a tube instance
record belongs to class zero so majority
with what will go through the class one
so we will classify this our training or
testing sample which hypothetically we
have created we assign it to the class
one so this thing we have manually
created we have it has just given us the
saw distance only and based on those
distance we have manually
decided that how many nearest neighbors
belongs to particular category and
majority what we'll go to which kind of
category so this functionality will just
give the nearest neighbor only but there
is another class for seok-ki nearest
classifier so what this genious
classifier will do instead of this just
nearest neighbor it will calculate this
nearest neighbor also and then it will
assign it to particular class category
also based on your matching criteria or
based on your majority of what will go
to which kind of category so in this way
with just couple of lines of code you
can analyze the data and you can apply
this K nearest neighbor stuff also so
this is one of the very useful library
in a machine learning so that's it for
this lecture in a future lecture we will
see about another nearest neighbor
functionality even a distance metric
functionality what kind of different
distance matrix measure what kind of
different algorithm you can apply rather
than some default algorithm so I hope
you enjoy listening this video please do
like comment and subscribe it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>