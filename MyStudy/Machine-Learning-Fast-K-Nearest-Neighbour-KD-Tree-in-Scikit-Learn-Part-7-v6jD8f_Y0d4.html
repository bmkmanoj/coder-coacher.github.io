<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning  Fast K Nearest Neighbour KD Tree in Scikit Learn   Part 7 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning  Fast K Nearest Neighbour KD Tree in Scikit Learn   Part 7 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning  Fast K Nearest Neighbour KD Tree in Scikit Learn   Part 7</b></h2><h5 class="post__date">2016-08-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v6jD8f_Y0d4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">machine learning so in the last lecture
we have seen about the fast kanan so how
the fast K nearest neighbor searching
algorithm works so we have seen one of
the algorithm K dimensional tree so we
have seen the theory of how actually the
KD tree works how the KD tree
construction works how the space
complexity increases in the case of KD
tree and how the testing time the
particular testing time will be
drastically reduced in case of KD tree
so in this tutorial we are going to see
about how we will implement or how will
use the ready-made functions available
inside the scikit-learn library for
using the k dimensional 3 so you know
earlier lectures of this K nearest
neighbor series we have already
implemented the nearest neighbor part
and we supplied some dummy data and we
experimented with it the same part the
nearest neighbor we are going to use
here ok so I have already open the two
window so in a one window we are going
to use this brute force search way of
implementing this key in an algorithm
and in another window
I have supplied the same code but will
supply and will try to find the nearest
neighbor with the help of KD 3 in a
scikit-learn library so let's begin
let's start with a brute force way so we
already earlier did it if you don't
supply any kind of algorithm by default
it will take autumn and automatically it
will be side based on the data but in
this case we will explicitly supply that
you use this brute force search so it's
a new base implementation we already
earlier implemented those thing so let's
just in both the cases we'll do a data
input so I have already created a very
small snippet for the data Cree
generation so we are going to use this
numpy library a random function and we
are going to generate somewhere around
100 million records and each and every
record having a three feature in a both
the window I have already created this
snippet so let me just
and it will create in the training data
for us in our
which is having a dimension of 100
million cross 3 so total hundred million
record in each record is having a three
feature the same thing in case of KT 3
or so okay so almost in a both the case
it will run we it will generate this
training data now let's create this
start with the training almost my
machine has been
planing parting about the cases we have
already earlier discussed in our
theoretical aspects of this knn
algorithm that in case of this
brute-force searching algorithm in here
I mean the new way implementation of
this finding nearest neighbor search it
doesn't require any kind of training
whatever the input data is available by
default itself is a training so I am
using this nearest neighbor constructor
and we are going to supply this phi phi
is nothing but phi total nearest
neighbor i wanted an algorithm we are
going to use which is nothing but the
brute which is brute-force search you
find the nearest neighbor for us and in
the case of this KD 3 the exactly same
thing only thing is that in case of
algorithm we are going to supply this
clearly underscore tree so what what is
the difference of running these two this
will just create this nearest neighbor
construct our nearest neighbor object
for ters for us then we will apply the
testing data on the top of this nearest
neighbor object but in case of this
brute force it just creates it and
training data as it is it will take it
as a model but in case of this KD tree
it will generate actually the KD 3 so it
requires a huge amount of time for the
creation of this KD 3 and it requires
that much amount of even memory extra
memory also so I am expecting that my
machine may freeze actually so let me
just run the training part of this KN
brute force way ok so it has immediately
created so that is nothing but it
signifies that it doesn't do any kind of
training it will take just input data as
it is and it will create table check and
it will fit those data as is inside this
nearest neighbor object now the same
thing suppose if we run in case of this
KD 3 in case of reading so you
create this k-dimensional tree hole
construction of k-dimensional tree going
on in this case and it will fit those
kd3 as a part of so you can observe that
this training is taking too much huge
amount of time that signifies that it
requires a exceptional amount of memory
also and plus the constructed tiny
construction time for the kd3 also
required that is a nothing but a part of
training so I require a huge amount of
memory because I have created almost 100
million I record now you know both the
cases still it is running so that means
it requires a great amount of time for
the training but then we will see about
that
how the testing part is immediately
improve in case of this scary dream
still it's running so in case of brute
force such this training time is almost
nil but in case of this KD tree the
training time is quite huge but that is
a kind of offline process so you can
bear with those kind of time okay let's
wait for some time and let's see how
much time it will take for creation of
this KD tree it's taking good amount of
time still
now before it runs let's go for the
tasting part of this brute-force search
me now in case of brute force search way
this day I have created a very small
snippet and I have even integrated with
the time so it will give us the total
amount of time taken by execution of
this code so I have created a very
simple data set same data set now both
the cases a three dimensional data set I
have reshaped it because that is the way
this testing part requires and in a top
of this nearest neighbor object I am
supplying this testing data now in case
of this testing data what happens that
whatever the query point we have
supplied it will compare this query
point with each and every other point
available in a training sets so it
requires a really exceptional amount of
time so let's do this tasting part and
let's see how much time it will take but
anyway to still this training part is
going on so what I am expecting that I
will reduce this training data a little
bit so it will run little likeliest
faster I have taken a good amount of so
I am just interrupting my kernel I'll
just reduce it by two steps and I login
regenerate data with help of just 100
thousand records let's see how fast it
is okay still it's taking
okay let me disable the whole jupiter
not book textbook colonel and i'll read
on it again with a small amount of data
okay so I'm just restarting my colonel
for the Jupiter notebook okay
it's reconnecting I'll just refresh it
I'll this time I am going to run with a
small amount of data and I will show you
that how the training is taking too much
time in case of this kd3 part and how
testing will be drastically reduced in
case of KD tree construction so I am
just reduced the data by 100,000 so let
me run it first okay so it has created
this training now training also
immediately created so training is a to
much less amount of time required
because it just takes data and training
is almost unknown now let's go for the
testing and let's calculate how much
time it we require so you can see that
for the whatever the testing record we
have supplied it has calculated this
testing record distance between this
testing record and each and every
training data it sorted those thing and
it has returned us the fine nearest
neighbor so in this case it's not
important what are the distance and
index that we have already seen earlier
for this lecture my intention was to
give you how this testing time will be
very high in case of this brute-force
search algorithm but the training is
almost zero so it is take it has taken
almost 122 milliseconds or loop so that
signifies that for searching the single
records nearest neighbor it requires it
to 122 millisecond now let's see how the
same thing will work in case of this kd3
so I am going to just supply this much
amount of data in case of KD tree
and let me run it okay now same thing
because of that only we interrupted our
whole coding part no we are just
supplying the hundred thousand record so
let's keep the finger crossed that it
won't take much time okay yeah it has
immediately written us but it takes
little amount of time compared to what
this brute-force search algorithm did
actually so it has created this KD tree
for us in this little amount of time so
training requires a little bit time now
we'll see compared to brute-force search
algorithm tasting part how this kd3
construction tasting part has
drastically improving okay so it's going
on actually it will run for a three
times so that's why it is giving us
impression that it's take too much time
but almost it has taken a best-of-three
so a 151 microseconds only so if you
consider this 122 millisecond compared
to 150 one microsecond so it's nearly
about that I am expecting almost
thousand time improvement compared to
brute-force search algorithm so that is
how the credit is very useful in case of
testing purpose so I hope you enjoy
listening this video and you have
understood that how this fast K nearest
neighbor algorithm will be very
beneficial in case of testing phase I
hope you enjoy listening this video
please do like comment and subscribe</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>