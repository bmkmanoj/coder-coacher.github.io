<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning DataScience interview questions  Explain Regularization effect on curve fitting | Coder Coacher - Coaching Coders</title><meta content="Machine Learning DataScience interview questions  Explain Regularization effect on curve fitting - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/MyStudy/">MyStudy</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning DataScience interview questions  Explain Regularization effect on curve fitting</b></h2><h5 class="post__date">2017-04-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I-ncaW_F6PY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi friends let's take one very important
question regarding to a regularization
and the effect of regularization on a
cavity so let's recall what is
regularization and what is the curve
fitting then we will try to correlate
that what is the different value of the
lambda which is nothing but the
regularization and its effect on a curve
fitting so what is regular edges and
regularization is we are using for
avoiding the overfitting effect so we
are introducing one more extra term into
our error function our hypothesis
function so hypothesis function is
nothing but the function of expected
minus actual plus we are introducing one
important term of regularization where
we are giving one hyper parametric value
lambda and that different value of
lambda how it is affected to the
different curve fitting that we we are
going to see so based on the different
value of this lambda which is nothing
but the regularization parameter I have
introduced here the for graph and for
different value of lambda how it is
affecting to the fitting based on the
different a based on the same polynomial
so let's try to understand let's say we
have this many points are available in a
first graph or even all this graph are
the fifth order polynomial
all our fifth order polynomial on e but
let's introduce the lam defect or a
regularization effect to each of this
graph and let's see how they are
behaving once we increase the value of
this regularization parameter lambda
once we decrease the regularization
parameter value lambda so for the first
one we are fitting to the fifth order
polynomial where we have introduced the
lambda value is equal to zero lambda
value now you can see that there is no
regularization we have introduced here
so it is completely stick to the input
data this curve and this curve has
almost memorized all those input data so
that is very good it is performing on a
training data but it is very stick and
it has memorized the input data training
data but that's why it won't work very
good enough testing data so overfitting
kind of phenomena is occurring so no
regularization overfitting is occurring
now let's introduce little
regularization or we'll introduce or we
will increase little value on a lambda
so this graph is corresponding to the
lambda is equal to 0.1 so once we
introduce little lambda into it the GAVI
graph is not completely fitting to the
all those input training data point but
it is moving towards the more
generalization this graph let me show
you it is even more higher value of
lambda so it is very close to 1 now we
introduce lambda even more higher let's
say it's a it is a tan so it will even
generalize better way so once if you
observe this we are introducing more and
more value or even higher and higher
value to over graph or even now into our
our error function the higher value of
the lambda regularization parameter it
is not completely stick to the input
data the overfitting kind of freedom
from graph one to four it is decreasing
it won't perform exactly very good on a
training data and that is a good for us
because it is moving towards the more
generalization it is moving towards the
less memorization because if you see the
very first graph it has completely
memorized the input data so it is
perform very well on a training data
good on training data but very bad on a
testing data because it won't generalize
well but once we introduce this lambda
parameter the regularization parameter
it is not very stick to the input
training data so the training error is
little bit increase but it is
generalizing well also so a neutral
perform at least better than this
phenomena or even a first graph it will
perform better in a this graph testing
error will decrease in the third graph
again you are putting or increasing the
lambda value so even more training error
will be introduced but it is
generalizing even more better way so
testing error with even more decrease
and we are moving towards a less and
less overfitting kind of phenomena we
are now in a photograph we have
introduced the lambda value 10 even more
or lambda value higher training error
but it is generalized very well on a
testing data so let's summarize that
based on the increasing from graph one
to four let's see we are moving from
graph one to four what is happening
overfitting phenomena is decreasing over
fitting decrease
training error little bit increase but
tasting errors is very good testing
error decrease lambda value increase so
from 1 to 4 we are introducing more and
more higher value of the lambda so that
overfitting kind of phenomena is
decreasing all forth graph we are
fitting to the fifth order polynomial
only they are all performed well on a
training data little bit training error
will be introduced once we introduce
towards the high biasing or but testing
error will continuously decreasing so
the conclusion is that we introduced the
regularization parameter and we have
removed kind of overfitting phenomena
occurring in our model but as the
overfitting is decreasing it our data or
our model will perform better on our
testing data so that is what the effect
of regularization parameter on a cow
fitting regression model so that's it
for this lecture friends I hope you
enjoy listening this video and if you
haven't subscribed my channel please
please please do subscribe to support
the channel and at last thank you guys
for the watching</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>