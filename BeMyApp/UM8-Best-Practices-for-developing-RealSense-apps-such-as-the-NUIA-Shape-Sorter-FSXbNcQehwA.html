<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>[UM8] Best Practices for developing RealSense apps such as the NUIA Shape Sorter | Coder Coacher - Coaching Coders</title><meta content="[UM8] Best Practices for developing RealSense apps such as the NUIA Shape Sorter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>[UM8] Best Practices for developing RealSense apps such as the NUIA Shape Sorter</b></h2><h5 class="post__date">2015-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FSXbNcQehwA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Stefan I'm CTO one of the
Calvin
and we are developing for some years no
for natural you sent action meaning
we're doing stuff and gesture
recognition speech recognition I
tracking mind control which is a little
bit far nephew Chester from today but
today we're talking about intel
realsense so like Thomas and Martin we
also using different gesture
technologies in in interacting with our
middle layer which is called new year
for natural user interaction and what I
going to do today is I going to present
some best practices what we learned
within the last year's and using
basically today concentrating on speech
and gesture recognition then I'm also
going to little deep dive into the new
year shapes order which is our contest
for the realsense contest our game
education game for the realsense contest
and that's it's basically what i'm
trying to do since this is still not
release them i'm basically controlling
now the powerpoint here with intro real
sense so I do a gesture like this and
then I just go to the next slide so this
is what we can see stuff the real sense
is really not just integrated into
existing or into new applications but
it's also integrated based on new year
for example into existing applications
so like PowerPoint I don't need to have
this presenter no because if i would
have this present I could grab my hand
on this but I wouldn't have a free hand
for doing anything else so why are we
talking about natural use of the action
basically as we see people are they are
satisfied with their computers today but
the problem is we're using mouse and
keyboard for 30 years now over 30 years
no 34 years now and we are going kind of
all the people are sitting here kind of
are used to this but if we look at
people who are the millennium so they
call it they are more like used to the
touch screens so like the first
iteration of natural user interaction
and if you ask people then if
is better they ask you because it's more
intuitive and it's straightforward you
directly manipulate stuff you're using
and not have to if you think about your
mother or your grandmother learning to
track the most like this to control the
cursor like this or then go to a
different computer we have different
acceleration settings for a mouse that's
awkward so you basically don't want to
do that stuff so why we do why we
consider this it's basically okay this
was one slide too much and what you can
see here now is I'm going into the best
practices later on what the problem is
of these kind of modalities new natural
use into action Modell's I have two kids
e 1 is 2 and a half and as the second
one is below 1 and they already did
below one is not pointing but the two
they start pointing with one year
because that's super easy and there my
daughter is controlling she's watching
YouTube videos and she even knows how to
get rid of the advertisement but the
thing about you sit there in front of a
normal computer she wouldn't know how to
use the mouse in the keyboard so that's
that's the main stuff the main advantage
of that stuff so posture this is
something like everyone is sitting at
this some you can stand up and using
speech control you can stand up like I'm
doing right now and using gesture
control etc and speech is well what how
we humans to communicate with each other
it's not by typing and stuff it's by
communicating it's using all our senses
and that makes sense real sense haha
because then basically it's it's we see
different kind of um if I talk to you
like this and I would not turn my voice
up and down like in the church it feels
awkward right but you see that I'm doing
something with my hands so that word
that sounds natural end and basically
our vision at 42 is that in some years
not far in the future we will
communicate with computers like we do
between humans today it sounds oh really
but in some years and what we see here
for example is a asus laptop with
integrated
real sense camera so these things come
to market within the next month already
say they're already shipped in Japan
started this stripping residence devices
so for the developers it's always the
question do I should I develop for
something which is not in market yet so
it's a chicken egg problem but they
going to be shaped and Intel is pushing
on this in this area so there will be
devices using this gesture modalities
also speech recognition and I guess I
tracking technology in the future so um
devices not only computers but also
handhelds will understand the
environment much more and more talking
about gays if you click at something you
first look at it and then you use your
hands off the keyboard go to the mouse
move the mouse click and back to the
keyboard but you already look for it so
you just look at it boom then you click
that's the advantage of case you're
already there so meaning every sensor
has its own advantages and disadvantages
and as you can see here the what I was
talking about these are the natural what
we call natural capabilities a speech
command for example is great for giving
complex commands attention command is
super for just swiping three pictures
right so like we got used to it with our
phones and then going up to gaze as I
said it's perfect pointing if you want
to point by speech that's nonsense if
you want to point by gesture it works
but it works a little bit of right if I
point to you you know that I'm pointing
at you because I'm looking at you and
pointing so using different modalities
at once gives you more precision about
what you really think the users doing I
can use speech import from from any
distance I even can go here and the
computer doesn't even know that I'm in
standing in front of it if i use gesture
real sense is optimal distance for real
sense interactions like this distance it
goes to this distance but if
if I am at this distance why should i
use gesture I just used a kid to touch
board right so it's a matter of where as
the use standing is he walking around
the room is he kind of like an assistant
dictating something or is he sitting in
front in a more productive mode so what
is basically what we're talking about
today is the difference between natural
user interfaces and gravity use
interfaces which we which were
introduced in the 80s or even before new
year hardware new new hardware is an
early stage we have to admit that so
it's the real sense diversion 2018 is
completely different in terms of
recognition rate precision etc like
basically again my daughter when the
first thing I said look at this she was
like uh what did what he mean and then
he was like yeah and then meanwhile she
knows exactly where I'm pointing it so
she knows the transition between
basically where I'm pointing it and her
eyes and basically is the same kind of
maturity will come to all kind of
natural user interaction interfaces
think about Syria google voice accretion
today and like eBay and via voice
whoever knows this still it's it was
crap back then a actually so you had to
train etc you don't want to train if you
just want to talk to it and it must
understand you even if it's Australian
accent again best practice I come to
this later on false positives and false
negatives both are an issue so
procession and recognition rate I
already talked about interfaces will be
more intuitive because you don't have
like fifty eight buttons which you need
to show an icon and try to differentiate
a 58 icons that's basically not possible
but and that's the problem is they will
also put get more not that intuitive
okay that was there was an internal joke
anyway so which ones commands can I use
right now which gestures can i use you
must be you must introduce these
gestures to the user so as we heard in
the top before it's an intuitive Cech to
do something like this but everybody
learned that doing this is in a in a
photo application go to the next picture
right and do this and that is to pinch
and zoom so you don't want to like
introduce new gestures for already known
stuff that basically everybody out there
knows and you can do direct manipulation
of objects think about for example again
a combination of sensors I look at
something at a picture and I do this
gesture so the computer knows which
object I want to manipulate so again I'm
using different sensors in this case
gaze and gesture but in with the real
sense you can use speech and gesture for
example to to integrate coming to the
best practices basically in general um
as I said you don't want to recreate
interfaces if we think back to 2001 Bill
Gates who was at his highest point
prolly back then but the nose he tried
to use a touch screen right but he
failed because of what because he
basically used windows XP a mouse
operating system to use by touchscreen
even by a well one of the first
iterations of touchscreens so it didn't
work and he put and he probably lost a
lot of money in that in that adventure
but what then the iphone basically has
the first mass device did it did it
right because they use touchscreen
modality and interfaces and that won't
fit together and that's the point if you
use if you integrate or if you invent
chester applications if you use speech
or gaze applications you cannot recreate
or a mouse application that that will
not work then you need to think of the
use of scenarios of course
so in an open space and why mad like
here I don't know how it works out if
the people are calling doing telephone
calls but if everybody calls like
maximize my window the guy in the
opposite of you the window is maximized
maybe then if you do not have a
directional microphone stuff like that
it's an issue and of course if you do
I'm swiping 3d pictures you really get
strong but them so you must think about
which gesture you repeat how often and
again false positive or negative we have
seen a lot of them already so you need
to do something like this that works now
no it didn't work so what happened we
showed that something he it's now
possible to do a gesture so I can the
user knows if it's fades away again and
nothing happened it was just not
recognized but if I don't show anything
in this in the gesture recognition is on
the user may I think the cameras off the
whole system is broken whatever so it's
about giving hints direct feedback even
if it's like delete something you don't
want to delete delete computer as a
speech command you and there's no undo
any more than network of course so you
want to basically for example add
something like do you really want to do
this so options to to do it or do not
then you we are also going in the
algorithm detection so any in a gesture
detection directly not just relying on
the standard Chester set from Intel but
doing gesture detection like the guys
remember for me did so you want to have
two different algorithms like the iphone
does that the iphone if you do a swipe
from left it checks if there's a
algorithm for coming in from the side or
from the within the system and the first
algorithm that says oh that's me then
that's algorithm triggers the command
and the other algorithms just says okay
that it wasn't me so maybe maybe you
used more than one ever since to make
sure that it really works and get a
better precision and robustness
yeah and that test test test not just
you if you do it it works on my machine
great but then you chip it and then it
still works on your machine but not on a
different another machine so simple
command I just them because I'm probably
already late right in 10 minutes okay so
simple simple example how to basically
combine centers from the influent real
sense if I'm talking to you you know
that I'm talking to you because I'm
looking at you right but if I'm actually
talking to him he will not respond to
that right don't feel asleep he um so if
I talk to my computer like this and the
system the phaser condition sees that
I'm looking at the monitor then the
computer things okay I'm talking to it
he's talking to me my master is talking
to me but if I'm talking like this I
need to have for it in this example a
trigger word I say okay for example knew
you could you please do that in this
because it's the same like humans if I I
don't know your name now but you would
respond if I would call your name right
probably yes maybe tomorrow so that's a
simple example how to combine sensors
and the information to compute a nose
because what we call it is empathic
computing that's the computer in the
future we'll know more and more and more
about you it's not about your personal
data it's about what is the environment
what is the context of you what are you
doing currently if I'm having a phone
call then probably and I'm even looking
at my at my computer I don't want my
computer to respond on something because
the computer knows that I'm calling
somebody and because there's a gesture
you could train that this is not a
normal position in front of a computer
for example so some best practices
define all this was general and now it's
going to cheshire control we heard of
some of that we already heard in talks
before so simple commands direct
manipulation that's where gestures is
great natural and intuitive gestures
real life inspired Oh
or at least like iconic so if you see
something and you show to the user it's
not like like a pistol shot you can
describe it or you do an icon in this
way then most of us are right-handed
right try to do that church of the left
hand it feels a little bit different for
the computer but we don't want to
lecture on the lefties right so we knew
to need to perform this the same gesture
and train it with both hands you were
the guy who didn't know Mahone it's a
great game just google it and it was a
great game in the 90s probably the first
the top game before before app stores
but that is something you cannot expect
that everybody in different countries or
in different cultures knows what this
gesture means basically this gesture
thumbs up but there has regions in this
world where this gesture is not a really
nice gesture so um you might think about
using these kind of gestures also like
them people at different habits like if
I have a really Australian accent or
something even even even the Apple
struggled with it the first shipment of
theory it didn't work in Australia and
like the the AUSA's were really pissed
about that anyway so then you don't want
to occlude the screen meaning like
Church like that fits awkward right so
you need to think about where is the
content in where is my hand field of
view fov is a very important thing that
the gesture camera like I said is this
distance works and then it goes up like
this and down like this like this this
is the the area so you want to tell
somebody if somebody does a gesture and
it's going out that something started a
movement and then he left the screen
again you need to hint the user that
something didn't work because of what
left the screen for example stay
consistent of course this means
everything is the same like in a picture
okay that's that's right that's the next
image but in a music application what is
this next track next album so need to
think about what is the chili on maybe
ask people what would you think is this
the right cheshire for that I and then
we see notebook here we have seen the ad
on camera honor honor any existing
device maybe even on a normal 24 or 27
30 inch screen and we see all in all in
months they have different user
distances they have different angles
 etc so you need to consider this
these environments when we talk about
voice control mm-hmm complex commands
and if I'm standing here the test in the
center or still knows what I'm talking
about I hope you too so natural natural
language you don't want to like if you
think about some voice-based automatic
voice based system you call sometimes
they stay feel strange awkward what the
voice is talking it's even better it's
better today but in the beginning so you
need to be like we uses like we humans
communicate with each other it's not
about like inventing something new talk
like you do I can use dozens of
different commands to do the same thing
think about that use synonyms it's not
just go to next page it's maybe also go
to the next page or go to just next page
or stuff like that coming to this avoid
single command words because the
recognition rate of single words is
really low for us humans it's quite okay
but for similar word even if it's a
short word that that even increases the
problem of recognition so use two or
three words it's also better if you say
next ok everybody else ends in an image
application but if you say next image
then the recognition radiation much
better so I talked about the dangerous
commands already be aware of pauses or
injections that's how we people talk
right so
if there's a voice command listening you
have to be aware that these commands
come in and if you in the recent s
decade as a command in a dictation mode
so the Commandant just listen for a
defined set of our vocabularies
dictation mode just goes for I think 30
seconds and then it stops but it tracks
every word you you say within this time
we talked about that already and so is
the comma is the system listening as you
can see here we see I'm sorry we see the
the lightning here everybody knows that
from from a webcam right so it gives the
use of the information that the camera
is currently listening to you kind of
you couldn't also in your interface you
should also tell the user when the user
is the voices is recorded yeah and if
you implement voice feedback that's also
a different modality of interview since
SDK so if you like you can talk to Syria
but everybody probably has seen this
make it intelligent and Chort so you
don't want to listen to howl like minute
feedback from the computer because then
you the user would turn it off basically
yeah and then that's basically the best
practices like we have seen most of you
probably know still know the logitech
camera on this little ball on top of
peace deep monitors right now every
device has a webcam in some years every
device will have chest your camps that's
probably sure so what are the people
doing with it and that's the currently
in the market entry face the biggest
problem so it's about you guys to
develop use cases to develop content for
that stuff that's why it responses the
food today right so um what our
technology new year does basically is we
have a that's a middleware that
basically uses all these what I talked
about different model
at ease for different devices and you
can enable your favorite your killer
applications because if you use picasa
and you have 12 thousands of pictures in
picasa in certain different albums you
don't want to use the SuperDuper new
image application which is optimized for
reasons so that's why we implemented
stuff that you can control Picasso or
whatever you want to do with real sense
technology or other technologies this is
a overview of some of the most important
applications we enabled it's all so it's
not even just desktop or Metro
application it's also browser
applications so web pages web services
actually and it's not about only the
applications you use today it's about
new applications optimized for gesture
and speeches at your control so this is
our our application for the reasons
contest it's the new year shapes order
again it's most of you probably know
that so you have this game where you
have to this shape and you need to put
it in the right target shape right which
is quite easy for people in our age but
it's not that easy for young kids so we
basically use this application I'm
trying to do a quick demo like right now
of this shapes order or for example use
here we use vector gesture recognition
then in this game you used a free hand
mode where you just place it and you
change the size and you change the
rotation etc then we use them as you can
see here here above a button then those
clock starts and and while you keep your
hand above this button the circle closes
and then you release this button so this
is just also to showcase different
options to trigger something and then on
the right side you can just you do not
need to enter your name by typing it you
can just Sade who's interested just
comes afterwards to me this is how it
works how we use how we used our
technology to basically add value to the
real sense
which already comes out so we use the
speech recognition and the hand tracking
of reasons and that's just us so if you
want to follow us that would be super
awesome also you guys in a video and of
course yeah if you have questions just
come to me afterwards and now I'm just
doing a quick demo and then we can have
food right yay so one important thing
also um this kind of lightning here
thence this is infrared based technology
this lighting or direct Sun might might
bring problems also like if we have the
screen recording right now running that
also might decrease the frame rate so I
don't know so if you can see here now
what I'm doing now I'm basically using
thank you I'm so I'm giving the user
feedback that basically he's controlling
this pointer basically and then if I'm
going like this I'm just starting this
new game just basically on hitting the
button I'm starting the game and then
basically it's um I'm going for this
what is it over this is called and then
I'm going like hang row yay and the line
we're slowing there it is see and what
you can see here is an early version of
our acceleration algorithm because what
we faced is the problem that you don't
want to you currently get absolute
coordinate so maybe you need to do this
and this to be to to move something from
one end to the other which is not a good
use case right because you need to do
stuff like that so we currently
implementing a kind of extra
acceleration algorithm that basically
does something like the mouse
that's it thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>