<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Webinar - League of Labels | Coder Coacher - Coaching Coders</title><meta content="Webinar - League of Labels - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Webinar - League of Labels</b></h2><h5 class="post__date">2015-10-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xh6Y6l6eTuE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">miracle project which this is all about
and I'll soon give you a short overview
on what miracle is about and what we try
to achieve with with your help and with
you next week in Brussels so if if you
have any questions throughout this
webinar please raise your hand you can
do that by clicking on on the yellow
hand button then we could we can give
you the right to speak through the
webinar so um let me give you a short
overview on the miracle project this is
a European Commission funded project
that currently runs and a miracle is an
acronym for a longer project name which
is called machine readable and
interoperable H classification labels in
Europe and because this is a very long
name we opted for the shorter league of
labels hackathon name because I think
that's much nicer if it comes to coding
and playing around with with a drop so
what what is this all about you might
ask if you have had a look at the
project website then it's already clear
that there's a problem out there um if
we talk about visual H labels and age
ratings were regarding media content
then you will find plenty of different
schemes systems icons and stuff like
that now so the current status coo of
age labels is general schemes media
specific schemes of 8 ratings and those
are mostly analog so we have visual H
labels and no no electronic
classification information at all that
was the starting point for the miracle
project and Merkel tries something that
is not so difficult on the technical
level but quite a few gold when it comes
to introducing this policy approach
in national rating schemes so what what
the miracle project rises to make the
specific scheme specific edge
classification information interoperable
on a technical level not so much on the
cultural level that's something for
later maybe but for now we try to bring
all those different visual age labels
together and try to make machines
understand what's all about in those
classification in those eight ratings
from different schemes the idea behind
that is if you have a globalized media
environment with a globally distributed
media content then specifically the
content distributed and the content
provider has the problem of dealing with
a magnitude full variety of different
national classification schemes making
it very difficult for those people to
actually cope with with the different
information that is included in those
ratings so amiracle tries to cope with
that by providing a common beta scheme
the miracle specification and M now can
met their age classification onto that
specification and then all those
classification is machine readable and
technologically interoperable so that's
what miracle tries to achieve and and
well the the main of objectives of this
project is we developed a common data
model for each classification
information we implemented it in
different classification context so we
already have a critical map
classification data and and we currently
implemented in different applications so
we try to show that as an added value in
interoperable classification why is that
relevant well I already mentioned that
especially globally or
operating industry players have to cope
with different schemes so an
interoperable standard is like makes it
easier for those industry players to to
aggregate that data and to play that
data out to attach their data to media
content and and so on so what we will
try to achieve during the hackathon is
that will will well as project partners
of the miracle project we already try to
implement the specification in different
contexts but we are quite narrow minded
so what we need are innovators and
developers who say well that's a lot of
data that you have at hand and and we
can aggregate it we can play with it we
can make new services out of that by by
combining them by referencing them with
other api's and stuff like that so numb
during the hackathon we will have some
industry representatives on site that
will try to give you an idea what the
most relevant issues for the industry
are for instance aggregating different
classification data in the back end of
their media asset management programs or
software so they they really need a
solution for that amiracle as a
specification might be an approach to do
so before we go a little bit deeper into
the miracle specification let me calm
you I would say so what miracle as a
project does is not rocket science at
all it's rather simple XML vacuum
vocabulary that the different
classification schemes can map onto so
if you have ever had to do with XML
schema definitions you know that there
are different elements of data so you
can just
find your draw that you need in an XML
data set and then map your data on that
specific data element so it's not it's
basically not very difficult to do so
and the miracle specification is quite
hierarch I i right quickly so it's easy
to comprehend and you can you can almost
read the data sets when looking at the
source code and add the XML files the
thing is that as I said miracle tries to
make it possible for for content
providers or 4-h classification bodies
to aggregate data to provide feeds with
different age classifications included
so that's why miracle also as a
specification allows the data provider
to include several different age
classifications in one and in one data
set that's why the miracle specification
is kind of nested so you have as all XML
data sets have to have you have the root
element which is label and within that
label there can be box and of H
declarations and so you can combine
different data sets within one miracle
data set and each of those edge
declarations has different blocks of
data that can provide more information
who is the issue of such an age
reciprocation what the scope is to what
kind of content or what specific type of
content in this can be attached the
actual age rating of course which is I
think the most relevant information and
of course there are several additional
data blocks main namely the content
descriptors or future descriptors where
we provide additional metadata on the
specific types or reasons why
cific type of content is age rated as 12
or 15 or something like that so to give
you an insight of how to vary the
current provided nerd code metadata I
can show you some demos current restful
api is that deliver miracle data sets on
request so let's let's have a look at
the different providers of miracle AAP
eyes and I'll show you some examples
from the BBFC which is the British
classification body for movies some of
my cam which is the counterpart and the
Netherlands also for games and I think
everybody of you knows Peggy which is
the pennant European game information
system con including many many gain
classifications and and then we have two
or three comparable comparable world
services that translate or map existing
data into the miracle specification for
instance from Germany DFS em where we
already have online labels for websites
so this is a service that can translate
those so-called HDE XML labels into a
miracle data sets so um I I'd say let's
have a look at some of those examples
for that I start my API requests
software which is on OSX em I don't know
if you know that but this is basically
just to show you how you can query the
different api's and then get the results
in form of miracle data sets
so as a first example the BBFC miracle
api is well as a as a first-hand
information you will find links to all
those API eyes on the be my app website
for this hackathon next week so it's all
documented um but just to give you an
example so I'm varying the AP the AP I
of the BBFC right here try to stick to
their URL parameters and query right now
for the game section with area 51 which
is the sci-fi shooter from some years
ago so and what we get back as a
response is a full-featured miracle data
set that I can use in in any software I
to develop our program of course we can
vary some some other game I don't know
grand theft autumn might be included in
the in the database so here again we get
back the data set of Grand Theft Auto
which actually has an has an 18 from the
BBFC so that's basically a simple
restful api that the BBFC provides I'll
they all also have a web front-end so if
you want to check out different pieces
of data beforehand then you will always
have the option to look into the website
front end so for instance this
is the documented web front end of the
VBS see when it comes to miracle miracle
explorer explorer where you will also
find the different URL parameters and
and options that you can M query here so
back to poor and our API query software
for instance regarding the Peggy API who
all who also offers um the database in
form of miracle datasets they are quite
interesting because you can actually
query for specific amount of time so for
instance if you if you want to know what
what classifications during the last
week or first week of October have been
issued then you can just query the from
and to date at this API so you will get
back a lot of mercure datasets then with
all the different information um
patients that have been published during
that week so in all those datasets
follow the same xml schema definition so
you will have no problems in machine
processing those those results from
those api's maybe in another example is
the fsm translation server which
translate the existing HDE xml files
currently used for labeling websites and
Germany into a miracle datasets and
again they provide an API that you can
query for instance i prepared the query
to
see whether the one of our sponsors
Electronic Arts actually labels their
website so if we query that API you will
find that you you will see that
Electronic Arts in Germany at least
Davis their websites regarding and each
and every game they provide depending on
the different age rating state they
received for those games and last but
not least when it comes to website
labeling we have a filter software
provider as a part of our project
consortium witches use pork and that
company basically provides a parental
control software and during the miracle
project they open their redacted URL
black list or H classification database
so you can also query that API with
different URLs and then get back the
actual the actual a trading done by use
pork for that page so for instance I
prepared to query playboy de to see what
the use of database has to say regarding
playboy and all germany's playboy
website and again as a result you will
get valid miracle data set and as you
see you spoke rated that website
so um that is basically how the api's
looked like so as you have seen this
that's not real rocket science hmm but
what i want to show you to give you some
inspiration for next week is where the
hooks can be found for external
references for combining different api's
or for attaching the age classification
data into something other to some other
information or a configuration so hooks
for such external references of course
are the content related metadata that
are provided together with the miracle
data sets for instance you will almost
always find information regarding the
the media content title the year of
production the EA encode or some other
unique identifiers so that you can Clary
other databases for instance the IMDb
database to attach that miracle
knowledge towards the IMDb result just
as just as an example and then the met
one of the main parts of the miracle
data sets is of course the actual age
rating so what you can do with that is
someone mentioned the idea of combining
face recognition software with miracle 8
ratings so for instance sitting in a
public library and the computer scans my
face and decides why are any you know
your age API of something like that M
identifies me as a 16 year old pupil
then I can actually use that information
to provide some specific age ratings or
to filter out specific websites or or
not
some some other ideas that we came up
with when it comes to what actually is
needed or what actually can be done with
interoperable data in this field would
be to provide something like API proxies
as a business solution for someone like
Netflix who queries that one stop shop
for all age classification information
regarding 11 specific movie or TV
episode so that could be in aggregating
service for those industry players we
have several interested companies that
try to map their current data towards
miracle and being non IT guys do in
those companies sometimes it would make
sense to provide them something like a
drag drag and drop mapping tool for
instance where you can pull your current
data schemes onto the miracle vocabulary
and then get basically the mapped
miracle data sets out of that tool or
the other way around if you have a lot
of miracle data sets you could also
provide or develop something like a
stylesheet generator for actually
visualizing the the information that is
included in the miracle data sets that's
something that browser plugins could use
as a framework when you want to show the
H classification of website or a movie
that you are currently watching on your
on your computer but also when it comes
to back-end solutions and if you try to
label your website or if you want to
provide your customers with website
labeling tools then content management
systems could use could make use of
miracle modules that you can actually
use to provide miracle based information
regarding your website or your customers
website something like that so those are
just some examples what you could do
with interoperable data in this field
but of course we expect you to be more
innovative and more playful regarding
this type of data so I think as a short
introduction of what we try to make out
of the hackathons and I'm hoping that
that you get the idea and grasp the
basic approach of the miracle project so
this would be my part of of this webinar
as long as it comes to showing slides or
API examples of course we're always
looking for questions of more specific
types if you have such questions and but
first I'm very happy that today someone
from the w3c is around at the webinar
and trying to give giving you a big
picture when it comes to test standards
and web labeling it's an old colleague
of mine and a member of the miracle
advisory board it's a lot sure and I'm
happy to introduce you to him now Phil
what do you like to say something oh
they'll give it very brief thank you
ever so I'm going to show you some very
just a few slides that I prepared to do
this so um hopefully you can now see
something on the screen which is a slide
deck that I've got that as URL on the
right hand side there you can click that
if you want to follow up you can even
follow along if you like these slides
are online so feel free um so yes I've
been around this space for some time and
w3c
it's been around for some time I should
just pause briefly if you're unfamiliar
with w3c we are the standards body for
the web my boss's boss's boss is tim
berners-lee we provide standards for you
know the make the web webs everything's
like HTML and XML and rdf and CSS and
all that stuff those are all defined
through our systems so let me show you
little thing um we actually a w3 have
quite a long history of working in the
space well there's not a very happy
history in fact the second standard we
ever created was in this space it was
called pics it was called the platform
for Internet content selection and it
was a way of providing what in today's
webinar were calling age labels how you
could provide age classification
information was designed in fact to
provide any master date and not just
about age classification but that's what
it became known for that was developed
as early as 1996 end up being
implemented in Internet Explorer three
and i think it's only just left with the
admission would with a release
knife-edge done I assume actually
checked edge i assume is not in edge but
it certainly was up as far as I 10 there
was some pics label reader and nobody
used it there was another thing that we
tried call p3p the platform for privacy
preferences and that was the way for you
for a content provider to provide
information about what the site would do
with information about you and would
allow users to declare what a sight
could do with their data would tell you
tell us I were you could use their name
or their address or whatever it may be
and it was an exchange between the
content owner and the user that was
built into ie6 and other browsers and
when nowhere ended up not be used and
then I came along and in the early in
the roughly ten years ago started
working on a replacement for pics which
is called powder which is all about
providing metadata about websites that
tried to fit
some of the problems that pics had to
try to make it easier to use and again
no one used it so we have a long history
of creating standards at w3 around this
idea of providing metadata that nobody
uses from a policy point of view some of
this stuff again has been around a long
time before the web there was the
entertainment software board
Entertainment Software ratings board yes
i'll be in the USA and some of our sack
the recreation or software Dreiser be
cast these two were both set up in the
states in the 80s and then the web came
along and one of them are sack moved on
to the web and that became the rotating
system that was built into internet
explorer and other browsers that became
a sec I that then transformed into Hydra
and microbrews around from around to
2002 about 2008 I was involved
throughout a large part of my truth
history I've talked about why it failed
before now there are some slides that I
did an earlier karam and Stefan and I
were together back in March 2010 as a
link to them there and I wrote at the
document about an hour after I could
finally packed up about why if held why
did I could fail we had this big system
with lots of companies all wanting to
support if you have people from wanting
to access information but anchor did not
succeed at all and a couple of questions
that one has to ask about providing age
labels what is it we're actually trying
to do if you're building an application
that uses age labels if you're building
an application as energy building a
filter what are you going to do about
content that is not labeled and what's
the incentive what is the incentive for
anyone to bother to provide that
metadata so the history of sort of
things that haven't quite worked miracle
comes along and tries to change the game
and I'm pleased that it has done so and
step down on the colleagues and in that
project you've been working hard to try
and do things their different to learn
from those previous failures and here's
an example are they very very very
successful
later providing thing if you haven't had
a skimmer dog check it out it's the
mechanism that is promoted by all the
major search engines so Google and
Yandex and Bing and Yahoo they all
support schema.org which is a way of
encouraging you to provide metadata the
girls alongside websites and it actually
helps you to expand the amount of real
estate you get on the search engine
pages I gave you an example here of a
thing about and have a look at if you
look at the source code of the results
of those pages you'll see that the ones
that take up more space on the search
results are ones that have a lot of the
skimmer that all metadata embed it so
I'm going to check that out and that
isn't there's nothing to do with age
ratings there's nothing to do with
online safety it's all about providing
good quality metadata for search engines
if you lie if you put in accurate method
edgerrin then that actually reduces your
search engine rank a few tell the truth
then that will enable the search engines
to make better use of the data we've got
that's the ideal it doesn't actually
increase your search engine ranking it
does increase the amount of space you
get on the set of change in pages so
that skimmer org is very very successful
roughly a quarter of webpages house
given that will mark up in them a
quarter that's a heck of a lot of web
pages it's very very successful and it's
not very old so how else might the
miracle work be used I'll leave you with
one slide this is what i get if i use a
modern browser and do a search for a
current movie in this case the Martian
you'll see the i live in a place called
ipswich which is why when I search the
browser obviously the web browser knows
where I am it knows I'm in ipswich it
knows therefore where my local cinemas
are and it tells me all about the film
it gives me reviews it gives me pointers
IMDb and it gives me showing times at my
local cinemas so I know that today I
could have gone to see the Martian at
1420 another showing has just started at
1730 so a lot of information is there
the google has
and Google get some information from all
over the place in various places where
that information comes from but you'll
notice that no one on the page as they
give you the age rating wouldn't it be
nice if it did and so what I'm hoping is
that the the work the miracle is doing
is going to make it easy for
applications like the search engines
like others like the applications that i
hope will come out of the hackathon next
week use their information properly
alongside other information to make
something interesting so there is hope
there is positive things look forward to
this data that comes from the rating
agencies is extensive as a lot of lit
it's usually very good quality data as
well and you can hook it in and make
some pretty cool apps about films and
games and you can mix it in with all
sorts of ideas and if you do that and
within that include the age pacification
that's going to be useful to people but
that's where I think the positive future
lies so with some background would not
go well I hope that we can look for to
some positive outcomes and again much
lighter there if you want to follow up
any of those wings okay that's me back
to you just ever so thank you Stefan I
think other work
iconia 7 ah i'm back online thankful for
the overview which makes it very clear
that we live in times where metadata
becomes even more relevant than then it
did before because now we have the
machines and apps that can use that data
in a meaningful in a meaningful way so
as an overview of what we try to achieve
I think this sums that quite up quite
good but I wanted to give you also a
kind of family feeling for next week so
um what we gonna do is we will have a
comp well a rather small hackathon
compared to the hundreds or thousands of
participants hackathons at you that you
find sometimes but we will be around 20
30 people that make use of the miracle
data sets so it will be cozy and we will
have plenty of time to discuss ideas to
help each other where possible or
necessary and there will be a lot of
people from the project around with IT
backgrounds with engineering or even
graphical interface backgrounds so there
will be a lot of support and help in
case you run into some issues or
problems and we will have around even
some members of the miracle consortium
that actually have access to the API
source so if you run into a block or you
need something in a more effective way
for instance then it's even possible to
ask the API provide us to change the
source code accordingly m and then um
well work with the data in a more
effective manner so I think that's me
that makes it quite a specific Agathon
and it makes it very productive from
your point of view because we are all
around so all the other ones that you
need to ask
shins or to ask for them changes in the
source code are around so I think that
makes it very interesting for for the
hackers and developers and innovators so
um that that would be it basically um
and I haven't seen any raised hands
right now or any questions so this would
be the time where where you get the word
and ask your questions um if you have
any
you
you
um the exxon i believe is notion of the
weekend is it done Stefan I'm looking at
the questions here yeah so I see so
somebody asked when the Exxon takes
place it will take place next week on
monday and tuesday so it's not over the
weekend but it's during weekdays
starting Monday morning and ending on
Tuesday late afternoon
so anyone with any questions we can we
can put your Mick online so you can ask
your question the right within the
webinar so we grant you the permission
to speak and then you are free to do so
any questions or problems
so some additional questions role in one
is from Anastasia she she asks if if
there's any social media related
educating feature on youtube because as
many of you know there's a safe search
save search HGH thing to be enabled in
on youtube so as far as i know youtube
users rating or flagging features for
comments too but they won't result in 2h
getting's or h classification but just
if they if they are infringing the terms
of service then they will just be
deleted so there's no h getting
implemented on the comment section of
youtube but of course this is something
that that can be achieved on because
there is classification knowledge at
hand so if you let your community rate
or flag content or well media content or
the content of the comments on the
social media side of YouTube then you
have classification knowledge on that
content you know that a specific amount
of people find that comment offensive so
that's actually classification knowledge
that you can provide for internal
reasons or also for external external
demand us so that's them something that
can be done I hope that answers your
question and then there's Carlo Carlo
asking if we are sure it's also for
non-programmers who well it's called a
hackathon but of course if you have a
great idea and you'd like to innovate
and you can you can build your team on
site so at the beginning of the
hackathon there will be a matchmaking
session where you can actually present
your idea that you have it doesn't
matter if you are are an IT person or
not it's not
as you as you have a good idea and and
you want to build a team that is taking
the the challenge and then you're free
to come so there will be there will be a
team-building session and what if you
convince others with your ideas and then
you will have a team at hand consisting
of developers and graphic designers
stuff like that so yeah of course we
think it's also for non-programmers we
we like ideas we we like programmers to
but but we like ideas too so I hope that
answers your question
so again several questions sonya rd asks
if we provide any documentation on the
api's yes we do please have a look at
the League of labels hackathon page and
well where you will find all the
documentation for the for the AP is so
if you go to the League of labels
website and scroll down then you will
have linked all the all the different
API URLs and documentation so of course
you can already practice with coding och
varying those ap is if you run into
problems then please contact me so it's
not a problem at all to support you when
it comes to finding your ideas i'll post
the URL fall of you and in the chat
so there were some additional questions
sunny day you're asking about schema.org
the I think you'll find if we look that
there I'll put on the chat window as
well let's get skimmer to August
self-describing but the that there are
lots of links on that you see the whole
whole thing so skim it all yourself is
the place to go for information about
that you'll find plenty there
any more animal I don't know how to make
that will work ok so you pay at the top
okay okay any anyone else with questions
or will we see um will we see each other
next week Carlos already says he he's
got an idea so I kind of kind of could
you could you present your idea is this
still mystery secret stuff Carlo are you
still there I think you can hear you
typing yes a lot yes hi we here I can
hear me okay yeah no I do have an idea
and I'll bring it with me on monday i
can give in short but it's more about a
music platform is that ok with you do
you're going into movie content well
music is currently not moderated by many
many classification bodies so there will
be very very limited amounts of miracle
datasets when it comes to music and I
got a tree custom oh sorry Rick videos
so music videos could be an idea okay i
think the ID could be applicable to
movies as well it's about more to work
with trusted brands
such as magazines or radio stations or
TV stations that have a certain taste it
could be in taste in movies like horror
movies or comical movies or something
like that to pick T an AP API to their
sites to see what is a relevant content
to them and to insert that content into
a certain box or like the virtual
digital box that you can where people
can see okay on one platform you can see
from certain magazine which is their
choice of movies at that moment or an
awful time yes it's a bit difficult to
explain now but i'll send you a document
where is it explained in terms of music
but you could apply it to films as well
okay it's like emotion-based targeting
or something no it's more it's more
curated discovery where you can as a
user you can say okay I like horror
movies so you can get access to the
selection of a trusted brand like a
magazine or a TV station or maybe a mute
a film critic that has a very distinct
taste in horror movies so you can see
the selection of that person with a lot
of credibility in depth taste actually
okay it's you okay that sounds
interesting um I'll send you in a very
brief description by mail this week
later this week in English can I love me
yeah that would be awesome but but you
don't have to do that so you can just
bring your idea to russell's next Monday
but of course you are calling invited so
if anyone else wants to talk over
microphone then please feel free to do
so so you don't have to type such a lot
of text
so Anastasia still sticks to the social
media and if I understand you correctly
Anastasia then you think about using the
Facebook API to so get to know the the
actual age of individuals on facebook
would you like to to talk more about
that oh um I think about like there's a
Facebook usually some people tends to
put the edge there but maybe if there is
some child or like the person who
actually like under 14 13 because a lot
of children have already smartphones but
is it possible to tag some Facebook and
use or like what your share their just
for the people for certain group maybe
well also also Facebook does H gate
content within the social media platform
if you remember the Mexican execution
videos in spring so they actually put a
black layer on that video so you have to
click twice on the video to actually see
that very violent content so again
facebook has information regarding the
eh classification or the content
descriptors of specific videos or
content in general so they they have the
infrastructure to do so but currently
they only use it internally for such
blacking out videos or double clicking
on videos before you can actually see
them so I'm not sure we can have a look
at the different Facebook api's before
monday and can give you more information
well where you might be able to hook in
when it comes to educating content there
Thanks I think it's just like about age
rating when you start talking about face
recognition and somebody who was in
library we just stick to my line and I
started thinking about like different
application yeah I could see ya well the
first I don't know if the person will
attend next week but um I had a nice
phone call from from him telling me that
it would be nice if he can just approach
the public library computer and then um
well if he is 16 or 18 he can view
everything on YouTube but if a
six-year-old child gets onto the
computer then YouTube is actually
automatically in the safe mode for
instance so you can well combine the
face recognition or the age recognition
together with each classification on
such such platforms ok I just heard
about like face recognition connected to
Coursera do you know that like where you
can learn some courses so it's just
they're trying to understand have you
got the content have you got the course
or not like but I haven't thought about
age recognition from that ok interesting
that I think there are two or three face
recognition ap is that can calculate
your actual age or the your estimated
age and we linked one or two of them
also on the League of labels hackathon
website I think I have seen some and I
have tried my photos in one case it gave
me like 20 years and another is just 30
so sometimes I did not working very well
yeah well the Microsoft based API is
better in that and we will have some
Microsoft IT guy around in Brussels too
so he will explain us the possible
features when it comes to face API for
the Microsoft face recognition api's
okay great thank you
so Philippus them has just joined the
webinar and asking whether there will be
a recording of this and my my panel
shows me that this session is being
recorded so I bet there will be some
kind of documentation of this webinar so
Philip you're not alone you can just
watch it later on and then come next
week to Russell's in the best prepared
way
okay any anyone else one wanting to talk
or to ask a question or present an idea
already
if not thank you very much yeah so we
call it quits for now and as I said
let's have a great age classification
data party next week in Brussels so I'm
looking forward meeting you all seeing
you all and I like to be flashed by your
great ideas we heard first ones today so
I'm looking forward to even more next
week see you on Monday in Brussels okay
thank you very much thank you have a
good time Brussels epic as well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>