<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UX guidelines and best practices to develop apps with the Intel® RealSense™ Technology | Coder Coacher - Coaching Coders</title><meta content="UX guidelines and best practices to develop apps with the Intel® RealSense™ Technology - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/BeMyApp/">BeMyApp</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>UX guidelines and best practices to develop apps with the Intel® RealSense™ Technology</b></h2><h5 class="post__date">2015-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JPvI-5AdaBg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello hi everyone and welcome to this
webinar today is Amy Allan is going to
speak to us about user experience
guidelines and best practices to develop
compelling applications with the intel
realsense on the right hand side of your
screen you will see a chat box do feel
free to use it ask any questions you're
asking yourself and then you will make a
few stops during his presentation to
answer them exactly i will let you now
introduce the two questions we have and
you can stop the double bina sure hi one
right now you'll see a first question
get on 20 seconds to answer it's for me
to literature the content of the
following presentation okay so first
question is do you own an entire sense
camera so you have a few questions to
answer it and then it will disappear
from your screen okay host everyone in
third thanks it seems south of you do
own camera already and you of us once
you are interested and I'm quite big
number of people don't know much about
three essence yet so so I'll have to go
through the basics again here the second
question and we'll get started so second
question are you familiar with augmented
reality virtual reality or 3d design
okay I almost ever voted thanks okay so
let's get started so welcome to this
webinar on designing better user
experiences for intelligence
applications so we talked mainly about
the f200 camera so some of you weren't
exactly up to speed with real sense from
what I saw from the pole so the f200
camera is very awesome camera is
designed to be integrated into laptops
all in ones and also some 21 so it's
mainly to be present at the top of the
screen like a usual webcam and four
screens that are at least 13 inches or
more so usually more on 14 15 and 17 are
with okay the fallen ones 24 inches 23
inches so that's really different that
and for tablets for example so we'll go
for wrong examples you need when you
develop a recent application you're
ready to need to understand the
strengths of other modalities you can
use for reasons why you should use it in
which case you should use it or are not
because it's not exactly meant to fully
replace keyboard or mouse or touch of
course and now you should minimize user
fatigue and we talked also about the
precision and the quality of data and
what you should do with it and also
about visual interfaces so we have time
for questions in the end but during all
the webinar you can shut a question in
the chat box and I'll try to answer it
if I see an opportunity to do it so I
reveal the question to everyone and then
sorry after so with real sense and never
have to run the camera are you have many
capabilities or aided by bhk so you have
speech hands and face tracking these are
the main ones so all these modalities of
course we have strength and that some
trouble
x 2 so 4 speech of course it's
compelling when you use things like Siri
or Google now can really cool when
you're in front of a computer sometimes
from not alone sometimes you in the
noisy place and so you need to take an
account for all the environmental noise
and all the things that are happening on
the computer so when you design for
speech a good thing to do is not to
listen to all I mean to be in free
dictation road and try to understand
sentences the best you can do is for
example to use grammar so this provided
by the SDK and when you use a grammar
you can make specific words or sentences
that the user are say so n Kumar is a
lot more reliable and Plus on top of
this if you want to have a very
compelling experience on speech you can
also couple this with face tracking so
for example you can in some at the user
only when is actually facing the screen
and moving is not bad it's even better
because so so this would be improve that
speech in direction with computers for
hands and is of course one of the
biggest usages with recent f200 camera
and to design for hands tracking you
really need to understand the field of
view of the camera and some common
issues with ants gestures such as
occlusion and over I'll talk more about
it later on the slides but yeah answers
for already rich and engaging
interactions and they're really cool
when you can really have a thing of
direct manipulation but you should try
to design it so design the interaction
so you the user don't need to make
really large moves and really high
because can be tiring and when the hands
also goes out of a field of view
but that's quite complicated so it can
be problematic for the user and then you
have a Facebook ad from bhk so you can
sense the shore expressions emotions and
of course with a free position of head
so precision of 3d tracking is very good
it's working well when you want to try
to measure our expressions such as
openness of mouth smiles and overall
emotions such as angular nodes which are
all provided like SDK you need to take
an account that wears a large
variability of these expressions across
a GSU juris personalities so when you
design for for face emotions and
expressions you should really have a
version phase so it's which is properly
adjusts what you're tracking to the
users of course you have already
existing ways to interact with computers
or such as touch keyboard mouse and all
these are not going to disappear right
now and be embraced by a sense so it's
better to really design your
applications to take advantage of all
the way its user can interact with your
computer so search is really good for to
the interactions with direct feedback so
you get an actual tactile feedback which
you don't get when you wave your hands
in mid hair so sometimes touch is best
and keyboards allows for quick entries
of course the text is quite useful but
also for comments that's if you go to
shortcuts and always valid ways
intuitive and then mouse is perfect for
to the interactions that's quite obvious
Mars is not always there so on on that
top sometimes you get a touch pad
instead so
Co concealer sometimes it's n striking
from reason scan it can be a big
improvement of our mouse but sometimes
you will need to redesign your
application so so it's more suitable for
real sense interaction we have more
examples later on the slide so when this
when you want to use real sense you need
of some sort of check is so first why
will you use real sense so in which way
is it's improving the interaction than
just with that are screen or mouth but
are common these days so whether it's
better because you have free interaction
or equal it's more in it can be more
intuitive our user can do is invoke lazy
position for example for DVD play for
video player or purple tie school
sometimes Jonah not just next to your
keyboard so you may want to do real
sense just yours too so if the user can
move the slides from our way of this
computer so if you want to integrate and
striking how can you do it so it's not
not a pain I mean it should work well to
be reliable and the user should expect
it to work and for this after me the
report directly trying to use the world
data from from v HK zovi he ends the
position of the hands with everything
tracked with all the gestures and of
course its data from a camera and is
working in environment are not always
perfect so when you deal with this kind
of data you absolutely have to smooth it
so you even SDKs providing you some
filters from the PhDs movers so it's
part of the documentation just tap so we
have a smooth or utility and accessible
from T axis mover and it's providing
values way of smoothing data so
turning interactions with hence racking
you absolutely have to smooth one hour
we are using our own smooth furnace or
your own movers are the one from unit
here whatsoever but it's just mandatory
you can't expect a really good
interaction if you don't do any spoofing
then the user also need to have a direct
feedback so we when your hands are in
air it's not like a touch screen so you
don't have a tactile feedback so
sometimes it's going to complicate it to
be presiding to really know what's
happening plus you have the issue of the
field of view that is not as well as
some user would want it to be and
someone is reporting is not able to hear
anything can someone just shut the
question saying is you can hear okay
thanks thanks so I got some someone at
initially Brazil nut up so sorry for him
so when you design real sense you need
to be then for ease of use so it
shouldn't be complicated for user to to
use the real sense but of the
application if it is maybe the
application you're working on is not
very suitable for reasons that happens
not all apps are meant to be used with
museums
you need to understand the capacity of
Technology so for example I've seen many
projects of people trying to win
commence sign language recognition sign
language is something we're quite
complicated so unless you're just a
machine learning guru I would say just
don't try it plus one sign language so
after that is something may be easier to
tourists and certainly possible for
camera but for alphabet it's better to
you the keyboard and if you want to go
to full sign language recognition then
you also need to get social expressions
and all the sign signs are usually drawn
in a wide space so going far away from
the field of view of the camera so
that's quite a limitation and and that
makes it complicated for this kind of
recognition so really when you further
want reason application and interaction
are includes an idea if you very have a
look at our camera walks and to see if
the gestures 12 implements and the
interaction you want to implement is
possible considered considering the
field of view of the camera the
occlusion so I mention i'll talk more
about occlusion week later and the
quality of the data when designing
interaction with ends and if your
application is going to be used on a
long period of time designing for for
wrecks or the user shouldn't have to be
like an hour with arms in here and with
hands in the air that can be very tiring
and also it you can if you want long
interactions you can also focus
interactions happening you have a bottom
of a field of view so the user don't
have to lift these are
ends too high so we can adjust for
that's pictures on the right top right
on the side for example so left and
rights our gestures are really easy to
do even for really lazy people i lapped
our also if you can design your
interactions for this kind of gestures I
think that's the best also in all the
case it in many cases sorry it's really
better to design for relay relay to
interactions instead of episode in
directions so for example I mean when
its relative you can have quite small
moves that would be affected by be in
moves on your applications on the screen
and that's a lot better because with
absolute motions you may face the issue
of a user going out of love you many
times and also in the face more issue of
occlusion so when the hands gets in just
in front in between the user and the
screen so he doesn't really see the
screen anymore so in many cases will
will eh it is way better plus if you
design for really interactions for
example you you can decide for a
specific state of the hands of our
example when it's a fist you start the
interaction and when it's fully open you
stop interaction and that's quite easy
to develop foreign that doesn't require
where that's working reliably living
with that noisy data anger all don't
expect a very precise input from the
user so again smooth it use 50 G's move
for utd see if you can you should always
move it and in the way you design your
applications just don't need any proof
small very small targets or which with
the hands or other things like that
keyboard with hand gestures is one of
the worst example you should we not do
that when user come on you to move and
striking you need to remember that the
speed of movements you're expecting may
be problematic depending on from data
you're using so especially for games
where you expect people to weigh whether
hands very fast very quickly then it's
not Oh similarly if you do really the
full hands fracking with the 22 Giants
and all the gestures actually the it is
documented so I'll people until then
it's really low speed so it's yeah less
than 1 meter per second and this is
problematic for games if you lower
resolution so cut the field of view in
half you can reach 2 meters per seconds
but that's not very suitable to all
let's get the field of view so the best
you can do especially for games is
instead of we're playing on full hand
striking with the 22 lines it's subtract
the hands in extremities mode or even
better in blood mode so the blood mode
is one of the most reliable in 5th
myself of realsense sdk plus it's not
consuming much CPU and it's just
reliable so let me direct a pop of the
example from vhd k so it's the blood
module a viewer so
you can see I can track hands I can
track points i can really quickly it's a
frame rates up will be now allows you to
actually see it you can see the closest
point it's all I strike reliably so if
you only need the closest point of the
guy pointing at the screen just use
blood module it's a lot more reliable
than tracking full hands if you don't
need full ends just really go with the
blob module plus the latency is just
quite perfect actually so you can use
consume the right side of this simple or
you can access blob data and what are
the parameters so you can track up to
four blurbs you have built in smoothing
and you can track like infirmities mode
for vans the closest point in right here
the center point of the shape that's
tracked and left right up and at'em
points so let's write and top and bottom
points can move quite much on the blobs
but they can be useful for example to
determine if the user hand is open or if
it's a fist for example depending on the
space between them so you should tell
out with modules who turn to design your
interaction with games or applications
in general especially you should just
need one point from user then blob is
really the way to Google+ with blob it
is just not tracking handsy striking
objects that are close so you can really
track anything so stick or whatsoever so
let's hood Ella for more precise
interaction from the user so is the
question that's right up to the point so
can you still get the finger fond
earnest data in blue module is that's
only track available in foods and track
exam so the blood model is actually
tracking objects blobs things and not
hands so you don't get any hands p seed
value like openness
if you want to design the openness of
the hands you can still track at a
distance between the topmost and the
bottom most points but you need to also
pay attention to the fact that sometimes
elbow can come into play so even better
is the distance from top and bottom
right over the actual center point this
will be more reliable then you can
determine in the end these fully clothed
or fully open even if you're actually
talking an object
so back on the field of view issue
there's a artistic view of the field of
view of a camera in of the users
interact with it so if the user is
reaching this keyboard the ends going
out of you so it seems more table and
everything that is right down and when
you design interactions field of you is
not that wide so it's quite common to to
have the user going out especially if
you have absolute tracking because the
user will always want to reach others
point with the positivity others to
reach and the thing is when the end
tracking is lusts is something it can
take seconds to get back because we're
the calibration phase if you're using
block tracking that's less of an issue
also cameras are integrated in laptops
and only ones so the placement is not
exactly the same and general let's find
but you should still tests interacting
with your applications from different
angles that's especially important for
four examples of each other head if
you're doing that tracking in games when
you stress on the laptop the pitch will
be always facing a little bit down and
for only ones that's more straight so
I've mentioned occasion like four or
five times three and it's still true so
here's a picture of defining what
exactly is the issue in occlusion so
when you design and striking and your
gait and you i expect the user to
interrupt you something on the screen
pay attention that you are not asking
the user to actually places hence be
exactly between work looks and the
screen because it will be able to see it
actually and that's why it's always
better to design for relative motions so
you start
for somewhere and it will will really
relate to this position or else you you
can have absolute tracking but starting
with zero that's not at exactly at the
center field on zero whatsoever but more
on the right bottom side but I would
really advise to go for relate emotions
if you can because it has many many
other advantages hey or generally since
you don't have way the feedback you have
tactile surface or anything like that
it's really important from aids
feedbacks to the user so from the the
screen it's really important to design
the US / user now that is actually
interacting with the application and to
know what is actually doing the
application so it's actually scaling an
object you should never all like when
you scale a window with the mouse on
windows and you should also have good
feedbacks for warnings from the system
so the rails and Suzuki is already
providing warnings for example if you
doing head tracking you'll get warnings
if heads is going out on field of view
showing against tracking you also get
one is if they go out feel the view they
are too far and so on so we use these
alerts so there is our call a lot since
I baaack we use these others a lot to
give a feedback to the user and to ask
for user to go back inside fill of
Google or 12 outgoing not so good thing
you can do is to just have another line
background of your application showing
the user so if you really wants precise
free interactions so we use or exactly
know what the application is seen
or you can put a small view button at
the bottom light so in this picture you
can see my full user with tracking but
it's not necessarily you don't
necessarily need to put the old user in
there you sure drink blood tracking for
example fighting the blood is enough so
the user know nose but what's actually
track baby is DK and that's enough a a
good feedback forum user so you can put
some distant problems you can air grams
to explain to the user where it should
be when you first try your application
same fraction so I've mentioned the
value of if you're scaling we know you
should choose gaining egg icon when the
user is actually performing the action
and it's same for pricing or authoring
any objects so the user should know well
you know instantaneously what he's doing
with his hands so all the application
see what he's doing so it allows the
user to really easily adjust is
interaction and to avoid any very big
troubles so I know what sort of work to
implement all feed bags and into
applications that can change the
interaction a lot and you can't expect
the user to follow like 10 minutes
tutorial all to interact with rehab and
a big training days that's not possible
but you should just add some good
digital feedbacks the user will leather
pretty quickly so again I'll in striking
if you want to implement a full free
interaction with objects consider the
side basement on the screen on the
screen over the occlusion you should
prevent the users left will reach the
edges of the screen because then it will
go out of a field of view and
again you should smooth your data so to
avoid failures of tracking so just read
mention ends because I think that's the
most complicated way of talking so we're
getting data from real sense but that's
also one of the most compelling ones
here are some general guidelines so it's
better when your first Nnanji
application also when the user first
launch application to know to teach in
what you can do so all you can interact
with the system so it can be from just
diagrams and quick words but since there
is no direct feed bags and moving hands
in air is not always something coming
across people using applications you
should tell the user about it then try
with real users so if people worked a
lot with real sense SDK we tend to make
the perfect gestures that are recognized
by PSD k lets users are not all eat they
don't all perfectly know how to do some
citric gestures so you may want to do a
little training face steel and test your
app with real users so other colleagues
or anyone you can find so there's a
checklist so when you implement a recent
interaction is the action to be more fun
better with other modalities of
interaction like touch on our mouse it's
so menu our gestures from user precise
and critical so if you want to do
something like remote surgery
maybe you should reconsider because you
will always have that I've it is not
always a hundred percent accurate but
that's the thing where we are you're
interacting with physical camera light
and it's not under present precise you
can't expect it's plus wedding is and
cindy air
you
cognition only when the user is actually
facing the screen I think that's kind of
good opportunity to use more than one
modality and really prove the results
because of this eve interaction has been
implemented already by someone as you
know more games try it and see if it's
working first I think it's its best and
see why what are the issues with it and
now you could improve it so in the end
before raising so check that for the
users it's easy to get on board the new
features will be implemented and if
there is feedbacks on especially on
field of view issues so which are
getting the added from bhk and we taking
this to the user so if your UI has been
properly designed for interactions that
are not hundred percent precise and if
you don't need to use user to have the
ends in via all the time unless it's a
short game okay that that's perfectly
fine and test we are users first now not
only with did but also worked with a
basic a lot so this was a demo of movies
game on the webinars bit complicated to
show video but this one is using the
blood module so it does for really quick
interactions the user has been placed at
the top left of the screen here so you
can see whether the game is seen so it
does for any quick interaction because
blood talking is very quick Emma and
Neal code with so this is a vm first
slide part here is a link i will copy
paste it into the chat box i guess these
are links to PDF armed with all the
guidelines where we have four real sense
user experience wind for people selling
directly in the chat box it's something
you should really go through if you want
to design apps by using the essence a
preferred good to have really you set of
guidelines and now i'm going to look
through the questions you adam haven't
answered yet and the meantime you can
shop more questions we have 20 minutes
left so that's plenty of time for
questions so one of the question was
what kind of processor is used by weird
sense so inside the camera module the f
201 is an ASIC that is doing that is
generating the depth information from
the cameras that are involved and then
all the end striking face tracking /
recognitions are doing by the sdk on
laptops and all in ones and these
computers are milling core cpus from a
priest's hazard generation the fourth
generation of course so computers from
one year ago to and subsequent ones the
question was what's the speed of
capturing the camera so the camera the
depth stream is being coded at 60fps and
the RGB strain is be recorded at 30fps
so for modalities that are using both
RGB and that it will be of course
limited to 30 fps
question was who is been a up and why do
you be this seminar so be mad people
here and maybe can answer but I can do
it quickly to sobe mayhap is attached an
urgency Intel is working with and I am
personally working for Intel and they're
called in a cheater vert eyes the
webinar and did all the logistics and
the organizing regularly events interest
participating in so meetups and over
thank you for the introduction there
you're welcome so someone had an issue
with built-in gestures from the SDK for
example add tabs so yeah taps has been
introduced not so long ago and like men
gesture sometimes it's hard to make it
recognized to really have a better
recognition of melting gestures from the
SDK and I would advise to limit the
number of gestures want to recognize and
tell vs DK about it so envious DK you
can enable gestures one by one or you
can enable all of them so don't don't
enable all of them but instead we've
unless only a bunch of this you'll get a
much better recognition and then some
just first still needs some learning
curve for some learning from the users
to do it well I think that is one of
these and most insurers are also
designed to be down in front of the
cameras and freddie from sides and so on
so if you're expecting the user to the
tabs in need directions it may be
complicated so you may have to think
again when we were expected to do taps
out maybe we consider the
are white the movements the user is
doing should be so again we're back on
the advice on doing more related
emotions rather than absolute motions
someone is asking for having adware
samples so to buy the camera Eric
everything is available on 1-click which
is our shop let me just find you the ink
so here it is it's in stock so on some
kindling telecom and you can just look
for f200 actually on google and you will
find it so another question is how is
this different from connector leap
motion so forceful NX is a big module
quite expensive you can attach the
computers so it's meant for more
long-range interactions if you take kind
of version 2 it's also working for close
interactions but again it's really heavy
and it's raining a lot of power so the
big difference with Kinect is girls only
close interactions and it's starting to
be integrated into computers so we've
low power and that's different kind of
interactions and we will align with
Microsoft's we should take for examples
and exertion of Windows Windows 10 on
medicines compatible pcs you get
featured called hello and that all those
for I'm already quick and check your
facial login face login to windows as
compatible with all the reasons memorize
it's done by Microsoft
and front portion so leap motion is
working at different ways so verra is
facing up and they are doing recognition
from to RGB and ir cameras very low well
so it's a bit different technology
sometimes it's also suitable for fossil
projects however I know about
integration of leap motion in only one
computer I don't know of any other so
it's still quite an external hardware
modules so quite many questions and any
plans of using the GPU in later versions
of ESD k so some modules are already
using the GPU through opencl I don't
have the exact list in mind right now
but yeah we have big plans to always
optimize the SDK and some parties are
going for opencl and GPU usage so not
all models are using it right now but
for those who aren't are still perform
train that's seen the ads in the plant
no question
what happen with four swipe gestures
from envy all has DK so last year is the
guy with all the camera because they
work very well all right now if we
introduced this for life all the swipe
gestures like swipe left swipe right
swipe up and swipe down I'm not so sure
about swipe up and down actually yeah
they are not working that well with the
conversion of VT k I mean we're walk-in
well once you know how to do is nestled
in my view 10 that for so over
Sweden fortunate that yes wives already
put gated to integrate if you want to
really differentiate between swipe left
and swipe right because from the point
of view of the camera the user goes out
of the feel of you or or just to one
swipe left and one swipe right it's
really hard to rain know what the user
is doing if it's the swipe right and
swipe left or if it was just a swipe and
depending on where which is four starts
and ends it's it's complicated to get it
right so for some space features you can
if you have for example for a pipe and
you or if you just want with a simple
swipe and you don't want to travel hands
further anything else you can repair man
to swipe yourself like the hands goes
from the lights to the left side and
disappear and that's that's wrong if you
want a an interaction where we've but
our but is very specifics on leaf swipes
then you can use both modules but for
the fugees recognitions yeah it's
especially complicated to make it work
because people may want to do swipes
really quickly but when you do the full
hands fracking it's
me too less than 180 per seconds and it
needs all loaded liberated so that's
quite sad actually but I would recommend
not to to to use the swipe gestures for
from the SDK right now the location is
about megapixels of the device so the
depths frame is vga and the RGB stream
is full HD lower question is about leg
supports and so on I'm nynex I've heard
about a driver led by an independent
guys or totally independent from Intel
and it works so you can go ahead and try
it for your hundred however we don't
provide a driver ourselves and the SDK
is not compatible with linux so we know
it's interesting to some people but
unfortunately we have over priorities
right now someone is would be interested
in getting access to hello now I don't
know how you can access to it quickly
yet maybe in the latest build of windows
10 and yep you'll have to check because
maybe the new GM will be treated for a
Windows driver so you can trigger
windows a data to check that as you
should wait front of our eyes of the DCM
which I don't know when it will happen
call me react i just--can say so
question is on when I'm operating from
may be high or of your innocence teams
Wilton's teams are best a bit all over
the world many are in Israel many are in
the u.s. some overalls are in India and
China myself you can get it from accents
I'm based in France on the compatibility
and is it only usable in Windows 8 so
yes we SDK is only usable on windows 8
dots 1880 2010 usually is we've got so
many is it possible to integrate tools
and cameras and get better fracking so
the app 200 is meant to be integrated as
the one and only camera in the system so
you should undo it however it may work
in the future I mean something we are
looking at but not to be integrated for
more specific projects if you want to
use powerful cameras it may be supported
when there are lower you can fly with
current versions of EDM maybe some
things are working some of us aren't and
general you should know there may be
some cross talks between camera and but
you can configure is too many different
patterns and so on our independence of X
and acrylic range of laptops and tablets
with new generations cameras especially
low and weaker signals to follow in
medical segments rice one issue is a
cost of camera module and chip you need
to operate it so for cameras integrating
into laptops and small devices will have
the art 100 and that's I don't
and we will try to reach the lower cost
again but the decision is up to OEMs and
if they want to pay the price for
virtual also in low and weaker segments
and that's not not sure about that I'm
not sure about that so there's a
question about a challenge if there will
be a new research chair and Fisher I
honestly not know I'm not organizing
with challenges and the lower commons is
at hundreds of your fav effect badly
each of our scanning activities yes
that's possible especially for doing
scanning you should look into modify
angle profiles of a camera so it can
emit different patterns because right
now in VA I think you can emit six
different patterns and use them in
different ones so where is not as much
crystals as you would want more question
is recently compatible with mobile so
iOS Android and Windows Phone or we
officially have plans for Android and
you get more information this later this
year we have audition four types of
actual tablets running on Android 60
inches phablets with built-in are 200
and we are working quite well I'd be
glad to see it on the market from OEMs
one day but we don't have any ETA for
four days of course
another question is again on the
extremities mode for hands tracking so
in full hands we are tracking the 22
Giants and we have the openness of the
hands extra in extremities mode you only
get the extreme each point and VN salads
but you don't get the openness of the
hand yes don't get any more questions
I've when all the ones I adds thanks for
so many versions and I only had one less
comments of someone who is actually
adding this webinar from this oh man
it's 4am there so congrats honestly
thanks for attending this webinar</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>